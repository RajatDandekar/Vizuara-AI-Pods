{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Multimodal Defect Detection at Stratos Manufacturing \u2014 Implementation Notebook"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Defect Detection at Stratos Manufacturing -- Implementation Notebook\n",
    "\n",
    "*Case Study Implementation: Multimodal Fusion for Semiconductor Quality Inspection*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision matplotlib numpy scikit-learn seaborn -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation (Synthetic)\n",
    "\n",
    "Since we do not have real semiconductor inspection data, we create a realistic synthetic dataset that captures the key multimodal structure of the problem."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDefectDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Synthetic multimodal defect detection dataset.\n",
    "\n",
    "    Generates correlated (image, process_log, text, label) tuples\n",
    "    where defects manifest across modalities.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_samples=5000, img_size=64):\n",
    "        self.n_samples = n_samples\n",
    "        self.img_size = img_size\n",
    "\n",
    "        # Class distribution: 85% good, 15% defective (split among 4 types)\n",
    "        self.labels = torch.zeros(n_samples, dtype=torch.long)\n",
    "        n_good = int(n_samples * 0.85)\n",
    "        n_defect_each = (n_samples - n_good) // 4\n",
    "\n",
    "        idx = n_good\n",
    "        for cls in range(1, 5):\n",
    "            self.labels[idx:idx+n_defect_each] = cls\n",
    "            idx += n_defect_each\n",
    "\n",
    "        # Shuffle\n",
    "        perm = torch.randperm(n_samples)\n",
    "        self.labels = self.labels[perm]\n",
    "\n",
    "        # Generate images\n",
    "        self.images = torch.zeros(n_samples, 3, img_size, img_size)\n",
    "\n",
    "        # Generate process logs (48 parameters)\n",
    "        self.process_logs = torch.randn(n_samples, 48) * 0.5\n",
    "\n",
    "        # Generate operator notes (encoded as category)\n",
    "        self.has_notes = torch.rand(n_samples) > 0.4  # 60% have notes\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            label = self.labels[i].item()\n",
    "            self._generate_sample(i, label)\n",
    "\n",
    "    def _generate_sample(self, idx, label):\n",
    "        img = torch.randn(3, self.img_size, self.img_size) * 0.1 + 0.5\n",
    "\n",
    "        if label == 0:  # Good die\n",
    "            pass  # Clean image, normal process\n",
    "        elif label == 1:  # Particle contamination\n",
    "            # Add bright spots\n",
    "            n_particles = np.random.randint(1, 5)\n",
    "            for _ in range(n_particles):\n",
    "                cx, cy = np.random.randint(5, self.img_size-5, 2)\n",
    "                r = np.random.randint(1, 4)\n",
    "                y, x = np.ogrid[-r:r+1, -r:r+1]\n",
    "                mask = x**2 + y**2 <= r**2\n",
    "                img[0, max(0,cy-r):cy+r+1, max(0,cx-r):cx+r+1][mask[:img.shape[1]-max(0,cy-r), :img.shape[2]-max(0,cx-r)]] += 0.5\n",
    "            self.process_logs[idx, 5] += 1.5  # Particle count sensor\n",
    "        elif label == 2:  # Scratch\n",
    "            # Add a line\n",
    "            y0, x0 = np.random.randint(10, self.img_size-10, 2)\n",
    "            angle = np.random.uniform(0, np.pi)\n",
    "            length = np.random.randint(15, 40)\n",
    "            for t in range(length):\n",
    "                yy = int(y0 + t * np.sin(angle))\n",
    "                xx = int(x0 + t * np.cos(angle))\n",
    "                if 0 <= yy < self.img_size and 0 <= xx < self.img_size:\n",
    "                    img[:, yy, max(0,xx-1):xx+2] -= 0.3\n",
    "            self.process_logs[idx, 12] += 2.0  # Vibration sensor\n",
    "        elif label == 3:  # Pattern defect\n",
    "            # Add a shifted pattern\n",
    "            x = torch.linspace(-3, 3, self.img_size)\n",
    "            y = torch.linspace(-3, 3, self.img_size)\n",
    "            XX, YY = torch.meshgrid(x, y, indexing='ij')\n",
    "            pattern = torch.sin(XX * 5 + np.random.uniform(-1, 1)) * 0.3\n",
    "            img[1] += pattern\n",
    "            self.process_logs[idx, 20] += 1.8  # Exposure dose deviation\n",
    "        elif label == 4:  # Process deviation\n",
    "            # Discoloration + process anomaly\n",
    "            img[0] += 0.15  # Red tint\n",
    "            img[2] -= 0.1\n",
    "            self.process_logs[idx, 30] += 2.5  # Temperature deviation\n",
    "            self.process_logs[idx, 31] += 1.5  # Pressure deviation\n",
    "\n",
    "        self.images[idx] = img.clamp(0, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.images[idx], self.process_logs[idx],\n",
    "                self.has_notes[idx].float(), self.labels[idx])\n",
    "\n",
    "# Create datasets\n",
    "train_data = SyntheticDefectDataset(5000)\n",
    "test_data = SyntheticDefectDataset(1000)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Class distribution\n",
    "class_names = ['Good', 'Particle', 'Scratch', 'Pattern', 'Process']\n",
    "counts = [(train_data.labels == c).sum().item() for c in range(5)]\n",
    "print(\"Class distribution:\")\n",
    "for name, count in zip(class_names, counts):\n",
    "    print(f\"  {name}: {count} ({count/len(train_data)*100:.1f}%)\")"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "\n",
    "for c in range(5):\n",
    "    idx = (train_data.labels == c).nonzero(as_tuple=True)[0][0].item()\n",
    "    img = train_data.images[idx]\n",
    "\n",
    "    axes[0, c].imshow(img.permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[0, c].set_title(f'{class_names[c]}', fontsize=12, fontweight='bold')\n",
    "    axes[0, c].axis('off')\n",
    "\n",
    "    # Process log deviations\n",
    "    process = train_data.process_logs[idx].numpy()\n",
    "    axes[1, c].bar(range(48), process, color='steelblue', alpha=0.7)\n",
    "    axes[1, c].set_title(f'Process Params')\n",
    "    axes[1, c].set_xlabel('Parameter Index')\n",
    "    axes[1, c].set_ylim(-4, 4)\n",
    "    axes[1, c].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.suptitle('Sample Images and Process Parameters by Defect Class', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class imbalance visualization\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "bars = ax.bar(class_names, counts, color=['#4CAF50', '#FF5722', '#FF9800', '#2196F3', '#9C27B0'])\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Class Distribution (Imbalanced -- 85% Good Dies)')\n",
    "for bar, count in zip(bars, counts):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 20,\n",
    "           str(count), ha='center', va='bottom', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline: Vision-Only Model"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionOnlyBaseline(nn.Module):\n",
    "    def __init__(self, img_size=64, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d(4),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 4 * 4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, images, process_logs=None, has_notes=None):\n",
    "        return self.encoder(images)\n",
    "\n",
    "baseline = VisionOnlyBaseline().to(device)\n",
    "print(f\"Baseline parameters: {sum(p.numel() for p in baseline.parameters()):,}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multimodal Fusion Model"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedCrossAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.cross_attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.gate = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, query, kv):\n",
    "        attn_out, attn_weights = self.cross_attn(self.norm(query), kv, kv)\n",
    "        gate_val = torch.tanh(self.gate)\n",
    "        return query + gate_val * attn_out, attn_weights\n",
    "\n",
    "\n",
    "class MultimodalDefectDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    Full multimodal model with gated cross-attention fusion.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=64, patch_size=8, embed_dim=128,\n",
    "                 process_dim=48, num_classes=5, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        patch_dim = patch_size * patch_size * 3\n",
    "\n",
    "        # Image encoder (patch-based)\n",
    "        self.patch_proj = nn.Linear(patch_dim, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches, embed_dim) * 0.02)\n",
    "        self.vis_transformer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads, dim_feedforward=embed_dim*4,\n",
    "            batch_first=True, dropout=0.1\n",
    "        )\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Process log encoder\n",
    "        self.process_proj = nn.Sequential(\n",
    "            nn.Linear(process_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        # Note encoder (simplified: just a learnable embedding for has/no notes)\n",
    "        self.note_embed = nn.Embedding(2, embed_dim)\n",
    "\n",
    "        # Gated cross-attention layers\n",
    "        self.vision_process_xattn = GatedCrossAttention(embed_dim, num_heads)\n",
    "        self.vision_note_xattn = GatedCrossAttention(embed_dim, num_heads)\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, images, process_logs, has_notes):\n",
    "        B = images.shape[0]\n",
    "        p = self.patch_size\n",
    "\n",
    "        # Encode image patches\n",
    "        patches = images.unfold(2, p, p).unfold(3, p, p)\n",
    "        patches = patches.contiguous().view(B, 3, -1, p, p)\n",
    "        patches = patches.permute(0, 2, 1, 3, 4).reshape(B, self.num_patches, -1)\n",
    "        vis_tokens = self.patch_proj(patches) + self.pos_embed\n",
    "        vis_tokens = self.vis_transformer(vis_tokens)\n",
    "\n",
    "        # Encode process logs\n",
    "        process_token = self.process_proj(process_logs).unsqueeze(1)\n",
    "\n",
    "        # Encode notes\n",
    "        note_ids = has_notes.long()\n",
    "        note_token = self.note_embed(note_ids).unsqueeze(1)\n",
    "\n",
    "        # Fuse: vision <- process\n",
    "        vis_tokens, attn_vp = self.vision_process_xattn(vis_tokens, process_token)\n",
    "        # Fuse: vision <- notes\n",
    "        vis_tokens, attn_vn = self.vision_note_xattn(vis_tokens, note_token)\n",
    "\n",
    "        # Pool and classify\n",
    "        pooled = vis_tokens.mean(dim=1)\n",
    "        logits = self.classifier(pooled)\n",
    "\n",
    "        return logits\n",
    "\n",
    "multimodal = MultimodalDefectDetector().to(device)\n",
    "print(f\"Multimodal parameters: {sum(p.numel() for p in multimodal.parameters()):,}\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(logits, targets, gamma=2.0, alpha=None):\n",
    "    \"\"\"Focal loss for class-imbalanced classification.\"\"\"\n",
    "    ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    focal = ((1 - pt) ** gamma) * ce_loss\n",
    "\n",
    "    if alpha is not None:\n",
    "        alpha_t = alpha[targets]\n",
    "        focal = alpha_t * focal\n",
    "\n",
    "    return focal.mean()\n",
    "\n",
    "# Class weights (inverse frequency)\n",
    "class_counts = torch.tensor([c for c in counts], dtype=torch.float32)\n",
    "class_weights = (1.0 / class_counts) * class_counts.sum() / len(class_counts)\n",
    "class_weights = class_weights.to(device)\n",
    "print(f\"Class weights: {class_weights.cpu().numpy()}\")\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs=30, lr=1e-3, use_focal=True):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    train_losses, test_f1s, escape_rates = [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for imgs, plogs, notes, labels in train_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            plogs = plogs.to(device)\n",
    "            notes = notes.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(imgs, plogs, notes)\n",
    "\n",
    "            if use_focal:\n",
    "                loss = focal_loss(logits, labels, gamma=2.0, alpha=class_weights)\n",
    "            else:\n",
    "                loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for imgs, plogs, notes, labels in test_loader:\n",
    "                imgs = imgs.to(device)\n",
    "                plogs = plogs.to(device)\n",
    "                notes = notes.to(device)\n",
    "                logits = model(imgs, plogs, notes)\n",
    "                preds = logits.argmax(1).cpu()\n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(labels)\n",
    "\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        test_f1s.append(f1)\n",
    "\n",
    "        # Escape rate: defective dies classified as good\n",
    "        defective = all_labels > 0\n",
    "        escaped = (all_preds[defective] == 0).float().mean().item() if defective.sum() > 0 else 0\n",
    "        escape_rates.append(escaped)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"  Epoch {epoch+1:3d}: Loss={train_losses[-1]:.4f}, \"\n",
    "                  f\"F1={f1:.4f}, Escape={escaped:.4f}\")\n",
    "\n",
    "    return train_losses, test_f1s, escape_rates"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Vision-Only Baseline...\")\n",
    "baseline = VisionOnlyBaseline().to(device)\n",
    "bl_losses, bl_f1s, bl_escapes = train_model(baseline, train_loader, test_loader, use_focal=False)\n",
    "\n",
    "print(\"\\nTraining Multimodal Model...\")\n",
    "multimodal = MultimodalDefectDetector().to(device)\n",
    "mm_losses, mm_f1s, mm_escapes = train_model(multimodal, train_loader, test_loader, use_focal=True)"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(bl_losses, label='Vision-Only', color='#FF5722', linewidth=2)\n",
    "axes[0].plot(mm_losses, label='Multimodal', color='#2196F3', linewidth=2)\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(bl_f1s, label='Vision-Only', color='#FF5722', linewidth=2)\n",
    "axes[1].plot(mm_f1s, label='Multimodal', color='#2196F3', linewidth=2)\n",
    "axes[1].set_title('Macro F1 Score')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(bl_escapes, label='Vision-Only', color='#FF5722', linewidth=2)\n",
    "axes[2].plot(mm_escapes, label='Multimodal', color='#2196F3', linewidth=2)\n",
    "axes[2].axhline(y=0.01, color='green', linestyle='--', label='Target (1%)')\n",
    "axes[2].set_title('Defect Escape Rate')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Vision-Only vs Multimodal Fusion Model', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Vision-Only: F1={bl_f1s[-1]:.4f}, Escape Rate={bl_escapes[-1]:.4f}\")\n",
    "print(f\"  Multimodal:  F1={mm_f1s[-1]:.4f}, Escape Rate={mm_escapes[-1]:.4f}\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for idx, (model, name) in enumerate([(baseline, 'Vision-Only'), (multimodal, 'Multimodal')]):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, plogs, notes, labels in test_loader:\n",
    "            logits = model(imgs.to(device), plogs.to(device), notes.to(device))\n",
    "            all_preds.append(logits.argmax(1).cpu())\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "               xticklabels=class_names, yticklabels=class_names)\n",
    "    axes[idx].set_title(f'{name}\\n{classification_report(all_labels, all_preds, target_names=class_names, output_dict=False)[:50]}...')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "    axes[idx].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze gate values -- how much visual information comes from each modality\n",
    "multimodal.eval()\n",
    "with torch.no_grad():\n",
    "    vp_gate = torch.tanh(multimodal.vision_process_xattn.gate).item()\n",
    "    vn_gate = torch.tanh(multimodal.vision_note_xattn.gate).item()\n",
    "\n",
    "print(f\"Learned Gate Values:\")\n",
    "print(f\"  Vision-Process gate: {vp_gate:.4f}\")\n",
    "print(f\"  Vision-Note gate:    {vn_gate:.4f}\")\n",
    "print(f\"\\nThe model learned to weight process data {abs(vp_gate)/abs(vn_gate):.1f}x more than notes.\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deployment Optimization"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latency benchmark\n",
    "import time\n",
    "\n",
    "multimodal.eval()\n",
    "dummy_imgs = torch.randn(1, 3, 64, 64).to(device)\n",
    "dummy_plogs = torch.randn(1, 48).to(device)\n",
    "dummy_notes = torch.tensor([1.0]).to(device)\n",
    "\n",
    "# Warmup\n",
    "for _ in range(10):\n",
    "    with torch.no_grad():\n",
    "        _ = multimodal(dummy_imgs, dummy_plogs, dummy_notes)\n",
    "\n",
    "# Benchmark\n",
    "times = []\n",
    "for _ in range(100):\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = multimodal(dummy_imgs, dummy_plogs, dummy_notes)\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    times.append((time.time() - start) * 1000)\n",
    "\n",
    "print(f\"Inference Latency (single die):\")\n",
    "print(f\"  Mean:  {np.mean(times):.2f} ms\")\n",
    "print(f\"  P50:   {np.percentile(times, 50):.2f} ms\")\n",
    "print(f\"  P95:   {np.percentile(times, 95):.2f} ms\")\n",
    "print(f\"  P99:   {np.percentile(times, 99):.2f} ms\")\n",
    "print(f\"  Target: <200 ms {'PASS' if np.percentile(times, 99) < 200 else 'FAIL'}\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ethics and Fairness Considerations"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate per-fab performance analysis\n",
    "print(\"Per-Fab Performance Analysis (Simulated)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Randomly assign test samples to fabs\n",
    "fab_ids = np.random.choice(['Fab A', 'Fab B', 'Fab C'], size=len(test_data))\n",
    "\n",
    "multimodal.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, plogs, notes, labels in test_loader:\n",
    "        logits = multimodal(imgs.to(device), plogs.to(device), notes.to(device))\n",
    "        all_preds.append(logits.argmax(1).cpu())\n",
    "        all_labels.append(labels)\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "for fab in ['Fab A', 'Fab B', 'Fab C']:\n",
    "    mask = fab_ids == fab\n",
    "    fab_labels = all_labels[mask]\n",
    "    fab_preds = all_preds[mask]\n",
    "\n",
    "    defective = fab_labels > 0\n",
    "    if defective.sum() > 0:\n",
    "        escape = (fab_preds[defective] == 0).mean()\n",
    "    else:\n",
    "        escape = 0\n",
    "\n",
    "    good = fab_labels == 0\n",
    "    if good.sum() > 0:\n",
    "        fpr = (fab_preds[good] > 0).mean()\n",
    "    else:\n",
    "        fpr = 0\n",
    "\n",
    "    f1 = f1_score(fab_labels, fab_preds, average='macro', zero_division=0)\n",
    "    print(f\"  {fab}: F1={f1:.4f}, Escape={escape:.4f}, FPR={fpr:.4f}\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCongratulations! You have built a multimodal defect detection system.\")\n",
    "print(\"Key achievement: combining vision, process data, and operator notes\")\n",
    "print(\"reduced the defect escape rate compared to vision-only inspection.\")"
   ],
   "id": "cell_25"
  }
 ]
}