{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Clinical NLP for Automated Medical Coding â€” Implementation Notebook"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Clinical NLP for Automated Medical Coding\n",
    "## Implementation Notebook\n",
    "\n",
    "---\n",
    "\n",
    "**Scenario:** You are an ML engineer at MedScribe AI, a healthtech startup building clinical documentation intelligence for hospitals. Your task is to build a BERT-based Named Entity Recognition (NER) system that extracts clinical entities (diagnoses, medications, procedures) from unstructured physician notes and maps them to ICD billing codes.\n",
    "\n",
    "**Current system:** A rule-based NLP pipeline with 68% entity extraction accuracy and 74% negation detection accuracy. Your target: 88%+ entity-level F1.\n",
    "\n",
    "**Why BERT:** The rule-based system fails on negation (\"patient denies chest pain\" vs. \"patient reports chest pain\"), context-dependent terminology, and implicit references. BERT's deep bidirectional self-attention can capture the full context around each word, making it ideal for clinical NER.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Data Acquisition and Preprocessing\n",
    "\n",
    "We use the **BC5CDR dataset** (BioCreative V Chemical Disease Relation) -- a publicly available biomedical NER benchmark with 1,500 annotated PubMed abstracts containing chemical and disease entity annotations. This closely mirrors the structure of clinical NER while being accessible without credentialed access.\n",
    "\n",
    "The BC5CDR dataset has the same core challenge as MedScribe's clinical notes: extracting biomedical entities from unstructured text where context determines meaning."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install transformers datasets seqeval matplotlib scikit-learn -q\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Load the BC5CDR dataset (biomedical NER)\n",
    "dataset = load_dataset(\"tner/bc5cdr\")\n",
    "\n",
    "# Examine the dataset structure\n",
    "print(\"Dataset splits:\", dataset)\n",
    "print(\"\\nSample entry:\")\n",
    "sample = dataset['train'][0]\n",
    "print(\"Tokens:\", sample['tokens'])\n",
    "print(\"Tags:\", sample['tags'])\n",
    "\n",
    "print(\"\\nLabel mapping:\")\n",
    "label_names = dataset['train'].features['tags'].feature.names\n",
    "for i, name in enumerate(label_names):\n",
    "    print(f\"  {i}: {name}\")\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "print(f\"\\nTokenizer vocabulary size: {tokenizer.vocab_size}\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1: Data Exploration\n",
    "\n",
    "Analyze the BC5CDR dataset to understand its characteristics before building any models."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Analyze the BC5CDR dataset to understand its characteristics.\n",
    "\n",
    "    Compute and print the following statistics:\n",
    "    1. Number of examples in train/validation/test splits\n",
    "    2. Average number of tokens per example\n",
    "    3. Distribution of entity types (count of B-Chemical, I-Chemical,\n",
    "       B-Disease, I-Disease, O labels across the training set)\n",
    "    4. Percentage of tokens that are entity tokens vs. O tokens\n",
    "    5. Average number of entities per example\n",
    "    6. Distribution of entity lengths (in tokens)\n",
    "\n",
    "    Args:\n",
    "        dataset: HuggingFace dataset with 'tokens' and 'tags' fields\n",
    "\n",
    "    Returns:\n",
    "        dict with keys: 'split_sizes', 'avg_tokens', 'label_distribution',\n",
    "              'entity_percentage', 'avg_entities', 'entity_lengths'\n",
    "\n",
    "    Hints:\n",
    "    - Use dataset['train'].features['tags'].feature.names to get label names\n",
    "    - An entity starts at any B- tag and continues through consecutive I- tags\n",
    "    - To count entities, iterate through tags and count B- tag occurrences\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "stats = explore_dataset(dataset)"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: Check your implementation\n",
    "assert 'split_sizes' in stats, \"Missing 'split_sizes' key\"\n",
    "assert 'label_distribution' in stats, \"Missing 'label_distribution' key\"\n",
    "assert stats['split_sizes']['train'] > 0, \"Training set should not be empty\"\n",
    "assert stats['entity_percentage'] < 100, \"Entity percentage should be less than 100%\"\n",
    "print(\"Basic verification passed.\")"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. What percentage of tokens are entity tokens vs. O tokens? What problem does this class imbalance cause for training?\n",
    "2. Are Chemical and Disease entities roughly equally represented? If not, what impact might this have on per-entity-type F1?\n",
    "3. What is the average entity length in tokens? How does this inform your max_length choice?"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Tokenization Alignment\n",
    "\n",
    "BERT uses WordPiece tokenization, which splits words into subword units. For NER, we need to carefully align the original word-level labels with the new subword tokens."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, tokenizer, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenize text and align NER labels with WordPiece subword tokens.\n",
    "\n",
    "    BERT's WordPiece tokenizer may split a single word into multiple subword\n",
    "    tokens. For example, \"myocardial\" might become [\"my\", \"##ocard\", \"##ial\"].\n",
    "    When this happens, we need to decide what label to assign to the subword\n",
    "    tokens.\n",
    "\n",
    "    Strategy: Assign the original word's label to the FIRST subword token.\n",
    "    Assign -100 to all subsequent subword tokens (this tells PyTorch to\n",
    "    ignore them in the loss computation).\n",
    "\n",
    "    Args:\n",
    "        examples: dict with 'tokens' (list of list of strings) and\n",
    "                  'tags' (list of list of ints)\n",
    "        tokenizer: BertTokenizerFast instance\n",
    "        max_length: maximum sequence length (default 128)\n",
    "\n",
    "    Returns:\n",
    "        dict with 'input_ids', 'attention_mask', 'labels' fields\n",
    "        ready for model training\n",
    "\n",
    "    Hints:\n",
    "    - Use tokenizer(examples['tokens'], is_split_into_words=True,\n",
    "          truncation=True, max_length=max_length, padding='max_length')\n",
    "    - Use tokenized_inputs.word_ids(batch_index=i) to get the mapping\n",
    "      from subword tokens to original words\n",
    "    - word_ids() returns None for special tokens ([CLS], [SEP], [PAD])\n",
    "      -- assign -100 to these positions\n",
    "    - For the first subword of each word, copy the original label\n",
    "    - For subsequent subwords of the same word, assign -100\n",
    "\n",
    "    Step-by-step:\n",
    "    1. Tokenize all examples with is_split_into_words=True\n",
    "    2. For each example in the batch:\n",
    "       a. Get word_ids for the tokenized sequence\n",
    "       b. Initialize labels list\n",
    "       c. Track previous_word_id (starts as None)\n",
    "       d. For each token position:\n",
    "          - If word_id is None: append -100\n",
    "          - If word_id != previous_word_id: append the original label\n",
    "          - If word_id == previous_word_id: append -100\n",
    "       e. Update previous_word_id\n",
    "    3. Add 'labels' to the tokenized output dict\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: Test tokenization alignment\n",
    "test_example = {\n",
    "    'tokens': [['The', 'patient', 'has', 'hypertension']],\n",
    "    'tags': [[0, 0, 0, 1]]  # Only 'hypertension' is an entity\n",
    "}\n",
    "result = tokenize_and_align_labels(test_example, tokenizer, max_length=16)\n",
    "assert 'labels' in result, \"Result must contain 'labels'\"\n",
    "assert len(result['labels'][0]) == 16, f\"Labels length should be 16, got {len(result['labels'][0])}\"\n",
    "# Check that [CLS] and [SEP] positions have -100\n",
    "assert result['labels'][0][0] == -100, \"[CLS] token should have label -100\"\n",
    "print(\"Tokenization alignment verification passed.\")\n",
    "print(f\"Input IDs: {result['input_ids'][0]}\")\n",
    "print(f\"Labels:    {result['labels'][0]}\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(result['input_ids'][0])\n",
    "print(f\"Tokens:    {tokens}\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.2 Exploratory Data Analysis\n",
    "\n",
    "Before building models, we need to understand the data distribution. This is especially important in NER because class imbalance (most tokens are O) and entity length distributions directly affect model performance.\n",
    "\n",
    "### TODO 3: Visualize Entity Distributions"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_entity_analysis(dataset, label_names):\n",
    "    \"\"\"\n",
    "    Create a comprehensive EDA visualization with 4 subplots:\n",
    "\n",
    "    1. Bar chart: Count of each entity type (B-Chemical, I-Chemical,\n",
    "       B-Disease, I-Disease, O) in the training set\n",
    "    2. Histogram: Distribution of sequence lengths (number of tokens\n",
    "       per example)\n",
    "    3. Histogram: Distribution of entity lengths (number of tokens per\n",
    "       entity) -- separate colors for Chemical vs Disease\n",
    "    4. Bar chart: Top 20 most frequent entity surface forms (the actual\n",
    "       text of extracted entities)\n",
    "\n",
    "    Args:\n",
    "        dataset: HuggingFace dataset with 'tokens' and 'tags'\n",
    "        label_names: list of label name strings\n",
    "\n",
    "    Hints:\n",
    "    - Use matplotlib with plt.subplots(2, 2, figsize=(14, 10))\n",
    "    - To extract entity surface forms, iterate through tokens/tags\n",
    "      and concatenate tokens that form each entity\n",
    "    - Use Counter from collections for frequency counting\n",
    "    - Add clear titles, axis labels, and a tight_layout()\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "plot_entity_analysis(dataset['train'], label_names)"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. Is the dataset balanced between Chemical and Disease entities? If not, should we use class-weighted loss?\n",
    "2. What is the most common entity length? Does this inform your choice of max_length?\n",
    "3. Are there any surprising entries in the top 20 entity list? (e.g., very short entities that might cause false positives)\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3 Baseline Approach\n",
    "\n",
    "Before training BERT, we establish a baseline using a simple dictionary-based approach -- similar to MedScribe's original rule-based system. This gives us a lower bound on performance and helps us appreciate the value of contextual representations.\n",
    "\n",
    "### TODO 4: Dictionary-Based NER Baseline"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_entity_dictionary(dataset, label_names):\n",
    "    \"\"\"\n",
    "    Build a simple entity dictionary from the training set.\n",
    "\n",
    "    Extract all entity surface forms (e.g., \"aspirin\", \"myocardial infarction\")\n",
    "    and their entity types from the training data. Store them in a dictionary\n",
    "    mapping surface form -> most common entity type.\n",
    "\n",
    "    Args:\n",
    "        dataset: training split with 'tokens' and 'tags'\n",
    "        label_names: list of label name strings\n",
    "\n",
    "    Returns:\n",
    "        dict mapping entity string (lowercased) -> entity type string\n",
    "\n",
    "    Hints:\n",
    "    - Iterate through each example's tokens and tags\n",
    "    - When you encounter a B- tag, start collecting tokens for a new entity\n",
    "    - Continue collecting while you see I- tags of the same type\n",
    "    - When the entity ends, join the tokens and add to the dictionary\n",
    "    - If an entity form appears with multiple types, keep the most common\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "def dictionary_ner(tokens, entity_dict):\n",
    "    \"\"\"\n",
    "    Perform NER using simple dictionary lookup with longest-match.\n",
    "\n",
    "    For each position in the token sequence, check if any subsequence\n",
    "    starting at that position matches an entry in the entity dictionary.\n",
    "    Use longest-match-first strategy.\n",
    "\n",
    "    Args:\n",
    "        tokens: list of token strings\n",
    "        entity_dict: dict mapping entity string -> entity type\n",
    "\n",
    "    Returns:\n",
    "        list of BIO tag strings, same length as tokens\n",
    "\n",
    "    Hints:\n",
    "    - For each start position, try matching subsequences of decreasing\n",
    "      length (longest match first)\n",
    "    - When a match is found, assign B-{type} to the first token and\n",
    "      I-{type} to subsequent tokens in the match\n",
    "    - Skip positions that are already tagged (inside a previously\n",
    "      matched entity)\n",
    "    - Assign 'O' to all unmatched tokens\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report, f1_score\n",
    "\n",
    "def evaluate_baseline(dataset_split, entity_dict, label_names):\n",
    "    \"\"\"Evaluate the dictionary baseline on a dataset split.\"\"\"\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    for example in dataset_split:\n",
    "        tokens = example['tokens']\n",
    "        true_tags = [label_names[t] for t in example['tags']]\n",
    "        pred_tags = dictionary_ner(tokens, entity_dict)\n",
    "        true_labels.append(true_tags)\n",
    "        pred_labels.append(pred_tags)\n",
    "\n",
    "    print(\"Dictionary Baseline Results:\")\n",
    "    print(classification_report(true_labels, pred_labels))\n",
    "    return f1_score(true_labels, pred_labels)\n",
    "\n",
    "# Build dictionary and evaluate\n",
    "entity_dict = build_entity_dictionary(dataset['train'], label_names)\n",
    "print(f\"Dictionary size: {len(entity_dict)} entities\")\n",
    "baseline_f1 = evaluate_baseline(dataset['test'], entity_dict, label_names)\n",
    "print(f\"\\nBaseline Entity-Level F1: {baseline_f1:.4f}\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. What is the dictionary baseline F1? How does this compare to MedScribe's rule-based system (0.68)?\n",
    "2. What types of entities does the dictionary approach miss? Why?\n",
    "3. Can a dictionary approach ever handle negation? Why or why not?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4 Model Design\n",
    "\n",
    "Now we build the BERT-based NER model. The architecture is straightforward: BERT-Base produces a 768-dimensional hidden state for each token, and we add a single linear classification layer that maps each hidden state to one of our entity labels.\n",
    "\n",
    "The power comes from BERT's pre-trained representations. Each token's 768-dimensional hidden state already encodes rich contextual information because of the bidirectional self-attention mechanism. The classification head simply learns to map this rich representation to entity labels.\n",
    "\n",
    "### TODO 5: Build the NER Model"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class BertForClinicalNER(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT-based model for clinical Named Entity Recognition.\n",
    "\n",
    "    Architecture:\n",
    "    1. BERT-Base encoder (768-dim hidden states, 12 layers, 12 heads)\n",
    "    2. Dropout layer for regularization\n",
    "    3. Linear classification head: 768 -> num_labels\n",
    "\n",
    "    The classification head is applied independently to each token position.\n",
    "    During training, tokens with label -100 are ignored in the loss computation.\n",
    "\n",
    "    Args:\n",
    "        num_labels: number of NER label classes (default: 11)\n",
    "        dropout_rate: dropout probability (default: 0.1)\n",
    "        model_name: pre-trained BERT model name (default: 'bert-base-uncased')\n",
    "\n",
    "    Forward pass:\n",
    "        Input: input_ids, attention_mask, labels (optional)\n",
    "        Output: dict with 'logits' (always) and 'loss' (if labels provided)\n",
    "\n",
    "    Hints:\n",
    "    - Use BertModel.from_pretrained(model_name) to load BERT (NOT\n",
    "      BertForTokenClassification -- build it yourself to understand\n",
    "      the architecture)\n",
    "    - The BERT output has shape (batch_size, seq_len, 768)\n",
    "    - Apply dropout to the BERT output before the linear layer\n",
    "    - Use nn.CrossEntropyLoss(ignore_index=-100) to handle masked tokens\n",
    "    - The loss should be computed over ALL non-masked token positions\n",
    "\n",
    "    Step-by-step for __init__:\n",
    "    1. Load pre-trained BERT model\n",
    "    2. Create dropout layer (nn.Dropout)\n",
    "    3. Create linear classifier (nn.Linear: 768 -> num_labels)\n",
    "\n",
    "    Step-by-step for forward:\n",
    "    1. Pass input_ids and attention_mask through BERT\n",
    "    2. Get the last hidden state from BERT output\n",
    "    3. Apply dropout\n",
    "    4. Pass through the linear classifier to get logits\n",
    "    5. If labels are provided, compute cross-entropy loss\n",
    "    6. Return dict with logits (and loss if applicable)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_labels=5, dropout_rate=0.1,\n",
    "                 model_name='bert-base-uncased'):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: Test your model\n",
    "model = BertForClinicalNER(num_labels=len(label_names))\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Test forward pass with dummy input\n",
    "dummy_input = tokenizer(\"The patient was diagnosed with diabetes mellitus\",\n",
    "                        return_tensors=\"pt\", padding=\"max_length\",\n",
    "                        max_length=32, truncation=True)\n",
    "dummy_labels = torch.zeros(1, 32, dtype=torch.long)\n",
    "output = model(dummy_input['input_ids'], dummy_input['attention_mask'],\n",
    "               labels=dummy_labels)\n",
    "\n",
    "assert 'logits' in output, \"Output must contain 'logits'\"\n",
    "assert 'loss' in output, \"Output must contain 'loss' when labels are provided\"\n",
    "assert output['logits'].shape == (1, 32, len(label_names)), \\\n",
    "    f\"Expected logits shape (1, 32, {len(label_names)}), got {output['logits'].shape}\"\n",
    "print(\"All assertions passed. Model architecture is correct.\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. Why do we use `ignore_index=-100` in the loss function? What tokens does this exclude?\n",
    "2. How many parameters are in the classification head vs. the BERT encoder? What does this ratio tell you about where the learning happens during fine-tuning?\n",
    "3. What would happen if we froze the BERT encoder and only trained the classification head?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.5 Training Strategy\n",
    "\n",
    "Fine-tuning BERT requires careful hyperparameter choices. The pre-trained weights encode valuable language knowledge, and aggressive training can destroy this knowledge (catastrophic forgetting). We use a small learning rate, warm up gradually, and apply weight decay selectively.\n",
    "\n",
    "**Why these choices:**\n",
    "- **Learning rate 3e-5:** Small enough to preserve pre-trained knowledge, large enough to adapt to clinical text. The original BERT paper recommends 2e-5 to 5e-5 for fine-tuning.\n",
    "- **AdamW over SGD:** AdamW handles the sparse gradients in NER well (most tokens are O, so gradients are sparse). It also decouples weight decay from the adaptive learning rate.\n",
    "- **Linear warmup:** Prevents large early updates that could destroy pre-trained features. The model gradually \"wakes up\" to the new task.\n",
    "- **Gradient clipping:** BERT's 12 transformer layers can produce large gradients during early training. Clipping at norm 1.0 prevents instability."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 3e-5\n",
    "NUM_EPOCHS = 5\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "# Prepare tokenized datasets\n",
    "tokenized_train = dataset['train'].map(\n",
    "    lambda x: tokenize_and_align_labels(x, tokenizer, max_length=128),\n",
    "    batched=True, remove_columns=dataset['train'].column_names\n",
    ")\n",
    "tokenized_val = dataset['validation'].map(\n",
    "    lambda x: tokenize_and_align_labels(x, tokenizer, max_length=128),\n",
    "    batched=True, remove_columns=dataset['validation'].column_names\n",
    ")\n",
    "\n",
    "tokenized_train.set_format('torch')\n",
    "tokenized_val.set_format('torch')\n",
    "\n",
    "train_loader = DataLoader(tokenized_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(tokenized_val, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Setup optimizer with weight decay (exclude bias and LayerNorm)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': WEIGHT_DECAY},\n",
    "    {'params': [p for n, p in model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE)\n",
    "\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(f\"Training on: {device}\")\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {warmup_steps}\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 6: Training and Evaluation Loops"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report as seq_classification_report\n",
    "from seqeval.metrics import f1_score as seq_f1_score\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, scheduler, device,\n",
    "                max_grad_norm=1.0):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model: BertForClinicalNER instance\n",
    "        train_loader: DataLoader for training data\n",
    "        optimizer: AdamW optimizer\n",
    "        scheduler: learning rate scheduler\n",
    "        device: torch device (cuda or cpu)\n",
    "        max_grad_norm: maximum gradient norm for clipping\n",
    "\n",
    "    Returns:\n",
    "        float: average training loss for the epoch\n",
    "\n",
    "    Hints:\n",
    "    - Set model to training mode with model.train()\n",
    "    - For each batch:\n",
    "      1. Move input_ids, attention_mask, labels to device\n",
    "      2. Zero the gradients\n",
    "      3. Forward pass through the model\n",
    "      4. Get the loss from the output\n",
    "      5. Backward pass (loss.backward())\n",
    "      6. Clip gradients with torch.nn.utils.clip_grad_norm_\n",
    "      7. Update weights (optimizer.step())\n",
    "      8. Update learning rate (scheduler.step())\n",
    "      9. Accumulate the loss for logging\n",
    "    - Return the average loss across all batches\n",
    "    - Print progress every 50 batches (batch loss and learning rate)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, device, label_names):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a validation/test set.\n",
    "\n",
    "    Args:\n",
    "        model: BertForClinicalNER instance\n",
    "        val_loader: DataLoader for validation/test data\n",
    "        device: torch device\n",
    "        label_names: list of label name strings\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, entity_f1, classification_report_string)\n",
    "\n",
    "    Hints:\n",
    "    - Set model to evaluation mode with model.eval()\n",
    "    - Use torch.no_grad() context manager\n",
    "    - For each batch:\n",
    "      1. Forward pass to get logits and loss\n",
    "      2. Get predictions via argmax over the label dimension\n",
    "      3. Convert predictions and true labels to label name strings\n",
    "      4. IMPORTANT: Skip tokens where the true label is -100\n",
    "         (subword tokens, special tokens, padding)\n",
    "    - Use seqeval.metrics.f1_score and classification_report\n",
    "    - Collect predictions at the SEQUENCE level (list of list of\n",
    "      label strings) for seqeval\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "best_f1 = 0.0\n",
    "train_losses = []\n",
    "val_f1_scores = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler,\n",
    "                             device, MAX_GRAD_NORM)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Evaluate\n",
    "    val_loss, val_f1, report = evaluate(model, val_loader, device, label_names)\n",
    "    val_f1_scores.append(val_f1)\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS} ({epoch_time:.1f}s)\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f}\")\n",
    "    print(f\"  Val F1:     {val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(f\"  New best model saved (F1: {best_f1:.4f})\")\n",
    "\n",
    "    print(\"\\n\" + report)\n",
    "\n",
    "print(f\"\\nTraining complete. Best validation F1: {best_f1:.4f}\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.6 Evaluation\n",
    "\n",
    "Now we perform a comprehensive evaluation comparing BERT's performance against the dictionary baseline.\n",
    "\n",
    "### TODO 7: Comprehensive Evaluation"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_evaluation(model, test_loader, device, label_names,\n",
    "                             baseline_f1, train_losses, val_f1_scores):\n",
    "    \"\"\"\n",
    "    Perform a thorough evaluation comparing BERT to the baseline.\n",
    "\n",
    "    Generate the following:\n",
    "    1. Entity-level classification report (precision, recall, F1 per type)\n",
    "    2. Confusion matrix for token-level predictions (use sklearn)\n",
    "    3. A comparison bar chart: Baseline F1 vs BERT F1 for each entity type\n",
    "    4. Training curves plot: train loss and validation F1 vs epoch\n",
    "\n",
    "    Args:\n",
    "        model: trained BertForClinicalNER (load best_model.pt weights)\n",
    "        test_loader: DataLoader for test data\n",
    "        device: torch device\n",
    "        label_names: list of label name strings\n",
    "        baseline_f1: float, the dictionary baseline F1 score\n",
    "        train_losses: list of per-epoch training losses\n",
    "        val_f1_scores: list of per-epoch validation F1 scores\n",
    "\n",
    "    Returns:\n",
    "        dict with 'overall_f1', 'per_type_f1', 'improvement_over_baseline'\n",
    "\n",
    "    Hints:\n",
    "    - Load best model weights: model.load_state_dict(torch.load('best_model.pt'))\n",
    "    - Use seqeval for entity-level metrics\n",
    "    - Use sklearn.metrics.confusion_matrix for token-level confusion\n",
    "    - Use matplotlib for all plots with plt.subplots(2, 2, figsize=(14, 10))\n",
    "    - For the comparison chart, you will need to compute per-entity-type\n",
    "      F1 for the dictionary baseline as well\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Prepare test data\n",
    "tokenized_test = dataset['test'].map(\n",
    "    lambda x: tokenize_and_align_labels(x, tokenizer, max_length=128),\n",
    "    batched=True, remove_columns=dataset['test'].column_names\n",
    ")\n",
    "tokenized_test.set_format('torch')\n",
    "test_loader = DataLoader(tokenized_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "results = comprehensive_evaluation(model, test_loader, device, label_names,\n",
    "                                   baseline_f1, train_losses, val_f1_scores)\n",
    "print(f\"\\nBERT Entity-Level F1: {results['overall_f1']:.4f}\")\n",
    "print(f\"Baseline F1: {baseline_f1:.4f}\")\n",
    "print(f\"Improvement: {results['improvement_over_baseline']:.4f}\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. Which entity type benefits most from BERT? Why might BERT help more for some entity types than others?\n",
    "2. Is there a significant gap between precision and recall for any entity type? What does this tell you about the model's behavior?\n",
    "3. Does the training loss curve suggest overfitting? How would you tell?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.7 Error Analysis\n",
    "\n",
    "Understanding where the model fails is as important as knowing where it succeeds. In a clinical setting, certain errors are more dangerous than others -- missing a diagnosis is worse than a boundary error.\n",
    "\n",
    "### TODO 8: Systematic Error Analysis"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(model, dataset_split, tokenizer, device, label_names,\n",
    "                   num_examples=100):\n",
    "    \"\"\"\n",
    "    Perform systematic error analysis on model predictions.\n",
    "\n",
    "    Analyze the first num_examples from the dataset and categorize errors\n",
    "    into the following types:\n",
    "    1. Boundary errors: entity detected but span boundaries are wrong\n",
    "    2. Type errors: entity span is correct but type is wrong\n",
    "    3. Missing entities: true entity completely missed (false negative)\n",
    "    4. Hallucinated entities: entity predicted where none exists (false positive)\n",
    "    5. Negation errors: entity in a negated context incorrectly tagged\n",
    "       (check for negation trigger words within 5 tokens)\n",
    "\n",
    "    For each error category, collect and print:\n",
    "    - Count and percentage of total errors\n",
    "    - 3 representative examples showing the token sequence, true labels,\n",
    "      and predicted labels (highlight the error region)\n",
    "\n",
    "    Args:\n",
    "        model: trained BertForClinicalNER\n",
    "        dataset_split: dataset split to analyze\n",
    "        tokenizer: BertTokenizerFast instance\n",
    "        device: torch device\n",
    "        label_names: list of label name strings\n",
    "        num_examples: number of examples to analyze\n",
    "\n",
    "    Returns:\n",
    "        dict mapping error_type -> list of error instances\n",
    "\n",
    "    Hints:\n",
    "    - For each example, compare predicted entity spans to true entity spans\n",
    "    - An entity span is defined by its start index, end index, and type\n",
    "    - Boundary error: overlap between predicted and true span, but not exact\n",
    "    - Use negation triggers: ['no', 'not', 'without', 'denies', 'denied',\n",
    "      'negative', 'absent', 'ruled out', 'unlikely']\n",
    "    - Format examples clearly with aligned columns for readability\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "errors = error_analysis(model, dataset['test'], tokenizer, device, label_names)"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. Which error category is most common? What does this suggest about the model's weaknesses?\n",
    "2. Are there patterns in the missing entities? (e.g., are they unusually long, rare, or ambiguous?)\n",
    "3. How would you prioritize fixing these errors in a production clinical system? Which are most clinically dangerous?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.8 Inference Optimization\n",
    "\n",
    "MedScribe requires code suggestions within 3 seconds of a note being finalized. Let us profile our model's inference performance and explore optimization strategies.\n",
    "\n",
    "### TODO 9: Latency Profiling"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_inference(model, tokenizer, device, sample_texts=None):\n",
    "    \"\"\"\n",
    "    Profile model inference latency and explore optimization strategies.\n",
    "\n",
    "    Steps:\n",
    "    1. Measure baseline inference latency on sample clinical texts\n",
    "       (average over 100 runs, report mean, p50, p95, p99)\n",
    "    2. Compare latency for different sequence lengths (32, 64, 128, 256, 512)\n",
    "    3. Measure the effect of batch size on throughput (notes/second)\n",
    "    4. Implement and measure dynamic padding (pad to longest in batch,\n",
    "       not max_length) -- report the speedup\n",
    "\n",
    "    Args:\n",
    "        model: trained BertForClinicalNER\n",
    "        tokenizer: BertTokenizerFast\n",
    "        device: torch device\n",
    "        sample_texts: optional list of sample clinical texts. If None,\n",
    "            use the following defaults:\n",
    "            [\n",
    "                \"Patient presents with acute chest pain radiating to left arm.\",\n",
    "                \"Prescribed metformin 500mg twice daily for type 2 diabetes.\",\n",
    "                \"CT scan of abdomen shows no evidence of appendicitis.\",\n",
    "                \"History of hypertension, currently on lisinopril 10mg.\",\n",
    "            ]\n",
    "\n",
    "    Returns:\n",
    "        dict with latency statistics and throughput measurements\n",
    "\n",
    "    Hints:\n",
    "    - Use torch.cuda.synchronize() before timing on GPU\n",
    "    - Use time.perf_counter() for high-resolution timing\n",
    "    - For dynamic padding, tokenize the batch without max_length,\n",
    "      then pad to the longest sequence in the batch\n",
    "    - Calculate throughput as: batch_size / batch_latency\n",
    "    - Create a line plot: sequence length vs latency\n",
    "    - Create a bar chart: batch size vs throughput\n",
    "\n",
    "    MedScribe's requirement: p95 latency < 500ms per note.\n",
    "    Does your model meet this requirement?\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "latency_stats = profile_inference(model, tokenizer, device)"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. How does inference latency scale with sequence length? Is it linear or quadratic? Why? (Hint: think about the self-attention computation.)\n",
    "2. What is the optimal batch size for throughput on your hardware? Is there a point of diminishing returns?\n",
    "3. Does dynamic padding provide meaningful speedup? In what scenarios would it matter most?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.9 Ethical and Regulatory Analysis\n",
    "\n",
    "Deploying an NLP model in a clinical setting carries significant ethical responsibilities. Errors can lead to wrong diagnoses, incorrect treatments, and harm to patients. Before deploying this model, we must carefully assess its biases, calibration, and failure modes.\n",
    "\n",
    "### TODO 10: Clinical AI Ethics Assessment"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ethics_assessment(model, tokenizer, device, label_names):\n",
    "    \"\"\"\n",
    "    Conduct an ethical impact assessment for deploying this NER model\n",
    "    in a clinical setting.\n",
    "\n",
    "    Implement the following analyses:\n",
    "\n",
    "    1. Demographic bias test: Run the model on matched sentence pairs\n",
    "       that differ only in demographic terms. For example:\n",
    "       - \"The [male/female] patient was diagnosed with depression.\"\n",
    "       - \"The [young/elderly] patient has chronic pain.\"\n",
    "       Compare entity predictions across demographic variants. Any\n",
    "       difference indicates potential bias.\n",
    "\n",
    "    2. Confidence calibration: For all predictions on the test set,\n",
    "       bin predictions by confidence (softmax probability) and compute\n",
    "       accuracy within each bin. Plot a reliability diagram. A well-\n",
    "       calibrated model should have accuracy ~= confidence in each bin.\n",
    "\n",
    "    3. Failure mode severity ranking: For each error type from\n",
    "       error_analysis, assign a clinical severity score:\n",
    "       - Critical (3): missed diagnosis, wrong medication\n",
    "       - Moderate (2): wrong anatomy, missed lab test\n",
    "       - Minor (1): boundary error, partial entity\n",
    "\n",
    "       Report the distribution of error severities.\n",
    "\n",
    "    Print a summary with:\n",
    "    - Whether demographic bias was detected (yes/no for each pair)\n",
    "    - Expected Calibration Error (ECE)\n",
    "    - Distribution of error severities\n",
    "    - A brief recommendation: is this model safe to deploy? What\n",
    "      guardrails are needed?\n",
    "\n",
    "    Args:\n",
    "        model: trained BertForClinicalNER\n",
    "        tokenizer: BertTokenizerFast\n",
    "        device: torch device\n",
    "        label_names: list of label name strings\n",
    "\n",
    "    Returns:\n",
    "        dict with 'bias_results', 'ece', 'severity_distribution',\n",
    "             'deployment_recommendation'\n",
    "\n",
    "    Hints:\n",
    "    - For bias testing, create pairs of sentences that are identical\n",
    "      except for the demographic term\n",
    "    - For calibration, use torch.softmax on logits and take the max\n",
    "      probability as the confidence score\n",
    "    - Use 10 equally-spaced bins for the reliability diagram\n",
    "    - ECE below 0.05 is considered well-calibrated\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "ethics_results = ethics_assessment(model, tokenizer, device, label_names)"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. Did you detect demographic bias? If so, what is the most likely cause and how would you mitigate it?\n",
    "2. Is the model well-calibrated? Why does calibration matter in a clinical setting? (Consider: a doctor sees \"confidence: 0.90\" -- what should that mean?)\n",
    "3. Based on your error severity analysis, what specific guardrails would you recommend for production deployment?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you built a complete clinical NER pipeline:\n",
    "\n",
    "1. **Data exploration**: Analyzed the BC5CDR biomedical NER dataset and understood its class imbalance and entity distributions\n",
    "2. **Tokenization alignment**: Solved the critical problem of aligning word-level NER labels with BERT's subword tokens\n",
    "3. **Dictionary baseline**: Built a rule-based NER system (similar to MedScribe's legacy system) and quantified its limitations\n",
    "4. **BERT NER model**: Constructed a BERT-based token classifier from scratch (not using a pre-built class)\n",
    "5. **Fine-tuning**: Trained the model with careful hyperparameter choices to prevent catastrophic forgetting\n",
    "6. **Evaluation**: Compared BERT against the baseline with entity-level F1 and per-type analysis\n",
    "7. **Error analysis**: Categorized model failures to understand where clinical risk lies\n",
    "8. **Latency profiling**: Verified the model meets MedScribe's 500ms inference budget\n",
    "9. **Ethics assessment**: Tested for demographic bias, calibration, and clinical error severity\n",
    "\n",
    "For next steps, read **Section 4** of the case study document (Production and System Design Extension) to understand how this model would be deployed at scale across MedScribe's 12 hospital clients, including API design, monitoring, drift detection, and A/B testing."
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "chatbot"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸ’¬ AI Teaching Assistant â€” Click â–¶ to start\n",
    "#@markdown This AI chatbot reads your notebook and can answer questions about any concept, code, or exercise.\n",
    "\n",
    "import json as _json\n",
    "import requests as _requests\n",
    "from google.colab import output as _output\n",
    "from IPython.display import display, HTML as _HTML, Markdown as _Markdown\n",
    "\n",
    "# --- Read notebook content for context ---\n",
    "def _get_notebook_context():\n",
    "    try:\n",
    "        from google.colab import _message\n",
    "        nb = _message.blocking_request(\"get_ipynb\", request=\"\", timeout_sec=10)\n",
    "        cells = nb.get(\"ipynb\", {}).get(\"cells\", [])\n",
    "        parts = []\n",
    "        for cell in cells:\n",
    "            src = \"\".join(cell.get(\"source\", []))\n",
    "            tags = cell.get(\"metadata\", {}).get(\"tags\", [])\n",
    "            if \"chatbot\" in tags:\n",
    "                continue\n",
    "            if src.strip():\n",
    "                ct = cell.get(\"cell_type\", \"unknown\")\n",
    "                parts.append(f\"[{ct.upper()}]\\n{src}\")\n",
    "        return \"\\n\\n---\\n\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"Notebook content unavailable.\"\n",
    "\n",
    "_NOTEBOOK_CONTEXT = _get_notebook_context()\n",
    "_CHAT_HISTORY = []\n",
    "_API_URL = \"https://course-creator-brown.vercel.app/api/chat\"\n",
    "\n",
    "def _notebook_chat(question):\n",
    "    global _CHAT_HISTORY\n",
    "    try:\n",
    "        resp = _requests.post(_API_URL, json={\n",
    "            'question': question,\n",
    "            'context': _NOTEBOOK_CONTEXT[:100000],\n",
    "            'history': _CHAT_HISTORY[-10:],\n",
    "        }, timeout=60)\n",
    "        data = resp.json()\n",
    "        answer = data.get('answer', 'Sorry, I could not generate a response.')\n",
    "        _CHAT_HISTORY.append({'role': 'user', 'content': question})\n",
    "        _CHAT_HISTORY.append({'role': 'assistant', 'content': answer})\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f'Error connecting to teaching assistant: {str(e)}'\n",
    "\n",
    "_output.register_callback('notebook_chat', _notebook_chat)\n",
    "\n",
    "def ask(question):\n",
    "    \"\"\"Ask the AI teaching assistant a question about this notebook.\"\"\"\n",
    "    answer = _notebook_chat(question)\n",
    "    display(_Markdown(answer))\n",
    "\n",
    "print(\"\\u2705 AI Teaching Assistant is ready!\")\n",
    "print(\"\\U0001f4a1 Use the chat below, or call ask(\\'your question\\') in any cell.\")\n",
    "\n",
    "# --- Display chat widget ---\n",
    "display(_HTML('''<style>\n  .vc-wrap{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;max-width:100%;border-radius:16px;overflow:hidden;box-shadow:0 4px 24px rgba(0,0,0,.12);background:#fff;border:1px solid #e5e7eb}\n  .vc-hdr{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;padding:16px 20px;display:flex;align-items:center;gap:12px}\n  .vc-avatar{width:42px;height:42px;background:rgba(255,255,255,.2);border-radius:50%;display:flex;align-items:center;justify-content:center;font-size:22px}\n  .vc-hdr h3{font-size:16px;font-weight:600;margin:0}\n  .vc-hdr p{font-size:12px;opacity:.85;margin:2px 0 0}\n  .vc-msgs{height:420px;overflow-y:auto;padding:16px;background:#f8f9fb;display:flex;flex-direction:column;gap:10px}\n  .vc-msg{display:flex;flex-direction:column;animation:vc-fade .25s ease}\n  .vc-msg.user{align-items:flex-end}\n  .vc-msg.bot{align-items:flex-start}\n  .vc-bbl{max-width:85%;padding:10px 14px;border-radius:16px;font-size:14px;line-height:1.55;word-wrap:break-word}\n  .vc-msg.user .vc-bbl{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border-bottom-right-radius:4px}\n  .vc-msg.bot .vc-bbl{background:#fff;color:#1a1a2e;border:1px solid #e8e8e8;border-bottom-left-radius:4px}\n  .vc-bbl code{background:rgba(0,0,0,.07);padding:2px 6px;border-radius:4px;font-size:13px;font-family:'Fira Code',monospace}\n  .vc-bbl pre{background:#1e1e2e;color:#cdd6f4;padding:12px;border-radius:8px;overflow-x:auto;margin:8px 0;font-size:13px}\n  .vc-bbl pre code{background:none;padding:0;color:inherit}\n  .vc-bbl h3,.vc-bbl h4{margin:10px 0 4px;font-size:15px}\n  .vc-bbl ul,.vc-bbl ol{margin:4px 0;padding-left:20px}\n  .vc-bbl li{margin:2px 0}\n  .vc-chips{display:flex;flex-wrap:wrap;gap:8px;padding:0 16px 12px;background:#f8f9fb}\n  .vc-chip{background:#fff;border:1px solid #d1d5db;border-radius:20px;padding:6px 14px;font-size:12px;cursor:pointer;transition:all .15s;color:#4b5563}\n  .vc-chip:hover{border-color:#667eea;color:#667eea;background:#f0f0ff}\n  .vc-input{display:flex;padding:12px 16px;background:#fff;border-top:1px solid #eee;gap:8px}\n  .vc-input input{flex:1;padding:10px 16px;border:2px solid #e8e8e8;border-radius:24px;font-size:14px;outline:none;transition:border-color .2s}\n  .vc-input input:focus{border-color:#667eea}\n  .vc-input button{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border:none;border-radius:50%;width:42px;height:42px;cursor:pointer;display:flex;align-items:center;justify-content:center;font-size:18px;transition:transform .1s}\n  .vc-input button:hover{transform:scale(1.05)}\n  .vc-input button:disabled{opacity:.5;cursor:not-allowed;transform:none}\n  .vc-typing{display:flex;gap:5px;padding:4px 0}\n  .vc-typing span{width:8px;height:8px;background:#667eea;border-radius:50%;animation:vc-bounce 1.4s infinite ease-in-out}\n  .vc-typing span:nth-child(2){animation-delay:.2s}\n  .vc-typing span:nth-child(3){animation-delay:.4s}\n  @keyframes vc-bounce{0%,80%,100%{transform:scale(0)}40%{transform:scale(1)}}\n  @keyframes vc-fade{from{opacity:0;transform:translateY(8px)}to{opacity:1;transform:translateY(0)}}\n  .vc-note{text-align:center;font-size:11px;color:#9ca3af;padding:8px 16px 12px;background:#fff}\n</style>\n<div class=\"vc-wrap\">\n  <div class=\"vc-hdr\">\n    <div class=\"vc-avatar\">&#129302;</div>\n    <div>\n      <h3>Vizuara Teaching Assistant</h3>\n      <p>Ask me anything about this notebook</p>\n    </div>\n  </div>\n  <div class=\"vc-msgs\" id=\"vcMsgs\">\n    <div class=\"vc-msg bot\">\n      <div class=\"vc-bbl\">&#128075; Hi! I've read through this entire notebook. Ask me about any concept, code block, or exercise &mdash; I'm here to help you learn!</div>\n    </div>\n  </div>\n  <div class=\"vc-chips\" id=\"vcChips\">\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Explain the main concept</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Help with the TODO exercise</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Summarize what I learned</span>\n  </div>\n  <div class=\"vc-input\">\n    <input type=\"text\" id=\"vcIn\" placeholder=\"Ask about concepts, code, exercises...\" />\n    <button id=\"vcSend\" onclick=\"vcSendMsg()\">&#10148;</button>\n  </div>\n  <div class=\"vc-note\">AI-generated &middot; Verify important information &middot; <a href=\"#\" onclick=\"vcClear();return false\" style=\"color:#667eea\">Clear chat</a></div>\n</div>\n<script>\n(function(){\n  var msgs=document.getElementById('vcMsgs'),inp=document.getElementById('vcIn'),\n      btn=document.getElementById('vcSend'),chips=document.getElementById('vcChips');\n\n  function esc(s){var d=document.createElement('div');d.textContent=s;return d.innerHTML}\n\n  function md(t){\n    return t\n      .replace(/```(\\w*)\\n([\\s\\S]*?)```/g,function(_,l,c){return '<pre><code>'+esc(c)+'</code></pre>'})\n      .replace(/`([^`]+)`/g,'<code>$1</code>')\n      .replace(/\\*\\*([^*]+)\\*\\*/g,'<strong>$1</strong>')\n      .replace(/\\*([^*]+)\\*/g,'<em>$1</em>')\n      .replace(/^#### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^## (.+)$/gm,'<h3>$1</h3>')\n      .replace(/^\\d+\\. (.+)$/gm,'<li>$1</li>')\n      .replace(/^- (.+)$/gm,'<li>$1</li>')\n      .replace(/\\n\\n/g,'<br><br>')\n      .replace(/\\n/g,'<br>');\n  }\n\n  function addMsg(text,isUser){\n    var m=document.createElement('div');m.className='vc-msg '+(isUser?'user':'bot');\n    var b=document.createElement('div');b.className='vc-bbl';\n    b.innerHTML=isUser?esc(text):md(text);\n    m.appendChild(b);msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function showTyping(){\n    var m=document.createElement('div');m.className='vc-msg bot';m.id='vcTyping';\n    m.innerHTML='<div class=\"vc-bbl\"><div class=\"vc-typing\"><span></span><span></span><span></span></div></div>';\n    msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function hideTyping(){var e=document.getElementById('vcTyping');if(e)e.remove()}\n\n  window.vcSendMsg=function(){\n    var q=inp.value.trim();if(!q)return;\n    inp.value='';chips.style.display='none';\n    addMsg(q,true);showTyping();btn.disabled=true;\n    google.colab.kernel.invokeFunction('notebook_chat',[q],{})\n      .then(function(r){\n        hideTyping();\n        var a=r.data['application/json'];\n        addMsg(typeof a==='string'?a:JSON.stringify(a),false);\n      })\n      .catch(function(){\n        hideTyping();\n        addMsg('Sorry, I encountered an error. Please check your internet connection and try again.',false);\n      })\n      .finally(function(){btn.disabled=false;inp.focus()});\n  };\n\n  window.vcAsk=function(q){inp.value=q;vcSendMsg()};\n  window.vcClear=function(){\n    msgs.innerHTML='<div class=\"vc-msg bot\"><div class=\"vc-bbl\">&#128075; Chat cleared. Ask me anything!</div></div>';\n    chips.style.display='flex';\n  };\n\n  inp.addEventListener('keypress',function(e){if(e.key==='Enter')vcSendMsg()});\n  inp.focus();\n})();\n</script>'''))"
   ],
   "id": "vizuara_chatbot"
  }
 ]
}