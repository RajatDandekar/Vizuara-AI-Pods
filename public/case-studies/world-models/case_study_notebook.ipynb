{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Dream-Trained Navigation for Autonomous Warehouse Robots â€” Implementation Notebook"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Dream-Trained Navigation for Autonomous Warehouse Robots\n",
    "## Implementation Notebook\n",
    "\n",
    "This notebook implements a World Model (Ha and Schmidhuber, 2018) for the CarRacing-v2 environment, framed as an autonomous warehouse robot navigation task. You will build the complete V-M-C architecture: a Variational Autoencoder (VAE) for visual compression, a Mixture Density Network RNN (MDN-RNN) for dynamics prediction, and a simple linear Controller trained entirely inside learned \"dreams.\"\n",
    "\n",
    "**Business context**: FleetPath Robotics deploys autonomous mobile robots in e-commerce fulfillment centers. Their current model-free RL pipeline requires 8-12M real-world training steps and costs \\$928,000 per warehouse in collision damage. Your task is to build a world model that achieves comparable navigation performance using fewer than 50,000 real environment steps -- a 200x improvement in sample efficiency.\n",
    "\n",
    "**What you will build**:\n",
    "1. A data collection pipeline that gathers rollouts from the environment\n",
    "2. A VAE that compresses 64x64 images into 32-dimensional latent codes\n",
    "3. An MDN-RNN that predicts multiple possible futures as mixtures of Gaussians\n",
    "4. A controller with only 867 parameters, trained by dreaming\n",
    "5. A complete evaluation comparing the world model agent against a rule-based baseline"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://pods.vizuara.ai/courses/world-models/practice/0/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Dependencies"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install gymnasium[box2d] torch torchvision matplotlib numpy cma -q\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Environment Setup and Data Collection\n",
    "\n",
    "Before building a world model, we need training data. In FleetPath's scenario, this corresponds to 2-3 days of teleoperated rollouts in a new warehouse. Here, we collect rollouts from CarRacing-v2 using a random policy.\n",
    "\n",
    "The environment provides 96x96 RGB observations. We will resize these to 64x64 and normalize pixel values to [0, 1]. The action space is continuous with three dimensions: steering ([-1, 1]), throttle ([0, 1]), and brake ([0, 1])."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: preprocess a single frame\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Resize to 64x64 and normalize to [0, 1].\"\"\"\n",
    "    img = Image.fromarray(frame)\n",
    "    img = img.resize((64, 64), Image.BILINEAR)\n",
    "    return np.array(img, dtype=np.float32) / 255.0"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1: Implement the data collection pipeline"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_rollouts(env, n_rollouts=100, max_steps=300):\n",
    "    \"\"\"\n",
    "    Collect rollout data from the environment using a random policy.\n",
    "\n",
    "    Args:\n",
    "        env: Gymnasium environment instance\n",
    "        n_rollouts: Number of episodes to collect\n",
    "        max_steps: Maximum steps per episode\n",
    "\n",
    "    Returns:\n",
    "        observations: List of arrays, each of shape (T, 64, 64, 3)\n",
    "        actions: List of arrays, each of shape (T, 3)\n",
    "\n",
    "    Hints:\n",
    "        1. For each rollout, reset the environment and collect frames\n",
    "        2. Sample random actions using env.action_space.sample()\n",
    "        3. Preprocess each frame using preprocess_frame()\n",
    "        4. Store observations and actions for each episode separately\n",
    "        5. Aim for ~10,000 total frames across all rollouts\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the data\n",
    "env = gym.make('CarRacing-v2', render_mode='rgb_array')\n",
    "observations, actions = collect_rollouts(env, n_rollouts=100, max_steps=300)\n",
    "total_frames = sum(len(obs) for obs in observations)\n",
    "print(f\"Collected {len(observations)} rollouts, {total_frames} total frames\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "assert len(observations) > 0, \"No rollouts collected\"\n",
    "assert observations[0].shape[1:] == (64, 64, 3), \\\n",
    "    f\"Expected frame shape (64, 64, 3), got {observations[0].shape[1:]}\"\n",
    "assert 0.0 <= observations[0].min() and observations[0].max() <= 1.0, \\\n",
    "    \"Pixel values should be in [0, 1]\"\n",
    "assert actions[0].shape[1] == 3, \\\n",
    "    f\"Expected action dimension 3, got {actions[0].shape[1]}\"\n",
    "print(\"All verification checks passed.\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought questions:**\n",
    "- Why do we use a random policy for data collection instead of an expert policy?\n",
    "- How does the quality of the collected data affect the world model's accuracy?\n",
    "- In FleetPath's scenario, why is teleoperation preferred over random exploration for initial data collection?"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Exploratory Data Analysis\n",
    "\n",
    "Before training any model, we need to understand the structure of our data. This mirrors the data audit that FleetPath's ML team would conduct on teleoperation logs.\n",
    "\n",
    "### TODO 2: Visualize and analyze the collected data"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_rollout_data(observations, actions):\n",
    "    \"\"\"\n",
    "    Perform exploratory data analysis on the collected rollout data.\n",
    "\n",
    "    Args:\n",
    "        observations: List of observation arrays from collect_rollouts\n",
    "        actions: List of action arrays from collect_rollouts\n",
    "\n",
    "    Tasks:\n",
    "        1. Plot a grid of 16 sample frames from different rollouts\n",
    "           to visualize the variety of scenes\n",
    "        2. Plot histograms of each action dimension (steering, throttle,\n",
    "           brake) across the entire dataset\n",
    "        3. Compute and plot the mean pixel difference between consecutive\n",
    "           frames -- this tells us how quickly the scene changes\n",
    "        4. Report summary statistics: total frames, mean episode length,\n",
    "           action value ranges\n",
    "\n",
    "    Hints:\n",
    "        - Use matplotlib with a 4x4 subplot grid for the frame samples\n",
    "        - For consecutive frame differences, compute\n",
    "          np.mean(np.abs(obs[t+1] - obs[t])) for each t\n",
    "        - The action distribution will be uniform (since we used a random\n",
    "          policy) -- note this and consider how it would differ with an\n",
    "          expert policy\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis\n",
    "analyze_rollout_data(observations, actions)"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought questions:**\n",
    "- What does the frame difference distribution tell us about the temporal dynamics of the environment?\n",
    "- If the action distribution is uniform, does that mean every region of the state space is equally well-covered? Why or why not?\n",
    "- What biases might exist in FleetPath's teleoperation data that a random policy would not have?"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Baseline: Rule-Based Proportional Controller\n",
    "\n",
    "Before building a complex world model, we need a baseline to compare against. FleetPath's warehouse robots originally used a PID controller that steered toward detected aisle centerlines. We implement a simpler version: a rule-based controller that steers based on the position of road pixels in the image.\n",
    "\n",
    "### TODO 3: Implement the rule-based baseline"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_controller(observation):\n",
    "    \"\"\"\n",
    "    A simple baseline controller that steers based on road position.\n",
    "\n",
    "    Args:\n",
    "        observation: A single frame of shape (64, 64, 3), values in [0, 1]\n",
    "\n",
    "    Returns:\n",
    "        action: numpy array of shape (3,) -- [steering, throttle, brake]\n",
    "\n",
    "    Algorithm:\n",
    "        1. Extract the bottom third of the image (where the road is\n",
    "           closest to the car)\n",
    "        2. Create a binary mask of road pixels (road is gray/dark,\n",
    "           grass is green)\n",
    "        3. Compute the centroid of the road pixels\n",
    "        4. Set steering proportional to how far the centroid is from\n",
    "           the image center\n",
    "        5. Set throttle to a constant value (e.g., 0.3)\n",
    "        6. Set brake to 0\n",
    "\n",
    "    Hints:\n",
    "        - Road pixels have similar R, G, B values (gray) while grass\n",
    "          pixels have high G relative to R and B\n",
    "        - A simple threshold: pixel is road if\n",
    "          abs(R - G) < 0.1 and abs(G - B) < 0.1\n",
    "        - Steering = K_p * (centroid_x - image_center_x) / image_width\n",
    "          where K_p is a proportional gain (~0.5 to 1.0)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_controller(env, controller_fn, n_episodes=5, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Evaluate a controller function on the environment.\n",
    "\n",
    "    Args:\n",
    "        env: Gymnasium environment instance\n",
    "        controller_fn: Function that takes an observation and returns an action\n",
    "        n_episodes: Number of evaluation episodes\n",
    "        max_steps: Maximum steps per episode\n",
    "\n",
    "    Returns:\n",
    "        mean_reward: Average total reward across episodes\n",
    "        std_reward: Standard deviation of rewards\n",
    "        episode_rewards: List of per-episode rewards\n",
    "\n",
    "    Hints:\n",
    "        1. For each episode, reset the environment\n",
    "        2. At each step, preprocess the observation and call controller_fn\n",
    "        3. Accumulate the reward\n",
    "        4. Return statistics across episodes\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the baseline\n",
    "baseline_mean, baseline_std, baseline_rewards = evaluate_controller(\n",
    "    env, rule_based_controller, n_episodes=5\n",
    ")\n",
    "print(f\"Baseline: {baseline_mean:.1f} +/- {baseline_std:.1f}\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "assert 100 < baseline_mean < 500, \\\n",
    "    f\"Baseline reward {baseline_mean:.1f} is outside expected range [100, 500]. \" \\\n",
    "    \"Check your road detection logic.\"\n",
    "print(f\"Baseline verification passed: reward = {baseline_mean:.1f}\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought questions:**\n",
    "- Why does the rule-based controller fail on sharp turns?\n",
    "- What information is the rule-based controller missing that a world model agent would have?\n",
    "- In what warehouse scenarios would a simple PID controller actually be sufficient?"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Model Design: The World Model Architecture\n",
    "\n",
    "Now we build the three-component world model: Vision (VAE), Memory (MDN-RNN), and Controller. This is the core of the case study, directly implementing the architecture from Ha and Schmidhuber (2018).\n",
    "\n",
    "The key insight: **the controller never sees raw pixels.** It lives entirely in a compressed \"dream space\" created by V and M. All the complexity of the visual world is handled by the vision and memory components, so the controller can be incredibly simple (867 parameters).\n",
    "\n",
    "### TODO 4a: Implement the VAE"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder for visual compression.\n",
    "\n",
    "    Compresses 64x64x3 images into 32-dimensional latent codes.\n",
    "\n",
    "    Architecture:\n",
    "        Encoder: 4 convolutional layers (stride 2) reducing spatial\n",
    "                 dimensions 64 -> 30 -> 14 -> 6 -> 2, followed by\n",
    "                 two linear heads for mu and logvar\n",
    "        Decoder: Linear layer expanding to 2x2x256, followed by 4\n",
    "                 transposed convolutional layers restoring to 64x64x3\n",
    "\n",
    "    Key concepts to implement:\n",
    "        - The encoder outputs parameters of a distribution (mu, logvar),\n",
    "          NOT a deterministic code\n",
    "        - The reparameterization trick: z = mu + std * epsilon\n",
    "        - The decoder uses transposed convolutions (the \"reverse\" of\n",
    "          convolutions)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super().__init__()\n",
    "        # TODO: Define encoder layers\n",
    "        # Hint: Use nn.Conv2d with kernel_size=4, stride=2\n",
    "        # Layer sizes: 3->32->64->128->256 channels\n",
    "        # After convolutions, flatten and project to mu and logvar\n",
    "\n",
    "        # TODO: Define decoder layers\n",
    "        # Hint: Use nn.ConvTranspose2d to reverse the encoder\n",
    "        # First project from latent_dim to 256*2*2 with a linear layer\n",
    "        # Then use transposed convolutions: 256->128->64->32->3 channels\n",
    "        # Use kernel sizes [5, 5, 6, 6] and stride 2 for the decoder\n",
    "        # Final activation should be Sigmoid (pixel values in [0,1])\n",
    "        pass\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encode an image to latent distribution parameters.\n",
    "\n",
    "        Args:\n",
    "            x: Image tensor of shape (batch, 3, 64, 64)\n",
    "\n",
    "        Returns:\n",
    "            mu: Mean of shape (batch, latent_dim)\n",
    "            logvar: Log-variance of shape (batch, latent_dim)\n",
    "        \"\"\"\n",
    "        # TODO: Pass x through encoder convolutions, flatten,\n",
    "        # project to mu and logvar\n",
    "        pass\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Sample from the latent distribution using the reparameterization trick.\n",
    "\n",
    "        Args:\n",
    "            mu: Mean of shape (batch, latent_dim)\n",
    "            logvar: Log-variance of shape (batch, latent_dim)\n",
    "\n",
    "        Returns:\n",
    "            z: Sampled latent code of shape (batch, latent_dim)\n",
    "\n",
    "        Key insight: z = mu + std * epsilon where epsilon ~ N(0, I)\n",
    "        std = exp(0.5 * logvar) because logvar = log(sigma^2),\n",
    "        so exp(0.5 * logvar) = sigma.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the reparameterization trick\n",
    "        pass\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decode a latent code back to an image.\n",
    "\n",
    "        Args:\n",
    "            z: Latent code of shape (batch, latent_dim)\n",
    "\n",
    "        Returns:\n",
    "            x_recon: Reconstructed image of shape (batch, 3, 64, 64)\n",
    "        \"\"\"\n",
    "        # TODO: Project z to spatial dimensions, apply transposed convolutions\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Full forward pass: encode, sample, decode.\"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 4b: Implement the MDN-RNN"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDNRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture Density Network RNN for dynamics prediction.\n",
    "\n",
    "    Takes a sequence of (latent_code, action) pairs and predicts\n",
    "    the distribution of the next latent code as a mixture of Gaussians.\n",
    "\n",
    "    Architecture:\n",
    "        - LSTM with hidden_dim=256\n",
    "        - Three linear heads projecting hidden state to:\n",
    "          pi (mixing coefficients), mu (means), sigma (std devs)\n",
    "          for K=5 Gaussian components\n",
    "\n",
    "    Key concepts:\n",
    "        - The LSTM hidden state h_t serves as the agent's memory\n",
    "        - The MDN head outputs parameters for K Gaussians, allowing\n",
    "          multimodal predictions (multiple possible futures)\n",
    "        - pi values are passed through softmax to form a valid\n",
    "          probability distribution\n",
    "        - sigma values are exponentiated to ensure positivity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=32, action_dim=3,\n",
    "                 hidden_dim=256, n_gaussians=5):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_gaussians = n_gaussians\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # TODO: Define the LSTM layer\n",
    "        # Input size: latent_dim + action_dim (concatenated)\n",
    "        # Hidden size: hidden_dim\n",
    "        # Use batch_first=True\n",
    "\n",
    "        # TODO: Define three linear heads for pi, mu, sigma\n",
    "        # Each outputs: latent_dim * n_gaussians values\n",
    "        pass\n",
    "\n",
    "    def forward(self, z, a, hidden=None):\n",
    "        \"\"\"\n",
    "        Predict the distribution of the next latent state.\n",
    "\n",
    "        Args:\n",
    "            z: Latent codes, shape (batch, seq_len, latent_dim)\n",
    "            a: Actions, shape (batch, seq_len, action_dim)\n",
    "            hidden: Optional LSTM hidden state tuple\n",
    "\n",
    "        Returns:\n",
    "            pi: Mixing coefficients (batch*seq_len, latent_dim, K)\n",
    "            mu: Means (batch*seq_len, latent_dim, K)\n",
    "            sigma: Std devs (batch*seq_len, latent_dim, K)\n",
    "            hidden: Updated LSTM hidden state\n",
    "\n",
    "        Steps:\n",
    "            1. Concatenate z and a along the last dimension\n",
    "            2. Pass through LSTM to get hidden states\n",
    "            3. Reshape LSTM output for the MDN heads\n",
    "            4. Apply softmax to pi (mixing coefficients must sum to 1)\n",
    "            5. Apply exp to sigma (std devs must be positive)\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass\n",
    "        pass"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 4c: Implement the Controller"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple linear controller.\n",
    "\n",
    "    Takes the concatenation of the current latent code z_t and the\n",
    "    LSTM hidden state h_t, and outputs an action.\n",
    "\n",
    "    Total parameters: (latent_dim + hidden_dim) * action_dim + action_dim\n",
    "                    = (32 + 256) * 3 + 3 = 867\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=32, hidden_dim=256, action_dim=3):\n",
    "        super().__init__()\n",
    "        # TODO: Define a single linear layer\n",
    "        # Input: latent_dim + hidden_dim\n",
    "        # Output: action_dim\n",
    "        pass\n",
    "\n",
    "    def forward(self, z, h):\n",
    "        \"\"\"\n",
    "        Choose an action given the current state representation.\n",
    "\n",
    "        Args:\n",
    "            z: Current latent code, shape (batch, latent_dim)\n",
    "            h: Current LSTM hidden state, shape (batch, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            action: Action vector, shape (batch, action_dim)\n",
    "                    Values in [-1, 1] (use tanh activation)\n",
    "        \"\"\"\n",
    "        # TODO: Concatenate z and h, pass through linear layer, apply tanh\n",
    "        pass"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: check parameter counts\n",
    "vae = VAE().to(device)\n",
    "mdnrnn = MDNRNN().to(device)\n",
    "controller = Controller().to(device)\n",
    "\n",
    "for name, model in [(\"VAE\", vae), (\"MDNRNN\", mdnrnn),\n",
    "                     (\"Controller\", controller)]:\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{name}: {n_params:,} parameters\")\n",
    "\n",
    "assert sum(p.numel() for p in controller.parameters()) == 867, \\\n",
    "    \"Controller should have exactly 867 parameters\"\n",
    "print(\"\\nController parameter count verified: 867\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought questions:**\n",
    "- Why does the controller use tanh activation but the VAE decoder uses sigmoid?\n",
    "- What would happen if we made the controller a 3-layer MLP instead of a single linear layer?\n",
    "- Why is the MDN-RNN's hidden dimension (256) much larger than the latent dimension (32)?"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Training Strategy\n",
    "\n",
    "Training happens in three sequential stages, mirroring FleetPath's deployment pipeline: first learn to see (V), then learn to predict (M), then learn to act (C).\n",
    "\n",
    "### Stage 1: Train the VAE\n",
    "\n",
    "The VAE loss has two terms:\n",
    "- **Reconstruction loss**: MSE between input and reconstructed image\n",
    "- **KL divergence**: Regularizes the latent space toward a standard Gaussian\n",
    "\n",
    "### TODO 5a: Implement the VAE training loop"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x_recon, x, mu, logvar, beta=1.0):\n",
    "    \"\"\"\n",
    "    Compute the VAE loss: reconstruction + KL divergence.\n",
    "\n",
    "    Args:\n",
    "        x_recon: Reconstructed image, shape (batch, 3, 64, 64)\n",
    "        x: Original image, shape (batch, 3, 64, 64)\n",
    "        mu: Encoder mean, shape (batch, latent_dim)\n",
    "        logvar: Encoder log-variance, shape (batch, latent_dim)\n",
    "        beta: KL weight (beta=1 is standard VAE, beta<1 sharper reconstructions)\n",
    "\n",
    "    Returns:\n",
    "        loss: Scalar loss value\n",
    "        recon_loss: Reconstruction component (for logging)\n",
    "        kl_loss: KL divergence component (for logging)\n",
    "\n",
    "    Formulas:\n",
    "        Reconstruction loss = MSE(x_recon, x) summed over pixels, averaged over batch\n",
    "        KL loss = -0.5 * sum(1 + logvar - mu^2 - exp(logvar)), averaged over batch\n",
    "\n",
    "    Hints:\n",
    "        - Use F.mse_loss with reduction='sum' and divide by batch size\n",
    "        - The KL formula is the closed-form KL divergence between N(mu, sigma^2) and N(0, 1)\n",
    "        - Sum the KL over latent dimensions, then average over the batch\n",
    "    \"\"\"\n",
    "    # TODO: Implement the VAE loss function\n",
    "    pass"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(vae, train_data, n_epochs=10, batch_size=64, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train the VAE on collected observation frames.\n",
    "\n",
    "    Args:\n",
    "        vae: VAE model instance\n",
    "        train_data: Tensor of frames, shape (N, 3, 64, 64)\n",
    "        n_epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        lr: Learning rate\n",
    "\n",
    "    Returns:\n",
    "        vae: Trained VAE model\n",
    "        losses: List of per-epoch average losses\n",
    "\n",
    "    Steps:\n",
    "        1. Create a DataLoader from train_data\n",
    "        2. Use Adam optimizer with the specified learning rate\n",
    "        3. For each epoch, iterate over batches:\n",
    "           a. Forward pass through the VAE\n",
    "           b. Compute the VAE loss\n",
    "           c. Backpropagate and update weights\n",
    "        4. Log reconstruction loss and KL loss separately\n",
    "        5. Every 2 epochs, visualize 8 original vs. reconstructed\n",
    "           images side by side\n",
    "\n",
    "    Hints:\n",
    "        - Use beta=1.0 initially; if reconstructions are too blurry, try beta=0.5\n",
    "        - Learning rate 1e-3 with Adam works well for this scale\n",
    "    \"\"\"\n",
    "    # TODO: Implement the training loop\n",
    "    pass"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data: stack all frames into a single tensor\n",
    "all_frames = np.concatenate(observations, axis=0)  # (N, 64, 64, 3)\n",
    "all_frames_tensor = torch.FloatTensor(all_frames).permute(0, 3, 1, 2).to(device)\n",
    "print(f\"Training data shape: {all_frames_tensor.shape}\")\n",
    "\n",
    "# Train the VAE\n",
    "vae, vae_losses = train_vae(vae, all_frames_tensor, n_epochs=10)"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(vae_losses, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('VAE Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Train the MDN-RNN\n",
    "\n",
    "With the VAE trained, we encode all observations into latent codes and train the MDN-RNN to predict the next latent state given the current state and action.\n",
    "\n",
    "### TODO 5b: Implement the MDN-RNN training loop"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_loss(pi, mu, sigma, z_next):\n",
    "    \"\"\"\n",
    "    Compute the MDN negative log-likelihood loss.\n",
    "\n",
    "    Args:\n",
    "        pi: Mixing coefficients, shape (batch, latent_dim, K)\n",
    "        mu: Means, shape (batch, latent_dim, K)\n",
    "        sigma: Std devs, shape (batch, latent_dim, K)\n",
    "        z_next: Actual next latent state, shape (batch, latent_dim)\n",
    "\n",
    "    Returns:\n",
    "        loss: Scalar negative log-likelihood\n",
    "\n",
    "    Steps:\n",
    "        1. Expand z_next to match the K dimension\n",
    "        2. Compute Gaussian log-probability for each component\n",
    "        3. Weight by mixing coefficients (in log space)\n",
    "        4. Use logsumexp across K for numerical stability\n",
    "        5. Sum over latent dimensions, average over batch\n",
    "\n",
    "    Hints:\n",
    "        - Use torch.logsumexp for numerical stability\n",
    "        - log N(x|mu,sigma) = -0.5*((x-mu)/sigma)^2 - log(sigma) - 0.5*log(2*pi)\n",
    "        - Clamp sigma to min 1e-6 to prevent division by zero\n",
    "    \"\"\"\n",
    "    # TODO: Implement the MDN loss function\n",
    "    pass"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mdnrnn(mdnrnn, vae, rollout_obs, rollout_actions,\n",
    "                  n_epochs=20, seq_len=32, batch_size=16, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train the MDN-RNN on encoded rollout sequences.\n",
    "\n",
    "    Args:\n",
    "        mdnrnn: MDNRNN model instance\n",
    "        vae: Trained VAE model (frozen, used only for encoding)\n",
    "        rollout_obs: List of observation arrays from rollouts\n",
    "        rollout_actions: List of action arrays from rollouts\n",
    "        n_epochs: Number of training epochs\n",
    "        seq_len: Sequence length for LSTM training\n",
    "        batch_size: Batch size\n",
    "        lr: Learning rate\n",
    "\n",
    "    Returns:\n",
    "        mdnrnn: Trained MDNRNN model\n",
    "        losses: List of per-epoch average losses\n",
    "\n",
    "    Steps:\n",
    "        1. Encode all observations into latent codes using the trained VAE\n",
    "           (with torch.no_grad)\n",
    "        2. Create training sequences of length seq_len from the encoded rollouts\n",
    "        3. For each epoch, iterate over batches of sequences:\n",
    "           a. Feed (z_t, a_t) for t=0..T-1 into the MDN-RNN\n",
    "           b. Compute MDN loss against z_{t+1} for t=0..T-1\n",
    "           c. Backpropagate and update weights\n",
    "\n",
    "    Hints:\n",
    "        - Freeze the VAE during this stage (vae.eval(), no_grad)\n",
    "        - Use truncated backpropagation: detach hidden states between sequences\n",
    "        - Learning rate 1e-3 with Adam, gradient clipping at 1.0\n",
    "    \"\"\"\n",
    "    # TODO: Implement the training loop\n",
    "    pass"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the MDN-RNN\n",
    "mdnrnn, mdnrnn_losses = train_mdnrnn(\n",
    "    mdnrnn, vae, observations, actions, n_epochs=20\n",
    ")"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(mdnrnn_losses, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MDN-RNN Loss (NLL)')\n",
    "plt.title('MDN-RNN Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3: Train the Controller in Dreams\n",
    "\n",
    "This is the key insight of the World Model architecture. The controller is trained entirely inside the learned world model -- it never touches the real environment during training. We use CMA-ES (Covariance Matrix Adaptation Evolution Strategy) to evolve the controller's 867 parameters.\n",
    "\n",
    "### TODO 5c: Implement dream-based controller training"
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dream_rollout(vae, mdnrnn, controller, initial_z, max_steps=200):\n",
    "    \"\"\"\n",
    "    Run a rollout entirely inside the learned world model (the dream).\n",
    "\n",
    "    Args:\n",
    "        vae: Trained VAE (used only for initial encoding)\n",
    "        mdnrnn: Trained MDN-RNN\n",
    "        controller: Controller to evaluate\n",
    "        initial_z: Starting latent code, shape (latent_dim,)\n",
    "        max_steps: Maximum dream steps\n",
    "\n",
    "    Returns:\n",
    "        total_reward: Estimated cumulative reward from the dream\n",
    "        dream_states: List of dreamed latent states\n",
    "\n",
    "    Steps:\n",
    "        1. Initialize h_0 (LSTM hidden state) as zeros\n",
    "        2. Set z_0 = initial_z\n",
    "        3. For each step t:\n",
    "           a. Get action from controller: a_t = C(z_t, h_t)\n",
    "           b. Predict next state distribution: pi, mu, sigma, h_{t+1} = M(z_t, a_t, h_t)\n",
    "           c. Sample z_{t+1} from the predicted mixture\n",
    "           d. Estimate reward (heuristic: reward = -|steering| + throttle)\n",
    "        4. Return total reward\n",
    "\n",
    "    Hints:\n",
    "        - For sampling from the mixture: first choose component k with\n",
    "          probability pi_k, then sample from N(mu_k, sigma_k^2)\n",
    "        - For reward estimation, use: reward = throttle - 0.1 * |steering|\n",
    "        - Keep all tensors on the same device\n",
    "    \"\"\"\n",
    "    # TODO: Implement dream rollouts\n",
    "    pass"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_controller_cmaes(vae, mdnrnn, controller,\n",
    "                            initial_observations, n_generations=50,\n",
    "                            population_size=32, max_dream_steps=200):\n",
    "    \"\"\"\n",
    "    Train the controller using CMA-ES inside the learned dream.\n",
    "\n",
    "    Args:\n",
    "        vae: Trained VAE\n",
    "        mdnrnn: Trained MDN-RNN\n",
    "        controller: Controller to train\n",
    "        initial_observations: Real observations to seed dreams\n",
    "        n_generations: Number of CMA-ES generations\n",
    "        population_size: Number of controllers per generation\n",
    "        max_dream_steps: Steps per dream rollout\n",
    "\n",
    "    Returns:\n",
    "        controller: Trained controller\n",
    "        fitness_history: List of best fitness per generation\n",
    "\n",
    "    Steps:\n",
    "        1. Extract initial controller parameters as a flat vector\n",
    "        2. Initialize CMA-ES with these parameters\n",
    "        3. For each generation:\n",
    "           a. Sample population_size parameter vectors\n",
    "           b. For each vector, load into controller, run dream_rollout\n",
    "           c. Record fitness (total reward)\n",
    "           d. Update CMA-ES distribution\n",
    "           e. Log best fitness\n",
    "        4. Load best parameters into controller\n",
    "\n",
    "    Hints:\n",
    "        - Install cma: already done in setup\n",
    "        - Use cma.CMAEvolutionStrategy(initial_params, sigma0)\n",
    "          with sigma0 = 0.1\n",
    "        - Use multiple dream rollouts per controller (3-5) and\n",
    "          average for stable evaluation\n",
    "    \"\"\"\n",
    "    # TODO: Implement CMA-ES training\n",
    "    pass"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode initial observations for seeding dreams\n",
    "with torch.no_grad():\n",
    "    sample_frames = all_frames_tensor[:100]\n",
    "    initial_mu, _ = vae.encode(sample_frames)\n",
    "\n",
    "# Train the controller in dreams\n",
    "controller, fitness_history = train_controller_cmaes(\n",
    "    vae, mdnrnn, controller, initial_mu, n_generations=50\n",
    ")"
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CMA-ES training progress\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(fitness_history, marker='o', markersize=3)\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Best Fitness (Dream Reward)')\n",
    "plt.title('Controller Training via CMA-ES (Inside the Dream)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought questions:**\n",
    "- Why do we train V, M, and C sequentially rather than end-to-end?\n",
    "- What happens if the world model (M) is inaccurate? How would this affect the controller?\n",
    "- The Dreamer algorithm (2020) replaces CMA-ES with backpropagation through the dream. What are the advantages and disadvantages of each approach?"
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Evaluation\n",
    "\n",
    "After training, we evaluate the world model agent in the *real* environment (not the dream) and compare against the rule-based baseline. This is the moment of truth -- does a controller trained entirely in dreams actually work in reality?\n",
    "\n",
    "### TODO 6: Evaluate the trained world model agent"
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_world_model_agent(env, vae, mdnrnn, controller,\n",
    "                                n_episodes=10, max_steps=1000,\n",
    "                                render=False):\n",
    "    \"\"\"\n",
    "    Evaluate the trained world model agent in the real environment.\n",
    "\n",
    "    Args:\n",
    "        env: Gymnasium environment instance\n",
    "        vae: Trained VAE\n",
    "        mdnrnn: Trained MDN-RNN\n",
    "        controller: Trained Controller\n",
    "        n_episodes: Number of evaluation episodes\n",
    "        max_steps: Maximum steps per episode\n",
    "        render: Whether to save rendered frames\n",
    "\n",
    "    Returns:\n",
    "        results: Dictionary containing:\n",
    "            - mean_reward: Average reward across episodes\n",
    "            - std_reward: Standard deviation\n",
    "            - episode_rewards: List of per-episode rewards\n",
    "            - episode_lengths: List of episode lengths\n",
    "            - frames: List of rendered frames (if render=True)\n",
    "\n",
    "    Steps:\n",
    "        1. For each episode:\n",
    "           a. Reset environment, get initial observation\n",
    "           b. Encode initial observation with VAE: z_0 = V(x_0)\n",
    "           c. Initialize LSTM hidden state: h_0 = zeros\n",
    "           d. At each step:\n",
    "              - Get action: a_t = C(z_t, h_t)\n",
    "              - Step the REAL environment: x_{t+1}, r_t = env.step(a_t)\n",
    "              - Encode new observation: z_{t+1} = V(x_{t+1})\n",
    "              - Update LSTM: _, _, _, h_{t+1} = M(z_t, a_t, h_t)\n",
    "              - Accumulate reward\n",
    "           e. Record episode reward and length\n",
    "        2. Compute and return statistics\n",
    "\n",
    "    Hints:\n",
    "        - Set all models to eval mode and use torch.no_grad()\n",
    "        - The controller outputs values in [-1, 1] via tanh;\n",
    "          map throttle and brake to [0, 1] using (action + 1) / 2\n",
    "        - Remember to preprocess observations (resize, normalize)\n",
    "    \"\"\"\n",
    "    # TODO: Implement evaluation\n",
    "    pass"
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the world model agent\n",
    "wm_results = evaluate_world_model_agent(env, vae, mdnrnn, controller, n_episodes=10)\n",
    "print(f\"World Model Agent: {wm_results['mean_reward']:.1f} +/- {wm_results['std_reward']:.1f}\")\n",
    "print(f\"Baseline:          {baseline_mean:.1f} +/- {baseline_std:.1f}\")"
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(baseline_rewards, wm_rewards):\n",
    "    \"\"\"\n",
    "    Create a comparison plot of baseline vs. world model performance.\n",
    "\n",
    "    Args:\n",
    "        baseline_rewards: List of episode rewards for the baseline\n",
    "        wm_rewards: List of episode rewards for the world model agent\n",
    "\n",
    "    Tasks:\n",
    "        1. Create a bar chart comparing mean rewards with error bars\n",
    "        2. Create a box plot showing the distribution of rewards\n",
    "        3. Print a summary table with mean, std, min, max for each\n",
    "\n",
    "    Hints:\n",
    "        - Use matplotlib subplots with 1 row, 2 columns\n",
    "        - Error bars should show standard deviation\n",
    "        - Include a horizontal line at reward=900 labeled \"Human-level reference\"\n",
    "    \"\"\"\n",
    "    # TODO: Implement comparison visualization\n",
    "    pass\n",
    "\n",
    "plot_comparison(baseline_rewards, wm_results['episode_rewards'])"
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought questions:**\n",
    "- How does the agent's performance in the real environment compare to its performance in dreams? What explains the gap?\n",
    "- If the world model had perfect fidelity, would the dream performance and real performance be identical? Why or why not?\n",
    "- What is the sample efficiency ratio? How many real environment steps did the world model agent use compared to the model-free baseline's 10M steps?"
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Error Analysis\n",
    "\n",
    "Understanding failure modes is critical for FleetPath's safety case. A robot that performs well on average but catastrophically fails in specific scenarios is not deployable.\n",
    "\n",
    "### TODO 7: Analyze failure modes"
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def failure_analysis(env, vae, mdnrnn, controller,\n",
    "                      n_episodes=20, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Identify and categorize failure modes of the trained agent.\n",
    "\n",
    "    Args:\n",
    "        env: Gymnasium environment\n",
    "        vae, mdnrnn, controller: Trained model components\n",
    "        n_episodes: Number of episodes to analyze\n",
    "        max_steps: Maximum steps per episode\n",
    "\n",
    "    Returns:\n",
    "        failures: List of dicts with failure details\n",
    "        summary: Dict with counts per failure category\n",
    "\n",
    "    Failure categories:\n",
    "        1. \"off_track\" -- car leaves the road (reward < -1 for 3+ consecutive steps)\n",
    "        2. \"stall\" -- car speed drops to near zero (< 10 steps of low reward)\n",
    "        3. \"oscillation\" -- rapid left-right steering (sign changes > 5 in 10 steps)\n",
    "        4. \"model_exploitation\" -- dream reward >> real reward for same trajectory\n",
    "\n",
    "    Steps:\n",
    "        1. Run episodes recording frame-by-frame data (actions, rewards, latents)\n",
    "        2. Detect failures using the heuristics above\n",
    "        3. For each failure, save the context (step number, type, observation)\n",
    "        4. Summarize: count failures by category, compute overall failure rate\n",
    "\n",
    "    Hints:\n",
    "        - Track the running reward to detect off_track events\n",
    "        - Track action history to detect oscillation (np.diff on steering sign)\n",
    "        - Compare dream-predicted z with actual encoded z for model exploitation\n",
    "    \"\"\"\n",
    "    # TODO: Implement failure analysis\n",
    "    pass"
   ],
   "id": "cell_51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run failure analysis\n",
    "failures, failure_summary = failure_analysis(env, vae, mdnrnn, controller)\n",
    "print(\"Failure Summary:\")\n",
    "for category, count in failure_summary.items():\n",
    "    print(f\"  {category}: {count}\")"
   ],
   "id": "cell_52"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought questions:**\n",
    "- Which failure modes are most concerning for warehouse deployment? Why?\n",
    "- How could the world model be improved to reduce model exploitation failures?\n",
    "- If you were FleetPath's safety engineer, what minimum failure rate would you require before approving deployment?"
   ],
   "id": "cell_53"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Scalability and Deployment Considerations\n",
    "\n",
    "FleetPath needs to deploy this model on NVIDIA Jetson AGX Orin with under 100ms inference latency. Let us profile the inference pipeline to verify feasibility.\n",
    "\n",
    "### TODO 8: Profile inference performance"
   ],
   "id": "cell_54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_inference(vae, mdnrnn, controller, device='cpu'):\n",
    "    \"\"\"\n",
    "    Profile the inference latency of each component.\n",
    "\n",
    "    Args:\n",
    "        vae: Trained VAE\n",
    "        mdnrnn: Trained MDN-RNN\n",
    "        controller: Trained Controller\n",
    "        device: Device to profile on ('cpu' or 'cuda')\n",
    "\n",
    "    Returns:\n",
    "        timing: Dictionary with per-component latency in ms\n",
    "\n",
    "    Steps:\n",
    "        1. Create a dummy input (random 64x64x3 image)\n",
    "        2. Time 100 forward passes through each component:\n",
    "           a. VAE encoder: x -> z\n",
    "           b. MDN-RNN: (z, a) -> (pi, mu, sigma, h)\n",
    "           c. Controller: (z, h) -> a\n",
    "           d. Full pipeline: x -> z -> (z, h) -> a\n",
    "        3. Report mean and std of latency for each component\n",
    "        4. Determine if the 100ms (10 Hz) budget is met\n",
    "\n",
    "    Hints:\n",
    "        - Use torch.cuda.synchronize() before timing on GPU\n",
    "        - Warm up with 10 untimed passes before measuring\n",
    "        - Use time.perf_counter() for precise timing\n",
    "    \"\"\"\n",
    "    # TODO: Implement inference profiling\n",
    "    pass"
   ],
   "id": "cell_55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile on available device\n",
    "timing = profile_inference(vae, mdnrnn, controller, device=str(device))\n",
    "for component, ms in timing.items():\n",
    "    print(f\"  {component}: {ms:.2f} ms\")\n",
    "\n",
    "total = timing.get('full_pipeline', sum(timing.values()))\n",
    "print(f\"\\n  Total: {total:.2f} ms\")\n",
    "print(f\"  Budget: 100 ms\")\n",
    "print(f\"  Status: {'PASS' if total < 100 else 'FAIL'}\")"
   ],
   "id": "cell_56"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought questions:**\n",
    "- Which component is the bottleneck? Why?\n",
    "- How would you reduce latency if the 100ms budget is not met?\n",
    "- What is the trade-off between model size and dream fidelity?"
   ],
   "id": "cell_57"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Ethical and Regulatory Analysis\n",
    "\n",
    "FleetPath's robots operate alongside human workers. Safety, fairness, and regulatory compliance are not optional -- they are prerequisites for deployment.\n",
    "\n",
    "### TODO 9: Ethical impact assessment"
   ],
   "id": "cell_58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ethical_assessment():\n",
    "    \"\"\"\n",
    "    Write a brief ethical impact assessment for deploying a\n",
    "    world-model-based navigation system in a fulfillment center.\n",
    "\n",
    "    Address these five areas (write your answers as a formatted\n",
    "    string and print it):\n",
    "\n",
    "    1. SAFETY: The world model may have inaccurate dynamics in some\n",
    "       regions. How should the system detect and respond when predictions\n",
    "       diverge from reality? What fallback mechanisms should exist?\n",
    "\n",
    "    2. WORKER IMPACT: Autonomous robots change warehouse work. What\n",
    "       safeguards ensure worker safety? How should the system handle\n",
    "       edge cases where a human is in an unexpected position?\n",
    "\n",
    "    3. BIAS: Training data collected during day shifts may not represent\n",
    "       night shift conditions (different lighting, fewer workers). How\n",
    "       should this be detected and mitigated?\n",
    "\n",
    "    4. ACCOUNTABILITY: If a dream-trained robot causes injury, who is\n",
    "       responsible? How should accountability be structured?\n",
    "\n",
    "    5. REGULATORY: What standards or certifications would FleetPath need?\n",
    "       Consider ISO 3691-4, OSHA, and emerging AI regulation.\n",
    "\n",
    "    Return your answers as a formatted string.\n",
    "    \"\"\"\n",
    "    # TODO: Write your ethical assessment\n",
    "    assessment = \"\"\"\n",
    "    [Write your assessment here -- address all 5 areas above]\n",
    "    \"\"\"\n",
    "    print(assessment)\n",
    "    return assessment\n",
    "\n",
    "ethical_assessment()"
   ],
   "id": "cell_59"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought questions:**\n",
    "- Should dream-trained robots be held to a higher safety standard than conventionally trained robots? Why or why not?\n",
    "- How does the \"model exploitation\" problem interact with safety requirements?\n",
    "- If you discovered that the world model consistently underestimates the speed of human workers, what would you do?"
   ],
   "id": "cell_60"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you built a complete World Model system for autonomous navigation:\n",
    "\n",
    "1. **Data Collection** -- Gathered rollout data from the environment (50,000 frames vs. 10M for model-free RL)\n",
    "2. **Vision (V)** -- Trained a VAE to compress 64x64 images into 32-dimensional latent codes (384:1 compression)\n",
    "3. **Memory (M)** -- Trained an MDN-RNN to predict multiple possible futures as mixtures of Gaussians\n",
    "4. **Controller (C)** -- Trained a 867-parameter linear controller entirely inside learned \"dreams\" using CMA-ES\n",
    "5. **Evaluation** -- Compared the dream-trained agent against a rule-based baseline in the real environment\n",
    "6. **Error Analysis** -- Identified and categorized failure modes for deployment safety\n",
    "7. **Deployment** -- Profiled inference latency against the 100ms production budget\n",
    "8. **Ethics** -- Assessed the societal implications of deploying dream-trained autonomous robots\n",
    "\n",
    "**Key takeaway**: The world model achieves comparable performance to model-free methods while using 200x fewer real-world interactions. The agent literally learns to drive by dreaming -- a paradigm that slashes FleetPath's warehouse onboarding cost from \\$928,000 to under \\$3,000.\n",
    "\n",
    "For further reading on production system design (API design, A/B testing, drift detection, cost analysis), see **Section 4** of the full case study document.\n",
    "\n",
    "### References\n",
    "\n",
    "- Ha and Schmidhuber, \"World Models\" (2018)\n",
    "- Hafner et al., \"Dream to Control: Learning Behaviors by Latent Imagination\" (Dreamer, 2020)\n",
    "- Hafner et al., \"Mastering Diverse Domains through World Models\" (DreamerV3, 2023)\n",
    "- Kingma and Welling, \"Auto-Encoding Variational Bayes\" (2014)"
   ],
   "id": "cell_61"
  }
 ]
}