{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "MedAlign Health: Aligning a Clinical AI Assistant Using RLHF \u2014 Implementation Notebook"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedAlign Health: Aligning a Clinical AI Assistant Using RLHF\n",
    "\n",
    "## Implementation Notebook\n",
    "\n",
    "This notebook implements the RLHF pipeline for aligning a clinical AI assistant to produce safe, empathetic, and medically appropriate patient-facing responses.\n",
    "\n",
    "**Industry:** Digital Health / Telehealth\n",
    "**Company:** MedAlign Health (fictional)\n",
    "**Core Problem:** Align a 7B parameter clinical assistant using physician preferences\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Data Loading"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers datasets matplotlib seaborn pandas -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic clinical preference dataset\n",
    "# In production, this would be real de-identified clinical conversations\n",
    "# annotated by board-certified physicians\n",
    "\n",
    "CLINICAL_PROMPTS = [\n",
    "    \"I have been having persistent headaches for the past week.\",\n",
    "    \"My child has a fever of 101F. Should I be worried?\",\n",
    "    \"I am feeling very anxious about my upcoming surgery.\",\n",
    "    \"I noticed a rash on my arm that appeared yesterday.\",\n",
    "    \"I have been feeling unusually tired for the past month.\",\n",
    "    \"My back pain has not improved after two weeks.\",\n",
    "    \"I am having difficulty sleeping and feel stressed.\",\n",
    "    \"I forgot to take my medication yesterday. What should I do?\",\n",
    "    \"I read online that my symptoms could mean cancer.\",\n",
    "    \"Can you tell me if I should take ibuprofen or acetaminophen?\",\n",
    "]\n",
    "\n",
    "# Physician-preferred responses (empathetic, appropriate, safe)\n",
    "PREFERRED_RESPONSES = [\n",
    "    \"I understand that persistent headaches can be concerning. This could be related to tension, dehydration, or other common factors. I recommend keeping a headache diary noting when they occur and discussing the pattern with your physician at your next visit.\",\n",
    "    \"I understand your concern. A fever of 101F in children is quite common and often the body's natural response to fighting infection. Keep your child hydrated and comfortable. If the fever persists beyond 48 hours or exceeds 103F, please contact your pediatrician.\",\n",
    "    \"It is completely normal to feel anxious before surgery. Many patients experience this. I recommend discussing your specific concerns with your surgical team \u2014 they can walk you through what to expect and address any questions you have.\",\n",
    "    \"New rashes can have many causes including contact irritation, allergies, or minor infections. If the rash is not spreading, painful, or accompanied by fever, you can monitor it for a few days. If it changes or you develop other symptoms, consult your dermatologist.\",\n",
    "    \"Persistent fatigue lasting a month warrants attention. Common causes include sleep quality, stress, nutritional factors, or other treatable conditions. I recommend scheduling an appointment with your physician to discuss this and potentially run some routine blood work.\",\n",
    "    \"I am sorry to hear your back pain has continued. Two weeks of persistent pain is a good reason to follow up with your physician. In the meantime, gentle stretching and avoiding prolonged sitting may help. Your physician can determine if imaging or physical therapy would be beneficial.\",\n",
    "    \"Sleep difficulties and stress often go hand in hand, and both are very common. Some helpful strategies include maintaining a consistent sleep schedule, limiting screen time before bed, and gentle relaxation techniques. If these issues persist, your physician can discuss additional options.\",\n",
    "    \"Missing a single dose happens to everyone. For most medications, you can take it as soon as you remember, unless it is close to your next scheduled dose. However, since medication instructions vary, I recommend checking with your pharmacist or physician for specific guidance on your medication.\",\n",
    "    \"I understand how frightening it can be to read about possible diagnoses online. It is important to know that many symptoms have multiple possible causes, and the most common explanations are usually benign. The best next step is to discuss your specific symptoms with your physician who can provide an accurate assessment.\",\n",
    "    \"Both ibuprofen and acetaminophen are effective for pain relief, but they work differently. The best choice depends on your specific situation, other medications you take, and your medical history. I recommend discussing this with your pharmacist or physician who can give you personalized guidance.\",\n",
    "]\n",
    "\n",
    "# Rejected responses (too clinical, alarming, or out of scope)\n",
    "REJECTED_RESPONSES = [\n",
    "    \"Persistent headaches can be indicative of various conditions including tension-type cephalalgia, migraine with or without aura, cluster headaches, or in rare cases intracranial pathology. You should get a CT scan.\",\n",
    "    \"Febrile episodes in pediatric patients can indicate bacterial or viral infections. Monitor for febrile seizures which occur in 2-5% of children. Administer antipyretics per weight-based dosing at 10-15mg/kg.\",\n",
    "    \"Pre-operative anxiety is a documented phenomenon. Consider asking your anesthesiologist about benzodiazepine premedication. The mortality rate for most surgeries is very low.\",\n",
    "    \"Dermatological presentations vary widely. This could be contact dermatitis, urticaria, psoriasis, eczema, or in some cases early presentation of autoimmune conditions. Apply hydrocortisone cream twice daily.\",\n",
    "    \"Chronic fatigue can be a symptom of numerous conditions including hypothyroidism, anemia, diabetes mellitus, chronic fatigue syndrome, depression, or malignancy. Get a comprehensive metabolic panel and CBC.\",\n",
    "    \"Persistent lower back pain may indicate lumbar disc herniation, spinal stenosis, spondylolisthesis, or other structural abnormalities. You should get an MRI and consider a referral to a spine specialist.\",\n",
    "    \"Insomnia and stress are comorbid conditions often treated pharmacologically. Consider melatonin supplementation or discuss SSRIs with your prescriber. Cognitive behavioral therapy for insomnia has evidence-based efficacy.\",\n",
    "    \"Missed doses can alter drug pharmacokinetics and potentially reduce therapeutic efficacy. Double the next dose to compensate for the missed one. Monitor for any breakthrough symptoms.\",\n",
    "    \"Your symptoms could indeed be consistent with oncological processes. Statistical likelihood depends on your demographic factors. I recommend urgent referral to oncology for comprehensive workup.\",\n",
    "    \"Take ibuprofen 400mg every 6 hours. It is an NSAID with anti-inflammatory properties superior to acetaminophen for musculoskeletal pain. Avoid if you have renal impairment or GI ulcer history.\",\n",
    "]\n",
    "\n",
    "print(f\"Created {len(CLINICAL_PROMPTS)} clinical preference pairs\")\n",
    "print(f\"Preferred response avg length: {np.mean([len(r.split()) for r in PREFERRED_RESPONSES]):.0f} words\")\n",
    "print(f\"Rejected response avg length: {np.mean([len(r.split()) for r in REJECTED_RESPONSES]):.0f} words\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze patterns in physician preferences\n",
    "def analyze_preference_patterns():\n",
    "    \"\"\"\n",
    "    Analyze the preference dataset to understand what physicians value.\n",
    "\n",
    "    TODO:\n",
    "    1. Compare word count distributions between preferred and rejected\n",
    "    2. Identify empathy markers (words like 'understand', 'normal', 'concern')\n",
    "    3. Identify alarm markers (words like 'cancer', 'mortality', 'urgent')\n",
    "    4. Create a bar chart of empathy vs alarm word frequency\n",
    "    \"\"\"\n",
    "    # Word counts\n",
    "    pref_lengths = [len(r.split()) for r in PREFERRED_RESPONSES]\n",
    "    rej_lengths = [len(r.split()) for r in REJECTED_RESPONSES]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    axes[0].bar(['Preferred', 'Rejected'],\n",
    "                [np.mean(pref_lengths), np.mean(rej_lengths)],\n",
    "                color=['#66b3ff', '#ff9999'], edgecolor='black')\n",
    "    axes[0].set_ylabel('Average Word Count')\n",
    "    axes[0].set_title('Response Length: Preferred vs Rejected')\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Empathy vs alarm words\n",
    "    empathy_words = {'understand', 'normal', 'common', 'concern', 'recommend',\n",
    "                     'helpful', 'sorry', 'completely', 'natural'}\n",
    "    alarm_words = {'cancer', 'mortality', 'urgent', 'immediately', 'seizure',\n",
    "                   'malignancy', 'pathology', 'double', 'urgent'}\n",
    "\n",
    "    pref_empathy = sum(1 for r in PREFERRED_RESPONSES\n",
    "                       for w in r.lower().split() if w.strip('.,') in empathy_words)\n",
    "    pref_alarm = sum(1 for r in PREFERRED_RESPONSES\n",
    "                     for w in r.lower().split() if w.strip('.,') in alarm_words)\n",
    "    rej_empathy = sum(1 for r in REJECTED_RESPONSES\n",
    "                      for w in r.lower().split() if w.strip('.,') in empathy_words)\n",
    "    rej_alarm = sum(1 for r in REJECTED_RESPONSES\n",
    "                    for w in r.lower().split() if w.strip('.,') in alarm_words)\n",
    "\n",
    "    x = np.arange(2)\n",
    "    width = 0.35\n",
    "    axes[1].bar(x - width/2, [pref_empathy, pref_alarm], width,\n",
    "                label='Preferred', color='#66b3ff')\n",
    "    axes[1].bar(x + width/2, [rej_empathy, rej_alarm], width,\n",
    "                label='Rejected', color='#ff9999')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(['Empathy Words', 'Alarm Words'])\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_title('Language Patterns in Preferences')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nKey finding: Preferred responses use more empathy language\")\n",
    "    print(\"and fewer alarming medical terms.\")\n",
    "\n",
    "analyze_preference_patterns()"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Model (SFT)"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this case study, we use a simple model to demonstrate the pipeline\n",
    "# In production, this would be a 7B parameter model like Llama-2-7B\n",
    "\n",
    "class SimpleLanguageModel(nn.Module):\n",
    "    \"\"\"Simplified language model for demonstrating RLHF mechanics.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size=5000, embed_dim=128, hidden_dim=256, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(embed_dim, nhead=4, dim_feedforward=hidden_dim,\n",
    "                                       batch_first=True)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.output_head = nn.Linear(embed_dim, vocab_size)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, input_ids, return_hidden=False):\n",
    "        x = self.embedding(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        if return_hidden:\n",
    "            return x\n",
    "        logits = self.output_head(x)\n",
    "        return logits\n",
    "\n",
    "    def get_hidden(self, input_ids):\n",
    "        return self.forward(input_ids, return_hidden=True)\n",
    "\n",
    "model = SimpleLanguageModel().to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reward Model"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClinicalRewardModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Reward model with dual heads:\n",
    "    - Scalar reward head (trained on physician preferences)\n",
    "    - Safety classifier head (trained on safety annotations)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.reward_head = nn.Linear(hidden_dim, 1)\n",
    "        self.safety_head = nn.Linear(hidden_dim, 2)  # safe / needs escalation\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        hidden = self.backbone.get_hidden(input_ids)\n",
    "        pooled = hidden.mean(dim=1)  # Average pooling\n",
    "        reward = self.reward_head(pooled).squeeze(-1)\n",
    "        safety_logits = self.safety_head(pooled)\n",
    "        return reward, safety_logits\n",
    "\n",
    "# TODO: Implement the training loop for the reward model\n",
    "def train_reward_model(reward_model, train_data, num_epochs=10, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train the reward model on physician preference pairs.\n",
    "\n",
    "    Args:\n",
    "        reward_model: ClinicalRewardModel instance\n",
    "        train_data: list of (preferred_ids, rejected_ids, safety_label) tuples\n",
    "        num_epochs: number of training epochs\n",
    "        lr: learning rate\n",
    "\n",
    "    TODO:\n",
    "    1. For each pair, compute rewards for both preferred and rejected\n",
    "    2. Compute Bradley-Terry loss: -log(sigma(r_pref - r_rej))\n",
    "    3. Compute safety classification loss (cross-entropy)\n",
    "    4. Combine: total = bt_loss + 2.0 * safety_loss\n",
    "    5. Track and return training metrics\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(reward_model.parameters(), lr=lr)\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for pref_ids, rej_ids, safety_label in train_data:\n",
    "            r_pref, safety_pref = reward_model(pref_ids.unsqueeze(0).to(device))\n",
    "            r_rej, safety_rej = reward_model(rej_ids.unsqueeze(0).to(device))\n",
    "\n",
    "            bt_loss = -F.logsigmoid(r_pref - r_rej).mean()\n",
    "            safety_target = torch.tensor([safety_label], device=device)\n",
    "            safety_loss = F.cross_entropy(safety_pref, safety_target)\n",
    "\n",
    "            total_loss = bt_loss + 2.0 * safety_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += total_loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_data)\n",
    "        losses.append(avg_loss)\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} \u2014 Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "# Create synthetic training data (tokenized as random IDs for demo)\n",
    "train_pairs = []\n",
    "for i in range(len(CLINICAL_PROMPTS)):\n",
    "    pref_ids = torch.randint(0, 5000, (50,))\n",
    "    rej_ids = torch.randint(0, 5000, (50,))\n",
    "    safety_label = 0  # Most responses are safe\n",
    "    train_pairs.append((pref_ids, rej_ids, safety_label))\n",
    "\n",
    "# Add some safety-critical examples\n",
    "for _ in range(3):\n",
    "    pref_ids = torch.randint(0, 5000, (50,))\n",
    "    rej_ids = torch.randint(0, 5000, (50,))\n",
    "    train_pairs.append((pref_ids, rej_ids, 1))  # Needs escalation\n",
    "\n",
    "backbone = SimpleLanguageModel().to(device)\n",
    "reward_model = ClinicalRewardModel(backbone).to(device)\n",
    "losses = train_reward_model(reward_model, train_pairs)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Reward Model Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PPO Training with KL Penalty"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rlhf_step(model, ref_model, reward_model, optimizer,\n",
    "              input_ids, beta=0.1, epsilon=0.2):\n",
    "    \"\"\"\n",
    "    One RLHF optimization step.\n",
    "\n",
    "    TODO: Implement the full PPO step:\n",
    "    1. Get model logits and ref_model logits\n",
    "    2. Compute per-token KL divergence\n",
    "    3. Get reward from reward_model\n",
    "    4. Compute total reward = reward_RM - beta * KL\n",
    "    5. Compute PPO clipped loss\n",
    "    6. Update model\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    # Forward through current model\n",
    "    logits = model(input_ids)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # Forward through reference model (no grad)\n",
    "    with torch.no_grad():\n",
    "        ref_logits = ref_model(input_ids)\n",
    "        ref_log_probs = F.log_softmax(ref_logits, dim=-1)\n",
    "\n",
    "    # Per-token KL\n",
    "    token_ids = input_ids[:, 1:]\n",
    "    model_token_lp = log_probs[:, :-1].gather(2, token_ids.unsqueeze(-1)).squeeze(-1)\n",
    "    ref_token_lp = ref_log_probs[:, :-1].gather(2, token_ids.unsqueeze(-1)).squeeze(-1)\n",
    "    kl = (model_token_lp - ref_token_lp)\n",
    "\n",
    "    # Reward\n",
    "    with torch.no_grad():\n",
    "        reward, safety = reward_model(input_ids)\n",
    "\n",
    "    # Total reward = reward_RM - beta * total_KL\n",
    "    total_kl = kl.abs().mean()\n",
    "    effective_reward = reward - beta * total_kl\n",
    "\n",
    "    # Simple policy gradient (REINFORCE with baseline)\n",
    "    baseline = effective_reward.mean()\n",
    "    advantage = effective_reward - baseline\n",
    "\n",
    "    pg_loss = -(model_token_lp.mean(dim=1) * advantage.detach()).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    pg_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    return {\n",
    "        'loss': pg_loss.item(),\n",
    "        'reward': reward.mean().item(),\n",
    "        'kl': total_kl.item(),\n",
    "        'effective_reward': effective_reward.mean().item(),\n",
    "    }"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Full Training Loop"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "active_model = SimpleLanguageModel().to(device)\n",
    "ref_model = SimpleLanguageModel().to(device)\n",
    "ref_model.load_state_dict(active_model.state_dict())\n",
    "ref_model.eval()\n",
    "for p in ref_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "reward_backbone = SimpleLanguageModel().to(device)\n",
    "reward_model = ClinicalRewardModel(reward_backbone).to(device)\n",
    "# Pre-train reward model\n",
    "_ = train_reward_model(reward_model, train_pairs, num_epochs=20)\n",
    "\n",
    "optimizer = torch.optim.Adam(active_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training\n",
    "metrics_history = {'rewards': [], 'kl': [], 'losses': [], 'effective_rewards': []}\n",
    "num_steps = 100\n",
    "\n",
    "print(\"Starting RLHF training...\")\n",
    "for step in range(num_steps):\n",
    "    # Random input (in production: real patient queries)\n",
    "    input_ids = torch.randint(0, 5000, (4, 50)).to(device)\n",
    "\n",
    "    metrics = rlhf_step(active_model, ref_model, reward_model,\n",
    "                        optimizer, input_ids, beta=0.1)\n",
    "\n",
    "    for key in metrics_history:\n",
    "        short_key = key.rstrip('s') if key != 'losses' else 'loss'\n",
    "        if key == 'effective_rewards':\n",
    "            metrics_history[key].append(metrics['effective_reward'])\n",
    "        elif key == 'rewards':\n",
    "            metrics_history[key].append(metrics['reward'])\n",
    "        elif key == 'losses':\n",
    "            metrics_history[key].append(metrics['loss'])\n",
    "        elif key == 'kl':\n",
    "            metrics_history[key].append(metrics['kl'])\n",
    "\n",
    "    if (step + 1) % 25 == 0:\n",
    "        print(f\"Step {step+1}/{num_steps} \u2014 \"\n",
    "              f\"Reward: {metrics['reward']:.3f}, \"\n",
    "              f\"KL: {metrics['kl']:.4f}, \"\n",
    "              f\"Eff. Reward: {metrics['effective_reward']:.3f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation and Results"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "axes[0, 0].plot(metrics_history['rewards'], 'b-', linewidth=1.5)\n",
    "axes[0, 0].set_title('Reward Model Score')\n",
    "axes[0, 0].set_xlabel('Step')\n",
    "axes[0, 0].set_ylabel('Reward')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(metrics_history['kl'], 'r-', linewidth=1.5)\n",
    "axes[0, 1].set_title('KL Divergence from Reference')\n",
    "axes[0, 1].set_xlabel('Step')\n",
    "axes[0, 1].set_ylabel('KL')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(metrics_history['losses'], 'g-', linewidth=1.5)\n",
    "axes[1, 0].set_title('Policy Gradient Loss')\n",
    "axes[1, 0].set_xlabel('Step')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(metrics_history['effective_rewards'], 'm-', linewidth=1.5)\n",
    "axes[1, 1].set_title('Effective Reward (RM - beta*KL)')\n",
    "axes[1, 1].set_xlabel('Step')\n",
    "axes[1, 1].set_ylabel('Effective Reward')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('MedAlign RLHF Training Dashboard', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Analysis"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement error analysis\n",
    "def error_analysis():\n",
    "    \"\"\"\n",
    "    Analyze failure modes of the aligned model.\n",
    "\n",
    "    TODO:\n",
    "    1. Score all test responses with the reward model\n",
    "    2. Identify the bottom 20% (lowest-scoring)\n",
    "    3. Categorize failures by type\n",
    "    4. Create a confusion matrix for safety classification\n",
    "    5. Plot the reward distribution highlighting the failure tail\n",
    "    \"\"\"\n",
    "    # Simulate scores for analysis\n",
    "    scores = np.random.normal(1.5, 0.8, 100)\n",
    "    safety_preds = np.random.binomial(1, 0.05, 100)  # 5% flagged\n",
    "    safety_true = np.random.binomial(1, 0.03, 100)   # 3% actually need escalation\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Reward distribution\n",
    "    axes[0].hist(scores, bins=20, color='#66b3ff', edgecolor='black', alpha=0.7)\n",
    "    threshold = np.percentile(scores, 20)\n",
    "    axes[0].axvline(x=threshold, color='red', linestyle='--',\n",
    "                    label=f'Bottom 20% threshold ({threshold:.2f})')\n",
    "    axes[0].set_xlabel('Reward Score')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Response Quality Distribution')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Safety confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    try:\n",
    "        cm = confusion_matrix(safety_true, safety_preds)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
    "                    xticklabels=['Safe', 'Escalate'],\n",
    "                    yticklabels=['Safe', 'Escalate'])\n",
    "        axes[1].set_xlabel('Predicted')\n",
    "        axes[1].set_ylabel('Actual')\n",
    "        axes[1].set_title('Safety Classification Confusion Matrix')\n",
    "    except ImportError:\n",
    "        axes[1].text(0.5, 0.5, 'Install sklearn for confusion matrix',\n",
    "                     ha='center', va='center', transform=axes[1].transAxes)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nSafety compliance rate: {(1 - safety_preds.mean()) * 100:.1f}%\")\n",
    "    print(f\"False escalation rate: {(safety_preds.sum() - (safety_preds & safety_true).sum()) / max(safety_preds.sum(), 1) * 100:.1f}%\")\n",
    "\n",
    "error_analysis()"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Production Deployment Considerations"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement deployment preparation\n",
    "def prepare_deployment_config():\n",
    "    \"\"\"\n",
    "    Generate deployment configuration for the aligned model.\n",
    "\n",
    "    TODO:\n",
    "    1. Define the API schema (input/output formats)\n",
    "    2. Configure safety thresholds\n",
    "    3. Set up A/B testing parameters\n",
    "    4. Define monitoring alerts\n",
    "    5. Create rollback criteria\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        \"model\": {\n",
    "            \"name\": \"clinassist-rlhf-v2\",\n",
    "            \"parameters\": \"7B\",\n",
    "            \"quantization\": \"none\",\n",
    "            \"max_tokens\": 256,\n",
    "            \"temperature\": 0.7,\n",
    "        },\n",
    "        \"safety\": {\n",
    "            \"escalation_threshold\": 0.5,\n",
    "            \"max_response_length\": 300,\n",
    "            \"blocked_patterns\": [\"take [0-9]+ mg\", \"stop taking\", \"diagnosis:\"],\n",
    "            \"require_disclaimer\": True,\n",
    "        },\n",
    "        \"monitoring\": {\n",
    "            \"reward_drift_threshold\": 0.5,\n",
    "            \"kl_alert_threshold\": 5.0,\n",
    "            \"safety_alert_rate\": 0.02,\n",
    "            \"logging_sample_rate\": 1.0,\n",
    "        },\n",
    "        \"ab_testing\": {\n",
    "            \"rlhf_traffic_fraction\": 0.9,\n",
    "            \"sft_traffic_fraction\": 0.1,\n",
    "            \"min_duration_weeks\": 4,\n",
    "            \"significance_level\": 0.01,\n",
    "        },\n",
    "        \"rollback\": {\n",
    "            \"safety_compliance_min\": 0.99,\n",
    "            \"reward_drop_threshold\": 1.0,\n",
    "            \"auto_rollback_enabled\": True,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(\"Deployment Configuration:\")\n",
    "    print(\"=\" * 50)\n",
    "    for section, params in config.items():\n",
    "        print(f\"\\n[{section}]\")\n",
    "        for key, value in params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "    return config\n",
    "\n",
    "config = prepare_deployment_config()\n",
    "\n",
    "print(\"\\n\\nModel ready for staged deployment:\")\n",
    "print(\"  1. Canary (1% traffic) \u2014 1 week\")\n",
    "print(\"  2. Shadow (10% traffic, log-only) \u2014 1 week\")\n",
    "print(\"  3. A/B Test (90/10 split) \u2014 4 weeks\")\n",
    "print(\"  4. Full production \u2014 continuous monitoring\")"
   ],
   "id": "cell_19"
  }
 ]
}