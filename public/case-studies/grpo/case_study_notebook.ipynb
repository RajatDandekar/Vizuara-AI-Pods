{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Financial Report Reasoning with GRPO -- Implementation Notebook"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Financial Report Reasoning with GRPO -- Implementation Notebook\n",
    "\n",
    "## Setup and Data\n",
    "\n",
    "This notebook implements the core components of the QuantaLedger Analytics GRPO training pipeline for financial reasoning."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.1: Data Pipeline"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialReasoningDataset:\n",
    "    \"\"\"\n",
    "    Synthetic dataset of financial reasoning questions.\n",
    "    Each question requires 1-3 steps of numerical computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_samples=1000, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.samples = self._generate_samples(n_samples)\n",
    "        print(f\"Generated {len(self.samples)} financial reasoning samples\")\n",
    "\n",
    "    def _generate_samples(self, n):\n",
    "        samples = []\n",
    "        for _ in range(n):\n",
    "            qtype = np.random.choice(['growth', 'margin', 'comparison', 'projection'])\n",
    "\n",
    "            if qtype == 'growth':\n",
    "                rev_old = round(np.random.uniform(0.5, 10.0), 1)\n",
    "                growth = round(np.random.uniform(-0.2, 0.5), 2)\n",
    "                rev_new = round(rev_old * (1 + growth), 1)\n",
    "\n",
    "                context = (f\"In Q3 2024, the company reported revenue of ${rev_new}B, \"\n",
    "                          f\"compared to ${rev_old}B in Q3 2023.\")\n",
    "                question = \"What was the year-over-year revenue growth rate?\"\n",
    "                answer = round(growth * 100, 1)\n",
    "                answer_type = 'percentage'\n",
    "                steps = 2\n",
    "\n",
    "            elif qtype == 'margin':\n",
    "                revenue = round(np.random.uniform(1.0, 15.0), 1)\n",
    "                margin_pct = round(np.random.uniform(0.05, 0.40), 2)\n",
    "                profit = round(revenue * margin_pct, 2)\n",
    "\n",
    "                context = (f\"The company generated revenue of ${revenue}B and \"\n",
    "                          f\"operating profit of ${profit}B in the fiscal year.\")\n",
    "                question = \"What was the operating margin?\"\n",
    "                answer = round(margin_pct * 100, 1)\n",
    "                answer_type = 'percentage'\n",
    "                steps = 1\n",
    "\n",
    "            elif qtype == 'comparison':\n",
    "                q1 = round(np.random.uniform(1.0, 5.0), 1)\n",
    "                q2 = round(np.random.uniform(1.0, 5.0), 1)\n",
    "                q3 = round(np.random.uniform(1.0, 5.0), 1)\n",
    "\n",
    "                context = (f\"Quarterly revenues: Q1=${q1}B, Q2=${q2}B, Q3=${q3}B.\")\n",
    "                question = \"What is the total revenue for the first three quarters?\"\n",
    "                answer = round(q1 + q2 + q3, 1)\n",
    "                answer_type = 'dollar'\n",
    "                steps = 1\n",
    "\n",
    "            else:  # projection\n",
    "                current = round(np.random.uniform(2.0, 10.0), 1)\n",
    "                growth = round(np.random.uniform(0.05, 0.25), 2)\n",
    "\n",
    "                context = (f\"Current annual revenue is ${current}B with \"\n",
    "                          f\"a {round(growth*100,0):.0f}% annual growth rate.\")\n",
    "                question = \"What is the projected revenue next year?\"\n",
    "                answer = round(current * (1 + growth), 1)\n",
    "                answer_type = 'dollar'\n",
    "                steps = 2\n",
    "\n",
    "            samples.append({\n",
    "                'context': context,\n",
    "                'question': question,\n",
    "                'answer': answer,\n",
    "                'answer_type': answer_type,\n",
    "                'steps': steps,\n",
    "                'qtype': qtype,\n",
    "            })\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "    def get_random(self):\n",
    "        return self.samples[np.random.randint(len(self.samples))]\n",
    "\n",
    "# Create dataset\n",
    "dataset = FinancialReasoningDataset(n_samples=500)\n",
    "\n",
    "# Show examples\n",
    "for i in range(3):\n",
    "    s = dataset[i]\n",
    "    print(f\"Context: {s['context']}\")\n",
    "    print(f\"Question: {s['question']}\")\n",
    "    print(f\"Answer: {s['answer']} ({s['answer_type']})\")\n",
    "    print(f\"Steps: {s['steps']}, Type: {s['qtype']}\")\n",
    "    print()"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.2: Exploratory Data Analysis"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze the dataset distribution\n",
    "# Hint: Count samples by qtype, steps, answer_type\n",
    "\n",
    "qtypes = [s['qtype'] for s in dataset.samples]\n",
    "steps = [s['steps'] for s in dataset.samples]\n",
    "atypes = [s['answer_type'] for s in dataset.samples]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Question types\n",
    "unique_qt, counts_qt = np.unique(qtypes, return_counts=True)\n",
    "axes[0].bar(unique_qt, counts_qt, color=['#4a90d9', '#50c878', '#ff6b6b', '#ffd700'])\n",
    "axes[0].set_title(\"Question Types\", fontweight='bold')\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Steps required\n",
    "unique_st, counts_st = np.unique(steps, return_counts=True)\n",
    "axes[1].bar([str(s) for s in unique_st], counts_st, color='#4a90d9')\n",
    "axes[1].set_title(\"Reasoning Steps Required\", fontweight='bold')\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "# Answer types\n",
    "unique_at, counts_at = np.unique(atypes, return_counts=True)\n",
    "axes[2].bar(unique_at, counts_at, color=['#50c878', '#ff6b6b'])\n",
    "axes[2].set_title(\"Answer Types\", fontweight='bold')\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"dataset_analysis.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.3: Model and Tokenizer"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    \"\"\"Character-level tokenizer for financial expressions.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        chars = list(\"0123456789.+-*/%=$B QqWwEeRrTtYyUuIiOoPpAaSsDdFfGgHhJjKkLlZzXxCcVvNnMm,:<>()\\\"'\")\n",
    "        self.char_to_id = {c: i+1 for i, c in enumerate(chars)}  # 0 = padding\n",
    "        self.id_to_char = {i+1: c for i, c in enumerate(chars)}\n",
    "        self.vocab_size = len(chars) + 1  # +1 for padding\n",
    "        self.pad_id = 0\n",
    "\n",
    "    def encode(self, text, max_len=None):\n",
    "        ids = [self.char_to_id.get(c, 0) for c in text]\n",
    "        if max_len:\n",
    "            ids = ids[:max_len]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return ''.join([self.id_to_char.get(i, '?') for i in ids if i > 0])\n",
    "\n",
    "tokenizer = CharTokenizer()\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Test\n",
    "test = \"Revenue: $2.3B\"\n",
    "encoded = tokenizer.encode(test)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"Encode '{test}': {encoded[:10]}...\")\n",
    "print(f\"Decode back: '{decoded}'\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialTransformer(nn.Module):\n",
    "    \"\"\"Small transformer for financial reasoning.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model=128, n_heads=4, n_layers=3, max_len=200):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "        self.max_len = max_len\n",
    "\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads,\n",
    "            dim_feedforward=256, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        pos = torch.arange(min(T, self.max_len), device=x.device).unsqueeze(0)\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(T).to(x.device)\n",
    "        h = self.embedding(x) + self.pos_embedding(pos[:, :T])\n",
    "        h = self.transformer(h, mask=mask, is_causal=True)\n",
    "        return self.head(h)\n",
    "\n",
    "    def generate(self, prompt_ids, max_new_tokens=30, temperature=1.0):\n",
    "        generated = prompt_ids.clone()\n",
    "        for _ in range(max_new_tokens):\n",
    "            if generated.shape[1] >= self.max_len:\n",
    "                break\n",
    "            logits = self.forward(generated)\n",
    "            next_logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "        return generated\n",
    "\n",
    "# Create models\n",
    "torch.manual_seed(42)\n",
    "model = FinancialTransformer(tokenizer.vocab_size, d_model=128, n_heads=4, n_layers=3).to(device)\n",
    "ref_model = FinancialTransformer(tokenizer.vocab_size, d_model=128, n_heads=4, n_layers=3).to(device)\n",
    "ref_model.load_state_dict(model.state_dict())\n",
    "ref_model.requires_grad_(False)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params:,}\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.4: Reward Function"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(text):\n",
    "    \"\"\"Extract numerical answer from model output.\"\"\"\n",
    "    # Look for pattern: Answer: <number>\n",
    "    m = re.search(r'Answer:\\s*([\\d.]+)', text)\n",
    "    if m:\n",
    "        try:\n",
    "            return float(m.group(1))\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    # Fallback: look for any number at the end\n",
    "    numbers = re.findall(r'[\\d.]+', text)\n",
    "    if numbers:\n",
    "        try:\n",
    "            return float(numbers[-1])\n",
    "        except:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def compute_reward(generated_text, ground_truth, answer_type, format_weight=0.1):\n",
    "    \"\"\"\n",
    "    Compute verifiable reward for financial reasoning.\n",
    "\n",
    "    TODO: Implement tolerance-based answer verification.\n",
    "    \"\"\"\n",
    "    predicted = extract_answer(generated_text)\n",
    "\n",
    "    # Correctness reward\n",
    "    if predicted is None:\n",
    "        correct = 0.0\n",
    "    else:\n",
    "        if answer_type == 'percentage':\n",
    "            correct = 1.0 if abs(predicted - ground_truth) < 0.5 else 0.0\n",
    "        elif answer_type == 'dollar':\n",
    "            correct = 1.0 if abs(predicted - ground_truth) < 0.1 else 0.0\n",
    "        else:\n",
    "            correct = 1.0 if abs(predicted - ground_truth) < 0.01 else 0.0\n",
    "\n",
    "    # Format reward\n",
    "    has_think = '<think>' in generated_text and '</think>' in generated_text\n",
    "    has_answer = 'Answer:' in generated_text\n",
    "    format_score = 1.0 if (has_think and has_answer) else 0.0\n",
    "\n",
    "    return correct + format_weight * format_score\n",
    "\n",
    "# Test\n",
    "print(compute_reward(\"<think>Growth = 21.1%</think> Answer: 21.1\", 21.1, 'percentage'))\n",
    "print(compute_reward(\"Answer: 22.0\", 21.1, 'percentage'))\n",
    "print(compute_reward(\"I think 21.1\", 21.1, 'percentage'))"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.5: GRPO Training"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grpo_advantages(rewards):\n",
    "    mean_r = rewards.mean()\n",
    "    std_r = rewards.std()\n",
    "    if std_r < 1e-8:\n",
    "        return torch.zeros_like(rewards)\n",
    "    return (rewards - mean_r) / std_r\n",
    "\n",
    "def grpo_step(model, ref_model, tokenizer, dataset, optimizer, G=8, epsilon=0.2, beta=0.04):\n",
    "    \"\"\"One GRPO training step.\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    sample = dataset.get_random()\n",
    "    prompt_text = f\"{sample['context']} Question: {sample['question']} \"\n",
    "    prompt_ids = torch.tensor([tokenizer.encode(prompt_text, max_len=150)]).to(device)\n",
    "    prompt_len = prompt_ids.shape[1]\n",
    "\n",
    "    # Generate G completions\n",
    "    completions = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(G):\n",
    "            comp = model.generate(prompt_ids, max_new_tokens=30, temperature=1.0)\n",
    "            completions.append(comp)\n",
    "\n",
    "    # Compute rewards\n",
    "    rewards = []\n",
    "    for comp in completions:\n",
    "        text = tokenizer.decode(comp[0, prompt_len:].tolist())\n",
    "        r = compute_reward(text, sample['answer'], sample['answer_type'])\n",
    "        rewards.append(r)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "\n",
    "    advantages = compute_grpo_advantages(rewards)\n",
    "\n",
    "    if advantages.abs().sum() < 1e-8:\n",
    "        return 0.0, rewards.mean().item()\n",
    "\n",
    "    # Pad and compute log probs\n",
    "    max_len = max(c.shape[1] for c in completions)\n",
    "    padded = torch.zeros(G, max_len, dtype=torch.long).to(device)\n",
    "    mask = torch.zeros(G, max(1, max_len - 1)).to(device)\n",
    "\n",
    "    for i, comp in enumerate(completions):\n",
    "        L = comp.shape[1]\n",
    "        padded[i, :L] = comp[0]\n",
    "        if max_len > 1:\n",
    "            mask[i, prompt_len:min(L-1, max_len-1)] = 1.0\n",
    "\n",
    "    if mask.sum() < 1:\n",
    "        return 0.0, rewards.mean().item()\n",
    "\n",
    "    logits = model(padded[:, :-1])\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    targets = padded[:, 1:]\n",
    "    token_log_probs = log_probs.gather(2, targets.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        old_token_log_probs = token_log_probs.detach()\n",
    "        ref_logits = ref_model(padded[:, :-1])\n",
    "        ref_log_probs = F.log_softmax(ref_logits, dim=-1)\n",
    "        ref_token_log_probs = ref_log_probs.gather(2, targets.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # GRPO loss\n",
    "    ratio = torch.exp(token_log_probs - old_token_log_probs)\n",
    "    adv = advantages.unsqueeze(1)\n",
    "    surr1 = ratio * adv\n",
    "    surr2 = torch.clamp(ratio, 1-epsilon, 1+epsilon) * adv\n",
    "    policy_obj = torch.min(surr1, surr2)\n",
    "\n",
    "    kl_ratio = torch.exp(ref_token_log_probs - token_log_probs)\n",
    "    kl = kl_ratio - torch.log(kl_ratio) - 1.0\n",
    "\n",
    "    per_token = policy_obj - beta * kl\n",
    "    per_response = (per_token * mask[:, :per_token.shape[1]]).sum(dim=1) / mask[:, :per_token.shape[1]].sum(dim=1).clamp(min=1)\n",
    "    loss = -per_response.mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), rewards.mean().item()\n",
    "\n",
    "print(\"GRPO training step ready!\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.6: Training Loop"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run the full training loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "n_steps = 300\n",
    "log_every = 30\n",
    "losses = []\n",
    "rewards_history = []\n",
    "\n",
    "print(\"Training with GRPO...\")\n",
    "start = time.time()\n",
    "\n",
    "for step in range(n_steps):\n",
    "    loss, mean_reward = grpo_step(model, ref_model, tokenizer, dataset, optimizer, G=8)\n",
    "    losses.append(loss)\n",
    "    rewards_history.append(mean_reward)\n",
    "\n",
    "    if (step + 1) % log_every == 0:\n",
    "        avg_loss = np.mean(losses[-log_every:])\n",
    "        avg_reward = np.mean(rewards_history[-log_every:])\n",
    "        print(f\"Step {step+1:4d} | Loss: {avg_loss:.4f} | Reward: {avg_reward:.3f} | Time: {time.time()-start:.0f}s\")\n",
    "\n",
    "print(\"Training complete!\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.7: Results and Error Analysis"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "window = 20\n",
    "if len(losses) > window:\n",
    "    loss_smooth = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "    reward_smooth = np.convolve(rewards_history, np.ones(window)/window, mode='valid')\n",
    "\n",
    "    axes[0].plot(loss_smooth, linewidth=2, color='red')\n",
    "    axes[0].set_title(\"Training Loss\", fontweight='bold')\n",
    "    axes[0].set_xlabel(\"Step\")\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1].plot(reward_smooth, linewidth=2, color='blue')\n",
    "    axes[1].set_title(\"Mean Reward\", fontweight='bold')\n",
    "    axes[1].set_xlabel(\"Step\")\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"financial_grpo_training.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate on test samples\n",
    "print(\"Evaluation on 20 test samples:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "correct = 0\n",
    "total = 20\n",
    "for i in range(total):\n",
    "    sample = dataset[i + 400]  # Use samples not seen in training\n",
    "    prompt = f\"{sample['context']} Question: {sample['question']} \"\n",
    "    prompt_ids = torch.tensor([tokenizer.encode(prompt, max_len=150)]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(prompt_ids, max_new_tokens=30, temperature=0.1)\n",
    "\n",
    "    generated = tokenizer.decode(output[0, prompt_ids.shape[1]:].tolist())\n",
    "    r = compute_reward(generated, sample['answer'], sample['answer_type'])\n",
    "    correct += (r >= 1.0)\n",
    "\n",
    "    status = \"OK\" if r >= 1.0 else \"MISS\"\n",
    "    print(f\"  [{status}] Q: {sample['question'][:40]}... A: {sample['answer']} | Gen: {generated[:30]}\")\n",
    "\n",
    "print(f\"\\nAccuracy: {correct}/{total} = {correct/total:.0%}\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.8-3.9: Deployment and Ethics"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Deployment readiness checklist\n",
    "print(\"=\" * 50)\n",
    "print(\"Deployment Readiness Checklist\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"[ ] Model quantized to INT8\")\n",
    "print(\"[ ] Latency < 2 seconds on A100\")\n",
    "print(\"[ ] Accuracy > 75% on test set\")\n",
    "print(\"[ ] KL divergence < 5.0\")\n",
    "print(\"[ ] Error analysis documented\")\n",
    "print(\"[ ] Monitoring dashboards configured\")\n",
    "print(\"[ ] A/B test plan approved\")\n",
    "print(\"[ ] Ethics review completed\")\n",
    "print()\n",
    "print(\"Ethics Considerations:\")\n",
    "print(\"  1. Hallucination: model may generate plausible but wrong numbers\")\n",
    "print(\"  2. Overconfidence: no uncertainty quantification\")\n",
    "print(\"  3. Regulatory: not a substitute for human financial analysis\")\n",
    "print(\"  4. Bias: performance may vary across industries\")"
   ],
   "id": "cell_20"
  }
 ]
}