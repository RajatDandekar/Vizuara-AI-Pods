{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Radiology Report Generation with Multimodal Instruction Tuning -- Implementation Notebook"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radiology Report Generation with Multimodal Instruction Tuning -- Implementation Notebook\n",
    "\n",
    "*Meridian Diagnostic Intelligence Case Study*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and installations\n",
    "!pip install -q torch torchvision matplotlib numpy scikit-learn Pillow\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "import random\n",
    "import time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Creation\n",
    "\n",
    "We create a synthetic radiology dataset that mimics the structure of MIMIC-CXR."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticRadiologyDataset:\n",
    "    \"\"\"Synthetic dataset mimicking chest X-ray report generation.\n",
    "\n",
    "    Each sample has:\n",
    "    - A synthetic grayscale image with abnormality patterns\n",
    "    - Multi-label annotations (14 CheXpert-style labels)\n",
    "    - A structured report text\n",
    "    \"\"\"\n",
    "\n",
    "    FINDINGS = [\n",
    "        \"cardiomegaly\", \"edema\", \"consolidation\", \"atelectasis\",\n",
    "        \"pleural_effusion\", \"pneumothorax\", \"pneumonia\", \"nodule\",\n",
    "        \"mass\", \"fracture\", \"emphysema\", \"fibrosis\",\n",
    "        \"thickening\", \"hernia\"\n",
    "    ]\n",
    "\n",
    "    TEMPLATES = {\n",
    "        \"cardiomegaly\": \"The cardiac silhouette is enlarged, suggesting cardiomegaly.\",\n",
    "        \"edema\": \"There is pulmonary edema with bilateral perihilar haziness.\",\n",
    "        \"consolidation\": \"Consolidation is present in the {location}.\",\n",
    "        \"atelectasis\": \"Linear atelectasis is noted in the {location}.\",\n",
    "        \"pleural_effusion\": \"There is a {side} pleural effusion.\",\n",
    "        \"pneumothorax\": \"A {side} pneumothorax is identified.\",\n",
    "        \"pneumonia\": \"Findings consistent with pneumonia in the {location}.\",\n",
    "        \"nodule\": \"A {size} nodule is noted in the {location}.\",\n",
    "        \"mass\": \"A mass is present in the {location}.\",\n",
    "        \"fracture\": \"A {side} rib fracture is identified.\",\n",
    "        \"emphysema\": \"Hyperinflation consistent with emphysema.\",\n",
    "        \"fibrosis\": \"Interstitial changes suggesting fibrosis.\",\n",
    "        \"thickening\": \"Pleural thickening noted on the {side}.\",\n",
    "        \"hernia\": \"A hiatal hernia is present.\",\n",
    "    }\n",
    "\n",
    "    LOCATIONS = [\"right lower lobe\", \"left lower lobe\", \"right upper lobe\",\n",
    "                 \"left upper lobe\", \"right middle lobe\", \"bilateral bases\"]\n",
    "    SIDES = [\"right\", \"left\", \"bilateral\"]\n",
    "    SIZES = [\"5mm\", \"8mm\", \"12mm\", \"15mm\"]\n",
    "\n",
    "    VOCAB = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2}\n",
    "    _next_id = 3\n",
    "\n",
    "    def __init__(self, n_samples=500, image_size=128):\n",
    "        self.image_size = image_size\n",
    "        self.data = []\n",
    "\n",
    "        # Build vocabulary from templates\n",
    "        for template in self.TEMPLATES.values():\n",
    "            for word in template.lower().replace(\",\", \"\").replace(\".\", \"\").split():\n",
    "                if word not in self.VOCAB:\n",
    "                    self.VOCAB[word] = self._next_id\n",
    "                    self._next_id += 1\n",
    "        for loc in self.LOCATIONS + self.SIDES + self.SIZES:\n",
    "            for word in loc.lower().split():\n",
    "                if word not in self.VOCAB:\n",
    "                    self.VOCAB[word] = self._next_id\n",
    "                    self._next_id += 1\n",
    "        # Add extra tokens for normal finding\n",
    "        for word in [\"no\", \"acute\", \"cardiopulmonary\", \"abnormality\", \"identified\", \"normal\", \"study\"]:\n",
    "            if word not in self.VOCAB:\n",
    "                self.VOCAB[word] = self._next_id\n",
    "                self._next_id += 1\n",
    "\n",
    "        self.INV_VOCAB = {v: k for k, v in self.VOCAB.items()}\n",
    "\n",
    "        for _ in range(n_samples):\n",
    "            # Random number of findings (0-3)\n",
    "            n_findings = np.random.choice([0, 1, 2, 3], p=[0.3, 0.35, 0.25, 0.1])\n",
    "            finding_indices = sorted(random.sample(range(14), n_findings))\n",
    "            labels = torch.zeros(14)\n",
    "            labels[finding_indices] = 1.0\n",
    "\n",
    "            # Generate image\n",
    "            img = self._create_image(finding_indices)\n",
    "\n",
    "            # Generate report\n",
    "            report = self._generate_report(finding_indices)\n",
    "            report_tokens = self._tokenize(report)\n",
    "\n",
    "            self.data.append({\n",
    "                \"image\": img,\n",
    "                \"labels\": labels,\n",
    "                \"report_text\": report,\n",
    "                \"report_tokens\": report_tokens,\n",
    "                \"findings\": [self.FINDINGS[i] for i in finding_indices],\n",
    "            })\n",
    "\n",
    "    def _create_image(self, finding_indices):\n",
    "        img = torch.ones(1, self.image_size, self.image_size) * 0.7\n",
    "        # Add noise pattern for each finding\n",
    "        y, x = torch.meshgrid(\n",
    "            torch.arange(self.image_size),\n",
    "            torch.arange(self.image_size), indexing='ij'\n",
    "        )\n",
    "        for idx in finding_indices:\n",
    "            cx = random.randint(20, self.image_size - 20)\n",
    "            cy = random.randint(20, self.image_size - 20)\n",
    "            r = random.randint(8, 20)\n",
    "            mask = ((x - cx)**2 + (y - cy)**2) < r**2\n",
    "            intensity = 0.3 + random.random() * 0.4\n",
    "            img[0][mask] = intensity\n",
    "        img += torch.randn_like(img) * 0.05\n",
    "        return img.clamp(0, 1)\n",
    "\n",
    "    def _generate_report(self, finding_indices):\n",
    "        if not finding_indices:\n",
    "            return \"No acute cardiopulmonary abnormality identified. Normal study.\"\n",
    "        sentences = []\n",
    "        for idx in finding_indices:\n",
    "            template = self.TEMPLATES[self.FINDINGS[idx]]\n",
    "            template = template.replace(\"{location}\", random.choice(self.LOCATIONS))\n",
    "            template = template.replace(\"{side}\", random.choice(self.SIDES))\n",
    "            template = template.replace(\"{size}\", random.choice(self.SIZES))\n",
    "            sentences.append(template)\n",
    "        return \" \".join(sentences)\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        words = text.lower().replace(\",\", \"\").replace(\".\", \"\").split()\n",
    "        return [self.VOCAB.get(w, 0) for w in words]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_data = SyntheticRadiologyDataset(n_samples=600)\n",
    "val_data = SyntheticRadiologyDataset(n_samples=100)\n",
    "test_data = SyntheticRadiologyDataset(n_samples=200)\n",
    "\n",
    "print(f\"Vocabulary size: {len(train_data.VOCAB)}\")\n",
    "print(f\"Training:   {len(train_data)} studies\")\n",
    "print(f\"Validation: {len(val_data)} studies\")\n",
    "print(f\"Test:       {len(test_data)} studies\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding distribution\n",
    "finding_counts = torch.stack([d[\"labels\"] for d in train_data.data]).sum(dim=0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Finding frequency\n",
    "bars = axes[0].barh(range(14), finding_counts.numpy(), color='steelblue')\n",
    "axes[0].set_yticks(range(14))\n",
    "axes[0].set_yticklabels(SyntheticRadiologyDataset.FINDINGS, fontsize=9)\n",
    "axes[0].set_xlabel(\"Count\")\n",
    "axes[0].set_title(\"Finding Frequency in Training Set\")\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Number of findings per study\n",
    "n_findings = [len(d[\"findings\"]) for d in train_data.data]\n",
    "axes[1].hist(n_findings, bins=range(5), color='coral', edgecolor='black', align='left')\n",
    "axes[1].set_xlabel(\"Number of Findings\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].set_title(\"Findings per Study\")\n",
    "axes[1].set_xticks(range(4))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample studies\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(8):\n",
    "    sample = train_data[i]\n",
    "    axes[i].imshow(sample[\"image\"][0].numpy(), cmap='gray')\n",
    "    findings = \", \".join(sample[\"findings\"]) if sample[\"findings\"] else \"Normal\"\n",
    "    axes[i].set_title(f\"Findings: {findings}\", fontsize=8, wrap=True)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Sample Chest X-rays with Findings\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Model"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template-based baseline\n",
    "def template_baseline(labels):\n",
    "    \"\"\"Generate report from labels using templates.\"\"\"\n",
    "    findings = []\n",
    "    for i, label in enumerate(labels):\n",
    "        if label > 0.5:\n",
    "            findings.append(SyntheticRadiologyDataset.FINDINGS[i])\n",
    "    if not findings:\n",
    "        return \"No acute cardiopulmonary abnormality identified.\"\n",
    "    return \"; \".join([f\"{f} detected\" for f in findings]) + \".\"\n",
    "\n",
    "# Evaluate baseline (using ground truth labels as perfect detection)\n",
    "from collections import Counter\n",
    "baseline_bleu_proxy = []\n",
    "for sample in test_data:\n",
    "    pred_report = template_baseline(sample[\"labels\"])\n",
    "    gt_report = sample[\"report_text\"]\n",
    "    pred_words = set(pred_report.lower().split())\n",
    "    gt_words = set(gt_report.lower().split())\n",
    "    overlap = len(pred_words & gt_words) / max(len(gt_words), 1)\n",
    "    baseline_bleu_proxy.append(overlap)\n",
    "\n",
    "print(f\"Baseline word overlap score: {np.mean(baseline_bleu_proxy):.3f}\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadiologyMultimodalModel(nn.Module):\n",
    "    \"\"\"LLaVA-style model for radiology report generation.\"\"\"\n",
    "\n",
    "    def __init__(self, image_size=128, patch_size=16, vision_dim=256,\n",
    "                 llm_dim=128, vocab_size=100, num_labels=14,\n",
    "                 num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        # Vision encoder (frozen)\n",
    "        self.vision_encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, vision_dim, kernel_size=patch_size, stride=patch_size),\n",
    "            nn.Flatten(2),\n",
    "        )\n",
    "\n",
    "        # Projection layer (always trainable)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(vision_dim, llm_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(llm_dim, llm_dim),\n",
    "        )\n",
    "\n",
    "        # Classification head (for finding detection)\n",
    "        self.cls_head = nn.Linear(vision_dim, num_labels)\n",
    "\n",
    "        # Language model\n",
    "        self.text_embed = nn.Embedding(vocab_size, llm_dim)\n",
    "        self.pos_embed = nn.Embedding(512, llm_dim)\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=llm_dim, nhead=num_heads,\n",
    "            dim_feedforward=llm_dim * 4, batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.output_head = nn.Linear(llm_dim, vocab_size)\n",
    "\n",
    "    def set_stage(self, stage):\n",
    "        for p in self.vision_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.projector.parameters():\n",
    "            p.requires_grad = True\n",
    "        for p in self.cls_head.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        llm_trainable = (stage == 2)\n",
    "        for module in [self.text_embed, self.pos_embed, self.decoder, self.output_head]:\n",
    "            for p in module.parameters():\n",
    "                p.requires_grad = llm_trainable\n",
    "\n",
    "        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        total = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"Stage {stage}: {trainable:,}/{total:,} params trainable ({trainable/total:.1%})\")\n",
    "\n",
    "    def forward(self, images, token_ids):\n",
    "        B = images.shape[0]\n",
    "\n",
    "        # Vision\n",
    "        with torch.no_grad():\n",
    "            vis = self.vision_encoder(images)\n",
    "            vis = vis.transpose(1, 2)\n",
    "\n",
    "        # Classification from pooled vision features\n",
    "        vis_pooled = vis.mean(dim=1)\n",
    "        label_logits = self.cls_head(vis_pooled)\n",
    "\n",
    "        # Projection\n",
    "        vis_tokens = self.projector(vis)\n",
    "\n",
    "        # Text\n",
    "        text_tokens = self.text_embed(token_ids)\n",
    "\n",
    "        # Combine\n",
    "        combined = torch.cat([vis_tokens, text_tokens], dim=1)\n",
    "        seq_len = combined.shape[1]\n",
    "        positions = torch.arange(seq_len, device=combined.device).unsqueeze(0)\n",
    "        combined = combined + self.pos_embed(positions)\n",
    "\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(combined.device)\n",
    "        memory = torch.zeros(B, 1, combined.shape[-1], device=combined.device)\n",
    "        output = self.decoder(combined, memory, tgt_mask=mask)\n",
    "\n",
    "        logits = self.output_head(output)\n",
    "        return logits, label_logits\n",
    "\n",
    "\n",
    "model = RadiologyMultimodalModel(vocab_size=len(train_data.VOCAB)).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Two-Stage Training"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, val_data, stage1_epochs=20, stage2_epochs=30):\n",
    "    \"\"\"Two-stage training pipeline.\"\"\"\n",
    "    history = {\"stage1_loss\": [], \"stage2_loss\": [], \"stage2_detect_loss\": []}\n",
    "\n",
    "    # Stage 1: Feature Alignment\n",
    "    model.set_stage(1)\n",
    "    opt = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "\n",
    "    for epoch in range(stage1_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        indices = list(range(len(train_data)))\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        for i in range(0, len(indices), 32):\n",
    "            batch = [train_data[j] for j in indices[i:i+32]]\n",
    "            images = torch.stack([b[\"image\"] for b in batch]).to(device)\n",
    "            labels = torch.stack([b[\"labels\"] for b in batch]).to(device)\n",
    "\n",
    "            # Simple captioning: predict first few report tokens\n",
    "            max_len = min(max(len(b[\"report_tokens\"]) for b in batch), 15)\n",
    "            input_ids = torch.zeros(len(batch), max_len, dtype=torch.long, device=device)\n",
    "            target_ids = torch.full((len(batch), max_len), -100, dtype=torch.long, device=device)\n",
    "\n",
    "            for j, b in enumerate(batch):\n",
    "                tokens = b[\"report_tokens\"][:max_len-1]\n",
    "                input_ids[j, 0] = 1  # <start>\n",
    "                for k, t in enumerate(tokens):\n",
    "                    input_ids[j, k+1] = t\n",
    "                    target_ids[j, k] = t\n",
    "                target_ids[j, len(tokens)] = 2  # <end>\n",
    "\n",
    "            logits, label_logits = model(images, input_ids)\n",
    "            text_logits = logits[:, model.num_patches:, :]\n",
    "            gen_loss = F.cross_entropy(text_logits.reshape(-1, text_logits.shape[-1]),\n",
    "                                       target_ids.reshape(-1), ignore_index=-100)\n",
    "            detect_loss = F.binary_cross_entropy_with_logits(label_logits, labels)\n",
    "            loss = gen_loss + 0.5 * detect_loss\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        history[\"stage1_loss\"].append(epoch_loss / (len(indices) // 32))\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"  Stage 1 Epoch {epoch+1}: loss={history['stage1_loss'][-1]:.4f}\")\n",
    "\n",
    "    # Stage 2: Instruction Tuning\n",
    "    model.set_stage(2)\n",
    "    opt = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-4)\n",
    "\n",
    "    for epoch in range(stage2_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_detect = 0\n",
    "        indices = list(range(len(train_data)))\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        for i in range(0, len(indices), 32):\n",
    "            batch = [train_data[j] for j in indices[i:i+32]]\n",
    "            images = torch.stack([b[\"image\"] for b in batch]).to(device)\n",
    "            labels = torch.stack([b[\"labels\"] for b in batch]).to(device)\n",
    "\n",
    "            max_len = min(max(len(b[\"report_tokens\"]) for b in batch) + 1, 20)\n",
    "            input_ids = torch.zeros(len(batch), max_len, dtype=torch.long, device=device)\n",
    "            target_ids = torch.full((len(batch), max_len), -100, dtype=torch.long, device=device)\n",
    "\n",
    "            for j, b in enumerate(batch):\n",
    "                tokens = b[\"report_tokens\"][:max_len-1]\n",
    "                input_ids[j, 0] = 1\n",
    "                for k, t in enumerate(tokens):\n",
    "                    input_ids[j, k+1] = t\n",
    "                    target_ids[j, k] = t\n",
    "                target_ids[j, len(tokens)] = 2\n",
    "\n",
    "            logits, label_logits = model(images, input_ids)\n",
    "            text_logits = logits[:, model.num_patches:, :]\n",
    "            gen_loss = F.cross_entropy(text_logits.reshape(-1, text_logits.shape[-1]),\n",
    "                                       target_ids.reshape(-1), ignore_index=-100)\n",
    "            detect_loss = F.binary_cross_entropy_with_logits(label_logits, labels)\n",
    "            loss = gen_loss + 0.5 * detect_loss\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            epoch_loss += gen_loss.item()\n",
    "            epoch_detect += detect_loss.item()\n",
    "\n",
    "        n = len(indices) // 32\n",
    "        history[\"stage2_loss\"].append(epoch_loss / n)\n",
    "        history[\"stage2_detect_loss\"].append(epoch_detect / n)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"  Stage 2 Epoch {epoch+1}: gen={history['stage2_loss'][-1]:.4f}, \"\n",
    "                  f\"detect={history['stage2_detect_loss'][-1]:.4f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "history = train_model(model, train_data, val_data)"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(history[\"stage1_loss\"], color='steelblue', linewidth=2)\n",
    "axes[0].set_title(\"Stage 1: Alignment Loss\")\n",
    "axes[0].set_xlabel(\"Epoch\"); axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history[\"stage2_loss\"], color='coral', linewidth=2)\n",
    "axes[1].set_title(\"Stage 2: Report Generation Loss\")\n",
    "axes[1].set_xlabel(\"Epoch\"); axes[1].set_ylabel(\"Loss\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(history[\"stage2_detect_loss\"], color='seagreen', linewidth=2)\n",
    "axes[2].set_title(\"Stage 2: Finding Detection Loss\")\n",
    "axes[2].set_xlabel(\"Epoch\"); axes[2].set_ylabel(\"Loss\")\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Two-Stage Training Curves\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate finding detection\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sample in test_data:\n",
    "        img = sample[\"image\"].unsqueeze(0).to(device)\n",
    "        dummy_ids = torch.ones(1, 1, dtype=torch.long, device=device)\n",
    "        _, label_logits = model(img, dummy_ids)\n",
    "        preds = (torch.sigmoid(label_logits) > 0.5).float().cpu()\n",
    "        all_preds.append(preds[0])\n",
    "        all_labels.append(sample[\"labels\"])\n",
    "\n",
    "preds_tensor = torch.stack(all_preds).numpy()\n",
    "labels_tensor = torch.stack(all_labels).numpy()\n",
    "\n",
    "# Per-finding F1\n",
    "print(\"Per-Finding F1 Scores:\")\n",
    "print(\"-\" * 40)\n",
    "for i, finding in enumerate(SyntheticRadiologyDataset.FINDINGS):\n",
    "    f1 = f1_score(labels_tensor[:, i], preds_tensor[:, i], zero_division=0)\n",
    "    print(f\"  {finding:20s}: {f1:.3f}\")\n",
    "\n",
    "macro_f1 = f1_score(labels_tensor, preds_tensor, average='macro', zero_division=0)\n",
    "micro_f1 = f1_score(labels_tensor, preds_tensor, average='micro', zero_division=0)\n",
    "print(f\"\\nMacro F1: {macro_f1:.3f}\")\n",
    "print(f\"Micro F1: {micro_f1:.3f}\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion analysis for critical findings\n",
    "critical_findings = [\"pneumothorax\", \"fracture\", \"mass\", \"pleural_effusion\"]\n",
    "critical_indices = [SyntheticRadiologyDataset.FINDINGS.index(f) for f in critical_findings]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(critical_findings), figsize=(20, 4))\n",
    "\n",
    "for i, (idx, finding) in enumerate(zip(critical_indices, critical_findings)):\n",
    "    cm = confusion_matrix(labels_tensor[:, idx], preds_tensor[:, idx])\n",
    "    im = axes[i].imshow(cm, cmap='Blues')\n",
    "    axes[i].set_title(finding, fontsize=11, fontweight='bold')\n",
    "    axes[i].set_xlabel(\"Predicted\")\n",
    "    axes[i].set_ylabel(\"True\")\n",
    "    axes[i].set_xticks([0, 1])\n",
    "    axes[i].set_xticklabels([\"Neg\", \"Pos\"])\n",
    "    axes[i].set_yticks([0, 1])\n",
    "    axes[i].set_yticklabels([\"Neg\", \"Pos\"])\n",
    "    for r in range(2):\n",
    "        for c in range(2):\n",
    "            axes[i].text(c, r, str(cm[r, c]), ha='center', va='center',\n",
    "                        fontsize=14, fontweight='bold',\n",
    "                        color='white' if cm[r, c] > cm.max()/2 else 'black')\n",
    "\n",
    "plt.suptitle(\"Confusion Matrices for Critical Findings\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Critical finding sensitivity\n",
    "print(\"\\nCritical Finding Sensitivity (must be > 98% for deployment):\")\n",
    "for idx, finding in zip(critical_indices, critical_findings):\n",
    "    tp = ((preds_tensor[:, idx] == 1) & (labels_tensor[:, idx] == 1)).sum()\n",
    "    fn = ((preds_tensor[:, idx] == 0) & (labels_tensor[:, idx] == 1)).sum()\n",
    "    sensitivity = tp / max(tp + fn, 1)\n",
    "    status = \"PASS\" if sensitivity >= 0.98 else \"FAIL\"\n",
    "    print(f\"  {finding:20s}: {sensitivity:.1%} [{status}]\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deployment Readiness"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference latency benchmark\n",
    "model.eval()\n",
    "latencies = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(50):\n",
    "        img = torch.randn(1, 1, 128, 128).to(device)\n",
    "        ids = torch.ones(1, 1, dtype=torch.long, device=device)\n",
    "        start = time.time()\n",
    "        _ = model(img, ids)\n",
    "        latencies.append((time.time() - start) * 1000)\n",
    "\n",
    "print(f\"Inference Latency:\")\n",
    "print(f\"  P50: {np.percentile(latencies, 50):.1f} ms\")\n",
    "print(f\"  P95: {np.percentile(latencies, 95):.1f} ms\")\n",
    "print(f\"  P99: {np.percentile(latencies, 99):.1f} ms\")\n",
    "print(f\"  Target: < 30,000 ms (30 seconds)\")\n",
    "\n",
    "# Model size\n",
    "param_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "print(f\"\\nModel Size: {param_bytes / 1024 / 1024:.1f} MB\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CASE STUDY SUMMARY: Radiology Report Generation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nCompany: Meridian Diagnostic Intelligence\")\n",
    "print(f\"Industry: Healthcare -- Diagnostic Radiology\")\n",
    "print(f\"\\nApproach: Multimodal Instruction Tuning (LLaVA-style)\")\n",
    "print(f\"  - Two-stage training: alignment + instruction tuning\")\n",
    "print(f\"  - MLP projection bridging vision and language\")\n",
    "print(f\"  - Multi-label detection as auxiliary objective\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  - Macro F1: {macro_f1:.3f}\")\n",
    "print(f\"  - Micro F1: {micro_f1:.3f}\")\n",
    "print(f\"  - Inference latency (P50): {np.percentile(latencies, 50):.1f} ms\")\n",
    "print(f\"\\nKey Takeaway: Multimodal instruction tuning provides a flexible,\")\n",
    "print(f\"end-to-end approach to radiology report generation that can handle\")\n",
    "print(f\"both structured detection and free-text generation in a single model.\")"
   ],
   "id": "cell_22"
  }
 ]
}