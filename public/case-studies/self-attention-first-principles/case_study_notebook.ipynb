{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Self-Attention Case Study \u2014 Implementation Notebook"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical Note Classification with Self-Attention -- Implementation Notebook\n",
    "\n",
    "*Vizuara Case Study: MedScribe Analytics*\n",
    "\n",
    "In this notebook, we implement the self-attention-based clinical note classifier described in the case study. We will build a Transformer encoder from scratch, train it on synthetic clinical notes, and analyze what the attention heads learn."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from collections import Counter\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Synthetic Clinical Notes Dataset\n",
    "\n",
    "We create a synthetic dataset that mimics the structure of real clinical notes with diagnostic codes."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medical vocabulary with semantic groupings\n",
    "VOCAB = ['<PAD>', '<UNK>', '<CLS>']\n",
    "\n",
    "# Symptom/diagnosis words grouped by category\n",
    "CARDIAC_TERMS = ['chest', 'pain', 'cardiac', 'heart', 'failure', 'arrhythmia',\n",
    "                 'palpitations', 'hypertension', 'blood', 'pressure', 'elevated',\n",
    "                 'ecg', 'abnormal', 'murmur', 'edema', 'dyspnea']\n",
    "RESPIRATORY_TERMS = ['cough', 'breath', 'shortness', 'pneumonia', 'lung',\n",
    "                     'respiratory', 'wheezing', 'sputum', 'oxygen', 'saturation',\n",
    "                     'bronchitis', 'pleurisy', 'crackles', 'consolidation']\n",
    "RENAL_TERMS = ['kidney', 'renal', 'creatinine', 'dialysis', 'urine',\n",
    "               'proteinuria', 'gfr', 'nephropathy', 'bladder', 'urinary']\n",
    "DIABETES_TERMS = ['diabetes', 'glucose', 'insulin', 'hba1c', 'hyperglycemia',\n",
    "                  'diabetic', 'neuropathy', 'retinopathy', 'metformin', 'sugar']\n",
    "NEGATION_TERMS = ['no', 'not', 'denies', 'absent', 'negative', 'without',\n",
    "                  'ruled', 'out', 'unlikely', 'excluded']\n",
    "GENERAL_TERMS = ['patient', 'presents', 'with', 'history', 'of', 'the',\n",
    "                 'and', 'is', 'was', 'has', 'reports', 'examination',\n",
    "                 'shows', 'diagnosis', 'treatment', 'assessment', 'plan',\n",
    "                 'follow', 'up', 'prescribed', 'admitted', 'discharged',\n",
    "                 'stable', 'condition', 'chronic', 'acute', 'mild',\n",
    "                 'moderate', 'severe', 'bilateral', 'left', 'right']\n",
    "\n",
    "VOCAB.extend(CARDIAC_TERMS + RESPIRATORY_TERMS + RENAL_TERMS +\n",
    "             DIABETES_TERMS + NEGATION_TERMS + GENERAL_TERMS)\n",
    "VOCAB = list(dict.fromkeys(VOCAB))  # Remove duplicates preserving order\n",
    "word2idx = {w: i for i, w in enumerate(VOCAB)}\n",
    "vocab_size = len(VOCAB)\n",
    "\n",
    "# Diagnostic code labels\n",
    "CODE_NAMES = ['Cardiac', 'Respiratory', 'Renal', 'Diabetes']\n",
    "num_classes = len(CODE_NAMES)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Number of diagnostic codes: {num_classes}\")"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clinical_note(max_len=64):\n",
    "    \"\"\"Generate a synthetic clinical note with associated diagnostic codes.\"\"\"\n",
    "    labels = np.zeros(num_classes, dtype=np.float32)\n",
    "\n",
    "    # Randomly select 1-3 diagnoses\n",
    "    n_diagnoses = random.randint(1, 3)\n",
    "    active_diagnoses = random.sample(range(num_classes), n_diagnoses)\n",
    "\n",
    "    tokens = [word2idx['<CLS>']]  # Start with CLS token\n",
    "\n",
    "    # Generate text for each diagnosis\n",
    "    term_groups = [CARDIAC_TERMS, RESPIRATORY_TERMS, RENAL_TERMS, DIABETES_TERMS]\n",
    "\n",
    "    for diag_idx in active_diagnoses:\n",
    "        # Decide if this diagnosis is negated (20% chance)\n",
    "        is_negated = random.random() < 0.2\n",
    "\n",
    "        if is_negated:\n",
    "            # Add negation phrase\n",
    "            neg_word = random.choice(NEGATION_TERMS[:6])  # 'no', 'not', 'denies', etc.\n",
    "            tokens.append(word2idx.get(neg_word, 1))\n",
    "            # Don't set the label for negated diagnoses\n",
    "        else:\n",
    "            labels[diag_idx] = 1.0\n",
    "\n",
    "        # Add general context\n",
    "        for _ in range(random.randint(2, 4)):\n",
    "            gen_word = random.choice(GENERAL_TERMS)\n",
    "            tokens.append(word2idx.get(gen_word, 1))\n",
    "\n",
    "        # Add diagnosis-specific terms\n",
    "        diag_terms = term_groups[diag_idx]\n",
    "        for _ in range(random.randint(3, 6)):\n",
    "            term = random.choice(diag_terms)\n",
    "            tokens.append(word2idx.get(term, 1))\n",
    "\n",
    "        # Add more general context\n",
    "        for _ in range(random.randint(1, 3)):\n",
    "            gen_word = random.choice(GENERAL_TERMS)\n",
    "            tokens.append(word2idx.get(gen_word, 1))\n",
    "\n",
    "    # Pad or truncate to max_len\n",
    "    if len(tokens) > max_len:\n",
    "        tokens = tokens[:max_len]\n",
    "    else:\n",
    "        tokens = tokens + [0] * (max_len - len(tokens))\n",
    "\n",
    "    return tokens, labels\n",
    "\n",
    "# Generate dataset\n",
    "n_train, n_val, n_test = 2000, 300, 300\n",
    "max_len = 64\n",
    "\n",
    "all_data = [generate_clinical_note(max_len) for _ in range(n_train + n_val + n_test)]\n",
    "all_tokens = torch.tensor([d[0] for d in all_data], dtype=torch.long)\n",
    "all_labels = torch.tensor(np.array([d[1] for d in all_data]), dtype=torch.float32)\n",
    "\n",
    "train_X, val_X, test_X = all_tokens[:n_train], all_tokens[n_train:n_train+n_val], all_tokens[n_train+n_val:]\n",
    "train_y, val_y, test_y = all_labels[:n_train], all_labels[n_train:n_train+n_val], all_labels[n_train+n_val:]\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_X, train_y), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(val_X, val_y), batch_size=32)\n",
    "test_loader = DataLoader(TensorDataset(test_X, test_y), batch_size=32)\n",
    "\n",
    "print(f\"Train: {len(train_X)}, Val: {len(val_X)}, Test: {len(test_X)}\")\n",
    "print(f\"Sequence length: {max_len}\")\n",
    "print(f\"Label distribution (train):\")\n",
    "for i, name in enumerate(CODE_NAMES):\n",
    "    count = train_y[:, i].sum().item()\n",
    "    print(f\"  {name}: {count:.0f} ({count/len(train_y)*100:.1f}%)\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architecture\n",
    "\n",
    "### 2.1 Sinusoidal Positional Encoding"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multi-Head Self-Attention"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, C = x.size()\n",
    "        Q = self.W_q(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn_weights = self.dropout(F.softmax(scores, dim=-1))\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "\n",
    "        context = context.transpose(1, 2).contiguous().view(B, T, self.d_model)\n",
    "        return self.W_o(context), attn_weights"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Transformer Block"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadSelfAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out, attn_weights = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_out))\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout2(ffn_out))\n",
    "        return x, attn_weights"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Clinical Note Classifier"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClinicalNoteClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based multi-label classifier for clinical notes.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model=128, num_heads=4, d_ff=512,\n",
    "                 num_layers=2, num_classes=4, max_len=128, dropout=0.15):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pos_encoding = SinusoidalPositionalEncoding(d_model, max_len)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Create padding mask\n",
    "        pad_mask = (x != 0).unsqueeze(1).unsqueeze(2)  # (B, 1, 1, T)\n",
    "\n",
    "        # Embedding + PE\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Transformer blocks\n",
    "        all_attn = []\n",
    "        for block in self.blocks:\n",
    "            x, attn = block(x, pad_mask)\n",
    "            all_attn.append(attn)\n",
    "\n",
    "        # Mean pooling (excluding padding)\n",
    "        mask_float = (pad_mask.squeeze(1).squeeze(1)).unsqueeze(-1).float()\n",
    "        x_pooled = (x * mask_float).sum(dim=1) / mask_float.sum(dim=1).clamp(min=1)\n",
    "\n",
    "        logits = self.classifier(x_pooled)\n",
    "        return logits, all_attn\n",
    "\n",
    "# Instantiate\n",
    "model = ClinicalNoteClassifier(\n",
    "    vocab_size=vocab_size, d_model=128, num_heads=4,\n",
    "    d_ff=512, num_layers=2, num_classes=num_classes,\n",
    "    max_len=max_len, dropout=0.15\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {total_params:,}\")\n",
    "print(f\"Architecture: {2} layers, {4} heads, d_model={128}\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_losses = []\n",
    "val_f1_scores = []\n",
    "\n",
    "num_epochs = 25\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        logits, _ = model(batch_X)\n",
    "        loss = criterion(logits, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    train_losses.append(epoch_loss / n_batches)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            logits, _ = model(batch_X)\n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_true.append(batch_y.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_true = torch.cat(all_true).numpy()\n",
    "    micro_f1 = f1_score(all_true, all_preds, average='micro', zero_division=0)\n",
    "    val_f1_scores.append(micro_f1)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}: loss={train_losses[-1]:.4f}  val_F1={micro_f1:.4f}\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(train_losses, color='#e74c3c', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('BCE Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(val_f1_scores, color='#2ecc71', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Micro F1 Score', fontsize=12)\n",
    "ax2.set_title('Validation F1 Score', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylim(0, 1.05)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Clinical Note Classifier Training', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_true = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        logits, _ = model(batch_X)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > 0.5).float()\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_true.append(batch_y.cpu())\n",
    "        all_probs.append(probs.cpu())\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_true = torch.cat(all_true).numpy()\n",
    "all_probs = torch.cat(all_probs).numpy()\n",
    "\n",
    "# Per-class metrics\n",
    "print(\"Per-class F1 Scores:\")\n",
    "print(\"-\" * 40)\n",
    "for i, name in enumerate(CODE_NAMES):\n",
    "    f1 = f1_score(all_true[:, i], all_preds[:, i], zero_division=0)\n",
    "    print(f\"  {name:15s}: {f1:.4f}\")\n",
    "\n",
    "micro_f1 = f1_score(all_true, all_preds, average='micro', zero_division=0)\n",
    "macro_f1 = f1_score(all_true, all_preds, average='macro', zero_division=0)\n",
    "print(f\"\\n  Micro F1: {micro_f1:.4f}\")\n",
    "print(f\"  Macro F1: {macro_f1:.4f}\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class F1 bar chart\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "per_class_f1 = [f1_score(all_true[:, i], all_preds[:, i], zero_division=0) for i in range(num_classes)]\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']\n",
    "bars = ax.bar(CODE_NAMES, per_class_f1, color=colors, edgecolor='black', linewidth=1)\n",
    "ax.set_ylabel('F1 Score', fontsize=12)\n",
    "ax.set_title('Per-Class F1 Scores on Test Set', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 1.05)\n",
    "for bar, f1 in zip(bars, per_class_f1):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "            f'{f1:.3f}', ha='center', fontsize=11, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attention Visualization\n",
    "\n",
    "This is the most important section for clinical interpretability. Let us see what the attention heads learn."
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a test sample and visualize attention\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "def visualize_attention(model, tokens, sample_idx=0):\n",
    "    \"\"\"Visualize attention patterns for a single clinical note.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits, all_attn = model(tokens.unsqueeze(0).to(device))\n",
    "\n",
    "    # Get the words (non-padding)\n",
    "    token_list = tokens.tolist()\n",
    "    words = [idx2word.get(t, '<UNK>') for t in token_list if t != 0]\n",
    "    n_words = len(words)\n",
    "\n",
    "    # Get predictions\n",
    "    probs = torch.sigmoid(logits[0]).cpu().numpy()\n",
    "    pred_codes = [CODE_NAMES[i] for i in range(num_classes) if probs[i] > 0.5]\n",
    "\n",
    "    # Plot attention from the last layer\n",
    "    layer_attn = all_attn[-1][0].cpu().numpy()  # (num_heads, seq, seq)\n",
    "    num_heads = layer_attn.shape[0]\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_heads, figsize=(5*num_heads, 5))\n",
    "    for h in range(num_heads):\n",
    "        ax = axes[h]\n",
    "        # Only show non-padding tokens\n",
    "        attn_sub = layer_attn[h, :n_words, :n_words]\n",
    "        im = ax.imshow(attn_sub, cmap='Blues', vmin=0, vmax=attn_sub.max())\n",
    "        ax.set_xticks(range(n_words))\n",
    "        ax.set_xticklabels(words, rotation=90, fontsize=7)\n",
    "        ax.set_yticks(range(n_words))\n",
    "        ax.set_yticklabels(words, fontsize=7)\n",
    "        ax.set_title(f'Head {h+1}', fontsize=11, fontweight='bold')\n",
    "\n",
    "    plt.suptitle(f'Attention Patterns\\nPredicted: {\", \".join(pred_codes) if pred_codes else \"None\"}',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a few samples\n",
    "for i in range(3):\n",
    "    true_codes = [CODE_NAMES[j] for j in range(num_classes) if test_y[i, j] > 0.5]\n",
    "    print(f\"\\n--- Sample {i+1} (True: {', '.join(true_codes)}) ---\")\n",
    "    token_words = [idx2word.get(t, '?') for t in test_X[i].tolist() if t != 0]\n",
    "    print(f\"Text: {' '.join(token_words[:30])}...\")\n",
    "    visualize_attention(model, test_X[i], i)"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ablation Study: Impact of Scaling"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare model with and without sqrt(d_k) scaling\n",
    "# Train two models:\n",
    "# 1. Standard attention with scaling\n",
    "# 2. Attention WITHOUT scaling (remove / math.sqrt(self.d_k))\n",
    "# Compare convergence speed and final accuracy\n",
    "\n",
    "print(\"Ablation Study: Effect of sqrt(d_k) Scaling\")\n",
    "print(\"=\" * 50)\n",
    "print(\"The standard model already includes scaling.\")\n",
    "print(\"To test without scaling, modify the attention scores line:\")\n",
    "print(\"  scores = torch.matmul(Q, K.transpose(-2, -1))  # No scaling\")\n",
    "print(\"Then retrain and compare the validation F1 curves.\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Summary"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CASE STUDY RESULTS: Clinical Note Classification\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nModel: Transformer Encoder ({2} layers, {4} heads)\")\n",
    "print(f\"Parameters: {total_params:,}\")\n",
    "print(f\"Training samples: {n_train}\")\n",
    "print(f\"Test Micro F1: {micro_f1:.4f}\")\n",
    "print(f\"Test Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"\\nPer-class performance:\")\n",
    "for i, name in enumerate(CODE_NAMES):\n",
    "    f1 = f1_score(all_true[:, i], all_preds[:, i], zero_division=0)\n",
    "    print(f\"  {name:15s}: F1 = {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nKey takeaway: Self-attention enables direct modeling of\")\n",
    "print(f\"long-range dependencies and negation patterns in clinical text,\")\n",
    "print(f\"outperforming sequential models on multi-label classification.\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "cell_26"
  }
 ]
}