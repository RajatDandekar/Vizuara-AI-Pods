{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "DQN Case Study -- Implementation Notebook"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Case Study: Automated Game Testing with Deep Q-Networks\n",
    "\n",
    "This notebook implements the core DQN-based game testing pipeline described in the Nexus Interactive case study. We will train DQN agents on procedurally generated dungeon levels and extract QA metrics: completability, difficulty estimation, and exploit detection.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup and Dependencies"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Dungeon Environment\n",
    "\n",
    "We create a simplified dungeon crawler environment with tile-based grid levels."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DungeonLevel:\n",
    "    \"\"\"\n",
    "    A procedurally generated dungeon level for testing.\n",
    "\n",
    "    State: 5-channel 32x32 grid\n",
    "      Channel 0: Terrain (0=floor, 1=wall, 0.5=door, 0.3=trap)\n",
    "      Channel 1: Items (0=empty, 1=key, 0.7=potion, 0.5=gold)\n",
    "      Channel 2: Enemies (0=empty, value=enemy health/10)\n",
    "      Channel 3: Player (1=position, health/10 in surrounding cells)\n",
    "      Channel 4: Explored (0=unexplored, 1=explored)\n",
    "\n",
    "    Actions: 0-3=move(up/down/left/right), 4=attack, 5=use_item, 6=interact, 7=wait\n",
    "    \"\"\"\n",
    "    def __init__(self, size=32, difficulty=5, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.size = size\n",
    "        self.difficulty = difficulty\n",
    "        self.n_actions = 8\n",
    "        self._generate_level()\n",
    "\n",
    "    def _generate_level(self):\n",
    "        \"\"\"Generate a random dungeon level.\"\"\"\n",
    "        self.terrain = np.zeros((self.size, self.size))\n",
    "        # Walls around the border\n",
    "        self.terrain[0, :] = 1.0\n",
    "        self.terrain[-1, :] = 1.0\n",
    "        self.terrain[:, 0] = 1.0\n",
    "        self.terrain[:, -1] = 1.0\n",
    "\n",
    "        # Internal walls (more walls = harder)\n",
    "        n_walls = int(self.difficulty * 8)\n",
    "        for _ in range(n_walls):\n",
    "            x, y = np.random.randint(2, self.size-2, size=2)\n",
    "            length = np.random.randint(2, 6)\n",
    "            if np.random.random() < 0.5:\n",
    "                self.terrain[y, x:min(x+length, self.size-1)] = 1.0\n",
    "            else:\n",
    "                self.terrain[y:min(y+length, self.size-1), x] = 1.0\n",
    "\n",
    "        # Traps (difficulty-dependent)\n",
    "        n_traps = int(self.difficulty * 2)\n",
    "        self.traps = []\n",
    "        for _ in range(n_traps):\n",
    "            x, y = np.random.randint(2, self.size-2, size=2)\n",
    "            if self.terrain[y, x] == 0:\n",
    "                self.terrain[y, x] = 0.3\n",
    "                self.traps.append((x, y))\n",
    "\n",
    "        # Items\n",
    "        self.items = np.zeros((self.size, self.size))\n",
    "        n_items = max(1, 8 - self.difficulty)\n",
    "        for _ in range(n_items):\n",
    "            x, y = np.random.randint(2, self.size-2, size=2)\n",
    "            if self.terrain[y, x] == 0:\n",
    "                self.items[y, x] = np.random.choice([1.0, 0.7, 0.5])\n",
    "\n",
    "        # Enemies\n",
    "        self.enemies = np.zeros((self.size, self.size))\n",
    "        n_enemies = int(self.difficulty * 1.5)\n",
    "        self.enemy_list = []\n",
    "        for _ in range(n_enemies):\n",
    "            x, y = np.random.randint(3, self.size-3, size=2)\n",
    "            if self.terrain[y, x] == 0:\n",
    "                health = min(1.0, 0.2 + self.difficulty * 0.08)\n",
    "                self.enemies[y, x] = health\n",
    "                self.enemy_list.append([x, y, health])\n",
    "\n",
    "        # Player start and goal\n",
    "        self.player_pos = [1, 1]\n",
    "        self.goal_pos = [self.size-2, self.size-2]\n",
    "        # Ensure goal is reachable (clear the goal area)\n",
    "        self.terrain[self.goal_pos[1]-1:self.goal_pos[1]+2,\n",
    "                     self.goal_pos[0]-1:self.goal_pos[0]+2] = 0\n",
    "        self.terrain[1, 1] = 0  # Clear start\n",
    "\n",
    "        self.player_health = 1.0\n",
    "        self.inventory = []\n",
    "        self.explored = np.zeros((self.size, self.size))\n",
    "        self.steps = 0\n",
    "        self.total_reward = 0\n",
    "        self.items_collected = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self._generate_level()\n",
    "        self._update_explored()\n",
    "        return self._get_state()\n",
    "\n",
    "    def _update_explored(self):\n",
    "        \"\"\"Update fog of war around player.\"\"\"\n",
    "        x, y = self.player_pos\n",
    "        for dx in range(-3, 4):\n",
    "            for dy in range(-3, 4):\n",
    "                nx, ny = x+dx, y+dy\n",
    "                if 0 <= nx < self.size and 0 <= ny < self.size:\n",
    "                    self.explored[ny, nx] = 1.0\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"Return 5-channel state tensor.\"\"\"\n",
    "        state = np.zeros((5, self.size, self.size), dtype=np.float32)\n",
    "        state[0] = self.terrain\n",
    "        state[1] = self.items\n",
    "        state[2] = self.enemies\n",
    "        # Player position\n",
    "        px, py = self.player_pos\n",
    "        state[3, py, px] = 1.0\n",
    "        # Encode health in surrounding cells\n",
    "        for dx in range(-1, 2):\n",
    "            for dy in range(-1, 2):\n",
    "                nx, ny = px+dx, py+dy\n",
    "                if 0 <= nx < self.size and 0 <= ny < self.size:\n",
    "                    state[3, ny, nx] = max(state[3, ny, nx], self.player_health * 0.5)\n",
    "        state[4] = self.explored\n",
    "        return torch.tensor(state)\n",
    "\n",
    "    def get_action_mask(self):\n",
    "        \"\"\"Return valid action mask.\"\"\"\n",
    "        mask = torch.ones(self.n_actions, dtype=torch.bool)\n",
    "        px, py = self.player_pos\n",
    "        # Check move validity\n",
    "        moves = [(0, -1), (0, 1), (-1, 0), (1, 0)]\n",
    "        for i, (dx, dy) in enumerate(moves):\n",
    "            nx, ny = px+dx, py+dy\n",
    "            if nx < 0 or nx >= self.size or ny < 0 or ny >= self.size or self.terrain[ny, nx] == 1.0:\n",
    "                mask[i] = False\n",
    "        # Attack only if enemy adjacent\n",
    "        has_adjacent_enemy = False\n",
    "        for dx, dy in moves:\n",
    "            nx, ny = px+dx, py+dy\n",
    "            if 0 <= nx < self.size and 0 <= ny < self.size and self.enemies[ny, nx] > 0:\n",
    "                has_adjacent_enemy = True\n",
    "        mask[4] = has_adjacent_enemy\n",
    "        # Use item only if inventory non-empty\n",
    "        mask[5] = len(self.inventory) > 0\n",
    "        # Interact only if on a special tile\n",
    "        mask[6] = self.items[py, px] > 0\n",
    "        return mask\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Execute action and return (next_state, reward, done).\"\"\"\n",
    "        self.steps += 1\n",
    "        reward = -0.01  # Small step penalty\n",
    "        done = False\n",
    "        px, py = self.player_pos\n",
    "\n",
    "        # Movement\n",
    "        if action < 4:\n",
    "            moves = [(0, -1), (0, 1), (-1, 0), (1, 0)]\n",
    "            dx, dy = moves[action]\n",
    "            nx, ny = px+dx, py+dy\n",
    "            if 0 <= nx < self.size and 0 <= ny < self.size and self.terrain[ny, nx] != 1.0:\n",
    "                # Check for trap\n",
    "                if self.terrain[ny, nx] == 0.3:\n",
    "                    self.player_health -= 0.1\n",
    "                    reward -= 0.05\n",
    "                self.player_pos = [nx, ny]\n",
    "                # Exploration bonus\n",
    "                old_explored = self.explored.sum()\n",
    "                self._update_explored()\n",
    "                new_explored = self.explored.sum()\n",
    "                reward += 0.1 * (new_explored - old_explored) / (self.size * self.size)\n",
    "\n",
    "        # Attack\n",
    "        elif action == 4:\n",
    "            for dx, dy in [(0,-1),(0,1),(-1,0),(1,0)]:\n",
    "                nx, ny = px+dx, py+dy\n",
    "                if 0 <= nx < self.size and 0 <= ny < self.size and self.enemies[ny, nx] > 0:\n",
    "                    self.enemies[ny, nx] -= 0.3\n",
    "                    if self.enemies[ny, nx] <= 0:\n",
    "                        self.enemies[ny, nx] = 0\n",
    "                        reward += 0.2\n",
    "                    break\n",
    "\n",
    "        # Use item\n",
    "        elif action == 5 and self.inventory:\n",
    "            item = self.inventory.pop(0)\n",
    "            if item == 'potion':\n",
    "                self.player_health = min(1.0, self.player_health + 0.3)\n",
    "                reward += 0.1\n",
    "\n",
    "        # Interact (pick up item)\n",
    "        elif action == 6:\n",
    "            px, py = self.player_pos\n",
    "            if self.items[py, px] > 0:\n",
    "                item_type = self.items[py, px]\n",
    "                self.items[py, px] = 0\n",
    "                self.items_collected += 1\n",
    "                if item_type == 1.0:  # Key\n",
    "                    self.inventory.append('key')\n",
    "                    reward += 0.15\n",
    "                elif item_type == 0.7:  # Potion\n",
    "                    self.inventory.append('potion')\n",
    "                    reward += 0.1\n",
    "                elif item_type == 0.5:  # Gold\n",
    "                    reward += 0.3\n",
    "\n",
    "        # Check win condition\n",
    "        if self.player_pos == self.goal_pos:\n",
    "            reward += 1.0\n",
    "            done = True\n",
    "\n",
    "        # Check death\n",
    "        if self.player_health <= 0:\n",
    "            reward -= 0.5\n",
    "            done = True\n",
    "\n",
    "        # Time limit\n",
    "        if self.steps >= 500:\n",
    "            done = True\n",
    "\n",
    "        self.total_reward += reward\n",
    "        return self._get_state(), reward, done\n",
    "\n",
    "# Test the environment\n",
    "env = DungeonLevel(difficulty=3, seed=42)\n",
    "state = env.reset()\n",
    "print(f\"State shape: {state.shape}\")\n",
    "print(f\"Actions: {env.n_actions}\")\n",
    "\n",
    "# Visualize the level\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "channel_names = ['Terrain', 'Items', 'Enemies', 'Player', 'Explored']\n",
    "for i, (ax, name) in enumerate(zip(axes, channel_names)):\n",
    "    ax.imshow(state[i].numpy(), cmap='viridis')\n",
    "    ax.set_title(name)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Dungeon Level State Channels', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Game Testing DQN"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameTestDQN(nn.Module):\n",
    "    \"\"\"DQN adapted for dungeon game testing (5-channel 32x32 input).\"\"\"\n",
    "    def __init__(self, n_actions=8):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(5, 32, kernel_size=5, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Calculate conv output size\n",
    "        test = torch.zeros(1, 5, 32, 32)\n",
    "        conv_out = self.conv(test)\n",
    "        self.conv_size = conv_out.view(1, -1).size(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.conv_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, action_mask=None):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        q_values = self.fc(x)\n",
    "        if action_mask is not None:\n",
    "            q_values = q_values.masked_fill(~action_mask, float('-inf'))\n",
    "        return q_values\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done, mask):\n",
    "        self.buffer.append((state, action, reward, next_state, done, mask))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        s, a, r, ns, d, m = zip(*batch)\n",
    "        return (torch.stack(s), torch.tensor(a, dtype=torch.long),\n",
    "                torch.tensor(r, dtype=torch.float32), torch.stack(ns),\n",
    "                torch.tensor(d, dtype=torch.bool), torch.stack(m))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# Test the network\n",
    "net = GameTestDQN().to(device)\n",
    "test_state = torch.randn(1, 5, 32, 32).to(device)\n",
    "test_mask = torch.ones(1, 8, dtype=torch.bool).to(device)\n",
    "q = net(test_state, test_mask)\n",
    "print(f\"Network output: {q.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in net.parameters()):,}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training Pipeline with QA Metrics"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn_on_level(difficulty, seed=None, n_episodes=300,\n",
    "                       buffer_size=50000, batch_size=64, gamma=0.99,\n",
    "                       lr=5e-4, target_update=500, eps_decay=10000):\n",
    "    \"\"\"\n",
    "    Train a Double DQN agent on a single level and extract QA metrics.\n",
    "\n",
    "    Returns:\n",
    "        metrics: dict with completability, difficulty, exploit scores\n",
    "        episode_rewards: list of rewards per episode\n",
    "        episode_completions: list of booleans\n",
    "    \"\"\"\n",
    "    env = DungeonLevel(difficulty=difficulty, seed=seed)\n",
    "\n",
    "    online = GameTestDQN().to(device)\n",
    "    target = GameTestDQN().to(device)\n",
    "    target.load_state_dict(online.state_dict())\n",
    "    target.eval()\n",
    "\n",
    "    optimizer = optim.Adam(online.parameters(), lr=lr)\n",
    "    buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "    episode_rewards = []\n",
    "    episode_completions = []\n",
    "    episode_lengths = []\n",
    "    step_count = 0\n",
    "    first_completion = None\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        ep_reward = 0\n",
    "        completed = False\n",
    "\n",
    "        for t in range(500):\n",
    "            # Epsilon-greedy with action masking\n",
    "            epsilon = max(0.05, 1.0 - step_count / eps_decay)\n",
    "            mask = env.get_action_mask()\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                valid_actions = torch.where(mask)[0]\n",
    "                action = valid_actions[random.randint(0, len(valid_actions)-1)].item()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q = online(state.unsqueeze(0).to(device), mask.unsqueeze(0).to(device))\n",
    "                    action = q.argmax(1).item()\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_mask = env.get_action_mask()\n",
    "            buffer.push(state, action, reward, next_state, done, next_mask)\n",
    "\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "            if env.player_pos == env.goal_pos:\n",
    "                completed = True\n",
    "\n",
    "            # Training (Double DQN)\n",
    "            if len(buffer) >= batch_size:\n",
    "                s, a, r, ns, d, m = buffer.sample(batch_size)\n",
    "                s, a, r, ns, d, m = (s.to(device), a.to(device), r.to(device),\n",
    "                                      ns.to(device), d.to(device), m.to(device))\n",
    "\n",
    "                q_vals = online(s).gather(1, a.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Double DQN: select with online, evaluate with target\n",
    "                    best_actions = online(ns, m).argmax(1)\n",
    "                    next_q = target(ns).gather(1, best_actions.unsqueeze(1)).squeeze(1)\n",
    "                    targets = r + gamma * next_q * (~d).float()\n",
    "\n",
    "                loss = F.smooth_l1_loss(q_vals, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(online.parameters(), 10.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            if step_count % target_update == 0:\n",
    "                target.load_state_dict(online.state_dict())\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(ep_reward)\n",
    "        episode_completions.append(completed)\n",
    "        episode_lengths.append(t + 1)\n",
    "\n",
    "        if completed and first_completion is None:\n",
    "            first_completion = episode\n",
    "\n",
    "    # Compute QA metrics\n",
    "    last_100 = episode_completions[-100:] if len(episode_completions) >= 100 else episode_completions\n",
    "    completion_rate = sum(last_100) / len(last_100)\n",
    "\n",
    "    difficulty_score = (first_completion / n_episodes * 10) if first_completion is not None else 10.0\n",
    "    difficulty_score = min(10.0, difficulty_score)\n",
    "\n",
    "    rewards_arr = np.array(episode_rewards)\n",
    "    if rewards_arr.std() > 0:\n",
    "        exploit_score = (rewards_arr.max() - rewards_arr.mean()) / rewards_arr.std()\n",
    "    else:\n",
    "        exploit_score = 0.0\n",
    "\n",
    "    metrics = {\n",
    "        'completion_rate': completion_rate,\n",
    "        'difficulty_score': difficulty_score,\n",
    "        'exploit_score': exploit_score,\n",
    "        'first_completion': first_completion,\n",
    "        'avg_reward': np.mean(episode_rewards[-50:]),\n",
    "        'avg_length': np.mean(episode_lengths[-50:]),\n",
    "    }\n",
    "\n",
    "    return metrics, episode_rewards, episode_completions\n",
    "\n",
    "print(\"Training pipeline ready.\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Running the QA Suite"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test across multiple difficulty levels\n",
    "difficulties = [1, 3, 5, 7, 9]\n",
    "all_metrics = {}\n",
    "all_rewards = {}\n",
    "\n",
    "print(\"Running QA suite across difficulty levels...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for diff in difficulties:\n",
    "    start = time.time()\n",
    "    metrics, rewards, completions = train_dqn_on_level(\n",
    "        difficulty=diff, seed=42+diff, n_episodes=200\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    all_metrics[diff] = metrics\n",
    "    all_rewards[diff] = rewards\n",
    "\n",
    "    status = \"PASS\" if metrics['completion_rate'] > 0.1 else \"FAIL\"\n",
    "    print(f\"Difficulty {diff:2d} | CR: {metrics['completion_rate']:.2f} | \"\n",
    "          f\"DS: {metrics['difficulty_score']:.1f}/10 | \"\n",
    "          f\"ES: {metrics['exploit_score']:.2f} | \"\n",
    "          f\"First complete: ep {metrics['first_completion']} | \"\n",
    "          f\"Status: {status} | {elapsed:.0f}s\")\n",
    "\n",
    "print(\"=\" * 70)"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Visualizing QA Results"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Training curves by difficulty\n",
    "ax = axes[0, 0]\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.1, 0.9, len(difficulties)))\n",
    "for diff, color in zip(difficulties, colors):\n",
    "    rewards = all_rewards[diff]\n",
    "    window = 15\n",
    "    if len(rewards) >= window:\n",
    "        smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "        ax.plot(range(window-1, len(rewards)), smoothed, label=f'Diff={diff}',\n",
    "                color=color, linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Training Curves by Difficulty')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Completion rate\n",
    "ax = axes[0, 1]\n",
    "crs = [all_metrics[d]['completion_rate'] for d in difficulties]\n",
    "bars = ax.bar(range(len(difficulties)), crs, color=['#2ecc71' if cr > 0.5 else '#f39c12' if cr > 0.1 else '#e74c3c' for cr in crs])\n",
    "ax.set_xticks(range(len(difficulties)))\n",
    "ax.set_xticklabels([f'Diff {d}' for d in difficulties])\n",
    "ax.set_ylabel('Completion Rate')\n",
    "ax.set_title('Level Completability')\n",
    "ax.axhline(y=0.1, color='red', linestyle='--', alpha=0.5, label='Min threshold')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Difficulty scores\n",
    "ax = axes[0, 2]\n",
    "ds_values = [all_metrics[d]['difficulty_score'] for d in difficulties]\n",
    "ax.plot(difficulties, ds_values, 'o-', color='#3498db', linewidth=2, markersize=10)\n",
    "ax.plot(difficulties, difficulties, '--', color='gray', alpha=0.5, label='Ideal (linear)')\n",
    "ax.set_xlabel('Target Difficulty')\n",
    "ax.set_ylabel('DQN Difficulty Score')\n",
    "ax.set_title('Difficulty Calibration')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Exploit scores\n",
    "ax = axes[1, 0]\n",
    "es_values = [all_metrics[d]['exploit_score'] for d in difficulties]\n",
    "bars = ax.bar(range(len(difficulties)), es_values,\n",
    "              color=['#e74c3c' if es > 5 else '#2ecc71' for es in es_values])\n",
    "ax.set_xticks(range(len(difficulties)))\n",
    "ax.set_xticklabels([f'Diff {d}' for d in difficulties])\n",
    "ax.set_ylabel('Exploit Score')\n",
    "ax.set_title('Exploit Detection')\n",
    "ax.axhline(y=5.0, color='red', linestyle='--', label='Alert threshold')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 5. Average episode length\n",
    "ax = axes[1, 1]\n",
    "lengths = [all_metrics[d]['avg_length'] for d in difficulties]\n",
    "ax.bar(range(len(difficulties)), lengths, color='#9b59b6', alpha=0.7)\n",
    "ax.set_xticks(range(len(difficulties)))\n",
    "ax.set_xticklabels([f'Diff {d}' for d in difficulties])\n",
    "ax.set_ylabel('Avg Episode Length')\n",
    "ax.set_title('Agent Behavior Complexity')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 6. Summary dashboard\n",
    "ax = axes[1, 2]\n",
    "ax.axis('off')\n",
    "summary_text = \"QA SUMMARY\\n\" + \"=\" * 30 + \"\\n\\n\"\n",
    "for d in difficulties:\n",
    "    m = all_metrics[d]\n",
    "    status = \"PASS\" if m['completion_rate'] > 0.1 else \"FAIL\"\n",
    "    flag = \" [!]\" if m['exploit_score'] > 5 else \"\"\n",
    "    summary_text += f\"Level (Diff {d}): {status}{flag}\\n\"\n",
    "    summary_text += f\"  CR={m['completion_rate']:.0%}  DS={m['difficulty_score']:.1f}  ES={m['exploit_score']:.1f}\\n\\n\"\n",
    "ax.text(0.05, 0.95, summary_text, transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.suptitle('DQN Game Testing QA Dashboard', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Summary"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CASE STUDY RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "We implemented a DQN-based automated game testing pipeline that:\n",
    "\n",
    "1. COMPLETABILITY: Trains a DQN agent per level. If the agent\n",
    "   cannot learn to complete the level (CR < 10%), it flags a\n",
    "   potential soft-lock or impossible state.\n",
    "\n",
    "2. DIFFICULTY: The number of episodes to first completion\n",
    "   provides a difficulty score (0-10) that correlates with\n",
    "   human-perceived difficulty.\n",
    "\n",
    "3. EXPLOIT DETECTION: Unusually high reward episodes\n",
    "   (>5 sigma) indicate potential reward exploits.\n",
    "\n",
    "Key Implementation Details:\n",
    "  - Double DQN to prevent overestimation (critical for accuracy)\n",
    "  - Action masking to handle invalid moves\n",
    "  - Reward shaping for faster convergence\n",
    "  - 5-channel grid state representation\n",
    "\n",
    "Business Impact:\n",
    "  - QA cycle: 8-10 days -> 3.5 days (65% reduction)\n",
    "  - Post-release bugs: 3.2 -> 0.8 per update (75% reduction)\n",
    "  - Enabled weekly releases (23% engagement increase)\n",
    "  - ROI: 29x return on infrastructure investment\n",
    "\"\"\")"
   ],
   "id": "cell_14"
  }
 ]
}