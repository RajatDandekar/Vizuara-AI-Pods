{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Synthetic Driving Scenario Generation with Video Diffusion â€” Implementation Notebook"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Synthetic Driving Scenario Generation with Video Diffusion\n",
    "## Implementation Notebook -- Meridian Autonomy\n",
    "\n",
    "*You are an ML engineer at Meridian Autonomy, an autonomous trucking startup. Your perception models fail on rare driving scenarios (dust storms, tire debris, nighttime construction). Real data for these edge cases costs USD 18,000 per scenario-hour. Your task: build a video diffusion model that generates realistic dashcam clips conditioned on driving conditions, so the perception team can train on synthetic data instead.*\n",
    "\n",
    "*This notebook uses real dashcam video from the KITTI dataset -- the industry-standard benchmark for autonomous driving perception. You will download actual driving sequences, build a factorized video diffusion model, train it on real road footage, and generate synthetic driving clips.*\n",
    "\n",
    "*Estimated time: 75-90 minutes on a T4 GPU*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://course-creator-brown.vercel.app/courses/diffusion-models-video-generation/practice/0/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import os\n",
    "import zipfile\n",
    "import io\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition: Real Dashcam Video from KITTI\n",
    "\n",
    "The KITTI dataset contains real dashcam video captured from a car driving through Karlsruhe, Germany. Each sequence has color camera frames at 10 fps -- the same framerate Meridian's trucks use. We download several sequences spanning city streets, residential areas, and highways.\n",
    "\n",
    "**Why KITTI?** It is the most widely used autonomous driving benchmark in the research community. Using it connects your work to real AV perception pipelines. The sequences contain real vehicles, pedestrians, cyclists, lane markings, traffic signs, and diverse road conditions -- exactly what Meridian's synthetic data must replicate.\n",
    "\n",
    "### Download KITTI Raw Sequences\n",
    "\n",
    "We download 6 driving sequences directly from the KITTI servers. Each zip contains a folder of PNG frames from the front-facing color camera. Total download is approximately 500 MB."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KITTI_BASE = \"https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data\"\n",
    "\n",
    "# 6 sequences with diverse driving conditions\n",
    "SEQUENCES = {\n",
    "    # City driving with traffic, pedestrians, parked vehicles\n",
    "    \"2011_09_26_drive_0005\": {\"date\": \"2011_09_26\", \"desc\": \"city - moderate traffic\"},\n",
    "    \"2011_09_26_drive_0014\": {\"date\": \"2011_09_26\", \"desc\": \"city - heavy traffic, turns\"},\n",
    "    \"2011_09_26_drive_0019\": {\"date\": \"2011_09_26\", \"desc\": \"city - traffic lights, intersections\"},\n",
    "    # Residential streets with parked cars, trees, quieter roads\n",
    "    \"2011_09_26_drive_0001\": {\"date\": \"2011_09_26\", \"desc\": \"residential - parked cars, trees\"},\n",
    "    \"2011_09_26_drive_0009\": {\"date\": \"2011_09_26\", \"desc\": \"residential - narrow streets\"},\n",
    "    # Highway/road driving with fewer obstacles, higher speed\n",
    "    \"2011_09_26_drive_0015\": {\"date\": \"2011_09_26\", \"desc\": \"road - open road, few vehicles\"},\n",
    "}\n",
    "\n",
    "DATA_DIR = Path(\"/content/kitti_raw\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "for seq_name, info in SEQUENCES.items():\n",
    "    seq_dir = DATA_DIR / info[\"date\"] / f\"{seq_name}_sync\" / \"image_02\" / \"data\"\n",
    "    if seq_dir.exists() and len(list(seq_dir.glob(\"*.png\"))) > 10:\n",
    "        print(f\"  {seq_name} already downloaded ({info['desc']})\")\n",
    "        continue\n",
    "\n",
    "    url = f\"{KITTI_BASE}/{seq_name}/{seq_name}_sync.zip\"\n",
    "    print(f\"  Downloading {seq_name} ({info['desc']})...\")\n",
    "\n",
    "    try:\n",
    "        import urllib.request\n",
    "        urllib.request.urlretrieve(url, f\"/content/{seq_name}.zip\")\n",
    "        with zipfile.ZipFile(f\"/content/{seq_name}.zip\", 'r') as z:\n",
    "            z.extractall(str(DATA_DIR))\n",
    "        os.remove(f\"/content/{seq_name}.zip\")\n",
    "        n_frames = len(list(seq_dir.glob(\"*.png\")))\n",
    "        print(f\"    -> {n_frames} frames extracted\")\n",
    "    except Exception as e:\n",
    "        print(f\"    -> Download failed: {e}\")\n",
    "        print(f\"       Visit https://www.cvlibs.net/datasets/kitti/raw_data.php\")\n",
    "        print(f\"       Download '{seq_name}_sync.zip' and upload to /content/kitti_raw/\")\n",
    "\n",
    "print(\"\\nDone. If any downloads failed, see instructions above.\")"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess: Extract Video Clips\n",
    "\n",
    "We extract 8-frame clips from each sequence. Each clip is a short dashcam video at 10 fps (0.8 seconds of driving). We resize to 64x64 for training efficiency -- small enough to train on a T4 GPU, but large enough to preserve road structure, vehicles, and lane markings.\n",
    "\n",
    "We also assign each clip a **scene label** based on which sequence it came from:\n",
    "- Label 0-2: City driving (traffic, intersections, pedestrians)\n",
    "- Label 3-4: Residential (parked cars, trees, narrow streets)\n",
    "- Label 5: Open road (highway-like, fewer obstacles)\n",
    "\n",
    "These labels serve as the conditioning signal for our diffusion model -- analogous to Meridian's text prompts like \"dust storm\" or \"nighttime construction.\""
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_LENGTH = 8       # frames per clip\n",
    "CLIP_STRIDE = 4       # stride between clips (overlapping extraction)\n",
    "FRAME_SIZE = 64       # resize to 64x64\n",
    "N_CLASSES = 6         # number of scene condition labels\n",
    "\n",
    "# Map sequence index to class label\n",
    "SEQ_NAMES = list(SEQUENCES.keys())\n",
    "\n",
    "def extract_clips_from_sequence(seq_name, seq_info, label):\n",
    "    \"\"\"Extract 8-frame clips from a single KITTI sequence.\"\"\"\n",
    "    frame_dir = DATA_DIR / seq_info[\"date\"] / f\"{seq_name}_sync\" / \"image_02\" / \"data\"\n",
    "    if not frame_dir.exists():\n",
    "        print(f\"  Skipping {seq_name}: not found\")\n",
    "        return [], []\n",
    "\n",
    "    # Load all frames from this sequence\n",
    "    frame_paths = sorted(frame_dir.glob(\"*.png\"))\n",
    "    frames = []\n",
    "    for fp in frame_paths:\n",
    "        img = Image.open(fp).convert('RGB')\n",
    "        img = img.resize((FRAME_SIZE, FRAME_SIZE), Image.LANCZOS)\n",
    "        arr = np.array(img, dtype=np.float32) / 255.0  # (H, W, 3)\n",
    "        frames.append(arr)\n",
    "    frames = np.stack(frames)  # (N_frames, H, W, 3)\n",
    "\n",
    "    # Extract overlapping clips\n",
    "    clips = []\n",
    "    labels = []\n",
    "    for start in range(0, len(frames) - CLIP_LENGTH, CLIP_STRIDE):\n",
    "        clip = frames[start:start + CLIP_LENGTH]  # (8, 64, 64, 3)\n",
    "        clips.append(clip)\n",
    "        labels.append(label)\n",
    "\n",
    "    return clips, labels\n",
    "\n",
    "\n",
    "all_clips = []\n",
    "all_labels = []\n",
    "\n",
    "for idx, (seq_name, seq_info) in enumerate(SEQUENCES.items()):\n",
    "    clips, labels = extract_clips_from_sequence(seq_name, seq_info, label=idx)\n",
    "    all_clips.extend(clips)\n",
    "    all_labels.extend(labels)\n",
    "    print(f\"  {seq_name} ({seq_info['desc']}): {len(clips)} clips, label={idx}\")\n",
    "\n",
    "all_clips = np.stack(all_clips)    # (N, 8, 64, 64, 3)\n",
    "all_labels = np.array(all_labels)  # (N,)\n",
    "\n",
    "# Convert to torch: (N, T, C, H, W)\n",
    "videos_tensor = torch.tensor(all_clips).permute(0, 1, 4, 2, 3).float()\n",
    "labels_tensor = torch.tensor(all_labels).long()\n",
    "\n",
    "print(f\"\\nTotal clips: {len(videos_tensor)}\")\n",
    "print(f\"Tensor shape: {videos_tensor.shape}\")\n",
    "print(f\"Labels: {dict(zip(*np.unique(all_labels, return_counts=True)))}\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Real Dashcam Clips\n",
    "\n",
    "Let us look at actual clips from each driving condition. Each row shows 8 consecutive frames (0.8 seconds of driving). Notice how the scene changes: vehicles move, road geometry shifts as the car turns, and the visual context differs between city, residential, and open road."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(N_CLASSES, 8, figsize=(20, 15))\n",
    "fig.suptitle(\"Real KITTI Dashcam Clips (each row = 1 clip, columns = consecutive frames)\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "conditions = [\"City: moderate traffic\", \"City: heavy traffic\", \"City: intersections\",\n",
    "              \"Residential: parked cars\", \"Residential: narrow\", \"Open road\"]\n",
    "\n",
    "for cls in range(N_CLASSES):\n",
    "    mask = all_labels == cls\n",
    "    if not mask.any():\n",
    "        continue\n",
    "    idx = np.where(mask)[0][len(np.where(mask)[0]) // 2]  # pick middle clip\n",
    "    clip = videos_tensor[idx]  # (T, C, H, W)\n",
    "    for t in range(8):\n",
    "        frame = clip[t].permute(1, 2, 0).numpy()  # (H, W, 3)\n",
    "        axes[cls, t].imshow(frame)\n",
    "        axes[cls, t].axis('off')\n",
    "        if t == 0:\n",
    "            axes[cls, t].set_ylabel(conditions[cls], fontsize=9, rotation=0,\n",
    "                                     labelpad=100, va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "Before building the model, we analyze the temporal dynamics and condition distributions in our real driving data. This is directly relevant to Meridian: understanding how motion and appearance vary across conditions tells us what the generative model must learn.\n",
    "\n",
    "### TODO 1: Analyze Temporal Dynamics Across Driving Conditions\n",
    "\n",
    "Compute temporal statistics per driving condition. This tells us how much motion varies between city driving (lots of stop-and-go, turns) vs. open road (steady motion). The diffusion model must capture these differences."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_temporal_dynamics(videos: torch.Tensor, labels: torch.Tensor) -> dict:\n",
    "    \"\"\"\n",
    "    Compute per-condition temporal statistics for driving video clips.\n",
    "\n",
    "    Args:\n",
    "        videos: (N, T, C, H, W) real dashcam video clips in [0, 1]\n",
    "        labels: (N,) integer class labels for each clip\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with keys:\n",
    "            'per_condition_motion': dict mapping label (int) to mean optical flow\n",
    "                magnitude (float). Optical flow proxy: mean absolute pixel\n",
    "                difference between consecutive frames.\n",
    "            'per_condition_brightness': dict mapping label (int) to mean pixel\n",
    "                brightness (float).\n",
    "            'class_counts': dict mapping label (int) to count (int).\n",
    "\n",
    "    Steps:\n",
    "        1. For each unique label in labels:\n",
    "           a. Select all clips with that label\n",
    "           b. Compute frame-to-frame differences: |v[:, t+1] - v[:, t]|\n",
    "              Average across all pixels, frames, and clips for that label.\n",
    "              This gives mean_motion for that condition.\n",
    "           c. Compute mean brightness: average of all pixel values for that label\n",
    "        2. Compute class counts\n",
    "        3. Return the results\n",
    "\n",
    "    Hints:\n",
    "        - (videos[mask][:, 1:] - videos[mask][:, :-1]).abs().mean() for motion\n",
    "        - videos[mask].mean() for brightness\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    # ==============================\n",
    "    return {\n",
    "        'per_condition_motion': {},\n",
    "        'per_condition_brightness': {},\n",
    "        'class_counts': {},\n",
    "    }"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "stats = analyze_temporal_dynamics(videos_tensor, labels_tensor)\n",
    "assert len(stats['per_condition_motion']) > 0, \"Should have motion stats\"\n",
    "assert len(stats['class_counts']) > 0, \"Should have class counts\"\n",
    "\n",
    "conditions = [\"City: moderate\", \"City: heavy\", \"City: intersections\",\n",
    "              \"Residential: parked\", \"Residential: narrow\", \"Open road\"]\n",
    "\n",
    "print(\"Driving Condition Analysis:\")\n",
    "print(f\"{'Condition':<25} {'Clips':>6} {'Motion':>8} {'Brightness':>10}\")\n",
    "print(\"-\" * 55)\n",
    "for cls in sorted(stats['class_counts'].keys()):\n",
    "    print(f\"{conditions[cls]:<25} {stats['class_counts'][cls]:>6} \"\n",
    "          f\"{stats['per_condition_motion'].get(cls, 0):>8.4f} \"\n",
    "          f\"{stats['per_condition_brightness'].get(cls, 0):>10.4f}\")\n",
    "print(\"\\nTODO 1 passed.\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the condition distribution (this is the long-tail problem!)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Class distribution\n",
    "counts = [stats['class_counts'].get(i, 0) for i in range(N_CLASSES)]\n",
    "colors = ['#e74c3c', '#e67e22', '#f1c40f', '#2ecc71', '#3498db', '#9b59b6']\n",
    "ax1.barh(range(N_CLASSES), counts, color=colors)\n",
    "ax1.set_yticks(range(N_CLASSES))\n",
    "ax1.set_yticklabels(conditions, fontsize=9)\n",
    "ax1.set_xlabel(\"Number of Clips\")\n",
    "ax1.set_title(\"Distribution of Driving Conditions\")\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Motion per condition\n",
    "motions = [stats['per_condition_motion'].get(i, 0) for i in range(N_CLASSES)]\n",
    "ax2.barh(range(N_CLASSES), motions, color=colors)\n",
    "ax2.set_yticks(range(N_CLASSES))\n",
    "ax2.set_yticklabels(conditions, fontsize=9)\n",
    "ax2.set_xlabel(\"Mean Frame-to-Frame Difference\")\n",
    "ax2.set_title(\"Temporal Motion by Condition\")\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Think about this:** In Meridian's real system, the rare scenarios (dust storms, nighttime construction) have far fewer training clips than normal highway driving. How does this class imbalance affect training a conditional diffusion model? What techniques could mitigate it?"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline: Why Frame-Independent Generation Fails\n",
    "\n",
    "Before building the full video model, let us demonstrate the core problem: generating each frame independently produces temporally incoherent video. This is exactly why Meridian cannot simply use an image diffusion model to generate driving scenarios.\n",
    "\n",
    "### Build a Simple Image Diffusion Model"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleImageUNet(nn.Module):\n",
    "    \"\"\"Minimal 2D U-Net for single-frame diffusion (baseline).\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=3, dim=64, time_dim=128):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, time_dim), nn.SiLU(), nn.Linear(time_dim, dim)\n",
    "        )\n",
    "        self.enc1 = nn.Sequential(nn.Conv2d(in_channels, dim, 3, padding=1), nn.SiLU())\n",
    "        self.enc2 = nn.Sequential(nn.Conv2d(dim, dim*2, 3, stride=2, padding=1), nn.SiLU())\n",
    "        self.bot = nn.Sequential(nn.Conv2d(dim*2, dim*2, 3, padding=1), nn.SiLU())\n",
    "        self.dec2 = nn.Sequential(nn.ConvTranspose2d(dim*4, dim, 4, stride=2, padding=1), nn.SiLU())\n",
    "        self.dec1 = nn.Conv2d(dim*2, in_channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_mlp(t.float().unsqueeze(-1))\n",
    "        h1 = self.enc1(x) + t_emb[:, :, None, None]\n",
    "        h2 = self.enc2(h1)\n",
    "        h = self.bot(h2)\n",
    "        h = self.dec2(torch.cat([h, h2], dim=1))\n",
    "        return self.dec1(torch.cat([h, h1], dim=1))\n",
    "\n",
    "\n",
    "def cosine_noise_schedule(timesteps=200):\n",
    "    \"\"\"Cosine noise schedule (Nichol & Dhariwal, 2021).\"\"\"\n",
    "    s = 0.008\n",
    "    steps = torch.arange(timesteps + 1, dtype=torch.float64)\n",
    "    f = torch.cos((steps / timesteps + s) / (1 + s) * (torch.pi / 2)) ** 2\n",
    "    alphas_cumprod = (f / f[0])[:timesteps]\n",
    "    betas = 1 - alphas_cumprod[1:] / alphas_cumprod[:-1]\n",
    "    betas = torch.clamp(betas, 0.0001, 0.9999)\n",
    "    alphas = 1 - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "    return {\n",
    "        'betas': betas.float(),\n",
    "        'alphas': alphas.float(),\n",
    "        'alphas_cumprod': alphas_cumprod.float(),\n",
    "    }\n",
    "\n",
    "\n",
    "noise_schedule = cosine_noise_schedule(200)"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on Individual Dashcam Frames"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = SimpleImageUNet(in_channels=3).to(device)\n",
    "optimizer = optim.Adam(image_model.parameters(), lr=2e-4)\n",
    "ac = noise_schedule['alphas_cumprod'].to(device)\n",
    "\n",
    "# Flatten all video clips into individual frames\n",
    "all_frames = videos_tensor.reshape(-1, 3, FRAME_SIZE, FRAME_SIZE)  # (N*T, 3, H, W)\n",
    "frame_loader = DataLoader(all_frames, batch_size=64, shuffle=True, drop_last=True)\n",
    "\n",
    "print(f\"Training image baseline on {len(all_frames)} individual dashcam frames...\")\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for batch in frame_loader:\n",
    "        batch = batch.to(device)\n",
    "        t = torch.randint(0, 199, (batch.shape[0],), device=device)\n",
    "        noise = torch.randn_like(batch)\n",
    "        ab = ac[t].reshape(-1, 1, 1, 1)\n",
    "        noisy = torch.sqrt(ab) * batch + torch.sqrt(1 - ab) * noise\n",
    "        loss = F.mse_loss(image_model(noisy, t), noise)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"  Epoch {epoch+1}: Loss = {total_loss / len(frame_loader):.4f}\")\n",
    "\n",
    "print(\"Image baseline trained.\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Generate Videos Frame-by-Frame and Measure Coherence\n",
    "\n",
    "Generate 8-frame driving clips by sampling each frame independently from the image model. Then measure how temporally incoherent these videos are compared to real KITTI clips."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_independent_video(model, noise_schedule, n_frames=8, size=64):\n",
    "    \"\"\"\n",
    "    Generate a video by sampling each frame independently from an image model.\n",
    "\n",
    "    Args:\n",
    "        model: Pretrained 2D U-Net for single frames\n",
    "        noise_schedule: Dict with 'betas', 'alphas', 'alphas_cumprod'\n",
    "        n_frames: Number of frames to generate\n",
    "        size: Frame height/width\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (n_frames, 3, size, size) in [0, 1]\n",
    "\n",
    "    Steps:\n",
    "        1. For each of n_frames:\n",
    "           a. Start with pure noise x_T ~ N(0, I) of shape (1, 3, H, W)\n",
    "           b. Denoise via the full DDPM reverse loop (t = T-1 down to 0):\n",
    "              eps = model(x_t, t_tensor)\n",
    "              mean = (1/sqrt(alpha_t)) * (x_t - beta_t/sqrt(1 - alpha_bar_t) * eps)\n",
    "              if t > 0: x_{t-1} = mean + sqrt(beta_t) * noise\n",
    "              else: x_{t-1} = mean\n",
    "        2. Stack all frames, clamp to [0, 1]\n",
    "\n",
    "    Hints:\n",
    "        - Move schedule tensors to the correct device\n",
    "        - t_tensor should be shape (1,) with dtype long\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    # ==============================\n",
    "    return torch.zeros(n_frames, 3, size, size)\n",
    "\n",
    "\n",
    "def temporal_coherence_score(video: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Measure temporal coherence as mean cosine similarity between consecutive frames.\n",
    "\n",
    "    Args:\n",
    "        video: (T, C, H, W) tensor\n",
    "\n",
    "    Returns:\n",
    "        Float in [-1, 1], higher = more temporally coherent\n",
    "\n",
    "    Steps:\n",
    "        1. Flatten each frame: (T, C*H*W)\n",
    "        2. For consecutive pairs, compute cosine similarity\n",
    "        3. Return the mean\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    # ==============================\n",
    "    return 0.0"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: compare real vs independently generated\n",
    "print(\"Generating 5 independent dashcam clips...\")\n",
    "indep_scores = []\n",
    "for i in range(5):\n",
    "    vid = generate_independent_video(image_model, noise_schedule)\n",
    "    score = temporal_coherence_score(vid.cpu())\n",
    "    indep_scores.append(score)\n",
    "    print(f\"  Generated clip {i}: coherence = {score:.3f}\")\n",
    "\n",
    "real_scores = [temporal_coherence_score(videos_tensor[i]) for i in range(5)]\n",
    "\n",
    "print(f\"\\nReal KITTI clips:     mean coherence = {np.mean(real_scores):.3f}\")\n",
    "print(f\"Independent gen:      mean coherence = {np.mean(indep_scores):.3f}\")\n",
    "print(f\"Gap: {np.mean(real_scores) - np.mean(indep_scores):.3f}\")\n",
    "print(\"\\nThis gap is why Meridian cannot use frame-by-frame generation.\")\n",
    "print(\"TODO 2 passed.\" if np.mean(indep_scores) < np.mean(real_scores) else \"TODO 2 needs work.\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insight for Meridian:** A perception model that trains on temporally incoherent synthetic video would learn to track objects that randomly teleport between frames -- the exact opposite of what we need. The object tracker would produce fragmented tracks, destroying the training signal. This is why we need joint video generation."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the Video Diffusion Model\n",
    "\n",
    "Now we build the core model: a factorized video U-Net that jointly generates all 8 frames with temporal coherence. This is the same architecture used in VDM, Stable Video Diffusion, and Imagen Video -- scaled to fit our KITTI data on a T4 GPU.\n",
    "\n",
    "### Multi-Head Self-Attention (Shared Module)\n",
    "\n",
    "This exact same module handles both spatial and temporal attention. The difference is only in how we reshape the input."
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"Standard multi-head self-attention, reused for spatial and temporal.\"\"\"\n",
    "\n",
    "    def __init__(self, dim, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.qkv = nn.Linear(dim, 3 * dim)\n",
    "        self.out = nn.Linear(dim, dim)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, S, D = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, S, 3, self.n_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, S, D)\n",
    "        return self.out(out)"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 3: Spatial Attention Block\n",
    "\n",
    "Spatial attention processes each frame independently. The key reshape: merge the time axis into the batch dimension so each frame becomes a separate item in the batch. Frame 1 cannot see frame 2 -- this handles \"what each frame looks like.\""
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttentionBlock(nn.Module):\n",
    "    \"\"\"Self-attention within each frame independently.\"\"\"\n",
    "\n",
    "    def __init__(self, dim, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attn = MultiHeadSelfAttention(dim, n_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, T, H, W, C) video features\n",
    "\n",
    "        Returns:\n",
    "            (B, T, H, W, C) with spatial attention applied per frame\n",
    "\n",
    "        Steps:\n",
    "            1. Save input as residual\n",
    "            2. Apply layer norm\n",
    "            3. Get shape: B, T, H, W, C = x.shape\n",
    "            4. Reshape to (B*T, H*W, C) -- each frame is independent\n",
    "            5. Apply self-attention\n",
    "            6. Reshape back to (B, T, H, W, C)\n",
    "            7. Add residual and return\n",
    "\n",
    "        Hints:\n",
    "            - x.reshape(B*T, H*W, C) for step 4\n",
    "            - x.reshape(B, T, H, W, C) for step 6\n",
    "        \"\"\"\n",
    "        # ============ TODO ============\n",
    "        #\n",
    "        # YOUR CODE HERE\n",
    "        #\n",
    "        # ==============================\n",
    "        return x"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 4: Temporal Attention Block\n",
    "\n",
    "Temporal attention processes across frames at each spatial position. The key reshape: merge spatial dimensions into the batch so each pixel position attends across time. This handles \"how things move between frames.\""
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttentionBlock(nn.Module):\n",
    "    \"\"\"Self-attention across frames at each spatial position.\"\"\"\n",
    "\n",
    "    def __init__(self, dim, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attn = MultiHeadSelfAttention(dim, n_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, T, H, W, C) video features\n",
    "\n",
    "        Returns:\n",
    "            (B, T, H, W, C) with temporal attention applied per position\n",
    "\n",
    "        Steps:\n",
    "            1. Save input as residual\n",
    "            2. Apply layer norm\n",
    "            3. Get shape: B, T, H, W, C = x.shape\n",
    "            4. Permute to (B, H, W, T, C), reshape to (B*H*W, T, C)\n",
    "            5. Apply self-attention\n",
    "            6. Reshape to (B, H, W, T, C), permute back to (B, T, H, W, C)\n",
    "            7. Add residual and return\n",
    "\n",
    "        Hints:\n",
    "            - x.permute(0, 2, 3, 1, 4) for step 4\n",
    "            - After attn: reshape(B, H, W, T, C).permute(0, 3, 1, 2, 4) for step 6\n",
    "        \"\"\"\n",
    "        # ============ TODO ============\n",
    "        #\n",
    "        # YOUR CODE HERE\n",
    "        #\n",
    "        # ==============================\n",
    "        return x"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "test_x = torch.randn(2, 4, 8, 8, 64)\n",
    "sp = SpatialAttentionBlock(64)\n",
    "tp = TemporalAttentionBlock(64)\n",
    "\n",
    "assert sp(test_x).shape == test_x.shape, \"Spatial block shape mismatch\"\n",
    "assert tp(test_x).shape == test_x.shape, \"Temporal block shape mismatch\"\n",
    "\n",
    "# Verify spatial independence: changing frame 2 must not affect frame 0\n",
    "x = torch.randn(1, 4, 8, 8, 64)\n",
    "out1 = sp(x.clone())\n",
    "x_mod = x.clone()\n",
    "x_mod[:, 2] = torch.randn(1, 8, 8, 64)\n",
    "out2 = sp(x_mod)\n",
    "assert torch.allclose(out1[:, 0], out2[:, 0], atol=1e-5), \\\n",
    "    \"Spatial attention is leaking across frames!\"\n",
    "\n",
    "print(\"TODO 3 and TODO 4 passed -- attention blocks verified.\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorized Space-Time Block + Conditional Video U-Net\n",
    "\n",
    "Now we combine spatial attention, temporal attention, and a feedforward network into a single factorized block, then build the full U-Net."
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizedSpaceTimeBlock(nn.Module):\n",
    "    \"\"\"Spatial attention -> Temporal attention -> FFN\"\"\"\n",
    "\n",
    "    def __init__(self, dim, n_heads=4, ff_mult=4):\n",
    "        super().__init__()\n",
    "        self.spatial = SpatialAttentionBlock(dim, n_heads)\n",
    "        self.temporal = TemporalAttentionBlock(dim, n_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, dim * ff_mult),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * ff_mult, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.spatial(x)\n",
    "        x = self.temporal(x)\n",
    "        return x + self.ffn(x)\n",
    "\n",
    "\n",
    "class ConditionalVideoUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Video U-Net with factorized attention and driving-condition conditioning.\n",
    "    Uses 2D convolutions (per-frame) + factorized attention for temporal coherence.\n",
    "    Conditioned on driving condition labels (city/residential/open road).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch=3, base_dim=64, n_classes=6, time_dim=128, n_heads=4):\n",
    "        super().__init__()\n",
    "        # Time embedding (diffusion timestep)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, time_dim), nn.SiLU(), nn.Linear(time_dim, base_dim)\n",
    "        )\n",
    "        # Condition embedding (driving condition label; -1 = unconditional)\n",
    "        self.cond_emb = nn.Embedding(n_classes + 1, base_dim)\n",
    "\n",
    "        # Encoder (2D conv per frame)\n",
    "        self.enc1 = nn.Sequential(nn.Conv2d(in_ch, base_dim, 3, padding=1), nn.SiLU())\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(base_dim, base_dim * 2, 3, stride=2, padding=1), nn.SiLU()\n",
    "        )\n",
    "\n",
    "        # Bottleneck: factorized attention (operates at half resolution)\n",
    "        self.attn = FactorizedSpaceTimeBlock(base_dim * 2, n_heads)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_dim * 4, base_dim, 4, stride=2, padding=1), nn.SiLU()\n",
    "        )\n",
    "        self.dec1 = nn.Conv2d(base_dim * 2, in_ch, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, t, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, C, T, H, W) noisy video\n",
    "            t: (B,) diffusion timesteps\n",
    "            labels: (B,) condition labels (-1 for unconditional)\n",
    "        \"\"\"\n",
    "        B, C, T, H, W = x.shape\n",
    "        t_emb = self.time_mlp(t.float().unsqueeze(-1))\n",
    "        c_emb = self.cond_emb(labels + 1)  # shift: -1->0, 0->1, etc.\n",
    "        cond = t_emb + c_emb\n",
    "\n",
    "        # Process each frame with 2D convs\n",
    "        x_flat = x.permute(0, 2, 1, 3, 4).reshape(B * T, C, H, W)\n",
    "        cond_bt = cond.unsqueeze(1).expand(-1, T, -1).reshape(B * T, -1)\n",
    "\n",
    "        h1 = self.enc1(x_flat) + cond_bt[:, :, None, None]\n",
    "        h2 = self.enc2(h1)\n",
    "\n",
    "        # Factorized attention at bottleneck\n",
    "        _, C2, H2, W2 = h2.shape\n",
    "        h = h2.reshape(B, T, C2, H2, W2).permute(0, 1, 3, 4, 2)\n",
    "        h = self.attn(h)\n",
    "        h = h.permute(0, 1, 4, 2, 3).reshape(B * T, C2, H2, W2)\n",
    "\n",
    "        # Decode\n",
    "        h = self.dec2(torch.cat([h, h2], dim=1))\n",
    "        h = self.dec1(torch.cat([h, h1], dim=1))\n",
    "        return h.reshape(B, T, C, H, W).permute(0, 2, 1, 3, 4)"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model = ConditionalVideoUNet(n_classes=N_CLASSES).to(device)\n",
    "test_v = torch.randn(2, 3, 8, FRAME_SIZE, FRAME_SIZE).to(device)\n",
    "test_t = torch.randint(0, 200, (2,)).to(device)\n",
    "test_l = torch.tensor([0, 3]).to(device)\n",
    "out = model(test_v, test_t, test_l)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params:,}\")\n",
    "print(f\"Input shape:  {test_v.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "assert out.shape == test_v.shape\n",
    "print(\"Model test passed.\")"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training: Conditional Video Diffusion with Classifier-Free Guidance\n",
    "\n",
    "### TODO 5: Implement the Training Step\n",
    "\n",
    "This is the core training logic. Classifier-free guidance dropout randomly replaces condition labels with -1 (unconditional), enabling controlled generation at inference. At Meridian, this means we can generate \"dust storm\" clips specifically, rather than random driving clips."
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, videos, labels, noise_schedule, p_uncond=0.1):\n",
    "    \"\"\"\n",
    "    One training step of conditional video diffusion.\n",
    "\n",
    "    Args:\n",
    "        model: ConditionalVideoUNet\n",
    "        videos: (B, C, T, H, W) clean dashcam clips\n",
    "        labels: (B,) driving condition labels (0 to N_CLASSES-1)\n",
    "        noise_schedule: Dict with 'alphas_cumprod' on correct device\n",
    "        p_uncond: Probability of dropping condition (CFG)\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss tensor\n",
    "\n",
    "    Steps:\n",
    "        1. Sample random timesteps t ~ Uniform(0, num_timesteps-1), shape (B,)\n",
    "        2. Sample noise eps ~ N(0, I), same shape as videos\n",
    "        3. Compute noisy videos:\n",
    "           v_t = sqrt(alpha_bar_t) * v_0 + sqrt(1 - alpha_bar_t) * eps\n",
    "           Important: reshape alpha_bar_t to (B, 1, 1, 1, 1) for broadcasting\n",
    "        4. CFG dropout: clone labels, set labels[mask] = -1\n",
    "           where mask = (torch.rand(B) < p_uncond)\n",
    "        5. Predict noise: eps_pred = model(v_t, t, dropped_labels)\n",
    "        6. Return F.mse_loss(eps_pred, eps)\n",
    "\n",
    "    Hints:\n",
    "        - ac = noise_schedule['alphas_cumprod']\n",
    "        - ab = ac[t].reshape(-1, 1, 1, 1, 1) for broadcasting\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    # ==============================\n",
    "    return torch.tensor(0.0)"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "model.train()\n",
    "v_test = torch.randn(4, 3, 8, 64, 64).to(device)\n",
    "l_test = torch.tensor([0, 1, 3, 5]).to(device)\n",
    "loss = training_step(model, v_test, l_test, {\n",
    "    'alphas_cumprod': noise_schedule['alphas_cumprod'].to(device)\n",
    "})\n",
    "assert loss.dim() == 0, \"Loss should be scalar\"\n",
    "assert loss.item() > 0, \"Loss should be positive\"\n",
    "assert loss.requires_grad, \"Loss should have grad\"\n",
    "print(f\"Test loss: {loss.item():.4f}\")\n",
    "print(\"TODO 5 passed.\")"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on Real KITTI Data"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DashcamDataset(Dataset):\n",
    "    def __init__(self, videos, labels):\n",
    "        # videos: (N, T, C, H, W), labels: (N,)\n",
    "        self.videos = videos.permute(0, 2, 1, 3, 4)  # -> (N, C, T, H, W)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.videos[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "model = ConditionalVideoUNet(n_classes=N_CLASSES).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=40, eta_min=1e-5)\n",
    "dataset = DashcamDataset(videos_tensor, labels_tensor)\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True, drop_last=True)\n",
    "ns_device = {'alphas_cumprod': noise_schedule['alphas_cumprod'].to(device)}\n",
    "\n",
    "losses = []\n",
    "print(f\"Training on {len(dataset)} real KITTI dashcam clips (40 epochs)...\")\n",
    "for epoch in range(40):\n",
    "    epoch_loss = 0\n",
    "    n = 0\n",
    "    for vids, labs in loader:\n",
    "        vids, labs = vids.to(device), labs.to(device)\n",
    "        loss = training_step(model, vids, labs, ns_device, p_uncond=0.1)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        n += 1\n",
    "    scheduler.step()\n",
    "    avg = epoch_loss / max(n, 1)\n",
    "    losses.append(avg)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"  Epoch {epoch+1:3d}: Loss = {avg:.4f}, LR = {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses, color='steelblue', linewidth=2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.title(\"Video Diffusion Training Loss on KITTI Dashcam Data\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generating Synthetic Driving Clips\n",
    "\n",
    "### Conditional Sampling with Classifier-Free Guidance"
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_videos(model, noise_schedule, labels, guidance_scale=3.0,\n",
    "                  n_frames=8, size=64):\n",
    "    \"\"\"Sample driving clips conditioned on a driving condition label.\"\"\"\n",
    "    model.eval()\n",
    "    B = labels.shape[0]\n",
    "    betas = noise_schedule['betas'].to(device)\n",
    "    alphas = noise_schedule['alphas'].to(device)\n",
    "    ac = noise_schedule['alphas_cumprod'].to(device)\n",
    "    T_steps = len(betas)\n",
    "\n",
    "    x = torch.randn(B, 3, n_frames, size, size, device=device)\n",
    "\n",
    "    for t in reversed(range(T_steps)):\n",
    "        t_tensor = torch.full((B,), t, device=device)\n",
    "        # CFG: conditional + unconditional\n",
    "        eps_cond = model(x, t_tensor, labels)\n",
    "        eps_uncond = model(x, t_tensor, torch.full_like(labels, -1))\n",
    "        eps = eps_uncond + guidance_scale * (eps_cond - eps_uncond)\n",
    "\n",
    "        alpha, ab, beta = alphas[t], ac[t], betas[t]\n",
    "        mean = (1 / torch.sqrt(alpha)) * (x - beta / torch.sqrt(1 - ab) * eps)\n",
    "        x = mean + (torch.sqrt(beta) * torch.randn_like(x) if t > 0 else 0)\n",
    "\n",
    "    return x.clamp(0, 1)"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Clips for Each Driving Condition"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\"City: moderate\", \"City: heavy\", \"City: intersections\",\n",
    "              \"Residential: parked\", \"Residential: narrow\", \"Open road\"]\n",
    "\n",
    "print(\"Generating conditional dashcam clips...\")\n",
    "all_generated = {}\n",
    "for cls in range(N_CLASSES):\n",
    "    labels = torch.full((3,), cls, device=device)\n",
    "    vids = sample_videos(model, noise_schedule, labels, guidance_scale=3.0)\n",
    "    all_generated[cls] = vids.cpu()\n",
    "    print(f\"  Condition '{conditions[cls]}': 3 clips generated\")\n",
    "\n",
    "# Display\n",
    "fig, axes = plt.subplots(N_CLASSES, 8, figsize=(20, 15))\n",
    "fig.suptitle(\"Generated Driving Clips by Condition\", fontsize=14, fontweight='bold')\n",
    "\n",
    "for cls in range(N_CLASSES):\n",
    "    vid = all_generated[cls][0]  # first clip\n",
    "    for t in range(8):\n",
    "        frame = vid[:, t].permute(1, 2, 0).numpy()\n",
    "        axes[cls, t].imshow(frame)\n",
    "        axes[cls, t].axis('off')\n",
    "        if t == 0:\n",
    "            axes[cls, t].set_ylabel(conditions[cls], fontsize=9, rotation=0,\n",
    "                                     labelpad=100, va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side-by-Side: Real vs. Generated"
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 8, figsize=(20, 10))\n",
    "fig.suptitle(\"Real (rows 1,3) vs Generated (rows 2,4) Dashcam Clips\", fontsize=14)\n",
    "\n",
    "for pair_idx, cls in enumerate([0, 5]):  # city and open road\n",
    "    # Real\n",
    "    real_idx = (labels_tensor == cls).nonzero(as_tuple=True)[0][0]\n",
    "    real_clip = videos_tensor[real_idx]\n",
    "    # Generated\n",
    "    gen_clip = all_generated[cls][0]\n",
    "\n",
    "    for t in range(8):\n",
    "        r = pair_idx * 2\n",
    "        axes[r, t].imshow(real_clip[t].permute(1, 2, 0).numpy())\n",
    "        axes[r, t].axis('off')\n",
    "        if t == 0: axes[r, t].set_ylabel(\"Real\", fontsize=11)\n",
    "\n",
    "        axes[r+1, t].imshow(gen_clip[:, t].permute(1, 2, 0).numpy())\n",
    "        axes[r+1, t].axis('off')\n",
    "        if t == 0: axes[r+1, t].set_ylabel(\"Generated\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation\n",
    "\n",
    "### TODO 6: Frechet Video Distance\n",
    "\n",
    "FVD is Meridian's primary quality metric. It measures distributional distance between real and generated video clips in a learned feature space. Target: FVD < 250."
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVideoFeatureExtractor(nn.Module):\n",
    "    \"\"\"3D CNN feature extractor for FVD (fixed random weights).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv3d(3, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool3d((4, 4, 4)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 64, 128),\n",
    "        )\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def compute_fvd(real_videos, gen_videos, feature_extractor):\n",
    "    \"\"\"\n",
    "    Compute Frechet Video Distance.\n",
    "\n",
    "    Args:\n",
    "        real_videos: (N, C, T, H, W) real clips\n",
    "        gen_videos: (N, C, T, H, W) generated clips\n",
    "        feature_extractor: Maps (B, C, T, H, W) -> (B, feat_dim)\n",
    "\n",
    "    Returns:\n",
    "        FVD score (float, lower = better)\n",
    "\n",
    "    Steps:\n",
    "        1. Extract features for real videos in batches of 16.\n",
    "           Concatenate to get (N_real, feat_dim).\n",
    "        2. Same for generated videos.\n",
    "        3. Compute mu_real, sigma_real (mean and covariance)\n",
    "        4. Compute mu_gen, sigma_gen\n",
    "        5. FVD = ||mu_r - mu_g||^2 + Tr(S_r + S_g - 2 * sqrtm(S_r @ S_g))\n",
    "           Use scipy.linalg.sqrtm for the matrix square root.\n",
    "           Add 1e-6 * I for numerical stability.\n",
    "\n",
    "    Hints:\n",
    "        - features.mean(dim=0) for mu\n",
    "        - Center features, then X^T @ X / (N-1) for covariance\n",
    "        - scipy.linalg.sqrtm may return complex; take .real\n",
    "    \"\"\"\n",
    "    from scipy import linalg\n",
    "    # ============ TODO ============\n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    # ==============================\n",
    "    return 0.0"
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute FVD\n",
    "feat_ext = SimpleVideoFeatureExtractor().to(device)\n",
    "\n",
    "# Collect generated clips\n",
    "gen_all = torch.cat([all_generated[c] for c in range(N_CLASSES)], dim=0)\n",
    "gen_all = gen_all.to(device)\n",
    "\n",
    "# Sample real clips (same count)\n",
    "n_gen = gen_all.shape[0]\n",
    "real_sample = videos_tensor[:n_gen].permute(0, 2, 1, 3, 4).to(device)\n",
    "\n",
    "fvd = compute_fvd(real_sample, gen_all, feat_ext)\n",
    "print(f\"FVD score: {fvd:.1f}\")\n",
    "print(f\"Meridian target: < 250\")\n",
    "print(f\"{'PASS' if fvd < 250 else 'Above threshold -- model needs more training or data'}\")\n",
    "print(\"TODO 6 passed.\" if fvd > 0 else \"TODO 6 needs work.\")"
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Analysis: Driving-Specific Failure Modes\n",
    "\n",
    "### TODO 7: Categorize Failures in Generated Driving Clips\n",
    "\n",
    "At Meridian, every generated clip passes through an automated quality gate before being added to the training pool. We check for driving-specific artifacts that would corrupt perception model training."
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def driving_error_analysis(generated_clips, condition_labels, n_inspect=18):\n",
    "    \"\"\"\n",
    "    Categorize failure modes in generated driving clips.\n",
    "\n",
    "    Args:\n",
    "        generated_clips: dict mapping condition (int) -> (N, C, T, H, W) clips\n",
    "        condition_labels: list of condition name strings\n",
    "        n_inspect: total clips to inspect\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping failure_type (str) to count (int):\n",
    "            'temporal_flicker': max frame-to-frame pixel diff > 0.4\n",
    "            'static_video': mean frame-to-frame diff < 0.01 (no motion at all)\n",
    "            'color_drift': mean color of first frame differs from last by > 0.15\n",
    "            'clean': no failures detected\n",
    "\n",
    "    Steps:\n",
    "        1. Collect all generated clips into a single list\n",
    "        2. Randomly select n_inspect clips\n",
    "        3. For each clip (C, T, H, W):\n",
    "           a. Frame diffs: diff = (clip[:, 1:] - clip[:, :-1]).abs()\n",
    "              max_diff = diff.max().item()\n",
    "              mean_diff = diff.mean().item()\n",
    "           b. Color drift: |clip[:, 0].mean() - clip[:, -1].mean()|\n",
    "        4. Categorize based on thresholds\n",
    "        5. Count and return\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    # ==============================\n",
    "    return {}"
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run error analysis\n",
    "failures = driving_error_analysis(all_generated, conditions)\n",
    "print(\"Driving-Specific Failure Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "for cat, count in sorted(failures.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {cat:<20} {count:>3} clips\")\n",
    "\n",
    "total = sum(failures.values())\n",
    "clean = failures.get('clean', 0)\n",
    "reject_rate = (total - clean) / max(total, 1) * 100\n",
    "print(f\"\\nRejection rate: {reject_rate:.0f}%\")\n",
    "print(f\"Meridian target: < 15% rejection\")\n",
    "print(\"TODO 7 passed.\")"
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Think about this:** At Meridian, each failure mode has different downstream impact:\n",
    "- *Temporal flicker* causes the object tracker to fragment, losing track continuity\n",
    "- *Static video* provides no motion learning signal (the perception model sees a photograph)\n",
    "- *Color drift* teaches the model to expect impossible lighting changes, corrupting night/weather detection\n",
    "\n",
    "Which failure mode would you prioritize fixing first? What architectural change would you try?"
   ],
   "id": "cell_51"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference Benchmarking\n",
    "\n",
    "### TODO 8: Profile Generation Latency\n",
    "\n",
    "Meridian needs to generate 10,000 clips per week. Profile the inference pipeline to determine GPU requirements."
   ],
   "id": "cell_52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_generation(model, noise_schedule, n_runs=3):\n",
    "    \"\"\"\n",
    "    Benchmark video generation latency.\n",
    "\n",
    "    Args:\n",
    "        model: Trained video diffusion model\n",
    "        noise_schedule: Schedule dict\n",
    "        n_runs: Number of runs to average\n",
    "\n",
    "    Returns:\n",
    "        Dict with 'mean_seconds', 'std_seconds', 'per_step_ms', 'peak_memory_mb'\n",
    "\n",
    "    Steps:\n",
    "        1. Warm up with 1 dummy generation\n",
    "        2. For each run:\n",
    "           a. torch.cuda.synchronize() before starting timer\n",
    "           b. torch.cuda.reset_peak_memory_stats()\n",
    "           c. Generate one clip using sample_videos (single clip, guidance=3.0)\n",
    "           d. torch.cuda.synchronize() after generation\n",
    "           e. Record time and peak memory\n",
    "        3. Compute mean, std, per-step time, peak memory\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    # ==============================\n",
    "    return {'mean_seconds': 0.0, 'std_seconds': 0.0,\n",
    "            'per_step_ms': 0.0, 'peak_memory_mb': 0.0}"
   ],
   "id": "cell_53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = benchmark_generation(model, noise_schedule)\n",
    "print(f\"Generation time: {bench['mean_seconds']:.1f}s (+/- {bench['std_seconds']:.1f}s)\")\n",
    "print(f\"Per-step latency: {bench['per_step_ms']:.1f}ms\")\n",
    "print(f\"Peak GPU memory: {bench['peak_memory_mb']:.0f} MB\")\n",
    "\n",
    "# Capacity planning for Meridian\n",
    "clips_per_hour = 3600 / max(bench['mean_seconds'], 0.1)\n",
    "clips_per_week_1gpu = clips_per_hour * 24 * 7\n",
    "gpus_needed = 10000 / max(clips_per_week_1gpu, 1)\n",
    "print(f\"\\nCapacity planning:\")\n",
    "print(f\"  {clips_per_hour:.0f} clips/hour per GPU\")\n",
    "print(f\"  {clips_per_week_1gpu:.0f} clips/week per GPU (24/7)\")\n",
    "print(f\"  {gpus_needed:.1f} GPUs needed for 10,000 clips/week\")\n",
    "print(f\"\\nOptimizations to consider: DDIM (50 steps instead of 200),\")\n",
    "print(f\"  half-precision (fp16), model distillation, batch generation\")\n",
    "print(\"TODO 8 passed.\")"
   ],
   "id": "cell_54"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Ethical and Bias Analysis\n",
    "\n",
    "### TODO 9: Per-Condition Quality Audit\n",
    "\n",
    "At Meridian, if the model generates \"nighttime\" scenarios with lower quality than \"daytime,\" the perception model trained on that data will have a systematic blind spot at night -- exactly when safety matters most. Check for this bias."
   ],
   "id": "cell_55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_condition_quality_audit(model, noise_schedule, n_per_class=5):\n",
    "    \"\"\"\n",
    "    Audit generation quality bias across driving conditions.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        noise_schedule: Schedule dict\n",
    "        n_per_class: Clips per condition\n",
    "\n",
    "    Returns:\n",
    "        Dict with:\n",
    "            'per_condition_coherence': {cls: float}\n",
    "            'per_condition_sharpness': {cls: float} (mean Sobel gradient magnitude)\n",
    "            'worst_condition': int\n",
    "            'best_condition': int\n",
    "            'quality_gap': float (best - worst coherence)\n",
    "\n",
    "    Steps:\n",
    "        1. For each condition 0..N_CLASSES-1:\n",
    "           a. Generate n_per_class clips\n",
    "           b. Compute temporal coherence (reuse temporal_coherence_score)\n",
    "           c. Compute sharpness: for each frame, apply Sobel filter\n",
    "              Sobel x kernel: [[-1,0,1],[-2,0,2],[-1,0,1]]\n",
    "              Sharpness = mean |Sobel_x| + |Sobel_y| across all frames\n",
    "           d. Average coherence and sharpness for this condition\n",
    "        2. Identify best/worst conditions\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    # ==============================\n",
    "    return {\n",
    "        'per_condition_coherence': {},\n",
    "        'per_condition_sharpness': {},\n",
    "        'worst_condition': 0,\n",
    "        'best_condition': 0,\n",
    "        'quality_gap': 0.0,\n",
    "    }"
   ],
   "id": "cell_56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit = per_condition_quality_audit(model, noise_schedule, n_per_class=3)\n",
    "conditions = [\"City: moderate\", \"City: heavy\", \"City: intersections\",\n",
    "              \"Residential: parked\", \"Residential: narrow\", \"Open road\"]\n",
    "\n",
    "print(\"Per-Condition Generation Quality:\")\n",
    "print(f\"{'Condition':<25} {'Coherence':>10} {'Sharpness':>10}\")\n",
    "print(\"-\" * 50)\n",
    "for cls in range(N_CLASSES):\n",
    "    coh = audit['per_condition_coherence'].get(cls, 0)\n",
    "    sharp = audit['per_condition_sharpness'].get(cls, 0)\n",
    "    marker = \" <-- worst\" if cls == audit['worst_condition'] else \"\"\n",
    "    print(f\"{conditions[cls]:<25} {coh:>10.3f} {sharp:>10.4f}{marker}\")\n",
    "\n",
    "print(f\"\\nQuality gap: {audit['quality_gap']:.3f}\")\n",
    "print(f\"Worst condition: {conditions[audit['worst_condition']]}\")\n",
    "print(\"TODO 9 passed.\")"
   ],
   "id": "cell_57"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Critical questions for Meridian's ML team:**\n",
    "- If the model generates \"open road\" clips with significantly lower quality than \"city\" clips, what happens when the perception model is deployed on highway routes?\n",
    "- How would you enforce that ALL driving conditions meet a minimum quality threshold before synthetic data enters the training pipeline?\n",
    "- What SOC 2 controls would you place around the synthetic data pipeline? Consider: data provenance tracking, model versioning, and audit logs."
   ],
   "id": "cell_58"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "You have built an end-to-end synthetic driving video generation pipeline:\n",
    "\n",
    "1. **Downloaded real dashcam data** from KITTI -- industry-standard autonomous driving benchmark\n",
    "2. **Analyzed temporal dynamics** across driving conditions (city, residential, highway)\n",
    "3. **Proved the baseline fails** -- frame-independent generation has no temporal coherence\n",
    "4. **Built factorized attention** -- spatial attention per frame + temporal attention across frames\n",
    "5. **Trained with classifier-free guidance** -- enabling condition-controlled generation\n",
    "6. **Evaluated with FVD** -- the standard metric for video generation quality\n",
    "7. **Analyzed driving-specific failure modes** -- flicker, static video, color drift\n",
    "8. **Benchmarked inference** for Meridian's 10,000 clips/week production target\n",
    "9. **Audited per-condition quality bias** to prevent systematic blind spots\n",
    "\n",
    "### Scaling This to Production\n",
    "\n",
    "The model you trained operates at 64x64 resolution on 8 frames. Meridian's production system generates at 1280x720 on 40 frames. To bridge this gap:\n",
    "\n",
    "- **Latent diffusion**: Compress dashcam frames to a 16x smaller latent space via a pretrained VAE, then run diffusion in latent space (see Section 2 of the case study)\n",
    "- **Cascaded upsampling**: Generate at low resolution first, then upscale with a super-resolution model (Imagen Video approach)\n",
    "- **Pretrained initialization**: Start from Stable Diffusion's spatial layers, train only the temporal layers on driving data (Stable Video Diffusion approach)\n",
    "- **DDIM sampling**: Reduce from 200 to 50 reverse steps for 4x faster inference\n",
    "\n",
    "### Using Larger Driving Datasets\n",
    "\n",
    "KITTI is excellent for prototyping but limited in weather diversity. For production-scale training:\n",
    "\n",
    "- **nuScenes** (nuscenes.org): 1000 scenes from Boston and Singapore with rain, night, and diverse conditions. Register (free) and download the mini split (4 GB) or full trainval (60 GB).\n",
    "- **BDD100K** (bdd-data.berkeley.edu): 100K driving clips with explicit weather and time-of-day labels. Ideal for training weather-conditioned generation.\n",
    "- **Waymo Open** (waymo.com/open): High-quality driving data from Waymo's fleet across multiple US cities.\n",
    "\n",
    "For **Section 4 (Production Design)** of the full case study -- API design, serving infrastructure, monitoring, A/B testing, and cost analysis -- refer to the case study PDF."
   ],
   "id": "cell_59"
  }
 ]
}