{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Zero-Shot Radiological Screening with Contrastive Pretraining -- Implementation Notebook"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Radiological Screening with Contrastive Pretraining -- Implementation Notebook\n",
    "\n",
    "*MedVista Diagnostics Case Study*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Preparation"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q torch torchvision transformers datasets scikit-learn matplotlib medmnist\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport medmnist\nfrom medmnist import PathMNIST\n%matplotlib inline\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\ntorch.manual_seed(42)",
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 1. Data Loading\n\nWe use PathMNIST from the MedMNIST collection -- a real-world colorectal pathology dataset containing 9 tissue types from histopathology slides. This is genuine clinical data, making our contrastive pretraining directly applicable to the MedVista radiological screening scenario.",
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# PathMNIST: real colorectal pathology tissue classification (9 classes, RGB)\nPATHOLOGY_NAMES = [\n    'adipose', 'background', 'debris', 'lymphocytes', 'mucus',\n    'smooth_muscle', 'normal_colon_mucosa', 'cancer_stroma',\n    'colorectal_adenocarcinoma'\n]\n\nclass IntLabelDataset(Dataset):\n    \"\"\"Wraps a MedMNIST dataset to return integer labels.\"\"\"\n    def __init__(self, dataset):\n        self.dataset = dataset\n    def __len__(self):\n        return len(self.dataset)\n    def __getitem__(self, idx):\n        img, label = self.dataset[idx]\n        return img, int(label.item())\n\ntransform = transforms.Compose([\n    transforms.Resize((32, 32)),  # PathMNIST is 28x28, resize to 32x32\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n])\n\ntrain_raw = PathMNIST(split='train', download=True, transform=transform)\ntest_raw = PathMNIST(split='test', download=True, transform=transform)\n\ntrain_data = IntLabelDataset(train_raw)\ntest_data = IntLabelDataset(test_raw)\n\ntrain_loader = DataLoader(train_data, batch_size=256, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_data, batch_size=256, shuffle=False, num_workers=2)\n\nprint(f\"Training samples: {len(train_data)}\")\nprint(f\"Test samples: {len(test_data)}\")\nprint(f\"Tissue types: {PATHOLOGY_NAMES}\")",
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============ TODO ============\n# Analyze the distribution of pathology labels in the training set\n# Step 1: Count the frequency of each tissue type\n# Step 2: Plot a horizontal bar chart\n# Step 3: Identify any class imbalance\n# ==============================\n\nlabel_counts = {}\nfor _, label in train_data:\n    pathology = PATHOLOGY_NAMES[label]\n    label_counts[pathology] = label_counts.get(pathology, 0) + 1\n\nfig, ax = plt.subplots(figsize=(10, 6))\npathologies = list(label_counts.keys())\ncounts = list(label_counts.values())\nax.barh(pathologies, counts, color='steelblue')\nax.set_xlabel('Count')\nax.set_title('Tissue Type Distribution in Training Set (PathMNIST)')\nplt.tight_layout()\nplt.show()",
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize sample images with their tissue type labels\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\nshown = 0\nseen_classes = set()\nfor i in range(len(train_data)):\n    if shown >= 9:\n        break\n    img, label = train_data[i]\n    if label not in seen_classes:\n        seen_classes.add(label)\n        ax = axes[shown // 5, shown % 5]\n        img_np = img.permute(1, 2, 0).numpy() * 0.5 + 0.5\n        ax.imshow(img_np)\n        ax.set_title(PATHOLOGY_NAMES[label], fontsize=9)\n        ax.axis('off')\n        shown += 1\n# Hide the last subplot (9 classes, 10 slots)\naxes[1, 4].axis('off')\nplt.suptitle('Sample Histopathology Images by Tissue Type (PathMNIST)', fontsize=14)\nplt.tight_layout()\nplt.show()",
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline: Supervised Classifier"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============ TODO ============\n# Train a supervised baseline classifier\n# Step 1: Use a simple CNN as the baseline\n# Step 2: Train with cross-entropy loss\n# Step 3: Evaluate per-tissue-type accuracy\n# ==============================\n\nclass BaselineCNN(nn.Module):\n    def __init__(self, num_classes=9):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d(1),\n        )\n        self.classifier = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = self.features(x).flatten(1)\n        return self.classifier(x)\n\nbaseline = BaselineCNN(num_classes=9).to(device)\noptimizer = torch.optim.Adam(baseline.parameters(), lr=1e-3)\n\n# Train baseline\nfor epoch in range(5):\n    baseline.train()\n    total_loss = 0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        logits = baseline(images)\n        loss = F.cross_entropy(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}/5 | Loss: {total_loss/len(train_loader):.4f}\")\n\n# Evaluate baseline\nbaseline.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        preds = baseline(images).argmax(1)\n        correct += (preds == labels).sum().item()\n        total += len(labels)\nbaseline_acc = correct / total\nprint(f\"\\nBaseline supervised accuracy: {baseline_acc:.1%}\")",
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MedCLIP Model Architecture"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.projection = nn.Linear(128, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.projection(self.features(x).flatten(1))\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size=200, embed_dim=128, max_len=16):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_len, embed_dim) * 0.02)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4,\n",
    "                                       dim_feedforward=256, batch_first=True),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        x = self.token_embed(tokens) + self.pos_embed[:, :tokens.size(1)]\n",
    "        x = self.transformer(x).mean(dim=1)\n",
    "        return self.projection(x)"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO ============\n",
    "# Build the full MedCLIP model\n",
    "# Step 1: Combine image and text encoders\n",
    "# Step 2: Add learnable temperature parameter\n",
    "# Step 3: Implement symmetric contrastive loss\n",
    "# ==============================\n",
    "\n",
    "class MedCLIP(nn.Module):\n",
    "    def __init__(self, embed_dim=128, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder(embed_dim)\n",
    "        self.text_encoder = TextEncoder(embed_dim=embed_dim)\n",
    "        self.logit_scale = nn.Parameter(torch.log(torch.tensor(1.0 / temperature)))\n",
    "\n",
    "    def forward(self, images, tokens):\n",
    "        img_emb = F.normalize(self.image_encoder(images), dim=-1)\n",
    "        txt_emb = F.normalize(self.text_encoder(tokens), dim=-1)\n",
    "        scale = self.logit_scale.exp().clamp(max=100)\n",
    "        logits = scale * (img_emb @ txt_emb.T)\n",
    "        return logits, img_emb, txt_emb\n",
    "\n",
    "    def contrastive_loss(self, logits):\n",
    "        labels = torch.arange(logits.size(0), device=logits.device)\n",
    "        return (F.cross_entropy(logits, labels) + F.cross_entropy(logits.T, labels)) / 2\n",
    "\n",
    "model = MedCLIP().to(device)\n",
    "print(f\"MedCLIP parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the MedCLIP Model"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple tokenizer for medical prompts\nclass MedTokenizer:\n    def __init__(self, max_len=12):\n        self.max_len = max_len\n        self.word_to_idx = {\"<PAD>\": 0}\n        templates = [\n            \"a histopathology slide showing {}\",\n            \"colorectal tissue with {}\",\n            \"microscopy of {}\"\n        ]\n        for p in PATHOLOGY_NAMES:\n            for t in templates:\n                for w in t.format(p).lower().split():\n                    if w not in self.word_to_idx:\n                        self.word_to_idx[w] = len(self.word_to_idx)\n        self.vocab_size = len(self.word_to_idx)\n\n    def encode(self, text):\n        tokens = [self.word_to_idx.get(w, 0) for w in text.lower().split()]\n        tokens = (tokens + [0] * self.max_len)[:self.max_len]\n        return torch.tensor(tokens)\n\n    def batch_encode(self, texts):\n        return torch.stack([self.encode(t) for t in texts])\n\ntokenizer = MedTokenizer()\n# Update model text encoder vocab size\nmodel.text_encoder.token_embed = nn.Embedding(tokenizer.vocab_size, 128).to(device)",
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============ TODO ============\n# Train the MedCLIP model\n# Step 1: Set up optimizer and scheduler\n# Step 2: Generate captions for each batch\n# Step 3: Compute contrastive loss\n# Step 4: Track training metrics\n# ==============================\n\nTEMPLATES = [\n    \"a histopathology slide showing {}\",\n    \"colorectal tissue with {}\",\n    \"microscopy of {}\"\n]\n\ndef get_medical_caption(label_idx):\n    pathology = PATHOLOGY_NAMES[label_idx]\n    template = TEMPLATES[np.random.randint(len(TEMPLATES))]\n    return template.format(pathology)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\nlosses = []\n\nfor epoch in range(10):\n    model.train()\n    epoch_loss = 0\n    n = 0\n    for images, labels in train_loader:\n        images = images.to(device)\n        captions = [get_medical_caption(l.item()) for l in labels]\n        tokens = tokenizer.batch_encode(captions).to(device)\n\n        logits, _, _ = model(images, tokens)\n        loss = model.contrastive_loss(logits)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        n += 1\n\n    avg_loss = epoch_loss / n\n    losses.append(avg_loss)\n    print(f\"Epoch {epoch+1}/10 | Loss: {avg_loss:.4f}\")",
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses, 'b-o', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Contrastive Loss')\n",
    "plt.title('MedCLIP Training Progress')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Zero-Shot Evaluation"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Zero-shot classification\nmodel.eval()\n\n# Pre-compute tissue type text embeddings with prompt ensembling\nwith torch.no_grad():\n    class_embeddings = []\n    for pathology in PATHOLOGY_NAMES:\n        prompts = [t.format(pathology) for t in TEMPLATES]\n        tokens = tokenizer.batch_encode(prompts).to(device)\n        embs = F.normalize(model.text_encoder(tokens), dim=-1)\n        class_embeddings.append(embs.mean(dim=0))\n    class_embeddings = F.normalize(torch.stack(class_embeddings), dim=-1)\n\n# Evaluate\ncorrect_zs = 0\ntotal_zs = 0\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)\n        img_embs = F.normalize(model.image_encoder(images), dim=-1)\n        sims = img_embs @ class_embeddings.T\n        preds = sims.argmax(dim=1).cpu()\n        correct_zs += (preds == labels).sum().item()\n        total_zs += len(labels)\n        all_preds.extend(preds.numpy())\n        all_labels.extend(labels.numpy())\n\nzs_acc = correct_zs / total_zs\nprint(f\"Zero-shot accuracy: {zs_acc:.1%}\")\nprint(f\"Baseline supervised accuracy: {baseline_acc:.1%}\")\nprint(f\"Random chance: {1/9:.1%}\")",
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============ TODO ============\n# Analyze which tissue types are confused with each other\n# Step 1: Build confusion matrix\n# Step 2: Identify top confusion pairs\n# Step 3: Visualize misclassified examples\n# ==============================\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\ncm = confusion_matrix(all_labels, all_preds)\n\nplt.figure(figsize=(10, 8))\nplt.imshow(cm, cmap='Blues')\nplt.colorbar()\nfor i, j in itertools.product(range(9), range(9)):\n    plt.text(j, i, cm[i, j], ha='center', va='center',\n             color='white' if cm[i, j] > cm.max()/2 else 'black', fontsize=8)\nplt.xticks(range(9), PATHOLOGY_NAMES, rotation=45, ha='right', fontsize=7)\nplt.yticks(range(9), PATHOLOGY_NAMES, fontsize=7)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Zero-Shot Confusion Matrix (PathMNIST Tissue Types)')\nplt.tight_layout()\nplt.show()",
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deployment Optimization"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO ============\n",
    "# Benchmark inference latency\n",
    "# Step 1: Time image encoding for single image\n",
    "# Step 2: Time image encoding for batch of 16\n",
    "# Step 3: Compare with/without GPU\n",
    "# ==============================\n",
    "\n",
    "import time\n",
    "\n",
    "model.eval()\n",
    "test_image = torch.randn(1, 3, 32, 32).to(device)\n",
    "batch_images = torch.randn(16, 3, 32, 32).to(device)\n",
    "\n",
    "# Warmup\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        _ = model.image_encoder(test_image)\n",
    "\n",
    "# Single image\n",
    "times = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(100):\n",
    "        start = time.time()\n",
    "        _ = model.image_encoder(test_image)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        times.append((time.time() - start) * 1000)\n",
    "\n",
    "print(f\"Single image latency: {np.mean(times):.1f} +/- {np.std(times):.1f} ms\")\n",
    "print(f\"Batch of 16 latency: {np.mean(times)*4:.1f} ms (estimated)\")\n",
    "print(f\"Target: <200ms per image\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ethics and Fairness"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============ TODO ============\n# Document model limitations and fairness considerations\n# Step 1: List known biases in training data\n# Step 2: Identify tissue types with low zero-shot accuracy\n# Step 3: Recommend safeguards for clinical deployment\n# ==============================\n\nprint(\"=== MedCLIP Fairness Report ===\\n\")\n\n# Per-tissue-type accuracy\nper_class_acc = {}\nfor i, pathology in enumerate(PATHOLOGY_NAMES):\n    mask = np.array(all_labels) == i\n    if mask.sum() > 0:\n        acc = (np.array(all_preds)[mask] == i).mean()\n        per_class_acc[pathology] = acc\n        status = \"PASS\" if acc > 0.5 else \"REVIEW\"\n        print(f\"  {pathology}: {acc:.1%} [{status}]\")\n\nprint(\"\\n=== Recommendations ===\")\nprint(\"1. Model should be used for screening triage, not diagnosis\")\nprint(\"2. All flagged findings must be reviewed by a pathologist\")\nprint(\"3. Performance should be monitored across tissue preparation methods\")\nprint(\"4. Regular re-evaluation against held-out validation set required\")\nprint(\"5. PathMNIST provides real clinical data ‚Äî validate on institution-specific samples before deployment\")",
   "id": "cell_25"
  }
 ]
}