{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Zero-Shot Radiological Screening with Contrastive Pretraining -- Implementation Notebook"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Radiological Screening with Contrastive Pretraining -- Implementation Notebook\n",
    "\n",
    "*MedVista Diagnostics Case Study*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Preparation"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision transformers datasets scikit-learn matplotlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(42)"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "For this implementation, we simulate a medical dataset using CIFAR-10 as a proxy. In production, this would be replaced with actual chest X-ray data and radiology reports."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "# Simulated medical dataset using CIFAR-10 as proxy\n",
    "PATHOLOGY_NAMES = [\n",
    "    'cardiomegaly', 'pleural_effusion', 'pneumonia', 'atelectasis',\n",
    "    'edema', 'consolidation', 'pneumothorax', 'mass',\n",
    "    'nodule', 'hernia'\n",
    "]\n",
    "\n",
    "# Map CIFAR-10 classes to simulated pathologies\n",
    "CIFAR_TO_PATHOLOGY = {i: PATHOLOGY_NAMES[i] for i in range(10)}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10('./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=256, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"Pathologies: {PATHOLOGY_NAMES}\")"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO ============\n",
    "# Analyze the distribution of pathology labels in the training set\n",
    "# Step 1: Count the frequency of each pathology\n",
    "# Step 2: Plot a horizontal bar chart\n",
    "# Step 3: Identify any class imbalance\n",
    "# ==============================\n",
    "\n",
    "label_counts = {}\n",
    "for _, label in train_data:\n",
    "    pathology = CIFAR_TO_PATHOLOGY[label]\n",
    "    label_counts[pathology] = label_counts.get(pathology, 0) + 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "pathologies = list(label_counts.keys())\n",
    "counts = list(label_counts.values())\n",
    "ax.barh(pathologies, counts, color='steelblue')\n",
    "ax.set_xlabel('Count')\n",
    "ax.set_title('Pathology Distribution in Training Set')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images with their pathology labels\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    img, label = train_data[i * 500]\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    img_np = img.permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
    "    ax.imshow(img_np)\n",
    "    ax.set_title(CIFAR_TO_PATHOLOGY[label], fontsize=10)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Sample Images with Pathology Labels', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline: Supervised Classifier"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO ============\n",
    "# Train a supervised baseline classifier\n",
    "# Step 1: Use a simple CNN as the baseline\n",
    "# Step 2: Train with cross-entropy loss\n",
    "# Step 3: Evaluate per-pathology accuracy\n",
    "# ==============================\n",
    "\n",
    "class BaselineCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x).flatten(1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "baseline = BaselineCNN().to(device)\n",
    "optimizer = torch.optim.Adam(baseline.parameters(), lr=1e-3)\n",
    "\n",
    "# Train baseline\n",
    "for epoch in range(5):\n",
    "    baseline.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits = baseline(images)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/5 | Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluate baseline\n",
    "baseline.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        preds = baseline(images).argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += len(labels)\n",
    "baseline_acc = correct / total\n",
    "print(f\"\\nBaseline supervised accuracy: {baseline_acc:.1%}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MedCLIP Model Architecture"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.projection = nn.Linear(128, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.projection(self.features(x).flatten(1))\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size=200, embed_dim=128, max_len=16):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_len, embed_dim) * 0.02)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4,\n",
    "                                       dim_feedforward=256, batch_first=True),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        x = self.token_embed(tokens) + self.pos_embed[:, :tokens.size(1)]\n",
    "        x = self.transformer(x).mean(dim=1)\n",
    "        return self.projection(x)"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO ============\n",
    "# Build the full MedCLIP model\n",
    "# Step 1: Combine image and text encoders\n",
    "# Step 2: Add learnable temperature parameter\n",
    "# Step 3: Implement symmetric contrastive loss\n",
    "# ==============================\n",
    "\n",
    "class MedCLIP(nn.Module):\n",
    "    def __init__(self, embed_dim=128, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder(embed_dim)\n",
    "        self.text_encoder = TextEncoder(embed_dim=embed_dim)\n",
    "        self.logit_scale = nn.Parameter(torch.log(torch.tensor(1.0 / temperature)))\n",
    "\n",
    "    def forward(self, images, tokens):\n",
    "        img_emb = F.normalize(self.image_encoder(images), dim=-1)\n",
    "        txt_emb = F.normalize(self.text_encoder(tokens), dim=-1)\n",
    "        scale = self.logit_scale.exp().clamp(max=100)\n",
    "        logits = scale * (img_emb @ txt_emb.T)\n",
    "        return logits, img_emb, txt_emb\n",
    "\n",
    "    def contrastive_loss(self, logits):\n",
    "        labels = torch.arange(logits.size(0), device=logits.device)\n",
    "        return (F.cross_entropy(logits, labels) + F.cross_entropy(logits.T, labels)) / 2\n",
    "\n",
    "model = MedCLIP().to(device)\n",
    "print(f\"MedCLIP parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the MedCLIP Model"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple tokenizer for medical prompts\n",
    "class MedTokenizer:\n",
    "    def __init__(self, max_len=12):\n",
    "        self.max_len = max_len\n",
    "        self.word_to_idx = {\"<PAD>\": 0}\n",
    "        templates = [\"a radiograph showing {}\", \"chest x-ray with {}\", \"findings of {}\"]\n",
    "        for p in PATHOLOGY_NAMES:\n",
    "            for t in templates:\n",
    "                for w in t.format(p).lower().split():\n",
    "                    if w not in self.word_to_idx:\n",
    "                        self.word_to_idx[w] = len(self.word_to_idx)\n",
    "        self.vocab_size = len(self.word_to_idx)\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = [self.word_to_idx.get(w, 0) for w in text.lower().split()]\n",
    "        tokens = (tokens + [0] * self.max_len)[:self.max_len]\n",
    "        return torch.tensor(tokens)\n",
    "\n",
    "    def batch_encode(self, texts):\n",
    "        return torch.stack([self.encode(t) for t in texts])\n",
    "\n",
    "tokenizer = MedTokenizer()\n",
    "# Update model text encoder vocab size\n",
    "model.text_encoder.token_embed = nn.Embedding(tokenizer.vocab_size, 128).to(device)"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO ============\n",
    "# Train the MedCLIP model\n",
    "# Step 1: Set up optimizer and scheduler\n",
    "# Step 2: Generate captions for each batch\n",
    "# Step 3: Compute contrastive loss\n",
    "# Step 4: Track training metrics\n",
    "# ==============================\n",
    "\n",
    "TEMPLATES = [\"a radiograph showing {}\", \"chest x-ray with {}\", \"findings of {}\"]\n",
    "\n",
    "def get_medical_caption(label_idx):\n",
    "    pathology = CIFAR_TO_PATHOLOGY[label_idx]\n",
    "    template = TEMPLATES[np.random.randint(len(TEMPLATES))]\n",
    "    return template.format(pathology)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "losses = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    n = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        captions = [get_medical_caption(l.item()) for l in labels]\n",
    "        tokens = tokenizer.batch_encode(captions).to(device)\n",
    "\n",
    "        logits, _, _ = model(images, tokens)\n",
    "        loss = model.contrastive_loss(logits)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        n += 1\n",
    "\n",
    "    avg_loss = epoch_loss / n\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/10 | Loss: {avg_loss:.4f}\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses, 'b-o', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Contrastive Loss')\n",
    "plt.title('MedCLIP Training Progress')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Zero-Shot Evaluation"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot classification\n",
    "model.eval()\n",
    "\n",
    "# Pre-compute pathology text embeddings with prompt ensembling\n",
    "with torch.no_grad():\n",
    "    class_embeddings = []\n",
    "    for pathology in PATHOLOGY_NAMES:\n",
    "        prompts = [t.format(pathology) for t in TEMPLATES]\n",
    "        tokens = tokenizer.batch_encode(prompts).to(device)\n",
    "        embs = F.normalize(model.text_encoder(tokens), dim=-1)\n",
    "        class_embeddings.append(embs.mean(dim=0))\n",
    "    class_embeddings = F.normalize(torch.stack(class_embeddings), dim=-1)\n",
    "\n",
    "# Evaluate\n",
    "correct_zs = 0\n",
    "total_zs = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        img_embs = F.normalize(model.image_encoder(images), dim=-1)\n",
    "        sims = img_embs @ class_embeddings.T\n",
    "        preds = sims.argmax(dim=1).cpu()\n",
    "        correct_zs += (preds == labels).sum().item()\n",
    "        total_zs += len(labels)\n",
    "        all_preds.extend(preds.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "zs_acc = correct_zs / total_zs\n",
    "print(f\"Zero-shot accuracy: {zs_acc:.1%}\")\n",
    "print(f\"Baseline supervised accuracy: {baseline_acc:.1%}\")\n",
    "print(f\"Random chance: {1/10:.1%}\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO ============\n",
    "# Analyze which pathologies are confused with each other\n",
    "# Step 1: Build confusion matrix\n",
    "# Step 2: Identify top confusion pairs\n",
    "# Step 3: Visualize misclassified examples\n",
    "# ==============================\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cm, cmap='Blues')\n",
    "plt.colorbar()\n",
    "for i, j in itertools.product(range(10), range(10)):\n",
    "    plt.text(j, i, cm[i, j], ha='center', va='center',\n",
    "             color='white' if cm[i, j] > cm.max()/2 else 'black', fontsize=8)\n",
    "plt.xticks(range(10), PATHOLOGY_NAMES, rotation=45, ha='right')\n",
    "plt.yticks(range(10), PATHOLOGY_NAMES)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Zero-Shot Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deployment Optimization"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO ============\n",
    "# Benchmark inference latency\n",
    "# Step 1: Time image encoding for single image\n",
    "# Step 2: Time image encoding for batch of 16\n",
    "# Step 3: Compare with/without GPU\n",
    "# ==============================\n",
    "\n",
    "import time\n",
    "\n",
    "model.eval()\n",
    "test_image = torch.randn(1, 3, 32, 32).to(device)\n",
    "batch_images = torch.randn(16, 3, 32, 32).to(device)\n",
    "\n",
    "# Warmup\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        _ = model.image_encoder(test_image)\n",
    "\n",
    "# Single image\n",
    "times = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(100):\n",
    "        start = time.time()\n",
    "        _ = model.image_encoder(test_image)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        times.append((time.time() - start) * 1000)\n",
    "\n",
    "print(f\"Single image latency: {np.mean(times):.1f} +/- {np.std(times):.1f} ms\")\n",
    "print(f\"Batch of 16 latency: {np.mean(times)*4:.1f} ms (estimated)\")\n",
    "print(f\"Target: <200ms per image\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ethics and Fairness"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO ============\n",
    "# Document model limitations and fairness considerations\n",
    "# Step 1: List known biases in training data\n",
    "# Step 2: Identify pathologies with low zero-shot accuracy\n",
    "# Step 3: Recommend safeguards for clinical deployment\n",
    "# ==============================\n",
    "\n",
    "print(\"=== MedCLIP Fairness Report ===\\n\")\n",
    "\n",
    "# Per-pathology accuracy\n",
    "per_class_acc = {}\n",
    "for i, pathology in enumerate(PATHOLOGY_NAMES):\n",
    "    mask = np.array(all_labels) == i\n",
    "    if mask.sum() > 0:\n",
    "        acc = (np.array(all_preds)[mask] == i).mean()\n",
    "        per_class_acc[pathology] = acc\n",
    "        status = \"PASS\" if acc > 0.5 else \"REVIEW\"\n",
    "        print(f\"  {pathology}: {acc:.1%} [{status}]\")\n",
    "\n",
    "print(\"\\n=== Recommendations ===\")\n",
    "print(\"1. Model should be used for screening triage, not diagnosis\")\n",
    "print(\"2. All flagged findings must be reviewed by a radiologist\")\n",
    "print(\"3. Performance should be monitored across demographic groups\")\n",
    "print(\"4. Regular re-evaluation against held-out validation set required\")"
   ],
   "id": "cell_25"
  }
 ]
}