{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "NovaPick Case Study: World Model + VLA for Autonomous Pick-and-Place"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Autonomous Pick-and-Place for High-SKU E-Commerce Fulfillment\n",
    "## Implementation Notebook"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://pods.vizuara.ai/courses/world-action-models/practice/0/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Context\n",
    "\n",
    "**Company:** NovaPick Robotics (Series B startup, 118 employees, $45M raised)\n",
    "\n",
    "**Problem:** NovaPick's current robotic pick-and-place system uses hand-engineered grasp planners that work for ~500 pre-programmed SKUs but fail on novel items (34.7% success rate). Their prospective client MegaMart has 83,000+ active SKUs. They need zero-shot generalization to novel objects with >90% pick success rate.\n",
    "\n",
    "**Approach:** World model (RSSM) for predicting grasp outcomes in latent space + Vision-Language-Action architecture with flow matching for generating smooth robot trajectories conditioned on camera input and language instructions.\n",
    "\n",
    "**Key Metrics:**\n",
    "\n",
    "| Metric | Target |\n",
    "|---|---|\n",
    "| Pick success rate (novel objects) | > 90% |\n",
    "| Pick success rate (known objects) | > 97% |\n",
    "| Cycle time | < 4.0 seconds |\n",
    "| Inference latency | < 200ms |\n",
    "| Damage rate | < 0.5% |"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional\n",
    "import time\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU detected. Go to Runtime > Change runtime type > GPU\")\n",
    "\n",
    "print(f\"PyTorch {torch.__version__}\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data Acquisition Strategy\n",
    "\n",
    "**Dataset:** A combination of:\n",
    "- **DROID (Distributed Robot Interaction Dataset):** 76K real-world robot manipulation trajectories with RGB-D images, proprioceptive states, and action labels\n",
    "- **Google Robot dataset (Open X-Embodiment):** 53K pick-and-place episodes with language annotations\n",
    "\n",
    "For this notebook, we use a curated subset: 10K episodes from DROID + 5K from Google Robot, pre-processed and hosted on Hugging Face.\n",
    "\n",
    "**Preprocessing pipeline:**\n",
    "- Resize RGB images to (3, 224, 224)\n",
    "- Normalize depth maps to [0, 1] range\n",
    "- Tokenize language instructions using a frozen CLIP text encoder\n",
    "- Normalize joint angles to [-1, 1] range\n",
    "- Segment episodes into overlapping action chunks of 16 timesteps"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Data Augmentation Pipeline"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_observation(rgb: torch.Tensor, depth: torch.Tensor,\n",
    "                         language: str) -> tuple[torch.Tensor, torch.Tensor, str]:\n",
    "    \"\"\"\n",
    "    Apply data augmentation to a single observation.\n",
    "\n",
    "    Args:\n",
    "        rgb: RGB image tensor of shape (3, 224, 224), values in [0, 1]\n",
    "        depth: Depth map tensor of shape (1, 224, 224), values in [0, 1]\n",
    "        language: Language instruction string\n",
    "\n",
    "    Returns:\n",
    "        Augmented (rgb, depth, language) tuple\n",
    "\n",
    "    Requirements:\n",
    "        1. Apply random color jitter to RGB (brightness +/-0.2, contrast +/-0.2,\n",
    "           saturation +/-0.2) -- this simulates variable warehouse lighting\n",
    "        2. Apply the SAME random crop (scale 0.8-1.0) to BOTH rgb and depth --\n",
    "           spatial correspondence must be preserved\n",
    "        3. Apply random horizontal flip with p=0.5 to BOTH rgb and depth --\n",
    "           remember to also flip any spatial terms in the language instruction\n",
    "           (swap \"left\" <-> \"right\")\n",
    "        4. Add Gaussian noise (std=0.01) to the depth map to simulate sensor noise\n",
    "\n",
    "    Hints:\n",
    "        - Use torchvision.transforms for color jitter\n",
    "        - For paired spatial transforms, generate random parameters first, then\n",
    "          apply the same params to both modalities\n",
    "        - For language flipping, use simple string replacement\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification: Data Augmentation"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your augmentation\n",
    "rgb_test = torch.rand(3, 224, 224)\n",
    "depth_test = torch.rand(1, 224, 224)\n",
    "lang_test = \"pick up the red bottle on the left\"\n",
    "\n",
    "rgb_aug, depth_aug, lang_aug = augment_observation(rgb_test, depth_test, lang_test)\n",
    "\n",
    "assert rgb_aug.shape == (3, 224, 224), f\"RGB shape mismatch: {rgb_aug.shape}\"\n",
    "assert depth_aug.shape == (1, 224, 224), f\"Depth shape mismatch: {depth_aug.shape}\"\n",
    "assert rgb_aug.min() >= 0.0, \"RGB values must be non-negative\"\n",
    "assert isinstance(lang_aug, str), \"Language must be a string\"\n",
    "print(\"Augmentation tests passed.\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Exploratory Data Analysis\n",
    "\n",
    "**Key distributions to plot:**\n",
    "- Distribution of action magnitudes per joint (are some joints used more than others?)\n",
    "- Distribution of episode lengths (are most episodes short or long?)\n",
    "- Language instruction length distribution and word frequency analysis\n",
    "- Depth map quality histogram (what fraction of pixels have valid depth?)\n",
    "- Gripper mode distribution across the dataset (suction vs. parallel-jaw vs. release)\n",
    "\n",
    "**Anomalies to investigate:**\n",
    "- Episodes with near-zero action variance (robot not moving -- possible data collection errors)\n",
    "- Depth maps with > 30% invalid pixels (sensor occlusion or failure)\n",
    "- Mismatched language instructions (instruction mentions object not visible in frame)"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: EDA Implementation"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dataset_statistics(dataloader: DataLoader) -> dict:\n",
    "    \"\"\"\n",
    "    Compute comprehensive statistics over the entire dataset.\n",
    "\n",
    "    Args:\n",
    "        dataloader: PyTorch DataLoader yielding (rgb, depth, language, actions,\n",
    "                    proprioception) tuples\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with keys:\n",
    "            - \"action_mean\": mean action per joint, shape (7,)\n",
    "            - \"action_std\": std action per joint, shape (7,)\n",
    "            - \"episode_lengths\": list of episode lengths\n",
    "            - \"depth_valid_fraction\": list of valid pixel fractions per sample\n",
    "            - \"num_anomalous_episodes\": count of episodes with action std < 0.01\n",
    "\n",
    "    Hints:\n",
    "        - Use running statistics (Welford's algorithm) to avoid loading all\n",
    "          data into memory\n",
    "        - For depth validity, count pixels where depth > 0.01 (near-zero\n",
    "          depth indicates invalid measurement)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought Questions: EDA\n",
    "\n",
    "1. Which joints have the highest action variance? What does this tell you about the most common manipulation motions?\n",
    "2. What fraction of episodes are \"anomalous\" (near-zero action)? Should these be filtered or kept? What are the tradeoffs?\n",
    "3. Is the depth data quality sufficient for learning, or do you need additional preprocessing?"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Baseline Approach\n",
    "\n",
    "Before building the world model + VLA system, we implement a simple baseline to establish a performance floor.\n",
    "\n",
    "**Baseline: Image-conditioned grasp quality network**\n",
    "\n",
    "Architecture: frozen ResNet-18 visual encoder + 3-layer MLP that predicts (x, y, z, roll, pitch, yaw, gripper_width) for a single grasp pose. No temporal reasoning, no language conditioning, no world model."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implement Baseline Model"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraspBaseline(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple grasp prediction baseline.\n",
    "\n",
    "    Takes an RGB-D image and predicts a single grasp pose.\n",
    "    No language input, no temporal reasoning, no world model.\n",
    "\n",
    "    Architecture:\n",
    "        1. Frozen ResNet-18 backbone (pretrained on ImageNet) processes the\n",
    "           RGB image -> 512-dim feature vector\n",
    "        2. A small CNN (3 conv layers) processes the depth map -> 128-dim\n",
    "           feature vector\n",
    "        3. Concatenate visual features (512 + 128 = 640 dims)\n",
    "        4. MLP: 640 -> 256 -> 128 -> 7 (grasp pose)\n",
    "        5. Output: (x, y, z, roll, pitch, yaw, gripper_width)\n",
    "\n",
    "    Hints:\n",
    "        - Use torchvision.models.resnet18(pretrained=True) and freeze with\n",
    "          requires_grad_(False)\n",
    "        - Remove the final FC layer of ResNet; use the 512-dim avgpool output\n",
    "        - The depth CNN should use (1, 224, 224) input with channels:\n",
    "          1 -> 32 -> 64 -> 128, kernel size 3, stride 2, with batch norm\n",
    "        - Apply tanh to the output to constrain predictions to [-1, 1]\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, rgb: torch.Tensor, depth: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rgb: (B, 3, 224, 224) RGB image\n",
    "            depth: (B, 1, 224, 224) depth map\n",
    "        Returns:\n",
    "            grasp_pose: (B, 7) predicted grasp pose\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Evaluation\n",
    "\n",
    "Train this baseline on the dataset using MSE loss against ground-truth grasp poses and evaluate pick success rate in a PyBullet simulation environment."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Model Design\n",
    "\n",
    "This is the core of the case study. We implement a simplified world model + VLA system inspired by the concepts from the article on World Action Models.\n",
    "\n",
    "**Architecture overview:**\n",
    "\n",
    "The system has three main components:\n",
    "1. **Observation Encoder:** A multimodal encoder that fuses RGB, depth, and language into a unified representation\n",
    "2. **World Model (RSSM):** A recurrent state-space model that predicts future latent states given actions\n",
    "3. **Action Generator (Flow Matching):** A conditional flow model that generates action chunks from noise, conditioned on the observation encoding and world model state"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implement Observation Encoder"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservationEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Multimodal encoder that fuses vision (RGB + depth) and language into a\n",
    "    single latent representation.\n",
    "\n",
    "    Architecture:\n",
    "        1. Visual encoder: Use a frozen DINOv2 ViT-S/14 backbone to encode\n",
    "           the RGB image into patch tokens. Concatenate depth as a 4th channel\n",
    "           via a learned linear projection.\n",
    "           Output: (B, N_patches, 384)\n",
    "\n",
    "        2. Language encoder: Use a frozen CLIP text encoder to get a language\n",
    "           embedding.\n",
    "           Output: (B, 512)\n",
    "\n",
    "        3. Cross-attention fusion: Use 2 layers of cross-attention where\n",
    "           language tokens attend to visual patch tokens. This allows the model\n",
    "           to focus on the part of the image relevant to the instruction.\n",
    "           Output: (B, 256)\n",
    "\n",
    "        4. Final projection: Linear layer to produce the observation embedding.\n",
    "           Output: (B, 256)\n",
    "\n",
    "    Hints:\n",
    "        - For DINOv2, use torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "        - For CLIP, use clip.load(\"ViT-B/32\") and freeze\n",
    "        - In cross-attention, language is the query, visual patches are key/value\n",
    "        - Use nn.MultiheadAttention with 4 heads and embed_dim=384\n",
    "        - Add a learnable [CLS] token to the language side for pooling\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_embed_dim: int = 256):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, rgb: torch.Tensor, depth: torch.Tensor,\n",
    "                language_tokens: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rgb: (B, 3, 224, 224)\n",
    "            depth: (B, 1, 224, 224)\n",
    "            language_tokens: (B, 77) CLIP tokenized text\n",
    "        Returns:\n",
    "            obs_embed: (B, 256) fused observation embedding\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought Question: Observation Encoder\n",
    "\n",
    "Why is cross-attention better than simple concatenation for fusing vision and language? Think about what happens when the instruction says \"pick up the RED bottle\" -- which image patches should receive the most attention?"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implement RSSM World Model"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSSM(nn.Module):\n",
    "    \"\"\"\n",
    "    Recurrent State-Space Model for world modeling.\n",
    "\n",
    "    This is the core world model from DreamerV3, adapted for manipulation.\n",
    "    It maintains both a deterministic recurrent state and a stochastic\n",
    "    latent state.\n",
    "\n",
    "    State components:\n",
    "        - h_t: deterministic state (GRU hidden state), shape (B, 512)\n",
    "        - z_t: stochastic state (categorical), shape (B, 32, 32)\n",
    "          -- 32 categorical variables, each with 32 classes\n",
    "          (flattened to (B, 1024) when needed)\n",
    "\n",
    "    Sub-modules:\n",
    "        1. Deterministic transition: GRU that takes [z_{t-1}, a_{t-1}] as\n",
    "           input and h_{t-1} as hidden state -> h_t\n",
    "\n",
    "        2. Prior (imagination): MLP that predicts z_t distribution from h_t\n",
    "           alone (used during dreaming)\n",
    "           h_t -> MLP(512, 512, 32*32) -> reshape to (B, 32, 32) ->\n",
    "           softmax over last dim\n",
    "\n",
    "        3. Posterior (encoder): MLP that predicts z_t distribution from\n",
    "           [h_t, obs_embed] (used during training with real observations)\n",
    "           [h_t, obs_embed] -> MLP(768, 512, 32*32) -> reshape -> softmax\n",
    "\n",
    "        4. Reward predictor: MLP that predicts expected reward from [h_t, z_t]\n",
    "           [h_t, z_t_flat] -> MLP(1536, 256, 1) -> output scalar\n",
    "\n",
    "    Hints:\n",
    "        - Use nn.GRUCell for the deterministic transition\n",
    "        - For the stochastic state, use straight-through gradients with\n",
    "          Gumbel-Softmax for differentiable sampling\n",
    "        - The KL loss between prior and posterior encourages the prior to\n",
    "          be useful for imagination\n",
    "        - Start with temperature=1.0 for Gumbel-Softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_embed_dim: int = 256, action_dim: int = 10,\n",
    "                 det_dim: int = 512, stoch_dim: int = 32,\n",
    "                 stoch_classes: int = 32):\n",
    "        super().__init__()\n",
    "        self.det_dim = det_dim\n",
    "        self.stoch_dim = stoch_dim\n",
    "        self.stoch_classes = stoch_classes\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def initial_state(self, batch_size: int, device: torch.device) -> tuple:\n",
    "        \"\"\"Return initial (h_0, z_0) with zeros.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def observe_step(self, obs_embed: torch.Tensor, action: torch.Tensor,\n",
    "                     h_prev: torch.Tensor, z_prev: torch.Tensor\n",
    "                     ) -> tuple[torch.Tensor, torch.Tensor, dict]:\n",
    "        \"\"\"\n",
    "        Single step with real observation (training time).\n",
    "\n",
    "        Returns: (h_t, z_t, info_dict)\n",
    "            info_dict contains 'prior_logits', 'posterior_logits' for KL loss\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def imagine_step(self, action: torch.Tensor, h_prev: torch.Tensor,\n",
    "                     z_prev: torch.Tensor\n",
    "                     ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Single step WITHOUT observation (imagination/dreaming).\n",
    "\n",
    "        Returns: (h_t, z_t) using the prior distribution\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def imagine_trajectory(self, initial_h: torch.Tensor,\n",
    "                           initial_z: torch.Tensor,\n",
    "                           actions: torch.Tensor) -> dict:\n",
    "        \"\"\"\n",
    "        Roll out imagination for multiple steps.\n",
    "\n",
    "        Args:\n",
    "            initial_h: (B, 512) starting deterministic state\n",
    "            initial_z: (B, 32, 32) starting stochastic state\n",
    "            actions: (B, T, 10) sequence of actions to imagine\n",
    "\n",
    "        Returns:\n",
    "            dict with 'h_states': (B, T, 512), 'z_states': (B, T, 32, 32),\n",
    "            'rewards': (B, T)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought Question: RSSM World Model\n",
    "\n",
    "Why does the RSSM use BOTH deterministic and stochastic states? Consider what would happen with only deterministic (cannot model uncertainty) or only stochastic (loses long-term memory) states."
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implement Flow Matching Action Generator"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowMatchingActionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Generates action chunks via conditional flow matching.\n",
    "\n",
    "    Instead of predicting actions directly (which produces jerky motions),\n",
    "    this module learns a velocity field that transforms Gaussian noise into\n",
    "    smooth action trajectories. This is the same principle used in pi-0.\n",
    "\n",
    "    Architecture:\n",
    "        - Input: noisy action chunk a_t, timestep t, conditioning c\n",
    "          where c = [h, z_flat, obs_embed] from the world model\n",
    "        - Network: 1D temporal U-Net (or simple MLP with timestep embedding)\n",
    "          For simplicity, use an MLP variant:\n",
    "          1. Embed timestep t using sinusoidal embedding -> (B, 128)\n",
    "          2. Concatenate [a_t_flat, t_embed, c] -> (B, 16*10 + 128 + 1792)\n",
    "          3. MLP: input_dim -> 1024 -> 512 -> 256 -> 16*10\n",
    "          4. Reshape output to (B, 16, 10) -- velocity prediction\n",
    "\n",
    "        - Training: Sample t ~ U(0,1), sample noise a_0 ~ N(0,I),\n",
    "          interpolate a_t = (1-t)*a_0 + t*a_1 (where a_1 is ground truth),\n",
    "          predict velocity v = a_1 - a_0\n",
    "\n",
    "        - Inference: Start from a_0 ~ N(0,I), integrate velocity field\n",
    "          using Euler method with N=10 steps\n",
    "\n",
    "    Hints:\n",
    "        - Sinusoidal embedding: [sin(t*freq), cos(t*freq)] for\n",
    "          freq in geometric series\n",
    "        - During training, the target velocity is simply (a_1 - a_0) --\n",
    "          this is the optimal transport direction\n",
    "        - Use 10 Euler integration steps at inference for good quality\n",
    "          vs. speed tradeoff\n",
    "        - Clip final actions to [-1, 1]\n",
    "    \"\"\"\n",
    "    def __init__(self, action_chunk_size: int = 16, action_dim: int = 10,\n",
    "                 cond_dim: int = 1792):\n",
    "        super().__init__()\n",
    "        self.action_chunk_size = action_chunk_size\n",
    "        self.action_dim = action_dim\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, noisy_actions: torch.Tensor, timestep: torch.Tensor,\n",
    "                condition: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict velocity field.\n",
    "\n",
    "        Args:\n",
    "            noisy_actions: (B, 16, 10) noisy action chunk\n",
    "            timestep: (B,) diffusion timestep in [0, 1]\n",
    "            condition: (B, 1792) conditioning from world model\n",
    "        Returns:\n",
    "            velocity: (B, 16, 10) predicted velocity\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, condition: torch.Tensor, num_steps: int = 10\n",
    "               ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate action chunk from noise via Euler integration.\n",
    "\n",
    "        Args:\n",
    "            condition: (B, 1792) conditioning from world model\n",
    "            num_steps: number of Euler integration steps\n",
    "        Returns:\n",
    "            actions: (B, 16, 10) generated action chunk\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought Question: Flow Matching\n",
    "\n",
    "Why is flow matching preferred over a standard diffusion model (DDPM) for action generation? Think about: (a) the straightness of transport paths, (b) the number of inference steps needed, and (c) the smoothness of generated trajectories."
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Training Strategy\n",
    "\n",
    "**Optimizer:** AdamW with weight decay 1e-4\n",
    "\n",
    "**Learning rate schedule:**\n",
    "- Warmup: linear from 0 to 3e-4 over first 5K steps\n",
    "- Cosine decay from 3e-4 to 1e-5 over remaining training\n",
    "- Total: 200K gradient steps\n",
    "\n",
    "**Regularization:**\n",
    "- Dropout 0.1 in MLPs\n",
    "- Gradient clipping: max norm 100\n",
    "- KL balancing (DreamerV3 style): 80% free nats, clipped KL loss\n",
    "\n",
    "**Training loop structure:**\n",
    "1. Sample a batch of episodes from the dataset\n",
    "2. Encode observations through the observation encoder\n",
    "3. Run the RSSM forward with real observations (observe mode) to get latent states\n",
    "4. Compute world model loss: reconstruction + KL\n",
    "5. Sample random timesteps and noise for flow matching\n",
    "6. Compute flow matching loss on action chunks\n",
    "7. Backpropagate and update all parameters jointly"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implement Training Loop"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model: dict, dataloader: DataLoader,\n",
    "                    optimizer: torch.optim.Optimizer,\n",
    "                    scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "                    device: torch.device, epoch: int) -> dict:\n",
    "    \"\"\"\n",
    "    Train all model components for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model: dict with keys 'encoder', 'rssm', 'flow_head'\n",
    "               (each an nn.Module)\n",
    "        dataloader: yields batches of (rgb, depth, language, actions,\n",
    "                    proprioception, rewards)\n",
    "        optimizer: shared optimizer for all components\n",
    "        scheduler: learning rate scheduler\n",
    "        device: torch device\n",
    "        epoch: current epoch number\n",
    "\n",
    "    Returns:\n",
    "        dict with 'flow_loss', 'wm_loss', 'kl_loss', 'total_loss'\n",
    "        (averaged over epoch)\n",
    "\n",
    "    Implementation steps:\n",
    "        1. For each batch:\n",
    "           a. Move all tensors to device\n",
    "           b. Encode observations: obs_embed = encoder(rgb, depth, language)\n",
    "           c. Initialize RSSM state: h, z = rssm.initial_state(B, device)\n",
    "           d. Loop over timesteps in the episode:\n",
    "              - h, z, info = rssm.observe_step(obs_embed[:, t], actions[:, t], h, z)\n",
    "              - Accumulate KL loss from info['prior_logits'] vs info['posterior_logits']\n",
    "              - Store h, z for action generation\n",
    "           e. Compute world model loss (KL + reward prediction)\n",
    "           f. Sample random t ~ U(0,1) for flow matching\n",
    "           g. Create noisy actions: a_t = (1-t)*noise + t*ground_truth_actions\n",
    "           h. Predict velocity: v_pred = flow_head(a_t, t, condition)\n",
    "           i. Flow loss = MSE(v_pred, ground_truth_actions - noise)\n",
    "           j. Total loss = flow_loss + 0.5 * wm_loss + 0.1 * grasp_loss\n",
    "           k. Backprop and optimizer step\n",
    "\n",
    "        2. Log losses every 100 steps using print or wandb\n",
    "        3. Step the scheduler after each batch\n",
    "\n",
    "    Hints:\n",
    "        - Use torch.distributions.kl_divergence for KL computation between\n",
    "          categorical distributions\n",
    "        - Detach the RSSM hidden state at the start of each episode to prevent\n",
    "          backprop through time explosion\n",
    "        - Use gradient clipping: torch.nn.utils.clip_grad_norm_(params, 100)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Evaluation\n",
    "\n",
    "**Quantitative evaluation:**\n",
    "- Run the trained model in a PyBullet pick-and-place simulation (provided in the notebook)\n",
    "- Evaluate on two object sets: (a) 50 objects seen during training, (b) 50 held-out novel objects\n",
    "- Compute all metrics from the evaluation table in Section 2\n",
    "- Compare against the baseline from Section 3.3\n",
    "\n",
    "| Metric | Target |\n",
    "|---|---|\n",
    "| Pick success rate (novel) | > 90% |\n",
    "| Pick success rate (known) | > 97% |\n",
    "| Cycle time | < 4.0 seconds |\n",
    "| Grasp damage rate | < 0.5% |\n",
    "| Action smoothness (jerk) | < 500 rad/s^3 |\n",
    "| Inference latency | < 200ms |\n",
    "| Collision rate | < 1% |"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implement Evaluation Pipeline"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: dict, eval_env, object_set: list,\n",
    "                   num_episodes: int = 100) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate the full model in simulation.\n",
    "\n",
    "    Args:\n",
    "        model: dict with 'encoder', 'rssm', 'flow_head'\n",
    "        eval_env: PyBullet simulation environment with methods:\n",
    "            - reset(object_name) -> (rgb, depth, instruction)\n",
    "            - step(action) -> (rgb, depth, reward, done, info)\n",
    "        object_set: list of object names to evaluate on\n",
    "        num_episodes: number of episodes per object\n",
    "\n",
    "    Returns:\n",
    "        dict with:\n",
    "            - \"pick_success_rate\": float\n",
    "            - \"mean_cycle_time\": float (seconds)\n",
    "            - \"mean_smoothness\": float (jerk metric)\n",
    "            - \"collision_rate\": float\n",
    "            - \"per_object_success\": dict mapping object name to success rate\n",
    "\n",
    "    Implementation steps:\n",
    "        1. For each object in object_set:\n",
    "           a. Reset environment with that object\n",
    "           b. Initialize RSSM state\n",
    "           c. Loop until done or max 200 steps:\n",
    "              - Encode observation\n",
    "              - Update RSSM state (observe mode)\n",
    "              - Build conditioning vector from RSSM state\n",
    "              - Generate action chunk via flow_head.sample()\n",
    "              - Execute first action from chunk in environment\n",
    "              - Record metrics\n",
    "           d. Log success/failure\n",
    "\n",
    "        2. Aggregate metrics across all objects and episodes\n",
    "        3. Print a summary table\n",
    "\n",
    "    Hints:\n",
    "        - Use action chunk overlap: generate a 16-step chunk but only\n",
    "          execute the first 4 steps, then re-plan. This provides temporal\n",
    "          consistency while allowing adaptation.\n",
    "        - Compute jerk as the finite difference of acceleration:\n",
    "          jerk = np.diff(actions, n=3, axis=0)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Error Analysis\n",
    "\n",
    "Systematically categorize and analyze failure cases from the evaluation. Understanding failure modes is critical for improving the system and for communicating results to stakeholders."
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Failure Mode Analysis"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_failures(eval_results: dict, model: dict, eval_env) -> dict:\n",
    "    \"\"\"\n",
    "    Systematically categorize and analyze failure cases.\n",
    "\n",
    "    From the evaluation results, identify all failed episodes and\n",
    "    categorize them into failure modes.\n",
    "\n",
    "    Expected failure categories:\n",
    "        1. \"grasp_slip\" -- object was contacted but slipped during lift\n",
    "        2. \"miss\" -- gripper did not contact the target object\n",
    "        3. \"wrong_object\" -- grasped a different object than instructed\n",
    "        4. \"collision\" -- collided with bin wall or other objects\n",
    "        5. \"timeout\" -- exceeded maximum steps without completing task\n",
    "\n",
    "    For each category:\n",
    "        a. Count the number of failures\n",
    "        b. Identify common object properties (size, shape, material)\n",
    "           that correlate with this failure mode\n",
    "        c. Visualize 3 example failures (save rgb frames at key moments)\n",
    "        d. Suggest a specific mitigation strategy\n",
    "\n",
    "    Returns:\n",
    "        dict mapping failure category to:\n",
    "            - \"count\": int\n",
    "            - \"fraction\": float\n",
    "            - \"correlated_properties\": list of strings\n",
    "            - \"mitigation\": string describing fix\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought Questions: Error Analysis\n",
    "\n",
    "1. Which failure mode is most common? Is this expected given the model architecture?\n",
    "2. Are there systematic differences in failure rates between suction and parallel-jaw grasps? Why?\n",
    "3. How does failure rate correlate with object size? With transparency? With deformability?\n",
    "4. If you could add ONE additional training signal to reduce failures, what would it be and why?"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Scalability and Deployment Considerations\n",
    "\n",
    "Before deploying this model in a real warehouse, we need to understand its inference performance characteristics and whether it meets the production latency targets."
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Inference Benchmarking"
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model: dict, device: torch.device,\n",
    "                        num_warmup: int = 10, num_runs: int = 100) -> dict:\n",
    "    \"\"\"\n",
    "    Profile inference latency for deployment feasibility.\n",
    "\n",
    "    Measure the wall-clock time for each stage of the inference pipeline:\n",
    "        1. Image preprocessing (resize, normalize)\n",
    "        2. Observation encoding (DINOv2 + CLIP + cross-attention)\n",
    "        3. RSSM state update\n",
    "        4. Flow matching action generation (N=10 Euler steps)\n",
    "        5. Total end-to-end latency\n",
    "\n",
    "    Run each stage independently to isolate bottlenecks.\n",
    "\n",
    "    Returns:\n",
    "        dict with:\n",
    "            - \"preprocess_ms\": mean preprocessing time in ms\n",
    "            - \"encode_ms\": mean encoding time in ms\n",
    "            - \"rssm_ms\": mean RSSM update time in ms\n",
    "            - \"flow_ms\": mean flow matching time in ms\n",
    "            - \"total_ms\": mean total time in ms\n",
    "            - \"throughput_hz\": maximum achievable control frequency\n",
    "            - \"meets_latency_target\": bool (total_ms < 200)\n",
    "\n",
    "    Also compute:\n",
    "        - Model parameter count per component\n",
    "        - GPU memory usage during inference\n",
    "        - Theoretical throughput on NVIDIA L4 vs T4 (Colab) vs A100\n",
    "\n",
    "    Hints:\n",
    "        - Use torch.cuda.synchronize() before timing to ensure accurate\n",
    "          GPU measurements\n",
    "        - Use torch.cuda.Event for precise GPU timing\n",
    "        - Discard warmup runs (first 10) to avoid JIT compilation effects\n",
    "        - Report mean, std, p50, p95, p99 latencies\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought Questions: Scalability\n",
    "\n",
    "1. Which component is the bottleneck? How would you optimize it for production?\n",
    "2. What is the tradeoff between flow matching steps (quality) and latency? Plot this curve.\n",
    "3. Could you use model distillation or quantization to meet the latency target on cheaper hardware?"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Ethical and Regulatory Analysis\n",
    "\n",
    "For each category below, provide 3-5 sentences of analysis.\n",
    "\n",
    "### 1. Labor Displacement\n",
    "- How many warehouse workers could this system displace at MegaMart?\n",
    "- What is the timeline for displacement?\n",
    "- What retraining or transition programs should NovaPick advocate for?\n",
    "- What is the net job impact (jobs displaced vs. new jobs created in robotics maintenance, supervision, ML engineering)?\n",
    "\n",
    "### 2. Safety\n",
    "- What happens when the model makes a confident but wrong prediction?\n",
    "- How should the system handle uncertainty? (When should it ask for human help vs. proceed?)\n",
    "- What are the physical safety risks to humans working near these robots?\n",
    "- How does ISO 10218-1 compliance affect the system design?\n",
    "\n",
    "### 3. Bias and Fairness\n",
    "- The training data comes from specific robot platforms in specific labs. What biases might this introduce?\n",
    "- If the system performs worse on certain product categories (e.g., products marketed to specific demographics), is this a fairness concern?\n",
    "- How would you audit the system for systematic performance disparities across product categories?\n",
    "\n",
    "### 4. Environmental Impact\n",
    "- Estimate the carbon footprint of training (128 A100s for 14 days)\n",
    "- Compare to the carbon savings from more efficient warehouse operations\n",
    "- Is the tradeoff justified?\n",
    "\n",
    "### 5. Dual Use and Misuse\n",
    "- Could this technology be misused for autonomous weapons or surveillance?\n",
    "- What safeguards should NovaPick implement?"
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Write Your Ethical Impact Assessment"
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANALYSIS HERE\n",
    "# Write your ethical impact assessment as a structured analysis (minimum 500 words total).\n",
    "# Replace this cell with a markdown cell containing your analysis, organized by the\n",
    "# five categories above."
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nIn this notebook, you built a simplified version of the NovaPick world model + VLA system for autonomous pick-and-place:\n\n1. **Data (3.1-3.2):** Loaded and explored 15K robot manipulation episodes from DROID and Open X-Embodiment, implementing paired RGB-D augmentation that preserves spatial correspondence.\n\n2. **Baseline (3.3):** Implemented a ResNet-18 + MLP grasp prediction baseline -- no language, no temporal reasoning, no world model. This establishes the performance floor.\n\n3. **Model (3.4):** Built the three core components:\n   - **ObservationEncoder** -- DINOv2 + CLIP with cross-attention fusion for grounded visual understanding\n   - **RSSM** -- deterministic + stochastic state-space model for predicting grasp outcomes in latent space\n   - **FlowMatchingActionHead** -- conditional flow model generating smooth 16-step action chunks\n\n4. **Training (3.5):** Implemented the composite training loop with flow matching loss, RSSM KL loss, and grasp prediction loss, using AdamW with cosine LR schedule.\n\n5. **Evaluation (3.6):** Tested the model in PyBullet simulation on both known and novel objects, measuring pick success rate, cycle time, smoothness, and collision rate.\n\n6. **Error Analysis (3.7):** Categorized failures into 5 modes (grasp slip, miss, wrong object, collision, timeout) and identified object properties correlated with each failure type.\n\n7. **Deployment (3.8):** Benchmarked inference latency per component to validate the 200ms latency target on an NVIDIA L4 GPU.\n\n8. **Ethics (3.9):** Analyzed labor displacement, safety, bias, environmental impact, and dual-use concerns.\n\n**Key takeaway:** The world model enables \"mental rehearsal\" -- predicting grasp outcomes before physical execution -- while the VLA architecture with flow matching generates smooth, language-conditioned trajectories that generalize zero-shot to novel objects. This combination transforms pick-and-place from a per-SKU engineering problem into a general-purpose learned capability.\n\nFor the full case study including production system design, see `case_study.md` in this directory.",
   "id": "cell_45"
  }
 ]
}