{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Inference and Scaling Case Study \u2014 Implementation Notebook"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Optimizing Cloud LLM Inference at Stratos AI\n",
    "## Implementation Notebook\n",
    "\n",
    "---\n",
    "\n",
    "**Scenario:** You are an inference engineer at Stratos AI, a cloud AI platform serving LLM inference for 120+ enterprise clients. Your GPU fleet is hitting memory limits, SLA violations are mounting, and inference costs consume 43% of revenue. Your task is to build the core optimizations -- paged KV cache, GPU-side sampling, continuous batching, and multi-tenant LoRA serving -- that will improve throughput by 3x and restore healthy margins.\n",
    "\n",
    "**Current system:** Naive KV cache with contiguous allocation, CPU-side sampling, static batching. 20 concurrent requests per GPU, 340 tokens/second, 12% SLA violation rate at peak.\n",
    "\n",
    "**Target:** 60+ concurrent requests per GPU, 1200+ tokens/second, < 1% SLA violations.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Baseline Inference Engine\n",
    "\n",
    "We start by building a minimal transformer model and establishing baseline performance metrics. This gives us concrete numbers to improve against."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch matplotlib numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple, Dict\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us define a small transformer model that is large enough to demonstrate the bottlenecks but small enough to run on a free Colab T4 GPU. We use dimensions inspired by a scaled-down version of Stratos's 13B model."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Transformer model configuration (scaled down for Colab).\"\"\"\n",
    "    vocab_size: int = 10000\n",
    "    n_layers: int = 8\n",
    "    n_heads: int = 8\n",
    "    d_model: int = 512\n",
    "    d_ff: int = 2048\n",
    "    max_seq_len: int = 1024\n",
    "    dropout: float = 0.0\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention with optional KV cache.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.n_heads = config.n_heads\n",
    "        self.d_head = config.d_model // config.n_heads\n",
    "        self.d_model = config.d_model\n",
    "\n",
    "        self.W_q = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.W_k = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.W_v = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.W_o = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "\n",
    "    def forward(self, x, kv_cache=None, use_cache=False):\n",
    "        B, T, D = x.shape\n",
    "\n",
    "        q = self.W_q(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        k = self.W_k(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        v = self.W_v(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            # Append new K, V to the cache\n",
    "            cached_k, cached_v = kv_cache\n",
    "            k = torch.cat([cached_k, k], dim=2)\n",
    "            v = torch.cat([cached_v, v], dim=2)\n",
    "\n",
    "        new_cache = (k, v) if use_cache else None\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
    "\n",
    "        # Causal mask\n",
    "        S = k.shape[2]\n",
    "        causal_mask = torch.triu(torch.ones(T, S, device=x.device), diagonal=S - T + 1).bool()\n",
    "        scores = scores.masked_fill(causal_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
    "        out = self.W_o(out)\n",
    "\n",
    "        return out, new_cache\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A single transformer decoder block.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.ln1 = nn.LayerNorm(config.d_model)\n",
    "        self.ln2 = nn.LayerNorm(config.d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(config.d_model, config.d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.d_ff, config.d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, kv_cache=None, use_cache=False):\n",
    "        # Pre-norm architecture\n",
    "        h = self.ln1(x)\n",
    "        attn_out, new_cache = self.attn(h, kv_cache=kv_cache, use_cache=use_cache)\n",
    "        x = x + attn_out\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x, new_cache\n",
    "\n",
    "\n",
    "class MiniLLM(nn.Module):\n",
    "    \"\"\"Minimal GPT-style language model.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_emb = nn.Embedding(config.max_seq_len, config.d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(config.d_model)\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids, kv_caches=None, use_cache=False, start_pos=0):\n",
    "        B, T = input_ids.shape\n",
    "        positions = torch.arange(start_pos, start_pos + T, device=input_ids.device)\n",
    "        x = self.token_emb(input_ids) + self.pos_emb(positions)\n",
    "\n",
    "        new_caches = []\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            cache = kv_caches[i] if kv_caches is not None else None\n",
    "            x, new_cache = block(x, kv_cache=cache, use_cache=use_cache)\n",
    "            new_caches.append(new_cache)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits, new_caches if use_cache else None\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "config = ModelConfig()\n",
    "model = MiniLLM(config).to(device)\n",
    "model.eval()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {total_params:,}\")\n",
    "print(f\"Model memory (fp32): {total_params * 4 / 1e6:.1f} MB\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1: Naive Autoregressive Generation (No KV Cache)\n",
    "\n",
    "Implement generation without any caching. At each step, feed the entire growing sequence through the model. This is the baseline Stratos is currently running (approximately)."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_naive(model, prompt_ids, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Generate tokens WITHOUT KV cache.\n",
    "    At each step, feed the ENTIRE sequence (prompt + generated so far) through the model.\n",
    "\n",
    "    Args:\n",
    "        model: MiniLLM instance\n",
    "        prompt_ids: tensor of shape (1, prompt_len) with token indices\n",
    "        max_new_tokens: number of tokens to generate\n",
    "\n",
    "    Returns:\n",
    "        generated_ids: tensor of all tokens (prompt + generated)\n",
    "        elapsed_time: total generation time in seconds\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    tokens = prompt_ids.clone()\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            # TODO: Feed the ENTIRE tokens sequence through the model (no cache)\n",
    "            # Get logits for the LAST position only\n",
    "            # Use greedy decoding (argmax) to select the next token\n",
    "            # Append the new token to the tokens tensor\n",
    "\n",
    "            # --- YOUR CODE HERE ---\n",
    "            logits, _ = model(tokens, use_cache=False)\n",
    "            next_logit = logits[:, -1, :]\n",
    "            next_token = torch.argmax(next_logit, dim=-1, keepdim=True)\n",
    "            tokens = torch.cat([tokens, next_token], dim=1)\n",
    "            # --- END YOUR CODE ---\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    return tokens, elapsed\n",
    "\n",
    "\n",
    "# Test naive generation\n",
    "prompt = torch.randint(0, config.vocab_size, (1, 64), device=device)\n",
    "output_naive, time_naive = generate_naive(model, prompt, max_new_tokens=50)\n",
    "print(f\"Naive generation: {50 / time_naive:.1f} tokens/sec, {time_naive:.3f}s total\")\n",
    "print(f\"Output shape: {output_naive.shape}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: KV Cache Generation\n",
    "\n",
    "Now implement generation with the KV cache. On the first pass, process the full prompt and cache all K, V tensors. On subsequent passes, feed only the newest token."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_cache(model, prompt_ids, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Generate tokens WITH KV cache.\n",
    "    First pass: process full prompt, cache K/V for all layers.\n",
    "    Subsequent passes: feed only the new token, append to cache.\n",
    "\n",
    "    Args:\n",
    "        model: MiniLLM instance\n",
    "        prompt_ids: tensor of shape (1, prompt_len)\n",
    "        max_new_tokens: number of tokens to generate\n",
    "\n",
    "    Returns:\n",
    "        generated_ids: tensor of all tokens (prompt + generated)\n",
    "        elapsed_time: total generation time in seconds\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    tokens = prompt_ids.clone()\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        # Prefill: process the entire prompt, cache K/V\n",
    "        # TODO: Run the model on the full prompt with use_cache=True\n",
    "        # Store the returned caches for use in the decode loop\n",
    "\n",
    "        # --- YOUR CODE HERE ---\n",
    "        logits, kv_caches = model(tokens, use_cache=True, start_pos=0)\n",
    "        next_logit = logits[:, -1, :]\n",
    "        next_token = torch.argmax(next_logit, dim=-1, keepdim=True)\n",
    "        tokens = torch.cat([tokens, next_token], dim=1)\n",
    "        # --- END YOUR CODE ---\n",
    "\n",
    "        # Decode: generate one token at a time using the cache\n",
    "        for step in range(1, max_new_tokens):\n",
    "            # TODO: Feed ONLY the last token through the model\n",
    "            # Pass the kv_caches and use_cache=True\n",
    "            # Set start_pos to the current sequence length - 1\n",
    "\n",
    "            # --- YOUR CODE HERE ---\n",
    "            pos = tokens.shape[1] - 1\n",
    "            logits, kv_caches = model(\n",
    "                tokens[:, -1:], kv_caches=kv_caches, use_cache=True, start_pos=pos\n",
    "            )\n",
    "            next_logit = logits[:, -1, :]\n",
    "            next_token = torch.argmax(next_logit, dim=-1, keepdim=True)\n",
    "            tokens = torch.cat([tokens, next_token], dim=1)\n",
    "            # --- END YOUR CODE ---\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    return tokens, elapsed\n",
    "\n",
    "\n",
    "# Test cached generation\n",
    "output_cached, time_cached = generate_with_cache(model, prompt, max_new_tokens=50)\n",
    "print(f\"Cached generation: {50 / time_cached:.1f} tokens/sec, {time_cached:.3f}s total\")\n",
    "print(f\"Speedup: {time_naive / time_cached:.2f}x\")\n",
    "\n",
    "# Verify outputs match (both use greedy decoding, so they should be identical)\n",
    "match = torch.equal(output_naive, output_cached)\n",
    "print(f\"Outputs match: {match}\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us benchmark both approaches across different prompt lengths to understand how the speedup scales."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_generation(model, prompt_lengths, max_new_tokens=100, n_trials=3):\n",
    "    \"\"\"Benchmark naive vs cached generation across prompt lengths.\"\"\"\n",
    "    results = {'prompt_len': [], 'naive_tps': [], 'cached_tps': [], 'speedup': []}\n",
    "\n",
    "    for plen in prompt_lengths:\n",
    "        naive_times = []\n",
    "        cached_times = []\n",
    "\n",
    "        for _ in range(n_trials):\n",
    "            prompt = torch.randint(0, config.vocab_size, (1, plen), device=device)\n",
    "\n",
    "            _, t_naive = generate_naive(model, prompt, max_new_tokens=max_new_tokens)\n",
    "            naive_times.append(t_naive)\n",
    "\n",
    "            _, t_cached = generate_with_cache(model, prompt, max_new_tokens=max_new_tokens)\n",
    "            cached_times.append(t_cached)\n",
    "\n",
    "        avg_naive = np.mean(naive_times)\n",
    "        avg_cached = np.mean(cached_times)\n",
    "\n",
    "        results['prompt_len'].append(plen)\n",
    "        results['naive_tps'].append(max_new_tokens / avg_naive)\n",
    "        results['cached_tps'].append(max_new_tokens / avg_cached)\n",
    "        results['speedup'].append(avg_naive / avg_cached)\n",
    "\n",
    "        print(f\"Prompt len {plen:4d}: naive={max_new_tokens/avg_naive:.0f} tok/s, \"\n",
    "              f\"cached={max_new_tokens/avg_cached:.0f} tok/s, \"\n",
    "              f\"speedup={avg_naive/avg_cached:.1f}x\")\n",
    "\n",
    "    return results\n",
    "\n",
    "prompt_lengths = [32, 64, 128, 256, 512]\n",
    "results = benchmark_generation(model, prompt_lengths, max_new_tokens=50)"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the benchmark results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(results['prompt_len'], results['naive_tps'], 'o-', color='#e74c3c', label='Naive (no cache)', linewidth=2)\n",
    "axes[0].plot(results['prompt_len'], results['cached_tps'], 's-', color='#2ecc71', label='With KV cache', linewidth=2)\n",
    "axes[0].set_xlabel('Prompt Length (tokens)')\n",
    "axes[0].set_ylabel('Tokens per Second')\n",
    "axes[0].set_title('Generation Throughput')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].bar(range(len(results['prompt_len'])), results['speedup'], color='#3498db')\n",
    "axes[1].set_xticks(range(len(results['prompt_len'])))\n",
    "axes[1].set_xticklabels(results['prompt_len'])\n",
    "axes[1].set_xlabel('Prompt Length (tokens)')\n",
    "axes[1].set_ylabel('Speedup Factor')\n",
    "axes[1].set_title('KV Cache Speedup vs Naive')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('benchmark_kv_cache.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Paged KV Cache\n",
    "\n",
    "The standard KV cache allocates a contiguous memory block for each request up to the maximum sequence length. At Stratos, this wastes 79% of allocated memory because most responses are far shorter than the 4096-token maximum.\n",
    "\n",
    "The **paged KV cache** allocates memory in fixed-size blocks (pages), growing on demand as the sequence lengthens. When a request completes, its pages are returned to a free pool and immediately available for new requests."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 3: Paged KV Cache Implementation\n",
    "\n",
    "Build a page table and block allocator that manages KV cache memory in fixed-size pages."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PagedKVCache:\n",
    "    \"\"\"\n",
    "    Paged KV cache that allocates memory in fixed-size blocks.\n",
    "\n",
    "    Instead of pre-allocating max_seq_len * d_model for each request,\n",
    "    we allocate pages of page_size tokens as needed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers: int,\n",
    "        n_heads: int,\n",
    "        d_head: int,\n",
    "        page_size: int = 16,\n",
    "        max_pages: int = 512,\n",
    "        device: str = 'cuda',\n",
    "        dtype: torch.dtype = torch.float32,\n",
    "    ):\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_head\n",
    "        self.page_size = page_size\n",
    "        self.max_pages = max_pages\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # TODO: Create the physical page pool\n",
    "        # A single large tensor of shape (max_pages, 2, n_layers, page_size, n_heads, d_head)\n",
    "        # The '2' dimension is for K and V\n",
    "        # Initialize a free list tracking which pages are available\n",
    "\n",
    "        # --- YOUR CODE HERE ---\n",
    "        self.page_pool = torch.zeros(\n",
    "            max_pages, 2, n_layers, page_size, n_heads, d_head,\n",
    "            device=device, dtype=dtype\n",
    "        )\n",
    "        self.free_pages = list(range(max_pages))\n",
    "\n",
    "        # Page tables: request_id -> list of page indices (in order)\n",
    "        self.page_tables: Dict[int, List[int]] = {}\n",
    "        # Track how many tokens are written in the last page of each request\n",
    "        self.seq_lengths: Dict[int, int] = {}\n",
    "        # --- END YOUR CODE ---\n",
    "\n",
    "    def allocate_request(self, request_id: int):\n",
    "        \"\"\"Register a new request. Allocate its first page.\"\"\"\n",
    "        if request_id in self.page_tables:\n",
    "            raise ValueError(f\"Request {request_id} already exists\")\n",
    "        if len(self.free_pages) == 0:\n",
    "            raise RuntimeError(\"Out of pages\")\n",
    "\n",
    "        page_idx = self.free_pages.pop(0)\n",
    "        self.page_tables[request_id] = [page_idx]\n",
    "        self.seq_lengths[request_id] = 0\n",
    "\n",
    "    def append_token(self, request_id: int, layer: int, k: torch.Tensor, v: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Append a single token's K and V to the cache for the given layer.\n",
    "\n",
    "        k, v: shape (n_heads, d_head)\n",
    "\n",
    "        If the current page is full, allocate a new page.\n",
    "        \"\"\"\n",
    "        # TODO: Determine the current position within the last page\n",
    "        # If the page is full, allocate a new page\n",
    "        # Write k and v into the correct position in the page pool\n",
    "\n",
    "        # --- YOUR CODE HERE ---\n",
    "        seq_len = self.seq_lengths[request_id]\n",
    "        page_offset = seq_len % self.page_size\n",
    "\n",
    "        # Need new page?\n",
    "        if page_offset == 0 and seq_len > 0:\n",
    "            if len(self.free_pages) == 0:\n",
    "                raise RuntimeError(\"Out of pages\")\n",
    "            new_page = self.free_pages.pop(0)\n",
    "            self.page_tables[request_id].append(new_page)\n",
    "\n",
    "        pages = self.page_tables[request_id]\n",
    "        current_page = pages[-1]\n",
    "\n",
    "        self.page_pool[current_page, 0, layer, page_offset] = k  # Key\n",
    "        self.page_pool[current_page, 1, layer, page_offset] = v  # Value\n",
    "\n",
    "        if layer == self.n_layers - 1:\n",
    "            # Only increment after the last layer writes\n",
    "            self.seq_lengths[request_id] = seq_len + 1\n",
    "        # --- END YOUR CODE ---\n",
    "\n",
    "    def get_kv(self, request_id: int, layer: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Retrieve all cached K and V for a request at a given layer.\n",
    "        Returns: (k, v) each of shape (seq_len, n_heads, d_head)\n",
    "        \"\"\"\n",
    "        seq_len = self.seq_lengths[request_id]\n",
    "        pages = self.page_tables[request_id]\n",
    "\n",
    "        # Gather from pages\n",
    "        k_parts = []\n",
    "        v_parts = []\n",
    "        tokens_remaining = seq_len\n",
    "\n",
    "        for page_idx in pages:\n",
    "            n_tokens = min(self.page_size, tokens_remaining)\n",
    "            k_parts.append(self.page_pool[page_idx, 0, layer, :n_tokens])\n",
    "            v_parts.append(self.page_pool[page_idx, 1, layer, :n_tokens])\n",
    "            tokens_remaining -= n_tokens\n",
    "\n",
    "        k = torch.cat(k_parts, dim=0)  # (seq_len, n_heads, d_head)\n",
    "        v = torch.cat(v_parts, dim=0)\n",
    "        return k, v\n",
    "\n",
    "    def free_request(self, request_id: int):\n",
    "        \"\"\"Release all pages for a completed request.\"\"\"\n",
    "        if request_id not in self.page_tables:\n",
    "            return\n",
    "        pages = self.page_tables.pop(request_id)\n",
    "        self.free_pages.extend(pages)\n",
    "        del self.seq_lengths[request_id]\n",
    "\n",
    "    def memory_stats(self) -> dict:\n",
    "        \"\"\"Report memory usage statistics.\"\"\"\n",
    "        total_pages = self.max_pages\n",
    "        used_pages = total_pages - len(self.free_pages)\n",
    "        bytes_per_page = 2 * self.n_layers * self.page_size * self.n_heads * self.d_head * 4\n",
    "        return {\n",
    "            'total_pages': total_pages,\n",
    "            'used_pages': used_pages,\n",
    "            'free_pages': len(self.free_pages),\n",
    "            'utilization': used_pages / total_pages,\n",
    "            'used_memory_mb': used_pages * bytes_per_page / 1e6,\n",
    "            'total_memory_mb': total_pages * bytes_per_page / 1e6,\n",
    "        }\n",
    "\n",
    "\n",
    "# Test the paged KV cache\n",
    "paged_cache = PagedKVCache(\n",
    "    n_layers=config.n_layers,\n",
    "    n_heads=config.n_heads,\n",
    "    d_head=config.d_model // config.n_heads,\n",
    "    page_size=16,\n",
    "    max_pages=256,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Simulate 5 concurrent requests with different lengths\n",
    "request_lengths = [47, 128, 15, 200, 83]\n",
    "\n",
    "for req_id, length in enumerate(request_lengths):\n",
    "    paged_cache.allocate_request(req_id)\n",
    "    for t in range(length):\n",
    "        for layer in range(config.n_layers):\n",
    "            k = torch.randn(config.n_heads, config.d_model // config.n_heads, device=device)\n",
    "            v = torch.randn(config.n_heads, config.d_model // config.n_heads, device=device)\n",
    "            paged_cache.append_token(req_id, layer, k, v)\n",
    "\n",
    "stats = paged_cache.memory_stats()\n",
    "print(f\"Pages used: {stats['used_pages']}/{stats['total_pages']}\")\n",
    "print(f\"Memory used: {stats['used_memory_mb']:.1f} MB / {stats['total_memory_mb']:.1f} MB\")\n",
    "print(f\"Utilization: {stats['utilization']:.1%}\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 4: Compare Memory Efficiency -- Contiguous vs Paged\n",
    "\n",
    "Simulate Stratos's workload: 50 concurrent requests with sequence lengths drawn from a log-normal distribution (median 847 tokens)."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_memory_efficiency(n_requests=50, max_seq_len=4096, n_layers=8, n_heads=8, d_head=64):\n",
    "    \"\"\"\n",
    "    Compare memory consumption between contiguous and paged KV cache.\n",
    "\n",
    "    Contiguous: allocates max_seq_len for every request upfront.\n",
    "    Paged: allocates only what is needed, in 16-token pages.\n",
    "    \"\"\"\n",
    "    # TODO: Draw sequence lengths from a log-normal distribution\n",
    "    # with parameters that give median ~847 tokens, clipped to [100, max_seq_len]\n",
    "    # Compute total memory for contiguous (n_requests * max_seq_len * per-token cost)\n",
    "    # Compute total memory for paged (sum of ceil(actual_len / page_size) * page_size * per-token cost)\n",
    "    # Report savings\n",
    "\n",
    "    # --- YOUR CODE HERE ---\n",
    "    # Log-normal: ln(847) ~ 6.74, use sigma=0.6 for reasonable spread\n",
    "    np.random.seed(42)\n",
    "    seq_lengths = np.random.lognormal(mean=np.log(847), sigma=0.6, size=n_requests)\n",
    "    seq_lengths = np.clip(seq_lengths, 100, max_seq_len).astype(int)\n",
    "\n",
    "    bytes_per_token = 2 * n_layers * n_heads * d_head * 4  # K + V, all layers, fp32\n",
    "\n",
    "    # Contiguous: every request reserves max_seq_len\n",
    "    contiguous_bytes = n_requests * max_seq_len * bytes_per_token\n",
    "\n",
    "    # Paged: each request uses ceil(actual_len / page_size) pages\n",
    "    page_size = 16\n",
    "    paged_bytes = 0\n",
    "    for length in seq_lengths:\n",
    "        n_pages = int(np.ceil(length / page_size))\n",
    "        paged_bytes += n_pages * page_size * bytes_per_token\n",
    "\n",
    "    contiguous_gb = contiguous_bytes / 1e9\n",
    "    paged_gb = paged_bytes / 1e9\n",
    "    savings = 1 - paged_gb / contiguous_gb\n",
    "    # --- END YOUR CODE ---\n",
    "\n",
    "    print(f\"Number of requests: {n_requests}\")\n",
    "    print(f\"Sequence length distribution: median={int(np.median(seq_lengths))}, \"\n",
    "          f\"mean={int(np.mean(seq_lengths))}, max={int(np.max(seq_lengths))}\")\n",
    "    print(f\"\\nContiguous allocation: {contiguous_gb:.2f} GB\")\n",
    "    print(f\"Paged allocation:     {paged_gb:.2f} GB\")\n",
    "    print(f\"Memory savings:       {savings:.1%}\")\n",
    "    print(f\"\\nAt 80 GB per A100:\")\n",
    "    print(f\"  Contiguous: can serve {int(80 / (contiguous_gb / n_requests))} concurrent requests\")\n",
    "    print(f\"  Paged:      can serve {int(80 / (paged_gb / n_requests))} concurrent requests\")\n",
    "\n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    axes[0].hist(seq_lengths, bins=30, color='#3498db', alpha=0.7, edgecolor='black')\n",
    "    axes[0].axvline(np.median(seq_lengths), color='#e74c3c', linestyle='--', linewidth=2,\n",
    "                    label=f'Median: {int(np.median(seq_lengths))}')\n",
    "    axes[0].set_xlabel('Sequence Length (tokens)')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Request Sequence Length Distribution')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    bars = axes[1].bar(['Contiguous', 'Paged'], [contiguous_gb, paged_gb],\n",
    "                       color=['#e74c3c', '#2ecc71'], edgecolor='black')\n",
    "    axes[1].set_ylabel('Total Memory (GB)')\n",
    "    axes[1].set_title(f'KV Cache Memory: {n_requests} Concurrent Requests')\n",
    "    for bar, val in zip(bars, [contiguous_gb, paged_gb]):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                     f'{val:.1f} GB', ha='center', fontsize=12, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('memory_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return seq_lengths\n",
    "\n",
    "seq_lengths = compare_memory_efficiency()"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 GPU-Optimized Sampling\n",
    "\n",
    "At Stratos, sampling happens on the CPU after transferring logits from the GPU. This adds 2-8 ms per token. Let us fix that."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 5: Fused GPU Sampling\n",
    "\n",
    "Implement temperature scaling and top-p sampling entirely on the GPU in a single function. No CPU transfers."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gpu_naive(logits, temperature=1.0, top_p=0.9):\n",
    "    \"\"\"\n",
    "    BASELINE: CPU-side sampling (simulating Stratos's current approach).\n",
    "    Transfer logits to CPU, sort, filter, sample, transfer back.\n",
    "    \"\"\"\n",
    "    logits_cpu = logits.cpu().float()\n",
    "    logits_cpu = logits_cpu / temperature\n",
    "    probs = F.softmax(logits_cpu, dim=-1)\n",
    "\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
    "    cumulative = torch.cumsum(sorted_probs, dim=-1)\n",
    "    mask = cumulative - sorted_probs >= top_p\n",
    "    sorted_probs[mask] = 0.0\n",
    "    sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    sampled_idx = torch.multinomial(sorted_probs, num_samples=1)\n",
    "    token = sorted_indices.gather(-1, sampled_idx)\n",
    "    return token.to(logits.device)\n",
    "\n",
    "\n",
    "def sample_gpu_fused(logits, temperature=1.0, top_p=0.9):\n",
    "    \"\"\"\n",
    "    TODO: Implement fused GPU sampling.\n",
    "    All operations stay on GPU -- no .cpu() calls.\n",
    "\n",
    "    Steps:\n",
    "    1. Divide logits by temperature\n",
    "    2. Compute softmax probabilities\n",
    "    3. Sort probabilities descending\n",
    "    4. Compute cumulative sum\n",
    "    5. Mask probabilities beyond top-p threshold\n",
    "    6. Renormalize\n",
    "    7. Sample with torch.multinomial\n",
    "\n",
    "    Args:\n",
    "        logits: shape (batch_size, vocab_size) on GPU\n",
    "        temperature: float > 0\n",
    "        top_p: float in (0, 1]\n",
    "\n",
    "    Returns:\n",
    "        token: shape (batch_size, 1) on GPU\n",
    "    \"\"\"\n",
    "    # --- YOUR CODE HERE ---\n",
    "    logits = logits / temperature\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
    "    cumulative = torch.cumsum(sorted_probs, dim=-1)\n",
    "    mask = cumulative - sorted_probs >= top_p\n",
    "    sorted_probs[mask] = 0.0\n",
    "    sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    sampled_idx = torch.multinomial(sorted_probs, num_samples=1)\n",
    "    token = sorted_indices.gather(-1, sampled_idx)\n",
    "    return token\n",
    "    # --- END YOUR CODE ---\n",
    "\n",
    "\n",
    "# Benchmark: CPU vs GPU sampling\n",
    "vocab_size = 32000  # Realistic vocabulary size\n",
    "batch_size = 1\n",
    "\n",
    "# Warm up\n",
    "dummy_logits = torch.randn(batch_size, vocab_size, device=device)\n",
    "for _ in range(10):\n",
    "    _ = sample_gpu_naive(dummy_logits)\n",
    "    _ = sample_gpu_fused(dummy_logits)\n",
    "\n",
    "# Time CPU sampling\n",
    "n_iters = 200\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(n_iters):\n",
    "    _ = sample_gpu_naive(dummy_logits)\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "cpu_time = (time.time() - start) / n_iters * 1000  # ms\n",
    "\n",
    "# Time GPU sampling\n",
    "start = time.time()\n",
    "for _ in range(n_iters):\n",
    "    _ = sample_gpu_fused(dummy_logits)\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "gpu_time = (time.time() - start) / n_iters * 1000  # ms\n",
    "\n",
    "print(f\"CPU-side sampling: {cpu_time:.2f} ms/token\")\n",
    "print(f\"GPU fused sampling: {gpu_time:.2f} ms/token\")\n",
    "print(f\"Speedup: {cpu_time / gpu_time:.1f}x\")\n",
    "print(f\"\\nOver 200 generated tokens:\")\n",
    "print(f\"  CPU total overhead: {cpu_time * 200:.0f} ms\")\n",
    "print(f\"  GPU total overhead: {gpu_time * 200:.0f} ms\")\n",
    "print(f\"  Savings: {(cpu_time - gpu_time) * 200:.0f} ms\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 6: Compare Sampling Strategies\n",
    "\n",
    "Generate completions under different sampling strategies and compare diversity, latency, and distribution shape."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sampling_strategies(model, prompt, n_samples=50, max_new_tokens=30):\n",
    "    \"\"\"\n",
    "    Generate multiple completions with different sampling strategies.\n",
    "    Compare output diversity and latency.\n",
    "    \"\"\"\n",
    "    strategies = {\n",
    "        'Greedy': {'temperature': 1.0, 'top_p': 1.0, 'greedy': True},\n",
    "        'Temp=0.7': {'temperature': 0.7, 'top_p': 1.0, 'greedy': False},\n",
    "        'Top-k=40': {'temperature': 1.0, 'top_p': 1.0, 'top_k': 40, 'greedy': False},\n",
    "        'Top-p=0.9': {'temperature': 0.8, 'top_p': 0.9, 'greedy': False},\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, params in strategies.items():\n",
    "        all_tokens = []\n",
    "        times = []\n",
    "\n",
    "        for _ in range(n_samples):\n",
    "            tokens = prompt.clone()\n",
    "            start = time.time()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits, kv_caches = model(tokens, use_cache=True, start_pos=0)\n",
    "\n",
    "                for step in range(max_new_tokens):\n",
    "                    last_logits = logits[:, -1, :]\n",
    "\n",
    "                    if params.get('greedy', False):\n",
    "                        next_token = torch.argmax(last_logits, dim=-1, keepdim=True)\n",
    "                    else:\n",
    "                        scaled_logits = last_logits / params['temperature']\n",
    "\n",
    "                        if 'top_k' in params:\n",
    "                            top_k = params['top_k']\n",
    "                            top_vals, _ = torch.topk(scaled_logits, top_k)\n",
    "                            threshold = top_vals[:, -1:]\n",
    "                            scaled_logits[scaled_logits < threshold] = float('-inf')\n",
    "\n",
    "                        next_token = sample_gpu_fused(scaled_logits, temperature=1.0, top_p=params['top_p'])\n",
    "\n",
    "                    tokens = torch.cat([tokens, next_token], dim=1)\n",
    "                    pos = tokens.shape[1] - 1\n",
    "                    logits, kv_caches = model(\n",
    "                        tokens[:, -1:], kv_caches=kv_caches, use_cache=True, start_pos=pos\n",
    "                    )\n",
    "\n",
    "            elapsed = time.time() - start\n",
    "            times.append(elapsed)\n",
    "            all_tokens.append(tokens[0, prompt.shape[1]:].cpu().tolist())\n",
    "\n",
    "        # Compute diversity: number of unique bigrams across all samples\n",
    "        all_bigrams = set()\n",
    "        for seq in all_tokens:\n",
    "            for i in range(len(seq) - 1):\n",
    "                all_bigrams.add((seq[i], seq[i+1]))\n",
    "\n",
    "        unique_sequences = len(set(tuple(seq) for seq in all_tokens))\n",
    "\n",
    "        results[name] = {\n",
    "            'unique_bigrams': len(all_bigrams),\n",
    "            'unique_sequences': unique_sequences,\n",
    "            'avg_latency_ms': np.mean(times) * 1000,\n",
    "            'p99_latency_ms': np.percentile(times, 99) * 1000,\n",
    "        }\n",
    "\n",
    "        print(f\"{name:12s}: {unique_sequences:3d} unique sequences, \"\n",
    "              f\"{len(all_bigrams):5d} unique bigrams, \"\n",
    "              f\"latency={np.mean(times)*1000:.1f}ms avg / {np.percentile(times, 99)*1000:.1f}ms p99\")\n",
    "\n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    names = list(results.keys())\n",
    "    bigrams = [results[n]['unique_bigrams'] for n in names]\n",
    "    latencies = [results[n]['avg_latency_ms'] for n in names]\n",
    "\n",
    "    colors = ['#95a5a6', '#e74c3c', '#f39c12', '#2ecc71']\n",
    "    axes[0].bar(names, bigrams, color=colors, edgecolor='black')\n",
    "    axes[0].set_ylabel('Unique Bigrams')\n",
    "    axes[0].set_title(f'Output Diversity ({n_samples} samples, {max_new_tokens} tokens each)')\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    axes[1].bar(names, latencies, color=colors, edgecolor='black')\n",
    "    axes[1].set_ylabel('Avg Latency (ms)')\n",
    "    axes[1].set_title('Generation Latency per Sample')\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sampling_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return results\n",
    "\n",
    "prompt = torch.randint(0, config.vocab_size, (1, 32), device=device)\n",
    "sampling_results = compare_sampling_strategies(model, prompt, n_samples=30, max_new_tokens=20)"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Continuous Batching\n",
    "\n",
    "Static batching waits for the entire batch to finish before starting a new batch. This means a 10-token response holds up a slot until the slowest 500-token response finishes. Continuous batching inserts and removes requests at every decode iteration."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 7: Continuous Batching Scheduler\n",
    "\n",
    "Build a scheduler that manages a dynamic batch, inserting new requests and removing completed ones at every decode step."
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InferenceRequest:\n",
    "    \"\"\"A single inference request in the queue.\"\"\"\n",
    "    request_id: int\n",
    "    prompt_ids: torch.Tensor  # shape (prompt_len,)\n",
    "    max_new_tokens: int\n",
    "    generated_tokens: list = None\n",
    "    is_complete: bool = False\n",
    "    arrival_time: float = 0.0\n",
    "    first_token_time: float = 0.0\n",
    "    completion_time: float = 0.0\n",
    "    n_generated: int = 0\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.generated_tokens is None:\n",
    "            self.generated_tokens = []\n",
    "\n",
    "\n",
    "class ContinuousBatchScheduler:\n",
    "    \"\"\"\n",
    "    Iteration-level continuous batching scheduler.\n",
    "\n",
    "    At each decode step:\n",
    "    1. Remove completed requests\n",
    "    2. Insert new requests from the waiting queue (up to max_batch_size)\n",
    "    3. Run one decode step for all active requests\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, max_batch_size=16, device='cuda'):\n",
    "        self.model = model\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.device = device\n",
    "\n",
    "        self.waiting_queue: List[InferenceRequest] = []\n",
    "        self.active_batch: List[InferenceRequest] = []\n",
    "        self.completed: List[InferenceRequest] = []\n",
    "\n",
    "        # Per-request KV caches (simplified: store as list of layer caches)\n",
    "        self.request_caches: Dict[int, list] = {}\n",
    "\n",
    "    def add_request(self, request: InferenceRequest):\n",
    "        \"\"\"Add a new request to the waiting queue.\"\"\"\n",
    "        request.arrival_time = time.time()\n",
    "        self.waiting_queue.append(request)\n",
    "\n",
    "    def _prefill_request(self, request: InferenceRequest):\n",
    "        \"\"\"Run prefill for a single request and cache KV.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            input_ids = request.prompt_ids.unsqueeze(0).to(self.device)\n",
    "            logits, kv_caches = self.model(input_ids, use_cache=True, start_pos=0)\n",
    "\n",
    "            # Greedy decode the first token\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1).item()\n",
    "            request.generated_tokens.append(next_token)\n",
    "            request.n_generated = 1\n",
    "            request.first_token_time = time.time()\n",
    "\n",
    "            self.request_caches[request.request_id] = kv_caches\n",
    "\n",
    "    def _decode_step(self):\n",
    "        \"\"\"\n",
    "        Run one decode step for all active requests.\n",
    "\n",
    "        TODO: For each active request, run the model on its latest token,\n",
    "        using its cached KV state. Append the new token. Mark as complete\n",
    "        if max_new_tokens is reached or EOS is generated.\n",
    "        \"\"\"\n",
    "        # --- YOUR CODE HERE ---\n",
    "        to_remove = []\n",
    "\n",
    "        for request in self.active_batch:\n",
    "            if request.is_complete:\n",
    "                continue\n",
    "\n",
    "            last_token = torch.tensor(\n",
    "                [[request.generated_tokens[-1]]], device=self.device\n",
    "            )\n",
    "            pos = len(request.prompt_ids) + request.n_generated\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits, new_caches = self.model(\n",
    "                    last_token,\n",
    "                    kv_caches=self.request_caches[request.request_id],\n",
    "                    use_cache=True,\n",
    "                    start_pos=pos,\n",
    "                )\n",
    "\n",
    "            self.request_caches[request.request_id] = new_caches\n",
    "\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1).item()\n",
    "            request.generated_tokens.append(next_token)\n",
    "            request.n_generated += 1\n",
    "\n",
    "            if request.n_generated >= request.max_new_tokens:\n",
    "                request.is_complete = True\n",
    "                request.completion_time = time.time()\n",
    "                to_remove.append(request)\n",
    "\n",
    "        # Remove completed requests\n",
    "        for request in to_remove:\n",
    "            self.active_batch.remove(request)\n",
    "            self.completed.append(request)\n",
    "            del self.request_caches[request.request_id]\n",
    "        # --- END YOUR CODE ---\n",
    "\n",
    "    def _admit_requests(self):\n",
    "        \"\"\"Move requests from waiting queue to active batch.\"\"\"\n",
    "        while self.waiting_queue and len(self.active_batch) < self.max_batch_size:\n",
    "            request = self.waiting_queue.pop(0)\n",
    "            self._prefill_request(request)\n",
    "            self.active_batch.append(request)\n",
    "\n",
    "    def run(self, max_iterations=5000):\n",
    "        \"\"\"Run the scheduler until all requests are complete or max iterations reached.\"\"\"\n",
    "        for _ in range(max_iterations):\n",
    "            # Admit new requests\n",
    "            self._admit_requests()\n",
    "\n",
    "            if not self.active_batch and not self.waiting_queue:\n",
    "                break\n",
    "\n",
    "            # Run one decode step\n",
    "            self._decode_step()\n",
    "\n",
    "        return self.completed\n",
    "\n",
    "\n",
    "# Test the scheduler\n",
    "scheduler = ContinuousBatchScheduler(model, max_batch_size=8, device=device)\n",
    "\n",
    "# Create 20 requests with varying generation lengths\n",
    "n_test_requests = 20\n",
    "for i in range(n_test_requests):\n",
    "    max_tokens = np.random.randint(10, 80)\n",
    "    prompt = torch.randint(0, config.vocab_size, (32,))\n",
    "    req = InferenceRequest(\n",
    "        request_id=i,\n",
    "        prompt_ids=prompt,\n",
    "        max_new_tokens=max_tokens,\n",
    "    )\n",
    "    scheduler.add_request(req)\n",
    "\n",
    "print(f\"Submitted {n_test_requests} requests to continuous batcher\")\n",
    "completed = scheduler.run()\n",
    "print(f\"Completed: {len(completed)} requests\")\n",
    "\n",
    "total_tokens = sum(r.n_generated for r in completed)\n",
    "total_time = max(r.completion_time for r in completed) - min(r.arrival_time for r in completed)\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Wall clock time: {total_time:.2f}s\")\n",
    "print(f\"Throughput: {total_tokens / total_time:.0f} tokens/sec\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 8: Throughput Under Load -- Static vs Continuous Batching\n",
    "\n",
    "Simulate a realistic Poisson arrival process and compare static batching against continuous batching."
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_static_batching(model, requests, batch_size=8, device='cuda'):\n",
    "    \"\"\"\n",
    "    Static batching: collect a full batch, process all to completion,\n",
    "    then start the next batch.\n",
    "    \"\"\"\n",
    "    completed = []\n",
    "    i = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    while i < len(requests):\n",
    "        batch = requests[i:i + batch_size]\n",
    "\n",
    "        # Process each request independently (worst case for static batching)\n",
    "        for req in batch:\n",
    "            tokens = req.prompt_ids.unsqueeze(0).to(device)\n",
    "            req.first_token_time = time.time()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits, kv_caches = model(tokens, use_cache=True, start_pos=0)\n",
    "                next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "                tokens = torch.cat([tokens, next_token], dim=1)\n",
    "\n",
    "                for step in range(1, req.max_new_tokens):\n",
    "                    pos = tokens.shape[1] - 1\n",
    "                    logits, kv_caches = model(\n",
    "                        tokens[:, -1:], kv_caches=kv_caches, use_cache=True, start_pos=pos\n",
    "                    )\n",
    "                    next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "                    tokens = torch.cat([tokens, next_token], dim=1)\n",
    "\n",
    "            req.n_generated = req.max_new_tokens\n",
    "            req.completion_time = time.time()\n",
    "            req.is_complete = True\n",
    "            completed.append(req)\n",
    "\n",
    "        i += batch_size\n",
    "\n",
    "    return completed\n",
    "\n",
    "\n",
    "def benchmark_batching(model, n_requests=30, batch_size=8, device='cuda'):\n",
    "    \"\"\"Compare static vs continuous batching.\"\"\"\n",
    "    # Create identical request sets\n",
    "    np.random.seed(42)\n",
    "    gen_lengths = np.random.randint(10, 60, size=n_requests)\n",
    "\n",
    "    # Static batching requests\n",
    "    static_requests = []\n",
    "    for i in range(n_requests):\n",
    "        prompt = torch.randint(0, config.vocab_size, (32,))\n",
    "        req = InferenceRequest(request_id=i, prompt_ids=prompt, max_new_tokens=int(gen_lengths[i]))\n",
    "        req.arrival_time = time.time()\n",
    "        static_requests.append(req)\n",
    "\n",
    "    print(\"Running static batching...\")\n",
    "    t0 = time.time()\n",
    "    static_completed = run_static_batching(model, static_requests, batch_size=batch_size, device=device)\n",
    "    static_time = time.time() - t0\n",
    "    static_tokens = sum(r.n_generated for r in static_completed)\n",
    "\n",
    "    # Continuous batching requests\n",
    "    continuous_requests = []\n",
    "    for i in range(n_requests):\n",
    "        prompt = torch.randint(0, config.vocab_size, (32,))\n",
    "        req = InferenceRequest(request_id=i, prompt_ids=prompt, max_new_tokens=int(gen_lengths[i]))\n",
    "        continuous_requests.append(req)\n",
    "\n",
    "    scheduler = ContinuousBatchScheduler(model, max_batch_size=batch_size, device=device)\n",
    "    for req in continuous_requests:\n",
    "        scheduler.add_request(req)\n",
    "\n",
    "    print(\"Running continuous batching...\")\n",
    "    t0 = time.time()\n",
    "    cont_completed = scheduler.run()\n",
    "    cont_time = time.time() - t0\n",
    "    cont_tokens = sum(r.n_generated for r in cont_completed)\n",
    "\n",
    "    print(f\"\\n{'Metric':<30} {'Static':>12} {'Continuous':>12} {'Improvement':>12}\")\n",
    "    print(\"-\" * 68)\n",
    "    print(f\"{'Total tokens':<30} {static_tokens:>12,} {cont_tokens:>12,}\")\n",
    "    print(f\"{'Wall clock time (s)':<30} {static_time:>12.2f} {cont_time:>12.2f}\")\n",
    "    print(f\"{'Throughput (tok/s)':<30} {static_tokens/static_time:>12.0f} \"\n",
    "          f\"{cont_tokens/cont_time:>12.0f} \"\n",
    "          f\"{cont_tokens/cont_time / (static_tokens/static_time):>11.1f}x\")\n",
    "\n",
    "    # Latency analysis\n",
    "    static_ttft = [(r.first_token_time - r.arrival_time) * 1000 for r in static_completed]\n",
    "    cont_ttft = [(r.first_token_time - r.arrival_time) * 1000 for r in cont_completed]\n",
    "\n",
    "    print(f\"\\n{'TTFT p50 (ms)':<30} {np.percentile(static_ttft, 50):>12.0f} \"\n",
    "          f\"{np.percentile(cont_ttft, 50):>12.0f}\")\n",
    "    print(f\"{'TTFT p99 (ms)':<30} {np.percentile(static_ttft, 99):>12.0f} \"\n",
    "          f\"{np.percentile(cont_ttft, 99):>12.0f}\")\n",
    "\n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    bars = axes[0].bar(\n",
    "        ['Static', 'Continuous'],\n",
    "        [static_tokens/static_time, cont_tokens/cont_time],\n",
    "        color=['#e74c3c', '#2ecc71'], edgecolor='black'\n",
    "    )\n",
    "    axes[0].set_ylabel('Tokens / Second')\n",
    "    axes[0].set_title('Throughput: Static vs Continuous Batching')\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    axes[1].boxplot(\n",
    "        [static_ttft, cont_ttft],\n",
    "        labels=['Static', 'Continuous'],\n",
    "        patch_artist=True,\n",
    "        boxprops=[dict(facecolor='#e74c3c', alpha=0.5), dict(facecolor='#2ecc71', alpha=0.5)],\n",
    "    )\n",
    "    axes[1].set_ylabel('Time to First Token (ms)')\n",
    "    axes[1].set_title('TTFT Distribution')\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('batching_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "benchmark_batching(model, n_requests=20, batch_size=8, device=device)"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Multi-Tenant LoRA Serving\n",
    "\n",
    "Stratos serves 120+ clients, many with custom fine-tuned models. LoRA makes this feasible: store one base model in GPU memory and swap lightweight adapters per request."
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 9: LoRA Adapter Manager\n",
    "\n",
    "Implement a LoRA manager that stores multiple adapters and applies the correct one per request."
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAAdapter:\n",
    "    \"\"\"A single LoRA adapter (A and B matrices for target layers).\"\"\"\n",
    "\n",
    "    def __init__(self, target_layers: List[int], d_model: int, rank: int, device='cuda'):\n",
    "        self.target_layers = target_layers\n",
    "        self.rank = rank\n",
    "        self.adapters = {}\n",
    "\n",
    "        for layer_idx in target_layers:\n",
    "            # LoRA for Q and V projections\n",
    "            for proj_name in ['q', 'v']:\n",
    "                key = f\"layer_{layer_idx}_{proj_name}\"\n",
    "                A = torch.randn(rank, d_model, device=device) * 0.01\n",
    "                B = torch.zeros(d_model, rank, device=device)\n",
    "                self.adapters[key] = {'A': A, 'B': B}\n",
    "\n",
    "    def get_delta(self, layer_idx: int, proj_name: str) -> Optional[torch.Tensor]:\n",
    "        \"\"\"Get the low-rank weight update: delta_W = B @ A.\"\"\"\n",
    "        key = f\"layer_{layer_idx}_{proj_name}\"\n",
    "        if key not in self.adapters:\n",
    "            return None\n",
    "        A = self.adapters[key]['A']\n",
    "        B = self.adapters[key]['B']\n",
    "        return B @ A  # (d_model, d_model)\n",
    "\n",
    "    def memory_bytes(self) -> int:\n",
    "        \"\"\"Total memory used by this adapter.\"\"\"\n",
    "        total = 0\n",
    "        for params in self.adapters.values():\n",
    "            total += params['A'].numel() * 4  # fp32\n",
    "            total += params['B'].numel() * 4\n",
    "        return total\n",
    "\n",
    "\n",
    "class LoRAManager:\n",
    "    \"\"\"\n",
    "    Manages multiple LoRA adapters for multi-tenant serving.\n",
    "\n",
    "    TODO: Implement adapter registration, lookup, and application.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_adapters: int = 32):\n",
    "        self.adapters: Dict[str, LoRAAdapter] = {}\n",
    "        self.max_adapters = max_adapters\n",
    "        self.access_order: List[str] = []  # For LRU eviction\n",
    "\n",
    "    def register_adapter(self, client_id: str, adapter: LoRAAdapter):\n",
    "        \"\"\"Register a LoRA adapter for a client.\"\"\"\n",
    "        # --- YOUR CODE HERE ---\n",
    "        if len(self.adapters) >= self.max_adapters and client_id not in self.adapters:\n",
    "            # LRU eviction\n",
    "            evict_id = self.access_order.pop(0)\n",
    "            del self.adapters[evict_id]\n",
    "            print(f\"Evicted adapter for client: {evict_id}\")\n",
    "\n",
    "        self.adapters[client_id] = adapter\n",
    "        if client_id in self.access_order:\n",
    "            self.access_order.remove(client_id)\n",
    "        self.access_order.append(client_id)\n",
    "        # --- END YOUR CODE ---\n",
    "\n",
    "    def get_adapter(self, client_id: str) -> Optional[LoRAAdapter]:\n",
    "        \"\"\"Retrieve adapter for a client, updating LRU order.\"\"\"\n",
    "        if client_id not in self.adapters:\n",
    "            return None\n",
    "        self.access_order.remove(client_id)\n",
    "        self.access_order.append(client_id)\n",
    "        return self.adapters[client_id]\n",
    "\n",
    "    def stats(self) -> dict:\n",
    "        total_memory = sum(a.memory_bytes() for a in self.adapters.values())\n",
    "        return {\n",
    "            'n_adapters': len(self.adapters),\n",
    "            'total_memory_mb': total_memory / 1e6,\n",
    "            'avg_memory_per_adapter_kb': (total_memory / max(len(self.adapters), 1)) / 1e3,\n",
    "        }\n",
    "\n",
    "\n",
    "# Create and register 3 client adapters with different LoRA ranks\n",
    "manager = LoRAManager(max_adapters=32)\n",
    "d_model = config.d_model\n",
    "target_layers = list(range(config.n_layers))\n",
    "\n",
    "clients = {\n",
    "    'fintech_client_a': {'rank': 8, 'description': 'Financial Q&A'},\n",
    "    'legal_client_b': {'rank': 16, 'description': 'Legal document summarization'},\n",
    "    'support_client_c': {'rank': 4, 'description': 'Customer support chatbot'},\n",
    "}\n",
    "\n",
    "for client_id, info in clients.items():\n",
    "    adapter = LoRAAdapter(target_layers, d_model, rank=info['rank'], device=device)\n",
    "    manager.register_adapter(client_id, adapter)\n",
    "    print(f\"Registered {client_id} (rank={info['rank']}): {adapter.memory_bytes()/1e3:.1f} KB\")\n",
    "\n",
    "stats = manager.stats()\n",
    "print(f\"\\nTotal adapters: {stats['n_adapters']}\")\n",
    "print(f\"Total adapter memory: {stats['total_memory_mb']:.2f} MB\")\n",
    "print(f\"Avg per adapter: {stats['avg_memory_per_adapter_kb']:.1f} KB\")\n",
    "\n",
    "# Compare to base model memory\n",
    "base_model_mb = sum(p.numel() * 4 for p in model.parameters()) / 1e6\n",
    "print(f\"\\nBase model memory: {base_model_mb:.1f} MB\")\n",
    "print(f\"Adapter overhead: {stats['total_memory_mb'] / base_model_mb:.2%} of base model\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 10: End-to-End System Benchmark\n",
    "\n",
    "Combine all optimizations and measure the final system performance against the original baseline."
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_to_end_benchmark(model, n_requests=30, device='cuda'):\n",
    "    \"\"\"\n",
    "    Compare the full baseline (naive gen, CPU sampling, static batching)\n",
    "    against the optimized system (KV cache, GPU sampling, continuous batching).\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    gen_lengths = np.random.randint(15, 60, size=n_requests)\n",
    "\n",
    "    # === BASELINE: Naive generation, no optimizations ===\n",
    "    print(\"Running BASELINE (naive generation, static batching)...\")\n",
    "    baseline_start = time.time()\n",
    "    baseline_total_tokens = 0\n",
    "\n",
    "    for i in range(n_requests):\n",
    "        prompt = torch.randint(0, config.vocab_size, (1, 32), device=device)\n",
    "        output, _ = generate_naive(model, prompt, max_new_tokens=int(gen_lengths[i]))\n",
    "        baseline_total_tokens += int(gen_lengths[i])\n",
    "\n",
    "    baseline_time = time.time() - baseline_start\n",
    "    baseline_tps = baseline_total_tokens / baseline_time\n",
    "\n",
    "    # === OPTIMIZED: KV cache + GPU sampling + continuous batching ===\n",
    "    print(\"Running OPTIMIZED (KV cache, GPU sampling, continuous batching)...\")\n",
    "    scheduler = ContinuousBatchScheduler(model, max_batch_size=8, device=device)\n",
    "\n",
    "    for i in range(n_requests):\n",
    "        prompt = torch.randint(0, config.vocab_size, (32,))\n",
    "        req = InferenceRequest(\n",
    "            request_id=i,\n",
    "            prompt_ids=prompt,\n",
    "            max_new_tokens=int(gen_lengths[i]),\n",
    "        )\n",
    "        scheduler.add_request(req)\n",
    "\n",
    "    opt_start = time.time()\n",
    "    completed = scheduler.run()\n",
    "    opt_time = time.time() - opt_start\n",
    "    opt_total_tokens = sum(r.n_generated for r in completed)\n",
    "    opt_tps = opt_total_tokens / opt_time\n",
    "\n",
    "    # Results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{'END-TO-END RESULTS':^60}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\n{'Metric':<35} {'Baseline':>10} {'Optimized':>10} {'Gain':>8}\")\n",
    "    print(\"-\" * 65)\n",
    "    print(f\"{'Total tokens generated':<35} {baseline_total_tokens:>10,} {opt_total_tokens:>10,}\")\n",
    "    print(f\"{'Wall clock time (s)':<35} {baseline_time:>10.2f} {opt_time:>10.2f}\")\n",
    "    print(f\"{'Throughput (tokens/s)':<35} {baseline_tps:>10.0f} {opt_tps:>10.0f} \"\n",
    "          f\"{opt_tps/baseline_tps:>7.1f}x\")\n",
    "\n",
    "    # Stratos projections\n",
    "    print(f\"\\n{'--- Stratos Projection (13B model, A100 80GB) ---':^60}\")\n",
    "    scale_factor = opt_tps / baseline_tps\n",
    "    print(f\"{'Baseline throughput (tok/s/GPU)':<35} {'340':>10}\")\n",
    "    print(f\"{'Projected optimized (tok/s/GPU)':<35} {340 * scale_factor:>10.0f}\")\n",
    "    print(f\"{'Baseline concurrent requests':<35} {'20':>10}\")\n",
    "    print(f\"{'Projected concurrent requests':<35} {20 * scale_factor:>10.0f}\")\n",
    "\n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    categories = ['Throughput\\n(tok/s)', 'Projected Concurrent\\nRequests/GPU']\n",
    "    baseline_vals = [baseline_tps, 20]\n",
    "    optimized_vals = [opt_tps, 20 * scale_factor]\n",
    "\n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "\n",
    "    bars1 = ax.bar(x - width/2, baseline_vals, width, label='Baseline',\n",
    "                   color='#e74c3c', edgecolor='black')\n",
    "    bars2 = ax.bar(x + width/2, optimized_vals, width, label='Optimized',\n",
    "                   color='#2ecc71', edgecolor='black')\n",
    "\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title('End-to-End: Baseline vs Optimized Inference')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(categories)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    for bar in bars1 + bars2:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, height + 0.5,\n",
    "                f'{height:.0f}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('end_to_end_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "end_to_end_benchmark(model, n_requests=20, device=device)"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this implementation notebook, you built the core inference optimizations that Stratos AI needs to restore their SLA compliance and improve margins:\n",
    "\n",
    "1. **KV Cache:** Eliminated redundant computation during autoregressive generation, achieving substantial speedups that scale with prompt length.\n",
    "\n",
    "2. **Paged KV Cache:** Replaced wasteful contiguous memory allocation with demand-paged blocks, reducing memory waste from 79% to near zero and enabling 3x more concurrent requests.\n",
    "\n",
    "3. **GPU-Fused Sampling:** Moved temperature scaling and top-p sampling from CPU to GPU, eliminating per-token transfer overhead that accumulated to seconds over a full generation.\n",
    "\n",
    "4. **Continuous Batching:** Replaced static batching with iteration-level scheduling, ensuring GPU slots are never wasted waiting for the slowest request in a batch.\n",
    "\n",
    "5. **Multi-Tenant LoRA:** Demonstrated serving multiple fine-tuned model variants from a single base model, with adapter memory overhead under 1% of the base model.\n",
    "\n",
    "These optimizations are not theoretical. Every major LLM serving system (vLLM, TensorRT-LLM, TGI) implements variants of these exact techniques. The specific numbers will differ at scale, but the principles -- eliminate redundant computation, allocate memory dynamically, keep the GPU busy, and share base model weights -- are universal."
   ],
   "id": "cell_32"
  }
 ]
}