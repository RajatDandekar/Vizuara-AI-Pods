{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "AI-Powered Skin Lesion Triage with Vision Transformers â€” Implementation Notebook"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: AI-Powered Skin Lesion Triage with Vision Transformers\n",
    "## Implementation Notebook\n",
    "\n",
    "Welcome to the implementation notebook for the SkinSight AI case study. In this notebook, you will build a Vision Transformer (ViT) model for classifying dermoscopic skin lesion images into 8 diagnostic categories.\n",
    "\n",
    "**What you will build:**\n",
    "- A data pipeline for the ISIC 2019 skin lesion dataset\n",
    "- A CNN baseline (ResNet-18) for comparison\n",
    "- A Vision Transformer from scratch, understanding each component\n",
    "- A full training and evaluation pipeline\n",
    "- Inference benchmarking for production readiness\n",
    "- An ethical impact assessment for clinical deployment\n",
    "\n",
    "**Prerequisites:** Familiarity with PyTorch, convolutional neural networks, and the Transformer architecture. Read the accompanying case study document (Sections 1-2) before starting.\n",
    "\n",
    "**Runtime:** This notebook is designed to run on Google Colab with a T4 GPU. Total runtime is approximately 60-90 minutes.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Data Acquisition Strategy\n",
    "\n",
    "We use the **ISIC 2019 Skin Lesion Classification Challenge** dataset, containing 25,331 dermoscopic images across 8 diagnostic categories. This is a real clinical dataset used in international dermatology AI competitions."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://pods.vizuara.ai/courses/vision-transformers-from-scratch/practice/0/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "!pip install -q datasets transformers timm torch torchvision matplotlib seaborn scikit-learn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    balanced_accuracy_score, roc_curve\n",
    ")\n",
    "from collections import Counter\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class names for the ISIC 2019 dataset\n",
    "CLASS_NAMES = [\n",
    "    \"Melanoma\", \"Melanocytic nevus\", \"Basal cell carcinoma\",\n",
    "    \"Actinic keratosis\", \"Benign keratosis\", \"Dermatofibroma\",\n",
    "    \"Vascular lesion\", \"Squamous cell carcinoma\"\n",
    "]\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "# Load the ISIC 2019 dataset from Hugging Face\n",
    "# This may take a few minutes on first run (downloads ~9GB)\n",
    "print(\"Loading ISIC 2019 dataset...\")\n",
    "dataset = load_dataset(\"marmal88/skin_cancer\", split=\"train\")\n",
    "print(f\"Total samples: {len(dataset)}\")\n",
    "print(f\"Features: {dataset.features}\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Data Augmentation Pipeline\n",
    "\n",
    "Dermoscopic images have no canonical orientation -- a lesion can appear at any angle. This means we can apply aggressive geometric augmentations (rotation, flips) without changing the diagnostic label. We also apply color jitter to simulate variations in lighting and camera settings."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transforms(img_size=224):\n",
    "    \"\"\"\n",
    "    Create training and evaluation image transforms for dermoscopic images.\n",
    "\n",
    "    Training transforms should include:\n",
    "    1. Resize to (img_size + 32) x (img_size + 32) for random cropping headroom\n",
    "    2. RandomResizedCrop to img_size x img_size (scale 0.8 to 1.0)\n",
    "    3. RandomHorizontalFlip (p=0.5)\n",
    "    4. RandomVerticalFlip (p=0.5) -- lesions have no canonical orientation\n",
    "    5. RandomRotation (0 to 360 degrees) -- lesions can appear at any angle\n",
    "    6. ColorJitter (brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
    "    7. ToTensor()\n",
    "    8. Normalize with ImageNet mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "\n",
    "    Evaluation transforms should include:\n",
    "    1. Resize to (img_size + 32) x (img_size + 32)\n",
    "    2. CenterCrop to img_size x img_size\n",
    "    3. ToTensor()\n",
    "    4. Normalize with ImageNet statistics\n",
    "\n",
    "    Hints:\n",
    "    - Use torchvision.transforms.Compose to chain transforms\n",
    "    - RandomRotation with expand=False keeps the image size constant\n",
    "    - ColorJitter simulates variations in lighting and camera settings during capture\n",
    "    - Dermoscopic images can be flipped and rotated without changing their diagnosis\n",
    "\n",
    "    Args:\n",
    "        img_size: Target image size (default 224 for ViT-Base)\n",
    "\n",
    "    Returns:\n",
    "        train_transform: Composed transform for training\n",
    "        eval_transform: Composed transform for evaluation\n",
    "    \"\"\"\n",
    "    # TODO: Implement training transforms\n",
    "    train_transform = None  # Your code here\n",
    "\n",
    "    # TODO: Implement evaluation transforms\n",
    "    eval_transform = None  # Your code here\n",
    "\n",
    "    return train_transform, eval_transform\n",
    "\n",
    "# Verification\n",
    "train_tf, eval_tf = create_transforms()\n",
    "assert train_tf is not None, \"Training transform not implemented\"\n",
    "assert eval_tf is not None, \"Evaluation transform not implemented\"\n",
    "print(\"Transforms created successfully.\")"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Wrapper"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISICSkinDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset wrapper for the ISIC skin lesion dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item[\"image\"].convert(\"RGB\")\n",
    "        label = item[\"dx\"]  # diagnostic label (integer)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Weighted Sampler for Class Imbalance\n",
    "\n",
    "The ISIC dataset is severely imbalanced (melanocytic nevus accounts for ~45% of images, while dermatofibroma is less than 2%). Without mitigation, a model could achieve high accuracy by simply predicting the majority class. A weighted sampler ensures that each training batch contains a balanced representation of all classes."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weighted_sampler(dataset_labels):\n",
    "    \"\"\"\n",
    "    Create a WeightedRandomSampler to handle class imbalance.\n",
    "\n",
    "    Steps:\n",
    "    1. Count the frequency of each class in dataset_labels\n",
    "    2. Compute a weight for each class: weight_c = 1.0 / count_c\n",
    "    3. Assign each sample the weight of its class\n",
    "    4. Create a WeightedRandomSampler with these per-sample weights\n",
    "\n",
    "    Hints:\n",
    "    - Use collections.Counter to count class frequencies\n",
    "    - The sampler's num_samples should equal len(dataset_labels) for one full epoch\n",
    "    - replacement=True allows oversampling of rare classes\n",
    "\n",
    "    Args:\n",
    "        dataset_labels: List or array of integer class labels for all samples\n",
    "\n",
    "    Returns:\n",
    "        sampler: torch.utils.data.WeightedRandomSampler\n",
    "    \"\"\"\n",
    "    # TODO: Implement weighted sampler\n",
    "    # Step 1: Count class frequencies\n",
    "\n",
    "    # Step 2: Compute per-class weights\n",
    "\n",
    "    # Step 3: Assign per-sample weights\n",
    "\n",
    "    # Step 4: Create and return WeightedRandomSampler\n",
    "\n",
    "    pass\n",
    "\n",
    "# Verification will happen after dataset split"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Dataset"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train (80%) and validation (20%)\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_tf, eval_tf = create_transforms()\n",
    "\n",
    "full_dataset = ISICSkinDataset(dataset, transform=None)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "# Use generator for reproducibility\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_indices, val_indices = random_split(\n",
    "    range(len(full_dataset)), [train_size, val_size], generator=generator\n",
    ")\n",
    "\n",
    "# Create datasets with appropriate transforms\n",
    "train_dataset = ISICSkinDataset(\n",
    "    dataset.select(train_indices.indices), transform=train_tf\n",
    ")\n",
    "val_dataset = ISICSkinDataset(\n",
    "    dataset.select(val_indices.indices), transform=eval_tf\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Get training labels for the weighted sampler\n",
    "train_labels = [dataset[i][\"dx\"] for i in train_indices.indices]\n",
    "sampler = create_weighted_sampler(train_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Training batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.2 Exploratory Data Analysis\n",
    "\n",
    "Before building any model, we need to understand the data. The class distribution, image characteristics, and visual features will inform our modeling decisions."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class distribution\n",
    "all_labels = [item[\"dx\"] for item in dataset]\n",
    "label_counts = Counter(all_labels)\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "print(\"-\" * 50)\n",
    "for idx, name in enumerate(CLASS_NAMES):\n",
    "    count = label_counts.get(idx, 0)\n",
    "    pct = 100.0 * count / len(all_labels)\n",
    "    print(f\"  {name:30s}: {count:5d} ({pct:5.1f}%)\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Visualize Class Distribution"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(label_counts, class_names):\n",
    "    \"\"\"\n",
    "    Create a horizontal bar chart showing the class distribution.\n",
    "\n",
    "    Requirements:\n",
    "    1. Plot horizontal bars for each class, sorted by count (descending)\n",
    "    2. Color malignant classes (Melanoma, BCC, AK, SCC) in red/orange\n",
    "       and benign classes (NV, BKL, DF, VASC) in blue/green\n",
    "    3. Add count labels at the end of each bar\n",
    "    4. Add a title: \"ISIC 2019 Class Distribution\"\n",
    "    5. Label axes appropriately\n",
    "\n",
    "    Hints:\n",
    "    - Use plt.barh() for horizontal bars\n",
    "    - Malignant class indices: [0, 2, 3, 7] (MEL, BCC, AK, SCC)\n",
    "    - Use different colors for malignant vs benign to highlight the clinical grouping\n",
    "    - The extreme imbalance should be visually obvious\n",
    "\n",
    "    Args:\n",
    "        label_counts: Counter object with class index -> count\n",
    "        class_names: List of class name strings\n",
    "    \"\"\"\n",
    "    # TODO: Implement the class distribution plot\n",
    "    pass\n",
    "\n",
    "plot_class_distribution(label_counts, CLASS_NAMES)"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Visualize Sample Images"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_images(dataset, class_names, samples_per_class=3):\n",
    "    \"\"\"\n",
    "    Display a grid of sample images from each class.\n",
    "\n",
    "    Requirements:\n",
    "    1. Create a grid with NUM_CLASSES rows and samples_per_class columns\n",
    "    2. For each class, randomly select samples_per_class images\n",
    "    3. Display each image with its class name as the row label\n",
    "    4. Set figure size to (samples_per_class * 3, NUM_CLASSES * 3)\n",
    "\n",
    "    Hints:\n",
    "    - Use plt.subplots(NUM_CLASSES, samples_per_class)\n",
    "    - Filter dataset indices by class label to find samples of each class\n",
    "    - Display raw images (no normalization) for visual inspection\n",
    "\n",
    "    Args:\n",
    "        dataset: The Hugging Face dataset object\n",
    "        class_names: List of class name strings\n",
    "        samples_per_class: Number of samples to show per class\n",
    "    \"\"\"\n",
    "    # TODO: Implement the sample image grid\n",
    "    pass\n",
    "\n",
    "plot_sample_images(dataset, CLASS_NAMES)"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought Questions\n",
    "\n",
    "1. How severe is the class imbalance? What is the ratio between the most common and rarest class? How might this affect model training without any mitigation?\n",
    "2. Looking at the sample images, what visual features distinguish melanoma from benign nevi? Can you identify features that would require global context (comparing distant regions of the image) versus purely local features?\n",
    "3. If you trained a model that always predicted the most common class, what would its accuracy be? What would its balanced accuracy be? Why is balanced accuracy a better metric for this dataset?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3 Baseline Approach\n",
    "\n",
    "We establish a CNN baseline using ResNet-18, a well-understood convolutional architecture. This gives us a performance floor and helps us quantify how much the Vision Transformer improves.\n",
    "\n",
    "### TODO: Create a CNN Baseline"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "def create_baseline_model(num_classes=8, pretrained=True):\n",
    "    \"\"\"\n",
    "    Create a pretrained ResNet-18 baseline model for skin lesion classification.\n",
    "\n",
    "    Steps:\n",
    "    1. Load a pretrained ResNet-18 using the timm library\n",
    "    2. Replace the classification head with a new Linear layer for num_classes\n",
    "    3. Move the model to the appropriate device\n",
    "\n",
    "    Hints:\n",
    "    - Use timm.create_model('resnet18', pretrained=True, num_classes=num_classes)\n",
    "    - timm handles the head replacement automatically when you specify num_classes\n",
    "    - This model has ~11M parameters (much smaller than ViT-Base's 86M)\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of output classes\n",
    "        pretrained: Whether to use ImageNet-pretrained weights\n",
    "\n",
    "    Returns:\n",
    "        model: The ResNet-18 model on the correct device\n",
    "    \"\"\"\n",
    "    # TODO: Create and return the baseline model\n",
    "    pass\n",
    "\n",
    "baseline_model = create_baseline_model()\n",
    "total_params = sum(p.numel() for p in baseline_model.parameters())\n",
    "print(f\"Baseline parameters: {total_params / 1e6:.1f}M\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Training and Evaluation Functions\n",
    "\n",
    "These functions will be reused for both the baseline and the ViT model."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "\n",
    "    Steps:\n",
    "    1. Set model to training mode\n",
    "    2. Iterate over batches in the dataloader\n",
    "    3. For each batch: forward pass, compute loss, backward pass, optimizer step\n",
    "    4. Track running loss and correct predictions\n",
    "\n",
    "    Hints:\n",
    "    - Remember to zero gradients before each backward pass\n",
    "    - Use torch.argmax(outputs, dim=1) to get predicted classes\n",
    "    - Return average loss and accuracy for the epoch\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        dataloader: Training DataLoader\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        device: torch device\n",
    "\n",
    "    Returns:\n",
    "        avg_loss: Average loss over the epoch\n",
    "        accuracy: Training accuracy (correct / total)\n",
    "    \"\"\"\n",
    "    # TODO: Implement the training loop for one epoch\n",
    "    pass\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset.\n",
    "\n",
    "    Steps:\n",
    "    1. Set model to evaluation mode\n",
    "    2. Disable gradient computation (torch.no_grad)\n",
    "    3. Iterate over batches, collecting predictions and true labels\n",
    "    4. Compute loss, accuracy, and balanced accuracy\n",
    "\n",
    "    Hints:\n",
    "    - Use torch.no_grad() context manager for efficiency\n",
    "    - Collect all predictions and labels for computing balanced accuracy\n",
    "    - Use sklearn.metrics.balanced_accuracy_score\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        dataloader: Evaluation DataLoader\n",
    "        criterion: Loss function\n",
    "        device: torch device\n",
    "\n",
    "    Returns:\n",
    "        avg_loss: Average loss\n",
    "        accuracy: Overall accuracy\n",
    "        balanced_acc: Balanced accuracy across classes\n",
    "        all_preds: List of all predicted class indices\n",
    "        all_labels: List of all true class labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement the evaluation function\n",
    "    pass"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Baseline"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline for 5 epochs (quick training to establish a floor)\n",
    "# Set up the training pipeline:\n",
    "# 1. Define criterion: CrossEntropyLoss with class weights\n",
    "# 2. Define optimizer: AdamW with lr=1e-4, weight_decay=0.01\n",
    "# 3. Train for 5 epochs, printing train loss, val loss, and val balanced accuracy\n",
    "\n",
    "def compute_class_weights(labels, num_classes=8):\n",
    "    \"\"\"\n",
    "    Compute inverse-frequency class weights for weighted cross-entropy loss.\n",
    "    Formula: w_c = N_total / (num_classes * N_c)\n",
    "    \"\"\"\n",
    "    counts = Counter(labels)\n",
    "    total = len(labels)\n",
    "    weights = torch.FloatTensor([\n",
    "        total / (num_classes * counts.get(i, 1)) for i in range(num_classes)\n",
    "    ])\n",
    "    return weights.to(device)\n",
    "\n",
    "class_weights = compute_class_weights(train_labels)\n",
    "print(\"Class weights:\", {CLASS_NAMES[i]: f\"{w:.2f}\" for i, w in enumerate(class_weights)})\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.AdamW(baseline_model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "print(\"\\nTraining ResNet-18 Baseline (5 epochs)...\")\n",
    "print(\"-\" * 65)\n",
    "baseline_history = {'train_loss': [], 'val_loss': [], 'val_balanced_acc': []}\n",
    "\n",
    "for epoch in range(5):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        baseline_model, train_loader, criterion, optimizer, device\n",
    "    )\n",
    "    val_loss, val_acc, val_bal_acc, val_preds, val_labels = evaluate(\n",
    "        baseline_model, val_loader, criterion, device\n",
    "    )\n",
    "    baseline_history['train_loss'].append(train_loss)\n",
    "    baseline_history['val_loss'].append(val_loss)\n",
    "    baseline_history['val_balanced_acc'].append(val_bal_acc)\n",
    "    print(f\"Epoch {epoch+1}/5 | Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Balanced Acc: {val_bal_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nBaseline best balanced accuracy: {max(baseline_history['val_balanced_acc']):.4f}\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.4 Model Design: Vision Transformer\n",
    "\n",
    "Now we build the Vision Transformer from scratch. Each component connects back to the mathematical foundations described in the article.\n",
    "\n",
    "Recall the three key stages:\n",
    "1. **Patch Embedding**: Divide image into 16x16 patches, flatten, project to D dimensions, add [CLS] token and position embeddings\n",
    "2. **Transformer Encoder**: Stack of L blocks, each with Multi-Head Self-Attention + MLP, both with pre-norm and residual connections\n",
    "3. **Classification Head**: Extract [CLS] token, apply LayerNorm, project to num_classes\n",
    "\n",
    "### TODO: Implement Patch Embedding\n",
    "\n",
    "The core insight of ViT: reshape an image into a sequence of patches and treat each patch as a token, just like words in a sentence."
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert an image into a sequence of patch embeddings.\n",
    "\n",
    "    Architecture:\n",
    "    1. Use nn.Conv2d with kernel_size=patch_size and stride=patch_size to extract\n",
    "       and project patches in one operation (equivalent to flatten + linear projection)\n",
    "    2. Create a learnable [CLS] token parameter of shape (1, 1, embed_dim)\n",
    "    3. Create learnable position embeddings of shape (1, num_patches + 1, embed_dim)\n",
    "\n",
    "    Forward pass:\n",
    "    1. Apply the Conv2d projection: (B, 3, 224, 224) -> (B, embed_dim, 14, 14)\n",
    "    2. Flatten spatial dims and transpose: -> (B, 196, embed_dim)\n",
    "    3. Expand [CLS] token to batch size and prepend: -> (B, 197, embed_dim)\n",
    "    4. Add position embeddings\n",
    "    5. Return the sequence of 197 embedded tokens\n",
    "\n",
    "    Hints:\n",
    "    - nn.Conv2d(in_channels, embed_dim, kernel_size=P, stride=P) extracts PxP patches\n",
    "    - Use .flatten(2).transpose(1, 2) to reshape Conv2d output to (B, N, D)\n",
    "    - Initialize [CLS] token and position embeddings with nn.Parameter(torch.randn(...))\n",
    "    - The [CLS] token must be expanded to match the batch dimension using .expand(B, -1, -1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # TODO: Implement the three components:\n",
    "        # 1. self.projection = nn.Conv2d(...)\n",
    "        # 2. self.cls_token = nn.Parameter(...)\n",
    "        # 3. self.position_embeddings = nn.Parameter(...)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # TODO: Implement the forward pass\n",
    "        # 1. Project patches\n",
    "        # 2. Reshape to (B, num_patches, embed_dim)\n",
    "        # 3. Prepend [CLS] token\n",
    "        # 4. Add position embeddings\n",
    "        # 5. Return\n",
    "\n",
    "        pass\n",
    "\n",
    "# Verification\n",
    "patch_embed = PatchEmbedding().to(device)\n",
    "test_input = torch.randn(2, 3, 224, 224).to(device)\n",
    "test_output = patch_embed(test_input)\n",
    "assert test_output.shape == (2, 197, 768), f\"Expected (2, 197, 768), got {test_output.shape}\"\n",
    "print(f\"PatchEmbedding output shape: {test_output.shape} -- correct!\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implement a Transformer Block\n",
    "\n",
    "Each block performs two operations with residual connections:\n",
    "- Multi-Head Self-Attention: every patch attends to every other patch\n",
    "- MLP: two-layer feed-forward network with GELU activation\n",
    "\n",
    "Pre-norm (LayerNorm before sub-layer) is used, following the ViT paper."
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer encoder block.\n",
    "\n",
    "    Architecture:\n",
    "    1. LayerNorm -> Multi-Head Self-Attention -> Residual Add\n",
    "    2. LayerNorm -> MLP (Linear -> GELU -> Dropout -> Linear -> Dropout) -> Residual Add\n",
    "\n",
    "    The MLP inner dimension is embed_dim * mlp_ratio (default 4x expansion).\n",
    "\n",
    "    Hints:\n",
    "    - Use nn.LayerNorm(embed_dim) for layer normalization\n",
    "    - Use nn.MultiheadAttention(embed_dim, num_heads, dropout, batch_first=True)\n",
    "    - The residual connection adds the INPUT to the OUTPUT of each sub-layer\n",
    "    - Pre-norm means LayerNorm is applied BEFORE the sub-layer, not after\n",
    "    - GELU activation: nn.GELU()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Implement the following components:\n",
    "        # self.ln1 = ...        (LayerNorm for attention)\n",
    "        # self.attn = ...       (MultiheadAttention)\n",
    "        # self.ln2 = ...        (LayerNorm for MLP)\n",
    "        # self.mlp = nn.Sequential(...)  (two-layer MLP with GELU)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass with pre-norm and residual connections\n",
    "        # 1. Apply LN1, then self-attention, then add residual\n",
    "        # 2. Apply LN2, then MLP, then add residual\n",
    "        pass\n",
    "\n",
    "# Verification\n",
    "block = TransformerBlock().to(device)\n",
    "test_tokens = torch.randn(2, 197, 768).to(device)\n",
    "block_output = block(test_tokens)\n",
    "assert block_output.shape == test_tokens.shape, f\"Shape mismatch: {block_output.shape}\"\n",
    "print(f\"TransformerBlock output shape: {block_output.shape} -- correct!\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Assemble the Full Vision Transformer"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    The complete Vision Transformer for image classification.\n",
    "\n",
    "    Architecture:\n",
    "    1. PatchEmbedding layer (image -> sequence of patch tokens)\n",
    "    2. Dropout\n",
    "    3. Stack of 'depth' TransformerBlocks\n",
    "    4. LayerNorm on the [CLS] token output\n",
    "    5. Linear classification head ([CLS] token -> num_classes logits)\n",
    "\n",
    "    The forward pass:\n",
    "    1. Embed patches (including [CLS] token and position embeddings)\n",
    "    2. Apply dropout\n",
    "    3. Pass through all Transformer blocks\n",
    "    4. Extract the [CLS] token (index 0 in the sequence dimension)\n",
    "    5. Apply LayerNorm\n",
    "    6. Pass through the classification head\n",
    "\n",
    "    Hints:\n",
    "    - Stack blocks using nn.Sequential(*[TransformerBlock(...) for _ in range(depth)])\n",
    "    - The [CLS] token is at position 0: x[:, 0] extracts it from all batches\n",
    "    - The classification head is a single nn.Linear(embed_dim, num_classes)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, img_size=224, patch_size=16, in_channels=3,\n",
    "        num_classes=8, embed_dim=768, depth=12,\n",
    "        num_heads=12, mlp_ratio=4.0, dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Implement the ViT components:\n",
    "        # self.patch_embed = ...\n",
    "        # self.dropout = ...\n",
    "        # self.blocks = nn.Sequential(...)\n",
    "        # self.ln = ...\n",
    "        # self.head = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass\n",
    "        pass\n",
    "\n",
    "# Verification\n",
    "vit = VisionTransformer(num_classes=NUM_CLASSES).to(device)\n",
    "test_img = torch.randn(2, 3, 224, 224).to(device)\n",
    "logits = vit(test_img)\n",
    "assert logits.shape == (2, NUM_CLASSES), f\"Expected (2, {NUM_CLASSES}), got {logits.shape}\"\n",
    "total_params = sum(p.numel() for p in vit.parameters())\n",
    "print(f\"ViT output shape: {logits.shape} -- correct!\")\n",
    "print(f\"Total parameters: {total_params / 1e6:.1f}M\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought Questions\n",
    "\n",
    "1. Why does ViT use a Conv2d for patch projection instead of actually flattening patches and multiplying by a weight matrix? (Hint: think about computational efficiency and memory layout.)\n",
    "2. What would happen if we removed position embeddings entirely? The model would treat the image as a \"bag of patches\" with no spatial information. How would this affect classification of skin lesions?\n",
    "3. The [CLS] token attends to all 196 patches through all 12 layers. Calculate the total number of attention operations the [CLS] token participates in across all layers and heads. Why is this important for building a holistic image representation?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.5 Training Strategy\n",
    "\n",
    "### Loading Pretrained Weights\n",
    "\n",
    "Training ViT from scratch on 25K images would fail due to overfitting. We use ImageNet-21k pretrained weights from the timm library, which provides strong visual features learned from 14 million images."
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "def create_vit_model(num_classes=8, pretrained=True):\n",
    "    \"\"\"\n",
    "    Create a ViT-Base model with pretrained ImageNet-21k weights.\n",
    "    We use timm to load pretrained weights, then replace the classification head.\n",
    "    \"\"\"\n",
    "    model = timm.create_model(\n",
    "        'vit_base_patch16_224',\n",
    "        pretrained=pretrained,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    return model.to(device)\n",
    "\n",
    "pretrained_vit = create_vit_model()\n",
    "pretrained_params = sum(p.numel() for p in pretrained_vit.parameters())\n",
    "print(f\"Pretrained ViT-Base parameters: {pretrained_params / 1e6:.1f}M\")"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Optimizer and Learning Rate Scheduler"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer_and_scheduler(model, lr=1e-4, weight_decay=0.05, warmup_epochs=5, total_epochs=30):\n",
    "    \"\"\"\n",
    "    Create an AdamW optimizer and a cosine learning rate scheduler with warmup.\n",
    "\n",
    "    Optimizer: AdamW\n",
    "    - lr: Peak learning rate (reached after warmup)\n",
    "    - weight_decay: 0.05 (standard for ViT fine-tuning)\n",
    "    - betas: (0.9, 0.999)\n",
    "\n",
    "    Scheduler: Linear warmup followed by cosine decay\n",
    "    - Warmup: linearly increase lr from 0 to peak over warmup_epochs\n",
    "    - Cosine: decay lr from peak to 0 over remaining epochs\n",
    "\n",
    "    Why AdamW over SGD?\n",
    "    - ViT training is sensitive to optimizer choice. Adam-family optimizers\n",
    "      provide per-parameter adaptive learning rates, which is important because\n",
    "      ViT has parameters with very different gradient scales (position embeddings\n",
    "      vs. attention weights vs. MLP weights).\n",
    "\n",
    "    Why cosine schedule with warmup?\n",
    "    - Warmup prevents the randomly initialized classification head from causing\n",
    "      large gradient updates that destabilize the pretrained features.\n",
    "    - Cosine decay smoothly reduces the learning rate, allowing fine-grained\n",
    "      optimization in later epochs.\n",
    "\n",
    "    Hints:\n",
    "    - Use torch.optim.AdamW for the optimizer\n",
    "    - Use torch.optim.lr_scheduler.CosineAnnealingLR for cosine decay\n",
    "    - For warmup, use torch.optim.lr_scheduler.LinearLR with start_factor=0.01\n",
    "    - Chain them with torch.optim.lr_scheduler.SequentialLR\n",
    "\n",
    "    Returns:\n",
    "        optimizer, scheduler\n",
    "    \"\"\"\n",
    "    # TODO: Implement optimizer and scheduler\n",
    "    pass\n",
    "\n",
    "optimizer, scheduler = create_optimizer_and_scheduler(pretrained_vit)\n",
    "assert optimizer is not None, \"Optimizer not created\"\n",
    "assert scheduler is not None, \"Scheduler not created\"\n",
    "print(\"Optimizer and scheduler created successfully.\")"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Full Training Loop"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vit(model, train_loader, val_loader, num_epochs=30, lr=1e-4):\n",
    "    \"\"\"\n",
    "    Full training loop for the Vision Transformer.\n",
    "\n",
    "    For each epoch:\n",
    "    1. Train on all batches (forward, loss, backward, step, scheduler step)\n",
    "    2. Evaluate on validation set\n",
    "    3. Track and print: train loss, val loss, val balanced accuracy, learning rate\n",
    "    4. Save the best model (by validation balanced accuracy)\n",
    "\n",
    "    Include:\n",
    "    - Class-weighted cross-entropy loss\n",
    "    - Gradient clipping (max_norm=1.0) to prevent exploding gradients\n",
    "    - Model checkpointing (save best model state_dict)\n",
    "\n",
    "    Why gradient clipping?\n",
    "    - Self-attention can produce large gradient norms, especially in early training\n",
    "      when the model is adapting pretrained features to a new domain.\n",
    "\n",
    "    Hints:\n",
    "    - Use torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    - Track best_val_balanced_acc and save model when it improves\n",
    "    - Print a formatted table of metrics for each epoch\n",
    "    - Return training history (lists of losses and metrics) for plotting\n",
    "\n",
    "    Returns:\n",
    "        history: Dict with keys 'train_loss', 'val_loss', 'val_balanced_acc', 'lr'\n",
    "    \"\"\"\n",
    "    # TODO: Implement the full training loop\n",
    "    pass"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the ViT"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 30 epochs (adjust num_epochs for Colab time constraints)\n",
    "# On a T4 GPU, 30 epochs takes approximately 45-60 minutes\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "print(f\"\\nTraining ViT-Base ({NUM_EPOCHS} epochs)...\")\n",
    "print(\"=\" * 65)\n",
    "vit_history = train_vit(pretrained_vit, train_loader, val_loader, num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(vit_history['train_loss'], label='Train')\n",
    "axes[0].plot(vit_history['val_loss'], label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(vit_history['val_balanced_acc'])\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Balanced Accuracy')\n",
    "axes[1].set_title('Validation Balanced Accuracy')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(vit_history['lr'])\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Learning Rate')\n",
    "axes[2].set_title('Learning Rate Schedule')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest validation balanced accuracy: {max(vit_history['val_balanced_acc']):.4f}\")"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.6 Evaluation\n",
    "\n",
    "Now we evaluate the trained ViT model and compare it against the ResNet-18 baseline.\n",
    "\n",
    "### TODO: Comprehensive Evaluation"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_evaluation(model, dataloader, class_names, device):\n",
    "    \"\"\"\n",
    "    Perform a comprehensive evaluation of the trained model.\n",
    "\n",
    "    Generate the following:\n",
    "    1. Classification report (precision, recall, F1 per class)\n",
    "    2. Confusion matrix heatmap\n",
    "    3. Per-class sensitivity bar chart (highlighting melanoma and other malignant classes)\n",
    "    4. ROC curves for each class (one-vs-rest) with AUC values\n",
    "    5. Macro-averaged AUROC\n",
    "\n",
    "    Hints:\n",
    "    - Use sklearn.metrics.classification_report with output_dict=True\n",
    "    - Use sklearn.metrics.confusion_matrix and seaborn.heatmap\n",
    "    - For ROC curves, you need predicted probabilities (apply softmax to logits)\n",
    "    - Use sklearn.metrics.roc_curve and roc_auc_score with multi_class='ovr'\n",
    "    - Create a 2x2 subplot figure for all visualizations\n",
    "\n",
    "    Returns:\n",
    "        results: Dict with 'balanced_accuracy', 'macro_auroc', 'melanoma_sensitivity',\n",
    "                 'per_class_sensitivity'\n",
    "    \"\"\"\n",
    "    # TODO: Implement comprehensive evaluation\n",
    "    pass\n",
    "\n",
    "# Evaluate the trained ViT\n",
    "print(\"Evaluating ViT-Base...\")\n",
    "vit_results = comprehensive_evaluation(pretrained_vit, val_loader, CLASS_NAMES, device)"
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Baseline vs. ViT Comparison"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_baseline_vs_vit(baseline_results, vit_results, class_names):\n",
    "    \"\"\"\n",
    "    Create a side-by-side comparison of baseline (ResNet-18) vs ViT results.\n",
    "\n",
    "    Generate:\n",
    "    1. A grouped bar chart comparing per-class sensitivity for both models\n",
    "    2. A summary table with overall metrics (balanced accuracy, macro AUROC, melanoma sensitivity)\n",
    "    3. Print the improvement percentage for each metric\n",
    "\n",
    "    Hints:\n",
    "    - Use matplotlib grouped bar chart with two bars per class\n",
    "    - Highlight the melanoma class in a different color\n",
    "    - Include percentage improvement annotations\n",
    "    \"\"\"\n",
    "    # TODO: Implement the comparison visualization\n",
    "    pass\n",
    "\n",
    "# Run the comparison (requires baseline_results from Section 3.3)\n",
    "# If you skipped baseline evaluation, evaluate the baseline first:\n",
    "# baseline_results = comprehensive_evaluation(baseline_model, val_loader, CLASS_NAMES, device)\n",
    "# compare_baseline_vs_vit(baseline_results, vit_results, CLASS_NAMES)"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought Questions\n",
    "\n",
    "1. Which classes does the ViT improve on most compared to the CNN baseline? Is there a pattern (e.g., classes with more spatial complexity)?\n",
    "2. Look at the confusion matrix. What are the most common misclassification pairs? Do these make clinical sense (e.g., melanoma confused with nevus is a known diagnostic challenge)?\n",
    "3. How calibrated are the model's probability outputs? If the model predicts 80% melanoma, is the true positive rate close to 80%? Why does calibration matter for clinical deployment?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.7 Error Analysis\n",
    "\n",
    "Understanding where and why the model fails is as important as measuring its accuracy. Systematic error analysis reveals failure patterns that inform model improvement.\n",
    "\n",
    "### TODO: Systematic Error Analysis"
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(model, dataloader, class_names, device, num_examples=5):\n",
    "    \"\"\"\n",
    "    Perform systematic error analysis on misclassified samples.\n",
    "\n",
    "    Steps:\n",
    "    1. Collect all misclassified samples with their predicted and true labels\n",
    "    2. Group errors by (true_class, predicted_class) pair\n",
    "    3. For the top 3 most frequent error pairs:\n",
    "       a. Display num_examples misclassified images\n",
    "       b. Show the model's full probability distribution for each\n",
    "       c. Identify common visual patterns in the errors\n",
    "    4. Compute error rates stratified by predicted confidence:\n",
    "       - High confidence errors (predicted probability > 0.8): most dangerous\n",
    "       - Medium confidence errors (0.5-0.8): potentially salvageable with thresholding\n",
    "       - Low confidence errors (< 0.5): model was uncertain, appropriate for human review\n",
    "\n",
    "    Hints:\n",
    "    - Store (image, true_label, pred_label, probabilities) for all misclassified samples\n",
    "    - Use Counter to find the most frequent error pairs\n",
    "    - High-confidence errors are the most clinically dangerous\n",
    "\n",
    "    Returns:\n",
    "        error_summary: Dict with error counts per category and confidence stratification\n",
    "    \"\"\"\n",
    "    # TODO: Implement error analysis\n",
    "    pass\n",
    "\n",
    "error_summary = error_analysis(pretrained_vit, val_loader, CLASS_NAMES, device)"
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought Questions\n",
    "\n",
    "1. Identify the top 3 failure modes. For each, propose a concrete mitigation strategy (data augmentation, architecture change, post-processing, or human-in-the-loop).\n",
    "2. Are the high-confidence errors clinically dangerous? What would happen if a PCP trusted the model's confident but wrong melanoma prediction?\n",
    "3. Propose a confidence-based triage system: what probability thresholds would you use for \"auto-approve,\" \"flag for dermatologist review,\" and \"urgent referral\"?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.8 Scalability and Deployment Considerations\n",
    "\n",
    "In production, the model must meet strict latency and throughput requirements. We benchmark inference performance to verify deployment readiness.\n",
    "\n",
    "### TODO: Inference Benchmarking"
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model, img_size=224, batch_sizes=[1, 4, 16, 64], num_warmup=10, num_runs=50):\n",
    "    \"\"\"\n",
    "    Benchmark model inference latency and throughput.\n",
    "\n",
    "    For each batch size:\n",
    "    1. Run num_warmup forward passes to warm up GPU\n",
    "    2. Run num_runs timed forward passes\n",
    "    3. Record: mean latency (ms), std latency, throughput (images/sec)\n",
    "\n",
    "    Also measure:\n",
    "    - Model size in MB (sum of parameter sizes)\n",
    "    - Peak GPU memory usage during inference\n",
    "\n",
    "    Hints:\n",
    "    - Use torch.cuda.synchronize() before timing to ensure GPU operations complete\n",
    "    - Use time.perf_counter() for high-resolution timing\n",
    "    - Use torch.cuda.max_memory_allocated() for peak memory\n",
    "    - Set model.eval() and use torch.no_grad()\n",
    "\n",
    "    Print a formatted table:\n",
    "    | Batch Size | Latency (ms) | Throughput (img/s) | GPU Memory (MB) |\n",
    "\n",
    "    Returns:\n",
    "        benchmarks: List of dicts with latency, throughput, memory per batch size\n",
    "    \"\"\"\n",
    "    # TODO: Implement inference benchmarking\n",
    "    pass\n",
    "\n",
    "benchmarks = benchmark_inference(pretrained_vit)"
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought Questions\n",
    "\n",
    "1. Does the ViT-Base model meet the 500ms latency requirement for single-image inference on a T4 GPU? What about on a CPU?\n",
    "2. How does latency scale with batch size? Is there a \"sweet spot\" batch size for maximizing throughput while staying within latency bounds?\n",
    "3. If the model were too slow, what optimization techniques could be applied? Consider: mixed precision (FP16), knowledge distillation to a smaller model, token pruning, or dynamic resolution.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.9 Ethical and Regulatory Analysis\n",
    "\n",
    "Deploying an AI model in healthcare carries significant ethical and regulatory responsibilities. This section asks you to think critically about the broader implications.\n",
    "\n",
    "### TODO: Ethical Impact Assessment"
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ethical_analysis():\n",
    "    \"\"\"\n",
    "    Write a structured ethical impact assessment for deploying this skin lesion\n",
    "    classifier in primary care clinics.\n",
    "\n",
    "    Address the following (print as formatted text):\n",
    "\n",
    "    1. BIAS AND FAIRNESS\n",
    "       - Fitzpatrick skin type representation: The ISIC dataset is heavily skewed\n",
    "         toward lighter skin types (Fitzpatrick I-III). How does this affect model\n",
    "         performance on darker skin (Fitzpatrick IV-VI)?\n",
    "       - What fairness metrics should be tracked?\n",
    "       - Propose a concrete mitigation strategy.\n",
    "\n",
    "    2. CLINICAL SAFETY\n",
    "       - What is the appropriate role of the AI? (decision support vs. autonomous diagnosis)\n",
    "       - What safeguards prevent over-reliance on the model by PCPs?\n",
    "       - How should model uncertainty be communicated to clinicians?\n",
    "\n",
    "    3. REGULATORY COMPLIANCE\n",
    "       - FDA 510(k) requirements for software as a medical device (SaMD)\n",
    "       - HIPAA compliance for patient image handling\n",
    "       - Clinical validation requirements (prospective study design)\n",
    "\n",
    "    4. FAILURE MODE RESPONSIBILITY\n",
    "       - If the model misses a melanoma, who bears liability?\n",
    "       - How should this be addressed in the product's terms of use?\n",
    "\n",
    "    This is a written analysis, not code. Use print() statements with formatted text.\n",
    "    \"\"\"\n",
    "    # TODO: Write the ethical impact assessment\n",
    "    pass\n",
    "\n",
    "ethical_analysis()"
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought Questions\n",
    "\n",
    "1. Should this model be deployed without separate clinical validation on darker skin types? What are the ethical implications of deploying a model with known demographic blind spots?\n",
    "2. How would you design a prospective clinical trial to validate this model? What would the control arm look like?\n",
    "3. If the model's false negative rate for melanoma is 8%, is it ethical to deploy it? Consider that dermatologists have a ~5% false negative rate and PCPs without AI assistance have a ~20% false negative rate.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you built an end-to-end Vision Transformer pipeline for clinical skin lesion classification:\n",
    "\n",
    "1. **Data Pipeline**: Loaded the ISIC 2019 dataset with augmentation and class-balanced sampling\n",
    "2. **CNN Baseline**: Established a ResNet-18 baseline for comparison\n",
    "3. **ViT from Scratch**: Implemented PatchEmbedding, TransformerBlock, and the full VisionTransformer class, understanding each component from first principles\n",
    "4. **Transfer Learning**: Fine-tuned a pretrained ViT-Base with warmup + cosine scheduling\n",
    "5. **Evaluation**: Comprehensive metrics including per-class sensitivity, ROC curves, and baseline comparison\n",
    "6. **Error Analysis**: Identified failure modes and confidence-stratified error rates\n",
    "7. **Deployment**: Benchmarked inference latency and throughput for production readiness\n",
    "8. **Ethics**: Assessed bias, safety, and regulatory considerations\n",
    "\n",
    "**Key takeaway**: The Vision Transformer's global self-attention mechanism enables it to capture long-range spatial relationships in dermoscopic images -- particularly important for atypical lesions where diagnostic features span distant regions of the image.\n",
    "\n",
    "**Next steps**: Read Section 4 (Production and System Design Extension) of the accompanying case study document for a detailed look at how SkinSight AI would deploy this model in production, including API design, monitoring, A/B testing, and cost analysis."
   ],
   "id": "cell_48"
  }
 ]
}