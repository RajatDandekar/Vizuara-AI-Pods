{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "NexaCode Personalization Case Study \u2014 Implementation Notebook"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Personalizing an Enterprise AI Coding Assistant with Conversation-Based RL\n",
    "\n",
    "## Implementation Notebook \u2014 NexaCode Technologies\n",
    "\n",
    "In this notebook, you will build a simplified but realistic version of NexaCode's developer personalization pipeline. You will implement session-aware rollout collection, a Process Reward Model with majority voting, GRPO-TCR training, and evaluate personalization on real conversation data.\n",
    "\n",
    "**Estimated time:** 75-90 minutes on a T4 GPU."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Environment"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter, deque\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Seed everything\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers accelerate"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition \u2014 Loading Anthropic HH-RLHF\n",
    "\n",
    "NexaCode's developers generate conversations with the AI assistant every day. For this case study, we use the Anthropic HH-RLHF dataset, which contains 170,000+ conversations with chosen (preferred) and rejected responses -- the same structure as NexaCode's correction data.\n",
    "\n",
    "Each conversation has a \"chosen\" path (the response the human preferred) and a \"rejected\" path (the response that was worse). We will use the human's follow-up messages as next-state signals, just as NexaCode's system uses developer corrections."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a subset for T4 memory constraints\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:5000]\")\n",
    "val_dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"test[:500]\")\n",
    "\n",
    "print(f\"Training samples: {len(dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"\\nSample conversation (chosen):\")\n",
    "print(dataset[0]['chosen'][:500])"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1: Parse Conversations into Training Triples\n",
    "\n",
    "The raw HH-RLHF data is formatted as alternating \"Human:\" and \"Assistant:\" turns. Parse each conversation into structured (context, response, feedback) triples."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_conversation(raw_text: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Parse a raw HH-RLHF conversation string into a list of turns.\n",
    "\n",
    "    The raw format looks like:\n",
    "        \\n\\nHuman: What is Python?\\n\\nAssistant: Python is a programming language...\n",
    "        \\n\\nHuman: Can you show me an example?\\n\\nAssistant: Sure, here is...\n",
    "\n",
    "    Args:\n",
    "        raw_text: Raw conversation string from the dataset\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with keys 'role' ('user' or 'assistant') and 'content'\n",
    "\n",
    "    Example:\n",
    "        >>> parse_conversation(\"\\\\n\\\\nHuman: Hi\\\\n\\\\nAssistant: Hello!\")\n",
    "        [{'role': 'user', 'content': 'Hi'}, {'role': 'assistant', 'content': 'Hello!'}]\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Split the raw text on \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" boundaries\n",
    "    # Step 2: For each segment, determine if it is a Human or Assistant turn\n",
    "    # Step 3: Strip whitespace and create the turn dict\n",
    "    # Step 4: Return the list of turns, filtering out empty turns\n",
    "    # Hint: Use str.split() and check if each segment starts with \"Human:\" or \"Assistant:\"\n",
    "    # ==============================\n",
    "\n",
    "    turns = []  # YOUR CODE HERE\n",
    "\n",
    "    return turns\n",
    "\n",
    "\n",
    "def extract_training_triples(turns: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extract (context, response, feedback) triples from parsed turns.\n",
    "\n",
    "    For each assistant turn that is followed by a user turn, create a triple:\n",
    "    - context: all turns before the assistant response\n",
    "    - response: the assistant's response text\n",
    "    - feedback: the user's next message (next-state signal)\n",
    "\n",
    "    Args:\n",
    "        turns: List of parsed turn dicts\n",
    "\n",
    "    Returns:\n",
    "        List of training triple dicts with keys:\n",
    "        'context' (list of turn dicts), 'response' (str), 'feedback' (str)\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Iterate through turns\n",
    "    # Step 2: For each assistant turn at index i, check if turns[i+1] is a user turn\n",
    "    # Step 3: If so, create a triple with context=turns[:i], response=turns[i], feedback=turns[i+1]\n",
    "    # Step 4: Return all triples\n",
    "    # ==============================\n",
    "\n",
    "    triples = []  # YOUR CODE HERE\n",
    "\n",
    "    return triples"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "sample_text = \"\\n\\nHuman: Write a sorting function in Python\\n\\nAssistant: Here is a JavaScript sorting function: function sort(arr) { return arr.sort(); }\\n\\nHuman: No, I asked for Python not JavaScript.\\n\\nAssistant: Sorry! Here is the Python version: def sort_list(lst): return sorted(lst)\\n\\nHuman: Perfect, thanks!\"\n",
    "\n",
    "turns = parse_conversation(sample_text)\n",
    "assert len(turns) == 5, f\"Expected 5 turns, got {len(turns)}\"\n",
    "assert turns[0]['role'] == 'user', f\"First turn should be user, got {turns[0]['role']}\"\n",
    "assert turns[1]['role'] == 'assistant', f\"Second turn should be assistant\"\n",
    "\n",
    "triples = extract_training_triples(turns)\n",
    "assert len(triples) == 2, f\"Expected 2 triples, got {len(triples)}\"\n",
    "assert \"Python not JavaScript\" in triples[0]['feedback'], \"First feedback should contain the correction\"\n",
    "print(\"Parser verification passed.\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "Let us understand the structure of the feedback signals in our data."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all conversations and extract triples\n",
    "all_triples = []\n",
    "parse_errors = 0\n",
    "\n",
    "for i, sample in enumerate(dataset):\n",
    "    try:\n",
    "        chosen_turns = parse_conversation(sample['chosen'])\n",
    "        chosen_triples = extract_training_triples(chosen_turns)\n",
    "        all_triples.extend(chosen_triples)\n",
    "    except Exception:\n",
    "        parse_errors += 1\n",
    "\n",
    "print(f\"Parsed {len(dataset)} conversations\")\n",
    "print(f\"Extracted {len(all_triples)} training triples\")\n",
    "print(f\"Parse errors: {parse_errors}\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Analyze Feedback Signal Distribution\n",
    "\n",
    "Generate three visualizations that characterize the training data."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feedback_signals(triples: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze the distribution of feedback signals in the training data.\n",
    "\n",
    "    Produce the following analysis:\n",
    "    1. Distribution of feedback lengths (in words)\n",
    "    2. Distribution of response lengths (in words)\n",
    "    3. Classify each feedback as 'positive', 'negative', or 'neutral' using\n",
    "       simple keyword heuristics (positive: \"thanks\", \"great\", \"perfect\", etc.;\n",
    "       negative: \"no\", \"wrong\", \"not\", \"incorrect\", etc.)\n",
    "\n",
    "    Args:\n",
    "        triples: List of training triple dicts\n",
    "\n",
    "    Returns:\n",
    "        Dict with keys:\n",
    "        - 'feedback_lengths': list of int (word counts)\n",
    "        - 'response_lengths': list of int (word counts)\n",
    "        - 'sentiments': list of str ('positive', 'negative', 'neutral')\n",
    "        - 'sentiment_counts': dict mapping sentiment to count\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Compute word counts for feedback and response\n",
    "    # Step 2: Classify each feedback using keyword matching\n",
    "    # Step 3: Count sentiment distribution\n",
    "    # Step 4: Return the analysis dict\n",
    "    # ==============================\n",
    "\n",
    "    analysis = {}  # YOUR CODE HERE\n",
    "\n",
    "    return analysis\n",
    "\n",
    "\n",
    "# Run analysis and generate 3 visualizations\n",
    "analysis = analyze_feedback_signals(all_triples)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Plot 1: Feedback length distribution\n",
    "# Plot 2: Response length distribution\n",
    "# Plot 3: Sentiment distribution bar chart\n",
    "\n",
    "# YOUR VISUALIZATION CODE HERE\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer these questions based on your EDA:\n",
    "1. What is the median feedback length? What does this suggest about the richness of the next-state signal?\n",
    "2. What fraction of feedback is corrective (negative)? Is this enough to train on?\n",
    "3. Are there any outliers in response length that might cause issues with the overlong reward shaping?"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline \u2014 Prompt Injection\n",
    "\n",
    "Before building the RL pipeline, let us establish a baseline using NexaCode's current approach: prepending recent corrections to the system prompt."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptInjectionBaseline:\n",
    "    \"\"\"\n",
    "    NexaCode's current baseline: cache the last N corrections and\n",
    "    inject them into the system prompt.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_corrections: int = 10):\n",
    "        self.correction_cache = deque(maxlen=max_corrections)\n",
    "\n",
    "    def add_correction(self, correction: str):\n",
    "        \"\"\"Add a developer correction to the cache.\"\"\"\n",
    "        self.correction_cache.append(correction)\n",
    "\n",
    "    def build_prompt(self, user_query: str) -> str:\n",
    "        \"\"\"Build a prompt with injected corrections.\"\"\"\n",
    "        corrections_text = \"\"\n",
    "        if self.correction_cache:\n",
    "            corrections_text = \"DEVELOPER PREFERENCES:\\n\"\n",
    "            for c in self.correction_cache:\n",
    "                corrections_text += f\"- {c}\\n\"\n",
    "            corrections_text += \"\\n\"\n",
    "        return corrections_text + user_query\n",
    "\n",
    "    def evaluate(self, triples: List[Dict]) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate baseline: for each triple, check if the correction\n",
    "        would have prevented the mistake (simplified heuristic).\n",
    "        \"\"\"\n",
    "        corrected = 0\n",
    "        total = 0\n",
    "        for triple in triples:\n",
    "            total += 1\n",
    "            feedback_lower = triple['feedback'].lower()\n",
    "            # If feedback is positive, the baseline \"succeeded\"\n",
    "            pos_words = ['thanks', 'great', 'perfect', 'good', 'correct', 'yes']\n",
    "            if any(w in feedback_lower for w in pos_words):\n",
    "                corrected += 1\n",
    "            else:\n",
    "                # Add the correction for future use\n",
    "                self.add_correction(triple['feedback'][:100])\n",
    "        return corrected / max(total, 1)\n",
    "\n",
    "baseline = PromptInjectionBaseline()\n",
    "baseline_accuracy = baseline.evaluate(all_triples[:200])\n",
    "print(f\"Baseline preference accuracy: {baseline_accuracy:.1%}\")\n",
    "print(f\"Corrections cached: {len(baseline.correction_cache)}\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Design \u2014 Building the Personalization Pipeline\n",
    "\n",
    "### 4.1 Process Reward Model with Majority Voting"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 3: Implement the PRM Scoring Function"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessRewardModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A Process Reward Model that scores (response, feedback) pairs.\n",
    "    Uses a small transformer encoder with a classification head.\n",
    "\n",
    "    Architecture:\n",
    "        - Embedding layer (shared vocab with the policy)\n",
    "        - 2-layer transformer encoder\n",
    "        - Mean pooling over sequence\n",
    "        - Linear classifier: hidden_size -> 3 (for scores -1, 0, +1)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, hidden_size: int = 128, num_heads: int = 4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=0)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size, nhead=num_heads,\n",
    "            dim_feedforward=256, batch_first=True, dropout=0.1\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.classifier = nn.Linear(hidden_size, 3)  # -1, 0, +1\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Forward pass. Returns logits for 3 classes.\n",
    "\n",
    "        Args:\n",
    "            input_ids: (batch, seq_len) token IDs\n",
    "            attention_mask: (batch, seq_len) binary mask\n",
    "\n",
    "        Returns:\n",
    "            logits: (batch, 3)\n",
    "        \"\"\"\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.encoder(x)\n",
    "        # Mean pooling (respecting the mask)\n",
    "        if attention_mask is not None:\n",
    "            mask = attention_mask.unsqueeze(-1).float()\n",
    "            x = (x * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "        else:\n",
    "            x = x.mean(dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def majority_vote(self, input_ids, attention_mask=None, num_votes=5):\n",
    "        \"\"\"\n",
    "        Run majority voting over num_votes forward passes with dropout.\n",
    "\n",
    "        Args:\n",
    "            input_ids: (batch, seq_len)\n",
    "            attention_mask: (batch, seq_len)\n",
    "            num_votes: Number of votes (m)\n",
    "\n",
    "        Returns:\n",
    "            rewards: (batch,) tensor with values in {-1, 0, +1}\n",
    "            all_votes: (num_votes, batch) tensor of individual votes\n",
    "        \"\"\"\n",
    "        # ============ TODO ============\n",
    "        # Step 1: Set model to train mode (enables dropout for diversity)\n",
    "        # Step 2: Run num_votes forward passes, collecting argmax predictions\n",
    "        # Step 3: Map class indices to scores: 0->-1, 1->0, 2->+1\n",
    "        # Step 4: For each sample, take the most common vote (majority)\n",
    "        # Step 5: Set model back to eval mode\n",
    "        # Step 6: Return (majority_rewards, all_votes_tensor)\n",
    "        # Hint: Use collections.Counter for majority voting\n",
    "        # ==============================\n",
    "\n",
    "        rewards = None  # YOUR CODE HERE\n",
    "        all_votes = None  # YOUR CODE HERE\n",
    "\n",
    "        return rewards, all_votes\n",
    "\n",
    "\n",
    "prm = ProcessRewardModel(vocab_size=len(tokenizer)).to(device)\n",
    "print(f\"PRM parameters: {sum(p.numel() for p in prm.parameters()):,}\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "test_ids = torch.randint(0, 100, (4, 20)).to(device)\n",
    "test_mask = torch.ones(4, 20).to(device)\n",
    "rewards, votes = prm.majority_vote(test_ids, test_mask, num_votes=5)\n",
    "assert rewards.shape == (4,), f\"Expected shape (4,), got {rewards.shape}\"\n",
    "assert all(r in [-1, 0, 1] for r in rewards.tolist()), \"Rewards must be in {-1, 0, +1}\"\n",
    "print(f\"PRM majority voting verification passed. Rewards: {rewards.tolist()}\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 GRPO Advantage Computation and Loss\n",
    "\n",
    "### TODO 4: Implement the GRPO-TCR Loss Function"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grpo_advantages(rewards: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute group-relative advantages from rewards.\n",
    "\n",
    "    A_i = (r_i - mean(r)) / std(r)\n",
    "\n",
    "    Args:\n",
    "        rewards: (G,) tensor of rewards for G responses to the same prompt\n",
    "\n",
    "    Returns:\n",
    "        advantages: (G,) tensor of normalized advantages\n",
    "    \"\"\"\n",
    "    mean = rewards.mean()\n",
    "    std = rewards.std()\n",
    "    if std < 1e-8:\n",
    "        return torch.zeros_like(rewards)\n",
    "    return (rewards - mean) / std\n",
    "\n",
    "\n",
    "def overlong_reward(response_length: int, L_max: int = 512, L_cache: int = 100) -> float:\n",
    "    \"\"\"\n",
    "    Compute the overlong response penalty.\n",
    "\n",
    "    Args:\n",
    "        response_length: Token count of the response\n",
    "        L_max: Maximum allowed length\n",
    "        L_cache: Size of the penalty transition zone\n",
    "\n",
    "    Returns:\n",
    "        Penalty value in [-1, 0]\n",
    "    \"\"\"\n",
    "    safe = L_max - L_cache\n",
    "    if response_length <= safe:\n",
    "        return 0.0\n",
    "    elif response_length <= L_max:\n",
    "        return (safe - response_length) / L_cache\n",
    "    else:\n",
    "        return -1.0\n",
    "\n",
    "\n",
    "def grpo_tcr_loss(\n",
    "    log_probs_new: torch.Tensor,\n",
    "    log_probs_ref: torch.Tensor,\n",
    "    advantages: torch.Tensor,\n",
    "    response_lengths: torch.Tensor,\n",
    "    eps_low: float = 0.2,\n",
    "    eps_high: float = 0.28,\n",
    "    beta_kl: float = 0.01,\n",
    "    L_max: int = 512,\n",
    "    L_cache: int = 100,\n",
    ") -> Tuple[torch.Tensor, Dict]:\n",
    "    \"\"\"\n",
    "    Compute the full GRPO-TCR loss.\n",
    "\n",
    "    Combines:\n",
    "    - Token-level clipped surrogate loss with clip-higher\n",
    "    - KL divergence penalty\n",
    "    - Overlong reward shaping\n",
    "\n",
    "    Args:\n",
    "        log_probs_new: (batch, seq_len) log probs under current policy\n",
    "        log_probs_ref: (batch, seq_len) log probs under reference policy\n",
    "        advantages: (batch,) group-relative advantages\n",
    "        response_lengths: (batch,) token counts per response\n",
    "        eps_low: Lower clip bound offset\n",
    "        eps_high: Upper clip bound offset (clip-higher)\n",
    "        beta_kl: KL penalty coefficient\n",
    "        L_max, L_cache: Overlong shaping parameters\n",
    "\n",
    "    Returns:\n",
    "        (loss, metrics_dict) where loss is scalar and metrics_dict contains\n",
    "        'mean_advantage', 'mean_kl', 'mean_length_penalty', 'mean_ratio'\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Compute policy ratio: ratio = exp(log_new - log_ref)\n",
    "    # Step 2: Expand advantages to token level: (batch,) -> (batch, 1)\n",
    "    # Step 3: Unclipped objective = ratio * advantages\n",
    "    # Step 4: Clipped ratio = clamp(ratio, 1-eps_low, 1+eps_high)\n",
    "    # Step 5: Clipped objective = clipped_ratio * advantages\n",
    "    # Step 6: Surrogate loss = -mean(min(unclipped, clipped))\n",
    "    # Step 7: KL penalty = beta_kl * mean(log_new - log_ref)\n",
    "    # Step 8: Length penalties: compute overlong_reward for each response\n",
    "    # Step 9: Total loss = surrogate + KL + mean(length_penalties)\n",
    "    # Step 10: Collect metrics\n",
    "    # ==============================\n",
    "\n",
    "    loss = None  # YOUR CODE HERE\n",
    "    metrics = {}  # YOUR CODE HERE\n",
    "\n",
    "    return loss, metrics"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "batch, seq = 8, 20\n",
    "log_new = torch.randn(batch, seq, device=device) * 0.1 - 2.0\n",
    "log_ref = torch.randn(batch, seq, device=device) * 0.1 - 2.0\n",
    "advs = torch.randn(batch, device=device)\n",
    "lengths = torch.randint(50, 600, (batch,), device=device).float()\n",
    "\n",
    "loss, metrics = grpo_tcr_loss(log_new, log_ref, advs, lengths)\n",
    "assert loss.dim() == 0, f\"Loss should be scalar, got shape {loss.shape}\"\n",
    "assert not torch.isnan(loss), \"Loss is NaN\"\n",
    "assert 'mean_kl' in metrics, \"Metrics should include mean_kl\"\n",
    "print(f\"GRPO-TCR loss verification passed. Loss: {loss.item():.4f}\")\n",
    "print(f\"Metrics: { {k: f'{v:.4f}' for k, v in metrics.items()} }\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Strategy\n",
    "\n",
    "### TODO 5: Implement the Training Loop"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePolicy(nn.Module):\n",
    "    \"\"\"Simplified policy model for T4 training.\"\"\"\n",
    "    def __init__(self, vocab_size, hidden_size=128, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=0)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size, nhead=num_heads,\n",
    "            dim_feedforward=256, batch_first=True, dropout=0.1\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.head = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.encoder(x)\n",
    "        return self.head(x)\n",
    "\n",
    "    def get_log_probs(self, input_ids, target_ids):\n",
    "        logits = self.forward(input_ids)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        return log_probs.gather(2, target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "\n",
    "def train_grpo_tcr(\n",
    "    policy: nn.Module,\n",
    "    ref_policy: nn.Module,\n",
    "    prm: ProcessRewardModel,\n",
    "    train_triples: List[Dict],\n",
    "    tokenizer,\n",
    "    num_steps: int = 300,\n",
    "    batch_size: int = 8,\n",
    "    lr: float = 1e-4,\n",
    "    beta_kl: float = 0.01,\n",
    ") -> Dict[str, List]:\n",
    "    \"\"\"\n",
    "    Train the policy using GRPO-TCR on conversation triples.\n",
    "\n",
    "    Args:\n",
    "        policy: The trainable policy model\n",
    "        ref_policy: Frozen reference policy (same architecture, initial weights)\n",
    "        prm: Process Reward Model for scoring\n",
    "        train_triples: List of (context, response, feedback) triples\n",
    "        tokenizer: Tokenizer for encoding text\n",
    "        num_steps: Number of training steps\n",
    "        batch_size: Batch size\n",
    "        lr: Learning rate\n",
    "        beta_kl: KL penalty coefficient\n",
    "\n",
    "    Returns:\n",
    "        Dict of training metrics lists: 'losses', 'kl_divs', 'mean_rewards',\n",
    "        'prm_accuracies'\n",
    "\n",
    "    Implementation:\n",
    "        1. Initialize AdamW optimizer with weight_decay=0.01\n",
    "        2. For each step:\n",
    "           a. Sample a batch of triples\n",
    "           b. Tokenize the response and feedback\n",
    "           c. Get PRM rewards via majority voting\n",
    "           d. Compute GRPO advantages from rewards\n",
    "           e. Get log-probs from policy and ref_policy\n",
    "           f. Compute GRPO-TCR loss\n",
    "           g. Backprop and step (with gradient clipping at max_norm=1.0)\n",
    "           h. Log metrics\n",
    "        3. Return metrics dict\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Implement the full training loop following the steps above.\n",
    "    # Log loss, KL divergence, mean reward, and PRM accuracy at each step.\n",
    "    # Print progress every 50 steps.\n",
    "    # ==============================\n",
    "\n",
    "    metrics = {\n",
    "        'losses': [],\n",
    "        'kl_divs': [],\n",
    "        'mean_rewards': [],\n",
    "    }\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return metrics"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "vocab_size = len(tokenizer)\n",
    "policy = SimplePolicy(vocab_size).to(device)\n",
    "ref_policy = SimplePolicy(vocab_size).to(device)\n",
    "ref_policy.load_state_dict(policy.state_dict())\n",
    "for p in ref_policy.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(f\"Policy parameters: {sum(p.numel() for p in policy.parameters()):,}\")\n",
    "print(\"Starting GRPO-TCR training...\")\n",
    "\n",
    "# Train (reduce num_steps for faster iteration during development)\n",
    "train_metrics = train_grpo_tcr(\n",
    "    policy, ref_policy, prm, all_triples,\n",
    "    tokenizer, num_steps=300, batch_size=8\n",
    ")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "\n",
    "### TODO 6: Evaluate and Compare Against Baseline"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_personalization(\n",
    "    policy: nn.Module,\n",
    "    ref_policy: nn.Module,\n",
    "    test_triples: List[Dict],\n",
    "    tokenizer,\n",
    "    baseline_accuracy: float,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate the personalized model against the baseline.\n",
    "\n",
    "    Compute:\n",
    "    1. Average log-probability improvement (personalized vs reference)\n",
    "    2. KL divergence distribution\n",
    "    3. Simulated correction rate reduction\n",
    "\n",
    "    Args:\n",
    "        policy: Trained personalized policy\n",
    "        ref_policy: Frozen reference policy\n",
    "        test_triples: Held-out test triples\n",
    "        tokenizer: Tokenizer\n",
    "        baseline_accuracy: The prompt injection baseline's accuracy\n",
    "\n",
    "    Returns:\n",
    "        Dict with 'preference_accuracy', 'mean_kl', 'correction_rate_reduction',\n",
    "        'log_prob_improvements'\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: For each test triple, tokenize the response\n",
    "    # Step 2: Compute log-probs under both policy and ref_policy\n",
    "    # Step 3: If policy log-prob > ref log-prob, count as \"improved\"\n",
    "    # Step 4: Compute KL divergence\n",
    "    # Step 5: Estimate correction rate reduction\n",
    "    # Step 6: Return results dict\n",
    "    # ==============================\n",
    "\n",
    "    results = {}  # YOUR CODE HERE\n",
    "\n",
    "    return results"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse validation triples\n",
    "val_triples = []\n",
    "for sample in val_dataset:\n",
    "    try:\n",
    "        turns = parse_conversation(sample['chosen'])\n",
    "        val_triples.extend(extract_training_triples(turns))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "eval_results = evaluate_personalization(\n",
    "    policy, ref_policy, val_triples[:100], tokenizer, baseline_accuracy\n",
    ")\n",
    "\n",
    "# Generate comparison plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Plot 1: Training loss curve\n",
    "axes[0].plot(train_metrics['losses'], alpha=0.3, color='blue')\n",
    "window = 20\n",
    "if len(train_metrics['losses']) > window:\n",
    "    smoothed = np.convolve(train_metrics['losses'], np.ones(window)/window, mode='valid')\n",
    "    axes[0].plot(smoothed, linewidth=2, color='blue')\n",
    "axes[0].set_xlabel('Training Step')\n",
    "axes[0].set_ylabel('GRPO-TCR Loss')\n",
    "axes[0].set_title('Training Convergence')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Preference accuracy comparison\n",
    "# YOUR CODE HERE \u2014 bar chart comparing baseline vs RL\n",
    "\n",
    "# Plot 3: KL divergence over training\n",
    "axes[2].plot(train_metrics['kl_divs'], alpha=0.3, color='red')\n",
    "if len(train_metrics['kl_divs']) > window:\n",
    "    smoothed_kl = np.convolve(train_metrics['kl_divs'], np.ones(window)/window, mode='valid')\n",
    "    axes[2].plot(smoothed_kl, linewidth=2, color='red')\n",
    "axes[2].axhline(y=5.0, color='black', linestyle='--', label='KL threshold')\n",
    "axes[2].set_xlabel('Training Step')\n",
    "axes[2].set_ylabel('KL Divergence')\n",
    "axes[2].set_title('Policy Drift Monitoring')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nEvaluation Results:\")\n",
    "for k, v in eval_results.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    elif isinstance(v, list):\n",
    "        print(f\"  {k}: {len(v)} values (mean={np.mean(v):.4f})\")"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a 200-word analysis: How does the RL-personalized model compare to the baseline? Where does it succeed, and where does it still struggle?"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis\n",
    "\n",
    "### TODO 7: Categorize Failure Modes"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_errors(\n",
    "    policy: nn.Module,\n",
    "    test_triples: List[Dict],\n",
    "    tokenizer,\n",
    "    num_samples: int = 50,\n",
    ") -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Categorize errors in the personalized model's outputs.\n",
    "\n",
    "    Error categories:\n",
    "    - 'style_mismatch': Response has wrong formatting or tone\n",
    "    - 'content_error': Response contains factually wrong information\n",
    "    - 'preference_ignored': Response ignores a stated preference\n",
    "    - 'too_verbose': Response is unnecessarily long\n",
    "    - 'too_brief': Response is too short to be useful\n",
    "    - 'correct': Response was good (not an error)\n",
    "\n",
    "    Args:\n",
    "        policy: Trained policy model\n",
    "        test_triples: Test data\n",
    "        tokenizer: Tokenizer\n",
    "        num_samples: Number of samples to analyze\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping error category to count\n",
    "\n",
    "    Approach:\n",
    "        1. For each sample, tokenize and get model output\n",
    "        2. Compare model output log-probs with reference\n",
    "        3. Use the feedback text to classify the error type\n",
    "           (keyword heuristics: \"wrong\" -> content_error,\n",
    "            \"verbose\"/\"long\" -> too_verbose, \"short\"/\"more\" -> too_brief,\n",
    "            \"style\"/\"format\" -> style_mismatch, \"prefer\"/\"want\" -> preference_ignored)\n",
    "        4. Return counts\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Implement error categorization\n",
    "    # ==============================\n",
    "\n",
    "    error_counts = {}  # YOUR CODE HERE\n",
    "\n",
    "    return error_counts"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = categorize_errors(policy, val_triples, tokenizer)\n",
    "print(\"Error Distribution:\")\n",
    "total = sum(errors.values())\n",
    "for category, count in sorted(errors.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {category}: {count} ({100*count/total:.0f}%)\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(errors.keys(), errors.values(), color='#e74c3c', alpha=0.8)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Error Distribution by Category')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the top 3 failure modes. For each, explain: (a) why the RL system fails here, and (b) what modification to the training pipeline would address it."
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Scalability and Deployment\n",
    "\n",
    "### TODO 8: Inference Latency Benchmarking"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(\n",
    "    model: nn.Module,\n",
    "    tokenizer,\n",
    "    num_samples: int = 100,\n",
    "    seq_lengths: List[int] = [32, 64, 128, 256, 512],\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Benchmark inference latency at various sequence lengths.\n",
    "\n",
    "    For each sequence length, run num_samples forward passes and\n",
    "    record p50, p95, p99 latencies in milliseconds.\n",
    "\n",
    "    Args:\n",
    "        model: The policy model\n",
    "        tokenizer: Tokenizer\n",
    "        num_samples: Number of samples per sequence length\n",
    "        seq_lengths: List of sequence lengths to test\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping seq_length (str) to latency dict with\n",
    "        keys 'p50', 'p95', 'p99' (values in milliseconds)\n",
    "    \"\"\"\n",
    "    import time\n",
    "    # ============ TODO ============\n",
    "    # Step 1: For each sequence length, generate random input_ids\n",
    "    # Step 2: Run num_samples forward passes, recording time for each\n",
    "    # Step 3: Compute p50, p95, p99 from the latency distribution\n",
    "    # Step 4: Return results\n",
    "    # Hint: Use torch.cuda.synchronize() before timing for accurate GPU measurement\n",
    "    # ==============================\n",
    "\n",
    "    results = {}  # YOUR CODE HERE\n",
    "\n",
    "    return results"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latency_results = benchmark_inference(policy, tokenizer)\n",
    "print(\"Inference Latency Benchmark:\")\n",
    "print(f\"{'Seq Length':>12} {'p50':>10} {'p95':>10} {'p99':>10}\")\n",
    "print(\"-\" * 45)\n",
    "for seq_len, stats in sorted(latency_results.items(), key=lambda x: int(x[0])):\n",
    "    print(f\"{seq_len:>12} {stats['p50']:>8.1f}ms {stats['p95']:>8.1f}ms {stats['p99']:>8.1f}ms\")\n",
    "\n",
    "budget_200ms = all(v['p99'] < 200 for v in latency_results.values())\n",
    "print(f\"\\nAll p99 latencies within 200ms budget: {'YES' if budget_200ms else 'NO'}\")"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ethical and Regulatory Analysis\n",
    "\n",
    "### TODO 9: Ethical Impact Assessment\n",
    "\n",
    "Write a 300-word ethical impact assessment for NexaCode's personalization system. Address the following three concerns:\n",
    "\n",
    "1. **Reinforcement of bad practices:** Could the RL system learn to produce insecure code because a developer consistently approves it? What guardrails should be in place?\n",
    "\n",
    "2. **Privacy of developer conversations:** The system learns from every conversation turn. What data minimization practices should NexaCode implement? How should correction data be handled when a developer leaves the company?\n",
    "\n",
    "3. **Fairness across experience levels:** Junior developers make more corrections (because they learn from the AI's output). Could the RL system inadvertently optimize differently for junior vs. senior developers, creating a two-tier experience?\n",
    "\n",
    "For each concern, propose a specific technical mitigation."
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your assessment as a multi-line string\n",
    "ethical_assessment = \"\"\"\n",
    "YOUR 300-WORD ASSESSMENT HERE\n",
    "\"\"\"\n",
    "\n",
    "print(ethical_assessment)\n",
    "assert len(ethical_assessment.split()) >= 250, \"Assessment should be at least 250 words\"\n",
    "print(f\"\\nWord count: {len(ethical_assessment.split())}\")"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this case study notebook, you built a simplified version of NexaCode's developer personalization pipeline:\n",
    "\n",
    "1. **Data acquisition:** Parsed Anthropic HH-RLHF conversations into training triples with next-state feedback signals.\n",
    "2. **EDA:** Analyzed feedback distributions and correction patterns.\n",
    "3. **Baseline:** Implemented and evaluated the prompt injection baseline.\n",
    "4. **PRM:** Built a Process Reward Model with majority voting.\n",
    "5. **GRPO-TCR:** Implemented the full loss function with clip-higher and overlong reward shaping.\n",
    "6. **Training:** Ran an RL training loop on conversation data.\n",
    "7. **Evaluation:** Compared RL personalization against the baseline.\n",
    "8. **Error analysis:** Categorized failure modes to guide future improvements.\n",
    "9. **Deployment:** Benchmarked inference latency for production readiness.\n",
    "\n",
    "For the production system design (multi-GPU architecture, API design, monitoring, A/B testing, and cost analysis), refer to **Section 4** of the full case study document.\n",
    "\n",
    "The key insight from this case study: **your everyday conversations with an AI assistant already contain rich training signal.** With the right RL framework (GRPO-TCR for implicit feedback, OPD for explicit corrections), this signal can be extracted and used to continuously personalize the model \u2014 all without manual labeling, without service interruption, and without compromising privacy."
   ],
   "id": "cell_41"
  }
 ]
}