{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Rebuilding the Context Pipeline for an AI Legal Research Assistant â€” Implementation Notebook"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Rebuilding the Context Pipeline for an AI Legal Research Assistant\n",
    "## Implementation Notebook\n",
    "\n",
    "Welcome to the implementation notebook for the BriefEngine context engineering case study. In this notebook, you will build a complete context engineering pipeline for a legal research assistant, progressing from data exploration through a fully evaluated retrieval-augmented generation system.\n",
    "\n",
    "**What you will build:**\n",
    "- A filtered retrieval pipeline over real U.S. case law data\n",
    "- A semantic search and reranking system using sentence transformers\n",
    "- A context assembly engine that implements all four context engineering strategies (Write, Select, Compress, Isolate)\n",
    "- A citation verification system\n",
    "- A complete evaluation framework comparing your pipeline against a BM25 baseline\n",
    "\n",
    "**Prerequisites:** Familiarity with Python, NumPy, and basic NLP concepts. Understanding of the context engineering article (the four failure modes and four strategies) is essential.\n",
    "\n",
    "**Estimated time:** 3-4 hours\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Data Acquisition and Preprocessing\n",
    "\n",
    "In this section, you will load and preprocess a subset of the CaseLaw Access Project (Harvard Law School) â€” a corpus of real U.S. court decisions. We work with California state courts and Ninth Circuit federal opinions from 2000-2024.\n",
    "\n",
    "The dataset has been pre-processed into JSON records. Each record contains the full opinion text, court metadata, date, citation string, jurisdiction, and a boolean indicating whether the case has been overruled by a later decision."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q sentence-transformers rank_bm25 numpy pandas matplotlib seaborn tqdm\n",
    "\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "from dataclasses import dataclass\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All dependencies loaded successfully.\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Synthetic Dataset Generation ---\n",
    "# Since the full CaseLaw Access Project requires bulk download approval,\n",
    "# we generate a realistic synthetic dataset that mirrors its structure.\n",
    "# The synthetic data preserves realistic: court names, citation formats,\n",
    "# opinion lengths, jurisdiction distributions, and citation networks.\n",
    "\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# California courts and Ninth Circuit federal courts\n",
    "COURTS = {\n",
    "    \"CA_supreme\": \"Supreme Court of California\",\n",
    "    \"CA_appeal_1\": \"California Court of Appeal, First District\",\n",
    "    \"CA_appeal_2\": \"California Court of Appeal, Second District\",\n",
    "    \"CA_appeal_4\": \"California Court of Appeal, Fourth District\",\n",
    "    \"9th_circuit\": \"United States Court of Appeals for the Ninth Circuit\",\n",
    "    \"ND_CA\": \"United States District Court for the Northern District of California\",\n",
    "    \"CD_CA\": \"United States District Court for the Central District of California\",\n",
    "}\n",
    "\n",
    "JURISDICTIONS = {\n",
    "    \"CA_supreme\": \"california\",\n",
    "    \"CA_appeal_1\": \"california\",\n",
    "    \"CA_appeal_2\": \"california\",\n",
    "    \"CA_appeal_4\": \"california\",\n",
    "    \"9th_circuit\": \"federal-9th\",\n",
    "    \"ND_CA\": \"federal-9th\",\n",
    "    \"CD_CA\": \"federal-9th\",\n",
    "}\n",
    "\n",
    "LEGAL_TOPICS = [\n",
    "    \"negligence\", \"strict liability\", \"breach of contract\", \"due process\",\n",
    "    \"equal protection\", \"employment discrimination\", \"wrongful termination\",\n",
    "    \"product liability\", \"medical malpractice\", \"premises liability\",\n",
    "    \"insurance bad faith\", \"fraud\", \"unfair business practices\",\n",
    "    \"intellectual property\", \"trade secrets\", \"environmental law\",\n",
    "    \"landlord-tenant\", \"family law\", \"criminal procedure\", \"civil rights\",\n",
    "    \"first amendment\", \"fourth amendment\", \"habeas corpus\",\n",
    "    \"class action\", \"arbitration\", \"statute of limitations\",\n",
    "    \"emotional distress\", \"defamation\", \"privacy\", \"antitrust\",\n",
    "]\n",
    "\n",
    "LEGAL_PHRASES = [\n",
    "    \"The court finds that the defendant's conduct constitutes\",\n",
    "    \"Under California Civil Code section\",\n",
    "    \"The standard of review for this claim is\",\n",
    "    \"Applying the test established in\",\n",
    "    \"The plaintiff has demonstrated a prima facie case of\",\n",
    "    \"The doctrine of res judicata bars this claim because\",\n",
    "    \"Pursuant to the holding in\",\n",
    "    \"The court grants summary judgment on the grounds that\",\n",
    "    \"The evidence presented at trial establishes that\",\n",
    "    \"Under the substantial evidence standard, we conclude\",\n",
    "    \"The trial court did not abuse its discretion in\",\n",
    "    \"We review de novo the question of whether\",\n",
    "    \"The statute requires a showing of\",\n",
    "    \"The burden of proof shifts to the defendant to demonstrate\",\n",
    "    \"Considering the totality of the circumstances,\",\n",
    "]\n",
    "\n",
    "\n",
    "def generate_opinion_text(topic: str, length_tokens: int) -> str:\n",
    "    \"\"\"Generate a synthetic legal opinion on a given topic.\"\"\"\n",
    "    paragraphs = []\n",
    "    tokens_so_far = 0\n",
    "    while tokens_so_far < length_tokens:\n",
    "        phrase = random.choice(LEGAL_PHRASES)\n",
    "        detail = (\n",
    "            f\"{topic}. The court examined the relevant statutory provisions \"\n",
    "            f\"and prior case law to determine the applicable standard. \"\n",
    "            f\"After careful consideration of the evidence and arguments \"\n",
    "            f\"presented by both parties, the court concludes that the \"\n",
    "            f\"requirements have been {'satisfied' if random.random() > 0.4 else 'not met'}. \"\n",
    "            f\"This determination is consistent with the precedent established \"\n",
    "            f\"in prior decisions of this court and the {'California Supreme Court' if random.random() > 0.5 else 'Ninth Circuit'}.\"\n",
    "        )\n",
    "        paragraph = f\"{phrase} {detail}\"\n",
    "        paragraphs.append(paragraph)\n",
    "        tokens_so_far += len(paragraph.split())\n",
    "    return \"\\n\\n\".join(paragraphs)\n",
    "\n",
    "\n",
    "def generate_citation(court_key: str, year: int, idx: int) -> str:\n",
    "    \"\"\"Generate a realistic citation string.\"\"\"\n",
    "    if court_key.startswith(\"CA_supreme\"):\n",
    "        return f\"{random.randint(1, 60)} Cal.{random.choice(['3d', '4th', '5th'])} {random.randint(100, 999)} ({year})\"\n",
    "    elif court_key.startswith(\"CA_appeal\"):\n",
    "        return f\"{random.randint(1, 300)} Cal.App.{random.choice(['4th', '5th'])} {random.randint(100, 999)} ({year})\"\n",
    "    elif court_key == \"9th_circuit\":\n",
    "        return f\"{random.randint(100, 999)} F.3d {random.randint(100, 1500)} ({year})\"\n",
    "    else:\n",
    "        return f\"{random.randint(100, 999)} F.Supp.{random.choice(['2d', '3d'])} {random.randint(100, 1500)} ({year})\"\n",
    "\n",
    "\n",
    "def generate_case_name() -> str:\n",
    "    \"\"\"Generate a realistic case name.\"\"\"\n",
    "    first_names = [\"Smith\", \"Johnson\", \"Williams\", \"Brown\", \"Jones\", \"Garcia\",\n",
    "                   \"Miller\", \"Davis\", \"Rodriguez\", \"Martinez\", \"Hernandez\",\n",
    "                   \"Lopez\", \"Gonzalez\", \"Wilson\", \"Anderson\", \"Thomas\"]\n",
    "    entities = [\"Pacific Mutual\", \"State of California\", \"City of Los Angeles\",\n",
    "                \"County of San Francisco\", \"United Healthcare\", \"Tesla Motors\",\n",
    "                \"Pacific Gas & Electric\", \"Wells Fargo\", \"Kaiser Permanente\",\n",
    "                \"University of California\", \"Walmart Inc.\", \"Amazon.com LLC\",\n",
    "                \"Department of Motor Vehicles\", \"Board of Education\"]\n",
    "\n",
    "    plaintiff = random.choice(first_names + entities)\n",
    "    defendant = random.choice(first_names + entities)\n",
    "    while defendant == plaintiff:\n",
    "        defendant = random.choice(first_names + entities)\n",
    "    return f\"{plaintiff} v. {defendant}\"\n",
    "\n",
    "\n",
    "# Generate 2,000 synthetic cases (scaled for Colab)\n",
    "NUM_CASES = 2000\n",
    "cases = []\n",
    "\n",
    "for i in range(NUM_CASES):\n",
    "    court_key = random.choice(list(COURTS.keys()))\n",
    "    year = random.randint(2000, 2024)\n",
    "    topic = random.choice(LEGAL_TOPICS)\n",
    "    length = random.randint(200, 2000)\n",
    "    is_overruled = random.random() < 0.05  # 5% overruled rate\n",
    "\n",
    "    case = {\n",
    "        \"id\": f\"case_{i:05d}\",\n",
    "        \"case_name\": generate_case_name(),\n",
    "        \"court\": COURTS[court_key],\n",
    "        \"court_key\": court_key,\n",
    "        \"jurisdiction\": JURISDICTIONS[court_key],\n",
    "        \"year\": year,\n",
    "        \"citation\": generate_citation(court_key, year, i),\n",
    "        \"topic\": topic,\n",
    "        \"is_overruled\": is_overruled,\n",
    "        \"overruled_by\": generate_citation(court_key, year + random.randint(1, 5), i + NUM_CASES) if is_overruled else None,\n",
    "        \"text\": generate_opinion_text(topic, length),\n",
    "    }\n",
    "    cases.append(case)\n",
    "\n",
    "df_cases = pd.DataFrame(cases)\n",
    "print(f\"Generated {len(cases)} synthetic cases\")\n",
    "print(f\"Courts: {df_cases['court'].nunique()} unique courts\")\n",
    "print(f\"Years: {df_cases['year'].min()} - {df_cases['year'].max()}\")\n",
    "print(f\"Overruled: {df_cases['is_overruled'].sum()} cases ({df_cases['is_overruled'].mean()*100:.1f}%)\")\n",
    "print(f\"\\nSample case:\")\n",
    "print(json.dumps({k: v for k, v in cases[0].items() if k != 'text'}, indent=2))\n",
    "print(f\"Text preview: {cases[0]['text'][:200]}...\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Chunking ---\n",
    "# Legal opinions are too long to embed as single documents.\n",
    "# We split them into overlapping chunks that preserve context.\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 512, overlap: int = 64) -> List[str]:\n",
    "    \"\"\"Split text into overlapping token-level chunks.\n",
    "\n",
    "    Args:\n",
    "        text: The full opinion text\n",
    "        chunk_size: Maximum tokens per chunk (using word-level approximation)\n",
    "        overlap: Number of overlapping tokens between consecutive chunks\n",
    "\n",
    "    Returns:\n",
    "        List of text chunks\n",
    "\n",
    "    Hints:\n",
    "        1. Split text into words (tokens ~ words for this approximation)\n",
    "        2. Use a sliding window with step = chunk_size - overlap\n",
    "        3. Join words back into strings for each chunk\n",
    "        4. Handle the final chunk (may be shorter than chunk_size)\n",
    "        5. Ensure no empty chunks are returned\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "# Verification cell\n",
    "# After implementing chunk_text, run this to verify:\n",
    "sample_text = \"word \" * 1500  # 1500-word text\n",
    "chunks = chunk_text(sample_text, chunk_size=512, overlap=64)\n",
    "assert chunks is not None, \"chunk_text returned None -- did you implement it?\"\n",
    "assert len(chunks) > 1, f\"Expected multiple chunks for a 1500-word text, got {len(chunks)}\"\n",
    "assert all(len(c.split()) <= 512 for c in chunks), \"Some chunks exceed chunk_size\"\n",
    "# Check overlap: last 64 words of chunk[0] should appear at start of chunk[1]\n",
    "words_0 = chunks[0].split()\n",
    "words_1 = chunks[1].split()\n",
    "assert words_0[-64:] == words_1[:64], \"Overlap not preserved between consecutive chunks\"\n",
    "print(f\"Chunking verified: {len(chunks)} chunks from 1500 words\")\n",
    "print(f\"Chunk sizes: {[len(c.split()) for c in chunks]}\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Metadata Extraction ---\n",
    "# Each chunk must retain its parent case's metadata for filtering.\n",
    "\n",
    "def extract_metadata(case: dict, chunk_idx: int) -> dict:\n",
    "    \"\"\"Extract structured metadata from a case record for a given chunk.\n",
    "\n",
    "    Args:\n",
    "        case: The full case record dict\n",
    "        chunk_idx: Index of the chunk within this case\n",
    "\n",
    "    Returns:\n",
    "        Dict with keys: 'case_id', 'case_name', 'court', 'jurisdiction',\n",
    "        'year', 'citation', 'is_overruled', 'overruled_by', 'topic', 'chunk_idx'\n",
    "\n",
    "    Hints:\n",
    "        1. Copy the relevant metadata fields from the case dict\n",
    "        2. Add chunk_idx so we can trace back to the parent case\n",
    "        3. Ensure 'year' is an integer (not string)\n",
    "        4. Ensure 'is_overruled' is a boolean\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "# Verification cell\n",
    "sample_meta = extract_metadata(cases[0], 0)\n",
    "assert sample_meta is not None, \"extract_metadata returned None\"\n",
    "assert \"case_id\" in sample_meta, \"Missing 'case_id' key\"\n",
    "assert \"jurisdiction\" in sample_meta, \"Missing 'jurisdiction' key\"\n",
    "assert isinstance(sample_meta[\"year\"], int), \"'year' should be an integer\"\n",
    "assert isinstance(sample_meta[\"is_overruled\"], bool), \"'is_overruled' should be a boolean\"\n",
    "print(f\"Metadata extraction verified: {sample_meta}\")"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build Chunk Corpus ---\n",
    "# Apply chunking and metadata extraction to all cases.\n",
    "\n",
    "all_chunks = []\n",
    "all_metadata = []\n",
    "\n",
    "for case in tqdm(cases, desc=\"Chunking cases\"):\n",
    "    chunks = chunk_text(case[\"text\"], chunk_size=512, overlap=64)\n",
    "    if chunks is None:\n",
    "        continue\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        all_chunks.append(chunk)\n",
    "        all_metadata.append(extract_metadata(case, idx))\n",
    "\n",
    "print(f\"\\nTotal chunks: {len(all_chunks)}\")\n",
    "print(f\"Average chunks per case: {len(all_chunks)/len(cases):.1f}\")\n",
    "print(f\"Chunks with overruled cases: {sum(1 for m in all_metadata if m['is_overruled'])}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. Why do we use overlapping chunks instead of non-overlapping splits? What information would be lost at chunk boundaries without overlap?\n",
    "2. If a legal argument spans 800 words but our chunk size is 512, what happens to that argument? How could you improve the chunking strategy to preserve argumentative coherence?\n",
    "3. Why is it critical that each chunk retains its parent case's metadata? What failure mode from the article does this help prevent?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2 Exploratory Data Analysis\n",
    "\n",
    "Before building the retrieval pipeline, we need to understand the structure and characteristics of our corpus. This analysis will inform design decisions about chunk size, filtering strategies, and retrieval parameters."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Opinion Length Distribution ---\n",
    "\n",
    "# TODO: Plot the distribution of opinion lengths (in words) across all cases.\n",
    "# Use df_cases and the 'text' column.\n",
    "# Include:\n",
    "#   1. A histogram with 50 bins\n",
    "#   2. Vertical lines for the mean and 95th percentile\n",
    "#   3. Proper axis labels and title\n",
    "#   4. Print the mean, median, and 95th percentile values\n",
    "\n",
    "# Hints:\n",
    "#   - Compute word counts: df_cases['text'].str.split().str.len()\n",
    "#   - Use np.percentile for P95\n",
    "#   - Use plt.axvline for vertical reference lines\n",
    "\n",
    "# YOUR CODE HERE"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Temporal Distribution ---\n",
    "\n",
    "# TODO: Create a bar chart showing the number of opinions per court per 5-year period.\n",
    "# Group years into bins: 2000-2004, 2005-2009, 2010-2014, 2015-2019, 2020-2024.\n",
    "#\n",
    "# Include:\n",
    "#   1. Grouped or stacked bars, one color per court type\n",
    "#   2. Simplify court types to: \"CA Supreme\", \"CA Appeal\", \"9th Circuit\", \"Federal District\"\n",
    "#   3. Legend, axis labels, and title\n",
    "#\n",
    "# Hints:\n",
    "#   - Create a 'period' column using pd.cut or integer division\n",
    "#   - Group by period and simplified court type\n",
    "#   - Use df.plot(kind='bar') or plt.bar with offsets for grouped bars\n",
    "\n",
    "# YOUR CODE HERE"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Citation Network Analysis ---\n",
    "\n",
    "# TODO: Analyze the overruled case statistics:\n",
    "#   1. What percentage of cases are overruled?\n",
    "#   2. Is the overruled rate uniform across courts, or do some courts have higher rates?\n",
    "#   3. Is there a temporal pattern (are older cases more likely to be overruled)?\n",
    "#\n",
    "# Create two plots:\n",
    "#   (a) Bar chart of overruled rate by court type\n",
    "#   (b) Line chart of overruled rate by year\n",
    "#\n",
    "# Hints:\n",
    "#   - Group by court/year and compute mean of 'is_overruled'\n",
    "#   - Use plt.subplots(1, 2, figsize=(14, 5)) for side-by-side plots\n",
    "\n",
    "# YOUR CODE HERE"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Topic Distribution ---\n",
    "\n",
    "# TODO: Create a horizontal bar chart of the top 15 most common legal topics.\n",
    "# Also compute: how many topics cover 80% of all cases?\n",
    "#\n",
    "# Hints:\n",
    "#   - Use df_cases['topic'].value_counts()\n",
    "#   - plt.barh for horizontal bars (easier to read long labels)\n",
    "#   - Cumulative sum for the 80% threshold\n",
    "\n",
    "# YOUR CODE HERE"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. Based on the opinion length distribution, what chunk size would capture 90% of legal arguments without exceeding 1,000 tokens? Show your reasoning.\n",
    "2. If older cases are more likely to be overruled, how should this affect our retrieval pipeline? Should we penalize older cases in ranking, or handle this differently?\n",
    "3. The topic distribution is likely imbalanced. How might this affect embedding quality for rare topics? What could you do about it?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3 Baseline: BM25 Keyword Search\n",
    "\n",
    "Before building the full context engineering pipeline, we establish a baseline using BM25 â€” a classical keyword-matching retrieval algorithm. This gives us a performance floor to measure improvements against.\n",
    "\n",
    "BM25 scores documents based on term frequency (TF) and inverse document frequency (IDF), without any semantic understanding. It represents the \"pre-context-engineering\" approach."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Build BM25 index over all chunks\n",
    "tokenized_chunks = [chunk.lower().split() for chunk in all_chunks]\n",
    "bm25_index = BM25Okapi(tokenized_chunks)\n",
    "print(f\"BM25 index built over {len(tokenized_chunks)} chunks\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BM25 Retrieval and Context Assembly ---\n",
    "\n",
    "def bm25_retrieve(query: str, bm25: BM25Okapi, chunks: List[str],\n",
    "                  metadata: List[dict], top_k: int = 20) -> List[Tuple[str, dict, float]]:\n",
    "    \"\"\"Retrieve top-K chunks using BM25 keyword matching.\n",
    "\n",
    "    Args:\n",
    "        query: The legal query text\n",
    "        bm25: A fitted BM25Okapi index\n",
    "        chunks: The chunk texts (aligned with the BM25 index)\n",
    "        metadata: The metadata dicts (aligned with chunks)\n",
    "        top_k: Number of results to return\n",
    "\n",
    "    Returns:\n",
    "        List of (chunk_text, metadata_dict, bm25_score) tuples, sorted by score descending\n",
    "\n",
    "    Hints:\n",
    "        1. Tokenize the query the same way as the corpus: query.lower().split()\n",
    "        2. Use bm25.get_scores(tokenized_query) to get all scores\n",
    "        3. Use np.argsort to find top-K indices\n",
    "        4. Return tuples of (chunk, metadata, score)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "def assemble_naive_context(system_prompt: str, query: str,\n",
    "                           retrieved: List[Tuple[str, dict, float]],\n",
    "                           max_tokens: int = 93000) -> str:\n",
    "    \"\"\"Assemble a naive context by concatenating all retrieved chunks.\n",
    "\n",
    "    This represents the baseline approach: no filtering, no reranking,\n",
    "    no strategic placement. Just dump everything in.\n",
    "\n",
    "    Args:\n",
    "        system_prompt: The system instruction text\n",
    "        query: The user's legal query\n",
    "        retrieved: List of (chunk, metadata, score) from bm25_retrieve\n",
    "        max_tokens: Maximum context tokens\n",
    "\n",
    "    Returns:\n",
    "        Assembled context string\n",
    "\n",
    "    Hints:\n",
    "        1. Start with the system prompt\n",
    "        2. Concatenate all retrieved chunks (in retrieval order)\n",
    "        3. Append the query at the end\n",
    "        4. Use len(text) // 4 as a rough token estimate\n",
    "        5. Truncate if over budget (just cut off excess chunks)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "# Verification\n",
    "sample_query = \"Does California law recognize strict liability for defective products?\"\n",
    "sample_results = bm25_retrieve(sample_query, bm25_index, all_chunks, all_metadata, top_k=20)\n",
    "assert sample_results is not None, \"bm25_retrieve returned None\"\n",
    "assert len(sample_results) <= 20, f\"Expected at most 20 results, got {len(sample_results)}\"\n",
    "print(f\"BM25 retrieved {len(sample_results)} chunks for query: '{sample_query}'\")\n",
    "print(f\"Top result score: {sample_results[0][2]:.4f}\")\n",
    "print(f\"Top result jurisdiction: {sample_results[0][1]['jurisdiction']}\")\n",
    "print(f\"Top result court: {sample_results[0][1]['court']}\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate Baseline ---\n",
    "\n",
    "# Test queries representing different legal research tasks\n",
    "TEST_QUERIES = [\n",
    "    {\"query\": \"Does California recognize strict liability for defective products?\",\n",
    "     \"target_jurisdiction\": \"california\", \"target_topic\": \"product liability\"},\n",
    "    {\"query\": \"What is the standard for wrongful termination in violation of public policy?\",\n",
    "     \"target_jurisdiction\": \"california\", \"target_topic\": \"wrongful termination\"},\n",
    "    {\"query\": \"When can a court compel arbitration of employment disputes?\",\n",
    "     \"target_jurisdiction\": \"california\", \"target_topic\": \"arbitration\"},\n",
    "    {\"query\": \"What constitutes intentional infliction of emotional distress under California law?\",\n",
    "     \"target_jurisdiction\": \"california\", \"target_topic\": \"emotional distress\"},\n",
    "    {\"query\": \"What is the statute of limitations for fraud claims in California?\",\n",
    "     \"target_jurisdiction\": \"california\", \"target_topic\": \"statute of limitations\"},\n",
    "    {\"query\": \"Under what circumstances can a landlord evict a tenant in California?\",\n",
    "     \"target_jurisdiction\": \"california\", \"target_topic\": \"landlord-tenant\"},\n",
    "    {\"query\": \"What is the standard for granting summary judgment in federal court?\",\n",
    "     \"target_jurisdiction\": \"federal-9th\", \"target_topic\": \"civil rights\"},\n",
    "    {\"query\": \"How does the Ninth Circuit evaluate Fourth Amendment search claims?\",\n",
    "     \"target_jurisdiction\": \"federal-9th\", \"target_topic\": \"fourth amendment\"},\n",
    "    {\"query\": \"What damages are available for employment discrimination under FEHA?\",\n",
    "     \"target_jurisdiction\": \"california\", \"target_topic\": \"employment discrimination\"},\n",
    "    {\"query\": \"What is the test for unfair business practices under California B&P Code 17200?\",\n",
    "     \"target_jurisdiction\": \"california\", \"target_topic\": \"unfair business practices\"},\n",
    "]\n",
    "\n",
    "\n",
    "def evaluate_baseline(queries: List[dict], bm25: BM25Okapi,\n",
    "                      chunks: List[str], metadata: List[dict],\n",
    "                      top_k: int = 10) -> dict:\n",
    "    \"\"\"Evaluate BM25 baseline on test queries.\n",
    "\n",
    "    Compute:\n",
    "    - Jurisdictional precision: % of retrieved chunks from target jurisdiction\n",
    "    - Topic relevance: % of retrieved chunks matching target topic\n",
    "    - Overruled rate: % of retrieved chunks from overruled cases\n",
    "    - Context utilization: % of retrieved tokens that are \"relevant\"\n",
    "      (matching jurisdiction AND topic)\n",
    "\n",
    "    Args:\n",
    "        queries: List of test query dicts with 'query', 'target_jurisdiction', 'target_topic'\n",
    "        bm25: Fitted BM25 index\n",
    "        chunks: Chunk texts\n",
    "        metadata: Chunk metadata\n",
    "        top_k: Number of chunks to retrieve per query\n",
    "\n",
    "    Returns:\n",
    "        Dict with average metrics across all queries\n",
    "\n",
    "    Hints:\n",
    "        1. For each query, call bm25_retrieve\n",
    "        2. Check each result's metadata against target_jurisdiction and target_topic\n",
    "        3. Compute the four metrics per query, then average across queries\n",
    "        4. Also track per-query results for later analysis\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "baseline_metrics = evaluate_baseline(TEST_QUERIES, bm25_index, all_chunks, all_metadata)\n",
    "assert baseline_metrics is not None, \"evaluate_baseline returned None\"\n",
    "print(\"\\n=== BM25 Baseline Metrics ===\")\n",
    "for metric, value in baseline_metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. Why does BM25 struggle with legal text? Identify two specific failure examples from your results (e.g., a query where the wrong jurisdiction dominated, or a topically irrelevant result scored highly).\n",
    "2. What is the overruled rate in your baseline retrievals? This directly corresponds to the \"context poisoning\" failure mode. How significant is this problem?\n",
    "3. What is the context utilization ratio? If only 40% of retrieved tokens are relevant, what does that imply about the \"context confusion\" failure mode?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4 Context Engineering Pipeline\n",
    "\n",
    "Now we build the full pipeline. Each stage addresses one or more of the four context failure modes using the corresponding strategy from the article.\n",
    "\n",
    "### Stage 1: Query Analysis (Select Strategy)"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LegalQuery:\n",
    "    \"\"\"Structured representation of a legal research query.\"\"\"\n",
    "    legal_question: str\n",
    "    jurisdiction_filter: List[str]\n",
    "    date_range: Tuple[int, int]\n",
    "    strategy_keywords: List[str]\n",
    "    excluded_topics: List[str]\n",
    "\n",
    "# Jurisdiction mapping: code -> list of acceptable court patterns\n",
    "JURISDICTION_MAP = {\n",
    "    \"california\": [\"Supreme Court of California\", \"California Court of Appeal\"],\n",
    "    \"federal-9th\": [\"Ninth Circuit\", \"Northern District of California\",\n",
    "                    \"Central District of California\"],\n",
    "    \"CA\": [\"Supreme Court of California\", \"California Court of Appeal\"],\n",
    "    \"9th-circuit\": [\"Ninth Circuit\"],\n",
    "}\n",
    "\n",
    "# Combined: California queries should also include relevant federal\n",
    "COMBINED_JURISDICTIONS = {\n",
    "    \"california\": [\"Supreme Court of California\", \"California Court of Appeal\",\n",
    "                   \"Ninth Circuit\", \"Northern District of California\",\n",
    "                   \"Central District of California\"],\n",
    "}\n",
    "\n",
    "\n",
    "def parse_legal_query(\n",
    "    query_text: str,\n",
    "    jurisdiction: str,\n",
    "    strategy_notes: str = \"\",\n",
    "    date_range: Optional[Tuple[int, int]] = None\n",
    ") -> LegalQuery:\n",
    "    \"\"\"Parse raw attorney input into a structured legal query.\n",
    "\n",
    "    Args:\n",
    "        query_text: The legal question or research request\n",
    "        jurisdiction: Target jurisdiction (e.g., \"california\", \"federal-9th\")\n",
    "        strategy_notes: Attorney's strategy preferences, comma-separated keywords\n",
    "        date_range: Tuple of (start_year, end_year) or None for default (2000, 2024)\n",
    "\n",
    "    Returns:\n",
    "        LegalQuery with parsed and validated fields\n",
    "\n",
    "    Hints:\n",
    "        1. Validate jurisdiction against JURISDICTION_MAP keys\n",
    "        2. If jurisdiction is 'california', use COMBINED_JURISDICTIONS to include\n",
    "           both state and relevant federal courts\n",
    "        3. Parse strategy_notes by splitting on commas and stripping whitespace\n",
    "        4. Default date_range to (2000, 2024) if not provided\n",
    "        5. excluded_topics can be extracted from strategy_notes if prefixed with \"not:\"\n",
    "           (e.g., \"strict liability, not: criminal\")\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "# Verification\n",
    "test_query = parse_legal_query(\n",
    "    \"Does California recognize strict liability for defective products?\",\n",
    "    \"california\",\n",
    "    \"strict liability, product defects, not: criminal\",\n",
    "    (2010, 2024)\n",
    ")\n",
    "assert test_query is not None, \"parse_legal_query returned None\"\n",
    "assert len(test_query.jurisdiction_filter) > 0, \"No jurisdiction filters set\"\n",
    "assert test_query.date_range == (2010, 2024), f\"Date range incorrect: {test_query.date_range}\"\n",
    "assert \"strict liability\" in test_query.strategy_keywords, \"Strategy keywords not parsed\"\n",
    "print(f\"Query parsed successfully:\")\n",
    "print(f\"  Question: {test_query.legal_question}\")\n",
    "print(f\"  Jurisdictions: {test_query.jurisdiction_filter}\")\n",
    "print(f\"  Date range: {test_query.date_range}\")\n",
    "print(f\"  Strategy keywords: {test_query.strategy_keywords}\")\n",
    "print(f\"  Excluded topics: {test_query.excluded_topics}\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Filtered Retrieval (Select Strategy â€” Hard Filters)"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_corpus(\n",
    "    chunks: List[str],\n",
    "    metadata: List[dict],\n",
    "    query: LegalQuery\n",
    ") -> Tuple[List[str], List[dict]]:\n",
    "    \"\"\"Filter corpus chunks by jurisdiction, date, and validity.\n",
    "\n",
    "    This is the first line of defense against context confusion and\n",
    "    context poisoning. By filtering BEFORE retrieval, we ensure that\n",
    "    irrelevant jurisdictions and overruled cases never enter the\n",
    "    candidate pool.\n",
    "\n",
    "    Args:\n",
    "        chunks: List of text chunks\n",
    "        metadata: List of metadata dicts (aligned with chunks)\n",
    "        query: Parsed LegalQuery with filter criteria\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (filtered_chunks, filtered_metadata)\n",
    "\n",
    "    Hints:\n",
    "        1. For each chunk, check if its court name contains any of the\n",
    "           jurisdiction_filter patterns (use substring matching)\n",
    "        2. Check if the case year falls within query.date_range\n",
    "        3. CRITICAL: Exclude any chunk where is_overruled is True\n",
    "        4. Keep both the chunk and its metadata in sync\n",
    "        5. Print the reduction ratio for debugging\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "# Verification\n",
    "test_lq = parse_legal_query(\n",
    "    \"strict liability for defective products\",\n",
    "    \"california\",\n",
    "    date_range=(2010, 2024)\n",
    ")\n",
    "filtered_chunks, filtered_meta = filter_corpus(all_chunks, all_metadata, test_lq)\n",
    "assert filtered_chunks is not None, \"filter_corpus returned None\"\n",
    "assert len(filtered_chunks) < len(all_chunks), \"Filtering should reduce corpus size\"\n",
    "assert all(not m[\"is_overruled\"] for m in filtered_meta), \"Overruled cases should be excluded\"\n",
    "assert all(m[\"year\"] >= 2010 for m in filtered_meta), \"Cases before 2010 should be excluded\"\n",
    "print(f\"Filtering results:\")\n",
    "print(f\"  Original: {len(all_chunks)} chunks\")\n",
    "print(f\"  Filtered: {len(filtered_chunks)} chunks\")\n",
    "print(f\"  Reduction: {(1 - len(filtered_chunks)/len(all_chunks))*100:.1f}%\")\n",
    "print(f\"  Jurisdictions present: {set(m['jurisdiction'] for m in filtered_meta)}\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3: Semantic Search + Reranking (Select Strategy â€” Soft Ranking)"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "# Load models (these are small enough for Colab)\n",
    "print(\"Loading embedding model...\")\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')  # 22M params, fast\n",
    "print(\"Loading reranker model...\")\n",
    "reranker_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')  # 22M params\n",
    "print(\"Models loaded.\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_retrieve(\n",
    "    query_text: str,\n",
    "    chunks: List[str],\n",
    "    model: SentenceTransformer,\n",
    "    top_k: int = 20\n",
    ") -> List[Tuple[str, float, int]]:\n",
    "    \"\"\"Retrieve top-K chunks by cosine similarity using sentence embeddings.\n",
    "\n",
    "    Args:\n",
    "        query_text: The legal query text\n",
    "        chunks: Filtered corpus chunks\n",
    "        model: A SentenceTransformer model for encoding\n",
    "        top_k: Number of candidates to retrieve\n",
    "\n",
    "    Returns:\n",
    "        List of (chunk_text, similarity_score, original_index) tuples,\n",
    "        sorted by score descending\n",
    "\n",
    "    Hints:\n",
    "        1. Encode the query: model.encode(query_text, normalize_embeddings=True)\n",
    "        2. Encode all chunks: model.encode(chunks, normalize_embeddings=True,\n",
    "           show_progress_bar=True, batch_size=64)\n",
    "        3. Compute cosine similarities via dot product (already normalized)\n",
    "        4. Use np.argsort(scores)[::-1][:top_k] for top-K indices\n",
    "        5. Return chunk text, score, and original index (for metadata lookup)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "def rerank(\n",
    "    query_text: str,\n",
    "    candidates: List[Tuple[str, float, int]],\n",
    "    reranker: CrossEncoder,\n",
    "    top_n: int = 10\n",
    ") -> List[Tuple[str, float, int]]:\n",
    "    \"\"\"Rerank candidates using a cross-encoder for fine-grained relevance.\n",
    "\n",
    "    The cross-encoder sees both query and document together, allowing it\n",
    "    to capture nuanced relevance signals that the bi-encoder misses.\n",
    "\n",
    "    Args:\n",
    "        query_text: The legal query text\n",
    "        candidates: List of (chunk_text, initial_score, original_index)\n",
    "        reranker: A CrossEncoder model\n",
    "        top_n: Number of final results to return\n",
    "\n",
    "    Returns:\n",
    "        List of (chunk_text, reranker_score, original_index) tuples,\n",
    "        sorted by reranker score descending\n",
    "\n",
    "    Hints:\n",
    "        1. Create input pairs: [(query_text, chunk_text) for chunk_text, _, _ in candidates]\n",
    "        2. Get scores: reranker.predict(pairs)\n",
    "        3. Sort by reranker scores (descending)\n",
    "        4. Return top_n results with reranker scores replacing initial scores\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "# Verification\n",
    "if filtered_chunks:\n",
    "    embed_results = embed_and_retrieve(\n",
    "        \"strict liability for defective products\",\n",
    "        filtered_chunks, embed_model, top_k=20\n",
    "    )\n",
    "    assert embed_results is not None, \"embed_and_retrieve returned None\"\n",
    "    assert len(embed_results) == min(20, len(filtered_chunks))\n",
    "    print(f\"Embedding retrieval: {len(embed_results)} candidates\")\n",
    "    print(f\"Score range: {embed_results[-1][1]:.4f} to {embed_results[0][1]:.4f}\")\n",
    "\n",
    "    reranked = rerank(\n",
    "        \"strict liability for defective products\",\n",
    "        embed_results, reranker_model, top_n=10\n",
    "    )\n",
    "    assert reranked is not None, \"rerank returned None\"\n",
    "    assert len(reranked) == min(10, len(embed_results))\n",
    "    print(f\"\\nAfter reranking: {len(reranked)} results\")\n",
    "    print(f\"Reranker score range: {reranked[-1][1]:.4f} to {reranked[0][1]:.4f}\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 4: Context Assembly (All Four Strategies)"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextEngine:\n",
    "    \"\"\"Assembles optimal context for legal brief generation.\n",
    "\n",
    "    Implements all four context engineering strategies:\n",
    "    - Write: Loads persisted case validity index and attorney preferences\n",
    "    - Select: Uses filtered retrieval and reranking results\n",
    "    - Compress: Truncates history to fit token budget, keeping most recent\n",
    "    - Isolate: Structures context with clear XML boundaries\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_tokens: int = 128000, reserved_output: int = 35000):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.reserved = reserved_output\n",
    "        self.available = max_tokens - reserved_output\n",
    "\n",
    "    def estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Estimate token count for a text string.\n",
    "\n",
    "        Hints:\n",
    "            1. Use len(text) // 4 as a rough heuristic\n",
    "            2. Legal text is slightly denser: consider len(text) // 3.8\n",
    "            3. Round up to be conservative\n",
    "        \"\"\"\n",
    "        # TODO: Implement this method\n",
    "        pass\n",
    "\n",
    "    def allocate_budget(\n",
    "        self,\n",
    "        system_tokens: int,\n",
    "        facts_tokens: int,\n",
    "        strategy_tokens: int,\n",
    "        num_rag_docs: int,\n",
    "        avg_doc_tokens: int\n",
    "    ) -> dict:\n",
    "        \"\"\"Dynamically allocate token budget across context components.\n",
    "\n",
    "        Args:\n",
    "            system_tokens: Tokens for system prompt (fixed)\n",
    "            facts_tokens: Tokens for case facts (fixed)\n",
    "            strategy_tokens: Tokens for strategy notes (fixed)\n",
    "            num_rag_docs: Number of retrieved documents available\n",
    "            avg_doc_tokens: Average tokens per retrieved document\n",
    "\n",
    "        Returns:\n",
    "            Dict with keys: 'system', 'facts', 'strategy', 'rag',\n",
    "            'history', 'buffer' and their token allocations\n",
    "\n",
    "        Hints:\n",
    "            1. Fixed allocations: system + facts + strategy\n",
    "            2. Remaining = self.available - fixed\n",
    "            3. RAG budget = min(num_rag_docs * avg_doc_tokens, remaining * 0.6)\n",
    "            4. History budget = min(remaining * 0.25, remaining - rag_budget)\n",
    "            5. Buffer = remaining - rag - history (at least 5% of remaining)\n",
    "            6. If RAG under-utilizes its budget, give excess to history\n",
    "        \"\"\"\n",
    "        # TODO: Implement this method\n",
    "        pass\n",
    "\n",
    "    def compress_history(self, history: List[str], budget_tokens: int) -> List[str]:\n",
    "        \"\"\"Keep most recent history within token budget.\n",
    "\n",
    "        Args:\n",
    "            history: List of conversation messages (oldest first)\n",
    "            budget_tokens: Maximum tokens for history\n",
    "\n",
    "        Returns:\n",
    "            List of retained messages (chronological order)\n",
    "\n",
    "        Hints:\n",
    "            1. Iterate from most recent (reversed) and accumulate tokens\n",
    "            2. Stop when adding the next message would exceed budget\n",
    "            3. Reverse back to chronological order before returning\n",
    "        \"\"\"\n",
    "        # TODO: Implement this method\n",
    "        pass\n",
    "\n",
    "    def assemble(\n",
    "        self,\n",
    "        system_prompt: str,\n",
    "        case_facts: str,\n",
    "        strategy_notes: str,\n",
    "        retrieved_docs: List[Tuple[str, float]],\n",
    "        history: List[str],\n",
    "        memory: List[str]\n",
    "    ) -> str:\n",
    "        \"\"\"Assemble the complete context with XML-tagged structure.\n",
    "\n",
    "        Args:\n",
    "            system_prompt: System instructions for the LLM\n",
    "            case_facts: The case facts provided by the attorney\n",
    "            strategy_notes: The attorney's strategy preferences\n",
    "            retrieved_docs: List of (chunk_text, relevance_score) tuples\n",
    "            history: Conversation history messages\n",
    "            memory: Persisted memory items (from prior sessions)\n",
    "\n",
    "        Returns:\n",
    "            Complete context string within token budget\n",
    "\n",
    "        Hints:\n",
    "            1. Compute token estimates for fixed components\n",
    "            2. Call allocate_budget to get per-component limits\n",
    "            3. Compress history to fit its budget\n",
    "            4. Truncate retrieved_docs to fit RAG budget (keep highest-scored)\n",
    "            5. IMPORTANT: Place the #1 and #2 most relevant docs at the\n",
    "               BEGINNING. Place the #3 most relevant doc at the END.\n",
    "               Fill middle positions with remaining docs.\n",
    "               This mitigates the \"Lost in the Middle\" effect.\n",
    "            6. Format with XML tags: <system>, <memory>, <case_facts>,\n",
    "               <strategy>, <history>, <retrieved_authority>, <query>\n",
    "            7. Include relevance scores in the retrieved section\n",
    "            8. Verify total tokens <= self.available\n",
    "        \"\"\"\n",
    "        # TODO: Implement this method\n",
    "        pass\n",
    "\n",
    "\n",
    "# Verification\n",
    "engine = ContextEngine(max_tokens=128000, reserved_output=35000)\n",
    "\n",
    "# Test token estimation\n",
    "test_tokens = engine.estimate_tokens(\"This is a test sentence with about ten words.\")\n",
    "assert test_tokens is not None, \"estimate_tokens returned None\"\n",
    "assert 5 < test_tokens < 20, f\"Token estimate seems off: {test_tokens}\"\n",
    "print(f\"Token estimation: '{test_tokens}' tokens for a ~10-word sentence\")\n",
    "\n",
    "# Test budget allocation\n",
    "budget = engine.allocate_budget(\n",
    "    system_tokens=500, facts_tokens=1000, strategy_tokens=200,\n",
    "    num_rag_docs=10, avg_doc_tokens=512\n",
    ")\n",
    "assert budget is not None, \"allocate_budget returned None\"\n",
    "assert sum(budget.values()) <= engine.available, \"Budget exceeds available tokens\"\n",
    "print(f\"\\nBudget allocation: {budget}\")\n",
    "print(f\"Total allocated: {sum(budget.values())} / {engine.available} available\")\n",
    "\n",
    "# Test full assembly\n",
    "sample_context = engine.assemble(\n",
    "    system_prompt=\"You are a legal research assistant for California litigation.\",\n",
    "    case_facts=\"The plaintiff alleges product defect in a consumer device.\",\n",
    "    strategy_notes=\"Argue strict liability under California law.\",\n",
    "    retrieved_docs=[(\"Sample legal text \" * 50, 0.92), (\"Another case \" * 50, 0.87)],\n",
    "    history=[\"Previous turn about case background\"],\n",
    "    memory=[\"Attorney prefers concise briefs\", \"California jurisdiction focus\"]\n",
    ")\n",
    "assert sample_context is not None, \"assemble returned None\"\n",
    "assert \"<system>\" in sample_context, \"Missing <system> tag\"\n",
    "assert \"<retrieved_authority>\" in sample_context, \"Missing <retrieved_authority> tag\"\n",
    "total_tokens = engine.estimate_tokens(sample_context)\n",
    "assert total_tokens <= engine.available, f\"Context exceeds budget: {total_tokens} > {engine.available}\"\n",
    "print(f\"\\nAssembled context: {total_tokens} tokens\")\n",
    "print(f\"First 500 chars:\\n{sample_context[:500]}\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 5: Citation Verification (Write Strategy)"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_validity_index(metadata: List[dict]) -> dict:\n",
    "    \"\"\"Build a citation validity lookup from corpus metadata.\n",
    "\n",
    "    This implements the 'Write' strategy: we persist a lookup table\n",
    "    of citation validity OUTSIDE the context window. Every generated\n",
    "    citation is checked against this index before being included.\n",
    "\n",
    "    Args:\n",
    "        metadata: List of chunk metadata dicts\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping citation string -> {\n",
    "            'status': 'valid' | 'overruled' | 'superseded',\n",
    "            'overruled_by': str or None,\n",
    "            'year': int,\n",
    "            'case_name': str\n",
    "        }\n",
    "\n",
    "    Hints:\n",
    "        1. Iterate through metadata, keyed by 'citation'\n",
    "        2. For overruled cases, set status='overruled' and include 'overruled_by'\n",
    "        3. For valid cases, set status='valid'\n",
    "        4. Handle duplicates: if the same citation appears multiple times\n",
    "           (from multiple chunks), keep the entry with the most information\n",
    "        5. This should be a simple dictionary lookup -- O(1) per citation check\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "def verify_citations(\n",
    "    generated_text: str,\n",
    "    validity_index: dict\n",
    ") -> List[dict]:\n",
    "    \"\"\"Extract and verify all case citations in generated text.\n",
    "\n",
    "    Args:\n",
    "        generated_text: The LLM-generated legal brief text\n",
    "        validity_index: The citation validity lookup from build_validity_index\n",
    "\n",
    "    Returns:\n",
    "        List of dicts, each with:\n",
    "        - 'citation': the extracted citation string\n",
    "        - 'status': 'valid', 'overruled', or 'unverified'\n",
    "        - 'found_in_index': bool\n",
    "        - 'recommendation': 'keep', 'flag', or 'remove'\n",
    "        - 'details': additional info (e.g., overruling case)\n",
    "\n",
    "    Hints:\n",
    "        1. Use regex to extract citations matching legal citation patterns:\n",
    "           r'\\\\d+\\\\s+(?:Cal\\\\.(?:3d|4th|5th)|Cal\\\\.App\\\\.(?:4th|5th)|F\\\\.3d|F\\\\.Supp\\\\.(?:2d|3d))\\\\s+\\\\d+\\\\s+\\\\(\\\\d{4}\\\\)'\n",
    "        2. For each extracted citation, look it up in the validity index\n",
    "        3. If found and valid -> recommendation='keep'\n",
    "        4. If found and overruled -> recommendation='remove', include overruling case\n",
    "        5. If not found -> recommendation='flag' (unverified)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "# Verification\n",
    "validity_idx = build_validity_index(all_metadata)\n",
    "assert validity_idx is not None, \"build_validity_index returned None\"\n",
    "assert len(validity_idx) > 0, \"Validity index is empty\"\n",
    "\n",
    "# Count overruled entries\n",
    "overruled_count = sum(1 for v in validity_idx.values() if v['status'] == 'overruled')\n",
    "print(f\"Validity index built: {len(validity_idx)} unique citations\")\n",
    "print(f\"  Valid: {sum(1 for v in validity_idx.values() if v['status'] == 'valid')}\")\n",
    "print(f\"  Overruled: {overruled_count}\")\n",
    "\n",
    "# Test citation verification on synthetic text\n",
    "sample_brief = \"The court in \" + list(validity_idx.keys())[0] + \" established that...\"\n",
    "results = verify_citations(sample_brief, validity_idx)\n",
    "assert results is not None, \"verify_citations returned None\"\n",
    "print(f\"\\nCitation verification test: {len(results)} citations found\")\n",
    "for r in results:\n",
    "    print(f\"  {r['citation']} -> {r['recommendation']}\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. The citation verification step happens AFTER generation. Could we prevent bad citations from being generated in the first place? How would you modify the pipeline?\n",
    "2. What if a citation is valid but from the wrong jurisdiction? The validity index only tracks overruled status. How would you extend it to also flag jurisdictional mismatches?\n",
    "3. The \"Write\" strategy persists the validity index outside the context window. What other information should be persisted? Think about what an attorney would want remembered across sessions.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.5 Training: Embedding Model Fine-tuning\n",
    "\n",
    "In this section, we fine-tune the embedding model to improve retrieval quality on legal text. The key insight: a general-purpose embedding model does not understand that \"battery\" (the tort) and \"battery\" (the device) should be far apart in embedding space when the query is about personal injury law."
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate Training Pairs ---\n",
    "# We create query-passage pairs from our corpus.\n",
    "# Positive pairs: query about a topic matched with a chunk on that topic\n",
    "# Hard negatives: chunks that are semantically similar but wrong topic/jurisdiction\n",
    "\n",
    "def generate_training_pairs(\n",
    "    chunks: List[str],\n",
    "    metadata: List[dict],\n",
    "    num_pairs: int = 500\n",
    ") -> List[dict]:\n",
    "    \"\"\"Generate training pairs for embedding fine-tuning.\n",
    "\n",
    "    Each pair consists of:\n",
    "    - query: a synthetic legal query about a topic\n",
    "    - positive: a chunk that matches the query's topic and jurisdiction\n",
    "    - hard_negatives: 3 chunks that are topically similar but wrong\n",
    "      jurisdiction or wrong subtopic\n",
    "\n",
    "    Args:\n",
    "        chunks: All corpus chunks\n",
    "        metadata: Aligned metadata\n",
    "        num_pairs: Number of training pairs to generate\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with keys: 'query', 'positive', 'hard_negatives'\n",
    "\n",
    "    Hints:\n",
    "        1. Group chunks by topic using metadata\n",
    "        2. For each pair, randomly select a topic and jurisdiction\n",
    "        3. Create a query template: f\"What does {jurisdiction} law say about {topic}?\"\n",
    "        4. Positive: random chunk matching both topic and jurisdiction\n",
    "        5. Hard negatives: chunks matching topic but WRONG jurisdiction (hardest),\n",
    "           or matching jurisdiction but WRONG topic (easier negative)\n",
    "        6. Include 2 wrong-jurisdiction and 1 wrong-topic negatives per pair\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "training_pairs = generate_training_pairs(all_chunks, all_metadata, num_pairs=500)\n",
    "assert training_pairs is not None, \"generate_training_pairs returned None\"\n",
    "assert len(training_pairs) > 0, \"No training pairs generated\"\n",
    "print(f\"Generated {len(training_pairs)} training pairs\")\n",
    "print(f\"Sample pair:\")\n",
    "print(f\"  Query: {training_pairs[0]['query'][:100]}...\")\n",
    "print(f\"  Positive: {training_pairs[0]['positive'][:100]}...\")\n",
    "print(f\"  Num hard negatives: {len(training_pairs[0]['hard_negatives'])}\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- InfoNCE Contrastive Loss ---\n",
    "\n",
    "def info_nce_loss(query_emb, positive_emb, negative_embs, temperature=0.07):\n",
    "    \"\"\"Compute InfoNCE contrastive loss.\n",
    "\n",
    "    This loss pulls the query embedding closer to the positive document\n",
    "    and pushes it away from negative documents in embedding space.\n",
    "\n",
    "    Args:\n",
    "        query_emb: Query embedding tensor, shape (batch_size, embed_dim)\n",
    "        positive_emb: Positive document embedding, shape (batch_size, embed_dim)\n",
    "        negative_embs: Negative document embeddings, shape (batch_size, num_neg, embed_dim)\n",
    "        temperature: Temperature parameter (controls sharpness of distribution)\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss value\n",
    "\n",
    "    Hints:\n",
    "        1. Compute positive similarity: (query_emb * positive_emb).sum(dim=-1) / temperature\n",
    "        2. Compute negative similarities: torch.bmm(negative_embs, query_emb.unsqueeze(-1)).squeeze(-1) / temperature\n",
    "        3. Concatenate positive and negative similarities: logits shape (batch_size, 1 + num_neg)\n",
    "        4. Labels are all zeros (positive is at index 0)\n",
    "        5. Use F.cross_entropy(logits, labels)\n",
    "\n",
    "    Mathematical reference:\n",
    "        L = -log(exp(sim(q, d+) / tau) / (exp(sim(q, d+) / tau) + sum(exp(sim(q, d-) / tau))))\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "# Verification with random tensors\n",
    "batch_q = torch.randn(4, 384)  # batch of 4 queries, 384-dim embeddings\n",
    "batch_p = torch.randn(4, 384)  # 4 positive docs\n",
    "batch_n = torch.randn(4, 3, 384)  # 3 negatives per query\n",
    "# Normalize\n",
    "batch_q = F.normalize(batch_q, dim=-1)\n",
    "batch_p = F.normalize(batch_p, dim=-1)\n",
    "batch_n = F.normalize(batch_n, dim=-1)\n",
    "\n",
    "loss = info_nce_loss(batch_q, batch_p, batch_n)\n",
    "assert loss is not None, \"info_nce_loss returned None\"\n",
    "assert loss.item() > 0, \"Loss should be positive\"\n",
    "print(f\"InfoNCE loss on random embeddings: {loss.item():.4f}\")\n",
    "print(f\"Expected: ~log(4) = {np.log(4):.4f} for random vectors\")"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fine-tuning Loop ---\n",
    "# Fine-tune the embedding model on legal domain data.\n",
    "\n",
    "# TODO: Implement the fine-tuning loop.\n",
    "#\n",
    "# Structure:\n",
    "# 1. Create a simple Dataset class that yields (query, positive, negatives) tuples\n",
    "# 2. Use a DataLoader with batch_size=8\n",
    "# 3. For each batch:\n",
    "#    a. Encode queries, positives, and negatives using embed_model.encode()\n",
    "#    b. Convert to tensors\n",
    "#    c. Compute InfoNCE loss\n",
    "#    d. Backpropagate and update (use AdamW optimizer, lr=2e-5)\n",
    "# 4. Train for 3 epochs\n",
    "# 5. Plot the training loss curve\n",
    "#\n",
    "# Note: For full fine-tuning you would use the sentence-transformers\n",
    "# training API. Here we demonstrate the loss computation and training\n",
    "# dynamics conceptually.\n",
    "#\n",
    "# Hints:\n",
    "#   - For Colab efficiency, you can fine-tune only the last 2 layers\n",
    "#   - Log loss every 10 steps\n",
    "#   - Save the loss values for plotting\n",
    "\n",
    "# YOUR CODE HERE"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare Before/After Fine-tuning ---\n",
    "\n",
    "# TODO: Run the embedding retrieval (embed_and_retrieve) on the test queries\n",
    "# using both the original and fine-tuned models. Compare Relevance@10 (using\n",
    "# the topic and jurisdiction matching from the baseline evaluation).\n",
    "#\n",
    "# Create a bar chart comparing:\n",
    "#   - BM25 baseline topic relevance\n",
    "#   - Original embedding model topic relevance\n",
    "#   - Fine-tuned embedding model topic relevance (if fine-tuning was run)\n",
    "#\n",
    "# Hints:\n",
    "#   - Reuse the evaluate_baseline pattern but with embed_and_retrieve instead of bm25_retrieve\n",
    "#   - You may need to re-filter the corpus for each query first\n",
    "\n",
    "# YOUR CODE HERE"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. Why do we use hard negatives (wrong jurisdiction, similar topic) rather than random negatives? How does this affect what the embedding model learns?\n",
    "2. The temperature parameter $\\tau$ in InfoNCE is set to 0.07. What happens if you increase it to 0.5? What if you decrease it to 0.01? Think about the gradient magnitude.\n",
    "3. We only fine-tune on 500 pairs. In production, BriefEngine would use thousands of pairs from attorney feedback. How would you collect high-quality training data from real attorney usage without burdening the attorneys?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.6 Evaluation\n",
    "\n",
    "Now we evaluate the complete pipeline (filtering + embedding retrieval + reranking + context assembly) against the BM25 baseline."
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_full_pipeline(\n",
    "    queries: List[dict],\n",
    "    all_chunks: List[str],\n",
    "    all_metadata: List[dict],\n",
    "    embed_model: SentenceTransformer,\n",
    "    reranker: CrossEncoder,\n",
    "    top_k_retrieval: int = 20,\n",
    "    top_n_rerank: int = 10\n",
    ") -> dict:\n",
    "    \"\"\"Evaluate the full context engineering pipeline on test queries.\n",
    "\n",
    "    For each query:\n",
    "    1. Parse the query into a LegalQuery\n",
    "    2. Filter the corpus by jurisdiction and date\n",
    "    3. Embed and retrieve top-K candidates\n",
    "    4. Rerank to top-N final results\n",
    "    5. Compute metrics: jurisdictional precision, topic relevance,\n",
    "       overruled rate, context utilization\n",
    "\n",
    "    Args:\n",
    "        queries: List of test query dicts\n",
    "        all_chunks, all_metadata: Full corpus\n",
    "        embed_model: SentenceTransformer for retrieval\n",
    "        reranker: CrossEncoder for reranking\n",
    "        top_k_retrieval: Candidates from embedding search\n",
    "        top_n_rerank: Final results after reranking\n",
    "\n",
    "    Returns:\n",
    "        Dict with average metrics and per-query results\n",
    "\n",
    "    Hints:\n",
    "        1. For each query, call parse_legal_query first\n",
    "        2. Then filter_corpus to narrow candidates\n",
    "        3. Then embed_and_retrieve on filtered set\n",
    "        4. Then rerank the candidates\n",
    "        5. Compute the same metrics as evaluate_baseline for fair comparison\n",
    "        6. Also measure: latency per query (time each stage)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "pipeline_metrics = evaluate_full_pipeline(\n",
    "    TEST_QUERIES, all_chunks, all_metadata, embed_model, reranker_model\n",
    ")\n",
    "assert pipeline_metrics is not None, \"evaluate_full_pipeline returned None\"\n",
    "print(\"\\n=== Full Pipeline Metrics ===\")\n",
    "for metric, value in pipeline_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric}: {value:.3f}\")"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Comparison Table ---\n",
    "\n",
    "# TODO: Create a comparison table (as a pandas DataFrame) showing\n",
    "# baseline vs. pipeline metrics side by side.\n",
    "#\n",
    "# Columns: Metric | BM25 Baseline | Full Pipeline | Improvement\n",
    "# Rows: Jurisdictional Precision, Topic Relevance, Overruled Rate,\n",
    "#        Context Utilization\n",
    "#\n",
    "# Also create a grouped bar chart visualizing the comparison.\n",
    "#\n",
    "# Hints:\n",
    "#   - Use pd.DataFrame with the metrics dicts\n",
    "#   - Improvement = (pipeline - baseline) / baseline * 100 for increases\n",
    "#   - For overruled rate, improvement = (baseline - pipeline) / baseline * 100 (lower is better)\n",
    "\n",
    "# YOUR CODE HERE"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Precision-Recall Curve ---\n",
    "\n",
    "# TODO: Plot a precision-recall curve for the retrieval component.\n",
    "#\n",
    "# For each query:\n",
    "# 1. Retrieve top-50 candidates via embedding search\n",
    "# 2. At each cutoff K (1, 2, 3, ..., 50), compute:\n",
    "#    - Precision@K = (relevant docs in top K) / K\n",
    "#    - Recall@K = (relevant docs in top K) / (total relevant docs)\n",
    "# 3. Plot the average P-R curve across all test queries\n",
    "#\n",
    "# Also mark the operating point (K=10 after reranking) on the curve.\n",
    "#\n",
    "# Hints:\n",
    "#   - \"Relevant\" means matching the target_topic from the test query\n",
    "#   - Use matplotlib to plot with labeled axes\n",
    "#   - The curve should show the precision-recall tradeoff clearly\n",
    "\n",
    "# YOUR CODE HERE"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. By how much did the pipeline improve over the BM25 baseline on each metric? Which improvement was largest? Why?\n",
    "2. What is the overruled rate in the full pipeline results? If it is not 0%, explain what went wrong â€” the filtering should have excluded all overruled cases.\n",
    "3. Look at the precision-recall curve. Is there a clear \"elbow\" point? What does this suggest about the optimal number of documents to include in context?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.7 Error Analysis\n",
    "\n",
    "Systematic error analysis is critical for improving the pipeline. We categorize errors into the four failure modes from the article."
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(\n",
    "    queries: List[dict],\n",
    "    pipeline_results: dict,\n",
    "    all_metadata: List[dict]\n",
    ") -> dict:\n",
    "    \"\"\"Categorize pipeline errors into the four context failure modes.\n",
    "\n",
    "    For each query result, check for:\n",
    "    1. Poisoning: Any overruled case made it into final results\n",
    "    2. Distraction: A highly relevant document was in the filtered corpus\n",
    "       but did not appear in the top-N results (false negative)\n",
    "    3. Confusion: An irrelevant document (wrong topic) appeared in top-N\n",
    "    4. Clash: Retrieved documents contain conflicting legal conclusions\n",
    "       on the same issue\n",
    "\n",
    "    Args:\n",
    "        queries: Test query dicts\n",
    "        pipeline_results: Output from evaluate_full_pipeline (includes per-query results)\n",
    "        all_metadata: Full corpus metadata (for checking what was missed)\n",
    "\n",
    "    Returns:\n",
    "        Dict with counts and examples for each failure mode:\n",
    "        {\n",
    "            'poisoning': {'count': int, 'examples': [...]},\n",
    "            'distraction': {'count': int, 'examples': [...]},\n",
    "            'confusion': {'count': int, 'examples': [...]},\n",
    "            'clash': {'count': int, 'examples': [...]},\n",
    "        }\n",
    "\n",
    "    Hints:\n",
    "        1. For poisoning: check is_overruled in result metadata\n",
    "        2. For distraction: check if any chunk with matching topic was in\n",
    "           the filtered set but NOT in the final top-N\n",
    "        3. For confusion: check if any top-N result has a non-matching topic\n",
    "        4. For clash: this is harder to detect automatically. Use a heuristic:\n",
    "           if two retrieved chunks are about the same topic but from different\n",
    "           court levels (e.g., trial vs. appellate), flag as potential clash\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "# Run error analysis\n",
    "# Note: This requires per-query results from evaluate_full_pipeline.\n",
    "# If your evaluate_full_pipeline stores per-query data, use it here.\n",
    "# Otherwise, re-run the pipeline with per-query result tracking.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# TODO: Print a summary of error counts by failure mode\n",
    "# TODO: For the most frequent failure mode, show 2 detailed examples\n",
    "# TODO: Answer: Which failure mode is hardest to detect automatically? Why?"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. Which failure mode was most common in your pipeline? Was this expected based on the pipeline design?\n",
    "2. Which failure mode is hardest to detect automatically? Propose a method for detecting it that goes beyond simple heuristics.\n",
    "3. For each of the top 3 failure modes, propose one specific pipeline change that would reduce its frequency. Be concrete â€” name the function you would modify and what you would change.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.8 Scalability and Deployment\n",
    "\n",
    "Profile the pipeline to understand latency characteristics and identify optimization opportunities."
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_pipeline(\n",
    "    query: str,\n",
    "    jurisdiction: str,\n",
    "    all_chunks: List[str],\n",
    "    all_metadata: List[dict],\n",
    "    embed_model: SentenceTransformer,\n",
    "    reranker: CrossEncoder,\n",
    "    num_runs: int = 5\n",
    ") -> dict:\n",
    "    \"\"\"Profile each stage of the pipeline for latency.\n",
    "\n",
    "    Run the full pipeline num_runs times and report:\n",
    "    - Mean and P95 latency for each stage\n",
    "    - Total end-to-end latency\n",
    "\n",
    "    Stages to profile:\n",
    "    1. Query parsing\n",
    "    2. Corpus filtering\n",
    "    3. Embedding + retrieval\n",
    "    4. Reranking\n",
    "    5. Context assembly\n",
    "\n",
    "    Args:\n",
    "        query: Test query string\n",
    "        jurisdiction: Target jurisdiction\n",
    "        all_chunks, all_metadata: Full corpus\n",
    "        embed_model: SentenceTransformer\n",
    "        reranker: CrossEncoder\n",
    "        num_runs: Number of profiling runs\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping stage_name -> {'mean_ms': float, 'p95_ms': float}\n",
    "\n",
    "    Hints:\n",
    "        1. Use time.perf_counter() for high-resolution timing\n",
    "        2. Run each stage independently within the loop\n",
    "        3. Store per-run timings in a list, then compute mean and P95\n",
    "        4. P95 = np.percentile(timings, 95)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "# Run profiling\n",
    "profile_results = profile_pipeline(\n",
    "    \"Does California recognize strict liability for defective products?\",\n",
    "    \"california\",\n",
    "    all_chunks, all_metadata, embed_model, reranker_model\n",
    ")\n",
    "assert profile_results is not None, \"profile_pipeline returned None\"\n",
    "print(\"\\n=== Pipeline Latency Profile ===\")\n",
    "print(f\"{'Stage':<25} {'Mean (ms)':<15} {'P95 (ms)':<15}\")\n",
    "print(\"-\" * 55)\n",
    "total_mean = 0\n",
    "for stage, timings in profile_results.items():\n",
    "    print(f\"{stage:<25} {timings['mean_ms']:<15.1f} {timings['p95_ms']:<15.1f}\")\n",
    "    total_mean += timings['mean_ms']\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'TOTAL':<25} {total_mean:<15.1f}\")"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optimization Analysis ---\n",
    "\n",
    "# TODO: Based on the profiling results:\n",
    "# 1. Identify the top 2 latency bottlenecks\n",
    "# 2. For each bottleneck, propose a specific optimization and estimate\n",
    "#    the expected speedup\n",
    "# 3. Implement ONE optimization (the simpler one) and re-profile to\n",
    "#    measure the actual improvement\n",
    "#\n",
    "# Common optimizations to consider:\n",
    "#   - Pre-compute and cache embeddings for the full corpus\n",
    "#   - Batch encoding instead of single-document encoding\n",
    "#   - Use approximate nearest neighbor (FAISS) instead of brute-force search\n",
    "#   - Reduce the number of reranking candidates\n",
    "#\n",
    "# Hints:\n",
    "#   - If embedding is the bottleneck, try pre-computing corpus embeddings once\n",
    "#   - If reranking is the bottleneck, try reducing top_k from 20 to 10\n",
    "#   - Measure the impact on both latency AND retrieval quality\n",
    "\n",
    "# YOUR CODE HERE"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. Which stage dominates latency? Is this consistent with BriefEngine's latency budget from the case study?\n",
    "2. Pre-computing corpus embeddings trades storage for latency. How much storage would 42 million 1024-dimensional float32 embeddings require? Is this feasible?\n",
    "3. If you reduce the reranking candidates from 20 to 10, how does retrieval quality change? Is the latency reduction worth the quality tradeoff?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.9 Ethical and Regulatory Analysis\n",
    "\n",
    "Legal AI raises unique ethical challenges that require careful consideration."
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Temporal Bias Analysis ---\n",
    "\n",
    "# TODO: Investigate whether the retrieval pipeline has a temporal bias.\n",
    "# Run all 10 test queries through the full pipeline and collect the\n",
    "# publication years of all retrieved documents.\n",
    "#\n",
    "# Create:\n",
    "# 1. A histogram of retrieved document years (across all queries)\n",
    "# 2. Compare it against the corpus distribution of years\n",
    "# 3. Compute the KL divergence between retrieved and corpus distributions\n",
    "#\n",
    "# If there is a bias toward older or newer cases, explain:\n",
    "# - Why it might exist (training data distribution? embedding space properties?)\n",
    "# - Whether it is desirable or harmful for legal research\n",
    "# - How you would correct it if harmful\n",
    "#\n",
    "# Hints:\n",
    "#   - Collect years from the metadata of all retrieved docs\n",
    "#   - Normalize both distributions to get probability distributions\n",
    "#   - KL(P||Q) = sum(P(x) * log(P(x) / Q(x))) for each year bin\n",
    "#   - Use 5-year bins for smoother distributions\n",
    "\n",
    "# YOUR CODE HERE"
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ethical Impact Assessment ---\n",
    "\n",
    "# TODO: Write a 500-word ethical impact assessment for BriefEngine's\n",
    "# AI legal research assistant. Address these four questions:\n",
    "#\n",
    "# (a) Who benefits from this system?\n",
    "# (b) Who could be harmed?\n",
    "# (c) What safeguards should be in place?\n",
    "# (d) Should this system be deployed without mandatory attorney review?\n",
    "#\n",
    "# Write your assessment as a multi-line string below.\n",
    "# This is a writing exercise, not a coding exercise.\n",
    "\n",
    "ethical_assessment = \"\"\"\n",
    "# Ethical Impact Assessment: BriefEngine AI Legal Research Assistant\n",
    "\n",
    "## (a) Who Benefits\n",
    "\n",
    "[YOUR ASSESSMENT HERE - discuss attorneys at mid-size firms, their clients,\n",
    "access to justice implications, efficiency gains]\n",
    "\n",
    "## (b) Who Could Be Harmed\n",
    "\n",
    "[YOUR ASSESSMENT HERE - discuss opposing parties if briefs contain errors,\n",
    "clients who over-rely on AI, junior attorneys whose skills may atrophy,\n",
    "communities affected by biased case law retrieval]\n",
    "\n",
    "## (c) Safeguards\n",
    "\n",
    "[YOUR ASSESSMENT HERE - discuss mandatory attorney review, citation verification,\n",
    "bias auditing, transparency requirements, insurance/liability]\n",
    "\n",
    "## (d) Deployment Without Attorney Review\n",
    "\n",
    "[YOUR ASSESSMENT HERE - take a clear position and justify it with specific\n",
    "risks and benefits]\n",
    "\"\"\"\n",
    "\n",
    "print(ethical_assessment)\n",
    "# Note: There is no \"right\" answer here. The goal is thoughtful engagement\n",
    "# with the ethical dimensions of deploying AI in high-stakes domains."
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. If BriefEngine's retrieval disproportionately surfaces cases from eras with discriminatory legal reasoning (e.g., pre-civil-rights decisions), how could this perpetuate injustice? How would you detect and mitigate this?\n",
    "2. Should an AI legal research tool tell the attorney that a particular argument it generated is \"strong\" or \"weak\"? What are the risks of the AI expressing confidence in legal conclusions?\n",
    "3. BriefEngine operates under attorney-client privilege. What technical measures must be in place to ensure that one firm's case data never leaks into another firm's context?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you built a complete context engineering pipeline for a legal research assistant. Here is what you implemented:\n",
    "\n",
    "1. **Data Acquisition**: Loaded and chunked a corpus of legal opinions with metadata preservation\n",
    "2. **Exploratory Analysis**: Analyzed the corpus structure, identifying key distributions and potential challenges\n",
    "3. **Baseline**: Implemented BM25 keyword search as a performance floor\n",
    "4. **Query Analysis**: Parsed attorney inputs into structured queries with jurisdiction filters (Select strategy)\n",
    "5. **Filtered Retrieval**: Applied hard metadata filters to prevent irrelevant documents from entering the pipeline (Select strategy)\n",
    "6. **Semantic Search + Reranking**: Used embeddings and cross-encoders for relevance ranking (Select strategy)\n",
    "7. **Context Assembly**: Built a ContextEngine that allocates token budgets and structures context with XML tags (all four strategies)\n",
    "8. **Citation Verification**: Created a persistent validity index to catch bad citations (Write strategy)\n",
    "9. **Fine-tuning**: Trained the embedding model on legal domain data using InfoNCE contrastive loss\n",
    "10. **Evaluation**: Compared the full pipeline against the baseline across multiple metrics\n",
    "11. **Error Analysis**: Categorized failures into the four context failure modes\n",
    "12. **Profiling**: Measured latency per pipeline stage and identified optimization opportunities\n",
    "13. **Ethics**: Analyzed temporal bias and wrote an ethical impact assessment\n",
    "\n",
    "**Key Takeaway**: The quality of an LLM's output is bounded by the quality of its context. By applying the four context engineering strategies (Write, Select, Compress, Isolate) systematically, you transformed a naive keyword-search pipeline into a precise, jurisdiction-aware, citation-verified legal research system.\n",
    "\n",
    "**Next Steps**: Read Section 4 (Production and System Design Extension) of the case study document for a detailed look at how this pipeline would operate at scale in BriefEngine's production environment, including API design, monitoring, A/B testing, and cost analysis."
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "chatbot"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸ’¬ AI Teaching Assistant â€” Click â–¶ to start\n",
    "#@markdown This AI chatbot reads your notebook and can answer questions about any concept, code, or exercise.\n",
    "\n",
    "import json as _json\n",
    "import requests as _requests\n",
    "from google.colab import output as _output\n",
    "from IPython.display import display, HTML as _HTML, Markdown as _Markdown\n",
    "\n",
    "# --- Read notebook content for context ---\n",
    "def _get_notebook_context():\n",
    "    try:\n",
    "        from google.colab import _message\n",
    "        nb = _message.blocking_request(\"get_ipynb\", request=\"\", timeout_sec=10)\n",
    "        cells = nb.get(\"ipynb\", {}).get(\"cells\", [])\n",
    "        parts = []\n",
    "        for cell in cells:\n",
    "            src = \"\".join(cell.get(\"source\", []))\n",
    "            tags = cell.get(\"metadata\", {}).get(\"tags\", [])\n",
    "            if \"chatbot\" in tags:\n",
    "                continue\n",
    "            if src.strip():\n",
    "                ct = cell.get(\"cell_type\", \"unknown\")\n",
    "                parts.append(f\"[{ct.upper()}]\\n{src}\")\n",
    "        return \"\\n\\n---\\n\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"Notebook content unavailable.\"\n",
    "\n",
    "_NOTEBOOK_CONTEXT = _get_notebook_context()\n",
    "_CHAT_HISTORY = []\n",
    "_API_URL = \"https://course-creator-brown.vercel.app/api/chat\"\n",
    "\n",
    "def _notebook_chat(question):\n",
    "    global _CHAT_HISTORY\n",
    "    try:\n",
    "        resp = _requests.post(_API_URL, json={\n",
    "            'question': question,\n",
    "            'context': _NOTEBOOK_CONTEXT[:100000],\n",
    "            'history': _CHAT_HISTORY[-10:],\n",
    "        }, timeout=60)\n",
    "        data = resp.json()\n",
    "        answer = data.get('answer', 'Sorry, I could not generate a response.')\n",
    "        _CHAT_HISTORY.append({'role': 'user', 'content': question})\n",
    "        _CHAT_HISTORY.append({'role': 'assistant', 'content': answer})\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f'Error connecting to teaching assistant: {str(e)}'\n",
    "\n",
    "_output.register_callback('notebook_chat', _notebook_chat)\n",
    "\n",
    "def ask(question):\n",
    "    \"\"\"Ask the AI teaching assistant a question about this notebook.\"\"\"\n",
    "    answer = _notebook_chat(question)\n",
    "    display(_Markdown(answer))\n",
    "\n",
    "print(\"\\u2705 AI Teaching Assistant is ready!\")\n",
    "print(\"\\U0001f4a1 Use the chat below, or call ask(\\'your question\\') in any cell.\")\n",
    "\n",
    "# --- Display chat widget ---\n",
    "display(_HTML('''<style>\n  .vc-wrap{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;max-width:100%;border-radius:16px;overflow:hidden;box-shadow:0 4px 24px rgba(0,0,0,.12);background:#fff;border:1px solid #e5e7eb}\n  .vc-hdr{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;padding:16px 20px;display:flex;align-items:center;gap:12px}\n  .vc-avatar{width:42px;height:42px;background:rgba(255,255,255,.2);border-radius:50%;display:flex;align-items:center;justify-content:center;font-size:22px}\n  .vc-hdr h3{font-size:16px;font-weight:600;margin:0}\n  .vc-hdr p{font-size:12px;opacity:.85;margin:2px 0 0}\n  .vc-msgs{height:420px;overflow-y:auto;padding:16px;background:#f8f9fb;display:flex;flex-direction:column;gap:10px}\n  .vc-msg{display:flex;flex-direction:column;animation:vc-fade .25s ease}\n  .vc-msg.user{align-items:flex-end}\n  .vc-msg.bot{align-items:flex-start}\n  .vc-bbl{max-width:85%;padding:10px 14px;border-radius:16px;font-size:14px;line-height:1.55;word-wrap:break-word}\n  .vc-msg.user .vc-bbl{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border-bottom-right-radius:4px}\n  .vc-msg.bot .vc-bbl{background:#fff;color:#1a1a2e;border:1px solid #e8e8e8;border-bottom-left-radius:4px}\n  .vc-bbl code{background:rgba(0,0,0,.07);padding:2px 6px;border-radius:4px;font-size:13px;font-family:'Fira Code',monospace}\n  .vc-bbl pre{background:#1e1e2e;color:#cdd6f4;padding:12px;border-radius:8px;overflow-x:auto;margin:8px 0;font-size:13px}\n  .vc-bbl pre code{background:none;padding:0;color:inherit}\n  .vc-bbl h3,.vc-bbl h4{margin:10px 0 4px;font-size:15px}\n  .vc-bbl ul,.vc-bbl ol{margin:4px 0;padding-left:20px}\n  .vc-bbl li{margin:2px 0}\n  .vc-chips{display:flex;flex-wrap:wrap;gap:8px;padding:0 16px 12px;background:#f8f9fb}\n  .vc-chip{background:#fff;border:1px solid #d1d5db;border-radius:20px;padding:6px 14px;font-size:12px;cursor:pointer;transition:all .15s;color:#4b5563}\n  .vc-chip:hover{border-color:#667eea;color:#667eea;background:#f0f0ff}\n  .vc-input{display:flex;padding:12px 16px;background:#fff;border-top:1px solid #eee;gap:8px}\n  .vc-input input{flex:1;padding:10px 16px;border:2px solid #e8e8e8;border-radius:24px;font-size:14px;outline:none;transition:border-color .2s}\n  .vc-input input:focus{border-color:#667eea}\n  .vc-input button{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border:none;border-radius:50%;width:42px;height:42px;cursor:pointer;display:flex;align-items:center;justify-content:center;font-size:18px;transition:transform .1s}\n  .vc-input button:hover{transform:scale(1.05)}\n  .vc-input button:disabled{opacity:.5;cursor:not-allowed;transform:none}\n  .vc-typing{display:flex;gap:5px;padding:4px 0}\n  .vc-typing span{width:8px;height:8px;background:#667eea;border-radius:50%;animation:vc-bounce 1.4s infinite ease-in-out}\n  .vc-typing span:nth-child(2){animation-delay:.2s}\n  .vc-typing span:nth-child(3){animation-delay:.4s}\n  @keyframes vc-bounce{0%,80%,100%{transform:scale(0)}40%{transform:scale(1)}}\n  @keyframes vc-fade{from{opacity:0;transform:translateY(8px)}to{opacity:1;transform:translateY(0)}}\n  .vc-note{text-align:center;font-size:11px;color:#9ca3af;padding:8px 16px 12px;background:#fff}\n</style>\n<div class=\"vc-wrap\">\n  <div class=\"vc-hdr\">\n    <div class=\"vc-avatar\">&#129302;</div>\n    <div>\n      <h3>Vizuara Teaching Assistant</h3>\n      <p>Ask me anything about this notebook</p>\n    </div>\n  </div>\n  <div class=\"vc-msgs\" id=\"vcMsgs\">\n    <div class=\"vc-msg bot\">\n      <div class=\"vc-bbl\">&#128075; Hi! I've read through this entire notebook. Ask me about any concept, code block, or exercise &mdash; I'm here to help you learn!</div>\n    </div>\n  </div>\n  <div class=\"vc-chips\" id=\"vcChips\">\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Explain the main concept</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Help with the TODO exercise</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Summarize what I learned</span>\n  </div>\n  <div class=\"vc-input\">\n    <input type=\"text\" id=\"vcIn\" placeholder=\"Ask about concepts, code, exercises...\" />\n    <button id=\"vcSend\" onclick=\"vcSendMsg()\">&#10148;</button>\n  </div>\n  <div class=\"vc-note\">AI-generated &middot; Verify important information &middot; <a href=\"#\" onclick=\"vcClear();return false\" style=\"color:#667eea\">Clear chat</a></div>\n</div>\n<script>\n(function(){\n  var msgs=document.getElementById('vcMsgs'),inp=document.getElementById('vcIn'),\n      btn=document.getElementById('vcSend'),chips=document.getElementById('vcChips');\n\n  function esc(s){var d=document.createElement('div');d.textContent=s;return d.innerHTML}\n\n  function md(t){\n    return t\n      .replace(/```(\\w*)\\n([\\s\\S]*?)```/g,function(_,l,c){return '<pre><code>'+esc(c)+'</code></pre>'})\n      .replace(/`([^`]+)`/g,'<code>$1</code>')\n      .replace(/\\*\\*([^*]+)\\*\\*/g,'<strong>$1</strong>')\n      .replace(/\\*([^*]+)\\*/g,'<em>$1</em>')\n      .replace(/^#### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^## (.+)$/gm,'<h3>$1</h3>')\n      .replace(/^\\d+\\. (.+)$/gm,'<li>$1</li>')\n      .replace(/^- (.+)$/gm,'<li>$1</li>')\n      .replace(/\\n\\n/g,'<br><br>')\n      .replace(/\\n/g,'<br>');\n  }\n\n  function addMsg(text,isUser){\n    var m=document.createElement('div');m.className='vc-msg '+(isUser?'user':'bot');\n    var b=document.createElement('div');b.className='vc-bbl';\n    b.innerHTML=isUser?esc(text):md(text);\n    m.appendChild(b);msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function showTyping(){\n    var m=document.createElement('div');m.className='vc-msg bot';m.id='vcTyping';\n    m.innerHTML='<div class=\"vc-bbl\"><div class=\"vc-typing\"><span></span><span></span><span></span></div></div>';\n    msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function hideTyping(){var e=document.getElementById('vcTyping');if(e)e.remove()}\n\n  window.vcSendMsg=function(){\n    var q=inp.value.trim();if(!q)return;\n    inp.value='';chips.style.display='none';\n    addMsg(q,true);showTyping();btn.disabled=true;\n    google.colab.kernel.invokeFunction('notebook_chat',[q],{})\n      .then(function(r){\n        hideTyping();\n        var a=r.data['application/json'];\n        addMsg(typeof a==='string'?a:JSON.stringify(a),false);\n      })\n      .catch(function(){\n        hideTyping();\n        addMsg('Sorry, I encountered an error. Please check your internet connection and try again.',false);\n      })\n      .finally(function(){btn.disabled=false;inp.focus()});\n  };\n\n  window.vcAsk=function(q){inp.value=q;vcSendMsg()};\n  window.vcClear=function(){\n    msgs.innerHTML='<div class=\"vc-msg bot\"><div class=\"vc-bbl\">&#128075; Chat cleared. Ask me anything!</div></div>';\n    chips.style.display='flex';\n  };\n\n  inp.addEventListener('keypress',function(e){if(e.key==='Enter')vcSendMsg()});\n  inp.focus();\n})();\n</script>'''))"
   ],
   "id": "vizuara_chatbot"
  }
 ]
}