{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "VLA for Autonomous Port Terminal Tractors â€” Implementation Notebook"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: VLA for Autonomous Port Terminal Tractors\n",
    "## Implementation Notebook\n",
    "\n",
    "Welcome to the implementation notebook for the VLA (Vision-Language-Action) case study. You will build a simplified VLA model that takes a port scene image and a dispatcher instruction, and generates a trajectory for an autonomous terminal tractor navigating a container shipping yard.\n",
    "\n",
    "**What you will build:**\n",
    "1. A synthetic port environment with container blocks, workers, and dispatcher instructions\n",
    "2. A vision encoder (CNN) that processes port scene images\n",
    "3. A text encoder (Transformer) that processes dispatcher instructions\n",
    "4. A diffusion-based trajectory decoder that generates smooth, collision-aware paths\n",
    "5. A complete training and evaluation pipeline\n",
    "\n",
    "**Prerequisites:** PyTorch, NumPy, Matplotlib. All run on a free Colab T4 GPU.\n",
    "\n",
    "**Estimated time:** 60-90 minutes.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Data Generation\n",
    "\n",
    "### 1.1 Imports and Configuration"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://course-creator-brown.vercel.app/courses/vla-autonomous-driving/practice/0/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Tuple, List, Dict\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Port Environment\n",
    "\n",
    "We model a container port yard as a 128x128 grid. Container blocks are arranged in rows with narrow corridors between them. Workers and temporary obstacles appear randomly. Each grid cell represents approximately 1.5 meters."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Port Environment Configuration ---\n",
    "PORT_SIZE = 128          # Grid size (128x128 = ~200m x 200m at 1.5m/cell)\n",
    "NUM_BLOCKS = 8           # Container blocks (A-H)\n",
    "ROWS_PER_BLOCK = 12      # Rows per block\n",
    "CORRIDOR_WIDTH = 2       # Grid cells between rows (~3 meters)\n",
    "BLOCK_GAP = 6            # Grid cells between blocks (~9 meters)\n",
    "NUM_WAYPOINTS = 32       # Trajectory length (3.2 seconds at 10Hz)\n",
    "MAX_WORKERS = 4          # Max dynamic obstacles per scene\n",
    "NUM_INSTRUCTION_TYPES = 4  # Types of dispatcher instructions\n",
    "\n",
    "\n",
    "class PortEnvironment:\n",
    "    \"\"\"Generates synthetic port yard layouts with container blocks, corridors, and obstacles.\"\"\"\n",
    "\n",
    "    BLOCK_NAMES = [chr(65 + i) for i in range(NUM_BLOCKS)]  # A, B, C, ...\n",
    "\n",
    "    def __init__(self, size: int = PORT_SIZE):\n",
    "        self.size = size\n",
    "        self.grid = np.zeros((size, size), dtype=np.float32)\n",
    "        self.block_positions = {}  # block_name -> (row_start, col_start, row_end, col_end)\n",
    "        self._build_layout()\n",
    "\n",
    "    def _build_layout(self):\n",
    "        \"\"\"Place container blocks in a grid pattern.\"\"\"\n",
    "        block_width = 8   # cells wide per block\n",
    "        block_height = ROWS_PER_BLOCK * (1 + CORRIDOR_WIDTH)\n",
    "\n",
    "        start_x = 10\n",
    "        start_y = 10\n",
    "        col_idx = 0\n",
    "        row_idx = 0\n",
    "\n",
    "        for i, name in enumerate(self.BLOCK_NAMES):\n",
    "            bx = start_x + col_idx * (block_width + BLOCK_GAP)\n",
    "            by = start_y + row_idx * (block_height + BLOCK_GAP)\n",
    "\n",
    "            if by + block_height >= self.size - 10:\n",
    "                col_idx += 1\n",
    "                row_idx = 0\n",
    "                bx = start_x + col_idx * (block_width + BLOCK_GAP)\n",
    "                by = start_y\n",
    "\n",
    "            self.block_positions[name] = (by, bx, by + block_height, bx + block_width)\n",
    "\n",
    "            # Fill container rows (leave corridors)\n",
    "            for r in range(ROWS_PER_BLOCK):\n",
    "                ry = by + r * (1 + CORRIDOR_WIDTH)\n",
    "                self.grid[ry:ry+1, bx:bx+block_width] = 1.0  # container row\n",
    "\n",
    "            row_idx += 1\n",
    "\n",
    "    def get_destination(self, block_name: str, row: int) -> Tuple[int, int]:\n",
    "        \"\"\"Get the grid coordinates for a block/row destination.\"\"\"\n",
    "        if block_name not in self.block_positions:\n",
    "            block_name = random.choice(list(self.block_positions.keys()))\n",
    "        by, bx, _, bx_end = self.block_positions[block_name]\n",
    "        row = min(row, ROWS_PER_BLOCK - 1)\n",
    "        dest_y = by + row * (1 + CORRIDOR_WIDTH) + 1  # in the corridor\n",
    "        dest_x = bx + (bx_end - bx) // 2  # center of block\n",
    "        return (dest_y, dest_x)\n",
    "\n",
    "    def add_random_workers(self, n_workers: int) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Add random worker positions in corridors.\"\"\"\n",
    "        workers = []\n",
    "        attempts = 0\n",
    "        while len(workers) < n_workers and attempts < 100:\n",
    "            y = random.randint(5, self.size - 5)\n",
    "            x = random.randint(5, self.size - 5)\n",
    "            if self.grid[y, x] == 0:  # only in free space\n",
    "                workers.append((y, x))\n",
    "            attempts += 1\n",
    "        return workers\n",
    "\n",
    "    def render(self, tractor_pos=None, trajectory=None, workers=None, destination=None):\n",
    "        \"\"\"Render the port as a 3-channel image (128x128x3).\"\"\"\n",
    "        img = np.zeros((self.size, self.size, 3), dtype=np.float32)\n",
    "        # Channel 0: container blocks (static obstacles)\n",
    "        img[:, :, 0] = self.grid\n",
    "        # Channel 1: dynamic obstacles (workers)\n",
    "        if workers:\n",
    "            for wy, wx in workers:\n",
    "                y_lo, y_hi = max(0, wy-1), min(self.size, wy+2)\n",
    "                x_lo, x_hi = max(0, wx-1), min(self.size, wx+2)\n",
    "                img[y_lo:y_hi, x_lo:x_hi, 1] = 1.0\n",
    "        # Channel 2: destination marker\n",
    "        if destination:\n",
    "            dy, dx = destination\n",
    "            y_lo, y_hi = max(0, dy-2), min(self.size, dy+3)\n",
    "            x_lo, x_hi = max(0, dx-2), min(self.size, dx+3)\n",
    "            img[y_lo:y_hi, x_lo:x_hi, 2] = 1.0\n",
    "        return img"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Expert Trajectory Planner\n",
    "\n",
    "The expert planner generates ground-truth trajectories using linear interpolation with noise and smoothing. In the real system, these would come from recorded human driver data."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_trajectory(grid, start, end, num_waypoints=NUM_WAYPOINTS):\n",
    "    \"\"\"\n",
    "    Plan a trajectory from start to end, avoiding obstacles.\n",
    "    Uses a simplified approach: straight-line with corridor-aware waypoint adjustment.\n",
    "    Returns normalized waypoints in [-1, 1] relative to start position.\n",
    "    \"\"\"\n",
    "    sy, sx = start\n",
    "    ey, ex = end\n",
    "\n",
    "    # Generate raw waypoints via linear interpolation\n",
    "    raw_y = np.linspace(sy, ey, num_waypoints + 10)\n",
    "    raw_x = np.linspace(sx, ex, num_waypoints + 10)\n",
    "\n",
    "    # Add slight noise and smooth for realistic trajectories\n",
    "    noise_scale = 0.3\n",
    "    raw_y += np.random.randn(len(raw_y)) * noise_scale\n",
    "    raw_x += np.random.randn(len(raw_x)) * noise_scale\n",
    "\n",
    "    # Simple smoothing (moving average)\n",
    "    kernel = np.ones(5) / 5\n",
    "    raw_y = np.convolve(raw_y, kernel, mode='same')\n",
    "    raw_x = np.convolve(raw_x, kernel, mode='same')\n",
    "\n",
    "    # Subsample to desired number of waypoints\n",
    "    indices = np.linspace(0, len(raw_y) - 1, num_waypoints).astype(int)\n",
    "    traj_y = raw_y[indices]\n",
    "    traj_x = raw_x[indices]\n",
    "\n",
    "    # Normalize: convert to offsets from start, scale to [-1, 1] range\n",
    "    traj_y = (traj_y - sy) / (PORT_SIZE / 2)\n",
    "    traj_x = (traj_x - sx) / (PORT_SIZE / 2)\n",
    "\n",
    "    # Compute heading at each waypoint\n",
    "    dy = np.gradient(traj_y)\n",
    "    dx = np.gradient(traj_x)\n",
    "    theta = np.arctan2(dy, dx)\n",
    "\n",
    "    trajectory = np.stack([traj_x, traj_y, theta], axis=-1).astype(np.float32)\n",
    "    return trajectory  # shape: (NUM_WAYPOINTS, 3)"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Dispatcher Instructions"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION_TEMPLATES = [\n",
    "    \"Navigate to Block {block}, Row {row}.\",\n",
    "    \"Pick up container at Block {block}, Row {row}. Proceed via main corridor.\",\n",
    "    \"Deliver to Block {block}, Row {row}. Avoid Row {avoid_row} in Block {avoid_block} â€” maintenance in progress.\",\n",
    "    \"Priority load. Go to Block {block}, Row {row} immediately.\",\n",
    "]\n",
    "\n",
    "def generate_instruction(dest_block: str, dest_row: int, env: PortEnvironment) -> str:\n",
    "    \"\"\"Generate a natural language dispatcher instruction.\"\"\"\n",
    "    template_idx = random.randint(0, len(INSTRUCTION_TEMPLATES) - 1)\n",
    "    template = INSTRUCTION_TEMPLATES[template_idx]\n",
    "\n",
    "    avoid_block = random.choice(env.BLOCK_NAMES)\n",
    "    avoid_row = random.randint(1, ROWS_PER_BLOCK)\n",
    "\n",
    "    instruction = template.format(\n",
    "        block=dest_block,\n",
    "        row=dest_row,\n",
    "        avoid_block=avoid_block,\n",
    "        avoid_row=avoid_row,\n",
    "    )\n",
    "    return instruction"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Dataset"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortVLADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Synthetic dataset for port VLA training.\n",
    "    Each sample: (image, instruction_tokens, egomotion, trajectory)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_samples: int = 5000, max_instruction_len: int = 32):\n",
    "        self.num_samples = num_samples\n",
    "        self.max_instruction_len = max_instruction_len\n",
    "        self.env = PortEnvironment()\n",
    "        self.data = []\n",
    "\n",
    "        # Build a simple word-to-index vocabulary from templates\n",
    "        self.vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "        all_words = set()\n",
    "        for t in INSTRUCTION_TEMPLATES:\n",
    "            for w in t.replace(\".\", \"\").replace(\",\", \"\").replace(\"{\", \"\").replace(\"}\", \"\").split():\n",
    "                all_words.add(w.lower())\n",
    "        for name in self.env.BLOCK_NAMES:\n",
    "            all_words.add(name.lower())\n",
    "        for i in range(1, ROWS_PER_BLOCK + 1):\n",
    "            all_words.add(str(i))\n",
    "        for i, w in enumerate(sorted(all_words), start=2):\n",
    "            self.vocab[w] = i\n",
    "        # Add special tokens for block names and numbers\n",
    "        for c in \"abcdefghijklmnopqrstuvwxyz\":\n",
    "            if c not in self.vocab:\n",
    "                self.vocab[c] = len(self.vocab)\n",
    "\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self._generate_all()\n",
    "\n",
    "    def tokenize(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"Simple whitespace tokenizer.\"\"\"\n",
    "        words = text.replace(\".\", \"\").replace(\",\", \"\").replace(\"â€”\", \" \").lower().split()\n",
    "        tokens = [self.vocab.get(w, 1) for w in words]  # 1 = <unk>\n",
    "        # Pad or truncate\n",
    "        if len(tokens) < self.max_instruction_len:\n",
    "            tokens += [0] * (self.max_instruction_len - len(tokens))\n",
    "        else:\n",
    "            tokens = tokens[:self.max_instruction_len]\n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "    def _generate_all(self):\n",
    "        \"\"\"Pre-generate all samples.\"\"\"\n",
    "        for _ in range(self.num_samples):\n",
    "            # Random start position (in a corridor)\n",
    "            start_y = random.randint(15, PORT_SIZE - 15)\n",
    "            start_x = random.randint(15, PORT_SIZE - 15)\n",
    "            while self.env.grid[start_y, start_x] > 0:\n",
    "                start_y = random.randint(15, PORT_SIZE - 15)\n",
    "                start_x = random.randint(15, PORT_SIZE - 15)\n",
    "\n",
    "            # Random destination\n",
    "            dest_block = random.choice(self.env.BLOCK_NAMES)\n",
    "            dest_row = random.randint(1, ROWS_PER_BLOCK)\n",
    "            dest_pos = self.env.get_destination(dest_block, dest_row - 1)\n",
    "\n",
    "            # Generate components\n",
    "            workers = self.env.add_random_workers(random.randint(0, MAX_WORKERS))\n",
    "            image = self.env.render(\n",
    "                tractor_pos=(start_y, start_x),\n",
    "                workers=workers,\n",
    "                destination=dest_pos\n",
    "            )\n",
    "            instruction = generate_instruction(dest_block, dest_row, self.env)\n",
    "            tokens = self.tokenize(instruction)\n",
    "            trajectory = plan_trajectory(self.env.grid, (start_y, start_x), dest_pos)\n",
    "\n",
    "            # Egomotion: simple straight-line history (16 waypoints, 3D)\n",
    "            ego_history = np.zeros((16, 3), dtype=np.float32)\n",
    "            ego_history[:, 0] = np.linspace(-0.1, 0, 16)  # slight forward motion\n",
    "\n",
    "            self.data.append({\n",
    "                \"image\": torch.tensor(image).permute(2, 0, 1),  # (3, 128, 128)\n",
    "                \"instruction\": tokens,\n",
    "                \"egomotion\": torch.tensor(ego_history),\n",
    "                \"trajectory\": torch.tensor(trajectory),\n",
    "                \"start\": (start_y, start_x),\n",
    "                \"dest\": dest_pos,\n",
    "                \"workers\": workers,\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = self.data[idx]\n",
    "        return d[\"image\"], d[\"instruction\"], d[\"egomotion\"], d[\"trajectory\"]\n",
    "\n",
    "\n",
    "# Generate datasets\n",
    "print(\"Generating training dataset (5000 samples)...\")\n",
    "train_dataset = PortVLADataset(num_samples=5000)\n",
    "print(\"Generating validation dataset (500 samples)...\")\n",
    "val_dataset = PortVLADataset(num_samples=500)\n",
    "\n",
    "print(f\"\\nVocabulary size: {train_dataset.vocab_size}\")\n",
    "print(f\"Sample image shape: {train_dataset[0][0].shape}\")\n",
    "print(f\"Sample instruction shape: {train_dataset[0][1].shape}\")\n",
    "print(f\"Sample trajectory shape: {train_dataset[0][3].shape}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 TODO 1: Data Augmentation\n",
    "\n",
    "Implement two data augmentation strategies to improve model robustness."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_trajectory_noise(trajectory: torch.Tensor, noise_std: float = 0.01) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Add small Gaussian noise to expert trajectories to improve robustness.\n",
    "\n",
    "    This simulates the natural variation in human driving â€” two expert drivers\n",
    "    navigating the same route will produce slightly different trajectories.\n",
    "    Augmenting with noise prevents the model from overfitting to the exact\n",
    "    waypoint positions in the training data.\n",
    "\n",
    "    Args:\n",
    "        trajectory: Expert trajectory, shape (32, 3) â€” [x, y, theta] per waypoint\n",
    "        noise_std: Standard deviation of Gaussian noise to add\n",
    "\n",
    "    Returns:\n",
    "        Augmented trajectory with same shape\n",
    "\n",
    "    Hints:\n",
    "        Step 1: Generate Gaussian noise with torch.randn_like, scaled by noise_std\n",
    "        Step 2: Add noise only to the x and y channels (indices 0 and 1), NOT theta\n",
    "                (heading noise should be recomputed from the noisy x,y positions)\n",
    "        Step 3: Recompute theta from the noisy x,y using torch.atan2 on the\n",
    "                finite differences (gradient) of y and x\n",
    "        Step 4: Clamp x and y to [-1, 1] to keep waypoints in valid range\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE (approximately 6-8 lines)\n",
    "    pass\n",
    "\n",
    "\n",
    "def augment_mirror(image: torch.Tensor, instruction: torch.Tensor,\n",
    "                   trajectory: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Mirror the scene horizontally â€” flip the image, negate the x-coordinates\n",
    "    of the trajectory, and negate the heading angles.\n",
    "\n",
    "    This effectively doubles the dataset by creating left-right mirror images.\n",
    "    In a symmetric port layout, a trajectory going left is equally valid as\n",
    "    one going right.\n",
    "\n",
    "    Args:\n",
    "        image: Port scene image, shape (3, 128, 128)\n",
    "        instruction: Tokenized instruction, shape (32,) â€” left unchanged\n",
    "        trajectory: Expert trajectory, shape (32, 3)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (flipped_image, instruction, flipped_trajectory)\n",
    "\n",
    "    Hints:\n",
    "        Step 1: Flip the image horizontally using torch.flip on the last dimension\n",
    "        Step 2: Negate the x-coordinates (index 0) of the trajectory\n",
    "        Step 3: Negate the theta values (index 2) of the trajectory\n",
    "        Step 4: Return the instruction unchanged (text is not mirrored)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE (approximately 4-5 lines)\n",
    "    pass"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verification for TODO 1 ---\n",
    "test_traj = torch.randn(32, 3)\n",
    "test_traj[:, :2] = test_traj[:, :2].clamp(-0.9, 0.9)\n",
    "\n",
    "if augment_trajectory_noise is not None and augment_trajectory_noise(test_traj) is not None:\n",
    "    aug_traj = augment_trajectory_noise(test_traj, noise_std=0.01)\n",
    "    assert aug_traj.shape == (32, 3), f\"Expected shape (32, 3), got {aug_traj.shape}\"\n",
    "    assert not torch.allclose(aug_traj[:, :2], test_traj[:, :2]), \"Noise was not added to x,y\"\n",
    "    assert (aug_traj[:, :2].abs() <= 1.0).all(), \"x,y values out of [-1, 1] range\"\n",
    "    print(\"[PASS] augment_trajectory_noise: shape, noise, and clamping correct\")\n",
    "else:\n",
    "    print(\"[SKIP] augment_trajectory_noise not yet implemented\")\n",
    "\n",
    "# Test augment_mirror\n",
    "test_img = torch.randn(3, 128, 128)\n",
    "test_inst = torch.randint(0, 50, (32,))\n",
    "test_traj2 = torch.randn(32, 3)\n",
    "\n",
    "if augment_mirror is not None and augment_mirror(test_img, test_inst, test_traj2) is not None:\n",
    "    f_img, f_inst, f_traj = augment_mirror(test_img, test_inst, test_traj2)\n",
    "    assert f_img.shape == test_img.shape, \"Image shape changed\"\n",
    "    assert torch.allclose(f_traj[:, 0], -test_traj2[:, 0]), \"x not negated\"\n",
    "    assert torch.allclose(f_traj[:, 2], -test_traj2[:, 2]), \"theta not negated\"\n",
    "    assert torch.allclose(f_inst, test_inst), \"Instruction should be unchanged\"\n",
    "    print(\"[PASS] augment_mirror: flip and negation correct\")\n",
    "else:\n",
    "    print(\"[SKIP] augment_mirror not yet implemented\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "### 2.1 Port Layout and Sample Visualization"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = train_dataset.env\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# 1. Port layout\n",
    "axes[0].imshow(env.grid, cmap='Oranges', origin='lower')\n",
    "for name, (by, bx, ey, ex) in env.block_positions.items():\n",
    "    axes[0].text(bx + (ex - bx) / 2, by - 2, f\"Block {name}\",\n",
    "                 ha='center', fontsize=8, fontweight='bold')\n",
    "axes[0].set_title(\"Port Yard Layout (Container Blocks)\")\n",
    "axes[0].set_xlabel(\"X (grid cells)\")\n",
    "axes[0].set_ylabel(\"Y (grid cells)\")\n",
    "\n",
    "# 2. Sample scene with trajectory\n",
    "sample = train_dataset.data[0]\n",
    "img = sample[\"image\"].permute(1, 2, 0).numpy()\n",
    "axes[1].imshow(img, origin='lower')\n",
    "traj = sample[\"trajectory\"].numpy()\n",
    "sy, sx = sample[\"start\"]\n",
    "plot_x = traj[:, 0] * (PORT_SIZE / 2) + sx\n",
    "plot_y = traj[:, 1] * (PORT_SIZE / 2) + sy\n",
    "axes[1].plot(plot_x, plot_y, 'g-', linewidth=2, label='Expert trajectory')\n",
    "axes[1].plot(plot_x[0], plot_y[0], 'go', markersize=8, label='Start')\n",
    "axes[1].plot(plot_x[-1], plot_y[-1], 'r*', markersize=12, label='End')\n",
    "axes[1].legend(fontsize=8)\n",
    "axes[1].set_title(\"Sample Scene + Trajectory\")\n",
    "\n",
    "# 3. Trajectory length distribution\n",
    "lengths = []\n",
    "for d in train_dataset.data[:500]:\n",
    "    t = d[\"trajectory\"].numpy()\n",
    "    dx = np.diff(t[:, 0])\n",
    "    dy = np.diff(t[:, 1])\n",
    "    length = np.sum(np.sqrt(dx**2 + dy**2))\n",
    "    lengths.append(length)\n",
    "axes[2].hist(lengths, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[2].set_xlabel(\"Trajectory Length (normalized units)\")\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "axes[2].set_title(\"Distribution of Trajectory Lengths\")\n",
    "axes[2].axvline(np.mean(lengths), color='red', linestyle='--', label=f'Mean: {np.mean(lengths):.2f}')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dataset Statistics"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total training samples: {len(train_dataset)}\")\n",
    "print(f\"Total validation samples: {len(val_dataset)}\")\n",
    "print(f\"Trajectory shape: {train_dataset[0][3].shape}\")\n",
    "print(f\"Trajectory value range: [{train_dataset[0][3].min():.3f}, {train_dataset[0][3].max():.3f}]\")\n",
    "print(f\"Mean trajectory length: {np.mean(lengths):.3f}\")\n",
    "print(f\"Std trajectory length: {np.std(lengths):.3f}\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 TODO 2: EDA Deep Dive"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: Exploratory Data Analysis\n",
    "#\n",
    "# Analyze and plot the following (write code for each):\n",
    "#\n",
    "# 1. Plot a 2x3 grid showing 6 random training samples â€” each subplot should show\n",
    "#    the port image with the expert trajectory overlaid. Title each with the\n",
    "#    destination block/row.\n",
    "#\n",
    "# 2. Compute and plot the distribution of waypoint HEADINGS (theta values) across\n",
    "#    the dataset. This tells you whether trajectories are predominantly straight,\n",
    "#    turning left, or turning right. A balanced distribution is important for\n",
    "#    training an unbiased model.\n",
    "#\n",
    "# 3. Plot a heatmap of trajectory ENDPOINTS across the port grid. This shows which\n",
    "#    destinations are sampled most frequently. Ideally, all blocks should have\n",
    "#    roughly equal representation.\n",
    "\n",
    "# YOUR CODE HERE"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought questions** (answer in the space below):\n",
    "- Q1: Are any trajectory heading directions underrepresented? How might this bias the model's turning behavior?\n",
    "- Q2: If certain port blocks have more endpoint samples, would the model navigate to those blocks more accurately? What data balancing strategy would you use?\n",
    "- Q3: How does the trajectory length distribution relate to the distance between blocks? Are short trajectories (nearby destinations) overrepresented?\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Baseline Approaches\n",
    "\n",
    "### 3.1 Straight-Line Baseline\n",
    "\n",
    "The simplest baseline: draw a straight line from current position to destination, ignoring all obstacles."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StraightLineBaseline:\n",
    "    \"\"\"Predict a straight-line trajectory from current position to destination.\"\"\"\n",
    "\n",
    "    def predict(self, image: torch.Tensor, instruction: torch.Tensor,\n",
    "                egomotion: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract destination from the image (channel 2 = destination marker)\n",
    "        and plan a straight line to it.\n",
    "        \"\"\"\n",
    "        # Find destination from image channel 2\n",
    "        dest_channel = image[2]  # (128, 128)\n",
    "        dest_mask = (dest_channel > 0.5)\n",
    "        if dest_mask.sum() == 0:\n",
    "            trajectory = torch.zeros(NUM_WAYPOINTS, 3)\n",
    "            trajectory[:, 1] = torch.linspace(0, 0.3, NUM_WAYPOINTS)\n",
    "            return trajectory\n",
    "\n",
    "        ys, xs = torch.where(dest_mask)\n",
    "        dest_y = ys.float().mean().item()\n",
    "        dest_x = xs.float().mean().item()\n",
    "\n",
    "        cy, cx = PORT_SIZE / 2, PORT_SIZE / 2\n",
    "        traj_x = torch.linspace(0, (dest_x - cx) / (PORT_SIZE / 2), NUM_WAYPOINTS)\n",
    "        traj_y = torch.linspace(0, (dest_y - cy) / (PORT_SIZE / 2), NUM_WAYPOINTS)\n",
    "        theta = torch.atan2(traj_y[-1] - traj_y[0], traj_x[-1] - traj_x[0]).expand(NUM_WAYPOINTS)\n",
    "\n",
    "        trajectory = torch.stack([traj_x, traj_y, theta], dim=-1)\n",
    "        return trajectory"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Evaluation Metrics"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred_traj: torch.Tensor, gt_traj: torch.Tensor) -> Dict[str, float]:\n",
    "    \"\"\"Compute ADE, FDE, and smoothness metrics.\"\"\"\n",
    "    # Average Displacement Error\n",
    "    displacement = torch.sqrt(((pred_traj[:, :2] - gt_traj[:, :2]) ** 2).sum(dim=-1))\n",
    "    ade = displacement.mean().item()\n",
    "\n",
    "    # Final Displacement Error\n",
    "    fde = displacement[-1].item()\n",
    "\n",
    "    # Smoothness (jerk = third derivative of position)\n",
    "    dt = 0.1  # 10Hz\n",
    "    velocity = torch.diff(pred_traj[:, :2], dim=0) / dt\n",
    "    acceleration = torch.diff(velocity, dim=0) / dt\n",
    "    jerk = torch.diff(acceleration, dim=0) / dt\n",
    "    smoothness = torch.sqrt((jerk ** 2).sum(dim=-1)).mean().item()\n",
    "\n",
    "    return {\"ADE\": ade, \"FDE\": fde, \"Jerk\": smoothness}"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate straight-line baseline\n",
    "baseline = StraightLineBaseline()\n",
    "baseline_metrics = {\"ADE\": [], \"FDE\": [], \"Jerk\": []}\n",
    "\n",
    "for i in range(min(200, len(val_dataset))):\n",
    "    image, instruction, egomotion, gt_traj = val_dataset[i]\n",
    "    pred = baseline.predict(image, instruction, egomotion)\n",
    "    metrics = compute_metrics(pred, gt_traj)\n",
    "    for k, v in metrics.items():\n",
    "        baseline_metrics[k].append(v)\n",
    "\n",
    "print(\"=== Straight-Line Baseline Results ===\")\n",
    "for k in baseline_metrics:\n",
    "    vals = baseline_metrics[k]\n",
    "    print(f\"  {k}: {np.mean(vals):.4f} +/- {np.std(vals):.4f}\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 TODO 3: Obstacle-Aware Baseline"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObstacleAwareBaseline:\n",
    "    \"\"\"\n",
    "    A baseline that plans a trajectory toward the destination while avoiding\n",
    "    obstacles visible in the image.\n",
    "\n",
    "    This represents what a traditional (non-ML) planner would do: extract\n",
    "    obstacle positions from the sensor data and plan around them using\n",
    "    geometric rules. It still cannot understand language instructions, but\n",
    "    it should produce collision-free trajectories.\n",
    "\n",
    "    Args:\n",
    "        safety_margin: Minimum distance (in grid cells) to maintain from obstacles\n",
    "\n",
    "    Hints:\n",
    "        Step 1: Extract the destination from image channel 2 (same as StraightLineBaseline)\n",
    "        Step 2: Extract obstacle positions from image channel 0 (containers) and\n",
    "                channel 1 (workers)\n",
    "        Step 3: Generate a straight-line trajectory to the destination\n",
    "        Step 4: For each waypoint, check if it is within safety_margin of any obstacle.\n",
    "                If so, shift it perpendicular to the trajectory direction by safety_margin.\n",
    "        Step 5: Smooth the adjusted trajectory using a simple moving average\n",
    "        Step 6: Recompute heading angles from the smoothed x,y positions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, safety_margin: float = 3.0):\n",
    "        self.safety_margin = safety_margin\n",
    "\n",
    "    def predict(self, image: torch.Tensor, instruction: torch.Tensor,\n",
    "                egomotion: torch.Tensor) -> torch.Tensor:\n",
    "        # YOUR CODE HERE (approximately 25-35 lines)\n",
    "        pass"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verification for TODO 3 ---\n",
    "if ObstacleAwareBaseline().predict(val_dataset[0][0], val_dataset[0][1], val_dataset[0][2]) is not None:\n",
    "    improved = ObstacleAwareBaseline()\n",
    "    imp_metrics = {\"ADE\": [], \"FDE\": [], \"Jerk\": []}\n",
    "    for i in range(min(200, len(val_dataset))):\n",
    "        image, instruction, egomotion, gt_traj = val_dataset[i]\n",
    "        pred = improved.predict(image, instruction, egomotion)\n",
    "        assert pred.shape == (NUM_WAYPOINTS, 3), f\"Wrong shape: {pred.shape}\"\n",
    "        m = compute_metrics(pred, gt_traj)\n",
    "        for k, v in m.items():\n",
    "            imp_metrics[k].append(v)\n",
    "\n",
    "    print(\"=== Obstacle-Aware Baseline Results ===\")\n",
    "    for k in imp_metrics:\n",
    "        vals = imp_metrics[k]\n",
    "        print(f\"  {k}: {np.mean(vals):.4f} +/- {np.std(vals):.4f}\")\n",
    "    print(\"\\n(Compare with straight-line baseline above)\")\n",
    "else:\n",
    "    print(\"[SKIP] ObstacleAwareBaseline not yet implemented\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. VLA Model Architecture\n",
    "\n",
    "### 4.1 Vision Encoder\n",
    "\n",
    "A CNN that processes the 3-channel port scene image into a 256-dimensional feature vector. In a production VLA (like Alpamayo), this would be a Vision Transformer processing multi-camera inputs. We use a CNN here for training efficiency."
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode the port scene image into a feature vector.\n",
    "\n",
    "    Input: (B, 3, 128, 128) â€” 3-channel port image\n",
    "    Output: (B, 256) â€” scene feature vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.fc = nn.Linear(128, feature_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B = x.shape[0]\n",
    "        h = self.conv(x).view(B, -1)\n",
    "        return self.fc(h)"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Text Encoder\n",
    "\n",
    "An embedding layer + single-layer Transformer that converts tokenized dispatcher instructions into a 256-dimensional feature vector. In a production VLA, this would be a pre-trained LLM like LLaMA."
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode dispatcher instructions into a feature vector.\n",
    "\n",
    "    Input: (B, max_len) â€” tokenized instruction\n",
    "    Output: (B, 256) â€” instruction feature vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, embed_dim: int = 128,\n",
    "                 feature_dim: int = 256, max_len: int = 32):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos_embedding = nn.Embedding(max_len, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=4, dim_feedforward=256,\n",
    "            dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.fc = nn.Linear(embed_dim, feature_dim)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        B, L = tokens.shape\n",
    "        positions = torch.arange(L, device=tokens.device).unsqueeze(0).expand(B, -1)\n",
    "        x = self.embedding(tokens) + self.pos_embedding(positions)\n",
    "        pad_mask = (tokens == 0)\n",
    "        x = self.transformer(x, src_key_padding_mask=pad_mask)\n",
    "        mask = (~pad_mask).float().unsqueeze(-1)\n",
    "        x = (x * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "        return self.fc(x)"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Diffusion Trajectory Decoder\n",
    "\n",
    "The core of our VLA: a denoiser network that predicts the noise added to a trajectory, conditioned on visual and linguistic features."
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalTimestepEmbedding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding for diffusion timestep.\"\"\"\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device, dtype=torch.float32) * -emb)\n",
    "        emb = t.float().unsqueeze(-1) * emb.unsqueeze(0)\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class TrajectoryDenoiser(nn.Module):\n",
    "    \"\"\"\n",
    "    Predicts noise added to a trajectory, conditioned on timestep and scene/language features.\n",
    "\n",
    "    Input:\n",
    "        noisy_traj: (B, 32, 3) â€” noisy trajectory\n",
    "        timestep: (B,) â€” diffusion timestep [0, T)\n",
    "        condition: (B, 512) â€” concatenated vision + text features\n",
    "\n",
    "    Output: (B, 32, 3) â€” predicted noise\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, traj_dim: int = 32 * 3, cond_dim: int = 512,\n",
    "                 hidden_dim: int = 512, time_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.time_embed = SinusoidalTimestepEmbedding(time_dim)\n",
    "        self.time_proj = nn.Linear(time_dim, hidden_dim)\n",
    "        self.cond_proj = nn.Linear(cond_dim, hidden_dim)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(traj_dim + hidden_dim + hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, traj_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, noisy_traj: torch.Tensor, timestep: torch.Tensor,\n",
    "                condition: torch.Tensor) -> torch.Tensor:\n",
    "        B = noisy_traj.shape[0]\n",
    "        traj_flat = noisy_traj.view(B, -1)  # (B, 96)\n",
    "        t_emb = self.time_proj(self.time_embed(timestep))  # (B, hidden)\n",
    "        c_emb = self.cond_proj(condition)  # (B, hidden)\n",
    "\n",
    "        x = torch.cat([traj_flat, t_emb, c_emb], dim=-1)\n",
    "        noise_pred = self.net(x)\n",
    "        return noise_pred.view(B, 32, 3)"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Noise Schedule"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseSchedule:\n",
    "    \"\"\"Linear noise schedule for diffusion.\"\"\"\n",
    "\n",
    "    def __init__(self, num_steps: int = 100, beta_start: float = 1e-4, beta_end: float = 0.02):\n",
    "        self.num_steps = num_steps\n",
    "        self.beta = torch.linspace(beta_start, beta_end, num_steps)\n",
    "        self.alpha = 1.0 - self.beta\n",
    "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
    "        self.sqrt_alpha_bar = torch.sqrt(self.alpha_bar)\n",
    "        self.sqrt_one_minus_alpha_bar = torch.sqrt(1.0 - self.alpha_bar)\n",
    "\n",
    "    def to(self, device):\n",
    "        self.beta = self.beta.to(device)\n",
    "        self.alpha = self.alpha.to(device)\n",
    "        self.alpha_bar = self.alpha_bar.to(device)\n",
    "        self.sqrt_alpha_bar = self.sqrt_alpha_bar.to(device)\n",
    "        self.sqrt_one_minus_alpha_bar = self.sqrt_one_minus_alpha_bar.to(device)\n",
    "        return self\n",
    "\n",
    "\n",
    "schedule = NoiseSchedule(num_steps=100)"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 TODO 4: Assemble the Full VLA Model\n",
    "\n",
    "This is the core exercise. Assemble VisionEncoder + TextEncoder + TrajectoryDenoiser into a complete VLA model with both training forward pass and inference sampling."
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortVLA(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified Vision-Language-Action model for port terminal tractors.\n",
    "\n",
    "    This model combines:\n",
    "    1. A VisionEncoder to process the port scene image\n",
    "    2. A TextEncoder to process the dispatcher instruction\n",
    "    3. A TrajectoryDenoiser (diffusion decoder) to generate trajectories\n",
    "\n",
    "    The forward pass for TRAINING:\n",
    "    - Encode the image and instruction into feature vectors\n",
    "    - Concatenate them into a conditioning vector\n",
    "    - Sample a random timestep and add noise to the expert trajectory\n",
    "    - Predict the noise using the denoiser\n",
    "    - Return the predicted noise and actual noise (for loss computation)\n",
    "\n",
    "    The forward pass for INFERENCE (sampling):\n",
    "    - Encode image and instruction\n",
    "    - Start from pure noise\n",
    "    - Iteratively denoise using the reverse diffusion process\n",
    "    - Return the generated trajectory\n",
    "\n",
    "    Hints:\n",
    "        __init__:\n",
    "            Step 1: Create VisionEncoder(feature_dim=vision_dim)\n",
    "            Step 2: Create TextEncoder(vocab_size, feature_dim=text_dim)\n",
    "            Step 3: Create TrajectoryDenoiser(cond_dim=vision_dim + text_dim)\n",
    "            Step 4: Create NoiseSchedule(num_steps=num_diffusion_steps)\n",
    "\n",
    "        forward (training mode):\n",
    "            Step 1: Encode image with vision encoder -> (B, vision_dim)\n",
    "            Step 2: Encode instruction with text encoder -> (B, text_dim)\n",
    "            Step 3: Concatenate vision and text features -> (B, vision_dim + text_dim)\n",
    "            Step 4: Sample random timesteps: torch.randint(0, T, (B,))\n",
    "            Step 5: Sample random noise: torch.randn_like(trajectory)\n",
    "            Step 6: Create noisy trajectory using forward diffusion:\n",
    "                    noisy = sqrt_alpha_bar[t] * trajectory + sqrt_one_minus_alpha_bar[t] * noise\n",
    "                    (Reshape schedule values for broadcasting: [..., None, None])\n",
    "            Step 7: Predict noise using denoiser(noisy, timesteps, condition)\n",
    "            Step 8: Return (predicted_noise, actual_noise)\n",
    "\n",
    "        sample (inference mode):\n",
    "            Step 1: Encode image and instruction, concatenate\n",
    "            Step 2: Start from pure noise: x = torch.randn(B, 32, 3)\n",
    "            Step 3: Loop from t = T-1 down to 0:\n",
    "                    a. Predict noise: eps = denoiser(x, t, condition)\n",
    "                    b. Compute the denoised mean using the DDPM reverse formula\n",
    "                    c. If t > 0, add scaled random noise\n",
    "            Step 4: Return the final denoised trajectory\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, vision_dim: int = 256,\n",
    "                 text_dim: int = 256, num_diffusion_steps: int = 100):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE (approximately 5-7 lines)\n",
    "        pass\n",
    "\n",
    "    def forward(self, image: torch.Tensor, instruction: torch.Tensor,\n",
    "                trajectory: torch.Tensor):\n",
    "        \"\"\"Training forward pass. Returns (predicted_noise, actual_noise).\"\"\"\n",
    "        # YOUR CODE HERE (approximately 10-12 lines)\n",
    "        pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, image: torch.Tensor, instruction: torch.Tensor,\n",
    "               num_samples: int = 1) -> torch.Tensor:\n",
    "        \"\"\"Generate trajectory samples via reverse diffusion.\"\"\"\n",
    "        # YOUR CODE HERE (approximately 15-20 lines)\n",
    "        pass"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verification for TODO 4 ---\n",
    "try:\n",
    "    model = PortVLA(vocab_size=train_dataset.vocab_size).to(device)\n",
    "\n",
    "    # Test training forward pass\n",
    "    test_img = torch.randn(4, 3, 128, 128).to(device)\n",
    "    test_inst = torch.randint(0, train_dataset.vocab_size, (4, 32)).to(device)\n",
    "    test_traj = torch.randn(4, 32, 3).to(device)\n",
    "\n",
    "    pred_noise, actual_noise = model(test_img, test_inst, test_traj)\n",
    "    assert pred_noise.shape == (4, 32, 3), f\"Wrong noise shape: {pred_noise.shape}\"\n",
    "    assert actual_noise.shape == (4, 32, 3), f\"Wrong noise shape: {actual_noise.shape}\"\n",
    "    print(\"[PASS] Training forward pass: correct shapes\")\n",
    "\n",
    "    # Test sampling\n",
    "    sampled = model.sample(test_img[:1], test_inst[:1], num_samples=1)\n",
    "    assert sampled.shape == (1, 32, 3), f\"Wrong sample shape: {sampled.shape}\"\n",
    "    print(\"[PASS] Sampling: correct shape\")\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] {e}\")\n",
    "    print(\"Implement PortVLA in TODO 4 above, then re-run this cell.\")"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Training\n",
    "\n",
    "### 5.1 Training Configuration"
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 3e-4\n",
    "NUM_EPOCHS = 30\n",
    "WEIGHT_DECAY = 1e-4\n",
    "WARMUP_STEPS = 200\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 TODO 5: Training Loop"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vla(model, train_loader, val_loader, num_epochs, lr, weight_decay, warmup_steps):\n",
    "    \"\"\"\n",
    "    Train the VLA model using the diffusion denoising objective.\n",
    "\n",
    "    The training objective is simple: predict the noise that was added to the\n",
    "    expert trajectory. The loss is MSE between predicted and actual noise.\n",
    "\n",
    "    We use AdamW (not Adam) because weight decay helps prevent overfitting on\n",
    "    our relatively small synthetic dataset. The cosine learning rate schedule\n",
    "    with warmup provides stable early training (warmup prevents large initial\n",
    "    gradient updates) and graceful convergence (cosine decay avoids the abrupt\n",
    "    drops of step schedules).\n",
    "\n",
    "    Args:\n",
    "        model: PortVLA model\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        num_epochs: Number of training epochs\n",
    "        lr: Peak learning rate\n",
    "        weight_decay: AdamW weight decay\n",
    "        warmup_steps: Number of linear warmup steps\n",
    "\n",
    "    Returns:\n",
    "        Dict with 'train_losses' and 'val_losses' lists\n",
    "\n",
    "    Hints:\n",
    "        Step 1: Create AdamW optimizer with the given lr and weight_decay\n",
    "        Step 2: Create a learning rate scheduler â€” use\n",
    "                torch.optim.lr_scheduler.CosineAnnealingLR (T_max = num_epochs * len(train_loader))\n",
    "                For warmup: manually scale LR for the first warmup_steps\n",
    "        Step 3: For each epoch:\n",
    "            a. Set model to train mode\n",
    "            b. For each batch (image, instruction, egomotion, trajectory):\n",
    "                - Move all tensors to device\n",
    "                - Forward pass: pred_noise, actual_noise = model(image, instruction, trajectory)\n",
    "                - Compute MSE loss: F.mse_loss(pred_noise, actual_noise)\n",
    "                - Backward pass and optimizer step\n",
    "                - Apply learning rate warmup if within warmup_steps\n",
    "                - Step the scheduler\n",
    "            c. Compute validation loss (model.eval(), no gradients)\n",
    "            d. Print epoch stats: train loss, val loss, learning rate\n",
    "        Step 4: Return the loss histories\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE (approximately 35-45 lines)\n",
    "    pass"
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run Training ---\n",
    "# (Uncomment after implementing TODO 5)\n",
    "\n",
    "# history = train_vla(model, train_loader, val_loader,\n",
    "#                     num_epochs=NUM_EPOCHS, lr=LEARNING_RATE,\n",
    "#                     weight_decay=WEIGHT_DECAY, warmup_steps=WARMUP_STEPS)"
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Training Curves"
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize Training Curves ---\n",
    "# (Uncomment after training completes)\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "#\n",
    "# ax1.plot(history['train_losses'], label='Train Loss', alpha=0.7)\n",
    "# ax1.plot(history['val_losses'], label='Val Loss', alpha=0.7)\n",
    "# ax1.set_xlabel('Epoch')\n",
    "# ax1.set_ylabel('MSE Loss')\n",
    "# ax1.set_title('Training and Validation Loss')\n",
    "# ax1.legend()\n",
    "# ax1.grid(True, alpha=0.3)\n",
    "#\n",
    "# ax2.plot(history.get('lrs', []), alpha=0.7)\n",
    "# ax2.set_xlabel('Step')\n",
    "# ax2.set_ylabel('Learning Rate')\n",
    "# ax2.set_title('Learning Rate Schedule')\n",
    "# ax2.grid(True, alpha=0.3)\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Evaluation\n",
    "\n",
    "### 6.1 Quantitative Evaluation"
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, num_samples=5):\n",
    "    \"\"\"\n",
    "    Evaluate the VLA model on the validation set.\n",
    "    For each sample, generate num_samples trajectories and pick the best (lowest ADE).\n",
    "    This leverages the diffusion model's multimodality.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_metrics = {\"ADE\": [], \"FDE\": [], \"Jerk\": []}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (image, instruction, egomotion, gt_traj) in enumerate(val_loader):\n",
    "            if batch_idx >= 10:  # Evaluate on 10 batches\n",
    "                break\n",
    "            image = image.to(device)\n",
    "            instruction = instruction.to(device)\n",
    "            gt_traj = gt_traj.to(device)\n",
    "\n",
    "            for i in range(min(4, image.shape[0])):\n",
    "                best_ade = float('inf')\n",
    "                best_pred = None\n",
    "\n",
    "                for _ in range(num_samples):\n",
    "                    pred = model.sample(image[i:i+1], instruction[i:i+1])\n",
    "                    m = compute_metrics(pred[0].cpu(), gt_traj[i].cpu())\n",
    "                    if m[\"ADE\"] < best_ade:\n",
    "                        best_ade = m[\"ADE\"]\n",
    "                        best_pred = pred[0].cpu()\n",
    "                        best_metrics = m\n",
    "\n",
    "                for k, v in best_metrics.items():\n",
    "                    all_metrics[k].append(v)\n",
    "\n",
    "    print(\"=== VLA Model Evaluation (best of 5 samples) ===\")\n",
    "    for k, vals in all_metrics.items():\n",
    "        print(f\"  {k}: {np.mean(vals):.4f} +/- {np.std(vals):.4f}\")\n",
    "\n",
    "    return all_metrics\n",
    "\n",
    "\n",
    "# Uncomment after training:\n",
    "# vla_metrics = evaluate_model(model, val_loader)"
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 TODO 6: Comparative Evaluation"
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 6: Generate a Comparative Evaluation\n",
    "#\n",
    "# After training, complete the following evaluation:\n",
    "#\n",
    "# 1. Run evaluate_model() on your trained VLA\n",
    "# 2. Collect baseline metrics from Section 3\n",
    "# 3. Create a bar chart comparing ADE, FDE, and Jerk for:\n",
    "#    - Straight-Line Baseline\n",
    "#    - Obstacle-Aware Baseline (your TODO 3)\n",
    "#    - VLA Model\n",
    "#    Use grouped bars with error bars showing +/- 1 std deviation.\n",
    "#    Include a horizontal dashed line at the target threshold for each metric.\n",
    "#\n",
    "# 4. Create a 2x3 figure showing 6 validation samples:\n",
    "#    - Each subplot: port image + ground-truth trajectory (green) + VLA prediction (red)\n",
    "#    - Title each with ADE for that sample\n",
    "#\n",
    "# 5. Answer these thought questions:\n",
    "#    Q1: Where does the VLA outperform the baselines most significantly? Why?\n",
    "#    Q2: Are there scenarios where the straight-line baseline is actually better\n",
    "#        than the VLA? Why might this happen?\n",
    "#    Q3: How would you expect performance to change if we increased the dataset\n",
    "#        from 5,000 to 50,000 samples? Which metric would improve most?\n",
    "\n",
    "# YOUR CODE HERE"
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Error Analysis\n",
    "\n",
    "### 7.1 TODO 7: Systematic Error Analysis"
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 7: Systematic Error Analysis\n",
    "#\n",
    "# After training and evaluation, perform a systematic error analysis:\n",
    "#\n",
    "# 1. Identify the 20 validation samples with the HIGHEST ADE (worst predictions).\n",
    "#    For each, visualize the scene image, ground-truth trajectory, and VLA prediction.\n",
    "#    Look for patterns: are failures concentrated in specific port areas, long\n",
    "#    distances, scenes with many workers, or particular instruction types?\n",
    "#\n",
    "# 2. Categorize failures into at least 3 categories. Suggested categories:\n",
    "#    - \"Long distance\": destination is far away, trajectory accumulates error\n",
    "#    - \"Obstacle avoidance\": trajectory clips an obstacle\n",
    "#    - \"Instruction misunderstanding\": trajectory goes to wrong destination\n",
    "#    - \"Multimodal confusion\": trajectory is a valid path but to a different\n",
    "#      plausible destination (evidence of mode averaging)\n",
    "#\n",
    "# 3. For each category:\n",
    "#    - Count how many of the top-20 failures fall into it\n",
    "#    - Propose a specific fix (more data, architectural change, loss modification)\n",
    "#    - Explain WHY that fix addresses the root cause\n",
    "#\n",
    "# 4. Compute the collision rate: what percentage of VLA-generated trajectories\n",
    "#    pass within 1 grid cell of an obstacle (container or worker)?\n",
    "\n",
    "# YOUR CODE HERE"
   ],
   "id": "cell_51"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Deployment Considerations\n",
    "\n",
    "### 8.1 Inference Latency Profiling"
   ],
   "id": "cell_52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_inference(model, image, instruction, num_runs=20):\n",
    "    \"\"\"Profile inference latency by component.\"\"\"\n",
    "    model.eval()\n",
    "    image = image.to(device)\n",
    "    instruction = instruction.to(device)\n",
    "\n",
    "    # Warm up\n",
    "    for _ in range(3):\n",
    "        _ = model.sample(image, instruction)\n",
    "\n",
    "    # Profile\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        start = time.perf_counter()\n",
    "        _ = model.sample(image, instruction)\n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        times.append((time.perf_counter() - start) * 1000)\n",
    "\n",
    "    times = np.array(times)\n",
    "    print(f\"=== Inference Latency ({num_runs} runs) ===\")\n",
    "    print(f\"  Mean: {times.mean():.1f} ms\")\n",
    "    print(f\"  Std:  {times.std():.1f} ms\")\n",
    "    print(f\"  P50:  {np.percentile(times, 50):.1f} ms\")\n",
    "    print(f\"  P95:  {np.percentile(times, 95):.1f} ms\")\n",
    "    print(f\"  P99:  {np.percentile(times, 99):.1f} ms\")\n",
    "    return times\n",
    "\n",
    "# Uncomment after training:\n",
    "# test_img = val_dataset[0][0].unsqueeze(0)\n",
    "# test_inst = val_dataset[0][1].unsqueeze(0)\n",
    "# latencies = profile_inference(model, test_img, test_inst)"
   ],
   "id": "cell_53"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 TODO 8: DDIM Accelerated Sampling"
   ],
   "id": "cell_54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddim_sample(model, image, instruction, num_ddim_steps=10):\n",
    "    \"\"\"\n",
    "    Implement DDIM (Denoising Diffusion Implicit Models) sampling to reduce\n",
    "    the number of denoising steps from 100 to 10.\n",
    "\n",
    "    DDIM is deterministic â€” it skips timesteps by taking larger denoising jumps.\n",
    "    This is critical for deployment: 100 denoising steps at ~2ms each = 200ms,\n",
    "    which barely meets the latency budget. 10 DDIM steps = ~20ms, leaving\n",
    "    headroom for vision and text encoding.\n",
    "\n",
    "    Args:\n",
    "        model: Trained PortVLA model\n",
    "        image: (1, 3, 128, 128) input image\n",
    "        instruction: (1, 32) tokenized instruction\n",
    "        num_ddim_steps: Number of DDIM sampling steps (default 10)\n",
    "\n",
    "    Returns:\n",
    "        trajectory: (1, 32, 3) generated trajectory\n",
    "\n",
    "    Hints:\n",
    "        Step 1: Compute the DDIM timestep subsequence â€” evenly space num_ddim_steps\n",
    "                timesteps across [0, T). Use torch.linspace(0, T-1, num_ddim_steps).long()\n",
    "        Step 2: Encode image and instruction to get the condition vector\n",
    "        Step 3: Start from noise x = torch.randn(1, 32, 3)\n",
    "        Step 4: Loop through timesteps in REVERSE order. At each step:\n",
    "                a. Predict noise: eps = denoiser(x, t, condition)\n",
    "                b. Compute predicted x0: x0_pred = (x - sqrt(1-alpha_bar_t) * eps) / sqrt(alpha_bar_t)\n",
    "                c. If not the last step, compute x at the previous timestep:\n",
    "                   x = sqrt(alpha_bar_prev) * x0_pred + sqrt(1 - alpha_bar_prev) * eps\n",
    "                   (This is the DDIM update â€” no stochastic noise added)\n",
    "                d. If last step, x = x0_pred\n",
    "        Step 5: Return x\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE (approximately 20-25 lines)\n",
    "    pass"
   ],
   "id": "cell_55"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Ethical and Regulatory Analysis\n",
    "\n",
    "### 9.1 TODO 9: Ethical Impact Assessment\n",
    "\n",
    "Write a brief (300-500 word) ethical impact assessment for deploying the VLA-based autonomous terminal tractor system in a markdown cell below. Address:\n",
    "\n",
    "1. **Worker Safety**: False positives (nuisance stops) vs. false negatives (missed detections). Which way should the system be biased, and why?\n",
    "2. **Labor Displacement**: 140 drivers reduced to 20 supervisors. What is the company's ethical obligation?\n",
    "3. **Accountability**: If an accident occurs, who is liable? How does the reasoning trace help?\n",
    "4. **Bias and Fairness**: Training data from one terminal â€” how might this bias generalization?\n",
    "5. **Regulatory Compliance**: List 3 specific regulations (MTSA, OSHA, ISO 3691) and one requirement from each.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you built a simplified Vision-Language-Action model for autonomous port terminal tractors. The core components you implemented:\n",
    "\n",
    "1. **Synthetic port environment** with container blocks, corridors, workers, and dispatcher instructions\n",
    "2. **Vision encoder** (CNN) for processing port scene images\n",
    "3. **Text encoder** (Transformer) for processing natural language instructions\n",
    "4. **Diffusion trajectory decoder** for generating smooth, multimodal trajectories\n",
    "5. **Training pipeline** using the denoising objective (behavioral cloning via diffusion)\n",
    "6. **Evaluation** comparing the VLA against straight-line and obstacle-aware baselines\n",
    "7. **Error analysis** to systematically categorize and address failure modes\n",
    "8. **DDIM sampling** for deployment-friendly inference latency\n",
    "\n",
    "The architecture mirrors what NVIDIA's Alpamayo uses at production scale â€” a vision encoder feeds into a VLM backbone which conditions a diffusion decoder. The key insight is that diffusion-based action generation handles **multimodal trajectory distributions** (multiple valid paths) without the mode-averaging problem of regression-based approaches.\n",
    "\n",
    "For further reading on production deployment, system design, and scaling considerations, see **Section 4** of the accompanying case study document (`case_study.pdf`)."
   ],
   "id": "cell_56"
  }
 ]
}