{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "NovaMind AI: Adaptive Content Recommendation \u2014 Implementation Notebook"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NovaMind AI: Adaptive Content Recommendation with Policy Gradient Methods -- Implementation Notebook\n",
    "\n",
    "## Environment Setup"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gymnasium torch numpy matplotlib seaborn scipy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulated Learner Environment\n",
    "\n",
    "We simulate an educational platform where learners interact with content modules. The environment models learner knowledge, engagement, and progression."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnerEnvironment:\n",
    "    \"\"\"\n",
    "    Simulated environment for adaptive content recommendation.\n",
    "    Models learner behavior as an MDP.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_modules=100, state_dim=32, num_topics=10, max_steps=15):\n",
    "        self.num_modules = num_modules\n",
    "        self.state_dim = state_dim\n",
    "        self.num_topics = num_topics\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        # Module properties\n",
    "        self.module_difficulty = np.random.uniform(0, 1, num_modules)\n",
    "        self.module_topic = np.random.randint(0, num_topics, num_modules)\n",
    "        self.module_quality = np.random.uniform(0.5, 1.0, num_modules)\n",
    "\n",
    "        # Module embeddings for scoring\n",
    "        self.module_embeddings = np.random.randn(num_modules, 16).astype(np.float32)\n",
    "\n",
    "    def reset(self, learner_level=None):\n",
    "        \"\"\"Reset for a new learner session.\"\"\"\n",
    "        if learner_level is None:\n",
    "            learner_level = np.random.uniform(0.1, 0.9)\n",
    "\n",
    "        self.learner_level = learner_level\n",
    "        self.knowledge = np.full(self.num_topics, learner_level)\n",
    "        self.step_count = 0\n",
    "        self.modules_seen = set()\n",
    "        self.session_engagement = 1.0  # Decays if bad recommendations\n",
    "\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"Construct the state vector.\"\"\"\n",
    "        state = np.zeros(self.state_dim, dtype=np.float32)\n",
    "        state[:self.num_topics] = self.knowledge\n",
    "        state[self.num_topics] = self.learner_level\n",
    "        state[self.num_topics + 1] = self.step_count / self.max_steps\n",
    "        state[self.num_topics + 2] = self.session_engagement\n",
    "        state[self.num_topics + 3] = len(self.modules_seen) / self.num_modules\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Learner interacts with recommended module.\"\"\"\n",
    "        assert 0 <= action < self.num_modules\n",
    "\n",
    "        difficulty = self.module_difficulty[action]\n",
    "        topic = self.module_topic[action]\n",
    "        quality = self.module_quality[action]\n",
    "\n",
    "        # Completion probability depends on difficulty-level match\n",
    "        difficulty_gap = abs(difficulty - self.knowledge[topic])\n",
    "        completion_prob = max(0.1, 1.0 - 2.0 * difficulty_gap) * quality\n",
    "\n",
    "        completed = np.random.random() < completion_prob\n",
    "\n",
    "        # Compute reward\n",
    "        reward = 0.0\n",
    "        if completed:\n",
    "            reward += 1.0\n",
    "            # Knowledge gain\n",
    "            gain = 0.05 * (1.0 - self.knowledge[topic])\n",
    "            self.knowledge[topic] = min(1.0, self.knowledge[topic] + gain)\n",
    "            # Quiz bonus (simulated)\n",
    "            quiz_score = min(1.0, self.knowledge[topic] + np.random.normal(0, 0.1))\n",
    "            reward += 0.5 * max(0, quiz_score)\n",
    "        else:\n",
    "            reward -= 0.3\n",
    "            self.session_engagement *= 0.9\n",
    "\n",
    "        # Engagement bonus for appropriate difficulty\n",
    "        if difficulty_gap < 0.15:\n",
    "            reward += 0.3\n",
    "\n",
    "        # Novelty penalty for repeated modules\n",
    "        if action in self.modules_seen:\n",
    "            reward -= 0.5\n",
    "\n",
    "        self.modules_seen.add(action)\n",
    "        self.step_count += 1\n",
    "\n",
    "        done = (self.step_count >= self.max_steps) or (self.session_engagement < 0.3)\n",
    "        info = {\n",
    "            \"completed\": completed,\n",
    "            \"difficulty_gap\": difficulty_gap,\n",
    "            \"knowledge_gain\": gain if completed else 0,\n",
    "            \"topic\": topic\n",
    "        }\n",
    "\n",
    "        return self._get_state(), reward, done, info\n",
    "\n",
    "    def get_candidates(self, k=20):\n",
    "        \"\"\"Return top-k candidate modules based on topic relevance.\"\"\"\n",
    "        topic_scores = np.zeros(self.num_modules)\n",
    "        for i in range(self.num_modules):\n",
    "            topic = self.module_topic[i]\n",
    "            # Prefer topics where knowledge is moderate (zone of proximal development)\n",
    "            topic_scores[i] = 1.0 - abs(self.knowledge[topic] - self.module_difficulty[i])\n",
    "            if i not in self.modules_seen:\n",
    "                topic_scores[i] += 0.2  # Novelty bonus\n",
    "\n",
    "        top_k = np.argsort(topic_scores)[-k:]\n",
    "        return top_k\n",
    "\n",
    "env = LearnerEnvironment()\n",
    "state = env.reset()\n",
    "print(f\"State dimension: {state.shape}\")\n",
    "print(f\"Number of modules: {env.num_modules}\")\n",
    "print(f\"Initial learner level: {env.learner_level:.3f}\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect episodes with random policy\n",
    "def collect_random_episodes(env, num_episodes=500):\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    completion_rates = []\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        completions = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            candidates = env.get_candidates(k=20)\n",
    "            action = np.random.choice(candidates)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if info[\"completed\"]:\n",
    "                completions += 1\n",
    "            steps += 1\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        completion_rates.append(completions / max(steps, 1))\n",
    "\n",
    "    return episode_rewards, episode_lengths, completion_rates\n",
    "\n",
    "rewards, lengths, completions = collect_random_episodes(env)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(rewards, bins=30, color='steelblue', edgecolor='white', alpha=0.8)\n",
    "axes[0].set_xlabel('Episode Reward')\n",
    "axes[0].set_title('Reward Distribution (Random Policy)')\n",
    "axes[0].axvline(np.mean(rewards), color='red', linestyle='--', label=f'Mean: {np.mean(rewards):.2f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(lengths, bins=15, color='seagreen', edgecolor='white', alpha=0.8)\n",
    "axes[1].set_xlabel('Episode Length')\n",
    "axes[1].set_title('Session Length Distribution')\n",
    "\n",
    "axes[2].hist(completions, bins=20, color='coral', edgecolor='white', alpha=0.8)\n",
    "axes[2].set_xlabel('Completion Rate')\n",
    "axes[2].set_title('Module Completion Rate')\n",
    "axes[2].axvline(np.mean(completions), color='red', linestyle='--', label=f'Mean: {np.mean(completions):.2%}')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Random policy baseline:\")\n",
    "print(f\"  Mean reward: {np.mean(rewards):.2f}\")\n",
    "print(f\"  Mean session length: {np.mean(lengths):.1f}\")\n",
    "print(f\"  Mean completion rate: {np.mean(completions):.2%}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline: Rule-Based Recommender"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleBasedRecommender:\n",
    "    \"\"\"Baseline: always recommend the module closest in difficulty to learner level.\"\"\"\n",
    "\n",
    "    def recommend(self, state, candidates, env):\n",
    "        learner_knowledge = state[:env.num_topics]\n",
    "        best_action = candidates[0]\n",
    "        best_gap = float('inf')\n",
    "\n",
    "        for c in candidates:\n",
    "            topic = env.module_topic[c]\n",
    "            gap = abs(env.module_difficulty[c] - learner_knowledge[topic])\n",
    "            if gap < best_gap:\n",
    "                best_gap = gap\n",
    "                best_action = c\n",
    "\n",
    "        return best_action\n",
    "\n",
    "# Evaluate baseline\n",
    "baseline = RuleBasedRecommender()\n",
    "baseline_rewards = []\n",
    "baseline_completions = []\n",
    "\n",
    "for _ in range(500):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    completions = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        candidates = env.get_candidates(k=20)\n",
    "        action = baseline.recommend(state, candidates, env)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if info[\"completed\"]:\n",
    "            completions += 1\n",
    "        steps += 1\n",
    "\n",
    "    baseline_rewards.append(total_reward)\n",
    "    baseline_completions.append(completions / max(steps, 1))\n",
    "\n",
    "print(f\"Rule-based baseline:\")\n",
    "print(f\"  Mean reward: {np.mean(baseline_rewards):.2f}\")\n",
    "print(f\"  Mean completion rate: {np.mean(baseline_completions):.2%}\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Actor-Critic Content Recommender"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentActor(nn.Module):\n",
    "    \"\"\"Policy network for content recommendation.\"\"\"\n",
    "    def __init__(self, state_dim, module_embed_dim=16, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.state_encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 64)\n",
    "        )\n",
    "        self.scorer = nn.Sequential(\n",
    "            nn.Linear(64 + module_embed_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, candidate_embeddings):\n",
    "        \"\"\"\n",
    "        state: (state_dim,)\n",
    "        candidate_embeddings: (K, embed_dim)\n",
    "        Returns: probabilities over K candidates\n",
    "        \"\"\"\n",
    "        encoded = self.state_encoder(state)  # (64,)\n",
    "        K = candidate_embeddings.shape[0]\n",
    "        encoded_expanded = encoded.unsqueeze(0).expand(K, -1)  # (K, 64)\n",
    "\n",
    "        combined = torch.cat([encoded_expanded, candidate_embeddings], dim=-1)  # (K, 64+16)\n",
    "        scores = self.scorer(combined).squeeze(-1)  # (K,)\n",
    "        probs = F.softmax(scores, dim=-1)\n",
    "        return probs\n",
    "\n",
    "    def sample_action(self, state, candidate_embeddings, candidates):\n",
    "        probs = self.forward(state, candidate_embeddings)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        idx = dist.sample()\n",
    "        return candidates[idx.item()], dist.log_prob(idx), dist.entropy()\n",
    "\n",
    "\n",
    "class ContentCritic(nn.Module):\n",
    "    \"\"\"Value network: estimates expected session return.\"\"\"\n",
    "    def __init__(self, state_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state).squeeze(-1)\n",
    "\n",
    "actor = ContentActor(state_dim=32)\n",
    "critic = ContentCritic(state_dim=32)\n",
    "\n",
    "# Count parameters\n",
    "actor_params = sum(p.numel() for p in actor.parameters())\n",
    "critic_params = sum(p.numel() for p in critic.parameters())\n",
    "print(f\"Actor parameters: {actor_params:,}\")\n",
    "print(f\"Critic parameters: {critic_params:,}\")\n",
    "print(f\"Total parameters: {actor_params + critic_params:,}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.95\n",
    "ENTROPY_COEFF = 0.01\n",
    "CRITIC_COEFF = 0.5\n",
    "NUM_EPISODES = 2000\n",
    "LR_ACTOR = 3e-4\n",
    "LR_CRITIC = 1e-3\n",
    "\n",
    "actor = ContentActor(state_dim=32)\n",
    "critic = ContentCritic(state_dim=32)\n",
    "actor_opt = torch.optim.Adam(actor.parameters(), lr=LR_ACTOR)\n",
    "critic_opt = torch.optim.Adam(critic.parameters(), lr=LR_CRITIC)\n",
    "\n",
    "reward_history = []\n",
    "completion_history = []\n",
    "entropy_history = []\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "    log_probs, entropies, values, rewards_ep = [], [], [], []\n",
    "    completions_ep = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        state_t = torch.tensor(state, dtype=torch.float32)\n",
    "        candidates = env.get_candidates(k=20)\n",
    "        cand_embeds = torch.tensor(env.module_embeddings[candidates], dtype=torch.float32)\n",
    "\n",
    "        # Actor: sample action\n",
    "        action, log_prob, entropy = actor.sample_action(state_t, cand_embeds, candidates)\n",
    "\n",
    "        # Critic: estimate value\n",
    "        value = critic(state_t)\n",
    "\n",
    "        # Step environment\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        if info[\"completed\"]:\n",
    "            completions_ep += 1\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        entropies.append(entropy)\n",
    "        values.append(value)\n",
    "        rewards_ep.append(reward)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # Compute returns\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards_ep):\n",
    "        G = r + GAMMA * G\n",
    "        returns.insert(0, G)\n",
    "    returns_t = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "    # Stack tensors\n",
    "    log_probs_t = torch.stack(log_probs)\n",
    "    values_t = torch.stack(values)\n",
    "    entropies_t = torch.stack(entropies)\n",
    "\n",
    "    # Advantages\n",
    "    advantages = returns_t - values_t.detach()\n",
    "    if len(advantages) > 1:\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    # Losses\n",
    "    actor_loss = -(log_probs_t * advantages).mean()\n",
    "    critic_loss = F.mse_loss(values_t, returns_t)\n",
    "    entropy_loss = -entropies_t.mean()\n",
    "\n",
    "    total_loss = actor_loss + CRITIC_COEFF * critic_loss + ENTROPY_COEFF * entropy_loss\n",
    "\n",
    "    # Update\n",
    "    actor_opt.zero_grad()\n",
    "    critic_opt.zero_grad()\n",
    "    total_loss.backward()\n",
    "    actor_opt.step()\n",
    "    critic_opt.step()\n",
    "\n",
    "    ep_reward = sum(rewards_ep)\n",
    "    ep_completion = completions_ep / max(len(rewards_ep), 1)\n",
    "    reward_history.append(ep_reward)\n",
    "    completion_history.append(ep_completion)\n",
    "    entropy_history.append(entropies_t.mean().item())\n",
    "\n",
    "    if (episode + 1) % 200 == 0:\n",
    "        avg_r = np.mean(reward_history[-200:])\n",
    "        avg_c = np.mean(completion_history[-200:])\n",
    "        avg_e = np.mean(entropy_history[-200:])\n",
    "        print(f\"Ep {episode+1:5d} | Reward: {avg_r:.2f} | Completion: {avg_c:.2%} | Entropy: {avg_e:.2f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate trained model vs baseline\n",
    "def evaluate(env, model_fn, num_episodes=500):\n",
    "    rewards, completions, lengths = [], [], []\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward, comps, steps = 0, 0, 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = model_fn(state, env)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if info[\"completed\"]:\n",
    "                comps += 1\n",
    "            steps += 1\n",
    "        rewards.append(total_reward)\n",
    "        completions.append(comps / max(steps, 1))\n",
    "        lengths.append(steps)\n",
    "    return {\"reward\": np.mean(rewards), \"completion\": np.mean(completions),\n",
    "            \"length\": np.mean(lengths), \"rewards_list\": rewards, \"completions_list\": completions}\n",
    "\n",
    "# Model inference functions\n",
    "def pg_recommend(state, env):\n",
    "    state_t = torch.tensor(state, dtype=torch.float32)\n",
    "    candidates = env.get_candidates(k=20)\n",
    "    cand_embeds = torch.tensor(env.module_embeddings[candidates], dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        probs = actor(state_t, cand_embeds)\n",
    "    return candidates[probs.argmax().item()]\n",
    "\n",
    "def baseline_recommend(state, env):\n",
    "    candidates = env.get_candidates(k=20)\n",
    "    return baseline.recommend(state, candidates, env)\n",
    "\n",
    "pg_results = evaluate(env, pg_recommend)\n",
    "bl_results = evaluate(env, baseline_recommend)\n",
    "\n",
    "print(f\"{'Metric':<25s} {'Baseline':>10s} {'Policy Gradient':>15s} {'Lift':>10s}\")\n",
    "print(\"-\" * 62)\n",
    "for metric in [\"reward\", \"completion\", \"length\"]:\n",
    "    bl_val = bl_results[metric]\n",
    "    pg_val = pg_results[metric]\n",
    "    lift = (pg_val - bl_val) / abs(bl_val) * 100 if bl_val != 0 else 0\n",
    "    fmt = \".2%\" if metric == \"completion\" else \".2f\"\n",
    "    print(f\"{metric:<25s} {bl_val:>10{fmt}} {pg_val:>15{fmt}} {lift:>9.1f}%\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Visualization"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Training reward curve\n",
    "window = 50\n",
    "smoothed_r = np.convolve(reward_history, np.ones(window)/window, mode='valid')\n",
    "axes[0, 0].plot(range(window-1, len(reward_history)), smoothed_r, color='steelblue', linewidth=2)\n",
    "axes[0, 0].axhline(bl_results[\"reward\"], color='red', linestyle='--', label='Baseline')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Session Reward')\n",
    "axes[0, 0].set_title('Training Reward Curve')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Completion rate\n",
    "smoothed_c = np.convolve(completion_history, np.ones(window)/window, mode='valid')\n",
    "axes[0, 1].plot(range(window-1, len(completion_history)), smoothed_c, color='seagreen', linewidth=2)\n",
    "axes[0, 1].axhline(bl_results[\"completion\"], color='red', linestyle='--', label='Baseline')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Completion Rate')\n",
    "axes[0, 1].set_title('Training Completion Rate')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Entropy\n",
    "smoothed_e = np.convolve(entropy_history, np.ones(window)/window, mode='valid')\n",
    "axes[1, 0].plot(range(window-1, len(entropy_history)), smoothed_e, color='coral', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('Policy Entropy')\n",
    "axes[1, 0].set_title('Policy Entropy Over Training')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Comparison bar chart\n",
    "metrics = ['Reward', 'Completion\\nRate', 'Session\\nLength']\n",
    "bl_vals = [bl_results['reward'], bl_results['completion'], bl_results['length']]\n",
    "pg_vals = [pg_results['reward'], pg_results['completion'], pg_results['length']]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "w = 0.35\n",
    "axes[1, 1].bar(x - w/2, bl_vals, w, label='Rule-based Baseline', color='lightcoral')\n",
    "axes[1, 1].bar(x + w/2, pg_vals, w, label='Policy Gradient', color='steelblue')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(metrics)\n",
    "axes[1, 1].set_title('Final Evaluation: Baseline vs Policy Gradient')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. A/B Test Simulation"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate A/B test\n",
    "np.random.seed(42)\n",
    "\n",
    "n_users = 500\n",
    "bl_session_rewards = []\n",
    "pg_session_rewards = []\n",
    "\n",
    "for _ in range(n_users):\n",
    "    # Baseline user\n",
    "    state = env.reset()\n",
    "    total_r = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = baseline_recommend(state, env)\n",
    "        state, r, done, _ = env.step(action)\n",
    "        total_r += r\n",
    "    bl_session_rewards.append(total_r)\n",
    "\n",
    "    # PG user (same initial conditions)\n",
    "    state = env.reset(learner_level=env.learner_level)\n",
    "    total_r = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = pg_recommend(state, env)\n",
    "        state, r, done, _ = env.step(action)\n",
    "        total_r += r\n",
    "    pg_session_rewards.append(total_r)\n",
    "\n",
    "# Statistical test\n",
    "t_stat, p_value = stats.ttest_ind(pg_session_rewards, bl_session_rewards)\n",
    "lift = (np.mean(pg_session_rewards) - np.mean(bl_session_rewards)) / abs(np.mean(bl_session_rewards)) * 100\n",
    "\n",
    "print(f\"A/B Test Results (n={n_users} per group):\")\n",
    "print(f\"  Baseline mean reward:       {np.mean(bl_session_rewards):.2f}\")\n",
    "print(f\"  Policy Gradient mean reward: {np.mean(pg_session_rewards):.2f}\")\n",
    "print(f\"  Lift: {lift:.1f}%\")\n",
    "print(f\"  t-statistic: {t_stat:.3f}\")\n",
    "print(f\"  p-value: {p_value:.6f}\")\n",
    "print(f\"  Significant at alpha=0.05: {'Yes' if p_value < 0.05 else 'No'}\")"
   ],
   "id": "cell_18"
  }
 ]
}