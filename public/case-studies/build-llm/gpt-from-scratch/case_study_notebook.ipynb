{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0",
      "mimetype": "text/x-python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "id": "setup_cell",
      "metadata": {},
      "source": [
        "# Setup: Run this cell first!\n",
        "# Check GPU availability and install dependencies\n",
        "\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "# Check GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"No GPU detected. Some cells may run slowly.\")\n",
        "    print(\"   Go to Runtime -> Change runtime type -> GPU\")\n",
        "\n",
        "print(f\"\\nPython {sys.version.split()[0]}\")\n",
        "print(f\"PyTorch {torch.__version__}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(f\"Random seed set to {SEED}\")\n",
        "\n",
        "%matplotlib inline"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell_1",
      "metadata": {},
      "source": [
        "# Case Study: Autoregressive Contract Clause Generation for Legal Document Drafting\n",
        "## Implementation Notebook\n",
        "\n",
        "---\n",
        "\n",
        "**Scenario:** You are an ML engineer at **Lexis Draft AI**, a legal technology startup building a contract drafting assistant deployed across 18 mid-size law firms. Your current system uses TF-IDF retrieval over a library of 45,000 clauses, achieving a 62% clause relevance score and 38% first-draft acceptance rate.\n",
        "\n",
        "**Your mission:** Build a GPT-style autoregressive transformer that generates novel, contextually appropriate contract clauses conditioned on deal parameters, previously drafted sections, and firm-specific style. Target: **85%+ clause relevance** and **65%+ first-draft acceptance rate**.\n",
        "\n",
        "**Why GPT over retrieval?**\n",
        "1. Retrieval cannot generate novel clauses for unprecedented deal structures (35% of queries).\n",
        "2. Retrieval selects each clause independently, causing inconsistent defined terms across sections.\n",
        "3. Retrieval cannot adapt to firm-specific drafting conventions.\n",
        "4. An autoregressive model generates one token at a time conditioned on the full context -- naturally handling all three failure modes.\n",
        "\n",
        "**Constraints:**\n",
        "- Model must fit in 24 GB GPU memory (A10G) at inference time (~350M parameters in FP16)\n",
        "- Clause generation latency < 5 seconds (100-300 tokens)\n",
        "- All data must stay within each firm's cloud environment (no external APIs)\n",
        "- Training data: ~405,000 contract clauses (~81M tokens)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "chatbot_link",
      "metadata": {},
      "source": [
        "# AI Teaching Assistant\n",
        "\n",
        "Need help with this notebook? Open the **AI Teaching Assistant** -- it has already read this entire notebook and can help with concepts, code, and exercises.\n",
        "\n",
        "**[Open AI Teaching Assistant](https://course-creator-brown.vercel.app/courses/build-llm/practice/2/assistant)**\n",
        "\n",
        "*Tip: Open it in a separate tab and work through this notebook side-by-side.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_2",
      "metadata": {},
      "source": [
        "## 3.1 Data Acquisition and Preprocessing\n",
        "\n",
        "In the Lexis Draft AI scenario, the real training data would consist of 405,000 contract clauses from 18 law firm clients and SEC EDGAR filings. For this notebook, we use a synthetic legal clause dataset that mirrors the structure and patterns of real contract language.\n",
        "\n",
        "The techniques you learn here -- character-level tokenization, sequence chunking, and data preparation for autoregressive training -- transfer directly to production systems. The main difference in production would be using a BPE tokenizer (32,000 tokens) trained on the legal corpus, where terms like \"indemnification\" become single tokens."
      ]
    },
    {
      "cell_type": "code",
      "id": "cell_3",
      "metadata": {},
      "source": [
        "# Install dependencies\n",
        "!pip install matplotlib numpy -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "cell_4",
      "metadata": {},
      "source": [
        "# Synthetic legal clause corpus\n",
        "# These clauses are representative of real contract language patterns\n",
        "# across the clause categories that Lexis Draft AI handles\n",
        "\n",
        "LEGAL_CLAUSES = [\n",
        "    # Limitation of Liability\n",
        "    \"IN NO EVENT SHALL EITHER PARTY BE LIABLE TO THE OTHER PARTY FOR ANY INDIRECT INCIDENTAL SPECIAL CONSEQUENTIAL OR PUNITIVE DAMAGES ARISING OUT OF OR RELATED TO THIS AGREEMENT REGARDLESS OF WHETHER SUCH DAMAGES ARE BASED ON CONTRACT TORT NEGLIGENCE STRICT LIABILITY OR ANY OTHER THEORY EVEN IF SUCH PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES\",\n",
        "    \"THE AGGREGATE LIABILITY OF LICENSOR UNDER THIS AGREEMENT SHALL NOT EXCEED THE TOTAL FEES PAID BY LICENSEE DURING THE TWELVE MONTH PERIOD IMMEDIATELY PRECEDING THE EVENT GIVING RISE TO SUCH LIABILITY\",\n",
        "    \"NOTWITHSTANDING ANYTHING TO THE CONTRARY HEREIN THE LIMITATIONS SET FORTH IN THIS SECTION SHALL NOT APPLY TO A PARTY BREACH OF ITS CONFIDENTIALITY OBLIGATIONS OR A PARTY INDEMNIFICATION OBLIGATIONS UNDER THIS AGREEMENT\",\n",
        "\n",
        "    # Indemnification\n",
        "    \"LICENSEE SHALL INDEMNIFY DEFEND AND HOLD HARMLESS LICENSOR AND ITS OFFICERS DIRECTORS EMPLOYEES AND AGENTS FROM AND AGAINST ANY AND ALL CLAIMS DAMAGES LOSSES LIABILITIES COSTS AND EXPENSES INCLUDING REASONABLE ATTORNEYS FEES ARISING OUT OF OR RELATED TO LICENSEE USE OF THE SOFTWARE IN VIOLATION OF THIS AGREEMENT\",\n",
        "    \"LICENSOR SHALL INDEMNIFY DEFEND AND HOLD HARMLESS LICENSEE FROM AND AGAINST ANY CLAIMS ALLEGING THAT THE SOFTWARE AS PROVIDED BY LICENSOR INFRINGES ANY UNITED STATES PATENT COPYRIGHT OR TRADE SECRET OF A THIRD PARTY\",\n",
        "    \"THE INDEMNIFYING PARTY SHALL HAVE THE RIGHT TO CONTROL THE DEFENSE AND SETTLEMENT OF ANY CLAIM SUBJECT TO THE INDEMNIFIED PARTY CONSENT WHICH SHALL NOT BE UNREASONABLY WITHHELD\",\n",
        "\n",
        "    # Confidentiality\n",
        "    \"EACH PARTY AGREES TO HOLD IN STRICT CONFIDENCE ALL CONFIDENTIAL INFORMATION RECEIVED FROM THE OTHER PARTY AND SHALL NOT DISCLOSE SUCH INFORMATION TO ANY THIRD PARTY WITHOUT THE PRIOR WRITTEN CONSENT OF THE DISCLOSING PARTY\",\n",
        "    \"CONFIDENTIAL INFORMATION SHALL NOT INCLUDE INFORMATION THAT IS OR BECOMES PUBLICLY AVAILABLE THROUGH NO FAULT OF THE RECEIVING PARTY WAS IN THE RECEIVING PARTY POSSESSION PRIOR TO DISCLOSURE OR IS INDEPENDENTLY DEVELOPED BY THE RECEIVING PARTY WITHOUT USE OF THE DISCLOSING PARTY CONFIDENTIAL INFORMATION\",\n",
        "    \"THE OBLIGATIONS OF CONFIDENTIALITY SET FORTH HEREIN SHALL SURVIVE THE TERMINATION OR EXPIRATION OF THIS AGREEMENT FOR A PERIOD OF THREE YEARS\",\n",
        "\n",
        "    # Termination\n",
        "    \"EITHER PARTY MAY TERMINATE THIS AGREEMENT FOR CAUSE UPON THIRTY DAYS PRIOR WRITTEN NOTICE IF THE OTHER PARTY MATERIALLY BREACHES ANY PROVISION OF THIS AGREEMENT AND FAILS TO CURE SUCH BREACH WITHIN THE NOTICE PERIOD\",\n",
        "    \"UPON TERMINATION OF THIS AGREEMENT FOR ANY REASON LICENSEE SHALL IMMEDIATELY CEASE ALL USE OF THE SOFTWARE AND SHALL RETURN OR DESTROY ALL COPIES OF THE SOFTWARE AND CONFIDENTIAL INFORMATION IN ITS POSSESSION\",\n",
        "    \"THE FOLLOWING PROVISIONS SHALL SURVIVE ANY TERMINATION OR EXPIRATION OF THIS AGREEMENT CONFIDENTIALITY LIMITATION OF LIABILITY INDEMNIFICATION AND ANY PROVISIONS WHICH BY THEIR NATURE ARE INTENDED TO SURVIVE\",\n",
        "\n",
        "    # Representations and Warranties\n",
        "    \"LICENSOR REPRESENTS AND WARRANTS THAT IT HAS THE FULL RIGHT POWER AND AUTHORITY TO ENTER INTO THIS AGREEMENT AND TO GRANT THE LICENSES AND RIGHTS GRANTED HEREIN\",\n",
        "    \"LICENSEE REPRESENTS AND WARRANTS THAT IT SHALL USE THE SOFTWARE IN COMPLIANCE WITH ALL APPLICABLE LAWS RULES AND REGULATIONS\",\n",
        "    \"EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT LICENSOR MAKES NO WARRANTIES EXPRESS OR IMPLIED INCLUDING WITHOUT LIMITATION ANY IMPLIED WARRANTIES OF MERCHANTABILITY FITNESS FOR A PARTICULAR PURPOSE OR NONINFRINGEMENT\",\n",
        "\n",
        "    # Governing Law and Dispute Resolution\n",
        "    \"THIS AGREEMENT SHALL BE GOVERNED BY AND CONSTRUED IN ACCORDANCE WITH THE LAWS OF THE STATE OF DELAWARE WITHOUT REGARD TO ITS CONFLICT OF LAWS PRINCIPLES\",\n",
        "    \"ANY DISPUTE ARISING OUT OF OR RELATING TO THIS AGREEMENT SHALL BE RESOLVED BY BINDING ARBITRATION ADMINISTERED BY THE AMERICAN ARBITRATION ASSOCIATION IN ACCORDANCE WITH ITS COMMERCIAL ARBITRATION RULES\",\n",
        "    \"THE PARTIES AGREE THAT ANY LEGAL ACTION OR PROCEEDING ARISING UNDER THIS AGREEMENT SHALL BE BROUGHT EXCLUSIVELY IN THE FEDERAL OR STATE COURTS LOCATED IN WILMINGTON DELAWARE\",\n",
        "\n",
        "    # Assignment\n",
        "    \"NEITHER PARTY MAY ASSIGN OR TRANSFER THIS AGREEMENT OR ANY RIGHTS OR OBLIGATIONS HEREUNDER WITHOUT THE PRIOR WRITTEN CONSENT OF THE OTHER PARTY EXCEPT THAT EITHER PARTY MAY ASSIGN THIS AGREEMENT WITHOUT CONSENT IN CONNECTION WITH A MERGER ACQUISITION OR SALE OF ALL OR SUBSTANTIALLY ALL OF ITS ASSETS\",\n",
        "\n",
        "    # Force Majeure\n",
        "    \"NEITHER PARTY SHALL BE LIABLE FOR ANY FAILURE OR DELAY IN PERFORMING ITS OBLIGATIONS UNDER THIS AGREEMENT TO THE EXTENT SUCH FAILURE OR DELAY RESULTS FROM CIRCUMSTANCES BEYOND THE REASONABLE CONTROL OF SUCH PARTY INCLUDING BUT NOT LIMITED TO ACTS OF GOD NATURAL DISASTERS PANDEMIC GOVERNMENT ACTIONS WAR TERRORISM OR CIVIL UNREST\",\n",
        "\n",
        "    # Intellectual Property\n",
        "    \"ALL INTELLECTUAL PROPERTY RIGHTS IN AND TO THE SOFTWARE INCLUDING ALL MODIFICATIONS ENHANCEMENTS AND DERIVATIVE WORKS SHALL REMAIN THE EXCLUSIVE PROPERTY OF LICENSOR AND NOTHING IN THIS AGREEMENT SHALL BE CONSTRUED AS TRANSFERRING ANY OWNERSHIP RIGHTS TO LICENSEE\",\n",
        "]\n",
        "\n",
        "# Repeat and augment to create a larger corpus\n",
        "corpus_text = \"\\n\\n\".join(LEGAL_CLAUSES * 20)\n",
        "print(f\"Corpus size: {len(corpus_text):,} characters\")\n",
        "print(f\"Number of clauses: {len(LEGAL_CLAUSES) * 20}\")\n",
        "print(f\"Unique clause types: {len(LEGAL_CLAUSES)}\")\n",
        "print(f\"\\nSample clause (first 200 chars):\")\n",
        "print(corpus_text[:200] + \"...\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell_5",
      "metadata": {},
      "source": [
        "### TODO 1: Build a Character-Level Tokenizer and Prepare Training Data\n",
        "\n",
        "In production, Lexis Draft AI uses a BPE tokenizer with 32,000 tokens trained on the legal corpus, where domain-specific terms like \"indemnification\" and \"notwithstanding\" each become a single token. For this notebook, we use character-level tokenization -- it is simpler to implement and lets us focus on the model architecture.\n",
        "\n",
        "The key insight is the same at both levels: we convert text into a sequence of integer IDs, then chop the corpus into fixed-length training sequences."
      ]
    },
    {
      "cell_type": "code",
      "id": "cell_6",
      "metadata": {},
      "source": [
        "def build_tokenizer_and_data(corpus_text, max_seq_len=256):\n",
        "    \"\"\"\n",
        "    Build a character-level tokenizer for the legal corpus and prepare\n",
        "    training sequences.\n",
        "\n",
        "    Args:\n",
        "        corpus_text: string containing the full training corpus\n",
        "        max_seq_len: maximum sequence length for training chunks\n",
        "\n",
        "    Returns:\n",
        "        - encode: function mapping string -> list[int]\n",
        "        - decode: function mapping list[int] -> string\n",
        "        - train_data: tensor of shape (num_sequences, max_seq_len)\n",
        "        - vocab_size: int\n",
        "\n",
        "    Steps:\n",
        "        1. Collect all unique characters in corpus_text, sort them.\n",
        "        2. Create char-to-id and id-to-char mappings (dicts).\n",
        "        3. Define encode(text) that maps each character to its ID.\n",
        "        4. Define decode(ids) that maps each ID back to a character\n",
        "           and joins them into a string.\n",
        "        5. Encode the full corpus into a 1D tensor of token IDs.\n",
        "        6. Truncate to a multiple of max_seq_len, then reshape into\n",
        "           non-overlapping chunks of shape (num_sequences, max_seq_len).\n",
        "        7. Return (encode, decode, train_data, vocab_size).\n",
        "\n",
        "    Hints:\n",
        "        - chars = sorted(set(corpus_text))\n",
        "        - char_to_id = {ch: i for i, ch in enumerate(chars)}\n",
        "        - id_to_char = {i: ch for ch, i in char_to_id.items()}\n",
        "        - encoded = torch.tensor([char_to_id[c] for c in corpus_text])\n",
        "        - total = (len(encoded) // max_seq_len) * max_seq_len\n",
        "        - train_data = encoded[:total].view(-1, max_seq_len)\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "\n",
        "# Build tokenizer and data\n",
        "encode, decode, train_data, vocab_size = build_tokenizer_and_data(corpus_text)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Training sequences: {train_data.shape[0]}\")\n",
        "print(f\"Sequence length: {train_data.shape[1]}\")\n",
        "print(f\"\\nSample decoded sequence:\")\n",
        "print(decode(train_data[0].tolist())[:120] + \"...\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "cell_6b",
      "metadata": {},
      "source": [
        "# Verification: check your tokenizer\n",
        "assert callable(encode), \"encode must be a function\"\n",
        "assert callable(decode), \"decode must be a function\"\n",
        "assert decode(encode(\"HELLO\")) == \"HELLO\", \"Round-trip encode/decode failed\"\n",
        "assert train_data.ndim == 2, f\"train_data should be 2D, got {train_data.ndim}D\"\n",
        "assert train_data.shape[1] == 256, f\"Sequence length should be 256, got {train_data.shape[1]}\"\n",
        "assert vocab_size > 0, \"vocab_size must be positive\"\n",
        "print(f\"All tokenizer checks passed.\")\n",
        "print(f\"Vocabulary: {decode(list(range(min(vocab_size, 30))))}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell_7",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3.2 Model Architecture: Building a GPT from Scratch\n",
        "\n",
        "Now we build the complete GPT model. The architecture has three main components:\n",
        "\n",
        "1. **CausalSelfAttention** -- Multi-head attention with a causal mask so each token can only attend to tokens at or before its position. This is what makes the model autoregressive.\n",
        "\n",
        "2. **TransformerBlock** -- Pre-LayerNorm architecture: `x = x + Attention(LN(x))` then `x = x + FFN(LN(x))`. The residual connections enable deep stacking.\n",
        "\n",
        "3. **GPT** -- Token embeddings + positional embeddings, N Transformer blocks, final LayerNorm, and a linear projection to vocabulary logits.\n",
        "\n",
        "**Mathematical foundation:**\n",
        "\n",
        "The causal self-attention computation is:\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right)V$$\n",
        "\n",
        "where $M_{ij} = 0$ if $i \\geq j$ and $M_{ij} = -\\infty$ if $i < j$. This ensures that generating token $t$ depends only on tokens $1$ through $t-1$.\n",
        "\n",
        "### TODO 2: Implement Causal Self-Attention"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell_8",
      "metadata": {},
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head causal (masked) self-attention.\n",
        "\n",
        "    Each token attends only to tokens at the same or earlier\n",
        "    positions. Future positions are masked with -inf before softmax.\n",
        "\n",
        "    Args:\n",
        "        d_model: model embedding dimension\n",
        "        n_heads: number of attention heads\n",
        "        max_seq_len: maximum sequence length (for causal mask buffer)\n",
        "\n",
        "    Forward:\n",
        "        Input: x of shape (batch, seq_len, d_model)\n",
        "        Output: out of shape (batch, seq_len, d_model)\n",
        "\n",
        "    Implementation steps in __init__:\n",
        "        1. Store n_heads and d_k = d_model // n_heads.\n",
        "        2. Create a single Linear layer: d_model -> 3 * d_model\n",
        "           (this produces Q, K, V in one matrix multiply).\n",
        "        3. Create an output projection: Linear(d_model, d_model).\n",
        "        4. Register a causal mask buffer:\n",
        "           mask = torch.triu(torch.ones(max_seq_len, max_seq_len), diagonal=1).bool()\n",
        "           self.register_buffer('mask', mask)\n",
        "\n",
        "    Implementation steps in forward(x):\n",
        "        1. Get B, T, C from x.shape.\n",
        "        2. Compute qkv = self.qkv(x), shape (B, T, 3*C).\n",
        "        3. Reshape to (B, T, 3, n_heads, d_k) and permute to\n",
        "           (3, B, n_heads, T, d_k). Then split into q, k, v.\n",
        "        4. Compute attention scores: (q @ k.transpose(-2, -1)) / sqrt(d_k).\n",
        "        5. Apply causal mask: att.masked_fill(self.mask[:T, :T], float('-inf')).\n",
        "        6. Apply softmax along the last dimension.\n",
        "        7. Multiply by v: (att @ v), shape (B, n_heads, T, d_k).\n",
        "        8. Transpose and reshape back to (B, T, C).\n",
        "        9. Apply output projection.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, n_heads, max_seq_len=256):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        # YOUR CODE HERE: define self.qkv, self.proj, register mask buffer\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        # YOUR CODE HERE\n",
        "        pass"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "cell_8b",
      "metadata": {},
      "source": [
        "# Verification: test CausalSelfAttention\n",
        "test_attn = CausalSelfAttention(d_model=64, n_heads=4, max_seq_len=32)\n",
        "test_input = torch.randn(2, 16, 64)\n",
        "test_output = test_attn(test_input)\n",
        "assert test_output.shape == (2, 16, 64), f\"Expected (2,16,64), got {test_output.shape}\"\n",
        "\n",
        "# Verify causality: changing a future token should not affect past outputs\n",
        "x1 = torch.randn(1, 8, 64)\n",
        "x2 = x1.clone()\n",
        "x2[0, 5, :] = torch.randn(64)  # change token at position 5\n",
        "out1 = test_attn(x1)\n",
        "out2 = test_attn(x2)\n",
        "# Positions 0-4 should be identical (they cannot see position 5)\n",
        "assert torch.allclose(out1[0, :5], out2[0, :5], atol=1e-5), \"Causal mask broken: future change affected past\"\n",
        "print(\"CausalSelfAttention: all checks passed (shape and causality).\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell_9",
      "metadata": {},
      "source": [
        "### TODO 3: Implement Transformer Block and Full GPT Model\n",
        "\n",
        "The Transformer block uses the **Pre-LN** architecture (as in GPT-2), where LayerNorm is applied *before* the sub-layer rather than after. This improves training stability for deeper models.\n",
        "\n",
        "The feed-forward network (FFN) expands the representation to 4x the model dimension and then compresses it back, using GELU activation. This expansion gives each token position a richer intermediate representation to work with."
      ]
    },
    {
      "cell_type": "code",
      "id": "cell_10",
      "metadata": {},
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Single GPT Transformer block with Pre-LN architecture.\n",
        "\n",
        "    Architecture:\n",
        "        x = x + Attention(LayerNorm(x))\n",
        "        x = x + FFN(LayerNorm(x))\n",
        "\n",
        "    FFN: Linear(d_model, 4*d_model) -> GELU -> Linear(4*d_model, d_model)\n",
        "\n",
        "    __init__ components:\n",
        "        - self.ln1: LayerNorm(d_model)\n",
        "        - self.attn: CausalSelfAttention(d_model, n_heads, max_seq_len)\n",
        "        - self.ln2: LayerNorm(d_model)\n",
        "        - self.ffn: nn.Sequential(\n",
        "              nn.Linear(d_model, 4 * d_model),\n",
        "              nn.GELU(),\n",
        "              nn.Linear(4 * d_model, d_model)\n",
        "          )\n",
        "\n",
        "    forward: apply the Pre-LN residual pattern.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, n_heads, max_seq_len=256):\n",
        "        super().__init__()\n",
        "        # YOUR CODE HERE\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # YOUR CODE HERE\n",
        "        pass\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete GPT language model for autoregressive generation.\n",
        "\n",
        "    Architecture:\n",
        "        1. Token embedding: nn.Embedding(vocab_size, d_model)\n",
        "        2. Positional embedding: nn.Embedding(max_seq_len, d_model)\n",
        "        3. Stack of N TransformerBlocks\n",
        "        4. Final LayerNorm\n",
        "        5. Linear head: d_model -> vocab_size (no bias)\n",
        "\n",
        "    Forward pass:\n",
        "        1. Look up token embeddings for input IDs.\n",
        "        2. Create position indices 0..T-1 and look up positional embeddings.\n",
        "        3. Add token + positional embeddings.\n",
        "        4. Pass through all Transformer blocks sequentially.\n",
        "        5. Apply final LayerNorm.\n",
        "        6. Project to vocabulary logits with the linear head.\n",
        "\n",
        "    Input: idx of shape (batch_size, seq_len) -- integer token IDs\n",
        "    Output: logits of shape (batch_size, seq_len, vocab_size)\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, n_heads, n_layers, max_seq_len=256):\n",
        "        super().__init__()\n",
        "        # YOUR CODE HERE\n",
        "        pass\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape\n",
        "        # YOUR CODE HERE\n",
        "        pass\n",
        "\n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "cell_10b",
      "metadata": {},
      "source": [
        "# Instantiate the model\n",
        "model = GPT(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=128,\n",
        "    n_heads=4,\n",
        "    n_layers=4,\n",
        "    max_seq_len=256,\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model parameters: {model.count_parameters():,}\")\n",
        "print(f\"\\nModel architecture:\")\n",
        "print(model)\n",
        "\n",
        "# Verification: test forward pass\n",
        "test_ids = torch.randint(0, vocab_size, (2, 32)).to(device)\n",
        "test_logits = model(test_ids)\n",
        "assert test_logits.shape == (2, 32, vocab_size), \\\n",
        "    f\"Expected (2, 32, {vocab_size}), got {test_logits.shape}\"\n",
        "print(f\"\\nForward pass check: input {test_ids.shape} -> output {test_logits.shape}\")\n",
        "print(\"All model checks passed.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell_11a",
      "metadata": {},
      "source": [
        "**Thought Questions:**\n",
        "1. Our model has ~800K parameters. The production Lexis Draft model would have ~350M. What architectural changes would you make to scale up? (Hint: increase d_model, n_heads, n_layers, and max_seq_len.)\n",
        "2. Why does the GPT model use a **learned** positional embedding rather than the fixed sinusoidal encoding from the original Transformer paper?\n",
        "3. Why is the output projection head (Linear to vocab_size) set to have no bias? (Hint: think about the interaction with the preceding LayerNorm.)\n",
        "\n",
        "---\n",
        "\n",
        "## 3.3 Training: Next-Token Prediction with Cross-Entropy Loss\n",
        "\n",
        "The training objective is simple but powerful: given a sequence of tokens, predict the next token at every position. The model learns by minimizing cross-entropy loss:\n",
        "\n",
        "$$\\mathcal{L} = -\\frac{1}{N}\\sum_{i=1}^{N} \\log P_\\theta(t_i \\mid t_1, t_2, \\ldots, t_{i-1})$$\n",
        "\n",
        "In the Lexis Draft scenario, we would apply the loss only to clause tokens (not prompt tokens) by masking prompt positions with label -100. For this notebook, we train on the full sequence.\n",
        "\n",
        "### Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell_11b",
      "metadata": {},
      "source": [
        "# Training hyperparameters (scaled for Colab)\n",
        "config = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'd_model': 128,\n",
        "    'n_heads': 4,\n",
        "    'n_layers': 4,\n",
        "    'max_seq_len': 256,\n",
        "    'batch_size': 32,\n",
        "    'learning_rate': 3e-4,\n",
        "    'num_steps': 2000,\n",
        "    'eval_interval': 100,\n",
        "}\n",
        "\n",
        "print(\"Training configuration:\")\n",
        "for k, v in config.items():\n",
        "    print(f\"  {k}: {v}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell_12a",
      "metadata": {},
      "source": [
        "### TODO 4: Implement the Training Loop\n",
        "\n",
        "The training loop follows the standard four-step process:\n",
        "1. **Forward pass:** compute logits from the model\n",
        "2. **Loss computation:** cross-entropy between predicted next-token distribution and actual next token\n",
        "3. **Backward pass:** compute gradients via backpropagation\n",
        "4. **Weight update:** adjust parameters with the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell_12",
      "metadata": {},
      "source": [
        "def train_model(model, train_data, num_steps=2000, batch_size=32,\n",
        "                learning_rate=3e-4, eval_interval=100, device='cpu'):\n",
        "    \"\"\"\n",
        "    Train the GPT model on next-token prediction.\n",
        "\n",
        "    At each training step:\n",
        "        1. Sample a random batch of sequences from train_data.\n",
        "        2. Create inputs x = batch[:, :-1]  (all tokens except the last).\n",
        "           Create targets y = batch[:, 1:]   (all tokens except the first).\n",
        "        3. Forward pass: logits = model(x), shape (B, T-1, vocab_size).\n",
        "        4. Flatten logits to (B*(T-1), vocab_size) and targets to (B*(T-1),)\n",
        "           then compute F.cross_entropy(logits_flat, targets_flat).\n",
        "        5. optimizer.zero_grad(), loss.backward(), optimizer.step().\n",
        "        6. Every eval_interval steps, print and record the loss.\n",
        "\n",
        "    Args:\n",
        "        model: GPT model instance\n",
        "        train_data: tensor (num_sequences, seq_len)\n",
        "        num_steps: total training iterations\n",
        "        batch_size: batch size\n",
        "        learning_rate: AdamW learning rate\n",
        "        eval_interval: steps between loss logging\n",
        "        device: torch device\n",
        "\n",
        "    Returns:\n",
        "        losses: list of (step, loss_value) tuples\n",
        "\n",
        "    Hints:\n",
        "        - Use torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "        - Batch indices: torch.randint(0, len(train_data), (batch_size,))\n",
        "        - Move batch to device: batch = train_data[idx].to(device)\n",
        "        - vocab_size can be read from logits.shape[-1]\n",
        "    \"\"\"\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    losses = []\n",
        "\n",
        "    # YOUR CODE HERE: implement the training loop\n",
        "\n",
        "    return losses\n",
        "\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "start_time = time.time()\n",
        "train_losses = train_model(\n",
        "    model, train_data,\n",
        "    num_steps=2000,\n",
        "    batch_size=32,\n",
        "    learning_rate=3e-4,\n",
        "    eval_interval=100,\n",
        "    device=device\n",
        ")\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\nTraining complete in {elapsed:.1f} seconds\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "cell_12c",
      "metadata": {},
      "source": [
        "# Plot training loss\n",
        "steps, losses = zip(*train_losses)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(steps, losses, alpha=0.3, color='blue')\n",
        "\n",
        "# Smoothed curve\n",
        "window = 5\n",
        "smoothed = [np.mean(losses[max(0, i - window):i + 1]) for i in range(len(losses))]\n",
        "plt.plot(steps, smoothed, color='blue', linewidth=2, label='Smoothed loss')\n",
        "\n",
        "# Random baseline: loss if the model predicted uniformly at random\n",
        "random_baseline = np.log(vocab_size)\n",
        "plt.axhline(y=random_baseline, color='red', linestyle='--',\n",
        "            label=f'Random baseline (ln({vocab_size}) = {random_baseline:.2f})')\n",
        "\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Cross-Entropy Loss')\n",
        "plt.title('GPT Training Loss on Legal Contract Clauses')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Initial loss: {losses[0]:.4f}\")\n",
        "print(f\"Final loss: {losses[-1]:.4f}\")\n",
        "print(f\"Random baseline: {random_baseline:.4f}\")\n",
        "print(f\"Loss reduction: {((losses[0] - losses[-1]) / losses[0]) * 100:.1f}%\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell_13a",
      "metadata": {},
      "source": [
        "**Thought Questions:**\n",
        "1. The random baseline loss is $\\ln(V)$ where $V$ is the vocabulary size. Why is this the expected loss for a model that predicts uniformly at random?\n",
        "2. If the final loss is significantly below the random baseline but still above zero, what does the remaining loss represent? (Hint: think about the inherent uncertainty in natural language.)\n",
        "3. In the Lexis Draft production system, we apply label smoothing ($\\epsilon = 0.1$). How would this change the training loss curve?\n",
        "\n",
        "---\n",
        "\n",
        "## 3.4 Generation: Autoregressive Sampling\n",
        "\n",
        "Generation works by repeatedly:\n",
        "1. Feeding the current sequence through the model\n",
        "2. Taking the logits at the last position\n",
        "3. Sampling from the resulting distribution\n",
        "4. Appending the sampled token\n",
        "\n",
        "The **temperature** parameter controls randomness: lower temperature makes the model more deterministic (sharper distribution), higher temperature makes it more random. **Top-k** filtering restricts sampling to the k most likely tokens, preventing the model from generating very unlikely characters.\n",
        "\n",
        "In the Lexis Draft scenario, generation continues until an `[END_CLAUSE]` token is produced or 500 tokens are reached.\n",
        "\n",
        "### TODO 5: Implement Autoregressive Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell_14",
      "metadata": {},
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, encode, decode, prompt, max_new_tokens=200,\n",
        "             temperature=0.8, top_k=40, device='cpu'):\n",
        "    \"\"\"\n",
        "    Generate text autoregressively from a prompt.\n",
        "\n",
        "    Algorithm:\n",
        "        1. Encode the prompt string to token IDs.\n",
        "        2. Create idx tensor of shape (1, len(prompt_ids)) on device.\n",
        "        3. For each new token to generate:\n",
        "           a. If idx is longer than max_seq_len, crop to the last\n",
        "              max_seq_len tokens.\n",
        "           b. Forward pass: logits = model(idx_cropped).\n",
        "           c. Take logits at the last position: logits[:, -1, :].\n",
        "           d. Divide by temperature.\n",
        "           e. If top_k > 0:\n",
        "              - Find the top-k logit values.\n",
        "              - Set all logits below the k-th largest to -inf.\n",
        "           f. Apply softmax to get probabilities.\n",
        "           g. Sample one token: torch.multinomial(probs, num_samples=1).\n",
        "           h. Append to idx: idx = torch.cat([idx, next_token], dim=1).\n",
        "        4. Decode the full sequence and return the string.\n",
        "\n",
        "    Args:\n",
        "        model: trained GPT model\n",
        "        encode: tokenizer encode function\n",
        "        decode: tokenizer decode function\n",
        "        prompt: string to condition generation on\n",
        "        max_new_tokens: maximum number of tokens to generate\n",
        "        temperature: sampling temperature (default 0.8)\n",
        "        top_k: top-k filtering (default 40; 0 = disabled)\n",
        "        device: torch device\n",
        "\n",
        "    Returns:\n",
        "        generated_text: string (prompt + generated continuation)\n",
        "\n",
        "    Hints:\n",
        "        - model.eval() before generation\n",
        "        - For top-k: v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "          logits[logits < v[:, [-1]]] = float('-inf')\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "\n",
        "# Generate sample contract clauses\n",
        "prompts = [\n",
        "    \"IN NO EVENT SHALL\",\n",
        "    \"LICENSEE SHALL INDEMNIFY\",\n",
        "    \"THIS AGREEMENT SHALL BE\",\n",
        "    \"NEITHER PARTY MAY\",\n",
        "    \"EACH PARTY AGREES\",\n",
        "]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"  GENERATED CONTRACT CLAUSES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for prompt in prompts:\n",
        "    generated = generate(model, encode, decode, prompt,\n",
        "                         max_new_tokens=200, temperature=0.7,\n",
        "                         top_k=30, device=device)\n",
        "    print(f\"\\nPrompt: '{prompt}'\")\n",
        "    print(f\"Generated:\\n{generated}\")\n",
        "    print(\"-\" * 70)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell_15a",
      "metadata": {},
      "source": [
        "**Thought Questions:**\n",
        "1. Try temperature=0.3 and temperature=1.5. How does the output change? Which would be more appropriate for legal document generation, and why?\n",
        "2. Why is top-k filtering important for generation quality? What happens without it?\n",
        "3. In the Lexis Draft production system, generation is conditioned on `[CONTRACT_TYPE]`, `[GOVERNING_LAW]`, `[FIRM_STYLE]`, and `[CONTEXT]` tokens. How would you modify the generate function to handle this structured prompt?\n",
        "\n",
        "---\n",
        "\n",
        "## 3.5 Evaluation and Analysis\n",
        "\n",
        "We evaluate our model on three axes:\n",
        "1. **Perplexity** -- a standard language modeling metric: how surprised is the model by held-out text?\n",
        "2. **Generation latency** -- can we generate a clause within the 5-second budget?\n",
        "3. **Attention patterns** -- what does the model learn to attend to?\n",
        "\n",
        "### TODO 6: Compute Perplexity, Latency, and Analyze Attention"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell_16",
      "metadata": {},
      "source": [
        "def evaluate_model(model, train_data, encode, decode, vocab_size,\n",
        "                   device='cpu', num_eval_batches=20, batch_size=32):\n",
        "    \"\"\"\n",
        "    Evaluate the trained GPT model.\n",
        "\n",
        "    Compute:\n",
        "        1. Perplexity on held-out data:\n",
        "           - Use the last 10% of train_data as eval data.\n",
        "           - For each batch: x = batch[:, :-1], y = batch[:, 1:].\n",
        "           - Compute cross-entropy loss and average across batches.\n",
        "           - Perplexity = exp(mean_loss).\n",
        "\n",
        "        2. Generation latency:\n",
        "           - Generate 200 tokens from a prompt.\n",
        "           - Measure wall-clock time.\n",
        "           - Compute tokens_per_second.\n",
        "\n",
        "        3. Next-character accuracy:\n",
        "           - For evaluation batches, compute the fraction of positions\n",
        "             where argmax(logits) == target.\n",
        "\n",
        "    Args:\n",
        "        model: trained GPT model\n",
        "        train_data: full training data tensor\n",
        "        encode, decode: tokenizer functions\n",
        "        vocab_size: vocabulary size\n",
        "        device: torch device\n",
        "        num_eval_batches: number of batches for evaluation\n",
        "        batch_size: batch size\n",
        "\n",
        "    Returns:\n",
        "        dict with 'perplexity', 'tokens_per_second', 'accuracy'\n",
        "\n",
        "    Hints:\n",
        "        - split_idx = int(0.9 * len(train_data))\n",
        "        - eval_data = train_data[split_idx:]\n",
        "        - Perplexity = torch.exp(torch.tensor(mean_loss)).item()\n",
        "        - Use time.time() for latency measurement\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "\n",
        "results = evaluate_model(model, train_data, encode, decode,\n",
        "                         vocab_size, device=device)\n",
        "print(f\"\\nEvaluation Results:\")\n",
        "print(f\"  Perplexity: {results['perplexity']:.2f}\")\n",
        "print(f\"  Tokens/second: {results['tokens_per_second']:.0f}\")\n",
        "print(f\"  Next-char accuracy: {results['accuracy']:.1%}\")\n",
        "\n",
        "# Context: Lexis Draft production targets\n",
        "print(f\"\\nProduction targets (for reference):\")\n",
        "print(f\"  Target perplexity: < 12.0 (ours is character-level, so not directly comparable)\")\n",
        "print(f\"  Target latency: < 5 sec for 100-300 tokens\")\n",
        "tokens_for_clause = 200  # typical clause length\n",
        "estimated_clause_time = tokens_for_clause / results['tokens_per_second']\n",
        "print(f\"  Estimated clause generation time: {estimated_clause_time:.2f} sec for {tokens_for_clause} tokens\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "cell_17",
      "metadata": {},
      "source": [
        "# Attention weight visualization\n",
        "def visualize_attention(model, encode, text, layer_idx=-1, head_idx=0,\n",
        "                        device='cpu'):\n",
        "    \"\"\"\n",
        "    Visualize attention weights for a given input text.\n",
        "\n",
        "    Uses a forward hook to capture Q and K from the specified attention\n",
        "    layer, computes the attention matrix, and plots it as a heatmap.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    attention_weights = []\n",
        "\n",
        "    def hook_fn(module, input, output):\n",
        "        x = input[0]\n",
        "        B, T, C = x.shape\n",
        "        qkv = module.qkv(x).reshape(B, T, 3, module.n_heads, module.d_k)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "        q, k = qkv[0], qkv[1]\n",
        "        att = (q @ k.transpose(-2, -1)) / (module.d_k ** 0.5)\n",
        "        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
        "        att = att.masked_fill(mask, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        attention_weights.append(att.detach().cpu())\n",
        "\n",
        "    # Register hook on the target layer's attention module\n",
        "    target_layer = list(model.blocks)[layer_idx]\n",
        "    hook = target_layer.attn.register_forward_hook(hook_fn)\n",
        "\n",
        "    ids = torch.tensor([encode(text)], device=device)\n",
        "    with torch.no_grad():\n",
        "        model(ids)\n",
        "\n",
        "    hook.remove()\n",
        "\n",
        "    if attention_weights:\n",
        "        attn = attention_weights[0][0, head_idx].numpy()\n",
        "        chars = list(text[:ids.shape[1]])\n",
        "        n = len(chars)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "        im = ax.imshow(attn[:n, :n], cmap='Blues')\n",
        "        ax.set_xticks(range(n))\n",
        "        ax.set_yticks(range(n))\n",
        "        ax.set_xticklabels(chars, rotation=90, fontfamily='monospace', fontsize=7)\n",
        "        ax.set_yticklabels(chars, fontfamily='monospace', fontsize=7)\n",
        "        ax.set_xlabel('Key (attending to)')\n",
        "        ax.set_ylabel('Query (attending from)')\n",
        "        ax.set_title(f'Attention Weights (Layer {layer_idx}, Head {head_idx})')\n",
        "        plt.colorbar(im)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# Visualize attention on a sample clause beginning\n",
        "sample_text = \"IN NO EVENT SHALL EITHER PARTY BE\"\n",
        "visualize_attention(model, encode, sample_text, layer_idx=-1, head_idx=0,\n",
        "                    device=device)\n",
        "\n",
        "# Try a different head to see different attention patterns\n",
        "visualize_attention(model, encode, sample_text, layer_idx=-1, head_idx=1,\n",
        "                    device=device)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell_18a",
      "metadata": {},
      "source": [
        "**Thought Questions:**\n",
        "1. What patterns do you see in the attention heatmap? Do different heads attend to different things?\n",
        "2. In legal text, certain words like \"SHALL\" and \"NOT\" fundamentally change the meaning of a clause. Do you see these words receiving high attention?\n",
        "3. The causal mask creates the triangular pattern. Why is this critical for autoregressive generation but not needed for models like BERT?\n",
        "\n",
        "---\n",
        "\n",
        "## 3.6 Temperature and Sampling Strategy Analysis\n",
        "\n",
        "For legal document generation, the choice of sampling strategy directly affects output quality. Too deterministic and the model produces repetitive text. Too random and it generates nonsensical legal language.\n",
        "\n",
        "### TODO 7: Compare Sampling Strategies"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell_18",
      "metadata": {},
      "source": [
        "def compare_sampling_strategies(model, encode, decode, prompt,\n",
        "                                 device='cpu'):\n",
        "    \"\"\"\n",
        "    Compare different sampling strategies for legal text generation.\n",
        "\n",
        "    Generate text from the same prompt using:\n",
        "        1. Greedy decoding (temperature=0.01, top_k=1)\n",
        "        2. Low temperature (temperature=0.3, top_k=0)\n",
        "        3. Medium temperature (temperature=0.7, top_k=30)\n",
        "        4. High temperature (temperature=1.2, top_k=0)\n",
        "        5. Top-k only (temperature=1.0, top_k=10)\n",
        "\n",
        "    For each strategy, generate 200 tokens and report:\n",
        "        - The generated text\n",
        "        - Unique character ratio (num_unique_chars / total_chars)\n",
        "        - Repetition score: fraction of 4-grams that appear more than once\n",
        "\n",
        "    Then create a bar chart comparing the repetition scores.\n",
        "\n",
        "    Args:\n",
        "        model: trained GPT model\n",
        "        encode, decode: tokenizer functions\n",
        "        prompt: string prompt\n",
        "        device: torch device\n",
        "\n",
        "    Hints:\n",
        "        - To compute repetition: extract all 4-character substrings,\n",
        "          count occurrences, and compute fraction with count > 1\n",
        "        - Legal drafting benefits from moderate temperature (0.5-0.8)\n",
        "          with top-k filtering\n",
        "    \"\"\"\n",
        "    strategies = [\n",
        "        ('Greedy', 0.01, 1),\n",
        "        ('Low temp (0.3)', 0.3, 0),\n",
        "        ('Medium temp (0.7) + top-k', 0.7, 30),\n",
        "        ('High temp (1.2)', 1.2, 0),\n",
        "        ('Top-k only (k=10)', 1.0, 10),\n",
        "    ]\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "\n",
        "compare_sampling_strategies(model, encode, decode,\n",
        "                             \"LICENSEE SHALL INDEMNIFY\",\n",
        "                             device=device)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell_19a",
      "metadata": {},
      "source": [
        "**Thought Questions:**\n",
        "1. Which strategy produces the most realistic legal language? Why?\n",
        "2. Greedy decoding often produces repetitive text (\"the the the...\"). Why does this happen in autoregressive models?\n",
        "3. For Lexis Draft's production system, what sampling strategy would you recommend and why? Consider that attorneys need both quality and diversity.\n",
        "\n",
        "---\n",
        "\n",
        "## 3.7 Scaling Analysis: From Notebook to Production\n",
        "\n",
        "Our notebook model has ~800K parameters. The production model for Lexis Draft AI would have ~350M parameters. Let us analyze how the key metrics scale.\n",
        "\n",
        "### TODO 8: Parameter Count and Memory Analysis"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell_19",
      "metadata": {},
      "source": [
        "def scaling_analysis(vocab_size):\n",
        "    \"\"\"\n",
        "    Analyze how model size, memory, and compute scale with architecture choices.\n",
        "\n",
        "    Compute and plot the following:\n",
        "\n",
        "    1. Parameter count for different configurations:\n",
        "       - Notebook:  d=128,  heads=4,  layers=4,  vocab=vocab_size\n",
        "       - Small:     d=512,  heads=8,  layers=8,  vocab=32000\n",
        "       - Medium:    d=768,  heads=12, layers=12, vocab=32000\n",
        "       - Production:d=1024, heads=16, layers=24, vocab=32000\n",
        "       - Large:     d=1600, heads=25, layers=48, vocab=32000\n",
        "\n",
        "       Parameter count formula:\n",
        "       - Embeddings: vocab * d + max_seq * d\n",
        "       - Per block: 4*d*d (QKV+proj) + 8*d*d (FFN) + 4*d (layernorms)\n",
        "       - Head: d * vocab\n",
        "       - Total: embeddings + layers * per_block + d (final LN) + head\n",
        "\n",
        "    2. FP16 memory for model weights (params * 2 bytes).\n",
        "\n",
        "    3. KV-cache memory for a single sequence:\n",
        "       - Per layer: 2 * seq_len * d * 2 bytes (FP16)\n",
        "       - Total: layers * per_layer\n",
        "\n",
        "    4. Create a table and bar chart of the results.\n",
        "\n",
        "    Args:\n",
        "        vocab_size: notebook model vocab size\n",
        "\n",
        "    Hints:\n",
        "        - Use 2048 as max_seq_len for all non-notebook configs\n",
        "        - The production config should have ~350M parameters\n",
        "        - Plot with log scale on y-axis for parameter counts\n",
        "    \"\"\"\n",
        "    configs = [\n",
        "        ('Notebook',   128,   4,  4,  vocab_size, 256),\n",
        "        ('Small',      512,   8,  8,  32000, 2048),\n",
        "        ('Medium',     768,  12, 12,  32000, 2048),\n",
        "        ('Production', 1024, 16, 24,  32000, 2048),\n",
        "        ('Large',      1600, 25, 48,  32000, 2048),\n",
        "    ]\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "\n",
        "scaling_analysis(vocab_size)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell_20a",
      "metadata": {},
      "source": [
        "**Thought Questions:**\n",
        "1. Does the production model (~350M params) fit within the 24 GB A10G GPU constraint? How much room is left for the KV-cache?\n",
        "2. The KV-cache grows linearly with sequence length. At what sequence length would the KV-cache exceed the remaining GPU memory?\n",
        "3. Lexis Draft uses LoRA adapters for firm-specific style. If each adapter adds 2-4M parameters, how does this compare to storing 18 separate full models?\n",
        "\n",
        "---\n",
        "\n",
        "## 3.8 Production Considerations\n",
        "\n",
        "### Output Validation Pipeline\n",
        "\n",
        "In the Lexis Draft production system, every generated clause passes through a validation pipeline before being shown to the attorney. Let us implement a simplified version."
      ]
    },
    {
      "cell_type": "code",
      "id": "cell_20",
      "metadata": {},
      "source": [
        "def validate_clause(generated_text, context_terms=None):\n",
        "    \"\"\"\n",
        "    Validate a generated contract clause.\n",
        "\n",
        "    Checks:\n",
        "    1. Defined term consistency: all capitalized multi-word terms\n",
        "       (e.g., LICENSOR, LICENSEE, AGREEMENT) should be in the\n",
        "       context_terms set.\n",
        "    2. Prohibited language: flag any deprecated legal terms.\n",
        "    3. Length check: clause should be between 50 and 500 tokens.\n",
        "    4. Structural check: clause should not end mid-sentence.\n",
        "\n",
        "    Returns:\n",
        "        dict with 'passed', 'warnings', 'errors'\n",
        "    \"\"\"\n",
        "    if context_terms is None:\n",
        "        context_terms = {\n",
        "            'LICENSOR', 'LICENSEE', 'AGREEMENT', 'SOFTWARE',\n",
        "            'PARTY', 'PARTIES', 'SECTION'\n",
        "        }\n",
        "\n",
        "    warnings = []\n",
        "    errors = []\n",
        "\n",
        "    # 1. Check defined terms\n",
        "    words = generated_text.split()\n",
        "    for word in words:\n",
        "        clean = word.strip('.,;:()')\n",
        "        if clean.isupper() and len(clean) > 2 and clean not in context_terms:\n",
        "            # Check if it's a common legal term we expect\n",
        "            common_terms = {'THE', 'AND', 'FOR', 'ANY', 'ALL', 'SHALL',\n",
        "                          'NOT', 'ITS', 'BUT', 'NOR', 'YET', 'HAS',\n",
        "                          'MAY', 'SUCH', 'FROM', 'WITH', 'THAT',\n",
        "                          'THIS', 'UNDER', 'UPON', 'INTO', 'THAN'}\n",
        "            if clean not in common_terms:\n",
        "                warnings.append(f\"Undefined term: {clean}\")\n",
        "\n",
        "    # 2. Prohibited language\n",
        "    prohibited = ['HERETOFORE', 'WITNESSETH', 'WHEREAS']\n",
        "    for term in prohibited:\n",
        "        if term in generated_text:\n",
        "            warnings.append(f\"Deprecated term: {term}\")\n",
        "\n",
        "    # 3. Length check\n",
        "    if len(generated_text) < 50:\n",
        "        errors.append(\"Clause too short (< 50 characters)\")\n",
        "    if len(generated_text) > 2000:\n",
        "        warnings.append(\"Clause unusually long (> 2000 characters)\")\n",
        "\n",
        "    # 4. Structural check\n",
        "    stripped = generated_text.strip()\n",
        "    if stripped and stripped[-1] not in '.;':\n",
        "        warnings.append(\"Clause does not end with proper punctuation\")\n",
        "\n",
        "    passed = len(errors) == 0\n",
        "    return {'passed': passed, 'warnings': warnings, 'errors': errors}\n",
        "\n",
        "\n",
        "# Validate our generated clauses\n",
        "print(\"Clause Validation Results:\")\n",
        "print(\"=\" * 50)\n",
        "for prompt in prompts:\n",
        "    generated = generate(model, encode, decode, prompt,\n",
        "                         max_new_tokens=200, temperature=0.7,\n",
        "                         top_k=30, device=device)\n",
        "    result = validate_clause(generated)\n",
        "    status = \"PASS\" if result['passed'] else \"FAIL\"\n",
        "    print(f\"\\n[{status}] Prompt: '{prompt}'\")\n",
        "    if result['errors']:\n",
        "        for e in result['errors']:\n",
        "            print(f\"  ERROR: {e}\")\n",
        "    if result['warnings']:\n",
        "        for w in result['warnings'][:3]:  # Show first 3 warnings\n",
        "            print(f\"  WARNING: {w}\")\n",
        "    if not result['errors'] and not result['warnings']:\n",
        "        print(f\"  No issues found.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell_21a",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "In this notebook, you built a complete GPT-style autoregressive language model for legal contract clause generation:"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell_21",
      "metadata": {},
      "source": [
        "# Final summary\n",
        "print(\"=\" * 70)\n",
        "print(\"  CASE STUDY SUMMARY: GPT from Scratch\")\n",
        "print(\"  Autoregressive Contract Clause Generation for Lexis Draft AI\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "print(\"Model Architecture:\")\n",
        "print(f\"  Type: Decoder-only Transformer (GPT)\")\n",
        "print(f\"  Parameters: {model.count_parameters():,}\")\n",
        "print(f\"  Layers: 4, Heads: 4, d_model: 128\")\n",
        "print(f\"  Tokenization: Character-level ({vocab_size} tokens)\")\n",
        "print(f\"  Max sequence length: 256\")\n",
        "print()\n",
        "print(\"Training:\")\n",
        "print(f\"  Data: {len(train_data)} sequences of legal contract clauses\")\n",
        "print(f\"  Objective: Next-token prediction (cross-entropy loss)\")\n",
        "print(f\"  Optimizer: AdamW (lr=3e-4)\")\n",
        "print(f\"  Steps: 2,000\")\n",
        "print()\n",
        "print(\"Key Findings:\")\n",
        "print(f\"  1. Training loss converged well below random baseline\")\n",
        "print(f\"     (random baseline = ln({vocab_size}) = {np.log(vocab_size):.2f})\")\n",
        "print(f\"  2. Model generates coherent legal language following\")\n",
        "print(f\"     contract drafting conventions\")\n",
        "print(f\"  3. Causal attention ensures each token conditions on all\")\n",
        "print(f\"     previous context, maintaining clause consistency\")\n",
        "print(f\"  4. Temperature and top-k filtering control the quality/\")\n",
        "print(f\"     diversity tradeoff for legal text\")\n",
        "print()\n",
        "print(\"Production Scaling (Lexis Draft AI):\")\n",
        "print(\"  - Scale to ~350M parameters (d=1024, heads=16, layers=24)\")\n",
        "print(\"  - Use BPE tokenizer (32K vocab) trained on legal corpus\")\n",
        "print(\"  - Add LoRA adapters for per-firm style adaptation\")\n",
        "print(\"  - Implement KV-cache for O(T) generation (vs O(T^2))\")\n",
        "print(\"  - Deploy with vLLM for continuous batching\")\n",
        "print(\"  - Output validation: defined terms, cross-references,\")\n",
        "print(\"     prohibited language, confidence thresholds\")\n",
        "print()\n",
        "print(\"Business Impact (Target):\")\n",
        "print(\"  - Clause relevance: 62% -> 85%\")\n",
        "print(\"  - First-draft acceptance: 38% -> 65%\")\n",
        "print(\"  - Attorney editing time: 3.2 hrs -> 1.5 hrs per contract\")\n",
        "print(\"  - Annual savings: ~$18.4M across 18 client firms\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell_22",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "For a deeper understanding of the production system design, read **Section 4** of the case study document, which covers:\n",
        "\n",
        "1. **Multi-firm model architecture** -- base model + LoRA adapters for firm-specific style\n",
        "2. **KV-cache** -- how to make generation efficient enough for interactive use\n",
        "3. **Data pipeline** -- attorney feedback collection and continuous learning\n",
        "4. **Monitoring and guardrails** -- defined term checks, cross-reference validation, confidence thresholds\n",
        "5. **Ethical considerations** -- attorney-client privilege, bias, and the boundary between drafting assistance and legal advice"
      ]
    }
  ]
}
