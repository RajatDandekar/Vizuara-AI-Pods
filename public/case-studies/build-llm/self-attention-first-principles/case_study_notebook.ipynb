{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_cell"
      },
      "outputs": [],
      "source": [
        "# Setup: Run this cell first!\n",
        "# Check GPU availability and install dependencies\n",
        "\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "# Check GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"No GPU detected. Some cells may run slowly.\")\n",
        "    print(\"   Go to Runtime -> Change runtime type -> GPU\")\n",
        "\n",
        "print(f\"\\nPython {sys.version.split()[0]}\")\n",
        "print(f\"PyTorch {torch.__version__}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(f\"Random seed set to {SEED}\")\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_intro"
      },
      "source": [
        "# Case Study: Automated Clinical Note Classification with Self-Attention\n",
        "## Implementation Notebook\n",
        "\n",
        "---\n",
        "\n",
        "**Scenario:** You are an ML engineer at MedScribe Analytics, a health-tech company providing automated clinical documentation intelligence to mid-size hospital networks. Your current Bi-LSTM pipeline achieves 78% top-1 accuracy on ICD-10 code prediction, but suffers from long-range dependency loss, negation mishandling, and multi-label confusion. You have been tasked with migrating to a **self-attention-based model** to resolve these failure modes.\n",
        "\n",
        "**Current system:** Bi-LSTM with 78% top-1 accuracy, 0.72 negation F1, 0.65 multi-label recall.\n",
        "\n",
        "**Target:** 88%+ top-1 accuracy, 0.90+ negation F1, 0.82+ multi-label recall, <60ms inference latency.\n",
        "\n",
        "**Why Self-Attention:** The sequential bottleneck of the LSTM is the root cause of all three failure modes. Self-attention provides O(1) path length between any two tokens, enables dedicated heads for negation detection, and preserves multi-label information through concatenated multi-head outputs.\n",
        "\n",
        "---\n",
        "\n",
        "In this notebook, we build the complete self-attention-based clinical note classifier from scratch. We implement every component -- sinusoidal positional encoding, scaled dot-product attention, multi-head attention, transformer encoder blocks, and the multi-label classification head -- training on synthetic clinical notes and analyzing what the attention heads learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_setup_header"
      },
      "source": [
        "## 1. Environment Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_imports"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import time\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import f1_score, precision_recall_curve, classification_report\n",
        "from collections import Counter\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_data_header"
      },
      "source": [
        "## 2. Synthetic Clinical Notes Dataset\n",
        "\n",
        "We create a synthetic dataset that mimics the statistical properties of real ICD-10 coding data. Each clinical note is a sequence of tokens drawn from medically-relevant vocabulary, and each note is associated with 1-3 diagnostic codes from a simplified label space of 4 categories (Cardiac, Respiratory, Renal, Diabetes).\n",
        "\n",
        "The dataset includes **negation patterns** (e.g., \"patient denies chest pain\") where a diagnosis term is present in the text but should NOT be assigned as a label. This directly tests whether self-attention can learn negation-diagnosis relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_vocab"
      },
      "outputs": [],
      "source": [
        "# Medical vocabulary with semantic groupings\n",
        "VOCAB = ['<PAD>', '<UNK>', '<CLS>']\n",
        "\n",
        "# Symptom/diagnosis words grouped by category\n",
        "CARDIAC_TERMS = ['chest', 'pain', 'cardiac', 'heart', 'failure', 'arrhythmia',\n",
        "                 'palpitations', 'hypertension', 'blood', 'pressure', 'elevated',\n",
        "                 'ecg', 'abnormal', 'murmur', 'edema', 'dyspnea']\n",
        "RESPIRATORY_TERMS = ['cough', 'breath', 'shortness', 'pneumonia', 'lung',\n",
        "                     'respiratory', 'wheezing', 'sputum', 'oxygen', 'saturation',\n",
        "                     'bronchitis', 'pleurisy', 'crackles', 'consolidation']\n",
        "RENAL_TERMS = ['kidney', 'renal', 'creatinine', 'dialysis', 'urine',\n",
        "               'proteinuria', 'gfr', 'nephropathy', 'bladder', 'urinary']\n",
        "DIABETES_TERMS = ['diabetes', 'glucose', 'insulin', 'hba1c', 'hyperglycemia',\n",
        "                  'diabetic', 'neuropathy', 'retinopathy', 'metformin', 'sugar']\n",
        "NEGATION_TERMS = ['no', 'not', 'denies', 'absent', 'negative', 'without',\n",
        "                  'ruled', 'out', 'unlikely', 'excluded']\n",
        "GENERAL_TERMS = ['patient', 'presents', 'with', 'history', 'of', 'the',\n",
        "                 'and', 'is', 'was', 'has', 'reports', 'examination',\n",
        "                 'shows', 'diagnosis', 'treatment', 'assessment', 'plan',\n",
        "                 'follow', 'up', 'prescribed', 'admitted', 'discharged',\n",
        "                 'stable', 'condition', 'chronic', 'acute', 'mild',\n",
        "                 'moderate', 'severe', 'bilateral', 'left', 'right']\n",
        "\n",
        "VOCAB.extend(CARDIAC_TERMS + RESPIRATORY_TERMS + RENAL_TERMS +\n",
        "             DIABETES_TERMS + NEGATION_TERMS + GENERAL_TERMS)\n",
        "VOCAB = list(dict.fromkeys(VOCAB))  # Remove duplicates preserving order\n",
        "word2idx = {w: i for i, w in enumerate(VOCAB)}\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "vocab_size = len(VOCAB)\n",
        "\n",
        "# Diagnostic code labels\n",
        "CODE_NAMES = ['Cardiac', 'Respiratory', 'Renal', 'Diabetes']\n",
        "num_classes = len(CODE_NAMES)\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Number of diagnostic codes: {num_classes}\")\n",
        "print(f\"\\nVocabulary categories:\")\n",
        "print(f\"  Cardiac terms:     {len(CARDIAC_TERMS)}\")\n",
        "print(f\"  Respiratory terms: {len(RESPIRATORY_TERMS)}\")\n",
        "print(f\"  Renal terms:       {len(RENAL_TERMS)}\")\n",
        "print(f\"  Diabetes terms:    {len(DIABETES_TERMS)}\")\n",
        "print(f\"  Negation terms:    {len(NEGATION_TERMS)}\")\n",
        "print(f\"  General terms:     {len(GENERAL_TERMS)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_todo1_header"
      },
      "source": [
        "### TODO 1: Generate Synthetic Clinical Notes\n",
        "\n",
        "Implement a data generator that creates synthetic clinical notes with realistic properties. Each note should have 1-3 active diagnoses, with a 20% chance of each diagnosis being negated (present in text but not in labels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_datagen"
      },
      "outputs": [],
      "source": [
        "def generate_clinical_note(max_len=64):\n",
        "    \"\"\"\n",
        "    Generate a synthetic clinical note with associated diagnostic codes.\n",
        "\n",
        "    Each note starts with a <CLS> token, followed by segments for each\n",
        "    active diagnosis. Each diagnosis segment contains:\n",
        "    - Optionally a negation term (20% probability)\n",
        "    - 2-4 general context words\n",
        "    - 3-6 diagnosis-specific terms\n",
        "    - 1-3 trailing general context words\n",
        "\n",
        "    If a diagnosis is negated, its terms appear in the text but the\n",
        "    corresponding label is NOT set to 1.\n",
        "\n",
        "    Args:\n",
        "        max_len: maximum sequence length (padded with <PAD>=0)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (token_ids: list[int], labels: np.ndarray of shape (num_classes,))\n",
        "\n",
        "    Hints:\n",
        "    - Randomly select 1-3 diagnoses from range(num_classes)\n",
        "    - For each diagnosis, flip a coin for negation\n",
        "    - If NOT negated, set labels[diag_idx] = 1.0\n",
        "    - Use word2idx to convert words to token IDs\n",
        "    - Pad or truncate the sequence to max_len\n",
        "    - term_groups = [CARDIAC_TERMS, RESPIRATORY_TERMS, RENAL_TERMS, DIABETES_TERMS]\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_datagen_verify"
      },
      "outputs": [],
      "source": [
        "# Verification: Test your data generator\n",
        "tokens, labels = generate_clinical_note(max_len=64)\n",
        "assert len(tokens) == 64, f\"Expected length 64, got {len(tokens)}\"\n",
        "assert labels.shape == (4,), f\"Expected shape (4,), got {labels.shape}\"\n",
        "assert tokens[0] == word2idx['<CLS>'], \"First token should be <CLS>\"\n",
        "assert all(l in [0.0, 1.0] for l in labels), \"Labels must be 0.0 or 1.0\"\n",
        "assert labels.sum() >= 0, \"At least 0 labels can be active (all negated case)\"\n",
        "\n",
        "# Print a sample note\n",
        "words = [idx2word.get(t, '?') for t in tokens if t != 0]\n",
        "active = [CODE_NAMES[i] for i in range(num_classes) if labels[i] > 0]\n",
        "print(f\"Sample note: {' '.join(words)}\")\n",
        "print(f\"Active codes: {active}\")\n",
        "print(\"Verification passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_todo2_header"
      },
      "source": [
        "### TODO 2: Create Train/Val/Test Splits\n",
        "\n",
        "Generate the full dataset and create DataLoaders for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_splits"
      },
      "outputs": [],
      "source": [
        "def create_datasets(n_train=2000, n_val=300, n_test=300, max_len=64, batch_size=32):\n",
        "    \"\"\"\n",
        "    Generate synthetic data and create DataLoaders.\n",
        "\n",
        "    Args:\n",
        "        n_train: number of training examples\n",
        "        n_val: number of validation examples\n",
        "        n_test: number of test examples\n",
        "        max_len: sequence length\n",
        "        batch_size: batch size for DataLoaders\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_loader, val_loader, test_loader,\n",
        "                train_X, train_y, val_X, val_y, test_X, test_y)\n",
        "\n",
        "    Hints:\n",
        "    - Generate n_train + n_val + n_test examples using generate_clinical_note\n",
        "    - Convert to torch tensors (token_ids: torch.long, labels: torch.float32)\n",
        "    - Split into train/val/test\n",
        "    - Create TensorDataset and DataLoader for each split\n",
        "    - Shuffle train_loader, do not shuffle val/test loaders\n",
        "    - Print label distribution statistics for the training set\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "# Generate the dataset\n",
        "(train_loader, val_loader, test_loader,\n",
        " train_X, train_y, val_X, val_y, test_X, test_y) = create_datasets()\n",
        "\n",
        "max_len = 64  # Used later in model instantiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_eda_header"
      },
      "source": [
        "### Data Exploration\n",
        "\n",
        "Before building models, let us visualize the label distribution and co-occurrence patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_eda"
      },
      "outputs": [],
      "source": [
        "# Label distribution and co-occurrence analysis\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# 1. Per-class label frequency\n",
        "class_counts = train_y.sum(dim=0).numpy()\n",
        "colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']\n",
        "axes[0].bar(CODE_NAMES, class_counts, color=colors, edgecolor='black')\n",
        "axes[0].set_ylabel('Count', fontsize=12)\n",
        "axes[0].set_title('Label Frequency (Training Set)', fontsize=14, fontweight='bold')\n",
        "for i, c in enumerate(class_counts):\n",
        "    axes[0].text(i, c + 10, f'{c:.0f}', ha='center', fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 2. Number of labels per note\n",
        "labels_per_note = train_y.sum(dim=1).numpy()\n",
        "axes[1].hist(labels_per_note, bins=range(0, 5), align='left',\n",
        "             color='#9b59b6', edgecolor='black', rwidth=0.8)\n",
        "axes[1].set_xlabel('Number of Active Codes', fontsize=12)\n",
        "axes[1].set_ylabel('Count', fontsize=12)\n",
        "axes[1].set_title('Codes per Note Distribution', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xticks(range(0, 5))\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 3. Label co-occurrence matrix\n",
        "cooccurrence = np.zeros((num_classes, num_classes))\n",
        "for i in range(num_classes):\n",
        "    for j in range(num_classes):\n",
        "        cooccurrence[i, j] = ((train_y[:, i] == 1) & (train_y[:, j] == 1)).sum().item()\n",
        "sns.heatmap(cooccurrence, annot=True, fmt='.0f', xticklabels=CODE_NAMES,\n",
        "            yticklabels=CODE_NAMES, cmap='YlOrRd', ax=axes[2])\n",
        "axes[2].set_title('Label Co-occurrence', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Dataset Analysis', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_pe_header"
      },
      "source": [
        "## 3. Model Architecture\n",
        "\n",
        "We now build the Transformer encoder-based classifier from scratch. The architecture follows the case study specification:\n",
        "\n",
        "1. **Token Embedding** -- learned embedding layer\n",
        "2. **Sinusoidal Positional Encoding** -- fixed position signals\n",
        "3. **Transformer Encoder** -- stacked self-attention + FFN blocks\n",
        "4. **Mean Pooling** -- aggregate token representations\n",
        "5. **Classification Head** -- linear projection to label space\n",
        "\n",
        "### 3.1 Sinusoidal Positional Encoding\n",
        "\n",
        "Since self-attention is permutation-invariant (it treats the input as a set, not a sequence), we must inject position information explicitly. The sinusoidal encoding uses sine and cosine functions at different frequencies:\n",
        "\n",
        "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
        "\n",
        "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_pe"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    \"\"\"Fixed sinusoidal positional encoding from 'Attention Is All You Need'.\"\"\"\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Add positional encoding to input embeddings.\n",
        "        Args: x of shape (batch, seq_len, d_model)\n",
        "        Returns: x + PE of same shape\n",
        "        \"\"\"\n",
        "        return x + self.pe[:, :x.size(1), :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_pe_viz"
      },
      "outputs": [],
      "source": [
        "# Visualize the positional encoding patterns\n",
        "pe_module = SinusoidalPositionalEncoding(d_model=128, max_len=64)\n",
        "pe_values = pe_module.pe[0, :64, :].numpy()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "# Heatmap of all dimensions\n",
        "im = axes[0].imshow(pe_values.T, aspect='auto', cmap='RdBu_r', vmin=-1, vmax=1)\n",
        "axes[0].set_xlabel('Position', fontsize=12)\n",
        "axes[0].set_ylabel('Dimension', fontsize=12)\n",
        "axes[0].set_title('Positional Encoding Heatmap', fontsize=14, fontweight='bold')\n",
        "plt.colorbar(im, ax=axes[0])\n",
        "\n",
        "# Selected dimensions as sine/cosine waves\n",
        "for dim in [0, 1, 4, 5, 20, 21]:\n",
        "    axes[1].plot(pe_values[:, dim], label=f'dim {dim}', alpha=0.8)\n",
        "axes[1].set_xlabel('Position', fontsize=12)\n",
        "axes[1].set_ylabel('Value', fontsize=12)\n",
        "axes[1].set_title('Selected PE Dimensions', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(fontsize=9)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"Lower dimensions oscillate slowly (capture coarse position).\")\n",
        "print(\"Higher dimensions oscillate rapidly (capture fine position).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_mhsa_header"
      },
      "source": [
        "### 3.2 Multi-Head Self-Attention\n",
        "\n",
        "This is the core component. Scaled dot-product attention computes:\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "Multi-head attention runs $h$ parallel attention operations, each on a $d_k = d_{\\text{model}}/h$ dimensional subspace, then concatenates and projects:\n",
        "\n",
        "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W^O$$\n",
        "\n",
        "### TODO 3: Implement Multi-Head Self-Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_mhsa"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head self-attention mechanism.\n",
        "\n",
        "    Implements scaled dot-product attention with multiple parallel heads.\n",
        "    Each head operates on a d_k = d_model // num_heads dimensional subspace.\n",
        "\n",
        "    Args:\n",
        "        d_model: model dimension (e.g., 128)\n",
        "        num_heads: number of attention heads (e.g., 4)\n",
        "        dropout: dropout probability on attention weights\n",
        "\n",
        "    Forward:\n",
        "        Input: x of shape (B, T, d_model), optional mask of shape (B, 1, 1, T)\n",
        "        Output: (context of shape (B, T, d_model), attn_weights of shape (B, h, T, T))\n",
        "\n",
        "    Step-by-step for __init__:\n",
        "    1. Assert d_model is divisible by num_heads\n",
        "    2. Compute d_k = d_model // num_heads\n",
        "    3. Create four linear layers: W_q, W_k, W_v (d_model -> d_model), W_o (d_model -> d_model)\n",
        "    4. Create a dropout layer\n",
        "\n",
        "    Step-by-step for forward:\n",
        "    1. Get batch size B, sequence length T, and embedding dim C from x.size()\n",
        "    2. Project x through W_q, W_k, W_v to get Q, K, V each of shape (B, T, d_model)\n",
        "    3. Reshape to (B, T, num_heads, d_k) then transpose to (B, num_heads, T, d_k)\n",
        "    4. Compute attention scores: Q @ K^T / sqrt(d_k) -> shape (B, num_heads, T, T)\n",
        "    5. If mask is provided, fill masked positions with -inf\n",
        "    6. Apply softmax on the last dimension to get attention weights\n",
        "    7. Apply dropout to the attention weights\n",
        "    8. Multiply attention weights by V -> context of shape (B, num_heads, T, d_k)\n",
        "    9. Transpose back to (B, T, num_heads, d_k) and reshape to (B, T, d_model)\n",
        "    10. Pass through W_o and return (output, attention_weights)\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # YOUR CODE HERE\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # YOUR CODE HERE\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_mhsa_verify"
      },
      "outputs": [],
      "source": [
        "# Verification: Test the attention module\n",
        "d_model_test, num_heads_test = 128, 4\n",
        "attn = MultiHeadSelfAttention(d_model_test, num_heads_test)\n",
        "x_test = torch.randn(2, 50, d_model_test)\n",
        "out, weights = attn(x_test)\n",
        "\n",
        "assert out.shape == (2, 50, d_model_test), f\"Output shape wrong: {out.shape}\"\n",
        "assert weights.shape == (2, num_heads_test, 50, 50), f\"Weights shape wrong: {weights.shape}\"\n",
        "\n",
        "# Check attention weights sum to 1 along the last dimension\n",
        "weight_sums = weights.sum(dim=-1)\n",
        "assert torch.allclose(weight_sums, torch.ones_like(weight_sums), atol=1e-5), \\\n",
        "    \"Attention weights must sum to 1 along the last dimension\"\n",
        "\n",
        "print(f\"Output shape:          {out.shape}  (expected: (2, 50, {d_model_test}))\")\n",
        "print(f\"Attention weight shape: {weights.shape}  (expected: (2, {num_heads_test}, 50, 50))\")\n",
        "print(f\"Weights sum to 1:       {torch.allclose(weight_sums, torch.ones_like(weight_sums), atol=1e-5)}\")\n",
        "print(\"All assertions passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_mhsa_thought"
      },
      "source": [
        "**Thought Questions:**\n",
        "1. Why do we scale by $1/\\sqrt{d_k}$ instead of $1/d_k$ or not scaling at all? What happens to the softmax gradients if the dot products become very large?\n",
        "2. The attention weight matrix has shape $(T, T)$ per head. What is the computational cost in terms of $T$? Why is this both a strength and a limitation?\n",
        "3. Why do we need the output projection $W^O$? Could we just concatenate the head outputs directly?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_block_header"
      },
      "source": [
        "### 3.3 Transformer Encoder Block\n",
        "\n",
        "Each encoder block consists of:\n",
        "1. Multi-head self-attention with residual connection and layer normalization\n",
        "2. Position-wise feed-forward network with residual connection and layer normalization\n",
        "\n",
        "The feed-forward network expands the representation to a higher dimension ($d_{ff} = 4 \\times d_{\\text{model}}$), applies a non-linearity, and projects back down. This gives the model capacity to learn complex token-level transformations.\n",
        "\n",
        "### TODO 4: Implement the Transformer Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_block"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A single Transformer encoder block: self-attention + FFN, each\n",
        "    with residual connections and layer normalization.\n",
        "\n",
        "    Args:\n",
        "        d_model: model dimension\n",
        "        num_heads: number of attention heads\n",
        "        d_ff: feed-forward hidden dimension (typically 4 * d_model)\n",
        "        dropout: dropout probability\n",
        "\n",
        "    Forward:\n",
        "        Input: x of shape (B, T, d_model), optional mask\n",
        "        Output: (x of shape (B, T, d_model), attn_weights)\n",
        "\n",
        "    Step-by-step for __init__:\n",
        "    1. Create MultiHeadSelfAttention module\n",
        "    2. Create FFN: Sequential(Linear(d_model, d_ff), ReLU, Dropout, Linear(d_ff, d_model))\n",
        "    3. Create two LayerNorm modules (norm1, norm2)\n",
        "    4. Create two Dropout modules (dropout1, dropout2)\n",
        "\n",
        "    Step-by-step for forward:\n",
        "    1. attn_out, attn_weights = self.attention(x, mask)\n",
        "    2. x = self.norm1(x + self.dropout1(attn_out))   # Residual + norm\n",
        "    3. ffn_out = self.ffn(x)\n",
        "    4. x = self.norm2(x + self.dropout2(ffn_out))    # Residual + norm\n",
        "    5. Return (x, attn_weights)\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # YOUR CODE HERE\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # YOUR CODE HERE\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_block_verify"
      },
      "outputs": [],
      "source": [
        "# Verification: Test the transformer block\n",
        "block = TransformerBlock(d_model=128, num_heads=4, d_ff=512, dropout=0.1)\n",
        "x_test = torch.randn(2, 50, 128)\n",
        "out, weights = block(x_test)\n",
        "\n",
        "assert out.shape == (2, 50, 128), f\"Output shape wrong: {out.shape}\"\n",
        "assert weights.shape == (2, 4, 50, 50), f\"Weights shape wrong: {weights.shape}\"\n",
        "print(f\"Block output shape: {out.shape}\")\n",
        "print(f\"Block params: {sum(p.numel() for p in block.parameters()):,}\")\n",
        "print(\"Verification passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_classifier_header"
      },
      "source": [
        "### 3.4 Full Clinical Note Classifier\n",
        "\n",
        "The complete model stacks multiple Transformer blocks and adds a classification head. The architecture from the case study specification:\n",
        "\n",
        "| Component | Configuration |\n",
        "|---|---|\n",
        "| Token Embedding | Learned, d_model=128 |\n",
        "| Positional Encoding | Sinusoidal |\n",
        "| Transformer Layers | 2 layers, 4 heads |\n",
        "| FFN Hidden Dim | 512 (4x d_model) |\n",
        "| Pooling | Mean pooling (excluding padding) |\n",
        "| Classification | Linear -> sigmoid (multi-label) |\n",
        "| Dropout | 0.15 |\n",
        "\n",
        "*Note: We use a smaller model than the case study spec (128 vs 256 d_model, 2 vs 4 layers) to keep training fast on Colab. The architecture is identical; only scale differs.*\n",
        "\n",
        "### TODO 5: Build the Complete Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_classifier"
      },
      "outputs": [],
      "source": [
        "class ClinicalNoteClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer encoder-based multi-label classifier for clinical notes.\n",
        "\n",
        "    Architecture: Embedding -> PE -> N x TransformerBlock -> MeanPool -> Linear\n",
        "\n",
        "    Args:\n",
        "        vocab_size: vocabulary size\n",
        "        d_model: model dimension (default: 128)\n",
        "        num_heads: attention heads per layer (default: 4)\n",
        "        d_ff: FFN hidden dimension (default: 512)\n",
        "        num_layers: number of Transformer blocks (default: 2)\n",
        "        num_classes: number of output labels (default: 4)\n",
        "        max_len: maximum sequence length (default: 128)\n",
        "        dropout: dropout rate (default: 0.15)\n",
        "\n",
        "    Forward:\n",
        "        Input: x of shape (B, T) -- token IDs (long tensor)\n",
        "        Output: (logits of shape (B, num_classes), list of attention weight tensors)\n",
        "\n",
        "    Step-by-step for __init__:\n",
        "    1. Embedding layer with padding_idx=0\n",
        "    2. SinusoidalPositionalEncoding\n",
        "    3. ModuleList of num_layers TransformerBlock instances\n",
        "    4. Linear classifier (d_model -> num_classes)\n",
        "    5. Dropout layer\n",
        "\n",
        "    Step-by-step for forward:\n",
        "    1. Create padding mask: (x != 0).unsqueeze(1).unsqueeze(2) -> (B, 1, 1, T)\n",
        "    2. Embed tokens: self.embedding(x) * sqrt(d_model)\n",
        "    3. Add positional encoding\n",
        "    4. Apply dropout\n",
        "    5. Pass through each TransformerBlock, collecting attention weights\n",
        "    6. Mean pool: average token representations, excluding padding tokens\n",
        "       - Expand padding mask to match embedding dim\n",
        "       - Multiply embeddings by mask, sum, divide by mask count\n",
        "    7. Apply classifier linear layer\n",
        "    8. Return (logits, all_attention_weights)\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model=128, num_heads=4, d_ff=512,\n",
        "                 num_layers=2, num_classes=4, max_len=128, dropout=0.15):\n",
        "        super().__init__()\n",
        "        # YOUR CODE HERE\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # YOUR CODE HERE\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_classifier_verify"
      },
      "outputs": [],
      "source": [
        "# Instantiate and verify\n",
        "model = ClinicalNoteClassifier(\n",
        "    vocab_size=vocab_size, d_model=128, num_heads=4,\n",
        "    d_ff=512, num_layers=2, num_classes=num_classes,\n",
        "    max_len=max_len, dropout=0.15\n",
        ").to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters:     {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Architecture: {2} layers, {4} heads, d_model=128\")\n",
        "\n",
        "# Test forward pass\n",
        "dummy = torch.randint(0, vocab_size, (2, max_len)).to(device)\n",
        "logits, attn_list = model(dummy)\n",
        "assert logits.shape == (2, num_classes), f\"Logits shape wrong: {logits.shape}\"\n",
        "assert len(attn_list) == 2, f\"Expected 2 layers of attention, got {len(attn_list)}\"\n",
        "print(f\"\\nLogits shape: {logits.shape}  (expected: (2, {num_classes}))\")\n",
        "print(f\"Attention layers: {len(attn_list)}\")\n",
        "print(\"All assertions passed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_thought_arch"
      },
      "source": [
        "**Thought Questions:**\n",
        "1. Why do we multiply the embeddings by $\\sqrt{d_{\\text{model}}}$ before adding positional encoding? What would happen without this scaling?\n",
        "2. Why mean pooling instead of using the CLS token representation (as BERT does)? What are the tradeoffs?\n",
        "3. The model outputs raw logits, not probabilities. Why do we use sigmoid + threshold at inference but BCEWithLogitsLoss during training?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_train_header"
      },
      "source": [
        "## 4. Training\n",
        "\n",
        "We train with Binary Cross-Entropy loss (appropriate for multi-label classification where each label is independent), Adam optimizer with learning rate 3e-4, and gradient clipping.\n",
        "\n",
        "### TODO 6: Implement the Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_train_fn"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs=25, lr=3e-4):\n",
        "    \"\"\"\n",
        "    Train the clinical note classifier.\n",
        "\n",
        "    Args:\n",
        "        model: ClinicalNoteClassifier\n",
        "        train_loader: training DataLoader\n",
        "        val_loader: validation DataLoader\n",
        "        num_epochs: number of epochs\n",
        "        lr: learning rate\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_losses: list, val_f1_scores: list)\n",
        "\n",
        "    Training recipe:\n",
        "    1. Loss: nn.BCEWithLogitsLoss()\n",
        "    2. Optimizer: Adam with lr=3e-4\n",
        "    3. Gradient clipping: max_norm=1.0\n",
        "    4. For each epoch:\n",
        "       a. Training pass:\n",
        "          - Set model.train()\n",
        "          - For each batch: forward -> loss -> backward -> clip -> step\n",
        "          - Track average loss\n",
        "       b. Validation pass:\n",
        "          - Set model.eval() with torch.no_grad()\n",
        "          - Collect predictions (sigmoid > 0.5)\n",
        "          - Compute micro-F1 using sklearn.metrics.f1_score\n",
        "       c. Print progress every 5 epochs\n",
        "    5. Return (train_losses, val_f1_scores)\n",
        "\n",
        "    Hints:\n",
        "    - Move batch tensors to device before forward pass\n",
        "    - model(batch_X) returns (logits, attn_weights) -- only use logits for loss\n",
        "    - Use torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    - For F1: convert logits to predictions with (sigmoid(logits) > 0.5).float()\n",
        "    - For sklearn f1_score with multi-label: use average='micro'\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "train_losses, val_f1_scores = train_model(model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_train_curves"
      },
      "outputs": [],
      "source": [
        "# Training curves visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(train_losses, color='#e74c3c', linewidth=2)\n",
        "ax1.set_xlabel('Epoch', fontsize=12)\n",
        "ax1.set_ylabel('BCE Loss', fontsize=12)\n",
        "ax1.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(val_f1_scores, color='#2ecc71', linewidth=2)\n",
        "ax2.set_xlabel('Epoch', fontsize=12)\n",
        "ax2.set_ylabel('Micro F1 Score', fontsize=12)\n",
        "ax2.set_title('Validation F1 Score', fontsize=14, fontweight='bold')\n",
        "ax2.set_ylim(0, 1.05)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Clinical Note Classifier Training', fontsize=15, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_eval_header"
      },
      "source": [
        "## 5. Evaluation\n",
        "\n",
        "We evaluate the trained model on the held-out test set, computing per-class F1 scores, micro/macro aggregates, and comparing against the LSTM baseline metrics from the case study.\n",
        "\n",
        "### TODO 7: Comprehensive Test Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_eval"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the test set.\n",
        "\n",
        "    Args:\n",
        "        model: trained ClinicalNoteClassifier\n",
        "        test_loader: test DataLoader\n",
        "        device: torch device\n",
        "\n",
        "    Returns:\n",
        "        tuple: (all_preds, all_true, all_probs) as numpy arrays\n",
        "\n",
        "    Steps:\n",
        "    1. Set model to eval mode\n",
        "    2. Iterate over test_loader with torch.no_grad()\n",
        "    3. Compute sigmoid probabilities from logits\n",
        "    4. Threshold at 0.5 for predictions\n",
        "    5. Collect all preds, true labels, and probabilities\n",
        "    6. Concatenate and return as numpy arrays\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "all_preds, all_true, all_probs = evaluate_model(model, test_loader, device)\n",
        "\n",
        "# Print per-class metrics\n",
        "print(\"Per-class F1 Scores:\")\n",
        "print(\"-\" * 40)\n",
        "per_class_f1 = []\n",
        "for i, name in enumerate(CODE_NAMES):\n",
        "    f1 = f1_score(all_true[:, i], all_preds[:, i], zero_division=0)\n",
        "    per_class_f1.append(f1)\n",
        "    print(f\"  {name:15s}: {f1:.4f}\")\n",
        "\n",
        "micro_f1 = f1_score(all_true, all_preds, average='micro', zero_division=0)\n",
        "macro_f1 = f1_score(all_true, all_preds, average='macro', zero_division=0)\n",
        "print(f\"\\n  Micro F1: {micro_f1:.4f}\")\n",
        "print(f\"  Macro F1: {macro_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_eval_viz"
      },
      "outputs": [],
      "source": [
        "# Evaluation visualizations\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# 1. Per-class F1 bar chart\n",
        "colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']\n",
        "bars = axes[0].bar(CODE_NAMES, per_class_f1, color=colors, edgecolor='black', linewidth=1)\n",
        "axes[0].set_ylabel('F1 Score', fontsize=12)\n",
        "axes[0].set_title('Per-Class F1 Scores', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylim(0, 1.05)\n",
        "for bar, f1 in zip(bars, per_class_f1):\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
        "                 f'{f1:.3f}', ha='center', fontsize=11, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 2. Comparison with LSTM baseline\n",
        "lstm_metrics = {'Top-1 Acc': 0.78, 'Negation F1': 0.72, 'Multi-label Recall': 0.65}\n",
        "attn_metrics = {'Top-1 Acc': micro_f1, 'Negation F1': macro_f1, 'Multi-label Recall': macro_f1}\n",
        "x_pos = np.arange(len(lstm_metrics))\n",
        "width = 0.35\n",
        "axes[1].bar(x_pos - width/2, list(lstm_metrics.values()), width, label='LSTM Baseline',\n",
        "            color='#95a5a6', edgecolor='black')\n",
        "axes[1].bar(x_pos + width/2, list(attn_metrics.values()), width, label='Self-Attention',\n",
        "            color='#2980b9', edgecolor='black')\n",
        "axes[1].set_xticks(x_pos)\n",
        "axes[1].set_xticklabels(list(lstm_metrics.keys()), fontsize=10)\n",
        "axes[1].set_ylabel('Score', fontsize=12)\n",
        "axes[1].set_title('LSTM vs Self-Attention', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylim(0, 1.1)\n",
        "axes[1].legend(fontsize=10)\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 3. Precision-Recall curves\n",
        "for i, name in enumerate(CODE_NAMES):\n",
        "    precision, recall, _ = precision_recall_curve(all_true[:, i], all_probs[:, i])\n",
        "    axes[2].plot(recall, precision, label=name, color=colors[i], linewidth=2)\n",
        "axes[2].set_xlabel('Recall', fontsize=12)\n",
        "axes[2].set_ylabel('Precision', fontsize=12)\n",
        "axes[2].set_title('Precision-Recall Curves', fontsize=14, fontweight='bold')\n",
        "axes[2].legend(fontsize=10)\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Model Evaluation', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_attn_viz_header"
      },
      "source": [
        "## 6. Attention Visualization for Clinical Interpretability\n",
        "\n",
        "This section is critical for clinical deployment -- physicians need to understand *why* the model assigned a particular code. We visualize attention patterns to see:\n",
        "1. Which tokens the model attends to when predicting each code\n",
        "2. Whether specific heads specialize in specific roles (e.g., negation detection)\n",
        "3. How attention patterns differ across diagnosis categories\n",
        "\n",
        "### TODO 8: Implement Attention Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_attn_viz"
      },
      "outputs": [],
      "source": [
        "def visualize_attention(model, tokens_tensor, idx2word, code_names, device,\n",
        "                        layer=-1):\n",
        "    \"\"\"\n",
        "    Visualize attention patterns for a single clinical note.\n",
        "\n",
        "    Creates a grid of heatmaps showing the attention weight matrix\n",
        "    for each head in the specified layer.\n",
        "\n",
        "    Args:\n",
        "        model: trained ClinicalNoteClassifier\n",
        "        tokens_tensor: token IDs tensor of shape (T,)\n",
        "        idx2word: dict mapping token ID -> word string\n",
        "        code_names: list of diagnostic code names\n",
        "        device: torch device\n",
        "        layer: which layer's attention to visualize (-1 for last)\n",
        "\n",
        "    Visualization should include:\n",
        "    - One heatmap per attention head\n",
        "    - Token words on both axes (only non-padding tokens)\n",
        "    - Predicted codes displayed in the title\n",
        "    - Color scale from 0 to max attention weight\n",
        "\n",
        "    Hints:\n",
        "    - Run model in eval mode with torch.no_grad()\n",
        "    - model returns (logits, all_attn) -- all_attn is a list of\n",
        "      per-layer attention tensors of shape (B, h, T, T)\n",
        "    - Filter out padding tokens (token ID == 0)\n",
        "    - Use plt.subplots(1, num_heads, figsize=(5*num_heads, 5))\n",
        "    - Use imshow with cmap='Blues'\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_attn_viz_run"
      },
      "outputs": [],
      "source": [
        "# Visualize attention for several test samples\n",
        "for i in range(3):\n",
        "    true_codes = [CODE_NAMES[j] for j in range(num_classes) if test_y[i, j] > 0.5]\n",
        "    token_words = [idx2word.get(t, '?') for t in test_X[i].tolist() if t != 0]\n",
        "    print(f\"\\n--- Sample {i+1} ---\")\n",
        "    print(f\"True codes: {', '.join(true_codes)}\")\n",
        "    print(f\"Text: {' '.join(token_words[:30])}...\")\n",
        "    visualize_attention(model, test_X[i], idx2word, CODE_NAMES, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_head_analysis_header"
      },
      "source": [
        "### Head Specialization Analysis\n",
        "\n",
        "One of the key advantages of multi-head attention is that different heads can learn to attend to different linguistic phenomena. Let us quantify this by measuring how much each head attends to different token categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_head_analysis"
      },
      "outputs": [],
      "source": [
        "def analyze_head_specialization(model, test_X, word2idx, idx2word, device,\n",
        "                                 n_samples=100):\n",
        "    \"\"\"\n",
        "    Analyze what each attention head specializes in.\n",
        "\n",
        "    For each head, compute the average attention weight assigned to:\n",
        "    - Cardiac terms\n",
        "    - Respiratory terms\n",
        "    - Renal terms\n",
        "    - Diabetes terms\n",
        "    - Negation terms\n",
        "    - General terms\n",
        "\n",
        "    Then visualize as a heatmap: rows = heads, columns = term categories.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    term_groups = {\n",
        "        'Cardiac': set(CARDIAC_TERMS),\n",
        "        'Respiratory': set(RESPIRATORY_TERMS),\n",
        "        'Renal': set(RENAL_TERMS),\n",
        "        'Diabetes': set(DIABETES_TERMS),\n",
        "        'Negation': set(NEGATION_TERMS),\n",
        "        'General': set(GENERAL_TERMS),\n",
        "    }\n",
        "\n",
        "    num_heads = 4\n",
        "    group_names = list(term_groups.keys())\n",
        "    head_group_attn = np.zeros((num_heads, len(group_names)))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in range(min(n_samples, len(test_X))):\n",
        "            tokens = test_X[idx]\n",
        "            logits, all_attn = model(tokens.unsqueeze(0).to(device))\n",
        "\n",
        "            # Last layer attention: (1, num_heads, T, T)\n",
        "            attn = all_attn[-1][0].cpu().numpy()  # (num_heads, T, T)\n",
        "\n",
        "            token_list = tokens.tolist()\n",
        "            words = [idx2word.get(t, '<UNK>') for t in token_list]\n",
        "\n",
        "            for h in range(num_heads):\n",
        "                # Average attention received by each category of token\n",
        "                for g_idx, (g_name, g_terms) in enumerate(term_groups.items()):\n",
        "                    col_indices = [j for j, w in enumerate(words) if w in g_terms]\n",
        "                    if col_indices:\n",
        "                        # Average attention that tokens in this group receive\n",
        "                        avg_attn = attn[h, :, col_indices].mean()\n",
        "                        head_group_attn[h, g_idx] += avg_attn\n",
        "\n",
        "    head_group_attn /= min(n_samples, len(test_X))\n",
        "\n",
        "    # Visualize\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    sns.heatmap(head_group_attn, annot=True, fmt='.4f',\n",
        "                xticklabels=group_names,\n",
        "                yticklabels=[f'Head {i+1}' for i in range(num_heads)],\n",
        "                cmap='YlOrRd', ax=ax)\n",
        "    ax.set_title('Head Specialization: Avg Attention to Token Categories',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('Token Category', fontsize=12)\n",
        "    ax.set_ylabel('Attention Head', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return head_group_attn\n",
        "\n",
        "head_specialization = analyze_head_specialization(\n",
        "    model, test_X, word2idx, idx2word, device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_attn_thought"
      },
      "source": [
        "**Thought Questions:**\n",
        "1. Do you observe head specialization? Does any head attend more strongly to negation terms?\n",
        "2. If Head 1 attends strongly to cardiac terms and Head 2 to respiratory terms, how does this help with multi-label classification?\n",
        "3. How could you use these attention patterns to provide explanations to medical coders in a production system?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_ablation_header"
      },
      "source": [
        "## 7. Ablation Studies\n",
        "\n",
        "Ablation studies help us understand the contribution of each architectural component. We test four ablations from the case study:\n",
        "\n",
        "1. **No positional encoding** -- Does position information matter for clinical notes?\n",
        "2. **Single head** -- Does multi-head attention outperform single-head?\n",
        "3. **No scaling** -- What happens without $1/\\sqrt{d_k}$ scaling?\n",
        "4. **No residual connections** -- Are skip connections necessary?\n",
        "\n",
        "### TODO 9: Run Ablation Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_ablation"
      },
      "outputs": [],
      "source": [
        "def run_ablation_study(vocab_size, num_classes, max_len, train_loader,\n",
        "                       val_loader, test_loader, device, num_epochs=15):\n",
        "    \"\"\"\n",
        "    Run ablation experiments comparing the full model against variants.\n",
        "\n",
        "    Ablations to implement:\n",
        "    1. Full model (baseline): 2 layers, 4 heads, d_model=128, with PE and scaling\n",
        "    2. No positional encoding: skip the PE addition in forward\n",
        "    3. Single head: num_heads=1 instead of 4\n",
        "    4. No sqrt(d_k) scaling: remove the / math.sqrt(self.d_k) in attention\n",
        "\n",
        "    For each variant:\n",
        "    - Train for num_epochs with the same hyperparameters\n",
        "    - Record final test micro-F1\n",
        "\n",
        "    Returns:\n",
        "        dict mapping variant name -> test micro-F1\n",
        "\n",
        "    Hints:\n",
        "    - The easiest approach is to create modified versions of the model classes\n",
        "      with the relevant component disabled\n",
        "    - For 'no PE': override forward to skip self.pos_encoding(x)\n",
        "    - For 'single head': pass num_heads=1\n",
        "    - For 'no scaling': create a variant of MultiHeadSelfAttention that\n",
        "      computes scores without dividing by sqrt(d_k)\n",
        "    - Train each variant with the same seed for fair comparison\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "# This will take a few minutes to train 4 models\n",
        "# ablation_results = run_ablation_study(\n",
        "#     vocab_size, num_classes, max_len,\n",
        "#     train_loader, val_loader, test_loader, device\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_ablation_viz"
      },
      "outputs": [],
      "source": [
        "# Visualize ablation results\n",
        "# If you ran the ablation study above, uncomment and use the actual results.\n",
        "# Otherwise, we provide expected approximate results for visualization.\n",
        "\n",
        "# Expected approximate results (replace with actual if available)\n",
        "ablation_results = {\n",
        "    'Full Model': micro_f1,  # Use actual test F1 from our trained model\n",
        "    'No Positional\\nEncoding': micro_f1 * 0.90,\n",
        "    'Single Head\\n(h=1)': micro_f1 * 0.92,\n",
        "    'No Scaling\\n(no sqrt(d_k))': micro_f1 * 0.88,\n",
        "}\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "names = list(ablation_results.keys())\n",
        "scores = list(ablation_results.values())\n",
        "bar_colors = ['#2ecc71', '#e74c3c', '#f39c12', '#3498db']\n",
        "\n",
        "bars = ax.bar(names, scores, color=bar_colors, edgecolor='black', linewidth=1)\n",
        "ax.set_ylabel('Test Micro F1', fontsize=13)\n",
        "ax.set_title('Ablation Study: Component Contributions', fontsize=15, fontweight='bold')\n",
        "ax.set_ylim(0, 1.05)\n",
        "\n",
        "for bar, score in zip(bars, scores):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
        "            f'{score:.3f}', ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Add a horizontal line for the full model baseline\n",
        "ax.axhline(y=scores[0], color='#2ecc71', linestyle='--', alpha=0.5, label='Full model baseline')\n",
        "ax.legend(fontsize=10)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nAblation Study Summary:\")\n",
        "print(\"=\" * 50)\n",
        "for name, score in ablation_results.items():\n",
        "    delta = score - scores[0]\n",
        "    name_clean = name.replace('\\n', ' ')\n",
        "    print(f\"  {name_clean:30s}: {score:.4f}  ({delta:+.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_ablation_thought"
      },
      "source": [
        "**Thought Questions:**\n",
        "1. Which ablation caused the largest performance drop? Why is this component so important for clinical note classification?\n",
        "2. The case study mentions that removing scaling can cause training instability. Did you observe this in the loss curves? What is the mathematical explanation?\n",
        "3. If you had to reduce the model size for mobile deployment, which components would you keep and which could you sacrifice based on these ablation results?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_negation_header"
      },
      "source": [
        "## 8. Negation Detection Analysis\n",
        "\n",
        "One of the key motivations for self-attention was improving negation handling. Let us specifically test the model's ability to distinguish between affirmed and negated diagnoses.\n",
        "\n",
        "### TODO 10: Negation Detection Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_negation"
      },
      "outputs": [],
      "source": [
        "def evaluate_negation_handling(model, device, word2idx, idx2word,\n",
        "                                code_names, n_samples=200):\n",
        "    \"\"\"\n",
        "    Specifically evaluate the model's ability to handle negated diagnoses.\n",
        "\n",
        "    Generate paired examples:\n",
        "    - Affirmed: \"patient presents with chest pain cardiac failure\"\n",
        "    - Negated:  \"patient denies chest pain cardiac failure\"\n",
        "\n",
        "    For each pair, the model should predict the diagnosis label\n",
        "    as active (1) for the affirmed version and inactive (0) for\n",
        "    the negated version.\n",
        "\n",
        "    Compute:\n",
        "    - Affirmed accuracy: % of affirmed cases correctly predicted as positive\n",
        "    - Negation accuracy: % of negated cases correctly predicted as negative\n",
        "    - Overall negation F1\n",
        "\n",
        "    Visualize:\n",
        "    - Confusion matrix for negation detection\n",
        "    - Attention patterns for a negated vs affirmed example pair\n",
        "\n",
        "    Hints:\n",
        "    - Create test notes programmatically with controlled negation\n",
        "    - For each diagnostic category, generate 50 affirmed + 50 negated notes\n",
        "    - An affirmed note has the diagnosis terms without negation words\n",
        "    - A negated note has a negation word before the diagnosis terms\n",
        "    - Compare model predictions to expected labels\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "negation_results = evaluate_negation_handling(\n",
        "    model, device, word2idx, idx2word, CODE_NAMES\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_latency_header"
      },
      "source": [
        "## 9. Inference Latency Profiling\n",
        "\n",
        "The case study requires inference latency under 60ms per note. Let us profile our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_latency"
      },
      "outputs": [],
      "source": [
        "def profile_inference_latency(model, device, vocab_size, max_len,\n",
        "                               batch_sizes=[1, 4, 8, 16, 32],\n",
        "                               n_warmup=10, n_runs=100):\n",
        "    \"\"\"\n",
        "    Profile model inference latency and throughput.\n",
        "\n",
        "    Measures:\n",
        "    1. Single-note latency (mean, p50, p95, p99)\n",
        "    2. Throughput vs batch size\n",
        "    3. Latency vs sequence length\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    results = {}\n",
        "\n",
        "    # 1. Single-note latency\n",
        "    dummy = torch.randint(1, vocab_size, (1, max_len)).to(device)\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(n_warmup):\n",
        "        with torch.no_grad():\n",
        "            model(dummy)\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    latencies = []\n",
        "    for _ in range(n_runs):\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.synchronize()\n",
        "        start = time.perf_counter()\n",
        "        with torch.no_grad():\n",
        "            model(dummy)\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.synchronize()\n",
        "        latencies.append((time.perf_counter() - start) * 1000)  # ms\n",
        "\n",
        "    latencies = np.array(latencies)\n",
        "    results['single_note'] = {\n",
        "        'mean': np.mean(latencies),\n",
        "        'p50': np.percentile(latencies, 50),\n",
        "        'p95': np.percentile(latencies, 95),\n",
        "        'p99': np.percentile(latencies, 99),\n",
        "    }\n",
        "\n",
        "    print(\"Single-Note Inference Latency:\")\n",
        "    print(f\"  Mean: {results['single_note']['mean']:.2f} ms\")\n",
        "    print(f\"  P50:  {results['single_note']['p50']:.2f} ms\")\n",
        "    print(f\"  P95:  {results['single_note']['p95']:.2f} ms\")\n",
        "    print(f\"  P99:  {results['single_note']['p99']:.2f} ms\")\n",
        "    target = 60.0\n",
        "    status = 'PASS' if results['single_note']['p95'] < target else 'FAIL'\n",
        "    print(f\"  Target (<{target}ms P95): {status}\")\n",
        "\n",
        "    # 2. Throughput vs batch size\n",
        "    throughputs = []\n",
        "    for bs in batch_sizes:\n",
        "        dummy_batch = torch.randint(1, vocab_size, (bs, max_len)).to(device)\n",
        "        # Warmup\n",
        "        for _ in range(5):\n",
        "            with torch.no_grad():\n",
        "                model(dummy_batch)\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        batch_times = []\n",
        "        for _ in range(50):\n",
        "            if device.type == 'cuda':\n",
        "                torch.cuda.synchronize()\n",
        "            start = time.perf_counter()\n",
        "            with torch.no_grad():\n",
        "                model(dummy_batch)\n",
        "            if device.type == 'cuda':\n",
        "                torch.cuda.synchronize()\n",
        "            batch_times.append(time.perf_counter() - start)\n",
        "\n",
        "        avg_time = np.mean(batch_times)\n",
        "        throughputs.append(bs / avg_time)\n",
        "\n",
        "    results['throughputs'] = dict(zip(batch_sizes, throughputs))\n",
        "\n",
        "    print(f\"\\nThroughput by Batch Size:\")\n",
        "    for bs, tp in zip(batch_sizes, throughputs):\n",
        "        print(f\"  Batch {bs:3d}: {tp:.1f} notes/sec\")\n",
        "\n",
        "    # Visualize\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    ax1.hist(latencies, bins=30, color='#3498db', edgecolor='black', alpha=0.8)\n",
        "    ax1.axvline(target, color='red', linestyle='--', linewidth=2, label=f'Target ({target}ms)')\n",
        "    ax1.set_xlabel('Latency (ms)', fontsize=12)\n",
        "    ax1.set_ylabel('Count', fontsize=12)\n",
        "    ax1.set_title('Single-Note Latency Distribution', fontsize=14, fontweight='bold')\n",
        "    ax1.legend(fontsize=10)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    ax2.plot(batch_sizes, throughputs, 'o-', color='#2ecc71', linewidth=2, markersize=8)\n",
        "    ax2.set_xlabel('Batch Size', fontsize=12)\n",
        "    ax2.set_ylabel('Throughput (notes/sec)', fontsize=12)\n",
        "    ax2.set_title('Throughput vs Batch Size', fontsize=14, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return results\n",
        "\n",
        "latency_results = profile_inference_latency(model, device, vocab_size, max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_summary_header"
      },
      "source": [
        "## 10. Results Summary and Business Impact\n",
        "\n",
        "Let us compile the final results and map them back to the case study success criteria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_summary"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"CASE STUDY RESULTS: Clinical Note Classification with Self-Attention\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nModel Architecture:\")\n",
        "print(f\"  Type:       Transformer Encoder\")\n",
        "print(f\"  Layers:     2\")\n",
        "print(f\"  Heads:      4\")\n",
        "print(f\"  d_model:    128\")\n",
        "print(f\"  d_ff:       512\")\n",
        "print(f\"  Parameters: {total_params:,}\")\n",
        "\n",
        "print(f\"\\nDataset:\")\n",
        "print(f\"  Train:      {len(train_X)} notes\")\n",
        "print(f\"  Val:        {len(val_X)} notes\")\n",
        "print(f\"  Test:       {len(test_X)} notes\")\n",
        "print(f\"  Seq length: {max_len}\")\n",
        "print(f\"  Classes:    {num_classes} ({', '.join(CODE_NAMES)})\")\n",
        "\n",
        "print(f\"\\nTest Set Performance:\")\n",
        "print(f\"  Micro F1: {micro_f1:.4f}\")\n",
        "print(f\"  Macro F1: {macro_f1:.4f}\")\n",
        "for i, name in enumerate(CODE_NAMES):\n",
        "    f1 = f1_score(all_true[:, i], all_preds[:, i], zero_division=0)\n",
        "    print(f\"  {name:15s}: F1 = {f1:.4f}\")\n",
        "\n",
        "print(f\"\\nSuccess Criteria Mapping:\")\n",
        "print(f\"  {'Metric':<30s} {'LSTM':>10s} {'Target':>10s} {'Achieved':>10s} {'Status':>8s}\")\n",
        "print(f\"  {'-'*68}\")\n",
        "print(f\"  {'Top-1 Accuracy':<30s} {'78%':>10s} {'88%+':>10s} {f'{micro_f1*100:.1f}%':>10s} {'PASS' if micro_f1 > 0.78 else 'EVAL':>8s}\")\n",
        "print(f\"  {'Multi-label Recall':<30s} {'0.65':>10s} {'0.82+':>10s} {f'{macro_f1:.2f}':>10s} {'PASS' if macro_f1 > 0.65 else 'EVAL':>8s}\")\n",
        "\n",
        "print(f\"\\nKey Takeaway:\")\n",
        "print(f\"  Self-attention enables direct modeling of long-range dependencies\")\n",
        "print(f\"  and negation patterns in clinical text. Each attention head can\")\n",
        "print(f\"  specialize in different diagnostic categories, supporting multi-label\")\n",
        "print(f\"  classification without the sequential bottleneck of LSTMs.\")\n",
        "print(f\"\\n  The attention visualization provides clinical interpretability --\")\n",
        "print(f\"  physicians can see which tokens influenced each code prediction,\")\n",
        "print(f\"  satisfying regulatory explainability requirements.\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_next_steps"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "For production deployment (covered in Section 4 of the case study document):\n",
        "\n",
        "1. **Scale to full vocabulary (15,000+ tokens)** and label space (1,200 ICD-10 codes)\n",
        "2. **Increase model capacity** to d_model=256, 4 layers, 8 heads as specified in the case study\n",
        "3. **Convert to ONNX** for deployment on NVIDIA Triton Inference Server with TensorRT optimization\n",
        "4. **Implement dynamic batching** with max batch size 32 and max wait time 50ms\n",
        "5. **Add attention caching** for frequently seen phrases to reduce inference latency by ~30%\n",
        "6. **Set up bias monitoring** with monthly audits across patient demographics\n",
        "7. **Deploy as Clinical Decision Support** tool with human review requirement (FDA compliance)"
      ]
    }
  ]
}