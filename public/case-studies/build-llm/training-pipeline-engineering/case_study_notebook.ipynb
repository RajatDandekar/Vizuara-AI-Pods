{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0",
      "mimetype": "text/x-python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "id": "setup_cell",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup: Run this cell first!\n",
        "# Check GPU availability and install dependencies\n",
        "\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "# Check GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"No GPU detected. Some cells may run slowly.\")\n",
        "    print(\"   Go to Runtime -> Change runtime type -> GPU\")\n",
        "\n",
        "print(f\"\\nPython {sys.version.split()[0]}\")\n",
        "print(f\"PyTorch {torch.__version__}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(f\"Random seed set to {SEED}\")\n",
        "\n",
        "%matplotlib inline"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_1",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Case Study: Training Pipeline Engineering\n",
        "## Building a Domain-Specific Medical Language Model\n",
        "\n",
        "---\n",
        "\n",
        "**Scenario:** You are an ML engineer at **Aethon Health**, a Series A healthcare AI startup building AI-assisted radiology workflows for community hospitals. Your task is to engineer the complete training pipeline -- tokenization, data loading, and optimization -- to train a 500M parameter language model for radiology report generation.\n",
        "\n",
        "**Current system:** A retrieval-augmented approach that fails on 35% of studies involving multi-system findings, forcing radiologists to rewrite reports from scratch.\n",
        "\n",
        "**Budget:** $12,000 compute (4x A100 for ~72 hours). No room for failed training runs.\n",
        "\n",
        "**Key Insight:** The Transformer architecture is well-understood. The real engineering challenge is the **training pipeline**: domain tokenization, efficient data loading via sequence packing, and stable optimization with careful learning rate scheduling.\n",
        "\n",
        "---\n",
        "\n",
        "### What You Will Build\n",
        "\n",
        "1. **Domain-specific BPE tokenizer** that preserves medical terms as single tokens\n",
        "2. **Sequence packer** that eliminates 40%+ wasted compute from padding\n",
        "3. **Masked loss function** that correctly handles packed sequences\n",
        "4. **Training loop** with warmup + cosine decay, gradient clipping, and mixed precision\n",
        "5. **Stability analysis dashboard** for monitoring training health\n",
        "6. **Report generation** with qualitative evaluation"
      ]
    },
    {
      "id": "vizuara_chatbot",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AI Teaching Assistant\n",
        "\n",
        "Need help with this notebook? Open the **AI Teaching Assistant** -- it has already read this entire notebook and can help with concepts, code, and exercises.\n",
        "\n",
        "**[Open AI Teaching Assistant](https://pods.vizuara.ai/courses/build-llm/practice/4/assistant)**\n",
        "\n",
        "*Tip: Open it in a separate tab and work through this notebook side-by-side.*"
      ]
    },
    {
      "id": "cell_2",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup and Data Loading\n",
        "\n",
        "We install `tiktoken` (OpenAI's BPE tokenizer library) to compare our domain tokenizer against the standard GPT-2 tokenizer, and use PyTorch for model training."
      ]
    },
    {
      "id": "cell_3",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q tiktoken\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import time\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import tiktoken\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_4",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simulating the Radiology Report Corpus\n",
        "\n",
        "Aethon Health has 2.3 million de-identified radiology reports. We simulate a representative corpus that captures the key properties:\n",
        "- Structured format (FINDINGS / IMPRESSION sections)\n",
        "- Domain-specific vocabulary (pneumoperitoneum, hepatosplenomegaly, etc.)\n",
        "- Highly variable report lengths (80-1200 tokens)"
      ]
    },
    {
      "id": "cell_5",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simulated radiology report corpus\n",
        "# In production, this would be 2.3M de-identified reports from hospital partners\n",
        "medical_corpus = \"\"\"\n",
        "FINDINGS: The heart is mildly enlarged with cardiomegaly. There is a small left\n",
        "pleural effusion. Bibasilar atelectasis is noted. No pneumothorax. The mediastinal\n",
        "contour is within normal limits. No focal consolidation. The osseous structures\n",
        "are unremarkable.\n",
        "\n",
        "IMPRESSION: Mild cardiomegaly with small left pleural effusion and bibasilar\n",
        "atelectasis. No acute cardiopulmonary process.\n",
        "\n",
        "FINDINGS: CT of the abdomen and pelvis with contrast. The liver demonstrates\n",
        "hepatosplenomegaly. There is cholelithiasis without evidence of cholecystitis.\n",
        "A small amount of free fluid is seen in the pelvis. No lymphadenopathy.\n",
        "The kidneys show mild bilateral hydronephrosis. No pneumoperitoneum.\n",
        "\n",
        "IMPRESSION: Hepatosplenomegaly with cholelithiasis. Mild bilateral hydronephrosis.\n",
        "Recommend clinical correlation.\n",
        "\n",
        "FINDINGS: High resolution CT of the chest demonstrates diffuse ground-glass\n",
        "opacities bilaterally. There is mediastinal lymphadenopathy measuring up to 1.5 cm.\n",
        "Findings are consistent with interstitial lung disease. Small bilateral pleural\n",
        "effusions are present. No pericardial effusion. The heart size is normal.\n",
        "\n",
        "IMPRESSION: Diffuse ground-glass opacities consistent with interstitial lung\n",
        "disease. Mediastinal lymphadenopathy. Bilateral pleural effusions.\n",
        "\n",
        "FINDINGS: PA and lateral chest radiograph. The cardiac silhouette is within\n",
        "normal limits. The lungs are clear bilaterally without focal consolidation,\n",
        "pleural effusion, or pneumothorax. No acute osseous abnormality.\n",
        "\n",
        "IMPRESSION: No acute cardiopulmonary abnormality.\n",
        "\n",
        "FINDINGS: CT of the chest with contrast. There is a 2.3 cm pulmonary nodule in\n",
        "the right upper lobe. Mediastinal lymphadenopathy is present with the largest\n",
        "node measuring 1.8 cm in the subcarinal region. Small right pleural effusion.\n",
        "Mild emphysematous changes are noted bilaterally. The heart demonstrates mild\n",
        "cardiomegaly. No pericardial effusion. Bilateral lower lobe bronchiectasis.\n",
        "\n",
        "IMPRESSION: Right upper lobe pulmonary nodule measuring 2.3 cm. Subcarinal\n",
        "lymphadenopathy. Recommend PET-CT for further evaluation. Additional findings\n",
        "include right pleural effusion, cardiomegaly, and bronchiectasis.\n",
        "\n",
        "FINDINGS: CT abdomen and pelvis without contrast. The liver is normal in size\n",
        "and attenuation. The gallbladder is surgically absent. Mild diverticulosis of\n",
        "the sigmoid colon without evidence of acute diverticulitis. The kidneys are\n",
        "unremarkable. The aorta demonstrates mild atherosclerotic calcification.\n",
        "No free fluid or lymphadenopathy.\n",
        "\n",
        "IMPRESSION: Sigmoid diverticulosis without diverticulitis. Post-cholecystectomy\n",
        "state. Mild aortic atherosclerosis.\n",
        "\"\"\" * 200  # Repeat to simulate larger corpus\n",
        "\n",
        "print(f\"Corpus size: {len(medical_corpus):,} characters\")\n",
        "print(f\"Approximate words: {len(medical_corpus.split()):,}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_6",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing Report Length Distribution\n",
        "\n",
        "Understanding the length distribution is critical for designing the data loader. Radiology reports range from short X-ray findings (~80 tokens) to complex multi-system CT reports (~1200 tokens)."
      ]
    },
    {
      "id": "cell_7",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simulate the real-world report length distribution\n",
        "np.random.seed(42)\n",
        "report_lengths = np.concatenate([\n",
        "    np.random.normal(80, 20, 5000).astype(int),     # Short (chest X-ray)\n",
        "    np.random.normal(350, 80, 3000).astype(int),     # Medium (CT scans)\n",
        "    np.random.normal(1200, 200, 1000).astype(int),   # Long (complex multi-system)\n",
        "])\n",
        "report_lengths = np.clip(report_lengths, 20, 2000)\n",
        "\n",
        "print(f\"Total reports: {len(report_lengths):,}\")\n",
        "print(f\"Mean length: {report_lengths.mean():.0f} tokens\")\n",
        "print(f\"Median length: {np.median(report_lengths):.0f} tokens\")\n",
        "print(f\"Min / Max: {report_lengths.min()} / {report_lengths.max()} tokens\")\n",
        "print(f\"\\nBreakdown:\")\n",
        "print(f\"  Short (<150 tokens):  {(report_lengths < 150).sum():,} reports ({(report_lengths < 150).mean()*100:.1f}%)\")\n",
        "print(f\"  Medium (150-600):     {((report_lengths >= 150) & (report_lengths < 600)).sum():,} reports\")\n",
        "print(f\"  Long (>600):          {(report_lengths >= 600).sum():,} reports\")\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "ax.hist(report_lengths, bins=60, color='#3498db', edgecolor='black',\n",
        "        linewidth=0.5, alpha=0.7)\n",
        "ax.axvline(x=512, color='red', linestyle='--', linewidth=2, label='Context length (512)')\n",
        "ax.axvline(x=np.median(report_lengths), color='orange', linestyle='--',\n",
        "           linewidth=2, label=f'Median ({np.median(report_lengths):.0f})')\n",
        "ax.set_xlabel('Report Length (tokens)', fontsize=12)\n",
        "ax.set_ylabel('Count', fontsize=12)\n",
        "ax.set_title('Aethon Health -- Radiology Report Length Distribution', fontsize=13, fontweight='bold')\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_8",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. The Tokenization Problem\n",
        "\n",
        "Standard BPE tokenizers (like GPT-2's) are trained on general web text. They fragment medical terminology into meaningless subword pieces, wasting model capacity and inflating sequence lengths.\n",
        "\n",
        "Let us quantify this problem."
      ]
    },
    {
      "id": "cell_9",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# GPT-2 tokenizer baseline\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Medical terms that appear in radiology reports\n",
        "medical_terms = [\n",
        "    \"pneumoperitoneum\", \"hepatosplenomegaly\", \"cardiomegaly\",\n",
        "    \"atelectasis\", \"pneumothorax\", \"consolidation\",\n",
        "    \"lymphadenopathy\", \"cholelithiasis\", \"hydronephrosis\",\n",
        "    \"osteophyte\", \"spondylolisthesis\", \"bronchiectasis\",\n",
        "    \"emphysema\", \"diverticulitis\", \"cholecystitis\",\n",
        "    \"ground-glass opacities\", \"mediastinal lymphadenopathy\",\n",
        "    \"interstitial lung disease\", \"pericardial effusion\",\n",
        "    \"aortic aneurysm\", \"pleural effusion\", \"pulmonary embolism\",\n",
        "]\n",
        "\n",
        "print(\"GPT-2 Tokenization of Medical Terms:\")\n",
        "print(\"=\" * 70)\n",
        "total_tokens = 0\n",
        "for term in medical_terms:\n",
        "    token_ids = enc.encode(term)\n",
        "    token_strings = [enc.decode([t]) for t in token_ids]\n",
        "    total_tokens += len(token_ids)\n",
        "    padding = \" \" * max(1, 35 - len(term))\n",
        "    print(f\"  {term}{padding}-> {len(token_ids)} tokens: {token_strings}\")\n",
        "\n",
        "avg_tokens = total_tokens / len(medical_terms)\n",
        "print(f\"\\nAverage tokens per medical term: {avg_tokens:.1f}\")\n",
        "print(f\"Compare: average English word ~1.3 tokens\")\n",
        "print(f\"Medical terms are {avg_tokens / 1.3:.1f}x more expensive!\")\n",
        "print(f\"\\nThis means:\")\n",
        "print(f\"  (a) Sequences are 30-40% longer than necessary -> quadratic attention cost\")\n",
        "print(f\"  (b) Model must learn that fragmented pieces = medical concepts -> wasted capacity\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_10",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Building a Domain-Specific BPE Tokenizer\n",
        "\n",
        "We train a BPE tokenizer from scratch on the medical corpus. The key insight: BPE merges are driven by **frequency**, so training on medical text naturally creates single tokens for common medical terms.\n",
        "\n",
        "### How BPE Works (Review)\n",
        "\n",
        "1. Start with a vocabulary of individual characters\n",
        "2. Count all adjacent character pairs in the corpus\n",
        "3. Merge the most frequent pair into a new token\n",
        "4. Repeat until vocabulary reaches target size\n",
        "\n",
        "### TODO 1: Implement BPE Tokenizer Training\n",
        "\n",
        "Complete the three helper functions below. The `train_medical_bpe` function orchestrates the full BPE training loop."
      ]
    },
    {
      "id": "cell_11",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_word_frequencies(text):\n",
        "    \"\"\"\n",
        "    Split text into words and compute frequency of each word.\n",
        "    Represent each word as a tuple of characters plus an end-of-word marker '_'.\n",
        "\n",
        "    Args:\n",
        "        text: string of text\n",
        "\n",
        "    Returns:\n",
        "        dict mapping tuple-of-chars -> frequency\n",
        "\n",
        "    Example:\n",
        "        \"the the cat\" -> {('t','h','e','_'): 2, ('c','a','t','_'): 1}\n",
        "\n",
        "    Hints:\n",
        "    - Use text.lower().split() to get words\n",
        "    - Use Counter for frequency counting\n",
        "    - Each word becomes tuple(list(word) + ['_'])\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "\n",
        "def get_pair_counts(vocab_freqs):\n",
        "    \"\"\"\n",
        "    Count all adjacent token pairs across the vocabulary.\n",
        "\n",
        "    Args:\n",
        "        vocab_freqs: dict mapping word (tuple of tokens) -> frequency\n",
        "\n",
        "    Returns:\n",
        "        dict mapping (token_a, token_b) -> count\n",
        "\n",
        "    Example:\n",
        "        {('c','a','t','_'): 5} -> {('c','a'): 5, ('a','t'): 5, ('t','_'): 5}\n",
        "\n",
        "    Hints:\n",
        "    - For each word, iterate through consecutive pairs\n",
        "    - Weight each pair count by the word frequency\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "\n",
        "def merge_pair(vocab_freqs, pair_to_merge):\n",
        "    \"\"\"\n",
        "    Merge all occurrences of a character pair into a single token.\n",
        "\n",
        "    Args:\n",
        "        vocab_freqs: dict mapping word (tuple of tokens) -> frequency\n",
        "        pair_to_merge: tuple of (token_a, token_b) to merge\n",
        "\n",
        "    Returns:\n",
        "        new vocab_freqs with the pair merged\n",
        "\n",
        "    Example:\n",
        "        vocab_freqs = {('c','a','t','_'): 5}\n",
        "        pair_to_merge = ('c', 'a')\n",
        "        -> {('ca','t','_'): 5}\n",
        "\n",
        "    Hints:\n",
        "    - Scan left to right through each word\n",
        "    - When you see the pair, combine into one token and skip ahead\n",
        "    - Otherwise keep the token as-is\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_12",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Verification: test your BPE helper functions\n",
        "test_freqs = get_word_frequencies(\"the the cat\")\n",
        "assert ('t', 'h', 'e', '_') in test_freqs, \"Word 'the' should be in vocab\"\n",
        "assert test_freqs[('t', 'h', 'e', '_')] == 2, \"'the' should have frequency 2\"\n",
        "\n",
        "test_pairs = get_pair_counts(test_freqs)\n",
        "assert ('t', 'h') in test_pairs, \"Pair ('t','h') should exist\"\n",
        "assert test_pairs[('t', 'h')] == 2, \"Pair ('t','h') should have count 2\"\n",
        "\n",
        "test_merged = merge_pair(test_freqs, ('t', 'h'))\n",
        "assert ('th', 'e', '_') in test_merged, \"After merge, 'the' should become ('th','e','_')\"\n",
        "\n",
        "print(\"All BPE helper function tests passed.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_13",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train_medical_bpe(text, num_merges=500):\n",
        "    \"\"\"Train BPE on medical corpus and return merge rules + vocabulary.\"\"\"\n",
        "    vocab_freqs = get_word_frequencies(text)\n",
        "    merge_rules = []\n",
        "\n",
        "    for step in range(num_merges):\n",
        "        pair_counts = get_pair_counts(vocab_freqs)\n",
        "        if not pair_counts:\n",
        "            break\n",
        "        best_pair = max(pair_counts, key=pair_counts.get)\n",
        "        vocab_freqs = merge_pair(vocab_freqs, best_pair)\n",
        "        merged_token = best_pair[0] + best_pair[1]\n",
        "        merge_rules.append((best_pair, merged_token))\n",
        "\n",
        "        if (step + 1) % 100 == 0:\n",
        "            print(f\"  Merge {step+1}: '{best_pair[0]}' + '{best_pair[1]}' -> '{merged_token}'\")\n",
        "\n",
        "    # Build final vocabulary\n",
        "    vocab = set()\n",
        "    for word in vocab_freqs:\n",
        "        for token in word:\n",
        "            vocab.add(token)\n",
        "\n",
        "    return merge_rules, vocab, vocab_freqs\n",
        "\n",
        "\n",
        "print(\"Training domain-specific BPE tokenizer on medical corpus...\")\n",
        "print(\"(In production, this runs on 2.3M reports with vocab_size=32,768)\")\n",
        "print()\n",
        "merge_rules, vocab, final_vocab = train_medical_bpe(medical_corpus, num_merges=500)\n",
        "print(f\"\\nVocabulary size: {len(vocab)}\")\n",
        "print(f\"Merge rules learned: {len(merge_rules)}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_14",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing Domain BPE vs. GPT-2 BPE\n",
        "\n",
        "Now we encode medical terms with both tokenizers and measure the improvement."
      ]
    },
    {
      "id": "cell_15",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def encode_bpe(text, merge_rules):\n",
        "    \"\"\"Encode text using our trained BPE merge rules.\"\"\"\n",
        "    words = text.lower().split()\n",
        "    all_tokens = []\n",
        "    for word in words:\n",
        "        tokens = list(word) + ['_']\n",
        "        for pair, merged in merge_rules:\n",
        "            new_tokens = []\n",
        "            i = 0\n",
        "            while i < len(tokens):\n",
        "                if (i < len(tokens) - 1 and tokens[i] == pair[0]\n",
        "                        and tokens[i + 1] == pair[1]):\n",
        "                    new_tokens.append(merged)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_tokens.append(tokens[i])\n",
        "                    i += 1\n",
        "            tokens = new_tokens\n",
        "        all_tokens.extend(tokens)\n",
        "    return all_tokens\n",
        "\n",
        "\n",
        "# Compare tokenization efficiency\n",
        "print(\"Domain BPE vs GPT-2 BPE on Medical Terms:\")\n",
        "print(\"=\" * 70)\n",
        "domain_total = 0\n",
        "gpt2_total = 0\n",
        "\n",
        "for term in medical_terms:\n",
        "    domain_tokens = encode_bpe(term, merge_rules)\n",
        "    gpt2_tokens = enc.encode(term)\n",
        "    domain_total += len(domain_tokens)\n",
        "    gpt2_total += len(gpt2_tokens)\n",
        "    improvement = len(gpt2_tokens) / max(len(domain_tokens), 1)\n",
        "    padding = \" \" * max(1, 35 - len(term))\n",
        "    print(f\"  {term}{padding}Domain: {len(domain_tokens):2d}  GPT-2: {len(gpt2_tokens):2d}  ({improvement:.1f}x)\")\n",
        "\n",
        "print(f\"\\nAverage tokens per term:\")\n",
        "print(f\"  Domain BPE: {domain_total / len(medical_terms):.1f}\")\n",
        "print(f\"  GPT-2 BPE:  {gpt2_total / len(medical_terms):.1f}\")\n",
        "print(f\"  Improvement: {gpt2_total / max(domain_total, 1):.1f}x fewer tokens\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_16",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO 2: Measure Domain Vocabulary Coverage\n",
        "\n",
        "Aethon's medical safety requirement: >99.5% of a curated list of 4,200 essential radiology terms must be representable as single tokens. Compute the coverage metric:\n",
        "\n",
        "$$\\text{Coverage}(V) = \\frac{|\\{w \\in D : w \\text{ is a single token in } V\\}|}{|D|}$$"
      ]
    },
    {
      "id": "cell_17",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def compute_domain_coverage(medical_terms, merge_rules):\n",
        "    \"\"\"\n",
        "    Compute the domain vocabulary coverage metric.\n",
        "\n",
        "    A term is \"covered\" if it encodes as a single token (plus the\n",
        "    end-of-word marker '_'). That is, encode_bpe(term) should return\n",
        "    at most 2 tokens (the term itself + '_').\n",
        "\n",
        "    Args:\n",
        "        medical_terms: list of medical term strings\n",
        "        merge_rules: BPE merge rules from training\n",
        "\n",
        "    Returns:\n",
        "        tuple of (coverage_ratio, covered_terms, uncovered_terms)\n",
        "        - coverage_ratio: float between 0 and 1\n",
        "        - covered_terms: list of terms that are single tokens\n",
        "        - uncovered_terms: list of terms that are fragmented\n",
        "\n",
        "    Hints:\n",
        "    - Use encode_bpe() to tokenize each term\n",
        "    - A term is covered if len(encode_bpe(term, merge_rules)) <= 2\n",
        "    - Report both the ratio and the specific terms in each category\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "\n",
        "coverage, covered, uncovered = compute_domain_coverage(medical_terms, merge_rules)\n",
        "print(f\"Domain Vocabulary Coverage: {coverage:.1%}\")\n",
        "print(f\"\\nCovered terms ({len(covered)}): {covered[:10]}\")\n",
        "print(f\"Uncovered terms ({len(uncovered)}): {uncovered[:10]}\")\n",
        "print(f\"\\nTarget: >99.5% -- {'MET' if coverage > 0.995 else 'NOT MET (need more training data)'}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_18",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Thought Questions:**\n",
        "1. Why might our small demo corpus not reach 99.5% coverage? What would change with 2.3M real reports?\n",
        "2. How does vocabulary size affect the tradeoff between sequence length and embedding memory?\n",
        "3. What happens to rare terms that appear fewer than 100 times in the corpus?\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Sequence Packing for Variable-Length Reports\n",
        "\n",
        "With a fixed context length of 512 tokens:\n",
        "- Short chest X-ray reports (~80 tokens) waste 84% of each sequence on padding\n",
        "- Long multi-system reports (~1200 tokens) must be split\n",
        "\n",
        "**Sequence packing** concatenates multiple short reports into a single sequence with separator tokens, achieving >92% token utilization vs. ~58% with naive padding."
      ]
    },
    {
      "id": "cell_19",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO 3: Implement the Sequence Packer\n",
        "\n",
        "Complete the `pack` method. The packer should:\n",
        "1. Concatenate short reports into fixed-length sequences with separators\n",
        "2. Split long reports across multiple sequences\n",
        "3. Generate attention masks (1 = real token, 0 = padding/separator)\n",
        "4. Achieve >90% packing efficiency"
      ]
    },
    {
      "id": "cell_20",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class SequencePacker:\n",
        "    \"\"\"\n",
        "    Pack variable-length reports into fixed-size sequences.\n",
        "    Uses a separator token between reports and generates loss masks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, context_length=512, separator_token_id=0):\n",
        "        self.context_length = context_length\n",
        "        self.separator_id = separator_token_id\n",
        "\n",
        "    def pack(self, reports):\n",
        "        \"\"\"\n",
        "        Pack a list of token sequences into fixed-length packed sequences.\n",
        "\n",
        "        Args:\n",
        "            reports: list of lists of token IDs (variable length)\n",
        "\n",
        "        Returns:\n",
        "            list of (tokens, mask) tuples where:\n",
        "            - tokens is a list of length context_length\n",
        "            - mask is a list of length context_length\n",
        "              (1 for real tokens, 0 for padding and separator tokens)\n",
        "\n",
        "        Algorithm:\n",
        "        1. Maintain a 'current' buffer for the sequence being built\n",
        "        2. For each report:\n",
        "           a. If buffer is non-empty, add a separator token (mask=0)\n",
        "           b. If report fits in remaining space, append it (mask=1)\n",
        "           c. If report doesn't fit, pad+save current buffer, start new one\n",
        "           d. If report exceeds context_length, split into chunks\n",
        "        3. Pad and save the final buffer\n",
        "\n",
        "        Hints:\n",
        "        - Use self.separator_id for separator tokens\n",
        "        - Pad with 0s at the end of each sequence\n",
        "        - The mask should be 0 for both padding AND separator tokens\n",
        "          (we don't want to compute loss on either)\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        pass"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_21",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Verification: test the packer\n",
        "packer = SequencePacker(context_length=512)\n",
        "\n",
        "# Generate synthetic reports of various lengths\n",
        "np.random.seed(42)\n",
        "test_lengths = [60, 80, 100, 300, 500, 800, 1200, 60, 80, 100]\n",
        "synthetic_reports = [\n",
        "    list(np.random.randint(1, 1000, size=length))\n",
        "    for length in test_lengths\n",
        "]\n",
        "\n",
        "packed = packer.pack(synthetic_reports)\n",
        "\n",
        "# Check correctness\n",
        "assert all(len(tokens) == 512 for tokens, _ in packed), \"All sequences must be 512 tokens\"\n",
        "assert all(len(mask) == 512 for _, mask in packed), \"All masks must be 512 elements\"\n",
        "\n",
        "total_real = sum(sum(mask) for _, mask in packed)\n",
        "total_positions = len(packed) * 512\n",
        "efficiency = total_real / total_positions * 100\n",
        "\n",
        "print(f\"Input: {len(synthetic_reports)} reports, lengths: {test_lengths}\")\n",
        "print(f\"Output: {len(packed)} packed sequences\")\n",
        "print(f\"Packing efficiency: {efficiency:.1f}%\")\n",
        "print(f\"Without packing: {len(synthetic_reports)} sequences of 512 = {len(synthetic_reports)*512:,} positions\")\n",
        "print(f\"With packing: {len(packed)} sequences of 512 = {total_positions:,} positions\")\n",
        "print(\"Verification passed.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_22",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Scale test: pack 100 reports with realistic length distribution\n",
        "np.random.seed(42)\n",
        "large_reports = [\n",
        "    list(np.random.randint(1, 1000, size=length))\n",
        "    for length in np.random.choice([60, 80, 100, 300, 500, 800, 1200], size=100)\n",
        "]\n",
        "\n",
        "packed_large = packer.pack(large_reports)\n",
        "total_real_large = sum(sum(mask) for _, mask in packed_large)\n",
        "total_pos_large = len(packed_large) * 512\n",
        "eff_large = total_real_large / total_pos_large * 100\n",
        "\n",
        "# Compare: naive padding\n",
        "naive_positions = len(large_reports) * 512\n",
        "naive_real = sum(min(len(r), 512) for r in large_reports)\n",
        "naive_eff = naive_real / naive_positions * 100\n",
        "\n",
        "print(f\"Scale test: {len(large_reports)} reports\")\n",
        "print(f\"\\nNaive padding:\")\n",
        "print(f\"  Sequences: {len(large_reports)}, Efficiency: {naive_eff:.1f}%\")\n",
        "print(f\"\\nSequence packing:\")\n",
        "print(f\"  Sequences: {len(packed_large)}, Efficiency: {eff_large:.1f}%\")\n",
        "print(f\"\\nReduction: {(1 - len(packed_large)/len(large_reports))*100:.1f}% fewer sequences\")\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "labels = ['Naive Padding', 'Sequence Packing']\n",
        "efficiencies = [naive_eff, eff_large]\n",
        "colors_bar = ['#e74c3c', '#2ecc71']\n",
        "bars = axes[0].bar(labels, efficiencies, color=colors_bar, edgecolor='black', linewidth=0.5)\n",
        "axes[0].set_ylabel('Token Utilization (%)', fontsize=12)\n",
        "axes[0].set_title('Packing Efficiency Comparison', fontsize=13, fontweight='bold')\n",
        "axes[0].set_ylim(0, 100)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "for bar, eff in zip(bars, efficiencies):\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                f'{eff:.1f}%', ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Show sequence counts\n",
        "seq_counts = [len(large_reports), len(packed_large)]\n",
        "bars2 = axes[1].bar(labels, seq_counts, color=colors_bar, edgecolor='black', linewidth=0.5)\n",
        "axes[1].set_ylabel('Number of Sequences', fontsize=12)\n",
        "axes[1].set_title('Sequence Count Comparison', fontsize=13, fontweight='bold')\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "for bar, cnt in zip(bars2, seq_counts):\n",
        "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "                str(cnt), ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Aethon Health -- Data Loading Optimization', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_23",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Thought Questions:**\n",
        "1. Why do we set the mask to 0 for separator tokens? What would happen if we computed loss on them?\n",
        "2. The packer uses a greedy first-fit strategy. Could a bin-packing algorithm (e.g., first-fit-decreasing) achieve better efficiency? At what cost?\n",
        "3. What happens to attention across report boundaries in a packed sequence? Do we need to modify the attention mask?\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Model Architecture\n",
        "\n",
        "We build a simplified GPT-style decoder-only Transformer for demonstration. In production, Aethon would use a 500M parameter model (24 layers, 16 heads, d_model=1024). Our demo uses a smaller model to train quickly on Colab."
      ]
    },
    {
      "id": "cell_24",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO 4: Build the Transformer Language Model\n",
        "\n",
        "Complete the `__init__` and `forward` methods. The architecture should include:\n",
        "- Token embedding + positional embedding\n",
        "- Pre-norm Transformer encoder layers with causal masking\n",
        "- Final layer norm + output projection (weight-tied with embedding)\n",
        "- Causal (autoregressive) attention mask"
      ]
    },
    {
      "id": "cell_25",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class SimpleTransformerLM(nn.Module):\n",
        "    \"\"\"\n",
        "    GPT-style decoder-only Transformer language model.\n",
        "\n",
        "    Architecture:\n",
        "    1. Token embedding (vocab_size -> d_model)\n",
        "    2. Positional embedding (context_length -> d_model)\n",
        "    3. Dropout\n",
        "    4. N Transformer layers (pre-norm, causal self-attention)\n",
        "    5. Final LayerNorm\n",
        "    6. Output projection (d_model -> vocab_size, weight-tied)\n",
        "\n",
        "    Args:\n",
        "        vocab_size: number of tokens in vocabulary\n",
        "        d_model: hidden dimension (default 256)\n",
        "        n_heads: number of attention heads (default 4)\n",
        "        n_layers: number of transformer layers (default 4)\n",
        "        context_length: maximum sequence length (default 128)\n",
        "        dropout: dropout probability (default 0.1)\n",
        "\n",
        "    Forward:\n",
        "        Input: x of shape (batch, seq_len) -- token IDs\n",
        "        Output: logits of shape (batch, seq_len, vocab_size)\n",
        "\n",
        "    Hints:\n",
        "    - Use nn.Embedding for both token and positional embeddings\n",
        "    - Use nn.TransformerEncoderLayer with norm_first=True and batch_first=True\n",
        "    - Use nn.TransformerEncoder to stack layers\n",
        "    - Create a causal mask using torch.triu (upper triangular = True)\n",
        "    - Weight tying: set self.head.weight = self.token_emb.weight\n",
        "    - In forward: create position indices with torch.arange\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model=256, n_heads=4, n_layers=4,\n",
        "                 context_length=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # YOUR CODE HERE\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # YOUR CODE HERE\n",
        "        pass"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_26",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Verification: test model architecture\n",
        "VOCAB_SIZE = 5000\n",
        "CONTEXT_LENGTH = 128\n",
        "\n",
        "model = SimpleTransformerLM(\n",
        "    VOCAB_SIZE, d_model=256, n_heads=4,\n",
        "    n_layers=4, context_length=CONTEXT_LENGTH\n",
        ").to(device)\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model parameters: {num_params:,}\")\n",
        "\n",
        "# Test forward pass\n",
        "test_input = torch.randint(0, VOCAB_SIZE, (2, CONTEXT_LENGTH)).to(device)\n",
        "test_output = model(test_input)\n",
        "assert test_output.shape == (2, CONTEXT_LENGTH, VOCAB_SIZE), \\\n",
        "    f\"Expected (2, {CONTEXT_LENGTH}, {VOCAB_SIZE}), got {test_output.shape}\"\n",
        "print(f\"Forward pass: input {test_input.shape} -> output {test_output.shape}\")\n",
        "print(\"Architecture verification passed.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_27",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. Training Data Preparation\n",
        "\n",
        "We create a simulated tokenized dataset with Zipf-distributed token frequencies (which mimics real text) and a dataset class that supports masked loss for packed sequences."
      ]
    },
    {
      "id": "cell_28",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simulate medical tokenized data with Zipf distribution\n",
        "# (few common words like 'the', 'is', 'no'; many rare medical terms)\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "num_tokens = 200_000\n",
        "token_probs = 1.0 / np.arange(1, VOCAB_SIZE + 1)\n",
        "token_probs /= token_probs.sum()\n",
        "all_tokens = np.random.choice(VOCAB_SIZE, size=num_tokens, p=token_probs)\n",
        "\n",
        "# Create masks: simulate separator tokens every ~100 tokens\n",
        "masks = np.ones(num_tokens, dtype=np.float32)\n",
        "for i in range(0, num_tokens, 100):\n",
        "    masks[i] = 0.0  # separator tokens -- don't compute loss\n",
        "\n",
        "\n",
        "class MaskedTextDataset(Dataset):\n",
        "    \"\"\"Dataset for packed sequences with loss masking.\"\"\"\n",
        "\n",
        "    def __init__(self, tokens, masks, context_length):\n",
        "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "        self.masks = torch.tensor(masks, dtype=torch.float)\n",
        "        self.context_length = context_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens) - self.context_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.tokens[idx:idx + self.context_length]        # input\n",
        "        y = self.tokens[idx + 1:idx + self.context_length + 1]  # target (shifted by 1)\n",
        "        m = self.masks[idx + 1:idx + self.context_length + 1]   # mask for target\n",
        "        return x, y, m\n",
        "\n",
        "\n",
        "# Train/val split\n",
        "BATCH_SIZE = 16\n",
        "split = int(0.9 * num_tokens)\n",
        "train_ds = MaskedTextDataset(all_tokens[:split], masks[:split], CONTEXT_LENGTH)\n",
        "val_ds = MaskedTextDataset(all_tokens[split:], masks[split:], CONTEXT_LENGTH)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
        "\n",
        "print(f\"Training samples: {len(train_ds):,}\")\n",
        "print(f\"Validation samples: {len(val_ds):,}\")\n",
        "print(f\"Batches per epoch: {len(train_loader):,}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_29",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. Masked Cross-Entropy Loss\n",
        "\n",
        "In packed sequences, we must not compute loss on padding tokens or separator tokens. The masked loss function applies a binary mask to the per-token cross-entropy:\n",
        "\n",
        "$$\\mathcal{L}_{\\text{masked}} = -\\frac{1}{\\sum_t m_t} \\sum_{t=1}^{T} m_t \\cdot \\log P_\\theta(x_t \\mid x_{<t})$$\n",
        "\n",
        "### TODO 5: Implement Masked Cross-Entropy Loss"
      ]
    },
    {
      "id": "cell_30",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def masked_cross_entropy(logits, targets, mask):\n",
        "    \"\"\"\n",
        "    Cross-entropy loss with mask for packed sequences.\n",
        "\n",
        "    Args:\n",
        "        logits: model output, shape (B, T, V) where V is vocab size\n",
        "        targets: target token IDs, shape (B, T)\n",
        "        mask: binary mask, shape (B, T). 1 = compute loss, 0 = ignore.\n",
        "\n",
        "    Returns:\n",
        "        scalar loss: average cross-entropy over masked positions only\n",
        "\n",
        "    Hints:\n",
        "    - Use F.cross_entropy with reduction='none' to get per-token losses\n",
        "    - Reshape logits to (B*T, V) and targets to (B*T) for cross_entropy\n",
        "    - Reshape back to (B, T) then multiply by mask\n",
        "    - Divide by mask.sum() (use .clamp(min=1) to avoid division by zero)\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "\n",
        "# Verification\n",
        "test_logits = torch.randn(2, 10, VOCAB_SIZE)\n",
        "test_targets = torch.randint(0, VOCAB_SIZE, (2, 10))\n",
        "test_mask = torch.tensor([[1,1,1,0,0,0,0,0,0,0],\n",
        "                          [1,1,1,1,1,0,0,0,0,0]], dtype=torch.float)\n",
        "\n",
        "loss = masked_cross_entropy(test_logits, test_targets, test_mask)\n",
        "assert loss.ndim == 0, \"Loss should be a scalar\"\n",
        "assert loss.item() > 0, \"Loss should be positive\"\n",
        "print(f\"Masked loss: {loss.item():.4f}\")\n",
        "\n",
        "# Verify masking works: loss with all-zero mask should be 0\n",
        "zero_mask = torch.zeros(2, 10)\n",
        "zero_loss = masked_cross_entropy(test_logits, test_targets, zero_mask)\n",
        "assert zero_loss.item() == 0.0, \"Loss with zero mask should be 0\"\n",
        "print(\"Masked cross-entropy verification passed.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_31",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 8. Learning Rate Schedule: Warmup + Cosine Decay\n",
        "\n",
        "Training from scratch requires careful learning rate scheduling:\n",
        "- **Linear warmup** (2000 steps): prevents large early updates from destabilizing randomly initialized weights\n",
        "- **Cosine decay**: smoothly reduces the learning rate, allowing fine-grained convergence\n",
        "\n",
        "### TODO 6: Implement the Learning Rate Schedule"
      ]
    },
    {
      "id": "cell_32",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_lr(step, warmup_steps, total_steps, lr_max=3e-4, lr_min=1e-5):\n",
        "    \"\"\"\n",
        "    Compute learning rate with linear warmup + cosine decay.\n",
        "\n",
        "    Args:\n",
        "        step: current training step\n",
        "        warmup_steps: number of warmup steps\n",
        "        total_steps: total number of training steps\n",
        "        lr_max: peak learning rate (default 3e-4)\n",
        "        lr_min: minimum learning rate (default 1e-5)\n",
        "\n",
        "    Returns:\n",
        "        float: learning rate for this step\n",
        "\n",
        "    Schedule:\n",
        "    - Steps [0, warmup_steps): linear ramp from 0 to lr_max\n",
        "    - Steps [warmup_steps, total_steps]: cosine decay from lr_max to lr_min\n",
        "\n",
        "    Formulas:\n",
        "    - Warmup: lr = lr_max * step / warmup_steps\n",
        "    - Cosine: lr = lr_min + 0.5 * (lr_max - lr_min) * (1 + cos(pi * progress))\n",
        "      where progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "\n",
        "# Verification + visualization\n",
        "total_steps = len(train_loader) * 10  # 10 epochs\n",
        "warmup_steps = int(0.1 * total_steps)\n",
        "\n",
        "steps = range(total_steps)\n",
        "lrs = [get_lr(s, warmup_steps, total_steps) for s in steps]\n",
        "\n",
        "assert abs(lrs[0] - 0.0) < 1e-8, \"LR at step 0 should be ~0\"\n",
        "assert abs(lrs[warmup_steps] - 3e-4) < 1e-6, \"LR at end of warmup should be lr_max\"\n",
        "assert lrs[-1] >= 1e-5, \"LR at end should be >= lr_min\"\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(steps, lrs, linewidth=2, color='#2ecc71')\n",
        "plt.axvline(x=warmup_steps, color='gray', linestyle='--', label=f'End warmup ({warmup_steps})')\n",
        "plt.xlabel('Step', fontsize=12)\n",
        "plt.ylabel('Learning Rate', fontsize=12)\n",
        "plt.title('Warmup + Cosine Decay Schedule', fontsize=13, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"LR schedule verification passed.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_33",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Thought Questions:**\n",
        "1. Why does the warmup prevent training instability? What would happen with a large LR from step 0?\n",
        "2. Why cosine decay instead of a step schedule or linear decay?\n",
        "3. How would you choose lr_max for a 500M parameter model vs. our small demo model?\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Complete Training Loop\n",
        "\n",
        "The training loop combines all pipeline components:\n",
        "- **AdamW optimizer** with beta2=0.95 (better for language models than default 0.999)\n",
        "- **Gradient clipping** at max_norm=1.0 to prevent gradient spikes\n",
        "- **Masked loss** for packed sequences\n",
        "- **Learning rate scheduling** with warmup + cosine decay\n",
        "- **Metric tracking** for stability monitoring"
      ]
    },
    {
      "id": "cell_34",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training configuration\n",
        "EPOCHS = 10\n",
        "LR_MAX = 3e-4\n",
        "LR_MIN = 1e-5\n",
        "WEIGHT_DECAY = 0.1\n",
        "MAX_GRAD_NORM = 1.0\n",
        "BETAS = (0.9, 0.95)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(), lr=LR_MAX,\n",
        "    betas=BETAS, weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "warmup_steps = int(0.1 * total_steps)\n",
        "\n",
        "# Tracking metrics\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "learning_rates = []\n",
        "grad_norms = []\n",
        "\n",
        "print(f\"Training for {EPOCHS} epochs ({total_steps:,} steps)\")\n",
        "print(f\"Warmup: {warmup_steps} steps\")\n",
        "print(f\"Optimizer: AdamW (lr={LR_MAX}, betas={BETAS}, wd={WEIGHT_DECAY})\")\n",
        "print(f\"Gradient clipping: max_norm={MAX_GRAD_NORM}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "step = 0\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch_x, batch_y, batch_m in train_loader:\n",
        "        batch_x = batch_x.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "        batch_m = batch_m.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(batch_x)\n",
        "        loss = masked_cross_entropy(logits, batch_y, batch_m)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        total_norm = torch.nn.utils.clip_grad_norm_(\n",
        "            model.parameters(), max_norm=MAX_GRAD_NORM\n",
        "        )\n",
        "        grad_norms.append(total_norm.item())\n",
        "\n",
        "        # Learning rate update\n",
        "        lr = get_lr(step, warmup_steps, total_steps, LR_MAX, LR_MIN)\n",
        "        for pg in optimizer.param_groups:\n",
        "            pg['lr'] = lr\n",
        "        learning_rates.append(lr)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "        epoch_loss += loss.item()\n",
        "        step += 1\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss_sum = 0\n",
        "    val_count = 0\n",
        "    with torch.no_grad():\n",
        "        for vx, vy, vm in val_loader:\n",
        "            vx, vy, vm = vx.to(device), vy.to(device), vm.to(device)\n",
        "            vlogits = model(vx)\n",
        "            vloss = masked_cross_entropy(vlogits, vy, vm)\n",
        "            val_loss_sum += vloss.item()\n",
        "            val_count += 1\n",
        "\n",
        "    avg_val = val_loss_sum / max(val_count, 1)\n",
        "    val_losses.append(avg_val)\n",
        "    avg_train = epoch_loss / len(train_loader)\n",
        "    ppl = math.exp(min(avg_val, 20))  # cap to avoid overflow\n",
        "    mean_grad = np.mean(grad_norms[-len(train_loader):])\n",
        "\n",
        "    print(f\"Epoch {epoch+1:>2d}/{EPOCHS} | \"\n",
        "          f\"Train: {avg_train:.4f} | Val: {avg_val:.4f} | \"\n",
        "          f\"PPL: {ppl:.1f} | LR: {lr:.2e} | \"\n",
        "          f\"Grad Norm: {mean_grad:.2f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_35",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 10. Training Stability Analysis\n",
        "\n",
        "With a $12,000 budget and no room for restarts, training stability is critical. We monitor four key signals:\n",
        "1. **Training loss curve** -- should decrease smoothly\n",
        "2. **Validation loss + perplexity** -- should track training loss without diverging\n",
        "3. **Learning rate schedule** -- confirms warmup + decay are working\n",
        "4. **Gradient norms** -- spikes indicate instability"
      ]
    },
    {
      "id": "cell_36",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Training loss (raw + smoothed)\n",
        "ax = axes[0, 0]\n",
        "ax.plot(train_losses, linewidth=0.5, alpha=0.3, color='#3498db', label='Raw')\n",
        "window = max(1, len(train_losses) // 20)\n",
        "smoothed = np.convolve(train_losses, np.ones(window)/window, mode='valid')\n",
        "ax.plot(range(window-1, len(train_losses)), smoothed,\n",
        "        linewidth=2, color='#2c3e50', label='Smoothed')\n",
        "ax.set_xlabel('Step')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Training Loss', fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "# 2. Validation loss + perplexity\n",
        "ax = axes[0, 1]\n",
        "ax2 = ax.twinx()\n",
        "epochs_x = range(1, len(val_losses) + 1)\n",
        "ax.plot(epochs_x, val_losses, 'o-', color='#e74c3c', linewidth=2, label='Val Loss')\n",
        "ppls = [math.exp(min(v, 20)) for v in val_losses]\n",
        "ax2.plot(epochs_x, ppls, 's--', color='#9b59b6', linewidth=2, label='Perplexity')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Validation Loss', color='#e74c3c')\n",
        "ax2.set_ylabel('Perplexity', color='#9b59b6')\n",
        "ax.set_title('Validation Metrics', fontweight='bold')\n",
        "ax.grid(alpha=0.3)\n",
        "lines1, labels1 = ax.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "\n",
        "# 3. Learning rate schedule\n",
        "ax = axes[1, 0]\n",
        "ax.plot(learning_rates, linewidth=2, color='#2ecc71')\n",
        "ax.set_xlabel('Step')\n",
        "ax.set_ylabel('Learning Rate')\n",
        "ax.set_title('LR Schedule (Warmup + Cosine)', fontweight='bold')\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "# 4. Gradient norms\n",
        "ax = axes[1, 1]\n",
        "ax.plot(grad_norms, linewidth=0.5, alpha=0.5, color='#e67e22')\n",
        "ax.axhline(y=MAX_GRAD_NORM, color='red', linestyle='--', linewidth=1,\n",
        "           label=f'Clip threshold ({MAX_GRAD_NORM})')\n",
        "mean_gn = np.mean(grad_norms)\n",
        "ax.axhline(y=mean_gn, color='black', linestyle=':', linewidth=1,\n",
        "           label=f'Mean ({mean_gn:.2f})')\n",
        "ax.set_xlabel('Step')\n",
        "ax.set_ylabel('Gradient Norm')\n",
        "ax.set_title('Gradient Norms (Stability Monitor)', fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "plt.suptitle('Aethon Health -- Training Pipeline Monitoring Dashboard',\n",
        "             fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Stability report\n",
        "spike_threshold = 5 * np.mean(grad_norms)\n",
        "num_spikes = sum(1 for g in grad_norms if g > spike_threshold)\n",
        "print(f\"\\nStability Report:\")\n",
        "print(f\"  Final validation loss: {val_losses[-1]:.4f}\")\n",
        "print(f\"  Final perplexity: {math.exp(min(val_losses[-1], 20)):.1f}\")\n",
        "print(f\"  Mean gradient norm: {np.mean(grad_norms):.3f}\")\n",
        "print(f\"  Max gradient norm: {np.max(grad_norms):.3f}\")\n",
        "print(f\"  Gradient spikes (>5x mean): {num_spikes}\")\n",
        "print(f\"  Training stable: {'Yes' if num_spikes < 10 else 'WARNING - investigate!'}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_37",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO 7: Gradient Clipping Ablation\n",
        "\n",
        "Train a second model WITHOUT gradient clipping and compare the gradient norm distributions. This demonstrates why clipping is essential for training stability."
      ]
    },
    {
      "id": "cell_38",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train_with_ablation(clip_grad=True, max_norm=1.0, epochs=3):\n",
        "    \"\"\"\n",
        "    Train a model with or without gradient clipping.\n",
        "\n",
        "    Args:\n",
        "        clip_grad: bool, whether to clip gradients\n",
        "        max_norm: gradient clipping threshold (only used if clip_grad=True)\n",
        "        epochs: number of training epochs\n",
        "\n",
        "    Returns:\n",
        "        dict with 'losses' (list) and 'grad_norms' (list)\n",
        "\n",
        "    Hints:\n",
        "    - Create a fresh model and optimizer\n",
        "    - Run the training loop but track grad norms BEFORE clipping\n",
        "      (compute the norm manually with torch.nn.utils.clip_grad_norm_\n",
        "      and record it, but only actually clip if clip_grad=True)\n",
        "    - Return the tracked metrics\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "\n",
        "print(\"Running ablation: gradient clipping vs. no clipping...\")\n",
        "# results_clipped = train_with_ablation(clip_grad=True, epochs=3)\n",
        "# results_unclipped = train_with_ablation(clip_grad=False, epochs=3)\n",
        "print(\"(Uncomment the above lines to run the ablation -- takes a few minutes)\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_39",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Thought Questions:**\n",
        "1. What is the relationship between gradient clipping threshold and effective learning rate?\n",
        "2. If you observe frequent gradient spikes, what does this suggest about the data or model?\n",
        "3. How would you set the clipping threshold for a 500M parameter model? Would 1.0 still be appropriate?\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Evaluation: Perplexity and Report Quality\n",
        "\n",
        "### TODO 8: Compute Validation Perplexity"
      ]
    },
    {
      "id": "cell_40",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@torch.no_grad()\n",
        "def compute_perplexity(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Compute validation perplexity using masked loss.\n",
        "\n",
        "    Perplexity = exp(average_cross_entropy_loss)\n",
        "\n",
        "    Args:\n",
        "        model: trained language model\n",
        "        dataloader: validation DataLoader yielding (x, y, mask)\n",
        "        device: torch device\n",
        "\n",
        "    Returns:\n",
        "        tuple of (perplexity, average_loss)\n",
        "\n",
        "    Hints:\n",
        "    - Set model to eval mode\n",
        "    - Accumulate total loss and total masked token count\n",
        "    - Use masked_cross_entropy for each batch but weight by mask.sum()\n",
        "    - Final perplexity = exp(total_loss / total_tokens)\n",
        "    - Use math.exp and cap loss at 20 to avoid overflow\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "\n",
        "ppl, avg_loss = compute_perplexity(model, val_loader, device)\n",
        "print(f\"Validation Perplexity: {ppl:.2f}\")\n",
        "print(f\"Validation Loss: {avg_loss:.4f}\")\n",
        "print(f\"\\nTarget: PPL < 12.0 -- {'MET' if ppl < 12.0 else 'NOT MET'}\")\n",
        "print(f\"(Note: on synthetic data, absolute PPL is not meaningful.\")\n",
        "print(f\" What matters is the training pipeline works correctly.)\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_41",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 12. Report Generation (Qualitative)\n",
        "\n",
        "We implement autoregressive text generation using the trained model. The generation function uses **top-p (nucleus) sampling** for diverse yet coherent text."
      ]
    },
    {
      "id": "cell_42",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generate(model, start_tokens, max_length=100, temperature=0.8, top_p=0.9):\n",
        "    \"\"\"\n",
        "    Generate text autoregressively with top-p sampling.\n",
        "\n",
        "    Args:\n",
        "        model: trained language model\n",
        "        start_tokens: list of starting token IDs\n",
        "        max_length: maximum tokens to generate\n",
        "        temperature: sampling temperature (lower = more deterministic)\n",
        "        top_p: nucleus sampling threshold\n",
        "\n",
        "    Returns:\n",
        "        list of generated token IDs (including start tokens)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    tokens = list(start_tokens)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Use the last context_length tokens\n",
        "            ctx = tokens[-model.context_length:] if hasattr(model, 'context_length') else tokens[-128:]\n",
        "            x = torch.tensor([ctx], dtype=torch.long).to(device)\n",
        "            logits = model(x)[:, -1, :]  # last position\n",
        "\n",
        "            # Temperature scaling\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Top-p (nucleus) sampling\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "            # Remove tokens with cumulative probability above threshold\n",
        "            sorted_mask = cumulative_probs - sorted_probs > top_p\n",
        "            sorted_probs[sorted_mask] = 0.0\n",
        "            sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "            # Sample\n",
        "            idx = torch.multinomial(sorted_probs, num_samples=1)\n",
        "            next_token = sorted_indices.gather(-1, idx).item()\n",
        "            tokens.append(next_token)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# Generate from different starting points\n",
        "print(\"Generating sequences from trained model:\")\n",
        "print(\"(Token IDs -- in production these would decode to medical text)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, start in enumerate([[1, 5, 10], [42, 100, 200], [3, 7, 15]]):\n",
        "    generated = generate(model, start, max_length=30, temperature=0.8)\n",
        "    print(f\"\\nSequence {i+1}: {generated[:20]}...\")\n",
        "    print(f\"  Length: {len(generated)} tokens\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_43",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO 9: Inference Latency Profiling\n",
        "\n",
        "Aethon's requirement: reports must be generated in under 3 seconds on a single A100 GPU. Profile the inference latency of our model."
      ]
    },
    {
      "id": "cell_44",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def profile_latency(model, device, sequence_lengths=[32, 64, 128, 256],\n",
        "                    num_runs=50):\n",
        "    \"\"\"\n",
        "    Profile inference latency across different sequence lengths.\n",
        "\n",
        "    For each sequence length:\n",
        "    1. Generate a random input of that length\n",
        "    2. Run num_runs forward passes\n",
        "    3. Record latency for each run\n",
        "    4. Report mean, p50, p95, p99\n",
        "\n",
        "    Args:\n",
        "        model: trained language model\n",
        "        device: torch device\n",
        "        sequence_lengths: list of sequence lengths to test\n",
        "        num_runs: number of forward passes per length\n",
        "\n",
        "    Returns:\n",
        "        dict mapping seq_length -> dict of latency stats\n",
        "\n",
        "    Hints:\n",
        "    - Set model to eval mode and use torch.no_grad()\n",
        "    - Use torch.cuda.synchronize() before timing on GPU\n",
        "    - Use time.perf_counter() for high-resolution timing\n",
        "    - Do a warmup run first (discard the first measurement)\n",
        "    - Report times in milliseconds\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "\n",
        "# latency_results = profile_latency(model, device)\n",
        "# (Uncomment to run -- provides meaningful results on GPU)\n",
        "print(\"Latency profiling: uncomment the above line to run.\")\n",
        "print(\"On a T4 GPU, expect ~5-15ms per forward pass for our small model.\")\n",
        "print(\"The production 500M model on A100 targets <3s for 512-token generation.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_45",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 13. Results Summary and Business Impact\n",
        "\n",
        "Let us compile all metrics and evaluate against Aethon Health's requirements."
      ]
    },
    {
      "id": "cell_46",
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\" * 65)\n",
        "print(\"  AETHON HEALTH -- TRAINING PIPELINE ENGINEERING RESULTS\")\n",
        "print(\"=\" * 65)\n",
        "print()\n",
        "print(f\"  Model: {num_params:,} parameters (demo)\")\n",
        "print(f\"  Production target: ~500M parameters\")\n",
        "print(f\"  Training epochs: {EPOCHS}\")\n",
        "print(f\"  Final validation loss: {val_losses[-1]:.4f}\")\n",
        "print(f\"  Final perplexity: {math.exp(min(val_losses[-1], 20)):.1f}\")\n",
        "print()\n",
        "print(\"  Pipeline Components:\")\n",
        "print(\"  [x] Domain-specific BPE tokenizer (2x token reduction)\")\n",
        "print(\"  [x] Sequence packing (>90% efficiency vs. ~58% naive)\")\n",
        "print(\"  [x] Masked cross-entropy loss (packed sequence support)\")\n",
        "print(\"  [x] AdamW optimizer (beta2=0.95, weight_decay=0.1)\")\n",
        "print(\"  [x] Warmup + cosine decay learning rate schedule\")\n",
        "print(\"  [x] Gradient clipping (max_norm=1.0)\")\n",
        "print(\"  [x] Training stability monitoring dashboard\")\n",
        "print()\n",
        "print(\"  Business Impact (if deployed with 500M model):\")\n",
        "print(\"  - Report time: 12 min -> 4 min for complex cases\")\n",
        "print(\"  - Addressable market: 800 -> 2,200 hospitals\")\n",
        "print(\"  - Estimated ARR uplift: $8.4M in 18 months\")\n",
        "print()\n",
        "print(\"  Key Insight:\")\n",
        "print(\"  The Transformer architecture is well-understood. The real\")\n",
        "print(\"  engineering challenge -- and the competitive advantage -- is\")\n",
        "print(\"  the training pipeline. Domain tokenization reduced sequence\")\n",
        "print(\"  length by ~2x, packing eliminated 40%+ wasted compute, and\")\n",
        "print(\"  careful optimization kept training stable within budget.\")\n",
        "print(\"=\" * 65)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cell_47",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 14. Production Extension: Scaling and Deployment\n",
        "\n",
        "### Deployment Architecture\n",
        "\n",
        "```\n",
        "RadAssist v2 Pipeline\n",
        "                                                               \n",
        "  CV Module    ->  Finding     ->  Medical LM         \n",
        "  (Images)         Extractor       (500M params)      \n",
        "                                   Context: 512       \n",
        "                                   Latency: <3s       \n",
        "                                       |              \n",
        "                                  Post-Processing     \n",
        "                                  - Format check      \n",
        "                                  - Term validation    \n",
        "                                  - Confidence score   \n",
        "                                       |              \n",
        "                                  Radiologist Review   \n",
        "                                  (Edit + Approve)     \n",
        "```\n",
        "\n",
        "### Inference Optimization Roadmap\n",
        "\n",
        "| Technique | Speedup | Memory Reduction | Complexity |\n",
        "|-----------|---------|-----------------|------------|\n",
        "| KV Cache | 2-5x | +50% memory | Low |\n",
        "| INT8 Quantization | 1.5-2x | 4x reduction | Medium |\n",
        "| Speculative Decoding | 2-3x | +10% memory | High |\n",
        "| Flash Attention | 2-4x | 5-20x reduction | Medium |\n",
        "\n",
        "### Monitoring in Production\n",
        "\n",
        "| Metric | Threshold | Action |\n",
        "|--------|-----------|--------|\n",
        "| Inference latency (p99) | > 3.5s | Scale GPU instances |\n",
        "| Report rejection rate | > 15% | Retrain or fine-tune |\n",
        "| Unknown token rate | > 0.5% | Retrain tokenizer |\n",
        "| Validation perplexity | > 15.0 | Early stop, investigate |\n",
        "\n",
        "### Ethical Considerations\n",
        "\n",
        "- All training data is de-identified per HIPAA Safe Harbor guidelines\n",
        "- The model generates **draft** reports only -- a licensed radiologist must review and approve every report\n",
        "- Confidence scores flag uncertain reports for immediate human review\n",
        "- Regular bias audits ensure equitable performance across demographics\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "In this case study, you engineered a complete LLM training pipeline:\n",
        "\n",
        "1. **Domain tokenization**: Trained BPE on medical text, achieving ~2x token reduction over GPT-2\n",
        "2. **Sequence packing**: Implemented variable-length report packing, improving token utilization from ~58% to >90%\n",
        "3. **Masked loss**: Built a cross-entropy loss that correctly ignores padding and separator tokens\n",
        "4. **Learning rate scheduling**: Implemented warmup + cosine decay for stable training from scratch\n",
        "5. **Training loop**: Combined AdamW, gradient clipping, and all components into a production-grade loop\n",
        "6. **Stability analysis**: Built a monitoring dashboard to detect gradient spikes and divergence\n",
        "7. **Evaluation**: Computed perplexity and profiled inference latency\n",
        "\n",
        "The key takeaway: **the training pipeline is the competitive advantage**, not the architecture. Every component -- from tokenization to optimization -- directly impacts whether a $12,000 training run succeeds or fails."
      ]
    }
  ]
}
