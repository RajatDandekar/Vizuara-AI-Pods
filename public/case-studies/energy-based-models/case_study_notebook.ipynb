{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Energy Based Models Case Study -- Implementation Notebook"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy-Based Anomaly Detection in Particle Physics -- Implementation Notebook\n",
    "\n",
    "## Setup and Data Generation\n",
    "\n",
    "We simulate particle physics collision data with known background distributions and injected anomalies."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data Generation (Simulated Particle Physics Events)"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_background_events(n_events, n_features=64):\n",
    "    \"\"\"\n",
    "    Generate simulated Standard Model background events.\n",
    "\n",
    "    The background is modeled as a mixture of correlated Gaussians,\n",
    "    representing different known physics processes (QCD jets, W/Z bosons, etc.).\n",
    "    \"\"\"\n",
    "    n_processes = 4  # QCD, W+jets, Z+jets, ttbar\n",
    "    process_weights = [0.6, 0.15, 0.15, 0.10]\n",
    "\n",
    "    events = []\n",
    "    for proc_idx in range(n_processes):\n",
    "        n_proc = int(n_events * process_weights[proc_idx])\n",
    "\n",
    "        # Each process has a characteristic mean and covariance\n",
    "        mean = np.random.randn(n_features) * (proc_idx + 1) * 0.3\n",
    "        # Create a random positive-definite covariance matrix\n",
    "        A = np.random.randn(n_features, n_features) * 0.1\n",
    "        cov = A @ A.T + np.eye(n_features) * 0.5\n",
    "\n",
    "        proc_events = np.random.multivariate_normal(mean, cov, size=n_proc)\n",
    "        events.append(proc_events)\n",
    "\n",
    "    events = np.concatenate(events, axis=0)\n",
    "    np.random.shuffle(events)\n",
    "    return events[:n_events]\n",
    "\n",
    "def generate_anomaly_events(n_events, n_features=64):\n",
    "    \"\"\"\n",
    "    Generate anomalous events (simulating Beyond Standard Model physics).\n",
    "\n",
    "    Anomalies are generated from a distribution that is offset from\n",
    "    the background in specific feature subspaces.\n",
    "    \"\"\"\n",
    "    # Anomalies live in a different region of feature space\n",
    "    mean_shift = np.zeros(n_features)\n",
    "    mean_shift[:10] = 3.0  # Strong deviation in first 10 features\n",
    "    mean_shift[10:20] = -2.0  # Moderate deviation in next 10\n",
    "\n",
    "    anomalies = np.random.randn(n_events, n_features) * 0.5 + mean_shift\n",
    "    return anomalies\n",
    "\n",
    "# Generate datasets\n",
    "N_TRAIN = 50000\n",
    "N_TEST = 10000\n",
    "N_ANOMALIES = 100  # 1% anomaly rate in test set\n",
    "\n",
    "X_train_raw = generate_background_events(N_TRAIN)\n",
    "X_test_bg = generate_background_events(N_TEST - N_ANOMALIES)\n",
    "X_test_anom = generate_anomaly_events(N_ANOMALIES)\n",
    "\n",
    "X_test_raw = np.concatenate([X_test_bg, X_test_anom], axis=0)\n",
    "y_test = np.concatenate([np.zeros(N_TEST - N_ANOMALIES), np.ones(N_ANOMALIES)])\n",
    "\n",
    "# Shuffle test set\n",
    "idx = np.random.permutation(len(y_test))\n",
    "X_test_raw = X_test_raw[idx]\n",
    "y_test = y_test[idx]\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_raw)\n",
    "X_test = scaler.transform(X_test_raw)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "print(f\"Training set: {X_train_t.shape[0]} events (background only)\")\n",
    "print(f\"Test set: {X_test_t.shape[0]} events ({int(y_test.sum())} anomalies)\")\n",
    "print(f\"Feature dimension: {X_train_t.shape[1]}\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Exploratory Data Analysis"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_test_pca = pca.fit_transform(X_test)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training data PCA\n",
    "X_train_pca = pca.transform(X_train)\n",
    "axes[0].scatter(X_train_pca[:, 0], X_train_pca[:, 1], s=1, alpha=0.1, c='steelblue')\n",
    "axes[0].set_title('Training Data (Background Only)', fontsize=13)\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "\n",
    "# Test data with labels\n",
    "bg_mask = y_test == 0\n",
    "axes[1].scatter(X_test_pca[bg_mask, 0], X_test_pca[bg_mask, 1],\n",
    "                s=1, alpha=0.1, c='steelblue', label='Background')\n",
    "axes[1].scatter(X_test_pca[~bg_mask, 0], X_test_pca[~bg_mask, 1],\n",
    "                s=20, alpha=0.8, c='red', marker='x', label='Anomaly')\n",
    "axes[1].set_title('Test Data (with Anomalies)', fontsize=13)\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_[:2].sum():.2%}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distribution comparison\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.hist(X_test[bg_mask, i], bins=50, alpha=0.5, density=True,\n",
    "            label='Background', color='steelblue')\n",
    "    ax.hist(X_test[~bg_mask, i], bins=30, alpha=0.5, density=True,\n",
    "            label='Anomaly', color='red')\n",
    "    ax.set_title(f'Feature {i}', fontsize=11)\n",
    "    if i == 0:\n",
    "        ax.legend(fontsize=9)\n",
    "plt.suptitle('Feature Distributions: Background vs Anomaly', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"First 10 features show the strongest separation between background and anomalies.\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Baseline: Autoencoder"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"Autoencoder baseline for anomaly detection.\"\"\"\n",
    "    def __init__(self, input_dim=64, latent_dim=8):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 128), nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "\n",
    "# Train autoencoder\n",
    "ae = Autoencoder(input_dim=64, latent_dim=8).to(device)\n",
    "ae_optimizer = torch.optim.Adam(ae.parameters(), lr=1e-3)\n",
    "ae_losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    idx = torch.randperm(len(X_train_t))[:512]\n",
    "    batch = X_train_t[idx].to(device)\n",
    "\n",
    "    recon = ae(batch)\n",
    "    loss = ((recon - batch) ** 2).sum(dim=-1).mean()\n",
    "\n",
    "    ae_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    ae_optimizer.step()\n",
    "    ae_losses.append(loss.item())\n",
    "\n",
    "print(f\"Autoencoder final loss: {ae_losses[-1]:.4f}\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder anomaly scores (reconstruction error)\n",
    "with torch.no_grad():\n",
    "    ae_recon = ae(X_test_t.to(device))\n",
    "    ae_scores = ((ae_recon - X_test_t.to(device)) ** 2).sum(dim=-1).cpu().numpy()\n",
    "\n",
    "ae_auroc = roc_auc_score(y_test, ae_scores)\n",
    "ae_auprc = average_precision_score(y_test, ae_scores)\n",
    "print(f\"Autoencoder AUROC: {ae_auroc:.4f}\")\n",
    "print(f\"Autoencoder AUPRC: {ae_auprc:.4f}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Energy-Based Model: Score Network"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleScoreNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Noise-conditioned score network for particle physics events.\n",
    "    Predicts s_theta(x, sigma) = -grad_x E(x).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=64, hidden_dim=256, n_sigmas=10):\n",
    "        super().__init__()\n",
    "        self.sigma_embed = nn.Embedding(n_sigmas, hidden_dim)\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(nn.SiLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "            for _ in range(3)\n",
    "        ])\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, sigma_idx):\n",
    "        h = self.input_proj(x) + self.sigma_embed(sigma_idx)\n",
    "        for block in self.blocks:\n",
    "            h = h + block(h)  # residual connections\n",
    "        return self.output(h)"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Training with Multi-Scale DSM"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise schedule\n",
    "N_SIGMAS = 10\n",
    "sigmas = torch.exp(torch.linspace(np.log(5.0), np.log(0.01), N_SIGMAS)).to(device)\n",
    "print(\"Noise schedule:\", [f\"{s:.4f}\" for s in sigmas.cpu()])\n",
    "\n",
    "# Train score network\n",
    "model = ParticleScoreNet(input_dim=64, hidden_dim=256, n_sigmas=N_SIGMAS).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "    idx = torch.randperm(len(X_train_t))[:512]\n",
    "    x = X_train_t[idx].to(device)\n",
    "\n",
    "    # Random noise level per sample\n",
    "    sigma_idx = torch.randint(0, N_SIGMAS, (len(x),)).to(device)\n",
    "    sigma = sigmas[sigma_idx].unsqueeze(-1)\n",
    "\n",
    "    # Add noise\n",
    "    noise = torch.randn_like(x)\n",
    "    x_noisy = x + sigma * noise\n",
    "\n",
    "    # DSM target and prediction\n",
    "    target = -noise / sigma\n",
    "    pred = model(x_noisy, sigma_idx)\n",
    "\n",
    "    # Weighted MSE\n",
    "    weights = sigma.squeeze() ** 2\n",
    "    loss = (weights * ((pred - target) ** 2).sum(dim=-1)).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"Epoch {epoch+1:5d} | Loss: {loss.item():.4f}\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.semilogy(train_losses, alpha=0.3, linewidth=0.5)\n",
    "smoothed = np.convolve(train_losses, np.ones(30)/30, mode='valid')\n",
    "plt.semilogy(smoothed, linewidth=2, color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Multi-Scale DSM Loss')\n",
    "plt.title('Score Network Training')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Anomaly Scoring and Evaluation"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score_anomaly(model, X, sigmas, n_sigmas_use=3):\n",
    "    \"\"\"Compute anomaly scores using score norm at low noise levels.\"\"\"\n",
    "    model.eval()\n",
    "    scores = np.zeros(len(X))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X), 256):\n",
    "            batch = X[i:i+256].to(device)\n",
    "            batch_scores = torch.zeros(len(batch), device=device)\n",
    "\n",
    "            # Average score norm across lowest noise levels\n",
    "            for sigma_i in range(n_sigmas_use):\n",
    "                sigma_idx = torch.full((len(batch),),\n",
    "                                       N_SIGMAS - 1 - sigma_i, dtype=torch.long).to(device)\n",
    "                s = model(batch, sigma_idx)\n",
    "                batch_scores += (s ** 2).sum(dim=-1)\n",
    "\n",
    "            batch_scores /= n_sigmas_use\n",
    "            scores[i:i+len(batch)] = batch_scores.cpu().numpy()\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Compute scores\n",
    "ebm_scores = compute_score_anomaly(model, X_test_t, sigmas)\n",
    "\n",
    "ebm_auroc = roc_auc_score(y_test, ebm_scores)\n",
    "ebm_auprc = average_precision_score(y_test, ebm_scores)\n",
    "\n",
    "print(f\"\\nResults Comparison:\")\n",
    "print(f\"{'Method':<25} {'AUROC':>8} {'AUPRC':>8}\")\n",
    "print(\"-\" * 43)\n",
    "print(f\"{'Autoencoder':<25} {ae_auroc:>8.4f} {ae_auprc:>8.4f}\")\n",
    "print(f\"{'EBM (Score Norm)':<25} {ebm_auroc:>8.4f} {ebm_auprc:>8.4f}\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC\n",
    "for scores, name, color in [(ae_scores, 'Autoencoder', 'steelblue'),\n",
    "                              (ebm_scores, 'EBM Score', 'coral')]:\n",
    "    fpr, tpr, _ = roc_curve(y_test, scores)\n",
    "    auroc = roc_auc_score(y_test, scores)\n",
    "    axes[0].plot(fpr, tpr, label=f'{name} (AUROC={auroc:.3f})', color=color, linewidth=2)\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Score distributions\n",
    "axes[1].hist(ebm_scores[y_test == 0], bins=50, alpha=0.5, density=True,\n",
    "             label='Background', color='steelblue')\n",
    "axes[1].hist(ebm_scores[y_test == 1], bins=30, alpha=0.5, density=True,\n",
    "             label='Anomaly', color='red')\n",
    "axes[1].set_xlabel('Anomaly Score')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].set_title('EBM Anomaly Score Distribution')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Error Analysis and Interpretability"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature attribution using score vectors\n",
    "def explain_anomaly(model, x_event, sigma_idx_val=9):\n",
    "    \"\"\"Explain why an event is anomalous using the score vector.\"\"\"\n",
    "    model.eval()\n",
    "    x = x_event.unsqueeze(0).to(device)\n",
    "    sigma_idx = torch.tensor([sigma_idx_val]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        score = model(x, sigma_idx)\n",
    "\n",
    "    # Score magnitude per feature = importance\n",
    "    feature_importance = (score ** 2).squeeze().cpu().numpy()\n",
    "    return feature_importance\n",
    "\n",
    "# Get top anomalies\n",
    "top_anomaly_idx = np.argsort(ebm_scores)[-10:]\n",
    "top_anomaly_events = X_test_t[top_anomaly_idx]\n",
    "\n",
    "# Feature importance for top anomaly\n",
    "importance = explain_anomaly(model, top_anomaly_events[0])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "ax.bar(range(len(importance)), importance, color='coral', alpha=0.7)\n",
    "ax.set_xlabel('Feature Index')\n",
    "ax.set_ylabel('Score Magnitude (Importance)')\n",
    "ax.set_title('Feature Attribution for Top Anomaly Event')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Features with highest score magnitude deviate most from the background.\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Deployment Optimization"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TorchScript export\n",
    "model.eval()\n",
    "example_x = torch.randn(1, 64).to(device)\n",
    "example_sigma = torch.zeros(1, dtype=torch.long).to(device)\n",
    "\n",
    "scripted = torch.jit.trace(model, (example_x, example_sigma))\n",
    "\n",
    "# Latency benchmark\n",
    "import time\n",
    "\n",
    "n_benchmark = 1000\n",
    "batch = torch.randn(1, 64).to(device)\n",
    "sigma_idx = torch.zeros(1, dtype=torch.long).to(device)\n",
    "\n",
    "# Warmup\n",
    "for _ in range(100):\n",
    "    _ = scripted(batch, sigma_idx)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "for _ in range(n_benchmark):\n",
    "    _ = scripted(batch, sigma_idx)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "elapsed = (time.time() - start) / n_benchmark * 1000\n",
    "\n",
    "print(f\"Inference latency: {elapsed:.2f} ms per event\")\n",
    "print(f\"{'PASS' if elapsed < 10 else 'FAIL'}: Target is < 10ms per event\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Ethics Discussion"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print ethical considerations\n",
    "print(\"\"\"\n",
    "ETHICAL CONSIDERATIONS FOR DEPLOYMENT\n",
    "======================================\n",
    "\n",
    "1. FALSE DISCOVERY RISK\n",
    "   A model-flagged anomaly is NOT a physics discovery. The standard\n",
    "   in particle physics is 5-sigma significance (p < 3e-7). The model\n",
    "   is a FILTER, not the final arbiter.\n",
    "\n",
    "2. REPRODUCIBILITY\n",
    "   All hyperparameters, seeds, and training data versions must be\n",
    "   logged. Results should be independently reproducible.\n",
    "\n",
    "3. TRAINING DATA BIAS\n",
    "   If the Standard Model simulation is incomplete, the model may\n",
    "   flag known physics as anomalous. Regular calibration against\n",
    "   updated simulations is essential.\n",
    "\n",
    "4. COMPUTATIONAL COST\n",
    "   Training and inference infrastructure has a carbon footprint.\n",
    "   Cost-benefit analysis should guide deployment decisions.\n",
    "\"\"\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "CASE STUDY SUMMARY\n",
    "==================\n",
    "Industry:  Particle Physics (NovaCERN Analytics)\n",
    "Problem:   Unsupervised anomaly detection in collision events\n",
    "Method:    Energy-Based Models with Score Matching\n",
    "\n",
    "Results:\n",
    "  Autoencoder AUROC: {ae_auroc:.4f}\n",
    "  EBM Score AUROC:   {ebm_auroc:.4f}\n",
    "\n",
    "Key Advantages of EBMs:\n",
    "  1. No bottleneck assumption (unlike autoencoders)\n",
    "  2. Principled density estimation via energy function\n",
    "  3. Interpretable anomaly attribution via score vectors\n",
    "  4. Direct connection to modern diffusion models\n",
    "\n",
    "Training: Multi-scale Denoising Score Matching\n",
    "Sampling: Annealed Langevin Dynamics (for background characterization)\n",
    "Inference: < 10ms per event (TorchScript optimized)\n",
    "\"\"\")"
   ],
   "id": "cell_26"
  }
 ]
}