{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Grounded Radiology Report Generation \u2014 MedSight AI Case Study"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grounded Radiology Report Generation \u2014 Implementation Notebook\n",
    "\n",
    "**MedSight AI Case Study** | Cross-Attention for Medical Image Captioning\n",
    "\n",
    "This notebook implements a grounded radiology report generation system using cross-attention. We build a simplified version of the MedSight AI pipeline that takes chest X-ray-like images and generates text reports with visualizable attention grounding."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "%matplotlib inline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Synthetic Medical Image Dataset\n",
    "\n",
    "We create a synthetic dataset that mimics chest X-ray + report pairs. Each \"image\" has patches with specific patterns representing findings (nodule, effusion, cardiomegaly), and each \"report\" describes those findings."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticRadiologyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Synthetic dataset for grounded report generation.\n",
    "    Each sample has:\n",
    "    - image_patches: (n_patches, d_vision) - simulated ViT output\n",
    "    - report_ids: (max_len,) - tokenized report\n",
    "    - finding_mask: (max_len, n_patches) - which patches each token refers to\n",
    "    \"\"\"\n",
    "\n",
    "    FINDINGS = {\n",
    "        0: (\"normal\", \"No acute findings. Heart size is normal. Lungs are clear.\"),\n",
    "        1: (\"nodule\", \"There is a small nodule in the right upper lobe.\"),\n",
    "        2: (\"effusion\", \"There is a moderate left-sided pleural effusion.\"),\n",
    "        3: (\"cardiomegaly\", \"The heart is enlarged, consistent with cardiomegaly.\"),\n",
    "        4: (\"pneumothorax\", \"Small right-sided pneumothorax is present.\"),\n",
    "    }\n",
    "\n",
    "    # Simplified vocabulary\n",
    "    VOCAB = [\"<pad>\", \"<bos>\", \"<eos>\", \"no\", \"acute\", \"findings\", \"heart\",\n",
    "             \"size\", \"is\", \"normal\", \"lungs\", \"are\", \"clear\", \"there\", \"a\",\n",
    "             \"small\", \"nodule\", \"in\", \"the\", \"right\", \"upper\", \"lobe\",\n",
    "             \"moderate\", \"left\", \"sided\", \"pleural\", \"effusion\", \"enlarged\",\n",
    "             \"consistent\", \"with\", \"cardiomegaly\", \"pneumothorax\", \"present\"]\n",
    "\n",
    "    def __init__(self, n_samples=1000, n_patches=16, d_vision=64, max_len=20):\n",
    "        self.n_samples = n_samples\n",
    "        self.n_patches = n_patches\n",
    "        self.d_vision = d_vision\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = len(self.VOCAB)\n",
    "\n",
    "        self.word2idx = {w: i for i, w in enumerate(self.VOCAB)}\n",
    "\n",
    "        # Pre-generate all samples\n",
    "        self.data = []\n",
    "        for _ in range(n_samples):\n",
    "            finding_idx = np.random.randint(0, 5)\n",
    "            self.data.append(self._generate_sample(finding_idx))\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        tokens = [self.word2idx.get(w.lower().rstrip(\".,\"), 0)\n",
    "                  for w in text.split()]\n",
    "        tokens = [self.word2idx[\"<bos>\"]] + tokens + [self.word2idx[\"<eos>\"]]\n",
    "        # Pad or truncate\n",
    "        if len(tokens) < self.max_len:\n",
    "            tokens += [0] * (self.max_len - len(tokens))\n",
    "        return tokens[:self.max_len]\n",
    "\n",
    "    def _generate_sample(self, finding_idx):\n",
    "        # Generate image patches\n",
    "        patches = torch.randn(self.n_patches, self.d_vision) * 0.3\n",
    "\n",
    "        # Finding location (which patches are abnormal)\n",
    "        finding_name, report_text = self.FINDINGS[finding_idx]\n",
    "\n",
    "        # Assign finding to specific patches\n",
    "        if finding_idx == 0:  # normal\n",
    "            finding_patches = []\n",
    "        elif finding_idx == 1:  # nodule - right upper (patches 0-3)\n",
    "            finding_patches = [0, 1]\n",
    "            patches[0] += torch.randn(self.d_vision) * 2 + 3.0\n",
    "            patches[1] += torch.randn(self.d_vision) * 1.5 + 2.0\n",
    "        elif finding_idx == 2:  # effusion - left lower (patches 12-15)\n",
    "            finding_patches = [12, 13, 14]\n",
    "            for p in finding_patches:\n",
    "                patches[p] += torch.randn(self.d_vision) * 2 + 2.5\n",
    "        elif finding_idx == 3:  # cardiomegaly - center (patches 5-10)\n",
    "            finding_patches = [5, 6, 9, 10]\n",
    "            for p in finding_patches:\n",
    "                patches[p] += torch.randn(self.d_vision) * 1.5 + 2.0\n",
    "        elif finding_idx == 4:  # pneumothorax - right (patches 3, 7)\n",
    "            finding_patches = [3, 7]\n",
    "            for p in finding_patches:\n",
    "                patches[p] += torch.randn(self.d_vision) * 2.5 + 3.5\n",
    "\n",
    "        # Tokenize report\n",
    "        token_ids = self._tokenize(report_text)\n",
    "\n",
    "        # Create grounding mask (which patches each token should attend to)\n",
    "        finding_mask = torch.zeros(self.max_len, self.n_patches)\n",
    "        if finding_patches:\n",
    "            # Finding-related tokens should attend to finding patches\n",
    "            for t_idx in range(len(token_ids)):\n",
    "                if token_ids[t_idx] > 2:  # skip special tokens\n",
    "                    for p in finding_patches:\n",
    "                        finding_mask[t_idx, p] = 1.0\n",
    "\n",
    "        return {\n",
    "            \"patches\": patches,\n",
    "            \"token_ids\": torch.tensor(token_ids, dtype=torch.long),\n",
    "            \"finding_mask\": finding_mask,\n",
    "            \"finding_idx\": finding_idx,\n",
    "            \"finding_name\": finding_name,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create dataset\n",
    "dataset = SyntheticRadiologyDataset(n_samples=2000)\n",
    "train_set = torch.utils.data.Subset(dataset, range(1600))\n",
    "val_set = torch.utils.data.Subset(dataset, range(1600, 2000))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True,\n",
    "                          collate_fn=lambda batch: {\n",
    "                              k: torch.stack([b[k] for b in batch]) if isinstance(batch[0][k], torch.Tensor)\n",
    "                              else [b[k] for b in batch]\n",
    "                              for k in batch[0]\n",
    "                          })\n",
    "\n",
    "print(f\"Dataset: {len(dataset)} samples\")\n",
    "print(f\"  Train: {len(train_set)}, Val: {len(val_set)}\")\n",
    "print(f\"  Patches per image: {dataset.n_patches}\")\n",
    "print(f\"  Vocab size: {dataset.vocab_size}\")\n",
    "print(f\"  Max report length: {dataset.max_len}\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze finding distribution\n",
    "finding_counts = {}\n",
    "for sample in dataset.data:\n",
    "    name = sample[\"finding_name\"]\n",
    "    finding_counts[name] = finding_counts.get(name, 0) + 1\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Finding distribution\n",
    "names = list(finding_counts.keys())\n",
    "counts = list(finding_counts.values())\n",
    "axes[0].bar(names, counts, color=['#2ecc71', '#e74c3c', '#3498db', '#f39c12', '#9b59b6'])\n",
    "axes[0].set_title('Finding Distribution', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=30)\n",
    "\n",
    "# Patch activation patterns\n",
    "finding_examples = {0: None, 1: None, 2: None, 3: None, 4: None}\n",
    "for s in dataset.data:\n",
    "    idx = s[\"finding_idx\"]\n",
    "    if finding_examples[idx] is None:\n",
    "        finding_examples[idx] = s[\"patches\"].norm(dim=-1).numpy()\n",
    "\n",
    "patch_grid = np.stack([finding_examples[i].reshape(4, 4) for i in range(5)])\n",
    "for i, name in enumerate(dataset.FINDINGS.values()):\n",
    "    if i == 0:\n",
    "        axes[1].plot(finding_examples[i], label=name[0], alpha=0.7)\n",
    "    else:\n",
    "        axes[1].plot(finding_examples[i], label=name[0], alpha=0.7)\n",
    "\n",
    "axes[1].set_title('Patch Activation by Finding Type', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Patch Index')\n",
    "axes[1].set_ylabel('Patch Norm')\n",
    "axes[1].legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroundedReportVLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified VLM for grounded radiology report generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_vision, d_model, num_heads, num_layers,\n",
    "                 vocab_size, max_len):\n",
    "        super().__init__()\n",
    "\n",
    "        # Token alignment\n",
    "        self.image_proj = nn.Linear(d_vision, d_model)\n",
    "\n",
    "        # Text embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        # Transformer decoder blocks\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.blocks.append(nn.ModuleDict({\n",
    "                'self_attn': nn.MultiheadAttention(d_model, num_heads, batch_first=True),\n",
    "                'cross_attn_q': nn.Linear(d_model, d_model),\n",
    "                'cross_attn_k': nn.Linear(d_model, d_model),\n",
    "                'cross_attn_v': nn.Linear(d_model, d_model),\n",
    "                'cross_attn_out': nn.Linear(d_model, d_model),\n",
    "                'ffn': nn.Sequential(\n",
    "                    nn.Linear(d_model, d_model * 4),\n",
    "                    nn.GELU(),\n",
    "                    nn.Linear(d_model * 4, d_model),\n",
    "                ),\n",
    "                'norm1': nn.LayerNorm(d_model),\n",
    "                'norm2': nn.LayerNorm(d_model),\n",
    "                'norm3': nn.LayerNorm(d_model),\n",
    "            }))\n",
    "\n",
    "        self.output_head = nn.Linear(d_model, vocab_size)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def cross_attention(self, block, text, image):\n",
    "        \"\"\"Manual cross-attention to extract weights.\"\"\"\n",
    "        B, T, D = text.shape\n",
    "        _, N, _ = image.shape\n",
    "\n",
    "        Q = block['cross_attn_q'](text)   # (B, T, D)\n",
    "        K = block['cross_attn_k'](image)  # (B, N, D)\n",
    "        V = block['cross_attn_v'](image)  # (B, N, D)\n",
    "\n",
    "        # Reshape for multi-head\n",
    "        Q = Q.view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(B, N, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(B, N, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attn_weights = F.softmax(scores, dim=-1)  # (B, H, T, N)\n",
    "\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(B, T, self.d_model)\n",
    "        output = block['cross_attn_out'](context)\n",
    "\n",
    "        return output, attn_weights\n",
    "\n",
    "    def forward(self, image_patches, token_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_patches: (B, N, d_vision)\n",
    "            token_ids: (B, T)\n",
    "        Returns:\n",
    "            logits: (B, T, vocab_size)\n",
    "            all_attn: list of (B, H, T, N) per layer\n",
    "        \"\"\"\n",
    "        B, T = token_ids.shape\n",
    "\n",
    "        # Embed and project\n",
    "        image_tokens = self.image_proj(image_patches)  # (B, N, d_model)\n",
    "        positions = torch.arange(T, device=token_ids.device).unsqueeze(0)\n",
    "        text_tokens = self.token_embedding(token_ids) + self.pos_embedding(positions)\n",
    "\n",
    "        all_attn = []\n",
    "        for block in self.blocks:\n",
    "            # Self-attention\n",
    "            normed = block['norm1'](text_tokens)\n",
    "            sa_out, _ = block['self_attn'](normed, normed, normed)\n",
    "            text_tokens = text_tokens + sa_out\n",
    "\n",
    "            # Cross-attention\n",
    "            normed = block['norm2'](text_tokens)\n",
    "            ca_out, attn = self.cross_attention(block, normed, image_tokens)\n",
    "            text_tokens = text_tokens + ca_out\n",
    "            all_attn.append(attn)\n",
    "\n",
    "            # FFN\n",
    "            normed = block['norm3'](text_tokens)\n",
    "            text_tokens = text_tokens + block['ffn'](normed)\n",
    "\n",
    "        logits = self.output_head(text_tokens)\n",
    "        return logits, all_attn\n",
    "\n",
    "# Initialize model\n",
    "model = GroundedReportVLM(\n",
    "    d_vision=64, d_model=64, num_heads=4, num_layers=2,\n",
    "    vocab_size=dataset.vocab_size, max_len=dataset.max_len\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training with Grounding Loss"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the combined loss function\n",
    "def compute_loss(logits, token_ids, attn_weights, finding_masks,\n",
    "                 lambda_ground=0.1, lambda_entropy=0.01):\n",
    "    \"\"\"\n",
    "    Compute combined training loss.\n",
    "\n",
    "    Args:\n",
    "        logits: (B, T, vocab_size)\n",
    "        token_ids: (B, T) ground truth tokens\n",
    "        attn_weights: list of (B, H, T, N) per layer\n",
    "        finding_masks: (B, T, N) ground truth grounding\n",
    "        lambda_ground: weight for grounding loss\n",
    "        lambda_entropy: weight for entropy regularization\n",
    "\n",
    "    Returns:\n",
    "        total_loss, ce_loss, ground_loss, entropy_loss\n",
    "\n",
    "    TODO:\n",
    "    1. Cross-entropy loss (shifted by 1 for next-token prediction)\n",
    "    2. Grounding loss (BCE between avg attention and masks)\n",
    "    3. Entropy regularization on attention weights\n",
    "    \"\"\"\n",
    "    # ============ YOUR CODE HERE ============\n",
    "    # Cross-entropy (teacher forcing)\n",
    "    shift_logits = logits[:, :-1].contiguous()\n",
    "    shift_targets = token_ids[:, 1:].contiguous()\n",
    "    ce_loss = F.cross_entropy(shift_logits.view(-1, logits.size(-1)),\n",
    "                               shift_targets.view(-1), ignore_index=0)\n",
    "\n",
    "    # Grounding loss\n",
    "    last_attn = attn_weights[-1]  # Use last layer\n",
    "    avg_attn = last_attn.mean(dim=1)  # Average across heads (B, T, N)\n",
    "\n",
    "    mask_exists = finding_masks.sum(dim=-1) > 0  # (B, T)\n",
    "    if mask_exists.any():\n",
    "        attn_for_ground = avg_attn[mask_exists]\n",
    "        masks_for_ground = finding_masks[mask_exists]\n",
    "        ground_loss = F.binary_cross_entropy(\n",
    "            attn_for_ground.clamp(1e-6, 1-1e-6),\n",
    "            masks_for_ground / (masks_for_ground.sum(dim=-1, keepdim=True) + 1e-6)\n",
    "        )\n",
    "    else:\n",
    "        ground_loss = torch.tensor(0.0, device=logits.device)\n",
    "\n",
    "    # Entropy regularization\n",
    "    entropy = -(avg_attn * (avg_attn + 1e-8).log()).sum(dim=-1).mean()\n",
    "\n",
    "    total = ce_loss + lambda_ground * ground_loss + lambda_entropy * entropy\n",
    "    # ========================================\n",
    "\n",
    "    return total, ce_loss, ground_loss, entropy"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        patches = batch[\"patches\"].to(device)\n",
    "        token_ids = batch[\"token_ids\"].to(device)\n",
    "        masks = batch[\"finding_mask\"].to(device)\n",
    "\n",
    "        logits, attn = model(patches, token_ids)\n",
    "        total_loss, ce, gnd, ent = compute_loss(logits, token_ids, attn, masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += total_loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}: loss={avg_loss:.4f}, CE={ce.item():.4f}, \"\n",
    "              f\"Ground={gnd.item():.4f}, Entropy={ent.item():.4f}\")\n",
    "\n",
    "# Plot training curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_losses, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss', fontsize=13, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation and Attention Visualization"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set and visualize grounding\n",
    "model.eval()\n",
    "\n",
    "# Pick one example per finding type\n",
    "fig, axes = plt.subplots(5, 3, figsize=(15, 20))\n",
    "\n",
    "for finding_idx in range(5):\n",
    "    # Find a sample with this finding\n",
    "    sample = None\n",
    "    for s in val_set:\n",
    "        if s[\"finding_idx\"] == finding_idx:\n",
    "            sample = s\n",
    "            break\n",
    "\n",
    "    if sample is None:\n",
    "        continue\n",
    "\n",
    "    patches = sample[\"patches\"].unsqueeze(0).to(device)\n",
    "    token_ids = sample[\"token_ids\"].unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits, attn = model(patches, token_ids)\n",
    "\n",
    "    # Get attention from last layer, averaged across heads\n",
    "    avg_attn = attn[-1][0].mean(dim=0).cpu().numpy()  # (T, N)\n",
    "\n",
    "    # Plot 1: Patch norms (image representation)\n",
    "    patch_norms = sample[\"patches\"].norm(dim=-1).numpy().reshape(4, 4)\n",
    "    axes[finding_idx, 0].imshow(patch_norms, cmap='gray')\n",
    "    axes[finding_idx, 0].set_title(f'{sample[\"finding_name\"].upper()}', fontsize=11, fontweight='bold')\n",
    "    axes[finding_idx, 0].set_ylabel('Image Patches')\n",
    "\n",
    "    # Plot 2: Attention heatmap\n",
    "    im = axes[finding_idx, 1].imshow(avg_attn[:10], cmap='YlOrRd', aspect='auto')\n",
    "    axes[finding_idx, 1].set_title('Cross-Attention Weights', fontsize=11)\n",
    "    axes[finding_idx, 1].set_xlabel('Image Patch')\n",
    "    axes[finding_idx, 1].set_ylabel('Text Token')\n",
    "\n",
    "    # Plot 3: Average attention per patch (grounding map)\n",
    "    mean_attn_per_patch = avg_attn[:10].mean(axis=0).reshape(4, 4)\n",
    "    axes[finding_idx, 2].imshow(mean_attn_per_patch, cmap='hot')\n",
    "    axes[finding_idx, 2].set_title('Grounding Map', fontsize=11)\n",
    "\n",
    "plt.suptitle('Grounded Report Generation: Attention Visualization by Finding Type',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"MedSight AI \u2014 Grounded Report Generator \u2014 Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Training epochs: 15\")\n",
    "print(f\"Final training loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Findings covered: {list(dataset.FINDINGS.keys())}\")\n",
    "print(f\"\\nThe cross-attention mechanism provides built-in grounding:\")\n",
    "print(\"- Each text token's attention weights show which image patches it queries\")\n",
    "print(\"- The grounding loss supervises these weights to match clinical ground truth\")\n",
    "print(\"- This satisfies the FDA explainability requirement for clinical AI\")"
   ],
   "id": "cell_14"
  }
 ]
}