{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "ClearClaim AI: Reasoning Model for Claims Adjudication \u2014 Implementation Notebook"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Teaching an Insurance Claims Engine to Reason Step by Step\n",
    "## Implementation Notebook\n",
    "\n",
    "Welcome to the ClearClaim AI case study implementation notebook. In this notebook, you will build a reasoning model that processes insurance claims by generating step-by-step payout calculations -- the same approach used by DeepSeek-R1 to teach language models to reason.\n",
    "\n",
    "You will implement the full three-stage pipeline:\n",
    "1. **SFT:** Supervised fine-tuning on chain-of-thought reasoning traces\n",
    "2. **GRPO:** Group Relative Policy Optimization with verifiable rewards\n",
    "3. **Evaluation:** Comprehensive accuracy and error analysis\n",
    "\n",
    "**Business context:** ClearClaim AI's current model achieves only 69% accuracy on multi-step insurance claims. Your goal is to build a reasoning model that exceeds 85% accuracy while producing auditable reasoning traces.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Data Acquisition and Preparation\n",
    "\n",
    "We use two datasets:\n",
    "1. **GSM8K** -- A math reasoning dataset for initial warm-up (the model learns general step-by-step reasoning)\n",
    "2. **Synthetic insurance claims** -- A custom dataset with configurable complexity that mirrors real multi-step payout calculations\n",
    "\n",
    "Each claim includes policy terms (deductible, coverage limit, depreciation rate, sub-limits, co-insurance) and a ground-truth payout computed deterministically from those terms."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load GSM8K for initial warm-up training\n",
    "gsm8k = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "print(f\"GSM8K train size: {len(gsm8k['train'])}\")\n",
    "print(f\"GSM8K test size: {len(gsm8k['test'])}\")\n",
    "\n",
    "# Inspect a sample\n",
    "sample = gsm8k['train'][0]\n",
    "print(f\"\\nQuestion: {sample['question'][:200]}...\")\n",
    "print(f\"\\nAnswer: {sample['answer'][:200]}...\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Synthetic Insurance Claims Dataset\n",
    "\n",
    "Each claim must have a deterministically verifiable payout. The computation order is fixed:\n",
    "1. Start with replacement cost for each item\n",
    "2. Apply depreciation to get actual cash value (ACV)\n",
    "3. Check each item against sub-limits (cap if needed)\n",
    "4. Sum all items\n",
    "5. Subtract deductible\n",
    "6. Apply co-insurance percentage\n",
    "7. Check against coverage limit (cap if needed)"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InsuranceClaim:\n",
    "    \"\"\"Represents a single insurance claim with policy terms and ground truth.\"\"\"\n",
    "    claim_description: str\n",
    "    policy_terms: dict\n",
    "    ground_truth_payout: float\n",
    "    reasoning_steps: List[str]\n",
    "    difficulty: str  # 'single_step', 'multi_step', 'complex'"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 1: Implement the claims dataset generator.**"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_claims_dataset(n_claims: int = 5000, difficulty_mix: dict = None) -> List[InsuranceClaim]:\n",
    "    \"\"\"\n",
    "    Generate a synthetic insurance claims dataset with verifiable payouts.\n",
    "\n",
    "    Each claim includes:\n",
    "    - A natural language claim description\n",
    "    - Structured policy terms (deductible, limits, depreciation, sub-limits, co-insurance)\n",
    "    - The correct payout amount (computed deterministically from the terms)\n",
    "    - Step-by-step reasoning showing how to arrive at the payout\n",
    "    - Difficulty level\n",
    "\n",
    "    The difficulty mix controls the distribution:\n",
    "    - 'single_step': Only deductible subtraction (e.g., damage - deductible)\n",
    "    - 'multi_step': Deductible + depreciation + sub-limits (3-4 steps)\n",
    "    - 'complex': All of the above + co-insurance + coverage limit checks (5-7 steps)\n",
    "\n",
    "    Args:\n",
    "        n_claims: Total number of claims to generate\n",
    "        difficulty_mix: Dict like {'single_step': 0.3, 'multi_step': 0.4, 'complex': 0.3}\n",
    "\n",
    "    Returns:\n",
    "        List of InsuranceClaim objects with verified ground truth payouts\n",
    "\n",
    "    Hints:\n",
    "        1. Start by defining ranges for each policy parameter:\n",
    "           - Deductibles: [$500, $1000, $2500, $5000]\n",
    "           - Coverage limits: [$50K, $100K, $250K, $500K]\n",
    "           - Depreciation rates: [5%, 10%, 15%] per year, item age 1-15 years\n",
    "           - Sub-limits: electronics $10K, jewelry $5K, art $25K\n",
    "           - Co-insurance: 80/20, 70/30, 90/10\n",
    "        2. For each claim, randomly sample parameters, then compute the payout\n",
    "           step by step (this becomes both the ground truth AND the reasoning trace)\n",
    "        3. The payout computation order matters:\n",
    "           a. Start with replacement cost for each item\n",
    "           b. Apply depreciation to get actual cash value (ACV)\n",
    "           c. Check each item against sub-limits (cap if needed)\n",
    "           d. Sum all items\n",
    "           e. Subtract deductible\n",
    "           f. Apply co-insurance percentage\n",
    "           g. Check against coverage limit (cap if needed)\n",
    "        4. Generate the natural language claim description from the parameters\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset\n",
    "claims = generate_claims_dataset(n_claims=5000, difficulty_mix={\n",
    "    'single_step': 0.3, 'multi_step': 0.4, 'complex': 0.3\n",
    "})\n",
    "print(f\"Generated {len(claims)} claims\")\n",
    "print(f\"Difficulty distribution: { {d: sum(1 for c in claims if c.difficulty == d) for d in ['single_step', 'multi_step', 'complex']} }\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: recompute payouts and verify they match ground truth\n",
    "def verify_dataset(claims: List[InsuranceClaim]) -> dict:\n",
    "    \"\"\"Recompute payouts from policy terms and verify they match ground truth.\"\"\"\n",
    "    correct = 0\n",
    "    errors = []\n",
    "    for i, claim in enumerate(claims):\n",
    "        # Recompute using the same deterministic logic\n",
    "        terms = claim.policy_terms\n",
    "        total = 0\n",
    "        for item in terms.get('items', []):\n",
    "            cost = item['replacement_cost']\n",
    "            # Apply depreciation\n",
    "            if 'depreciation_rate' in terms and 'age_years' in item:\n",
    "                cost = cost * (1 - terms['depreciation_rate'] * item['age_years'])\n",
    "                cost = max(cost, 0)\n",
    "            # Check sub-limits\n",
    "            if item.get('category') in terms.get('sub_limits', {}):\n",
    "                cost = min(cost, terms['sub_limits'][item['category']])\n",
    "            total += cost\n",
    "        # Apply deductible\n",
    "        total = max(total - terms.get('deductible', 0), 0)\n",
    "        # Apply co-insurance\n",
    "        if 'co_insurance_insurer' in terms:\n",
    "            total = total * terms['co_insurance_insurer']\n",
    "        # Check coverage limit\n",
    "        if 'coverage_limit' in terms:\n",
    "            total = min(total, terms['coverage_limit'])\n",
    "        total = round(total, 2)\n",
    "\n",
    "        if abs(total - claim.ground_truth_payout) < 0.01:\n",
    "            correct += 1\n",
    "        else:\n",
    "            errors.append((i, total, claim.ground_truth_payout))\n",
    "\n",
    "    accuracy = correct / len(claims)\n",
    "    print(f\"Dataset verification: {correct}/{len(claims)} ({accuracy:.1%}) payouts verified\")\n",
    "    if errors:\n",
    "        print(f\"First 3 errors: {errors[:3]}\")\n",
    "    assert accuracy == 1.0, f\"Dataset has {len(errors)} computation errors!\"\n",
    "    return {\"verified\": correct, \"total\": len(claims)}\n",
    "\n",
    "verify_dataset(claims)"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought questions:**\n",
    "- Why is it critical that the synthetic dataset is deterministically verifiable? What would happen during GRPO training if the ground-truth payouts were sometimes wrong?\n",
    "- How does the difficulty mix affect the training curriculum? Should we train on easy claims first or mix all difficulties from the start?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2 Exploratory Data Analysis\n",
    "\n",
    "Before training, we need to understand the distribution of our data."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Analyze GSM8K answer distribution\n",
    "gsm8k_answers = []\n",
    "for ex in gsm8k['train']:\n",
    "    match = re.search(r'#### (\\-?[\\d,]+)', ex['answer'])\n",
    "    if match:\n",
    "        gsm8k_answers.append(float(match.group(1).replace(',', '')))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(gsm8k_answers, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Answer Value')\n",
    "plt.ylabel('Count')\n",
    "plt.title('GSM8K Answer Distribution')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist([len(ex['answer'].split()) for ex in gsm8k['train']], bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Reasoning Length (words)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('GSM8K Reasoning Trace Length')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2: Analyze the insurance claims dataset.**"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_claims_dataset(claims: List[InsuranceClaim]):\n",
    "    \"\"\"\n",
    "    Produce exploratory data analysis plots for the insurance claims dataset.\n",
    "\n",
    "    Generate the following visualizations:\n",
    "    1. Payout distribution by difficulty level (three overlapping histograms)\n",
    "    2. Number of reasoning steps vs. payout amount (scatter plot)\n",
    "    3. Distribution of policy parameters (deductibles, limits, depreciation rates)\n",
    "    4. Correlation heatmap between policy parameters and payout amount\n",
    "\n",
    "    Also compute and print:\n",
    "    - Mean and median payout by difficulty level\n",
    "    - Average number of reasoning steps by difficulty level\n",
    "    - The fraction of claims where sub-limits affect the payout\n",
    "    - The fraction of claims where the coverage limit caps the payout\n",
    "\n",
    "    Hints:\n",
    "        1. Use matplotlib with 2x2 subplots for the four visualizations\n",
    "        2. For the correlation heatmap, extract numeric fields from policy_terms\n",
    "        3. Pay attention to the relationship between number of steps and accuracy --\n",
    "           this tells us how difficult multi-step reasoning is for the model\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "analyze_claims_dataset(claims)"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought questions:**\n",
    "- What does the relationship between reasoning length and accuracy tell you about why SFT alone is insufficient?\n",
    "- If the payout distribution is heavily right-skewed (a few very large claims), how might this affect GRPO training?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3 Baseline: Direct Payout Prediction\n",
    "\n",
    "Before building a reasoning model, we implement the current baseline -- a model that directly predicts the payout amount without intermediate steps."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Use a small model for the baseline experiment\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float32)\n",
    "\n",
    "# If pad token is not set, use eos token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def format_direct_prompt(claim: InsuranceClaim) -> str:\n",
    "    \"\"\"Format a claim as a direct (no reasoning) prompt.\"\"\"\n",
    "    return f\"\"\"<claim>\n",
    "{claim.claim_description}\n",
    "</claim>\n",
    "\n",
    "<policy>\n",
    "{json.dumps(claim.policy_terms, indent=2)}\n",
    "</policy>\n",
    "\n",
    "What is the payout amount? Answer with just the dollar amount.\"\"\"\n",
    "\n",
    "print(\"Example direct prompt:\")\n",
    "print(format_direct_prompt(claims[0]))"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 3: Implement and evaluate the direct prediction baseline.**"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_direct_baseline(model, tokenizer, claims: List[InsuranceClaim], n_eval: int = 200) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate the direct (no-reasoning) payout prediction baseline.\n",
    "\n",
    "    For each claim:\n",
    "    1. Format the claim as a direct prompt (no reasoning requested)\n",
    "    2. Generate the model's completion (max 50 tokens)\n",
    "    3. Extract the predicted payout amount from the completion\n",
    "    4. Compare against ground truth\n",
    "\n",
    "    Args:\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        claims: List of InsuranceClaim objects\n",
    "        n_eval: Number of claims to evaluate\n",
    "\n",
    "    Returns:\n",
    "        Dict with:\n",
    "        - 'exact_match': fraction of exact matches (within $1)\n",
    "        - 'within_10pct': fraction within 10% of ground truth\n",
    "        - 'mean_absolute_error': average dollar error\n",
    "        - 'accuracy_by_difficulty': dict mapping difficulty to exact_match rate\n",
    "\n",
    "    Hints:\n",
    "        1. Use model.generate() with temperature=0 for deterministic evaluation\n",
    "        2. Extract dollar amounts using regex: r'\\\\$?([\\\\d,]+\\\\.?\\\\d*)'\n",
    "        3. Handle cases where the model outputs text instead of a number\n",
    "        4. Track accuracy separately for each difficulty level\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "# Run baseline evaluation\n",
    "# baseline_results = evaluate_direct_baseline(model, tokenizer, claims[-200:])\n",
    "# print(f\"Baseline exact match: {baseline_results['exact_match']:.1%}\")\n",
    "# print(f\"Baseline by difficulty: {baseline_results['accuracy_by_difficulty']}\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought questions:**\n",
    "- Why is the direct baseline likely to perform worse on multi-step claims?\n",
    "- If you increased the model size from 0.5B to 7B, would the direct baseline improve significantly on multi-step claims? Why or why not?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4 Model Design: Chain-of-Thought Reasoning\n",
    "\n",
    "Our reasoning model uses the same base architecture but is trained to produce step-by-step reasoning traces. The architecture does not change -- only the training procedure and expected output format change."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reasoning_prompt(claim: InsuranceClaim) -> str:\n",
    "    \"\"\"Format a claim as a reasoning prompt with <think> tags.\"\"\"\n",
    "    return f\"\"\"<claim>\n",
    "{claim.claim_description}\n",
    "</claim>\n",
    "\n",
    "<policy>\n",
    "{json.dumps(claim.policy_terms, indent=2)}\n",
    "</policy>\n",
    "\n",
    "Calculate the claim payout step by step.\"\"\"\n",
    "\n",
    "\n",
    "def format_reasoning_target(claim: InsuranceClaim) -> str:\n",
    "    \"\"\"Format the target completion with reasoning trace.\"\"\"\n",
    "    steps = \"\\n\".join(f\"Step {i+1}: {step}\" for i, step in enumerate(claim.reasoning_steps))\n",
    "    return f\"\"\"<think>\n",
    "{steps}\n",
    "</think>\n",
    "\n",
    "The payout is ${claim.ground_truth_payout:,.2f}.\"\"\"\n",
    "\n",
    "\n",
    "# Inspect the format\n",
    "print(\"=== PROMPT ===\")\n",
    "print(format_reasoning_prompt(claims[0]))\n",
    "print(\"\\n=== TARGET ===\")\n",
    "print(format_reasoning_target(claims[0]))"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 4: Implement the SFT training loop.**\n",
    "\n",
    "The SFT loss is: $\\mathcal{L}_{\\text{SFT}} = -\\sum_{t=1}^{T} \\log p_\\theta(y_t \\mid y_{<t}, x)$\n",
    "\n",
    "The key detail: we only compute loss on the TARGET tokens, not the prompt tokens. This is done by setting prompt token labels to -100."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sft_training_step(model, tokenizer, prompt: str, target: str, optimizer) -> float:\n",
    "    \"\"\"\n",
    "    One step of supervised fine-tuning on a chain-of-thought example.\n",
    "\n",
    "    Args:\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        prompt: The input prompt (claim + policy)\n",
    "        target: The target completion (reasoning trace + answer)\n",
    "        optimizer: The optimizer\n",
    "\n",
    "    Returns:\n",
    "        The loss value (float)\n",
    "\n",
    "    Hints:\n",
    "        1. Concatenate prompt + target into one sequence\n",
    "        2. Tokenize the full sequence\n",
    "        3. Create a labels tensor where prompt tokens are set to -100\n",
    "           (so the loss is only computed on the target tokens)\n",
    "        4. Forward pass through the model\n",
    "        5. Compute cross-entropy loss only on target tokens\n",
    "        6. Backpropagate and update\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "def run_sft_training(model, tokenizer, claims: List[InsuranceClaim],\n",
    "                     n_epochs: int = 2, lr: float = 2e-5) -> List[float]:\n",
    "    \"\"\"\n",
    "    Run the full SFT training loop.\n",
    "\n",
    "    Args:\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        claims: Training claims with reasoning traces\n",
    "        n_epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "\n",
    "    Returns:\n",
    "        List of loss values for plotting\n",
    "\n",
    "    Hints:\n",
    "        1. Use AdamW optimizer with the specified learning rate\n",
    "        2. Shuffle claims at the start of each epoch\n",
    "        3. Log loss every 100 steps\n",
    "        4. Save a checkpoint at the end of each epoch\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SFT training (use a subset for speed in Colab)\n",
    "# sft_losses = run_sft_training(model, tokenizer, claims[:1000], n_epochs=2)\n",
    "# plt.plot(sft_losses)\n",
    "# plt.xlabel('Step')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('SFT Training Loss')\n",
    "# plt.show()"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought questions:**\n",
    "- Why do we mask the prompt tokens instead of training on the full sequence?\n",
    "- After SFT, the model can produce well-formatted reasoning traces. But can it produce CORRECT reasoning? What fundamental limitation of maximum likelihood training causes this gap?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.5 Training Strategy: GRPO with Verifiable Rewards\n",
    "\n",
    "This is the core of the case study. GRPO uses the ground-truth payout as a verifiable reward signal -- no reward model needed.\n",
    "\n",
    "The key equations:\n",
    "\n",
    "**Group-relative advantage:** $\\hat{A}_i = \\frac{r_i - \\bar{r}}{\\sigma_r + \\varepsilon}$\n",
    "\n",
    "**Clipped surrogate loss:** $\\mathcal{L}_{\\text{GRPO}} = -\\frac{1}{G}\\sum_{i=1}^{G} \\min\\left(\\rho_i \\hat{A}_i, \\text{clip}(\\rho_i, 1-\\epsilon, 1+\\epsilon)\\hat{A}_i\\right)$\n",
    "\n",
    "**KL-penalized reward:** $R_{\\text{total}} = r(y, y^*) - \\beta \\cdot D_{\\text{KL}}(\\pi_\\theta \\| \\pi_{\\text{ref}})$"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRPO_CONFIG = {\n",
    "    'group_size': 8,\n",
    "    'epsilon': 0.2,\n",
    "    'beta': 0.05,\n",
    "    'lr': 1e-6,\n",
    "    'max_new_tokens': 512,\n",
    "    'temperature': 0.7,\n",
    "}\n",
    "\n",
    "def extract_payout(completion: str) -> Optional[float]:\n",
    "    \"\"\"Extract the dollar payout amount from a model completion.\"\"\"\n",
    "    patterns = [\n",
    "        r'payout\\s+is\\s+\\$?([\\d,]+\\.?\\d*)',\n",
    "        r'####\\s+\\$?([\\d,]+\\.?\\d*)',\n",
    "        r'total.*?\\$?([\\d,]+\\.?\\d*)\\s*$',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, completion, re.IGNORECASE | re.MULTILINE)\n",
    "        if match:\n",
    "            return float(match.group(1).replace(',', ''))\n",
    "    return None\n",
    "\n",
    "\n",
    "def compute_reward(completion: str, ground_truth: float) -> float:\n",
    "    \"\"\"Binary reward: 1 if extracted payout matches ground truth, 0 otherwise.\"\"\"\n",
    "    predicted = extract_payout(completion)\n",
    "    if predicted is None:\n",
    "        return 0.0\n",
    "    return 1.0 if abs(predicted - ground_truth) < 0.01 else 0.0"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 5: Implement GRPO training.**"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grpo_loss(model, ref_model, tokenizer, prompt: str,\n",
    "                      completions: List[str], advantages: torch.Tensor,\n",
    "                      epsilon: float = 0.2, beta: float = 0.05) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the GRPO loss for a group of completions.\n",
    "\n",
    "    Args:\n",
    "        model: Current policy model\n",
    "        ref_model: Frozen reference model (post-SFT checkpoint)\n",
    "        tokenizer: The tokenizer\n",
    "        prompt: The input prompt\n",
    "        completions: List of G completions from the old policy\n",
    "        advantages: Group-relative advantages, shape (G,)\n",
    "        epsilon: Clipping parameter\n",
    "        beta: KL penalty weight\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss tensor\n",
    "\n",
    "    Hints:\n",
    "        1. For each completion, compute log p(completion | prompt) under both\n",
    "           the current model and the reference model\n",
    "        2. The probability ratio is rho_i = exp(log_p_current - log_p_old)\n",
    "        3. Compute the clipped and unclipped objectives\n",
    "        4. Take the element-wise minimum\n",
    "        5. Add the KL penalty: beta * mean(log_p_current - log_p_ref)\n",
    "        6. Return the negative mean (we minimize the loss)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "def grpo_training_step(model, ref_model, tokenizer, claim: InsuranceClaim,\n",
    "                       optimizer, config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    One complete GRPO training step for a single claim.\n",
    "\n",
    "    Steps:\n",
    "    1. Format the claim as a reasoning prompt\n",
    "    2. Generate G completions from the current model\n",
    "    3. Compute binary rewards for each completion\n",
    "    4. Compute group-relative advantages\n",
    "    5. Compute GRPO loss\n",
    "    6. Backpropagate and update\n",
    "\n",
    "    Args:\n",
    "        model: Current policy model\n",
    "        ref_model: Frozen reference model\n",
    "        tokenizer: The tokenizer\n",
    "        claim: The insurance claim to train on\n",
    "        optimizer: The optimizer\n",
    "        config: GRPO hyperparameters\n",
    "\n",
    "    Returns:\n",
    "        Dict with 'loss', 'mean_reward', 'n_correct', 'advantages'\n",
    "\n",
    "    Hints:\n",
    "        1. Use model.generate() with do_sample=True\n",
    "        2. After computing rewards, check if all rewards are the same.\n",
    "           If so, skip the update (advantages would be all zeros).\n",
    "        3. Compute advantages as (rewards - mean) / (std + 1e-8)\n",
    "        4. Detach completions from the computation graph\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GRPO training (use a subset for speed)\n",
    "# import copy\n",
    "# ref_model = copy.deepcopy(model)\n",
    "# ref_model.eval()\n",
    "# for param in ref_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "#\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=GRPO_CONFIG['lr'])\n",
    "# grpo_rewards = []\n",
    "# for step, claim in enumerate(claims[:500]):\n",
    "#     result = grpo_training_step(model, ref_model, tokenizer, claim, optimizer, GRPO_CONFIG)\n",
    "#     grpo_rewards.append(result['mean_reward'])\n",
    "#     if step % 50 == 0:\n",
    "#         print(f\"Step {step}: reward={result['mean_reward']:.2f}, loss={result['loss']:.4f}\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought questions:**\n",
    "- What happens when all G completions get the same reward? Why must we skip the update?\n",
    "- Why do we use temperature=0.7 instead of greedy decoding during GRPO?\n",
    "- What would happen if we set beta=1.0? What about beta=0.001?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.6 Evaluation\n",
    "\n",
    "We evaluate the reasoning model against the direct prediction baseline."
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reasoning_model(model, tokenizer, test_claims: List[InsuranceClaim],\n",
    "                             n_eval: int = 200) -> dict:\n",
    "    \"\"\"Evaluate the reasoning model on held-out claims.\"\"\"\n",
    "    results = {\n",
    "        'exact_match': 0, 'step_correct': 0, 'format_compliant': 0,\n",
    "        'total': 0, 'by_difficulty': {}, 'latencies': [],\n",
    "    }\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for claim in test_claims[:n_eval]:\n",
    "            prompt = format_reasoning_prompt(claim)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "            import time\n",
    "            start = time.time()\n",
    "            outputs = model.generate(inputs.input_ids, max_new_tokens=512, temperature=0.0, do_sample=False)\n",
    "            elapsed = time.time() - start\n",
    "            completion = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "            # Check metrics\n",
    "            predicted = extract_payout(completion)\n",
    "            is_correct = predicted is not None and abs(predicted - claim.ground_truth_payout) < 0.01\n",
    "            has_think = '<think>' in completion and '</think>' in completion\n",
    "\n",
    "            results['exact_match'] += int(is_correct)\n",
    "            results['format_compliant'] += int(has_think)\n",
    "            results['total'] += 1\n",
    "            results['latencies'].append(elapsed)\n",
    "\n",
    "            d = claim.difficulty\n",
    "            if d not in results['by_difficulty']:\n",
    "                results['by_difficulty'][d] = {'correct': 0, 'total': 0}\n",
    "            results['by_difficulty'][d]['total'] += 1\n",
    "            results['by_difficulty'][d]['correct'] += int(is_correct)\n",
    "\n",
    "    n = results['total']\n",
    "    results['exact_match_rate'] = results['exact_match'] / n\n",
    "    results['format_compliance_rate'] = results['format_compliant'] / n\n",
    "    results['mean_latency'] = np.mean(results['latencies'])\n",
    "    for d in results['by_difficulty']:\n",
    "        info = results['by_difficulty'][d]\n",
    "        info['accuracy'] = info['correct'] / info['total']\n",
    "    return results"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 6: Implement step-level verification and comparison plots.**"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_reasoning_steps(completion: str, policy_terms: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Verify that each step in the reasoning trace is arithmetically correct.\n",
    "\n",
    "    Args:\n",
    "        completion: The model's full completion (with <think> block)\n",
    "        policy_terms: The policy terms for reference\n",
    "\n",
    "    Returns:\n",
    "        Dict with 'n_steps', 'n_correct_steps', 'step_details', 'format_valid'\n",
    "\n",
    "    Hints:\n",
    "        1. Extract text between <think> and </think> tags\n",
    "        2. Split on \"Step N:\" patterns\n",
    "        3. For each step, look for arithmetic expressions (A * B = C, etc.)\n",
    "        4. Evaluate the expression and compare to the stated result\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "def plot_evaluation_comparison(baseline_results: dict, reasoning_results: dict):\n",
    "    \"\"\"\n",
    "    Create side-by-side comparison plots.\n",
    "\n",
    "    Generate:\n",
    "    1. Grouped bar chart: accuracy by difficulty level\n",
    "    2. Scatter plot: payout error vs claim complexity\n",
    "    3. Histogram: reasoning trace lengths for correct vs incorrect\n",
    "    4. Line plot: accuracy vs number of reasoning steps\n",
    "\n",
    "    Hints:\n",
    "        1. Use matplotlib with 2x2 subplots\n",
    "        2. Blue for baseline, orange for reasoning model\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.7 Error Analysis\n",
    "\n",
    "Understanding failure modes is critical for improving the model and building trust.\n",
    "\n",
    "**TODO 7: Implement systematic error categorization.**"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_errors(model, tokenizer, test_claims: List[InsuranceClaim],\n",
    "                      n_eval: int = 200) -> dict:\n",
    "    \"\"\"\n",
    "    Categorize errors from the reasoning model.\n",
    "\n",
    "    Error categories:\n",
    "    1. 'arithmetic_error': Incorrect arithmetic in a step\n",
    "    2. 'step_ordering_error': Steps applied in wrong order\n",
    "    3. 'missing_step': A required step is omitted\n",
    "    4. 'hallucinated_value': Model uses a number not in the claim/policy\n",
    "    5. 'format_error': Malformed output\n",
    "    6. 'extraction_error': Answer present but unparseable\n",
    "\n",
    "    Args:\n",
    "        model: The reasoning model\n",
    "        tokenizer: The tokenizer\n",
    "        test_claims: Held-out test claims\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping error category to list of error instances\n",
    "\n",
    "    Hints:\n",
    "        1. Use verify_reasoning_steps() to identify arithmetic errors\n",
    "        2. Expected step order: depreciation -> sub-limits -> sum ->\n",
    "           deductible -> co-insurance -> coverage limit\n",
    "        3. Check if all policy terms are referenced in the trace\n",
    "        4. Compare numbers in trace against claim/policy numbers\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought questions:**\n",
    "- Which error category do you expect to be most common? Why?\n",
    "- If the model consistently applies deductible before depreciation, is this an SFT problem or a GRPO problem?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.8 Scalability and Deployment\n",
    "\n",
    "ClearClaim needs to serve ~3,300 claims per day within a 15-second latency budget."
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_inference(model, tokenizer, test_claims: List[InsuranceClaim],\n",
    "                        n_runs: int = 50) -> dict:\n",
    "    \"\"\"Benchmark inference latency for the reasoning model.\"\"\"\n",
    "    latencies = []\n",
    "    token_counts = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for claim in test_claims[:n_runs]:\n",
    "            prompt = format_reasoning_prompt(claim)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "            start = time.time()\n",
    "            outputs = model.generate(inputs.input_ids, max_new_tokens=512, temperature=0.0, do_sample=False)\n",
    "            elapsed = time.time() - start\n",
    "            n_tokens = outputs.shape[1] - inputs.input_ids.shape[1]\n",
    "            latencies.append(elapsed)\n",
    "            token_counts.append(n_tokens)\n",
    "\n",
    "    return {\n",
    "        'mean_latency_s': np.mean(latencies),\n",
    "        'p95_latency_s': np.percentile(latencies, 95),\n",
    "        'mean_tokens': np.mean(token_counts),\n",
    "        'tokens_per_second': np.mean([t/l for t, l in zip(token_counts, latencies)]),\n",
    "    }\n",
    "\n",
    "# results = benchmark_inference(model, tokenizer, claims[-50:])\n",
    "# print(f\"Mean latency: {results['mean_latency_s']:.2f}s\")\n",
    "# print(f\"P95 latency: {results['p95_latency_s']:.2f}s\")\n",
    "# print(f\"Tokens/sec: {results['tokens_per_second']:.1f}\")"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 8: Write an inference optimization plan.**"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_inference_optimization(benchmark_results: dict, target_latency_s: float = 15.0) -> str:\n",
    "    \"\"\"\n",
    "    Analyze benchmarks and recommend optimizations.\n",
    "\n",
    "    Consider:\n",
    "    1. KV-cache optimization\n",
    "    2. Speculative decoding\n",
    "    3. Model quantization (INT8/INT4)\n",
    "    4. Batching strategy\n",
    "    5. Early stopping after answer detection\n",
    "\n",
    "    For each: estimate speedup, accuracy tradeoff, implementation complexity.\n",
    "\n",
    "    Returns:\n",
    "        Formatted string report\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.9 Ethical and Regulatory Analysis\n",
    "\n",
    "Insurance claims adjudication directly affects people's financial well-being. Regulatory compliance is mandatory, not optional.\n",
    "\n",
    "**TODO 9: Conduct an ethical impact assessment.**"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ethical_impact_assessment(model, tokenizer, test_claims: List[InsuranceClaim]) -> str:\n",
    "    \"\"\"\n",
    "    Address:\n",
    "    1. BIAS: Does accuracy vary by claim type or payout size?\n",
    "    2. FAIRNESS: Demographic parity and equalized odds across subgroups\n",
    "    3. REGULATION: NAIC Model Bulletin, state commissioner guidelines, SOC 2\n",
    "    4. FAILURE RISKS: Worst-case overpayment vs underpayment scenarios\n",
    "    5. HUMAN-IN-THE-LOOP: When to auto-approve vs require human review\n",
    "\n",
    "    Hints:\n",
    "        1. Group claims by type and compare error rates\n",
    "        2. Underpayment causes more individual harm than overpayment\n",
    "        3. A good human-review threshold: claims where top-2 predictions\n",
    "           differ by more than 10%\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought questions:**\n",
    "- Should the model's confidence score affect auto-approval? How would you compute confidence from a generative model?\n",
    "- If the model is more accurate on large claims, is this a fairness issue?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you built the complete pipeline for training a reasoning model for insurance claims adjudication:\n",
    "\n",
    "1. **Data:** Synthetic insurance claims with deterministically verifiable payouts\n",
    "2. **Baseline:** Direct payout prediction (no reasoning)\n",
    "3. **SFT:** Supervised fine-tuning on chain-of-thought traces to learn the FORMAT of reasoning\n",
    "4. **GRPO:** Reinforcement learning with verifiable rewards to learn the QUALITY of reasoning\n",
    "5. **Evaluation:** Comprehensive accuracy, step-level verification, and error analysis\n",
    "6. **Deployment:** Latency benchmarking and optimization planning\n",
    "7. **Ethics:** Bias analysis, fairness metrics, and regulatory compliance\n",
    "\n",
    "The key insight: by defining what success looks like (correct payout amounts) and letting the model discover HOW to get there, GRPO produces emergent reasoning behaviors -- self-verification, backtracking, and extended thinking for complex claims -- that SFT alone cannot achieve.\n",
    "\n",
    "For the full production system design (architecture diagrams, API design, monitoring, A/B testing, and cost analysis), refer to Section 4 of the case study document."
   ],
   "id": "cell_39"
  }
 ]
}