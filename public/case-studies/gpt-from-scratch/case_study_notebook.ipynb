{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "GPT Case Study \u2014 Implementation Notebook"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Autoregressive Contract Clause Generation\n",
    "## Implementation Notebook\n",
    "\n",
    "---\n",
    "\n",
    "**Scenario:** You are an ML engineer at Lexis Draft AI, a legal technology startup building a contract drafting assistant for law firms. Your task is to build a GPT-style autoregressive model that generates contextually appropriate contract clauses conditioned on deal parameters and previously drafted sections.\n",
    "\n",
    "**Current system:** A retrieval-based pipeline with 62% clause relevance score and 38% first-draft acceptance rate. Your target: 85%+ clause relevance and 65%+ first-draft acceptance.\n",
    "\n",
    "**Why GPT:** The retrieval system cannot generate novel clauses for unprecedented deal structures, cannot maintain defined-term consistency across sections, and cannot adapt to firm-specific drafting style. A GPT-style autoregressive model generates one token at a time conditioned on the full context, naturally handling all three failure modes.\n",
    "\n",
    "---"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data Acquisition and Preprocessing\n",
    "\n",
    "We use a synthetic legal clause dataset for this notebook. In the Lexis Draft AI scenario, the real training data would consist of 405,000 contract clauses from 18 law firm clients and SEC EDGAR filings. The synthetic dataset mirrors the structure and patterns of real contract language while being freely usable."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install matplotlib numpy -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic legal clause corpus\n",
    "# These clauses are representative of real contract language patterns\n",
    "LEGAL_CLAUSES = [\n",
    "    # Limitation of Liability\n",
    "    \"IN NO EVENT SHALL EITHER PARTY BE LIABLE TO THE OTHER PARTY FOR ANY INDIRECT INCIDENTAL SPECIAL CONSEQUENTIAL OR PUNITIVE DAMAGES ARISING OUT OF OR RELATED TO THIS AGREEMENT REGARDLESS OF WHETHER SUCH DAMAGES ARE BASED ON CONTRACT TORT NEGLIGENCE STRICT LIABILITY OR ANY OTHER THEORY EVEN IF SUCH PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES\",\n",
    "    \"THE AGGREGATE LIABILITY OF LICENSOR UNDER THIS AGREEMENT SHALL NOT EXCEED THE TOTAL FEES PAID BY LICENSEE DURING THE TWELVE MONTH PERIOD IMMEDIATELY PRECEDING THE EVENT GIVING RISE TO SUCH LIABILITY\",\n",
    "    \"NOTWITHSTANDING ANYTHING TO THE CONTRARY HEREIN THE LIMITATIONS SET FORTH IN THIS SECTION SHALL NOT APPLY TO A PARTY BREACH OF ITS CONFIDENTIALITY OBLIGATIONS OR A PARTY INDEMNIFICATION OBLIGATIONS UNDER THIS AGREEMENT\",\n",
    "\n",
    "    # Indemnification\n",
    "    \"LICENSEE SHALL INDEMNIFY DEFEND AND HOLD HARMLESS LICENSOR AND ITS OFFICERS DIRECTORS EMPLOYEES AND AGENTS FROM AND AGAINST ANY AND ALL CLAIMS DAMAGES LOSSES LIABILITIES COSTS AND EXPENSES INCLUDING REASONABLE ATTORNEYS FEES ARISING OUT OF OR RELATED TO LICENSEE USE OF THE SOFTWARE IN VIOLATION OF THIS AGREEMENT\",\n",
    "    \"LICENSOR SHALL INDEMNIFY DEFEND AND HOLD HARMLESS LICENSEE FROM AND AGAINST ANY CLAIMS ALLEGING THAT THE SOFTWARE AS PROVIDED BY LICENSOR INFRINGES ANY UNITED STATES PATENT COPYRIGHT OR TRADE SECRET OF A THIRD PARTY\",\n",
    "    \"THE INDEMNIFYING PARTY SHALL HAVE THE RIGHT TO CONTROL THE DEFENSE AND SETTLEMENT OF ANY CLAIM SUBJECT TO THE INDEMNIFIED PARTY CONSENT WHICH SHALL NOT BE UNREASONABLY WITHHELD\",\n",
    "\n",
    "    # Confidentiality\n",
    "    \"EACH PARTY AGREES TO HOLD IN STRICT CONFIDENCE ALL CONFIDENTIAL INFORMATION RECEIVED FROM THE OTHER PARTY AND SHALL NOT DISCLOSE SUCH INFORMATION TO ANY THIRD PARTY WITHOUT THE PRIOR WRITTEN CONSENT OF THE DISCLOSING PARTY\",\n",
    "    \"CONFIDENTIAL INFORMATION SHALL NOT INCLUDE INFORMATION THAT IS OR BECOMES PUBLICLY AVAILABLE THROUGH NO FAULT OF THE RECEIVING PARTY WAS IN THE RECEIVING PARTY POSSESSION PRIOR TO DISCLOSURE OR IS INDEPENDENTLY DEVELOPED BY THE RECEIVING PARTY WITHOUT USE OF THE DISCLOSING PARTY CONFIDENTIAL INFORMATION\",\n",
    "    \"THE OBLIGATIONS OF CONFIDENTIALITY SET FORTH HEREIN SHALL SURVIVE THE TERMINATION OR EXPIRATION OF THIS AGREEMENT FOR A PERIOD OF THREE YEARS\",\n",
    "\n",
    "    # Termination\n",
    "    \"EITHER PARTY MAY TERMINATE THIS AGREEMENT FOR CAUSE UPON THIRTY DAYS PRIOR WRITTEN NOTICE IF THE OTHER PARTY MATERIALLY BREACHES ANY PROVISION OF THIS AGREEMENT AND FAILS TO CURE SUCH BREACH WITHIN THE NOTICE PERIOD\",\n",
    "    \"UPON TERMINATION OF THIS AGREEMENT FOR ANY REASON LICENSEE SHALL IMMEDIATELY CEASE ALL USE OF THE SOFTWARE AND SHALL RETURN OR DESTROY ALL COPIES OF THE SOFTWARE AND CONFIDENTIAL INFORMATION IN ITS POSSESSION\",\n",
    "    \"THE FOLLOWING PROVISIONS SHALL SURVIVE ANY TERMINATION OR EXPIRATION OF THIS AGREEMENT CONFIDENTIALITY LIMITATION OF LIABILITY INDEMNIFICATION AND ANY PROVISIONS WHICH BY THEIR NATURE ARE INTENDED TO SURVIVE\",\n",
    "\n",
    "    # Representations and Warranties\n",
    "    \"LICENSOR REPRESENTS AND WARRANTS THAT IT HAS THE FULL RIGHT POWER AND AUTHORITY TO ENTER INTO THIS AGREEMENT AND TO GRANT THE LICENSES AND RIGHTS GRANTED HEREIN\",\n",
    "    \"LICENSEE REPRESENTS AND WARRANTS THAT IT SHALL USE THE SOFTWARE IN COMPLIANCE WITH ALL APPLICABLE LAWS RULES AND REGULATIONS\",\n",
    "    \"EXCEPT AS EXPRESSLY SET FORTH IN THIS AGREEMENT LICENSOR MAKES NO WARRANTIES EXPRESS OR IMPLIED INCLUDING WITHOUT LIMITATION ANY IMPLIED WARRANTIES OF MERCHANTABILITY FITNESS FOR A PARTICULAR PURPOSE OR NONINFRINGEMENT\",\n",
    "\n",
    "    # Governing Law and Dispute Resolution\n",
    "    \"THIS AGREEMENT SHALL BE GOVERNED BY AND CONSTRUED IN ACCORDANCE WITH THE LAWS OF THE STATE OF DELAWARE WITHOUT REGARD TO ITS CONFLICT OF LAWS PRINCIPLES\",\n",
    "    \"ANY DISPUTE ARISING OUT OF OR RELATING TO THIS AGREEMENT SHALL BE RESOLVED BY BINDING ARBITRATION ADMINISTERED BY THE AMERICAN ARBITRATION ASSOCIATION IN ACCORDANCE WITH ITS COMMERCIAL ARBITRATION RULES\",\n",
    "    \"THE PARTIES AGREE THAT ANY LEGAL ACTION OR PROCEEDING ARISING UNDER THIS AGREEMENT SHALL BE BROUGHT EXCLUSIVELY IN THE FEDERAL OR STATE COURTS LOCATED IN WILMINGTON DELAWARE\",\n",
    "\n",
    "    # Assignment\n",
    "    \"NEITHER PARTY MAY ASSIGN OR TRANSFER THIS AGREEMENT OR ANY RIGHTS OR OBLIGATIONS HEREUNDER WITHOUT THE PRIOR WRITTEN CONSENT OF THE OTHER PARTY EXCEPT THAT EITHER PARTY MAY ASSIGN THIS AGREEMENT WITHOUT CONSENT IN CONNECTION WITH A MERGER ACQUISITION OR SALE OF ALL OR SUBSTANTIALLY ALL OF ITS ASSETS\",\n",
    "\n",
    "    # Force Majeure\n",
    "    \"NEITHER PARTY SHALL BE LIABLE FOR ANY FAILURE OR DELAY IN PERFORMING ITS OBLIGATIONS UNDER THIS AGREEMENT TO THE EXTENT SUCH FAILURE OR DELAY RESULTS FROM CIRCUMSTANCES BEYOND THE REASONABLE CONTROL OF SUCH PARTY INCLUDING BUT NOT LIMITED TO ACTS OF GOD NATURAL DISASTERS PANDEMIC GOVERNMENT ACTIONS WAR TERRORISM OR CIVIL UNREST\",\n",
    "\n",
    "    # Intellectual Property\n",
    "    \"ALL INTELLECTUAL PROPERTY RIGHTS IN AND TO THE SOFTWARE INCLUDING ALL MODIFICATIONS ENHANCEMENTS AND DERIVATIVE WORKS SHALL REMAIN THE EXCLUSIVE PROPERTY OF LICENSOR AND NOTHING IN THIS AGREEMENT SHALL BE CONSTRUED AS TRANSFERRING ANY OWNERSHIP RIGHTS TO LICENSEE\",\n",
    "]\n",
    "\n",
    "# Repeat and augment to create a larger corpus\n",
    "corpus_text = \"\\n\\n\".join(LEGAL_CLAUSES * 20)\n",
    "print(f\"Corpus size: {len(corpus_text)} characters\")\n",
    "print(f\"Sample:\\n{corpus_text[:200]}...\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1: Build a Character-Level Tokenizer and Prepare Training Data"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokenizer_and_data(corpus_text, max_seq_len=256):\n",
    "    \"\"\"\n",
    "    Build a character-level tokenizer for the legal corpus and prepare\n",
    "    training sequences.\n",
    "\n",
    "    Args:\n",
    "        corpus_text: string containing the full training corpus\n",
    "        max_seq_len: maximum sequence length for training chunks\n",
    "\n",
    "    Returns:\n",
    "        - encode: function mapping string -> list[int]\n",
    "        - decode: function mapping list[int] -> string\n",
    "        - train_data: tensor of shape (num_sequences, max_seq_len)\n",
    "        - vocab_size: int\n",
    "\n",
    "    Steps:\n",
    "        1. Collect all unique characters in corpus_text, sort them.\n",
    "        2. Create char-to-id and id-to-char mappings.\n",
    "        3. Define encode(text) and decode(ids) functions.\n",
    "        4. Encode the full corpus into a 1D tensor.\n",
    "        5. Reshape into non-overlapping chunks of max_seq_len.\n",
    "           Discard any leftover tokens that do not fill a complete chunk.\n",
    "        6. Return the four values.\n",
    "\n",
    "    Hints:\n",
    "        - chars = sorted(set(corpus_text))\n",
    "        - char_to_id = {ch: i for i, ch in enumerate(chars)}\n",
    "        - To chunk: total_tokens = (len(encoded) // max_seq_len) * max_seq_len\n",
    "          then encoded[:total_tokens].view(-1, max_seq_len)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "# Build tokenizer and data\n",
    "encode, decode, train_data, vocab_size = build_tokenizer_and_data(corpus_text)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Training sequences: {train_data.shape[0]}\")\n",
    "print(f\"Sequence length: {train_data.shape[1]}\")\n",
    "print(f\"\\nSample decoded sequence:\\n{decode(train_data[0].tolist())[:100]}...\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Model Architecture: Building a GPT from Scratch\n",
    "\n",
    "We build the complete GPT model following the architecture from the article: token embeddings, positional embeddings, N Transformer blocks (each with causal multi-head self-attention, layer normalization, and a feed-forward network), a final layer norm, and a linear projection to the vocabulary.\n",
    "\n",
    "### TODO 2: Implement Causal Self-Attention"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head causal self-attention.\n",
    "\n",
    "    Each token attends only to tokens at the same or earlier\n",
    "    positions. Future positions are masked with -inf before softmax.\n",
    "\n",
    "    Args:\n",
    "        d_model: model embedding dimension\n",
    "        n_heads: number of attention heads\n",
    "        max_seq_len: maximum sequence length\n",
    "\n",
    "    Forward:\n",
    "        Input: x of shape (batch, seq_len, d_model)\n",
    "        Output: out of shape (batch, seq_len, d_model)\n",
    "\n",
    "    Implementation steps:\n",
    "        1. Project input to Q, K, V using a single linear layer\n",
    "           (d_model -> 3 * d_model), then split into Q, K, V.\n",
    "        2. Reshape Q, K, V to (batch, n_heads, seq_len, d_k)\n",
    "           where d_k = d_model // n_heads.\n",
    "        3. Compute attention scores: (Q @ K^T) / sqrt(d_k).\n",
    "        4. Apply causal mask: set upper-triangle entries to -inf.\n",
    "        5. Apply softmax to get attention weights.\n",
    "        6. Multiply attention weights by V.\n",
    "        7. Reshape back to (batch, seq_len, d_model).\n",
    "        8. Apply output projection (d_model -> d_model).\n",
    "\n",
    "    Hints:\n",
    "        - Register the causal mask as a buffer (not a parameter)\n",
    "          using self.register_buffer\n",
    "        - torch.triu(torch.ones(max_seq_len, max_seq_len), diagonal=1).bool()\n",
    "          creates the mask\n",
    "        - Use .masked_fill(mask, float('-inf')) before softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "        # YOUR CODE HERE: define self.qkv, self.proj, and register causal mask buffer\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 3: Implement Transformer Block and GPT Model"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Single GPT Transformer block (Pre-LN architecture).\n",
    "\n",
    "    Architecture:\n",
    "        x = x + Attention(LayerNorm(x))\n",
    "        x = x + FFN(LayerNorm(x))\n",
    "\n",
    "    FFN: Linear(d_model, 4*d_model) -> GELU -> Linear(4*d_model, d_model)\n",
    "\n",
    "    Args:\n",
    "        d_model: model dimension\n",
    "        n_heads: number of attention heads\n",
    "        max_seq_len: maximum sequence length\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete GPT language model.\n",
    "\n",
    "    Architecture:\n",
    "        1. Token embedding table (vocab_size x d_model)\n",
    "        2. Positional embedding table (max_seq_len x d_model)\n",
    "        3. Stack of N TransformerBlocks\n",
    "        4. Final LayerNorm\n",
    "        5. Linear head projecting d_model -> vocab_size (no bias)\n",
    "\n",
    "    Forward pass:\n",
    "        1. Look up token embeddings\n",
    "        2. Add positional embeddings for positions 0..T-1\n",
    "        3. Pass through all Transformer blocks\n",
    "        4. Apply final LayerNorm\n",
    "        5. Project to vocabulary logits\n",
    "\n",
    "    Args:\n",
    "        vocab_size, d_model, n_heads, n_layers, max_seq_len\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=128,\n",
    "    n_heads=4,\n",
    "    n_layers=4,\n",
    "    max_seq_len=256,\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {model.count_parameters():,}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Training: Loss and Backpropagation\n",
    "\n",
    "The training loop implements the exact four-step process from the article: forward pass (compute logits), compute cross-entropy loss, backward pass (compute gradients), and weight update (optimizer step).\n",
    "\n",
    "### TODO 4: Implement the Training Loop"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, num_steps=2000, batch_size=32,\n",
    "                learning_rate=3e-4, eval_interval=200, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train the GPT model on next-token prediction.\n",
    "\n",
    "    Training step:\n",
    "        1. Sample a random batch of sequences from train_data.\n",
    "        2. Inputs x = batch[:, :-1] (all tokens except last).\n",
    "           Targets y = batch[:, 1:] (all tokens except first).\n",
    "        3. Forward pass: logits = model(x).\n",
    "        4. Compute loss: F.cross_entropy on flattened logits and targets.\n",
    "        5. optimizer.zero_grad(), loss.backward(), optimizer.step().\n",
    "\n",
    "    Args:\n",
    "        model: GPT model\n",
    "        train_data: tensor of shape (N, seq_len)\n",
    "        num_steps: number of training iterations\n",
    "        batch_size: batch size\n",
    "        learning_rate: AdamW learning rate\n",
    "        eval_interval: steps between loss logging\n",
    "        device: torch device\n",
    "\n",
    "    Returns:\n",
    "        list of (step, train_loss) tuples\n",
    "\n",
    "    Hints:\n",
    "        - Use torch.optim.AdamW\n",
    "        - Sample batch indices: torch.randint(0, len(train_data), (batch_size,))\n",
    "        - Flatten logits: logits.view(-1, vocab_size)\n",
    "        - Flatten targets: y.view(-1)\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    losses = []\n",
    "\n",
    "    # YOUR CODE HERE: implement the training loop\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "train_losses = train_model(\n",
    "    model, train_data,\n",
    "    num_steps=2000,\n",
    "    batch_size=32,\n",
    "    learning_rate=3e-4,\n",
    "    eval_interval=100,\n",
    "    device=device\n",
    ")\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nTraining complete in {elapsed:.1f} seconds\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "steps, losses = zip(*train_losses)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(steps, losses, alpha=0.3, color='blue')\n",
    "\n",
    "# Smoothed curve\n",
    "window = 5\n",
    "smoothed = [np.mean(losses[max(0,i-window):i+1]) for i in range(len(losses))]\n",
    "plt.plot(steps, smoothed, color='blue', linewidth=2, label='Smoothed loss')\n",
    "\n",
    "random_baseline = np.log(vocab_size)\n",
    "plt.axhline(y=random_baseline, color='red', linestyle='--',\n",
    "            label=f'Random baseline ({random_baseline:.2f})')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('GPT Training Loss on Legal Contract Clauses')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Generation: Sampling from the Trained Model\n",
    "\n",
    "With the model trained, we generate contract clauses autoregressively. Each token is sampled from the model's predicted distribution, conditioned on all previously generated tokens.\n",
    "\n",
    "### TODO 5: Implement Autoregressive Generation"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, encode, decode, prompt, max_new_tokens=200,\n",
    "             temperature=0.8, top_k=40, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate text autoregressively from a prompt.\n",
    "\n",
    "    Algorithm:\n",
    "        1. Encode the prompt string to token IDs.\n",
    "        2. At each step:\n",
    "           a. Feed the current sequence through the model.\n",
    "           b. Take logits at the last position.\n",
    "           c. Divide by temperature.\n",
    "           d. If top_k > 0, zero out all logits below the k-th largest.\n",
    "           e. Apply softmax to get probabilities.\n",
    "           f. Sample one token from the distribution.\n",
    "           g. Append to the sequence.\n",
    "        3. Decode the full sequence back to text.\n",
    "\n",
    "    Args:\n",
    "        model: trained GPT model\n",
    "        encode: tokenizer encode function\n",
    "        decode: tokenizer decode function\n",
    "        prompt: string prompt to condition on\n",
    "        max_new_tokens: maximum tokens to generate\n",
    "        temperature: sampling temperature\n",
    "        top_k: top-k filtering (0 = disabled)\n",
    "        device: torch device\n",
    "\n",
    "    Returns:\n",
    "        generated_text: string (prompt + generated continuation)\n",
    "\n",
    "    Hints:\n",
    "        - model.eval() before generation\n",
    "        - Crop sequence to max_seq_len if it exceeds the model's limit\n",
    "        - For top-k: v, _ = torch.topk(logits, top_k)\n",
    "          then logits[logits < v[..., [-1]]] = float('-inf')\n",
    "        - torch.multinomial(probs, num_samples=1) for sampling\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "# Generate sample clauses\n",
    "prompts = [\n",
    "    \"IN NO EVENT SHALL\",\n",
    "    \"LICENSEE SHALL INDEMNIFY\",\n",
    "    \"THIS AGREEMENT SHALL BE\",\n",
    "    \"NEITHER PARTY MAY\",\n",
    "    \"EACH PARTY AGREES\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  GENERATED CONTRACT CLAUSES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate(model, encode, decode, prompt,\n",
    "                        max_new_tokens=200, temperature=0.7,\n",
    "                        top_k=30, device=device)\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(f\"Generated:\\n{generated}\")\n",
    "    print(\"-\" * 60)"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Evaluation and Analysis\n",
    "\n",
    "### TODO 6: Compute Perplexity and Analyze Generation Quality"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_data, encode, decode, vocab_size,\n",
    "                   device='cpu', num_eval_batches=20, batch_size=32):\n",
    "    \"\"\"\n",
    "    Evaluate the trained model.\n",
    "\n",
    "    Compute:\n",
    "        1. Perplexity on a held-out portion of the data:\n",
    "           perplexity = exp(average cross-entropy loss)\n",
    "        2. Generation latency: tokens per second on GPU/CPU\n",
    "        3. Character-level accuracy: percentage of correctly predicted\n",
    "           next characters across evaluation data\n",
    "\n",
    "    Args:\n",
    "        model: trained GPT model\n",
    "        train_data: full training data tensor\n",
    "        encode, decode: tokenizer functions\n",
    "        vocab_size: vocabulary size\n",
    "        device: torch device\n",
    "        num_eval_batches: number of batches for evaluation\n",
    "        batch_size: batch size\n",
    "\n",
    "    Returns:\n",
    "        dict with 'perplexity', 'tokens_per_second', 'accuracy'\n",
    "\n",
    "    Hints:\n",
    "        - Use the last 10% of train_data as eval data\n",
    "        - Perplexity = exp(mean_loss)\n",
    "        - For latency, generate 200 tokens and measure wall-clock time\n",
    "        - Accuracy: (logits.argmax(-1) == targets).float().mean()\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "results = evaluate_model(model, train_data, encode, decode,\n",
    "                         vocab_size, device=device)\n",
    "print(f\"\\nEvaluation Results:\")\n",
    "print(f\"  Perplexity: {results['perplexity']:.2f}\")\n",
    "print(f\"  Tokens/second: {results['tokens_per_second']:.0f}\")\n",
    "print(f\"  Next-char accuracy: {results['accuracy']:.1%}\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention weight visualization\n",
    "def visualize_attention(model, encode, text, layer_idx=-1, head_idx=0,\n",
    "                        device='cpu'):\n",
    "    \"\"\"\n",
    "    Visualize attention weights for a given input text.\n",
    "\n",
    "    Extracts attention weights from the specified layer and head,\n",
    "    and plots them as a heatmap showing which tokens attend to which.\n",
    "\n",
    "    Args:\n",
    "        model: GPT model (must be modified to return attention weights,\n",
    "               or use forward hooks)\n",
    "        encode: tokenizer encode function\n",
    "        text: input string to visualize\n",
    "        layer_idx: which transformer block (-1 = last)\n",
    "        head_idx: which attention head\n",
    "        device: torch device\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Register a forward hook to capture attention weights\n",
    "    attention_weights = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        # Capture the attention weights before they are used\n",
    "        # This requires modifying forward to also return weights,\n",
    "        # or computing them here from the Q, K we can access\n",
    "        x = input[0]\n",
    "        B, T, C = x.shape\n",
    "        qkv = module.qkv(x).reshape(B, T, 3, module.n_heads, module.d_k)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k = qkv[0], qkv[1]\n",
    "        att = (q @ k.transpose(-2, -1)) / (module.d_k ** 0.5)\n",
    "        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
    "        att = att.masked_fill(mask, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        attention_weights.append(att.detach().cpu())\n",
    "\n",
    "    # Register hook on the target layer\n",
    "    target_layer = list(model.blocks)[layer_idx]\n",
    "    hook = target_layer.attn.register_forward_hook(hook_fn)\n",
    "\n",
    "    # Forward pass\n",
    "    ids = torch.tensor([encode(text)], device=device)\n",
    "    with torch.no_grad():\n",
    "        model(ids)\n",
    "\n",
    "    hook.remove()\n",
    "\n",
    "    # Plot\n",
    "    if attention_weights:\n",
    "        attn = attention_weights[0][0, head_idx].numpy()\n",
    "        chars = list(text[:ids.shape[1]])\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        im = ax.imshow(attn[:len(chars), :len(chars)], cmap='Blues')\n",
    "        ax.set_xticks(range(len(chars)))\n",
    "        ax.set_yticks(range(len(chars)))\n",
    "        ax.set_xticklabels(chars, rotation=90, fontfamily='monospace', fontsize=8)\n",
    "        ax.set_yticklabels(chars, fontfamily='monospace', fontsize=8)\n",
    "        ax.set_xlabel('Key (attending to)')\n",
    "        ax.set_ylabel('Query (attending from)')\n",
    "        ax.set_title(f'Attention Weights (Layer {layer_idx}, Head {head_idx})')\n",
    "        plt.colorbar(im)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Visualize attention on a sample clause\n",
    "sample_text = \"IN NO EVENT SHALL EITHER PARTY BE\"\n",
    "visualize_attention(model, encode, sample_text, layer_idx=-1, head_idx=0,\n",
    "                    device=device)"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of results\n",
    "print(\"=\" * 60)\n",
    "print(\"  CASE STUDY SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Model: GPT-style autoregressive transformer\")\n",
    "print(f\"Parameters: {model.count_parameters():,}\")\n",
    "print(f\"Architecture: {4} layers, {4} heads, d_model={128}\")\n",
    "print(f\"Training data: {len(train_data)} sequences of legal clauses\")\n",
    "print()\n",
    "print(\"Key findings:\")\n",
    "print(f\"  1. Final training loss significantly below random baseline\")\n",
    "print(f\"     (random baseline = log({vocab_size}) = {np.log(vocab_size):.2f})\")\n",
    "print(f\"  2. Model generates coherent legal language that follows\")\n",
    "print(f\"     contract drafting conventions\")\n",
    "print(f\"  3. Causal attention allows each token to condition on all\")\n",
    "print(f\"     previous context, maintaining clause consistency\")\n",
    "print(f\"  4. The same architecture scales from this notebook demo\")\n",
    "print(f\"     to production models with 350M+ parameters\")\n",
    "print()\n",
    "print(\"Production considerations:\")\n",
    "print(\"  - Scale to 350M parameters for production quality\")\n",
    "print(\"  - Use BPE tokenizer trained on legal corpus\")\n",
    "print(\"  - Add LoRA adapters for firm-specific style\")\n",
    "print(\"  - Implement KV-cache for efficient generation\")\n",
    "print(\"  - Deploy with vLLM for continuous batching\")"
   ],
   "id": "cell_19"
  }
 ]
}