{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Real-Time Clinical Documentation with Diffusion LLMs ‚Äî Implementation Notebook"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Real-Time Clinical Documentation with Diffusion Language Models\n",
    "## Implementation Notebook\n",
    "\n",
    "*MedScribe AI -- Building a masked diffusion language model for clinical note generation*\n",
    "*Estimated time: 60-75 minutes (including ~10 minutes of training)*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ü§ñ AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** ‚Äî it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[üëâ Open AI Teaching Assistant](https://pods.vizuara.ai/courses/diffusion-llms/practice/0/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment\n",
    "\n",
    "Before we begin, let us set up our environment and install the necessary dependencies. This notebook is designed to run on Google Colab with a T4 GPU."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q datasets rouge-score numpy matplotlib seaborn tqdm"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Plot styling\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "print(\"All imports ready.\")"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Industry Context: The Clinical Documentation Problem\n",
    "\n",
    "MedScribe AI is a health-tech startup building an AI-powered clinical documentation assistant. Their current autoregressive model generates structured SOAP notes (Subjective, Objective, Assessment, Plan) from patient-physician conversations, but it suffers from three critical limitations:\n",
    "\n",
    "1. **Latency**: 12-20 seconds per note, too slow for real-time use during encounters\n",
    "2. **No bidirectional editing**: Edits in one section cannot propagate to other sections\n",
    "3. **Brittle infilling**: Completing partially-filled templates yields poor quality (52% vs 68% acceptance rate)\n",
    "\n",
    "In this notebook, we will build a **masked diffusion language model** that addresses all three problems by generating clinical notes through iterative unmasking rather than left-to-right token generation.\n",
    "\n",
    "Our goal: generate a complete SOAP note in under 500ms by predicting all tokens in parallel across 15-25 denoising steps, while maintaining physician acceptance quality."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Acquisition and Preprocessing\n",
    "\n",
    "We use a synthetic clinical notes dataset that mimics the structure of MIMIC-III discharge summaries. Each note follows the SOAP format with clearly delimited sections.\n",
    "\n",
    "In a production setting, MedScribe would train on de-identified MIMIC-III data under a PhysioNet credential. For this notebook, we generate realistic synthetic data that captures the key structural properties: section headers, clinical vocabulary, and variable note lengths."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic clinical note generator\n",
    "# In production, this would be replaced by MIMIC-III data loading\n",
    "\n",
    "SECTION_HEADERS = [\"HPI:\", \"ROS:\", \"EXAM:\", \"ASSESSMENT:\", \"PLAN:\"]\n",
    "SECTION_LABELS = {\"HPI\": 0, \"ROS\": 1, \"EXAM\": 2, \"ASSESSMENT\": 3, \"PLAN\": 4, \"OTHER\": 5}\n",
    "\n",
    "# Clinical vocabulary pools for synthetic data generation\n",
    "HPI_PHRASES = [\n",
    "    \"patient presents with\", \"chief complaint of\", \"reports onset of\",\n",
    "    \"denies any\", \"history of\", \"states that symptoms began\",\n",
    "    \"worsening over the past\", \"associated with\", \"no prior episodes of\",\n",
    "    \"previously treated with\", \"medication compliance has been\",\n",
    "    \"symptoms include\", \"pain described as\", \"located in the\",\n",
    "    \"radiating to\", \"aggravated by\", \"relieved by\", \"duration of\",\n",
    "    \"frequency of episodes\", \"last seen by physician on\",\n",
    "]\n",
    "\n",
    "ROS_PHRASES = [\n",
    "    \"negative for fever\", \"denies chest pain\", \"no shortness of breath\",\n",
    "    \"reports mild fatigue\", \"denies nausea or vomiting\", \"no weight changes\",\n",
    "    \"positive for headache\", \"denies dizziness\", \"no vision changes\",\n",
    "    \"reports occasional\", \"constitutional symptoms absent\",\n",
    "    \"cardiovascular review unremarkable\", \"respiratory review negative\",\n",
    "    \"gastrointestinal symptoms denied\", \"musculoskeletal pain noted\",\n",
    "]\n",
    "\n",
    "EXAM_PHRASES = [\n",
    "    \"vitals BP\", \"HR\", \"RR\", \"temp\", \"SpO2\", \"BMI\",\n",
    "    \"general appearance alert and oriented\", \"no acute distress\",\n",
    "    \"lungs clear to auscultation bilaterally\", \"heart regular rate and rhythm\",\n",
    "    \"abdomen soft nontender\", \"extremities no edema\",\n",
    "    \"neurological exam grossly intact\", \"skin warm and dry\",\n",
    "    \"HEENT normocephalic atraumatic\", \"neck supple no lymphadenopathy\",\n",
    "]\n",
    "\n",
    "ASSESSMENT_PHRASES = [\n",
    "    \"type 2 diabetes mellitus\", \"essential hypertension\",\n",
    "    \"hyperlipidemia\", \"chronic kidney disease stage\",\n",
    "    \"osteoarthritis of the\", \"major depressive disorder\",\n",
    "    \"generalized anxiety disorder\", \"chronic low back pain\",\n",
    "    \"well controlled\", \"poorly controlled\", \"stable\",\n",
    "    \"acute exacerbation of\", \"new diagnosis of\", \"suspected\",\n",
    "    \"differential includes\", \"consistent with\", \"likely secondary to\",\n",
    "]\n",
    "\n",
    "PLAN_PHRASES = [\n",
    "    \"continue current medications\", \"increase dose of\", \"add\",\n",
    "    \"refer to specialist\", \"follow up in\", \"weeks\",\n",
    "    \"obtain labs including\", \"CBC CMP lipid panel\",\n",
    "    \"schedule imaging\", \"CT scan\", \"MRI of\",\n",
    "    \"lifestyle modifications discussed\", \"diet and exercise counseling\",\n",
    "    \"return if symptoms worsen\", \"patient educated on\",\n",
    "    \"prescription sent to pharmacy\", \"prior authorization submitted\",\n",
    "]\n",
    "\n",
    "\n",
    "def generate_section(phrases, min_phrases=3, max_phrases=8):\n",
    "    \"\"\"Generate a synthetic clinical section from phrase pools.\"\"\"\n",
    "    n = np.random.randint(min_phrases, max_phrases + 1)\n",
    "    selected = np.random.choice(phrases, size=n, replace=False)\n",
    "    # Add some random numbers for vitals/dates\n",
    "    text = \" \".join(selected)\n",
    "    text = text.replace(\"BP\", f\"BP {np.random.randint(110,160)}/{np.random.randint(60,95)}\")\n",
    "    text = text.replace(\"HR\", f\"HR {np.random.randint(60,100)}\")\n",
    "    text = text.replace(\"temp\", f\"temp {np.random.uniform(97.5, 99.5):.1f}\")\n",
    "    text = text.replace(\"SpO2\", f\"SpO2 {np.random.randint(94,100)}%\")\n",
    "    text = text.replace(\"stage\", f\"stage {np.random.randint(1,5)}\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def generate_synthetic_note():\n",
    "    \"\"\"Generate a complete synthetic SOAP note.\"\"\"\n",
    "    sections = {\n",
    "        \"HPI\": generate_section(HPI_PHRASES, 4, 8),\n",
    "        \"ROS\": generate_section(ROS_PHRASES, 3, 6),\n",
    "        \"EXAM\": generate_section(EXAM_PHRASES, 4, 7),\n",
    "        \"ASSESSMENT\": generate_section(ASSESSMENT_PHRASES, 2, 5),\n",
    "        \"PLAN\": generate_section(PLAN_PHRASES, 3, 7),\n",
    "    }\n",
    "\n",
    "    note = \"\"\n",
    "    for header in SECTION_HEADERS:\n",
    "        section_name = header.replace(\":\", \"\")\n",
    "        note += f\"{header} {sections[section_name]} \"\n",
    "    return note.strip(), sections\n",
    "\n",
    "\n",
    "# Generate dataset\n",
    "NUM_NOTES = 5000\n",
    "notes_data = []\n",
    "for i in range(NUM_NOTES):\n",
    "    note_text, sections = generate_synthetic_note()\n",
    "    notes_data.append({\"text\": note_text, \"sections\": sections})\n",
    "\n",
    "print(f\"Generated {len(notes_data)} synthetic clinical notes\")\n",
    "print(f\"\\nSample note:\\n{notes_data[0]['text'][:500]}...\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Section Parsing\n",
    "\n",
    "A critical preprocessing step is identifying where each SOAP section begins and ends in the tokenized sequence. This is needed for: (a) the section structure loss term, and (b) evaluating whether the model maintains correct note structure."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_section_labels(note_text, tokenizer):\n",
    "    \"\"\"\n",
    "    Parse a clinical note and return per-token section labels.\n",
    "\n",
    "    Given a note with section headers (HPI:, ROS:, EXAM:, ASSESSMENT:, PLAN:),\n",
    "    return a list of integer labels the same length as the tokenized note,\n",
    "    where each label indicates which SOAP section that token belongs to.\n",
    "\n",
    "    Args:\n",
    "        note_text: Raw clinical note text with section headers\n",
    "        tokenizer: Tokenizer with encode() method\n",
    "\n",
    "    Returns:\n",
    "        List[int]: Section label for each token (0=HPI, 1=ROS, 2=EXAM,\n",
    "                   3=ASSESSMENT, 4=PLAN, 5=OTHER)\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(note_text)\n",
    "    labels = []\n",
    "    current_section = SECTION_LABELS[\"OTHER\"]\n",
    "\n",
    "    # Build a character-to-section mapping\n",
    "    char_sections = [SECTION_LABELS[\"OTHER\"]] * len(note_text)\n",
    "    for header in SECTION_HEADERS:\n",
    "        section_name = header.replace(\":\", \"\")\n",
    "        # Find all occurrences of this header\n",
    "        start = 0\n",
    "        while True:\n",
    "            idx = note_text.find(header, start)\n",
    "            if idx == -1:\n",
    "                break\n",
    "            # Find the next header or end of string\n",
    "            next_header_pos = len(note_text)\n",
    "            for other_header in SECTION_HEADERS:\n",
    "                pos = note_text.find(other_header, idx + len(header))\n",
    "                if pos != -1 and pos < next_header_pos:\n",
    "                    next_header_pos = pos\n",
    "            # Label all characters in this section\n",
    "            for i in range(idx, next_header_pos):\n",
    "                char_sections[i] = SECTION_LABELS[section_name]\n",
    "            start = idx + len(header)\n",
    "\n",
    "    return tokens, char_sections"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implement Section Filtering\n",
    "\n",
    "Students must implement the filtering function that ensures each note in the dataset meets quality requirements for training."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_notes(notes_data, tokenizer, min_tokens=50, max_tokens=256,\n",
    "                 min_sections=3):\n",
    "    \"\"\"\n",
    "    Filter clinical notes based on quality criteria.\n",
    "\n",
    "    A note passes the filter if:\n",
    "    1. Its tokenized length is between min_tokens and max_tokens\n",
    "    2. It contains at least min_sections distinct SOAP sections\n",
    "    3. No single section exceeds 60% of the total note length\n",
    "\n",
    "    Args:\n",
    "        notes_data: List of dicts with 'text' and 'sections' keys\n",
    "        tokenizer: Tokenizer with encode() method\n",
    "        min_tokens: Minimum number of tokens per note\n",
    "        max_tokens: Maximum number of tokens per note\n",
    "        min_sections: Minimum number of distinct sections required\n",
    "\n",
    "    Returns:\n",
    "        filtered: List of dicts that pass all criteria\n",
    "        stats: Dict with counts of notes filtered by each criterion\n",
    "\n",
    "    Hints:\n",
    "        - Use tokenizer.encode() to get the token count\n",
    "        - Count distinct sections by checking which SECTION_HEADERS\n",
    "          appear in the note text\n",
    "        - For the 60% rule, tokenize each section individually and\n",
    "          compare to total length\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Initialize filtered list and stats counters\n",
    "    # Step 2: For each note, check the three criteria\n",
    "    # Step 3: Track why notes were filtered (too short, too long,\n",
    "    #         too few sections, section imbalance)\n",
    "    # Step 4: Return the filtered list and stats dict\n",
    "    # ==============================\n",
    "\n",
    "    filtered = ???  # YOUR CODE HERE\n",
    "    stats = ???     # YOUR CODE HERE\n",
    "\n",
    "    return filtered, stats"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification cell -- run after implementing filter_notes\n",
    "\n",
    "# Simple tokenizer for testing\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab_size=2000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word2idx = {\"[PAD]\": 0, \"[MASK]\": 1, \"[UNK]\": 2}\n",
    "        self.idx2word = {0: \"[PAD]\", 1: \"[MASK]\", 2: \"[UNK]\"}\n",
    "        self._next_idx = 3\n",
    "\n",
    "    def build_vocab(self, texts, max_vocab=2000):\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            word_counts.update(text.lower().split())\n",
    "        for word, _ in word_counts.most_common(max_vocab - 3):\n",
    "            if word not in self.word2idx:\n",
    "                self.word2idx[word] = self._next_idx\n",
    "                self.idx2word[self._next_idx] = word\n",
    "                self._next_idx += 1\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.word2idx.get(w, 2) for w in text.lower().split()]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return \" \".join(self.idx2word.get(i, \"[UNK]\") for i in ids)\n",
    "\n",
    "\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.build_vocab([n[\"text\"] for n in notes_data])\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "\n",
    "filtered, stats = filter_notes(notes_data, tokenizer)\n",
    "print(f\"\\nFiltering results:\")\n",
    "print(f\"  Original notes: {len(notes_data)}\")\n",
    "print(f\"  Filtered notes: {len(filtered)}\")\n",
    "print(f\"  Filter stats: {stats}\")\n",
    "assert len(filtered) > 0, \"No notes passed filtering -- check your implementation\"\n",
    "assert len(filtered) < len(notes_data), \"All notes passed -- filter is too lenient\"\n",
    "print(\"\\nFilter implementation looks correct!\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought Questions: Data Preprocessing\n",
    "\n",
    "1. Why do we set a maximum token length? What happens to a diffusion model if training sequences have highly variable lengths?\n",
    "2. Why is the \"no single section exceeds 60% of the note\" rule important for training a balanced model?\n",
    "3. In production, MedScribe would use MIMIC-III data. What additional preprocessing challenges would real clinical text introduce compared to our synthetic data?"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis\n",
    "\n",
    "Before building the model, we need to understand the structure and distribution of our clinical notes dataset."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note length distribution\n",
    "note_lengths = [len(tokenizer.encode(n[\"text\"])) for n in filtered]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Token length distribution\n",
    "axes[0].hist(note_lengths, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].set_xlabel(\"Tokens per Note\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\"Distribution of Note Lengths\")\n",
    "axes[0].axvline(np.mean(note_lengths), color='red', linestyle='--',\n",
    "                label=f'Mean: {np.mean(note_lengths):.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Section length distribution\n",
    "section_lengths = {s: [] for s in SECTION_HEADERS}\n",
    "for note in filtered:\n",
    "    for header in SECTION_HEADERS:\n",
    "        section_name = header.replace(\":\", \"\")\n",
    "        if section_name in note[\"sections\"]:\n",
    "            section_lengths[header].append(\n",
    "                len(tokenizer.encode(note[\"sections\"][section_name]))\n",
    "            )\n",
    "\n",
    "section_means = [np.mean(section_lengths[h]) for h in SECTION_HEADERS]\n",
    "section_stds = [np.std(section_lengths[h]) for h in SECTION_HEADERS]\n",
    "axes[1].bar(range(len(SECTION_HEADERS)), section_means, yerr=section_stds,\n",
    "            capsize=5, color=['#2196F3', '#4CAF50', '#FF9800', '#F44336', '#9C27B0'],\n",
    "            edgecolor='black', alpha=0.8)\n",
    "axes[1].set_xticks(range(len(SECTION_HEADERS)))\n",
    "axes[1].set_xticklabels([h.replace(\":\", \"\") for h in SECTION_HEADERS])\n",
    "axes[1].set_ylabel(\"Tokens per Section\")\n",
    "axes[1].set_title(\"Section Length Distribution\")\n",
    "\n",
    "# Top vocabulary\n",
    "all_words = []\n",
    "for note in filtered:\n",
    "    all_words.extend(note[\"text\"].lower().split())\n",
    "word_counts = Counter(all_words)\n",
    "top_20 = word_counts.most_common(20)\n",
    "words, counts = zip(*top_20)\n",
    "axes[2].barh(range(len(words)), counts, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[2].set_yticks(range(len(words)))\n",
    "axes[2].set_yticklabels(words)\n",
    "axes[2].invert_yaxis()\n",
    "axes[2].set_xlabel(\"Frequency\")\n",
    "axes[2].set_title(\"Top 20 Words in Clinical Notes\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDataset summary:\")\n",
    "print(f\"  Total notes: {len(filtered)}\")\n",
    "print(f\"  Mean note length: {np.mean(note_lengths):.1f} tokens\")\n",
    "print(f\"  Min/Max length: {min(note_lengths)}/{max(note_lengths)} tokens\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: EDA -- Section Ordering Analysis\n",
    "\n",
    "Analyze how consistent the SOAP section ordering is across the dataset. In real clinical notes, sections sometimes appear out of order or are missing entirely."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_section_ordering(notes_data):\n",
    "    \"\"\"\n",
    "    Analyze the section ordering patterns in the clinical notes dataset.\n",
    "\n",
    "    For each note, determine:\n",
    "    1. Which sections are present\n",
    "    2. Whether sections appear in the standard SOAP order\n",
    "    3. Which sections are most frequently missing\n",
    "\n",
    "    Args:\n",
    "        notes_data: List of dicts with 'text' key\n",
    "\n",
    "    Returns:\n",
    "        analysis: Dict with keys:\n",
    "            - 'correct_order_pct': float, percentage of notes with correct ordering\n",
    "            - 'missing_sections': Dict[str, int], count of missing sections by name\n",
    "            - 'section_presence': Dict[str, float], fraction of notes containing each section\n",
    "\n",
    "    Hints:\n",
    "        - Standard order is: HPI, ROS, EXAM, ASSESSMENT, PLAN\n",
    "        - Use str.find() to locate each section header in the note text\n",
    "        - A section is in correct order if its position is after all\n",
    "          preceding sections' positions\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: For each note, find the character position of each section header\n",
    "    # Step 2: Check if positions are in ascending order (standard SOAP order)\n",
    "    # Step 3: Count which sections are missing from each note\n",
    "    # Step 4: Compute summary statistics\n",
    "    # ==============================\n",
    "\n",
    "    analysis = ???  # YOUR CODE HERE\n",
    "\n",
    "    return analysis"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification cell\n",
    "analysis = analyze_section_ordering(filtered)\n",
    "print(\"Section ordering analysis:\")\n",
    "print(f\"  Notes with correct SOAP order: {analysis['correct_order_pct']:.1f}%\")\n",
    "print(f\"\\n  Section presence rates:\")\n",
    "for section, rate in analysis['section_presence'].items():\n",
    "    print(f\"    {section}: {rate:.1%}\")\n",
    "print(f\"\\n  Missing section counts:\")\n",
    "for section, count in analysis['missing_sections'].items():\n",
    "    print(f\"    {section}: {count}\")\n",
    "assert 'correct_order_pct' in analysis, \"Missing 'correct_order_pct' key\"\n",
    "assert 'section_presence' in analysis, \"Missing 'section_presence' key\"\n",
    "print(\"\\nEDA implementation looks correct!\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought Questions: EDA\n",
    "\n",
    "1. Which SOAP sections tend to be longest? Why might the HPI section typically be longer than the PLAN section?\n",
    "2. If 15% of notes are missing the ROS section, how might this affect the diffusion model's ability to generate complete notes?\n",
    "3. Looking at the top vocabulary words, what do you notice about the ratio of clinical terms to function words? What does this imply about token difficulty during unmasking?"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline: Autoregressive Clinical Note Generator\n",
    "\n",
    "Before building our diffusion model, we implement a simple autoregressive baseline. This establishes the quality floor and latency ceiling that the diffusion approach must beat."
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalTransformer(nn.Module):\n",
    "    \"\"\"Small causal (left-to-right) Transformer for autoregressive note generation.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=4,\n",
    "                 max_len=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.Embedding(max_len, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=d_model * 4,\n",
    "            dropout=dropout, batch_first=True, activation='gelu'\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_head = nn.Linear(d_model, vocab_size)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L = x.shape\n",
    "        positions = torch.arange(L, device=x.device).unsqueeze(0)\n",
    "        h = self.embedding(x) + self.pos_encoding(positions)\n",
    "        # Causal mask: each position can only attend to itself and earlier positions\n",
    "        causal_mask = torch.triu(\n",
    "            torch.ones(L, L, device=x.device) * float('-inf'), diagonal=1\n",
    "        )\n",
    "        h = self.transformer(h, mask=causal_mask)\n",
    "        return self.output_head(h)"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClinicalNoteDataset(Dataset):\n",
    "    \"\"\"Dataset for clinical notes.\"\"\"\n",
    "\n",
    "    def __init__(self, notes, tokenizer, max_len=128):\n",
    "        self.data = []\n",
    "        for note in notes:\n",
    "            ids = tokenizer.encode(note[\"text\"])[:max_len]\n",
    "            # Pad to max_len\n",
    "            ids = ids + [tokenizer.word2idx[\"[PAD]\"]] * (max_len - len(ids))\n",
    "            self.data.append(torch.tensor(ids, dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "MAX_LEN = 128\n",
    "PAD_ID = tokenizer.word2idx[\"[PAD]\"]\n",
    "MASK_ID = tokenizer.word2idx[\"[MASK]\"]\n",
    "\n",
    "dataset = ClinicalNoteDataset(filtered, tokenizer, max_len=MAX_LEN)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_ds, val_ds, test_ds = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=64)\n",
    "test_loader = DataLoader(test_ds, batch_size=64)\n",
    "\n",
    "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n",
    "print(f\"Sequence length: {MAX_LEN}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the autoregressive baseline\n",
    "ar_model = CausalTransformer(tokenizer.vocab_size, d_model=256, nhead=4,\n",
    "                              num_layers=4, max_len=MAX_LEN).to(device)\n",
    "optimizer = torch.optim.AdamW(ar_model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "num_params = sum(p.numel() for p in ar_model.parameters())\n",
    "print(f\"Autoregressive model parameters: {num_params:,}\")\n",
    "\n",
    "ar_losses = []\n",
    "ar_model.train()\n",
    "for epoch in range(5):\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        # Input: tokens 0..L-2, Target: tokens 1..L-1\n",
    "        logits = ar_model(batch[:, :-1])\n",
    "        target = batch[:, 1:]\n",
    "        # Mask out padding from loss\n",
    "        loss_mask = (target != PAD_ID).float()\n",
    "        loss = F.cross_entropy(logits.reshape(-1, tokenizer.vocab_size),\n",
    "                                target.reshape(-1), reduction='none')\n",
    "        loss = (loss * loss_mask.reshape(-1)).sum() / loss_mask.sum()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(ar_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    epoch_loss /= len(train_loader)\n",
    "    ar_losses.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch+1}: loss = {epoch_loss:.4f}\")\n",
    "\n",
    "print(\"\\nAutoregressive baseline training complete!\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline: generation latency\n",
    "ar_model.eval()\n",
    "\n",
    "def generate_ar(model, tokenizer, max_len=128, temperature=0.8):\n",
    "    \"\"\"Generate a clinical note autoregressively.\"\"\"\n",
    "    # Start with a random first token from common clinical words\n",
    "    start_tokens = [\"patient\", \"hpi:\", \"the\"]\n",
    "    start_id = tokenizer.word2idx.get(start_tokens[0], 3)\n",
    "    ids = [start_id]\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len - 1):\n",
    "            x = torch.tensor([ids], device=device)\n",
    "            logits = model(x)[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, 1).item()\n",
    "            if next_id == PAD_ID:\n",
    "                break\n",
    "            ids.append(next_id)\n",
    "    elapsed = time.time() - start_time\n",
    "    return tokenizer.decode(ids), elapsed\n",
    "\n",
    "# Generate 10 notes and measure latency\n",
    "latencies = []\n",
    "for i in range(10):\n",
    "    text, elapsed = generate_ar(ar_model, tokenizer)\n",
    "    latencies.append(elapsed)\n",
    "    if i == 0:\n",
    "        print(f\"Sample generated note:\\n{text[:300]}...\\n\")\n",
    "\n",
    "print(f\"Autoregressive generation latency:\")\n",
    "print(f\"  Mean: {np.mean(latencies)*1000:.1f} ms\")\n",
    "print(f\"  Std:  {np.std(latencies)*1000:.1f} ms\")\n",
    "print(f\"  This is the latency ceiling our diffusion model must beat.\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought Questions: Baseline\n",
    "\n",
    "1. Why does autoregressive generation latency scale linearly with sequence length?\n",
    "2. If MedScribe generates a 400-token SOAP note, how many forward passes does the autoregressive model require? How does this compare to a 20-step diffusion model?\n",
    "3. What is the fundamental architectural difference that prevents the autoregressive model from doing bidirectional edits?"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Diffusion Transformer Model Design\n",
    "\n",
    "Now we build the core model: a bidirectional Transformer with time conditioning for masked diffusion generation. The key difference from the autoregressive model is the absence of a causal mask -- every position attends to every other position.\n",
    "\n",
    "### 6.1 Time Conditioning\n",
    "\n",
    "The masking ratio $t$ is a critical input to the model. It tells the model how much of the note is currently masked, which calibrates its prediction confidence. We convert the scalar $t$ into a $d$-dimensional vector using a small MLP with sinusoidal features."
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeConditioningMLP(nn.Module):\n",
    "    \"\"\"Convert scalar masking ratio t into a d-dimensional conditioning vector.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_period=10000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "        )\n",
    "        # Precompute sinusoidal frequency bands\n",
    "        half = d_model // 2\n",
    "        freqs = torch.exp(\n",
    "            -np.log(max_period) * torch.arange(half, dtype=torch.float32) / half\n",
    "        )\n",
    "        self.register_buffer(\"freqs\", freqs)\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            t: (B,) tensor of masking ratios in [0, 1]\n",
    "        Returns:\n",
    "            (B, d_model) time conditioning vector\n",
    "        \"\"\"\n",
    "        # Sinusoidal embedding of t\n",
    "        t_emb = t[:, None] * self.freqs[None, :]  # (B, d_model//2)\n",
    "        t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)  # (B, d_model)\n",
    "        return self.mlp(t_emb)"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 The Full Diffusion Transformer"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClinicalDiffusionLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional Transformer for masked diffusion clinical note generation.\n",
    "\n",
    "    Architecture:\n",
    "    - Token embedding + sinusoidal positional encoding\n",
    "    - Time conditioning via addition\n",
    "    - Full (non-causal) Transformer encoder\n",
    "    - Linear output head over vocabulary\n",
    "\n",
    "    The model sees all positions bidirectionally, which enables:\n",
    "    1. Parallel prediction of all masked tokens\n",
    "    2. Bidirectional context propagation for edits\n",
    "    3. Native infilling of partially-masked notes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=6,\n",
    "                 max_len=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Token embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Sinusoidal positional encoding\n",
    "        self.pos_encoding = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        # Time conditioning\n",
    "        self.time_mlp = TimeConditioningMLP(d_model)\n",
    "\n",
    "        # Bidirectional Transformer (NO causal mask)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=d_model * 4,\n",
    "            dropout=dropout, batch_first=True, activation='gelu',\n",
    "            norm_first=True  # Pre-norm for training stability\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output head\n",
    "        self.output_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, x, t, pad_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, L) token IDs (with [MASK] at masked positions)\n",
    "            t: (B,) masking ratios in [0, 1]\n",
    "            pad_mask: (B, L) boolean mask, True for padding positions\n",
    "\n",
    "        Returns:\n",
    "            logits: (B, L, V) unnormalized log-probabilities over vocabulary\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        positions = torch.arange(L, device=x.device).unsqueeze(0)\n",
    "\n",
    "        # Embed tokens and add positional encoding\n",
    "        h = self.embedding(x) + self.pos_encoding(positions)\n",
    "\n",
    "        # Add time conditioning (broadcast across sequence positions)\n",
    "        t_emb = self.time_mlp(t)  # (B, d_model)\n",
    "        h = h + t_emb.unsqueeze(1)  # (B, L, d_model)\n",
    "\n",
    "        # Bidirectional Transformer (no causal mask)\n",
    "        if pad_mask is not None:\n",
    "            h = self.transformer(h, src_key_padding_mask=pad_mask)\n",
    "        else:\n",
    "            h = self.transformer(h)\n",
    "\n",
    "        # Project to vocabulary\n",
    "        logits = self.output_head(h)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "diff_model = ClinicalDiffusionLM(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=256,\n",
    "    nhead=4,\n",
    "    num_layers=6,\n",
    "    max_len=MAX_LEN,\n",
    ").to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in diff_model.parameters())\n",
    "print(f\"Diffusion model parameters: {num_params:,}\")\n",
    "print(f\"Autoregressive baseline had: {sum(p.numel() for p in ar_model.parameters()):,}\")"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Forward Masking Process\n",
    "\n",
    "The forward process takes a clean note and masks each token independently with probability $t$. Padding tokens are never masked -- they stay as padding regardless of $t$."
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_mask(x, t, pad_id=PAD_ID, mask_id=MASK_ID):\n",
    "    \"\"\"\n",
    "    Apply the forward masking process to a batch of notes.\n",
    "\n",
    "    Each non-padding token is independently replaced with [MASK]\n",
    "    with probability t.\n",
    "\n",
    "    Args:\n",
    "        x: (B, L) clean token IDs\n",
    "        t: (B,) masking ratios in [0, 1]\n",
    "        pad_id: token ID for padding\n",
    "        mask_id: token ID for [MASK]\n",
    "\n",
    "    Returns:\n",
    "        x_t: (B, L) masked token IDs\n",
    "        mask: (B, L) boolean tensor, True at positions that were masked\n",
    "    \"\"\"\n",
    "    B, L = x.shape\n",
    "    # Sample masking decisions: Bernoulli with probability t per batch element\n",
    "    rand = torch.rand(B, L, device=x.device)\n",
    "    t_expanded = t[:, None].expand(B, L)\n",
    "    should_mask = rand < t_expanded\n",
    "\n",
    "    # Never mask padding tokens\n",
    "    is_pad = (x == pad_id)\n",
    "    should_mask = should_mask & ~is_pad\n",
    "\n",
    "    # Apply masking\n",
    "    x_t = x.clone()\n",
    "    x_t[should_mask] = mask_id\n",
    "\n",
    "    return x_t, should_mask"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the masking process on a sample note\n",
    "sample = dataset[0].unsqueeze(0).to(device)\n",
    "sample_text = tokenizer.decode(sample[0].cpu().tolist())\n",
    "print(f\"Original note (first 20 tokens):\\n{' '.join(sample_text.split()[:20])}\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 3))\n",
    "for i, t_val in enumerate([0.2, 0.4, 0.6, 0.9]):\n",
    "    t = torch.tensor([t_val], device=device)\n",
    "    x_t, mask = forward_mask(sample, t)\n",
    "    masked_text = tokenizer.decode(x_t[0].cpu().tolist())\n",
    "    words = masked_text.split()[:20]\n",
    "\n",
    "    colors = ['red' if w == '[mask]' else 'black' for w in words]\n",
    "    axes[i].set_xlim(0, 1)\n",
    "    axes[i].set_ylim(0, len(words))\n",
    "    for j, (word, color) in enumerate(zip(words, colors)):\n",
    "        axes[i].text(0.05, len(words) - j - 0.5, word, fontsize=9,\n",
    "                     color=color, family='monospace')\n",
    "    axes[i].set_title(f\"t = {t_val}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Forward Masking Process on a Clinical Note\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training the Diffusion Model\n",
    "\n",
    "### 7.1 Diffusion Training Loss\n",
    "\n",
    "The training objective is the weighted cross-entropy at masked positions:\n",
    "\n",
    "$$\\mathcal{L} = -\\mathbb{E}_{t} \\left[ \\frac{1}{t \\cdot L} \\sum_{i: x_t^i = \\texttt{[MASK]}} \\log p_\\theta(x_0^i \\mid x_t) \\right]$$"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_loss(model, x, pad_id=PAD_ID, mask_id=MASK_ID):\n",
    "    \"\"\"\n",
    "    Compute the masked diffusion training loss.\n",
    "\n",
    "    Steps:\n",
    "    1. Sample t ~ U(0.02, 1.0) for each batch element\n",
    "    2. Apply forward masking\n",
    "    3. Get model predictions at masked positions\n",
    "    4. Compute weighted cross-entropy loss\n",
    "\n",
    "    Args:\n",
    "        model: ClinicalDiffusionLM\n",
    "        x: (B, L) clean token IDs\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar loss value\n",
    "    \"\"\"\n",
    "    B, L = x.shape\n",
    "\n",
    "    # Sample masking ratios\n",
    "    t = torch.rand(B, device=x.device) * 0.98 + 0.02  # U(0.02, 1.0)\n",
    "\n",
    "    # Forward mask\n",
    "    x_t, mask = forward_mask(x, t, pad_id, mask_id)\n",
    "\n",
    "    # Padding mask for the Transformer\n",
    "    pad_mask = (x == pad_id)\n",
    "\n",
    "    # Get predictions\n",
    "    logits = model(x_t, t, pad_mask=pad_mask)\n",
    "\n",
    "    # Cross-entropy at masked positions only\n",
    "    loss_per_token = F.cross_entropy(\n",
    "        logits.reshape(-1, model.vocab_size),\n",
    "        x.reshape(-1),\n",
    "        reduction='none'\n",
    "    ).reshape(B, L)\n",
    "\n",
    "    # Zero out loss at non-masked and padding positions\n",
    "    loss_per_token = loss_per_token * mask.float()\n",
    "\n",
    "    # Weight by 1/(t * L) -- the ELBO-derived importance weight\n",
    "    n_masked = mask.float().sum(dim=1).clamp(min=1)  # avoid div by zero\n",
    "    loss_per_sample = loss_per_token.sum(dim=1) / (t * L)\n",
    "\n",
    "    return loss_per_sample.mean()"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Training Loop"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(diff_model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15, eta_min=1e-5)\n",
    "\n",
    "diff_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Training diffusion model...\")\n",
    "for epoch in range(15):\n",
    "    # Train\n",
    "    diff_model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        loss = diffusion_loss(diff_model, batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(diff_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    epoch_loss /= len(train_loader)\n",
    "    diff_losses.append(epoch_loss)\n",
    "\n",
    "    # Validate\n",
    "    diff_model.eval()\n",
    "    v_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            v_loss += diffusion_loss(diff_model, batch).item()\n",
    "    v_loss /= len(val_loader)\n",
    "    val_losses.append(v_loss)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1:2d}: train_loss={epoch_loss:.4f}  val_loss={v_loss:.4f}  \"\n",
    "          f\"lr={scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "print(\"\\nDiffusion model training complete!\")"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(range(1, len(diff_losses)+1), diff_losses, 'b-o', label='Train Loss', markersize=4)\n",
    "ax.plot(range(1, len(val_losses)+1), val_losses, 'r-o', label='Val Loss', markersize=4)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Diffusion Loss\")\n",
    "ax.set_title(\"Diffusion Model Training Progress\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Your Turn: Implement Confidence-Based Generation\n",
    "\n",
    "This is the core generation algorithm. Starting from a fully masked sequence, the model iteratively unmasks tokens by selecting the most confident predictions at each step."
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_diffusion(model, tokenizer, n_steps=20, max_len=128,\n",
    "                       temperature=0.8, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generate a clinical note using confidence-based iterative unmasking.\n",
    "\n",
    "    Algorithm:\n",
    "    1. Start with a sequence of all [MASK] tokens\n",
    "    2. For each step s = 1, ..., n_steps:\n",
    "       a. Compute the current masking ratio t = 1 - s/n_steps\n",
    "       b. Forward pass: get logits at all positions\n",
    "       c. Divide logits by temperature\n",
    "       d. Apply softmax to get probabilities\n",
    "       e. Sample tokens from the distribution\n",
    "       f. Compute confidence = probability of the sampled token\n",
    "       g. Determine how many tokens to unmask at this step:\n",
    "          k = (number of currently masked tokens) // (n_steps - s + 1)\n",
    "       h. Among currently masked positions, keep the top-k most confident\n",
    "       i. Remask all other positions\n",
    "    3. Return the final sequence\n",
    "\n",
    "    Args:\n",
    "        model: ClinicalDiffusionLM\n",
    "        tokenizer: SimpleTokenizer\n",
    "        n_steps: Number of denoising steps\n",
    "        max_len: Length of generated sequence\n",
    "        temperature: Sampling temperature (lower = more conservative)\n",
    "        device: torch device\n",
    "\n",
    "    Returns:\n",
    "        generated_ids: List[int], the generated token IDs\n",
    "        steps_history: List[List[int]], token IDs at each step (for visualization)\n",
    "\n",
    "    Hints:\n",
    "        - Use torch.multinomial(probs, 1) for sampling\n",
    "        - Use torch.gather to get the probability of each sampled token\n",
    "        - Use torch.topk to find the most confident positions\n",
    "        - Masked positions are where x == MASK_ID\n",
    "    \"\"\"\n",
    "    MASK_ID = tokenizer.word2idx[\"[MASK]\"]\n",
    "\n",
    "    # Start fully masked\n",
    "    x = torch.full((1, max_len), MASK_ID, dtype=torch.long, device=device)\n",
    "    steps_history = [x[0].cpu().tolist()]\n",
    "\n",
    "    for s in range(1, n_steps + 1):\n",
    "        t = torch.tensor([1.0 - s / n_steps], device=device)\n",
    "\n",
    "        # ============ TODO ============\n",
    "        # Step 1: Forward pass to get logits\n",
    "        #         logits = model(x, t)\n",
    "        #\n",
    "        # Step 2: Apply temperature scaling\n",
    "        #         logits = logits / temperature\n",
    "        #\n",
    "        # Step 3: Convert to probabilities with softmax\n",
    "        #         probs = F.softmax(logits, dim=-1)  (over the vocab dimension)\n",
    "        #\n",
    "        # Step 4: Sample tokens at every position\n",
    "        #         sampled = torch.multinomial(probs[0], 1).squeeze(-1)\n",
    "        #\n",
    "        # Step 5: Compute confidence for each sampled token\n",
    "        #         confidence = torch.gather(probs[0], 1, sampled.unsqueeze(-1)).squeeze(-1)\n",
    "        #\n",
    "        # Step 6: Determine which positions are currently masked\n",
    "        #         is_masked = (x[0] == MASK_ID)\n",
    "        #\n",
    "        # Step 7: Calculate how many tokens to unmask this step\n",
    "        #         n_masked = is_masked.sum().item()\n",
    "        #         k = max(1, n_masked // (n_steps - s + 1))\n",
    "        #\n",
    "        # Step 8: Among masked positions, find the top-k most confident\n",
    "        #         Set confidence of non-masked positions to -1 so they are not selected\n",
    "        #         Use torch.topk to get the indices of the k most confident masked positions\n",
    "        #\n",
    "        # Step 9: Unmask the top-k positions by assigning their sampled tokens\n",
    "        #         Keep everything else unchanged\n",
    "        # ==============================\n",
    "\n",
    "        pass  # YOUR CODE HERE -- replace this with the steps above\n",
    "\n",
    "        steps_history.append(x[0].cpu().tolist())\n",
    "\n",
    "    return x[0].cpu().tolist(), steps_history"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification cell -- run after implementing generate_diffusion\n",
    "generated_ids, history = generate_diffusion(diff_model, tokenizer, n_steps=20,\n",
    "                                             max_len=MAX_LEN, device=device)\n",
    "generated_text = tokenizer.decode(generated_ids)\n",
    "print(\"Generated clinical note:\")\n",
    "print(generated_text[:500])\n",
    "print(f\"\\nTotal tokens: {len(generated_ids)}\")\n",
    "n_masks = sum(1 for t in generated_ids if t == MASK_ID)\n",
    "print(f\"Remaining [MASK] tokens: {n_masks}\")\n",
    "assert n_masks == 0, f\"Generation incomplete: {n_masks} masks remaining\"\n",
    "print(\"\\nGeneration function working correctly!\")"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop and Think\n",
    "\n",
    "Before moving on, consider these questions:\n",
    "\n",
    "1. Why do we divide the logits by temperature *before* softmax rather than adjusting the probabilities after softmax?\n",
    "2. What happens if temperature is very low (e.g., 0.1)? Very high (e.g., 2.0)?\n",
    "3. Why is confidence-based unmasking better than random unmasking? Think about what information the easy-to-predict tokens give the model when predicting harder tokens."
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation: Diffusion vs. Autoregressive\n",
    "\n",
    "Now we compare our diffusion model against the autoregressive baseline on the key metrics."
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate notes with both models and measure quality + latency\n",
    "\n",
    "def compute_rouge_l(generated, reference):\n",
    "    \"\"\"Compute ROUGE-L F1 score between two strings.\"\"\"\n",
    "    gen_words = generated.lower().split()\n",
    "    ref_words = reference.lower().split()\n",
    "    if len(gen_words) == 0 or len(ref_words) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # LCS computation\n",
    "    m, n = len(gen_words), len(ref_words)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if gen_words[i-1] == ref_words[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "    lcs = dp[m][n]\n",
    "\n",
    "    precision = lcs / m if m > 0 else 0\n",
    "    recall = lcs / n if n > 0 else 0\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def check_section_structure(text):\n",
    "    \"\"\"Check if a generated note has correct SOAP section ordering.\"\"\"\n",
    "    positions = []\n",
    "    for header in SECTION_HEADERS:\n",
    "        pos = text.lower().find(header.lower())\n",
    "        positions.append(pos if pos >= 0 else float('inf'))\n",
    "\n",
    "    # Count present sections\n",
    "    present = sum(1 for p in positions if p < float('inf'))\n",
    "    # Check ordering of present sections\n",
    "    filtered_pos = [p for p in positions if p < float('inf')]\n",
    "    correct_order = all(a < b for a, b in zip(filtered_pos, filtered_pos[1:]))\n",
    "\n",
    "    return present, correct_order\n",
    "\n",
    "\n",
    "# Evaluate both models\n",
    "n_eval = 50\n",
    "ref_notes = [filtered[i][\"text\"] for i in range(min(n_eval, len(filtered)))]\n",
    "\n",
    "# Diffusion evaluation\n",
    "diff_rouges = []\n",
    "diff_latencies = []\n",
    "diff_sections = []\n",
    "print(\"Evaluating diffusion model...\")\n",
    "for i in tqdm(range(n_eval)):\n",
    "    start = time.time()\n",
    "    gen_ids, _ = generate_diffusion(diff_model, tokenizer, n_steps=20,\n",
    "                                     max_len=MAX_LEN, device=device)\n",
    "    elapsed = time.time() - start\n",
    "    diff_latencies.append(elapsed)\n",
    "\n",
    "    gen_text = tokenizer.decode(gen_ids)\n",
    "    diff_rouges.append(compute_rouge_l(gen_text, ref_notes[i]))\n",
    "    n_sec, _ = check_section_structure(gen_text)\n",
    "    diff_sections.append(n_sec)\n",
    "\n",
    "# Autoregressive evaluation\n",
    "ar_rouges = []\n",
    "ar_latencies = []\n",
    "ar_sections = []\n",
    "print(\"Evaluating autoregressive model...\")\n",
    "for i in tqdm(range(n_eval)):\n",
    "    gen_text, elapsed = generate_ar(ar_model, tokenizer, max_len=MAX_LEN)\n",
    "    ar_latencies.append(elapsed)\n",
    "    ar_rouges.append(compute_rouge_l(gen_text, ref_notes[i]))\n",
    "    n_sec, _ = check_section_structure(gen_text)\n",
    "    ar_sections.append(n_sec)\n",
    "\n",
    "# Results table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS: Diffusion vs. Autoregressive\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<25} {'Diffusion':>15} {'Autoregressive':>15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'ROUGE-L (mean)':<25} {np.mean(diff_rouges):>15.3f} {np.mean(ar_rouges):>15.3f}\")\n",
    "print(f\"{'Latency mean (ms)':<25} {np.mean(diff_latencies)*1000:>15.1f} {np.mean(ar_latencies)*1000:>15.1f}\")\n",
    "print(f\"{'Latency P99 (ms)':<25} {np.percentile(diff_latencies, 99)*1000:>15.1f} {np.percentile(ar_latencies, 99)*1000:>15.1f}\")\n",
    "print(f\"{'Sections found (mean)':<25} {np.mean(diff_sections):>15.1f} {np.mean(ar_sections):>15.1f}\")\n",
    "print(f\"{'Speedup':<25} {np.mean(ar_latencies)/np.mean(diff_latencies):>15.1f}x {1.0:>15.1f}x\")\n",
    "print(\"=\"*60)"
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: latency comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Latency distribution\n",
    "axes[0].hist(np.array(ar_latencies)*1000, bins=20, alpha=0.6, label='Autoregressive',\n",
    "             color='#F44336', edgecolor='black')\n",
    "axes[0].hist(np.array(diff_latencies)*1000, bins=20, alpha=0.6, label='Diffusion',\n",
    "             color='#2196F3', edgecolor='black')\n",
    "axes[0].set_xlabel(\"Latency (ms)\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\"Generation Latency Distribution\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Quality vs Speed scatter\n",
    "axes[1].scatter(np.array(ar_latencies)*1000, ar_rouges, alpha=0.5,\n",
    "                label='Autoregressive', color='#F44336', s=40)\n",
    "axes[1].scatter(np.array(diff_latencies)*1000, diff_rouges, alpha=0.5,\n",
    "                label='Diffusion', color='#2196F3', s=40)\n",
    "axes[1].set_xlabel(\"Latency (ms)\")\n",
    "axes[1].set_ylabel(\"ROUGE-L\")\n",
    "axes[1].set_title(\"Quality vs. Speed Tradeoff\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualizing the Unmasking Process\n",
    "\n",
    "Watch a clinical note crystallize from a sea of [MASK] tokens into structured text. This is the visual payoff of diffusion-based generation."
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a note and visualize the unmasking trajectory\n",
    "gen_ids, history = generate_diffusion(diff_model, tokenizer, n_steps=20,\n",
    "                                       max_len=MAX_LEN, device=device)\n",
    "\n",
    "# Show steps 1, 5, 10, 15, 20\n",
    "steps_to_show = [0, 4, 9, 14, 19]  # indices into history\n",
    "fig, axes = plt.subplots(len(steps_to_show), 1, figsize=(16, len(steps_to_show)*2))\n",
    "\n",
    "for ax_idx, step_idx in enumerate(steps_to_show):\n",
    "    step_ids = history[step_idx]\n",
    "    words = tokenizer.decode(step_ids).split()[:40]  # Show first 40 tokens\n",
    "\n",
    "    ax = axes[ax_idx]\n",
    "    ax.set_xlim(0, len(words))\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word == '[mask]':\n",
    "            color = '#E0E0E0'\n",
    "            text_color = '#999999'\n",
    "        elif word in ['hpi:', 'ros:', 'exam:', 'assessment:', 'plan:']:\n",
    "            color = '#BBDEFB'\n",
    "            text_color = '#1565C0'\n",
    "        else:\n",
    "            color = '#C8E6C9'\n",
    "            text_color = '#2E7D32'\n",
    "\n",
    "        ax.add_patch(plt.Rectangle((i, 0.1), 0.9, 0.8, facecolor=color,\n",
    "                                    edgecolor='#666666', linewidth=0.5))\n",
    "        ax.text(i + 0.45, 0.5, word[:8], ha='center', va='center',\n",
    "                fontsize=7, color=text_color, family='monospace')\n",
    "\n",
    "    ax.set_ylabel(f\"Step {step_idx + 1}\", fontsize=11, fontweight='bold')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.suptitle(\"Clinical Note Unmasking Trajectory\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Error Analysis\n",
    "\n",
    "### TODO: Implement Error Categorization\n",
    "\n",
    "Analyze the failure modes of the diffusion model by categorizing errors in generated notes."
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_errors(generated_text, reference_text=None):\n",
    "    \"\"\"\n",
    "    Categorize errors in a generated clinical note.\n",
    "\n",
    "    Error types:\n",
    "    1. Section confusion: Content appears under the wrong SOAP section\n",
    "    2. Repetition: Repeated phrases within a section (3+ consecutive\n",
    "       repeated words)\n",
    "    3. Missing sections: Standard SOAP sections that are absent\n",
    "    4. Truncation: Incomplete sentences (ending without punctuation\n",
    "       or standard clinical phrasing)\n",
    "\n",
    "    Args:\n",
    "        generated_text: The generated clinical note text\n",
    "        reference_text: Optional reference note for comparison\n",
    "\n",
    "    Returns:\n",
    "        errors: Dict with keys:\n",
    "            - 'section_confusion': List of (expected_section, actual_section, content) tuples\n",
    "            - 'repetitions': List of repeated phrases found\n",
    "            - 'missing_sections': List of section names that are absent\n",
    "            - 'truncations': int, count of apparently truncated sentences\n",
    "            - 'total_errors': int, total error count\n",
    "\n",
    "    Hints:\n",
    "        - Use SECTION_HEADERS to check which sections are present\n",
    "        - To detect repetition, look for 3+ word sequences that appear\n",
    "          more than once in the same section\n",
    "        - A truncated sentence is one that ends with a common word rather\n",
    "          than a period, comma, or clinical term\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Check which SOAP sections are present/missing\n",
    "    # Step 2: For each section, check for repeated phrases\n",
    "    # Step 3: Check for truncated sentences (heuristic: last word\n",
    "    #         is not punctuation or a known clinical end-word)\n",
    "    # Step 4: If reference is provided, check for section confusion\n",
    "    #         (content from one section appearing in another)\n",
    "    # Step 5: Return the error dict\n",
    "    # ==============================\n",
    "\n",
    "    errors = ???  # YOUR CODE HERE\n",
    "\n",
    "    return errors"
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification cell\n",
    "print(\"Analyzing errors in 20 generated notes...\")\n",
    "all_errors = {\"section_confusion\": 0, \"repetitions\": 0,\n",
    "              \"missing_sections\": 0, \"truncations\": 0}\n",
    "for i in range(20):\n",
    "    gen_ids, _ = generate_diffusion(diff_model, tokenizer, n_steps=20,\n",
    "                                     max_len=MAX_LEN, device=device)\n",
    "    gen_text = tokenizer.decode(gen_ids)\n",
    "    errs = categorize_errors(gen_text)\n",
    "    for k in all_errors:\n",
    "        if isinstance(errs.get(k), list):\n",
    "            all_errors[k] += len(errs[k])\n",
    "        elif isinstance(errs.get(k), int):\n",
    "            all_errors[k] += errs[k]\n",
    "\n",
    "print(\"\\nError summary (across 20 notes):\")\n",
    "for error_type, count in all_errors.items():\n",
    "    print(f\"  {error_type}: {count}\")\n",
    "assert isinstance(all_errors, dict), \"categorize_errors should return a dict\"\n",
    "print(\"\\nError analysis implementation complete!\")"
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought Questions: Error Analysis\n",
    "\n",
    "1. Which error type is most common? Why might diffusion models be prone to this particular failure mode?\n",
    "2. How might the section structure loss ($\\mathcal{L}_{\\text{section}}$) from the technical formulation help reduce section confusion errors?\n",
    "3. If you could add one post-processing rule to fix the most common error, what would it be?"
   ],
   "id": "cell_51"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Scalability and Deployment Benchmarking\n",
    "\n",
    "### TODO: Inference Benchmarking\n",
    "\n",
    "Profile the diffusion model's inference performance across different step counts to map the quality-speed tradeoff."
   ],
   "id": "cell_52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model, tokenizer, step_counts, n_samples=10,\n",
    "                        max_len=128, device='cuda'):\n",
    "    \"\"\"\n",
    "    Benchmark diffusion generation across different step counts.\n",
    "\n",
    "    For each step count, measure:\n",
    "    - Mean generation latency\n",
    "    - Mean ROUGE-L quality (against random reference notes)\n",
    "    - Tokens per second throughput\n",
    "\n",
    "    Args:\n",
    "        model: ClinicalDiffusionLM\n",
    "        tokenizer: SimpleTokenizer\n",
    "        step_counts: List[int], denoising step counts to benchmark\n",
    "        n_samples: Number of samples per step count\n",
    "        max_len: Sequence length\n",
    "        device: torch device\n",
    "\n",
    "    Returns:\n",
    "        results: Dict with keys:\n",
    "            - 'step_counts': List[int]\n",
    "            - 'latencies_ms': List[float], mean latency per step count\n",
    "            - 'rouge_scores': List[float], mean ROUGE-L per step count\n",
    "            - 'tokens_per_sec': List[float], throughput per step count\n",
    "\n",
    "    Hints:\n",
    "        - Use time.time() for latency measurement\n",
    "        - Warm up with 2 dummy generations before timing\n",
    "        - Tokens per second = max_len / (latency in seconds)\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: For each step count, generate n_samples notes\n",
    "    # Step 2: Measure latency for each generation\n",
    "    # Step 3: Compute ROUGE-L against reference notes\n",
    "    # Step 4: Calculate tokens/second throughput\n",
    "    # Step 5: Return results dict\n",
    "    # ==============================\n",
    "\n",
    "    results = ???  # YOUR CODE HERE\n",
    "\n",
    "    return results"
   ],
   "id": "cell_53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification cell\n",
    "step_counts = [1, 3, 5, 10, 15, 20]\n",
    "results = benchmark_inference(diff_model, tokenizer, step_counts,\n",
    "                               n_samples=5, max_len=MAX_LEN, device=device)\n",
    "\n",
    "print(\"Inference benchmark results:\")\n",
    "print(f\"{'Steps':>6} {'Latency (ms)':>14} {'ROUGE-L':>10} {'Tok/sec':>10}\")\n",
    "print(\"-\" * 45)\n",
    "for i, steps in enumerate(results['step_counts']):\n",
    "    print(f\"{steps:>6} {results['latencies_ms'][i]:>14.1f} \"\n",
    "          f\"{results['rouge_scores'][i]:>10.3f} {results['tokens_per_sec'][i]:>10.0f}\")\n",
    "\n",
    "assert len(results['step_counts']) == len(step_counts), \"Missing step counts\"\n",
    "assert results['latencies_ms'][0] < results['latencies_ms'][-1], \\\n",
    "    \"Latency should increase with more steps\"\n",
    "print(\"\\nBenchmark implementation correct!\")"
   ],
   "id": "cell_54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the quality-speed tradeoff\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot(results['step_counts'], results['rouge_scores'], 'b-o',\n",
    "         linewidth=2, markersize=8, label='ROUGE-L Quality')\n",
    "ax2.plot(results['step_counts'], results['latencies_ms'], 'g-s',\n",
    "         linewidth=2, markersize=8, label='Latency (ms)')\n",
    "\n",
    "ax1.set_xlabel(\"Denoising Steps\", fontsize=12)\n",
    "ax1.set_ylabel(\"ROUGE-L Score\", color='blue', fontsize=12)\n",
    "ax2.set_ylabel(\"Latency (ms)\", color='green', fontsize=12)\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "# Mark the sweet spot\n",
    "sweet_spot_idx = len(results['step_counts']) // 2\n",
    "ax1.axvline(x=results['step_counts'][sweet_spot_idx], color='red',\n",
    "            linestyle='--', alpha=0.5, label='Sweet Spot')\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='center right')\n",
    "\n",
    "plt.title(\"Quality vs. Speed Tradeoff: Choosing the Right Step Count\",\n",
    "          fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_55"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Infilling Demonstration\n",
    "\n",
    "One of the key advantages of diffusion models over autoregressive models: native infilling. We can fix some sections of a note and let the model generate the rest."
   ],
   "id": "cell_56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infill_note(model, tokenizer, partial_note_text, sections_to_generate,\n",
    "                n_steps=20, max_len=128, temperature=0.8, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generate missing sections of a partially completed clinical note.\n",
    "\n",
    "    Args:\n",
    "        model: ClinicalDiffusionLM\n",
    "        tokenizer: SimpleTokenizer\n",
    "        partial_note_text: Text with some sections filled in\n",
    "        sections_to_generate: List of section names to generate (e.g., [\"PLAN\"])\n",
    "        n_steps: Denoising steps\n",
    "        max_len: Sequence length\n",
    "        temperature: Sampling temperature\n",
    "        device: torch device\n",
    "\n",
    "    Returns:\n",
    "        completed_text: The full note with generated sections\n",
    "    \"\"\"\n",
    "    MASK_ID = tokenizer.word2idx[\"[MASK]\"]\n",
    "    PAD_ID = tokenizer.word2idx[\"[PAD]\"]\n",
    "\n",
    "    # Encode the partial note\n",
    "    tokens = tokenizer.encode(partial_note_text)[:max_len]\n",
    "    tokens = tokens + [PAD_ID] * (max_len - len(tokens))\n",
    "    x = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "\n",
    "    # Identify which positions to mask (positions belonging to sections_to_generate)\n",
    "    text_lower = partial_note_text.lower()\n",
    "    for section in sections_to_generate:\n",
    "        header = f\"{section.lower()}:\"\n",
    "        start = text_lower.find(header)\n",
    "        if start >= 0:\n",
    "            # Find the end of this section (next header or end of text)\n",
    "            end = len(partial_note_text)\n",
    "            for other_header in SECTION_HEADERS:\n",
    "                pos = text_lower.find(other_header.lower(), start + len(header))\n",
    "                if pos >= 0 and pos < end:\n",
    "                    end = pos\n",
    "            # Mask the tokens in this section range\n",
    "            # Approximate: mask tokens corresponding to words in this range\n",
    "            section_text = partial_note_text[start:end]\n",
    "            section_tokens = tokenizer.encode(section_text)\n",
    "            # Find and mask these tokens in x\n",
    "            word_idx = 0\n",
    "            all_words = partial_note_text.lower().split()\n",
    "            section_words = section_text.lower().split()\n",
    "            for i, word in enumerate(all_words):\n",
    "                if i < max_len and word in [w.lower() for w in section_words]:\n",
    "                    x[0, i] = MASK_ID\n",
    "\n",
    "    # Run diffusion generation on masked positions only\n",
    "    for s in range(1, n_steps + 1):\n",
    "        t = torch.tensor([1.0 - s / n_steps], device=device)\n",
    "        logits = model(x, t) / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        sampled = torch.multinomial(probs[0], 1).squeeze(-1)\n",
    "        confidence = torch.gather(probs[0], 1, sampled.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        is_masked = (x[0] == MASK_ID)\n",
    "        n_masked = is_masked.sum().item()\n",
    "        if n_masked == 0:\n",
    "            break\n",
    "\n",
    "        k = max(1, n_masked // (n_steps - s + 1))\n",
    "        conf_masked = confidence.clone()\n",
    "        conf_masked[~is_masked] = -1.0\n",
    "        _, top_indices = torch.topk(conf_masked, min(k, n_masked))\n",
    "        x[0, top_indices] = sampled[top_indices]\n",
    "\n",
    "    return tokenizer.decode(x[0].cpu().tolist())\n",
    "\n",
    "\n",
    "# Demonstration: infill the PLAN section\n",
    "partial = filtered[0][\"text\"]\n",
    "# Keep everything except the PLAN section\n",
    "plan_start = partial.lower().find(\"plan:\")\n",
    "if plan_start >= 0:\n",
    "    partial_no_plan = partial[:plan_start] + \"PLAN: [to be generated]\"\n",
    "else:\n",
    "    partial_no_plan = partial\n",
    "\n",
    "print(\"Partial note (PLAN section removed):\")\n",
    "print(partial_no_plan[:300])\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "completed = infill_note(diff_model, tokenizer, partial_no_plan,\n",
    "                        sections_to_generate=[\"PLAN\"],\n",
    "                        n_steps=20, device=device)\n",
    "print(\"Completed note (PLAN generated by diffusion):\")\n",
    "print(completed[:500])"
   ],
   "id": "cell_57"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Ethical and Regulatory Analysis\n",
    "\n",
    "### TODO: Ethical Impact Assessment\n",
    "\n",
    "Write a brief ethical impact assessment for MedScribe's diffusion-based clinical documentation system."
   ],
   "id": "cell_58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ethical_impact_assessment():\n",
    "    \"\"\"\n",
    "    Generate a structured ethical impact assessment for deploying\n",
    "    a diffusion language model in clinical documentation.\n",
    "\n",
    "    Returns:\n",
    "        assessment: Dict with keys:\n",
    "            - 'bias_risks': List of identified bias risks with mitigation strategies\n",
    "            - 'privacy_concerns': List of privacy/HIPAA concerns with safeguards\n",
    "            - 'liability_framework': Description of who is responsible for errors\n",
    "            - 'transparency_measures': How physicians can understand/verify outputs\n",
    "            - 'overall_risk_level': 'low', 'medium', or 'high' with justification\n",
    "\n",
    "    Consider:\n",
    "        - The model trains on historical clinical notes, which may reflect\n",
    "          demographic disparities in care quality\n",
    "        - Diffusion models generate all tokens simultaneously, making it\n",
    "          harder to trace which input influenced which output\n",
    "        - Clinical notes are legal documents; errors have real patient consequences\n",
    "        - HIPAA requires that patient data be protected at all stages\n",
    "\n",
    "    This is an open-ended assessment. There are no wrong answers, but\n",
    "    your analysis should be specific to the clinical documentation use case\n",
    "    and reference concrete risks and mitigations.\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Write your ethical impact assessment here.\n",
    "    # Return a dict with the keys described above.\n",
    "    # Each value should contain substantive analysis, not placeholder text.\n",
    "    #\n",
    "    # Example structure for bias_risks:\n",
    "    # [\n",
    "    #     {\n",
    "    #         \"risk\": \"Demographic documentation quality disparity\",\n",
    "    #         \"description\": \"Notes for certain patient populations may be...\",\n",
    "    #         \"mitigation\": \"Stratified evaluation across demographics...\"\n",
    "    #     },\n",
    "    #     ...\n",
    "    # ]\n",
    "    # ==============================\n",
    "\n",
    "    assessment = ???  # YOUR CODE HERE\n",
    "\n",
    "    return assessment"
   ],
   "id": "cell_59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification cell\n",
    "assessment = ethical_impact_assessment()\n",
    "required_keys = ['bias_risks', 'privacy_concerns', 'liability_framework',\n",
    "                 'transparency_measures', 'overall_risk_level']\n",
    "for key in required_keys:\n",
    "    assert key in assessment, f\"Missing key: {key}\"\n",
    "    assert assessment[key] is not None and assessment[key] != \"???\", \\\n",
    "        f\"Key '{key}' must contain substantive content\"\n",
    "print(\"Ethical impact assessment structure is complete!\")\n",
    "print(f\"\\nOverall risk level: {assessment['overall_risk_level']}\")\n",
    "print(f\"Bias risks identified: {len(assessment['bias_risks'])}\")\n",
    "print(f\"Privacy concerns identified: {len(assessment['privacy_concerns'])}\")"
   ],
   "id": "cell_60"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought Questions: Ethics\n",
    "\n",
    "1. If the diffusion model generates a note with an incorrect medication dosage that leads to patient harm, who bears legal responsibility -- the physician who approved the note, the AI company, or both?\n",
    "2. How would you design an audit trail for AI-generated clinical notes that satisfies both HIPAA requirements and malpractice liability concerns?\n",
    "3. What specific fairness metrics would you track to ensure the model performs equally well across patient demographics (age, race, gender, primary language)?"
   ],
   "id": "cell_61"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary and Next Steps\n",
    "\n",
    "Congratulations! In this notebook, you have:\n",
    "\n",
    "1. **Built a synthetic clinical notes dataset** mimicking MIMIC-III structure\n",
    "2. **Implemented and trained an autoregressive baseline** to establish quality and latency benchmarks\n",
    "3. **Built a masked diffusion Transformer** with time conditioning and bidirectional attention\n",
    "4. **Trained the diffusion model** using the ELBO-derived weighted cross-entropy loss\n",
    "5. **Implemented confidence-based generation** -- the core iterative unmasking algorithm\n",
    "6. **Compared diffusion vs. autoregressive** on quality, latency, and structural metrics\n",
    "7. **Visualized the unmasking process** showing how clinical notes crystallize from masks\n",
    "8. **Benchmarked the quality-speed tradeoff** across different step counts\n",
    "9. **Demonstrated native infilling** -- completing partial notes without prompt engineering\n",
    "10. **Conducted an ethical impact assessment** for healthcare AI deployment\n",
    "\n",
    "For further reading, see **Section 4 of the case study document** which covers the full production system design: API endpoints, serving infrastructure, latency budgets, monitoring, A/B testing, CI/CD pipelines, and cost analysis for deploying this system at MedScribe's scale.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Diffusion LLMs trade sequential generation for parallel prediction, achieving order-of-magnitude latency improvements\n",
    "- Confidence-based unmasking naturally prioritizes easy predictions first, creating a \"structure before content\" generation pattern\n",
    "- Bidirectional attention enables capabilities impossible with autoregressive models: infilling, edit propagation, and section-level regeneration\n",
    "- The quality-speed tradeoff curve has diminishing returns: 10-20 steps capture most of the quality, making sub-second generation feasible\n",
    "- Healthcare AI requires careful ethical analysis: bias, privacy, liability, and transparency are not optional considerations"
   ],
   "id": "cell_62"
  }
 ]
}