{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Optimizing 5D Parallelism for Training a Financial Domain LLM â€” Implementation Notebook"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Optimizing 5D Parallelism for Training a Financial Domain LLM\n",
    "## Implementation Notebook\n",
    "\n",
    "In this notebook, you will work through the core technical challenge facing Meridian AI: configuring a 5D parallelism strategy (Data, Tensor, Pipeline, Sequence, and Expert Parallelism) to train a 72B-parameter Mixture-of-Experts language model on 1,024 H100 GPUs.\n",
    "\n",
    "You will work at a reduced scale â€” a 1.3B parameter, 8-expert MoE model on a simulated 8-GPU setup â€” but every parallelism dimension and tradeoff is preserved. By the end, you will have built a parallelism configuration optimizer that generalizes to any model size and cluster.\n",
    "\n",
    "**Prerequisites:** PyTorch 2.x, basic understanding of distributed computing, familiarity with Transformer architecture.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Data Acquisition Strategy\n",
    "\n",
    "We use two datasets for this case study:\n",
    "\n",
    "1. **SEC-EDGAR Filings** (sampled) â€” A subset of 10-K annual filings from publicly traded companies, representing the long-document workload that MeridianFin must handle.\n",
    "2. **FinanceBench** â€” A curated QA dataset grounded in SEC filings, used for post-training evaluation.\n",
    "\n",
    "For the parallelism experiments, we generate synthetic token sequences that match the statistical profile of financial text (sequence length distribution, vocabulary usage). This lets us focus on the systems engineering aspects without requiring the full 4T-token corpus."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://course-creator-brown.vercel.app/courses/5d-parallelism-from-scratch/practice/0/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from torch.distributed import ProcessGroup\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "import json\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA available: {torch.cuda.device_count()} GPU(s)\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"  GPU {i}: {props.name}, {props.total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"No CUDA GPUs available. This notebook requires GPU(s).\")\n",
    "    print(\"In Colab: Runtime -> Change runtime type -> T4 GPU\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration dataclass for the model and hardware\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for the MoE Transformer model.\"\"\"\n",
    "    vocab_size: int = 32000\n",
    "    hidden_dim: int = 2048          # d_model\n",
    "    num_layers: int = 24            # Total transformer layers\n",
    "    num_heads: int = 16             # Attention heads\n",
    "    head_dim: int = 128             # Per-head dimension\n",
    "    ffn_dim: int = 5504             # MLP intermediate dimension (non-expert layers)\n",
    "    num_experts: int = 8            # Number of MoE experts\n",
    "    top_k: int = 2                  # Top-K expert routing\n",
    "    expert_ffn_dim: int = 2048      # Per-expert MLP intermediate dimension\n",
    "    max_seq_len: int = 4096         # Maximum sequence length (reduced scale)\n",
    "    dropout: float = 0.0\n",
    "\n",
    "    @property\n",
    "    def total_params(self) -> int:\n",
    "        \"\"\"Estimate total parameter count.\"\"\"\n",
    "        # Embedding\n",
    "        embed = self.vocab_size * self.hidden_dim\n",
    "        # Per non-MoE layer: attention + MLP\n",
    "        attn_per_layer = 4 * self.hidden_dim * self.hidden_dim  # Q, K, V, O projections\n",
    "        mlp_per_layer = 2 * self.hidden_dim * self.ffn_dim      # up + down projections\n",
    "        # Per MoE layer: attention + N experts + router\n",
    "        expert_params = self.num_experts * 2 * self.hidden_dim * self.expert_ffn_dim\n",
    "        router_params = self.hidden_dim * self.num_experts\n",
    "        # Assume half the layers are MoE, half are dense (simplified)\n",
    "        n_moe_layers = self.num_layers // 2\n",
    "        n_dense_layers = self.num_layers - n_moe_layers\n",
    "        total = (embed\n",
    "                + n_dense_layers * (attn_per_layer + mlp_per_layer)\n",
    "                + n_moe_layers * (attn_per_layer + expert_params + router_params)\n",
    "                + embed)  # output projection (tied weights typically, but count for memory)\n",
    "        return total\n",
    "\n",
    "    @property\n",
    "    def active_params(self) -> int:\n",
    "        \"\"\"Estimate active parameters per token (with top-K routing).\"\"\"\n",
    "        embed = self.vocab_size * self.hidden_dim\n",
    "        attn_per_layer = 4 * self.hidden_dim * self.hidden_dim\n",
    "        mlp_per_layer = 2 * self.hidden_dim * self.ffn_dim\n",
    "        active_expert_params = self.top_k * 2 * self.hidden_dim * self.expert_ffn_dim\n",
    "        router_params = self.hidden_dim * self.num_experts\n",
    "        n_moe_layers = self.num_layers // 2\n",
    "        n_dense_layers = self.num_layers - n_moe_layers\n",
    "        total = (embed\n",
    "                + n_dense_layers * (attn_per_layer + mlp_per_layer)\n",
    "                + n_moe_layers * (attn_per_layer + active_expert_params + router_params)\n",
    "                + embed)\n",
    "        return total\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HardwareConfig:\n",
    "    \"\"\"Configuration for the GPU cluster.\"\"\"\n",
    "    num_gpus: int = 8               # Total GPUs (reduced scale)\n",
    "    gpus_per_node: int = 8          # GPUs per node\n",
    "    gpu_memory_gb: float = 80.0     # Memory per GPU in GB (simulated)\n",
    "    nvlink_bw_gbps: float = 900.0   # NVLink bandwidth (GB/s)\n",
    "    ib_bw_gbps: float = 50.0        # InfiniBand bandwidth per link (GB/s)\n",
    "    num_ib_links: int = 8           # IB links per node\n",
    "    peak_tflops: float = 989.0      # BF16 peak TFLOPS per GPU (H100 SXM)\n",
    "\n",
    "    @property\n",
    "    def num_nodes(self) -> int:\n",
    "        return self.num_gpus // self.gpus_per_node\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ParallelismConfig:\n",
    "    \"\"\"5D parallelism configuration.\"\"\"\n",
    "    dp: int = 1     # Data Parallelism degree\n",
    "    tp: int = 1     # Tensor Parallelism degree\n",
    "    pp: int = 1     # Pipeline Parallelism degree\n",
    "    sp: int = 1     # Sequence/Context Parallelism degree\n",
    "    ep: int = 1     # Expert Parallelism degree\n",
    "    zero_stage: int = 0         # ZeRO optimization stage (0, 1, 2, 3)\n",
    "    num_microbatches: int = 1   # Micro-batches for pipeline parallelism\n",
    "    activation_checkpointing: bool = False\n",
    "\n",
    "    def validate(self, hw: HardwareConfig, model: ModelConfig) -> List[str]:\n",
    "        \"\"\"Validate the configuration against hardware and model constraints.\"\"\"\n",
    "        errors = []\n",
    "        # GPU count constraint\n",
    "        if self.dp * self.tp * self.pp * self.ep != hw.num_gpus:\n",
    "            errors.append(\n",
    "                f\"DP({self.dp}) x TP({self.tp}) x PP({self.pp}) x EP({self.ep}) = \"\n",
    "                f\"{self.dp * self.tp * self.pp * self.ep} != {hw.num_gpus} GPUs\"\n",
    "            )\n",
    "        # TP within node\n",
    "        if self.tp > hw.gpus_per_node:\n",
    "            errors.append(f\"TP({self.tp}) > GPUs per node({hw.gpus_per_node})\")\n",
    "        # Layer divisibility\n",
    "        if model.num_layers % self.pp != 0:\n",
    "            errors.append(f\"Layers({model.num_layers}) not divisible by PP({self.pp})\")\n",
    "        # Expert divisibility\n",
    "        if model.num_experts % self.ep != 0:\n",
    "            errors.append(f\"Experts({model.num_experts}) not divisible by EP({self.ep})\")\n",
    "        # SP must equal TP (Megatron-style)\n",
    "        if self.sp != self.tp and self.sp != 1:\n",
    "            errors.append(f\"SP({self.sp}) must equal TP({self.tp}) or be 1\")\n",
    "        return errors\n",
    "\n",
    "\n",
    "# Initialize configurations\n",
    "model_cfg = ModelConfig()\n",
    "hw_cfg = HardwareConfig()\n",
    "\n",
    "print(f\"Model: {model_cfg.total_params / 1e9:.2f}B total parameters\")\n",
    "print(f\"Model: {model_cfg.active_params / 1e9:.2f}B active parameters per token\")\n",
    "print(f\"Hardware: {hw_cfg.num_gpus} GPUs across {hw_cfg.num_nodes} node(s)\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pipeline: Synthetic financial text sequences for parallelism experiments\n",
    "\n",
    "def create_synthetic_dataset(\n",
    "    num_sequences: int,\n",
    "    seq_length: int,\n",
    "    vocab_size: int,\n",
    "    seed: int = 42\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create synthetic token sequences for parallelism experiments.\n",
    "\n",
    "    In production, this would be replaced by the tokenized SEC-EDGAR corpus.\n",
    "    We use synthetic data here to focus on the systems engineering aspects.\n",
    "\n",
    "    Args:\n",
    "        num_sequences: Number of sequences to generate\n",
    "        seq_length: Tokens per sequence\n",
    "        vocab_size: Vocabulary size\n",
    "        seed: Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (num_sequences, seq_length) with token IDs\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    # Zipf-like distribution to mimic natural language token frequencies\n",
    "    probs = 1.0 / torch.arange(1, vocab_size + 1).float()\n",
    "    probs = probs / probs.sum()\n",
    "    tokens = torch.multinomial(probs.expand(num_sequences, -1), seq_length, replacement=True)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# TODO: Implement a data-parallel-aware data loader\n",
    "def create_dp_dataloader(\n",
    "    dataset: torch.Tensor,\n",
    "    dp_rank: int,\n",
    "    dp_world_size: int,\n",
    "    micro_batch_size: int,\n",
    "    shuffle: bool = True,\n",
    "    seed: int = 42\n",
    ") -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Shard the dataset across Data Parallel ranks and return micro-batches.\n",
    "\n",
    "    Each DP rank should see a DISJOINT subset of the data. This is critical:\n",
    "    if two DP ranks see the same data, we waste compute and introduce bias.\n",
    "\n",
    "    Args:\n",
    "        dataset: Full dataset tensor of shape (N, seq_len)\n",
    "        dp_rank: This rank's position in the DP group (0 to dp_world_size-1)\n",
    "        dp_world_size: Total number of DP ranks\n",
    "        micro_batch_size: Number of sequences per micro-batch\n",
    "        shuffle: Whether to shuffle before sharding\n",
    "        seed: Random seed for reproducible shuffling\n",
    "\n",
    "    Returns:\n",
    "        List of micro-batch tensors, each of shape (micro_batch_size, seq_len)\n",
    "\n",
    "    Hints:\n",
    "        1. If shuffle=True, generate a permutation of indices using the seed\n",
    "        2. Shard the (shuffled) indices: rank i gets indices[i::dp_world_size]\n",
    "        3. Group the shard into micro-batches of size micro_batch_size\n",
    "        4. Drop the last incomplete micro-batch if it exists\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Implement DP-aware data sharding\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: Test your data loader\n",
    "dataset = create_synthetic_dataset(1024, 128, model_cfg.vocab_size)\n",
    "print(f\"Dataset shape: {dataset.shape}\")\n",
    "\n",
    "# Simulate 4-way DP: each rank should get 256 sequences\n",
    "for rank in range(4):\n",
    "    batches = create_dp_dataloader(dataset, dp_rank=rank, dp_world_size=4, micro_batch_size=8)\n",
    "    total_seqs = sum(b.shape[0] for b in batches)\n",
    "    print(f\"  DP rank {rank}: {len(batches)} micro-batches, {total_seqs} sequences total\")\n",
    "\n",
    "# Verify no overlap between ranks\n",
    "all_indices_r0 = set(range(0, 1024, 4))  # Expected for rank 0 with stride sharding\n",
    "print(f\"\\nExpected ~256 sequences per rank, ~32 micro-batches per rank\")"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. Why is it important that DP ranks see disjoint subsets of the data? What would happen to gradient estimates if ranks processed overlapping data?\n",
    "2. In the production system with 4T tokens, the dataset is too large to fit in memory. How would you modify this data loader to stream from disk?\n",
    "3. How does the micro-batch size interact with Pipeline Parallelism? (Hint: more micro-batches reduce the pipeline bubble.)\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2 Exploratory Data Analysis: Profiling the Workload\n",
    "\n",
    "Before optimizing the parallelism configuration, we need to understand where the computational and memory bottlenecks are. This section builds a memory model from first principles."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_memory_bytes(\n",
    "    model: ModelConfig,\n",
    "    par: ParallelismConfig,\n",
    "    micro_batch_size: int,\n",
    "    seq_length: int,\n",
    "    bytes_per_param_training: int = 16  # BF16 weights + grads + FP32 optimizer\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Estimate per-GPU memory consumption for a given parallelism config.\n",
    "\n",
    "    This is the analytical memory model from Section 2 of the case study.\n",
    "    Understanding this model is essential for choosing the right configuration.\n",
    "\n",
    "    Args:\n",
    "        model: Model architecture configuration\n",
    "        par: Parallelism configuration\n",
    "        micro_batch_size: Micro-batch size per GPU\n",
    "        seq_length: Sequence length\n",
    "        bytes_per_param_training: Bytes per parameter for training state (16 for BF16+Adam)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with memory breakdown in bytes:\n",
    "        - 'model_states': Weights + gradients + optimizer states\n",
    "        - 'activations': Intermediate activations (per layer, summed)\n",
    "        - 'attention': Attention score matrices\n",
    "        - 'communication': Estimated communication buffer overhead\n",
    "        - 'total': Sum of all components\n",
    "    \"\"\"\n",
    "    # Model states memory\n",
    "    # With parallelism, parameters are distributed across TP, PP, and EP groups.\n",
    "    # With ZeRO, the DP dimension further shards the states.\n",
    "    zero_factor = par.dp if par.zero_stage == 3 else (\n",
    "        par.dp if par.zero_stage == 2 else (\n",
    "            par.dp if par.zero_stage == 1 else 1\n",
    "        )\n",
    "    )\n",
    "    # ZeRO-1 only shards optimizer states (12 bytes out of 16)\n",
    "    # ZeRO-2 shards optimizer states + gradients (14 out of 16)\n",
    "    # ZeRO-3 shards everything (16 out of 16)\n",
    "    if par.zero_stage == 0:\n",
    "        model_bytes_per_param = bytes_per_param_training\n",
    "    elif par.zero_stage == 1:\n",
    "        model_bytes_per_param = 4 + 12.0 / par.dp  # weights+grads local, optimizer sharded\n",
    "    elif par.zero_stage == 2:\n",
    "        model_bytes_per_param = 2 + 14.0 / par.dp  # weights local, grads+optimizer sharded\n",
    "    else:  # stage 3\n",
    "        model_bytes_per_param = bytes_per_param_training / par.dp\n",
    "\n",
    "    params_per_gpu = model.total_params / (par.tp * par.pp * par.ep)\n",
    "    model_states = params_per_gpu * model_bytes_per_param\n",
    "\n",
    "    # Activation memory per layer\n",
    "    # Each transformer layer stores intermediate activations for the backward pass.\n",
    "    # Key activations: input to each sublayer, attention scores, MLP intermediates\n",
    "    layers_per_gpu = model.num_layers // par.pp\n",
    "    sp_factor = par.sp if par.sp > 1 else 1\n",
    "\n",
    "    # Simplified activation estimate: ~34 bytes per element per layer\n",
    "    # (covers attention input/output, MLP input/intermediate/output, residuals, norms)\n",
    "    act_elements_per_layer = micro_batch_size * (seq_length // sp_factor) * model.hidden_dim\n",
    "    act_bytes_per_layer = act_elements_per_layer * 34  # empirical constant\n",
    "\n",
    "    if par.activation_checkpointing:\n",
    "        # With checkpointing, we only store layer inputs (recompute during backward)\n",
    "        # Memory is ~2x a single layer's activations regardless of depth\n",
    "        activations = act_bytes_per_layer * 2\n",
    "    else:\n",
    "        activations = act_bytes_per_layer * layers_per_gpu\n",
    "\n",
    "    # Attention memory (quadratic in sequence length)\n",
    "    heads_per_gpu = model.num_heads // par.tp\n",
    "    seq_per_gpu = seq_length // sp_factor\n",
    "    # Attention scores: (batch, heads, seq, seq) in BF16\n",
    "    attention = micro_batch_size * heads_per_gpu * seq_per_gpu * seq_length * 2  # 2 bytes for BF16\n",
    "\n",
    "    # Communication buffers (rough estimate)\n",
    "    communication = 2 * params_per_gpu * 2 / par.tp  # AllReduce buffers\n",
    "\n",
    "    total = model_states + activations + attention + communication\n",
    "\n",
    "    return {\n",
    "        'model_states': model_states,\n",
    "        'activations': activations,\n",
    "        'attention': attention,\n",
    "        'communication': communication,\n",
    "        'total': total,\n",
    "    }"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile memory for the baseline configuration (pure FSDP, no TP/PP/EP)\n",
    "baseline_par = ParallelismConfig(dp=8, tp=1, pp=1, sp=1, ep=1, zero_stage=3)\n",
    "baseline_mem = estimate_memory_bytes(model_cfg, baseline_par, micro_batch_size=1, seq_length=4096)\n",
    "\n",
    "print(\"=== Baseline Memory Profile (FSDP ZeRO-3, 8 GPUs) ===\")\n",
    "for key, val in baseline_mem.items():\n",
    "    print(f\"  {key:20s}: {val / 1e9:8.2f} GB\")\n",
    "\n",
    "print(f\"\\n  GPU memory budget: {hw_cfg.gpu_memory_gb:.0f} GB\")\n",
    "print(f\"  Fits in memory: {'YES' if baseline_mem['total'] / 1e9 <= hw_cfg.gpu_memory_gb else 'NO'}\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Profile memory for different sequence lengths and identify the breaking point\n",
    "\n",
    "def find_max_sequence_length(\n",
    "    model: ModelConfig,\n",
    "    par: ParallelismConfig,\n",
    "    hw: HardwareConfig,\n",
    "    micro_batch_size: int = 1,\n",
    "    safety_margin_gb: float = 4.0\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Find the maximum sequence length that fits in GPU memory for a given config.\n",
    "\n",
    "    Binary search over sequence lengths to find the largest one where\n",
    "    total memory <= gpu_memory - safety_margin.\n",
    "\n",
    "    Args:\n",
    "        model: Model configuration\n",
    "        par: Parallelism configuration\n",
    "        hw: Hardware configuration\n",
    "        micro_batch_size: Micro-batch size\n",
    "        safety_margin_gb: Reserved memory for framework overhead (GB)\n",
    "\n",
    "    Returns:\n",
    "        Maximum feasible sequence length (rounded down to nearest 256)\n",
    "\n",
    "    Hints:\n",
    "        1. Use binary search between seq_length=256 and seq_length=model.max_seq_len\n",
    "        2. For each candidate, call estimate_memory_bytes and check against budget\n",
    "        3. Round down to nearest multiple of 256 for practical alignment\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Implement binary search for max sequence length\")\n",
    "\n",
    "\n",
    "# Test: Find max seq length for the baseline config\n",
    "max_seq = find_max_sequence_length(model_cfg, baseline_par, hw_cfg)\n",
    "print(f\"Maximum sequence length for baseline config: {max_seq} tokens\")\n",
    "print(f\"Target sequence length: {model_cfg.max_seq_len} tokens\")\n",
    "if max_seq < model_cfg.max_seq_len:\n",
    "    print(\"PROBLEM: Baseline cannot handle the target sequence length!\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Memory breakdown across different sequence lengths\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seq_lengths = [512, 1024, 2048, 4096, 8192, 16384]\n",
    "components = ['model_states', 'activations', 'attention', 'communication']\n",
    "colors = ['#2196F3', '#4CAF50', '#FF9800', '#9C27B0']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bottoms = [0] * len(seq_lengths)\n",
    "\n",
    "for comp, color in zip(components, colors):\n",
    "    values = []\n",
    "    for sl in seq_lengths:\n",
    "        mem = estimate_memory_bytes(model_cfg, baseline_par, micro_batch_size=1, seq_length=sl)\n",
    "        values.append(mem[comp] / 1e9)\n",
    "    ax.bar(range(len(seq_lengths)), values, bottom=bottoms, label=comp, color=color)\n",
    "    bottoms = [b + v for b, v in zip(bottoms, values)]\n",
    "\n",
    "ax.axhline(y=hw_cfg.gpu_memory_gb, color='red', linestyle='--', linewidth=2, label='GPU Memory (80 GB)')\n",
    "ax.set_xticks(range(len(seq_lengths)))\n",
    "ax.set_xticklabels([str(sl) for sl in seq_lengths])\n",
    "ax.set_xlabel('Sequence Length (tokens)')\n",
    "ax.set_ylabel('Memory (GB)')\n",
    "ax.set_title('Per-GPU Memory Breakdown vs Sequence Length (Baseline FSDP)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. Which memory component grows fastest with sequence length? Why does this matter for long-context training?\n",
    "2. At what sequence length does attention memory dominate? How does this change with Tensor Parallelism (which splits attention heads)?\n",
    "3. The memory model uses a constant 34 bytes/element for activations. What are the actual intermediate tensors stored, and why is 34 a reasonable estimate?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3 Baseline Approach: Pure Data Parallelism (FSDP)\n",
    "\n",
    "We implement a minimal FSDP training loop to establish baseline throughput and identify bottlenecks. Since Colab may have a single GPU, we simulate multi-GPU behavior by running the memory model analytically and implementing the key computation kernels on a single GPU."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformerLayer(nn.Module):\n",
    "    \"\"\"A simplified Transformer layer for profiling (no MoE, single GPU).\"\"\"\n",
    "\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(config.hidden_dim)\n",
    "        self.attn_qkv = nn.Linear(config.hidden_dim, 3 * config.hidden_dim, bias=False)\n",
    "        self.attn_out = nn.Linear(config.hidden_dim, config.hidden_dim, bias=False)\n",
    "        self.norm2 = nn.LayerNorm(config.hidden_dim)\n",
    "        self.mlp_up = nn.Linear(config.hidden_dim, config.ffn_dim, bias=False)\n",
    "        self.mlp_down = nn.Linear(config.ffn_dim, config.hidden_dim, bias=False)\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = config.head_dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, S, D = x.shape\n",
    "        # Self-attention\n",
    "        h = self.norm1(x)\n",
    "        qkv = self.attn_qkv(h).reshape(B, S, 3, self.num_heads, self.head_dim)\n",
    "        q, k, v = qkv.unbind(dim=2)\n",
    "        q = q.transpose(1, 2)  # (B, H, S, D_h)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        attn = F.scaled_dot_product_attention(q, k, v)\n",
    "        attn = attn.transpose(1, 2).reshape(B, S, D)\n",
    "        x = x + self.attn_out(attn)\n",
    "        # MLP\n",
    "        h = self.norm2(x)\n",
    "        x = x + self.mlp_down(F.gelu(self.mlp_up(h)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    \"\"\"Simplified Transformer for baseline profiling.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ModelConfig, num_layers: int = None):\n",
    "        super().__init__()\n",
    "        n = num_layers or config.num_layers\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.hidden_dim)\n",
    "        self.layers = nn.ModuleList([SimpleTransformerLayer(config) for _ in range(n)])\n",
    "        self.norm = nn.LayerNorm(config.hidden_dim)\n",
    "        self.head = nn.Linear(config.hidden_dim, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embed(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        return self.head(x)"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile the baseline: measure actual single-GPU memory and compute time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Use a smaller layer count for single-GPU profiling\n",
    "profile_cfg = ModelConfig(num_layers=4, max_seq_len=2048, hidden_dim=1024, num_heads=8, head_dim=128, ffn_dim=2752)\n",
    "model_profile = SimpleTransformer(profile_cfg, num_layers=4).to(device).bfloat16()\n",
    "\n",
    "param_count = sum(p.numel() for p in model_profile.parameters())\n",
    "print(f\"Profile model: {param_count / 1e6:.1f}M parameters\")\n",
    "\n",
    "# Warm up\n",
    "dummy = torch.randint(0, profile_cfg.vocab_size, (1, 512), device=device)\n",
    "_ = model_profile(dummy)\n",
    "\n",
    "# Measure forward + backward time and memory\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "batch = torch.randint(0, profile_cfg.vocab_size, (2, 1024), device=device)\n",
    "start = time.time()\n",
    "logits = model_profile(batch)\n",
    "loss = F.cross_entropy(logits.view(-1, profile_cfg.vocab_size), batch.view(-1))\n",
    "loss.backward()\n",
    "torch.cuda.synchronize()\n",
    "elapsed = time.time() - start\n",
    "\n",
    "peak_mem = torch.cuda.max_memory_allocated() / 1e9\n",
    "print(f\"Forward + backward time: {elapsed * 1000:.1f} ms\")\n",
    "print(f\"Peak GPU memory: {peak_mem:.2f} GB\")\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Estimate MFU\n",
    "flops_per_token = 6 * param_count  # 6 = 2 (fwd) * 3 (fwd+bwd)\n",
    "total_tokens = 2 * 1024  # batch_size * seq_len\n",
    "total_flops = flops_per_token * total_tokens\n",
    "peak_flops = hw_cfg.peak_tflops * 1e12  # Convert TFLOPS to FLOPS\n",
    "mfu = total_flops / (elapsed * peak_flops)\n",
    "print(f\"Estimated MFU: {mfu * 100:.1f}%\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Estimate baseline FSDP throughput at the full scale (1,024 GPUs)\n",
    "\n",
    "def estimate_fsdp_throughput(\n",
    "    model: ModelConfig,\n",
    "    hw: HardwareConfig,\n",
    "    global_batch_size: int,\n",
    "    seq_length: int,\n",
    "    zero_stage: int = 3\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Estimate the throughput of pure FSDP (Data Parallelism only) at full scale.\n",
    "\n",
    "    This function should compute:\n",
    "    1. Compute time per step (based on FLOPs and peak GPU throughput)\n",
    "    2. Communication time (AllGather + ReduceScatter for ZeRO-3)\n",
    "    3. Total step time and tokens per second\n",
    "    4. MFU\n",
    "\n",
    "    Args:\n",
    "        model: Model configuration\n",
    "        hw: Hardware configuration\n",
    "        global_batch_size: Total batch size across all GPUs\n",
    "        seq_length: Sequence length\n",
    "        zero_stage: ZeRO stage (0-3)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - 'compute_time_s': Time spent on computation\n",
    "        - 'comm_time_s': Time spent on communication\n",
    "        - 'step_time_s': Total step time\n",
    "        - 'tokens_per_sec': Training throughput\n",
    "        - 'mfu': Model FLOPs Utilization\n",
    "\n",
    "    Hints:\n",
    "        1. Compute FLOPs per step: 6 * active_params * global_batch_size * seq_length\n",
    "        2. Compute time = FLOPs / (num_gpus * peak_tflops * 1e12)\n",
    "           (This gives the ideal compute time; actual MFU will be lower)\n",
    "        3. For ZeRO-3 communication: 2 AllGather + 2 ReduceScatter per step\n",
    "           Each moves total_params * 2 bytes (BF16) through the network.\n",
    "           The effective bandwidth for AllReduce across N GPUs is:\n",
    "           bw_effective = 2 * (N-1)/N * per_link_bw * num_links_per_node / 2\n",
    "           (The factor of 2*(N-1)/N accounts for the ring AllReduce algorithm)\n",
    "        4. MFU = useful_compute_time / (step_time * num_gpus * peak_tflops * 1e12)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Implement FSDP throughput estimation\")\n",
    "\n",
    "\n",
    "# Test at full scale\n",
    "fsdp_perf = estimate_fsdp_throughput(model_cfg, hw_cfg, global_batch_size=256, seq_length=4096)\n",
    "print(\"=== Baseline FSDP Performance (Full Scale) ===\")\n",
    "for key, val in fsdp_perf.items():\n",
    "    if 'time' in key:\n",
    "        print(f\"  {key}: {val * 1000:.1f} ms\")\n",
    "    elif key == 'mfu':\n",
    "        print(f\"  {key}: {val * 100:.1f}%\")\n",
    "    else:\n",
    "        print(f\"  {key}: {val:,.0f}\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. Why does FSDP's communication overhead become a bottleneck at 1,024 GPUs? (Hint: think about the ratio of compute to communication as the number of GPUs grows.)\n",
    "2. What happens to MFU if we double the batch size? Why?\n",
    "3. Why can FSDP not reduce activation memory, even though it shards weights and optimizer states?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4 Model Design: Configuring 5D Parallelism\n",
    "\n",
    "### 3.4.1 Tensor Parallelism\n",
    "\n",
    "Tensor Parallelism splits individual weight matrices across GPUs within a node. We implement the two fundamental building blocks: column-parallel and row-parallel linear layers."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement Column-Parallel Linear Layer\n",
    "\n",
    "class ColumnParallelLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer with weight matrix split column-wise across TP ranks.\n",
    "\n",
    "    Given Y = XW, we split W = [W_0 | W_1 | ... | W_{tp-1}] along columns.\n",
    "    Each TP rank i computes Y_i = X @ W_i (a partial output).\n",
    "    The full output is obtained by concatenating: Y = [Y_0 | Y_1 | ... | Y_{tp-1}].\n",
    "\n",
    "    For the Megatron-LM MLP, this is used for the FIRST linear layer (gate/up projection).\n",
    "    The key insight: we do NOT need to concatenate explicitly if the next operation\n",
    "    (GeLU) is element-wise â€” each rank can apply GeLU to its local output independently.\n",
    "\n",
    "    Args:\n",
    "        in_features: Input dimension (not split)\n",
    "        out_features: Output dimension (split across TP ranks)\n",
    "        tp_rank: This rank's position in the TP group (0 to tp_world_size-1)\n",
    "        tp_world_size: Number of TP ranks\n",
    "        bias: Whether to include a bias term\n",
    "\n",
    "    Hints:\n",
    "        1. The local output dimension is out_features // tp_world_size\n",
    "        2. Initialize weight of shape (in_features, local_out_features)\n",
    "        3. In forward(), simply compute x @ weight (+ bias if applicable)\n",
    "        4. The output has shape (batch, seq, local_out_features)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        tp_rank: int = 0,\n",
    "        tp_world_size: int = 1,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tp_rank = tp_rank\n",
    "        self.tp_world_size = tp_world_size\n",
    "        # YOUR CODE HERE: Define the weight parameter with the correct shape\n",
    "        # Remember: out_features is split across TP ranks\n",
    "        raise NotImplementedError(\"Define weight parameter\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: x @ weight.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq, in_features)\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (batch, seq, local_out_features)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Implement forward pass\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement Row-Parallel Linear Layer\n",
    "\n",
    "class RowParallelLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer with weight matrix split row-wise across TP ranks.\n",
    "\n",
    "    Given Y = XW, we split W along rows: W = [W_0; W_1; ...; W_{tp-1}].\n",
    "    The input X must also be split: X = [X_0 | X_1 | ... | X_{tp-1}].\n",
    "    Each TP rank i computes Y_i = X_i @ W_i (a partial sum).\n",
    "    The full output is: Y = sum(Y_0, Y_1, ..., Y_{tp-1}) via AllReduce.\n",
    "\n",
    "    For the Megatron-LM MLP, this is used for the SECOND linear layer (down projection).\n",
    "    The input to this layer is already split across TP ranks (output of ColumnParallel + GeLU),\n",
    "    so no explicit input splitting is needed.\n",
    "\n",
    "    Args:\n",
    "        in_features: Input dimension (split across TP ranks)\n",
    "        out_features: Output dimension (not split)\n",
    "        tp_rank: This rank's position in the TP group\n",
    "        tp_world_size: Number of TP ranks\n",
    "        bias: Whether to include a bias term\n",
    "\n",
    "    Hints:\n",
    "        1. The local input dimension is in_features // tp_world_size\n",
    "        2. Weight shape is (local_in_features, out_features)\n",
    "        3. In forward(), compute x @ weight â€” this gives a partial result\n",
    "        4. In a real distributed setup, you would AllReduce the partial results.\n",
    "           For this single-GPU implementation, just return the partial result.\n",
    "           We will simulate the AllReduce in the combined MLP module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        tp_rank: int = 0,\n",
    "        tp_world_size: int = 1,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tp_rank = tp_rank\n",
    "        self.tp_world_size = tp_world_size\n",
    "        # YOUR CODE HERE: Define the weight parameter with the correct shape\n",
    "        raise NotImplementedError(\"Define weight parameter\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: x @ weight (partial result, needs AllReduce for full output).\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq, local_in_features)\n",
    "\n",
    "        Returns:\n",
    "            Partial output tensor of shape (batch, seq, out_features)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Implement forward pass\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the Megatron-LM Tensor-Parallel MLP\n",
    "\n",
    "class TensorParallelMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Megatron-LM style MLP with Tensor Parallelism.\n",
    "\n",
    "    Architecture:\n",
    "        Input -> ColumnParallelLinear (gate_proj) -> GeLU -> RowParallelLinear (down_proj) -> Output\n",
    "                                                                    ^\n",
    "                                                              AllReduce here\n",
    "\n",
    "    The beauty of this design:\n",
    "    - ColumnParallel splits the output, so each rank has a different slice\n",
    "    - GeLU is element-wise, so it can be applied locally (NO communication)\n",
    "    - RowParallel takes the split input and produces partial sums\n",
    "    - Only ONE AllReduce is needed (after RowParallel), not two\n",
    "\n",
    "    This is the key insight from the Megatron-LM paper (Shoeybi et al., 2019).\n",
    "\n",
    "    Args:\n",
    "        hidden_dim: Model hidden dimension\n",
    "        ffn_dim: MLP intermediate dimension\n",
    "        tp_rank: TP rank\n",
    "        tp_world_size: TP world size\n",
    "\n",
    "    Hints:\n",
    "        1. gate_proj: ColumnParallelLinear(hidden_dim, ffn_dim, ...)\n",
    "        2. down_proj: RowParallelLinear(ffn_dim, hidden_dim, ...)\n",
    "        3. Forward: x -> gate_proj -> gelu -> down_proj -> (allreduce) -> output\n",
    "        4. Simulate AllReduce by summing partial results across TP ranks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim: int, ffn_dim: int, tp_rank: int = 0, tp_world_size: int = 1):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE: Initialize gate_proj and down_proj\n",
    "        raise NotImplementedError(\"Initialize MLP layers\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with Megatron-style TP.\n",
    "\n",
    "        Args:\n",
    "            x: Input of shape (batch, seq, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            Output of shape (batch, seq, hidden_dim)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Implement Megatron MLP forward\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: Test Tensor Parallel MLP\n",
    "# Create TP=2 MLPs and verify their outputs match a single-GPU MLP\n",
    "\n",
    "torch.manual_seed(42)\n",
    "H, FFN = 256, 512\n",
    "x_test = torch.randn(2, 16, H, device=device, dtype=torch.bfloat16)\n",
    "\n",
    "# Single-GPU reference\n",
    "mlp_full = nn.Sequential(\n",
    "    nn.Linear(H, FFN, bias=False),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(FFN, H, bias=False)\n",
    ").to(device).bfloat16()\n",
    "\n",
    "y_ref = mlp_full(x_test)\n",
    "\n",
    "# TP=2 version (simulated on single GPU)\n",
    "mlp_tp0 = TensorParallelMLP(H, FFN, tp_rank=0, tp_world_size=2).to(device).bfloat16()\n",
    "mlp_tp1 = TensorParallelMLP(H, FFN, tp_rank=1, tp_world_size=2).to(device).bfloat16()\n",
    "\n",
    "# Copy weights from reference to TP shards\n",
    "with torch.no_grad():\n",
    "    # Column split for gate_proj\n",
    "    half_ffn = FFN // 2\n",
    "    mlp_tp0.gate_proj.weight.copy_(mlp_full[0].weight[:half_ffn, :].t())  # Adjust based on your impl\n",
    "    mlp_tp1.gate_proj.weight.copy_(mlp_full[0].weight[half_ffn:, :].t())\n",
    "    # Row split for down_proj\n",
    "    mlp_tp0.down_proj.weight.copy_(mlp_full[2].weight[:, :half_ffn].t())\n",
    "    mlp_tp1.down_proj.weight.copy_(mlp_full[2].weight[:, half_ffn:].t())\n",
    "\n",
    "# Compute TP outputs and simulate AllReduce (sum)\n",
    "y_tp0 = mlp_tp0(x_test)\n",
    "y_tp1 = mlp_tp1(x_test)\n",
    "y_tp = y_tp0 + y_tp1  # Simulated AllReduce\n",
    "\n",
    "print(f\"Reference output shape: {y_ref.shape}\")\n",
    "print(f\"TP output shape: {y_tp.shape}\")\n",
    "print(f\"Max difference: {(y_ref - y_tp).abs().max().item():.6f}\")\n",
    "print(f\"Match (tolerance 1e-2): {torch.allclose(y_ref, y_tp, atol=1e-2)}\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. Why must Tensor Parallelism stay within a single node (NVLink boundary)? What would happen to throughput if TP spanned across nodes using InfiniBand?\n",
    "2. The Megatron design achieves ONE AllReduce per MLP block. Could you design a scheme with ZERO AllReduces? Why or why not?\n",
    "3. How does Tensor Parallelism affect the memory consumed by activations? (Hint: consider what each GPU stores after the column-parallel split.)"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Pipeline Parallelism\n",
    "\n",
    "Pipeline Parallelism splits the model's layers across GPUs in sequence, like an assembly line. The key challenge is minimizing the \"pipeline bubble\" â€” the fraction of time GPUs sit idle."
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a Pipeline Stage\n",
    "\n",
    "class PipelineStage:\n",
    "    \"\"\"\n",
    "    Represents one stage of the pipeline, holding a subset of the model's layers.\n",
    "\n",
    "    In production Pipeline Parallelism (e.g., Megatron-LM), each stage:\n",
    "    1. Receives activations from the previous stage (or input embeddings for stage 0)\n",
    "    2. Runs forward through its local layers\n",
    "    3. Sends activations to the next stage (or computes loss for the last stage)\n",
    "\n",
    "    For the backward pass, gradients flow in reverse.\n",
    "\n",
    "    Args:\n",
    "        layers: List of nn.Module layers assigned to this stage\n",
    "        stage_id: This stage's index (0 to num_stages-1)\n",
    "        num_stages: Total number of pipeline stages\n",
    "\n",
    "    Hints:\n",
    "        1. Store the layers as an nn.ModuleList\n",
    "        2. forward() runs input through each layer sequentially\n",
    "        3. Track whether this is the first stage (receives input tokens)\n",
    "           or last stage (computes loss)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers: List[nn.Module], stage_id: int, num_stages: int):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Initialize pipeline stage\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Run forward pass through this stage's layers.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Implement stage forward\")\n",
    "\n",
    "    def backward(self, grad: torch.Tensor):\n",
    "        \"\"\"Run backward pass (gradient flows in reverse through layers).\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Implement stage backward\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the 1F1B Pipeline Schedule\n",
    "\n",
    "def one_f_one_b_schedule(\n",
    "    num_stages: int,\n",
    "    num_microbatches: int\n",
    ") -> List[List[Tuple[str, int]]]:\n",
    "    \"\"\"\n",
    "    Generate the 1F1B (one-forward-one-backward) pipeline schedule.\n",
    "\n",
    "    The schedule tells each stage WHEN to run a forward (F) or backward (B)\n",
    "    pass for each micro-batch. The 1F1B schedule interleaves F and B to\n",
    "    reduce peak memory (activations are released earlier).\n",
    "\n",
    "    The schedule has three phases:\n",
    "    1. Warmup: Each stage runs forward passes to fill the pipeline\n",
    "    2. Steady state: Alternating 1 forward + 1 backward per step\n",
    "    3. Cooldown: Remaining backward passes drain the pipeline\n",
    "\n",
    "    Args:\n",
    "        num_stages: Number of pipeline stages (P)\n",
    "        num_microbatches: Number of micro-batches (M)\n",
    "\n",
    "    Returns:\n",
    "        Schedule as a list of lists. schedule[stage_id] is a list of\n",
    "        (action, microbatch_id) tuples, where action is 'F' or 'B'.\n",
    "\n",
    "    Example for P=4, M=8:\n",
    "        Stage 0: [F0, F1, F2, F3, B0, F4, B1, F5, B2, F6, B3, F7, B4, B5, B6, B7]\n",
    "        Stage 1: [__, F0, F1, F2, F3, B0, F4, B1, F5, B2, F6, B3, F7, B4, B5, B6]\n",
    "        ...\n",
    "\n",
    "    Hints:\n",
    "        1. Warmup phase for stage s: run (num_stages - 1 - s) forward passes\n",
    "        2. Steady state: alternate F and B for the remaining micro-batches\n",
    "        3. Cooldown: run remaining backward passes\n",
    "        4. Count the idle slots ('__') â€” this is the bubble\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Implement 1F1B schedule\")\n",
    "\n",
    "\n",
    "# Compute the bubble fraction\n",
    "def bubble_fraction(num_stages: int, num_microbatches: int) -> float:\n",
    "    \"\"\"\n",
    "    Compute the pipeline bubble fraction.\n",
    "\n",
    "    Formula: (P - 1) / (P - 1 + M)\n",
    "\n",
    "    Args:\n",
    "        num_stages: P\n",
    "        num_microbatches: M\n",
    "\n",
    "    Returns:\n",
    "        Bubble fraction (0 to 1)\n",
    "    \"\"\"\n",
    "    return (num_stages - 1) / (num_stages - 1 + num_microbatches)"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: Visualize the pipeline schedule and bubble\n",
    "\n",
    "def visualize_schedule(schedule: List[List[Tuple[str, int]]], num_stages: int):\n",
    "    \"\"\"Print a text-based visualization of the pipeline schedule.\"\"\"\n",
    "    if schedule is None:\n",
    "        print(\"Schedule not implemented yet\")\n",
    "        return\n",
    "\n",
    "    max_steps = max(len(stage_schedule) for stage_schedule in schedule)\n",
    "    print(f\"\\n{'Stage':<8}\", end='')\n",
    "    for t in range(max_steps):\n",
    "        print(f\"{'t=' + str(t):<6}\", end='')\n",
    "    print()\n",
    "    print('-' * (8 + max_steps * 6))\n",
    "\n",
    "    for s in range(num_stages):\n",
    "        print(f\"GPU {s:<4}\", end='')\n",
    "        for t in range(max_steps):\n",
    "            if t < len(schedule[s]):\n",
    "                action, mb = schedule[s][t]\n",
    "                print(f\"{action}{mb:<5}\", end='')\n",
    "            else:\n",
    "                print(f\"{'--':<6}\", end='')\n",
    "        print()\n",
    "\n",
    "P, M = 4, 8\n",
    "schedule = one_f_one_b_schedule(P, M)\n",
    "visualize_schedule(schedule, P)\n",
    "print(f\"\\nBubble fraction: {bubble_fraction(P, M) * 100:.1f}%\")\n",
    "\n",
    "# Plot bubble fraction vs number of micro-batches\n",
    "microbatches = list(range(1, 33))\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "for p in [2, 4, 8, 16]:\n",
    "    bubbles = [bubble_fraction(p, m) * 100 for m in microbatches]\n",
    "    ax.plot(microbatches, bubbles, label=f'P={p} stages')\n",
    "ax.set_xlabel('Number of micro-batches (M)')\n",
    "ax.set_ylabel('Bubble fraction (%)')\n",
    "ax.set_title('Pipeline Bubble vs Micro-batches')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 100)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. The bubble fraction formula is $(P-1)/(P-1+M)$. Derive this from first principles by counting the idle slots in the pipeline.\n",
    "2. Why does 1F1B use less peak memory than GPipe (all-forward-then-all-backward)?\n",
    "3. What is the tradeoff between increasing the number of micro-batches $M$ and the effective batch size? (Hint: $M$ micro-batches means each has $B_{\\text{global}} / (M \\times N_{\\text{DP}})$ sequences.)"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Sequence Parallelism (Context Parallelism)\n",
    "\n",
    "For long sequences (128K+ tokens), the attention score matrix becomes too large for a single GPU. Ring Attention distributes the attention computation across GPUs arranged in a ring."
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement simplified Ring Attention\n",
    "\n",
    "class RingAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Ring Attention for Context Parallelism.\n",
    "\n",
    "    The idea: split Q, K, V along the sequence dimension across GPUs in a ring.\n",
    "    Each GPU holds a local chunk of Q and iterates through all K, V chunks by\n",
    "    passing them around the ring.\n",
    "\n",
    "    For this single-GPU simulation, we split the sequence into chunks and\n",
    "    simulate the ring communication by iterating through the chunks.\n",
    "\n",
    "    Steps for each GPU (rank r, ring of size R):\n",
    "    1. Start with local Q_r, K_r, V_r (each has seq_len/R tokens)\n",
    "    2. For step s = 0, 1, ..., R-1:\n",
    "       a. Compute attention: A_s = softmax(Q_r @ K_{(r+s)%R}^T / sqrt(d)) @ V_{(r+s)%R}\n",
    "       b. Accumulate the output using the log-sum-exp trick for numerical stability\n",
    "       c. Pass K, V to the next GPU in the ring (receive from previous)\n",
    "    3. Final output O_r is the correctly weighted combination of all partial attentions\n",
    "\n",
    "    Args:\n",
    "        hidden_dim: Model hidden dimension\n",
    "        num_heads: Number of attention heads\n",
    "        head_dim: Dimension per head\n",
    "        cp_world_size: Context Parallelism (ring) size\n",
    "\n",
    "    Hints:\n",
    "        1. Split Q, K, V into cp_world_size chunks along the sequence dimension\n",
    "        2. For each ring step, compute attention between local Q and current K, V\n",
    "        3. Use the online softmax trick: track running max and running sum of exp\n",
    "           to numerically combine attention outputs from different K, V chunks\n",
    "        4. The final output for each Q chunk should be identical to full attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim: int, num_heads: int, head_dim: int, cp_world_size: int = 1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.cp_world_size = cp_world_size\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        q: torch.Tensor,\n",
    "        k: torch.Tensor,\n",
    "        v: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute Ring Attention.\n",
    "\n",
    "        Args:\n",
    "            q: Query tensor of shape (batch, num_heads, seq_len, head_dim)\n",
    "            k: Key tensor of shape (batch, num_heads, seq_len, head_dim)\n",
    "            v: Value tensor of shape (batch, num_heads, seq_len, head_dim)\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (batch, num_heads, seq_len, head_dim)\n",
    "        \"\"\"\n",
    "        B, H, S, D = q.shape\n",
    "        R = self.cp_world_size\n",
    "\n",
    "        if R == 1:\n",
    "            # No Context Parallelism â€” standard attention\n",
    "            return F.scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "        chunk_size = S // R\n",
    "        # Split Q into chunks (one per ring rank)\n",
    "        q_chunks = q.split(chunk_size, dim=2)  # R chunks of (B, H, S/R, D)\n",
    "        k_chunks = k.split(chunk_size, dim=2)\n",
    "        v_chunks = v.split(chunk_size, dim=2)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        # Simulate each ring rank\n",
    "        for rank in range(R):\n",
    "            # YOUR CODE HERE:\n",
    "            # For this rank's Q chunk, compute attention against ALL K, V chunks\n",
    "            # by simulating the ring rotation.\n",
    "            #\n",
    "            # Use the online softmax (log-sum-exp) trick:\n",
    "            #   - Track running_max (max of attention logits seen so far)\n",
    "            #   - Track running_sum (sum of exp(logits - running_max))\n",
    "            #   - Track running_output (weighted sum of V)\n",
    "            #   - For each new K, V chunk:\n",
    "            #       1. Compute logits = Q_local @ K_chunk^T * scale\n",
    "            #       2. new_max = max(running_max, logits.max)\n",
    "            #       3. Rescale running terms: multiply by exp(running_max - new_max)\n",
    "            #       4. Add new contribution: exp(logits - new_max) @ V_chunk\n",
    "            #       5. Update running_max, running_sum, running_output\n",
    "            #   - Final output = running_output / running_sum\n",
    "            raise NotImplementedError(\"Implement ring attention for one rank\")\n",
    "\n",
    "        # Concatenate outputs from all ranks\n",
    "        output = torch.cat(outputs, dim=2)  # (B, H, S, D)\n",
    "        return output"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: Ring Attention should produce identical results to standard attention\n",
    "\n",
    "torch.manual_seed(42)\n",
    "B, H, S, D = 1, 4, 256, 64\n",
    "q = torch.randn(B, H, S, D, device=device, dtype=torch.bfloat16)\n",
    "k = torch.randn(B, H, S, D, device=device, dtype=torch.bfloat16)\n",
    "v = torch.randn(B, H, S, D, device=device, dtype=torch.bfloat16)\n",
    "\n",
    "# Reference: standard attention\n",
    "ref_out = F.scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "# Ring Attention with CP=4\n",
    "ring_attn = RingAttention(H * D, H, D, cp_world_size=4).to(device)\n",
    "ring_out = ring_attn(q, k, v)\n",
    "\n",
    "print(f\"Reference output shape: {ref_out.shape}\")\n",
    "print(f\"Ring Attention output shape: {ring_out.shape}\")\n",
    "print(f\"Max difference: {(ref_out - ring_out).abs().max().item():.6f}\")\n",
    "print(f\"Match (tolerance 1e-2): {torch.allclose(ref_out, ring_out, atol=1e-2)}\")\n",
    "\n",
    "# Memory comparison\n",
    "full_attn_mem = B * H * S * S * 2 / 1e6  # Full attention score matrix in BF16\n",
    "ring_attn_mem = B * H * (S // 4) * (S // 4) * 2 / 1e6  # One chunk at a time\n",
    "print(f\"\\nFull attention memory: {full_attn_mem:.1f} MB\")\n",
    "print(f\"Ring attention memory (per step): {ring_attn_mem:.1f} MB\")\n",
    "print(f\"Memory reduction: {full_attn_mem / ring_attn_mem:.1f}x\")"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. Ring Attention trades memory for communication. How many bytes of K, V data does each GPU send per ring step? For 128K tokens with 8192 hidden dim, what is the total communication volume?\n",
    "2. Why is the online softmax trick necessary? What would happen if you naively averaged the attention outputs from each ring step?\n",
    "3. In practice, the ring communication (sending K, V) can be overlapped with the attention computation. Draw a timeline showing how this overlap works for 4 ring steps."
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4 Expert Parallelism\n",
    "\n",
    "Expert Parallelism distributes MoE experts across GPUs. Each GPU hosts a subset of experts and processes only the tokens routed to its experts."
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement Expert-Parallel MoE Layer\n",
    "\n",
    "class ExpertParallelMoE(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture-of-Experts layer with Expert Parallelism.\n",
    "\n",
    "    Components:\n",
    "    1. Router: A linear layer that produces gate logits for each token\n",
    "    2. Top-K selection: Select the top-K experts for each token\n",
    "    3. All-to-All dispatch: Send tokens to the GPU hosting their assigned expert\n",
    "    4. Expert computation: Each GPU runs its local experts on received tokens\n",
    "    5. All-to-All collect: Send results back to the original GPU\n",
    "    6. Weighted combination: Combine expert outputs using gate weights\n",
    "\n",
    "    For this single-GPU simulation, we implement all experts locally but simulate\n",
    "    the All-to-All communication cost and load balancing.\n",
    "\n",
    "    Args:\n",
    "        hidden_dim: Model hidden dimension\n",
    "        expert_ffn_dim: Per-expert MLP intermediate dimension\n",
    "        num_experts: Total number of experts\n",
    "        top_k: Number of experts activated per token\n",
    "        ep_world_size: Expert Parallelism degree (experts per GPU = num_experts / ep_world_size)\n",
    "\n",
    "    Hints:\n",
    "        1. Router: nn.Linear(hidden_dim, num_experts)\n",
    "        2. Top-K: Use torch.topk on gate logits, then softmax to normalize\n",
    "        3. Expert MLP: nn.Linear(hidden_dim, expert_ffn_dim) + GeLU + nn.Linear(expert_ffn_dim, hidden_dim)\n",
    "        4. Load balancing loss: alpha * N_experts * sum(f_i * p_i)\n",
    "           where f_i = fraction of tokens routed to expert i\n",
    "           and p_i = mean gate probability for expert i\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        expert_ffn_dim: int,\n",
    "        num_experts: int = 8,\n",
    "        top_k: int = 2,\n",
    "        ep_world_size: int = 1,\n",
    "        balance_loss_weight: float = 0.01,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.ep_world_size = ep_world_size\n",
    "        self.experts_per_gpu = num_experts // ep_world_size\n",
    "        self.balance_loss_weight = balance_loss_weight\n",
    "\n",
    "        # YOUR CODE HERE:\n",
    "        # 1. Define the router (linear layer: hidden_dim -> num_experts)\n",
    "        # 2. Define expert MLPs (one for each expert)\n",
    "        #    Each expert: Linear(hidden_dim, expert_ffn_dim) -> GeLU -> Linear(expert_ffn_dim, hidden_dim)\n",
    "        #    Use nn.ModuleList to hold all experts\n",
    "        raise NotImplementedError(\"Initialize MoE components\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through the MoE layer.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            Tuple of:\n",
    "            - Output tensor of shape (batch, seq, hidden_dim)\n",
    "            - Auxiliary load-balancing loss (scalar)\n",
    "\n",
    "        Steps:\n",
    "            1. Compute gate logits: router(x) -> (batch, seq, num_experts)\n",
    "            2. Apply softmax to get gate probabilities\n",
    "            3. Select top-K experts per token\n",
    "            4. Renormalize top-K gate weights to sum to 1\n",
    "            5. For each token, compute weighted sum of selected expert outputs\n",
    "            6. Compute load-balancing loss\n",
    "\n",
    "        Hints for load-balancing loss:\n",
    "            - f_i = (number of tokens where expert i is in top-K) / total_tokens\n",
    "            - p_i = mean of gate_probs[:, :, i] across all tokens\n",
    "            - loss = balance_loss_weight * num_experts * sum(f_i * p_i)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Implement MoE forward pass\")"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: Test MoE layer and check load balancing\n",
    "\n",
    "torch.manual_seed(42)\n",
    "moe = ExpertParallelMoE(\n",
    "    hidden_dim=256,\n",
    "    expert_ffn_dim=512,\n",
    "    num_experts=8,\n",
    "    top_k=2,\n",
    "    ep_world_size=1,\n",
    "    balance_loss_weight=0.01\n",
    ").to(device).bfloat16()\n",
    "\n",
    "x_test = torch.randn(2, 64, 256, device=device, dtype=torch.bfloat16)\n",
    "output, aux_loss = moe(x_test)\n",
    "\n",
    "print(f\"Input shape: {x_test.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Auxiliary loss: {aux_loss.item():.6f}\")\n",
    "\n",
    "# Check expert utilization\n",
    "with torch.no_grad():\n",
    "    gate_logits = moe.router(x_test.float())\n",
    "    gate_probs = F.softmax(gate_logits, dim=-1)\n",
    "    top_k_indices = torch.topk(gate_probs, k=2, dim=-1).indices  # (B, S, K)\n",
    "\n",
    "    expert_counts = torch.zeros(8)\n",
    "    for i in range(8):\n",
    "        expert_counts[i] = (top_k_indices == i).sum().item()\n",
    "\n",
    "    print(f\"\\nExpert utilization (token counts):\")\n",
    "    for i in range(8):\n",
    "        bar = '#' * int(expert_counts[i] / 5)\n",
    "        print(f\"  Expert {i}: {expert_counts[i]:5.0f} tokens  {bar}\")\n",
    "\n",
    "    max_load = expert_counts.max().item()\n",
    "    mean_load = expert_counts.mean().item()\n",
    "    imbalance = max_load / mean_load if mean_load > 0 else float('inf')\n",
    "    print(f\"\\nLoad imbalance (max/mean): {imbalance:.2f}\")\n",
    "    print(f\"Target: <= 1.10\")"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. Why does Expert Parallelism require All-to-All communication (not AllReduce)? How does the communication pattern differ?\n",
    "2. If you disable the load-balancing loss, what happens to expert utilization over many training steps? Why?\n",
    "3. DeepSeek-V3 uses 256 experts with top-8 routing. How many GPUs would you need for Expert Parallelism if each GPU hosts 8 experts? What is the All-to-All communication cost?"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.5 Combining All Five Dimensions\n",
    "\n",
    "Now we bring everything together: find the optimal 5D parallelism configuration that maximizes throughput while fitting within the memory budget."
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the parallelism configuration optimizer\n",
    "\n",
    "def estimate_throughput(\n",
    "    model: ModelConfig,\n",
    "    hw: HardwareConfig,\n",
    "    par: ParallelismConfig,\n",
    "    global_batch_size: int,\n",
    "    seq_length: int,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Estimate training throughput for a given 5D parallelism configuration.\n",
    "\n",
    "    This combines the memory model and throughput model from Section 2.\n",
    "\n",
    "    Args:\n",
    "        model: Model configuration\n",
    "        hw: Hardware configuration\n",
    "        par: Parallelism configuration\n",
    "        global_batch_size: Total batch size\n",
    "        seq_length: Sequence length\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - 'memory_gb': Peak per-GPU memory (GB)\n",
    "        - 'fits_in_memory': Whether config fits in GPU memory\n",
    "        - 'compute_time_s': Compute time per step\n",
    "        - 'tp_comm_time_s': Tensor Parallel communication time\n",
    "        - 'pp_comm_time_s': Pipeline Parallel communication time\n",
    "        - 'dp_comm_time_s': Data Parallel communication time\n",
    "        - 'ep_comm_time_s': Expert Parallel communication time\n",
    "        - 'bubble_time_s': Pipeline bubble time\n",
    "        - 'step_time_s': Total step time\n",
    "        - 'tokens_per_sec': Throughput in tokens/second\n",
    "        - 'mfu': Model FLOPs Utilization\n",
    "\n",
    "    Hints:\n",
    "        1. First check memory: call estimate_memory_bytes and verify it fits\n",
    "        2. Compute FLOPs per step: 6 * active_params * global_batch_size * seq_length\n",
    "        3. Compute time = FLOPs / (num_gpus * peak_tflops * 1e12)\n",
    "        4. TP comm: 2 AllReduce per layer * layers_per_stage / NVLink bandwidth\n",
    "           Message size per AllReduce: batch * seq * hidden * 2 bytes (BF16)\n",
    "        5. PP comm: point-to-point activation transfer at stage boundaries\n",
    "           Message size: micro_batch * seq * hidden * 2 bytes\n",
    "           Over InfiniBand bandwidth\n",
    "        6. DP comm: 1 AllReduce of gradients per step\n",
    "           Message size: params_per_gpu * 2 bytes / dp group size (ring AllReduce)\n",
    "           Over InfiniBand\n",
    "        7. EP comm: All-to-All for each MoE layer\n",
    "           Message size: batch * seq * hidden * 2 bytes * (ep - 1) / ep\n",
    "        8. Bubble: bubble_fraction(pp, num_microbatches) * compute_time\n",
    "        9. MFU = useful_flops / (step_time * num_gpus * peak_tflops * 1e12)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Implement throughput estimation\")\n",
    "\n",
    "\n",
    "def find_optimal_config(\n",
    "    model: ModelConfig,\n",
    "    hw: HardwareConfig,\n",
    "    seq_length: int,\n",
    "    global_batch_size: int = 256,\n",
    ") -> Tuple[ParallelismConfig, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Search for the optimal 5D parallelism configuration.\n",
    "\n",
    "    Enumerate all valid configurations and return the one with highest throughput\n",
    "    that fits within the GPU memory budget.\n",
    "\n",
    "    Args:\n",
    "        model: Model configuration\n",
    "        hw: Hardware configuration\n",
    "        seq_length: Target sequence length\n",
    "        global_batch_size: Target global batch size\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (best config, performance metrics)\n",
    "\n",
    "    Hints:\n",
    "        1. Generate all valid (DP, TP, PP, EP) tuples where:\n",
    "           - DP * TP * PP * EP = num_gpus\n",
    "           - TP in {1, 2, 4, 8} and TP <= gpus_per_node\n",
    "           - PP divides num_layers evenly\n",
    "           - EP divides num_experts evenly\n",
    "        2. For each config, try different ZeRO stages (0, 1, 2, 3)\n",
    "        3. For each config, try activation_checkpointing on/off\n",
    "        4. Set num_microbatches = max(4, global_batch_size // (DP * micro_batch_size))\n",
    "        5. Evaluate throughput and track the best\n",
    "        6. Return the configuration with highest MFU among those that fit in memory\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Implement configuration search\")"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the optimizer\n",
    "print(\"Searching for optimal parallelism configuration...\")\n",
    "print(f\"Model: {model_cfg.total_params / 1e9:.2f}B params, {model_cfg.num_layers} layers, {model_cfg.num_experts} experts\")\n",
    "print(f\"Hardware: {hw_cfg.num_gpus} GPUs, {hw_cfg.gpus_per_node} per node\")\n",
    "print(f\"Target: seq_len={model_cfg.max_seq_len}, batch_size=256\\n\")\n",
    "\n",
    "best_config, best_perf = find_optimal_config(model_cfg, hw_cfg, seq_length=model_cfg.max_seq_len)\n",
    "\n",
    "print(\"=== Optimal Configuration ===\")\n",
    "print(f\"  DP = {best_config.dp}\")\n",
    "print(f\"  TP = {best_config.tp}\")\n",
    "print(f\"  PP = {best_config.pp}\")\n",
    "print(f\"  SP = {best_config.sp} (= TP)\")\n",
    "print(f\"  EP = {best_config.ep}\")\n",
    "print(f\"  ZeRO Stage = {best_config.zero_stage}\")\n",
    "print(f\"  Activation Checkpointing = {best_config.activation_checkpointing}\")\n",
    "print(f\"  Micro-batches = {best_config.num_microbatches}\")\n",
    "print(f\"\\n=== Performance Metrics ===\")\n",
    "for key, val in best_perf.items():\n",
    "    if 'time' in key:\n",
    "        print(f\"  {key}: {val * 1000:.1f} ms\")\n",
    "    elif key == 'mfu':\n",
    "        print(f\"  {key}: {val * 100:.1f}%\")\n",
    "    elif key == 'memory_gb':\n",
    "        print(f\"  {key}: {val:.1f} GB / {hw_cfg.gpu_memory_gb:.0f} GB\")\n",
    "    elif key == 'fits_in_memory':\n",
    "        print(f\"  {key}: {val}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {val:,.0f}\")"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Compare top 5 configurations\n",
    "\n",
    "def compare_top_configs(model, hw, seq_length, global_batch_size=256, top_n=5):\n",
    "    \"\"\"Find and visualize the top-N parallelism configurations.\"\"\"\n",
    "    configs = []\n",
    "    tp_options = [t for t in [1, 2, 4, 8] if t <= hw.gpus_per_node]\n",
    "    pp_options = [p for p in [1, 2, 4, 8] if model.num_layers % p == 0]\n",
    "    ep_options = [e for e in [1, 2, 4, 8] if model.num_experts % e == 0]\n",
    "\n",
    "    for tp in tp_options:\n",
    "        for pp in pp_options:\n",
    "            for ep in ep_options:\n",
    "                dp = hw.num_gpus // (tp * pp * ep)\n",
    "                if dp < 1 or dp * tp * pp * ep != hw.num_gpus:\n",
    "                    continue\n",
    "                for zero in [1, 3]:\n",
    "                    for ac in [True, False]:\n",
    "                        par = ParallelismConfig(\n",
    "                            dp=dp, tp=tp, pp=pp, sp=tp, ep=ep,\n",
    "                            zero_stage=zero,\n",
    "                            activation_checkpointing=ac,\n",
    "                            num_microbatches=max(4, global_batch_size // dp)\n",
    "                        )\n",
    "                        errors = par.validate(hw, model)\n",
    "                        if errors:\n",
    "                            continue\n",
    "                        perf = estimate_throughput(model, hw, par, global_batch_size, seq_length)\n",
    "                        if perf['fits_in_memory']:\n",
    "                            configs.append((par, perf))\n",
    "\n",
    "    # Sort by MFU\n",
    "    configs.sort(key=lambda x: x[1]['mfu'], reverse=True)\n",
    "\n",
    "    print(f\"\\nTop {top_n} Configurations:\\n\")\n",
    "    print(f\"{'Rank':<6}{'DP':<5}{'TP':<5}{'PP':<5}{'EP':<5}{'ZeRO':<6}{'AC':<5}{'Memory (GB)':<13}{'MFU (%)':<10}{'Tok/s':<12}\")\n",
    "    print('-' * 76)\n",
    "    for i, (par, perf) in enumerate(configs[:top_n]):\n",
    "        print(f\"{i+1:<6}{par.dp:<5}{par.tp:<5}{par.pp:<5}{par.ep:<5}{par.zero_stage:<6}{'Y' if par.activation_checkpointing else 'N':<5}{perf['memory_gb']:<13.1f}{perf['mfu']*100:<10.1f}{perf['tokens_per_sec']:<12,.0f}\")\n",
    "\n",
    "    return configs[:top_n]\n",
    "\n",
    "top_configs = compare_top_configs(model_cfg, hw_cfg, model_cfg.max_seq_len)"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. Why does the optimal configuration typically have TP=8 (full intra-node)? Under what circumstances might TP=4 or TP=2 be better?\n",
    "2. How does the Expert Parallelism degree interact with Pipeline Parallelism? Can you have both EP=8 and PP=8 on a 1024-GPU cluster? Why or why not?\n",
    "3. For the Meridian production model (72B params, 128K seq length, 1024 GPUs), estimate the optimal configuration by scaling up from your small-model results. What changes when you scale by 50x in parameters?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.5 Training Strategy"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop integrating all 5 parallelism dimensions (single-GPU simulation)\n",
    "\n",
    "def distributed_training_step(\n",
    "    model: nn.Module,\n",
    "    batch: torch.Tensor,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    config: ParallelismConfig,\n",
    "    model_cfg: ModelConfig,\n",
    "    grad_clip_norm: float = 1.0,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Simulate one distributed training step with 5D parallelism.\n",
    "\n",
    "    On a single GPU, we simulate the effects of parallelism:\n",
    "    - TP: Compute with reduced hidden dimension (simulated split)\n",
    "    - PP: Process only a subset of layers (num_layers // pp)\n",
    "    - DP: Process 1/dp of the batch\n",
    "    - EP: Process tokens only through top-K of local experts\n",
    "    - SP: Process 1/sp of the sequence\n",
    "\n",
    "    In production, this would use torch.distributed with NCCL collectives.\n",
    "\n",
    "    Args:\n",
    "        model: The model (full or subset depending on simulation)\n",
    "        batch: Input token IDs\n",
    "        optimizer: Optimizer (AdamW)\n",
    "        config: Parallelism configuration\n",
    "        model_cfg: Model configuration\n",
    "        grad_clip_norm: Max gradient norm for clipping\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with loss, gradient norm, step time, and simulated MFU\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(batch)\n",
    "    targets = batch[:, 1:].contiguous()\n",
    "    logits = logits[:, :-1].contiguous()\n",
    "    loss = F.cross_entropy(logits.view(-1, model_cfg.vocab_size), targets.view(-1))\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient clipping\n",
    "    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
    "\n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    step_time = time.time() - start_time\n",
    "\n",
    "    return {\n",
    "        'loss': loss.item(),\n",
    "        'grad_norm': grad_norm.item() if isinstance(grad_norm, torch.Tensor) else grad_norm,\n",
    "        'step_time': step_time,\n",
    "    }"
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a learning rate schedule with warmup and cosine decay\n",
    "\n",
    "def get_cosine_schedule_with_warmup(\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    num_warmup_steps: int,\n",
    "    num_training_steps: int,\n",
    "    min_lr_ratio: float = 0.1,\n",
    ") -> torch.optim.lr_scheduler.LambdaLR:\n",
    "    \"\"\"\n",
    "    Create a cosine learning rate schedule with linear warmup.\n",
    "\n",
    "    The schedule:\n",
    "    - Steps 0 to num_warmup_steps: Linear increase from 0 to base_lr\n",
    "    - Steps num_warmup_steps to num_training_steps: Cosine decay from base_lr to base_lr * min_lr_ratio\n",
    "\n",
    "    Args:\n",
    "        optimizer: The optimizer\n",
    "        num_warmup_steps: Number of warmup steps\n",
    "        num_training_steps: Total training steps\n",
    "        min_lr_ratio: Ratio of minimum LR to base LR (default 0.1)\n",
    "\n",
    "    Returns:\n",
    "        LambdaLR scheduler\n",
    "\n",
    "    Hints:\n",
    "        1. During warmup: lr_scale = step / num_warmup_steps\n",
    "        2. After warmup: progress = (step - warmup) / (total - warmup)\n",
    "           lr_scale = min_lr_ratio + 0.5 * (1 - min_lr_ratio) * (1 + cos(pi * progress))\n",
    "        3. Create a lambda function and pass to LambdaLR\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Implement cosine schedule with warmup\")"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a short training loop\n",
    "\n",
    "# Use a small model for actual training on Colab\n",
    "train_cfg = ModelConfig(\n",
    "    vocab_size=32000, hidden_dim=512, num_layers=6, num_heads=8,\n",
    "    head_dim=64, ffn_dim=1376, num_experts=4, top_k=2,\n",
    "    expert_ffn_dim=512, max_seq_len=512\n",
    ")\n",
    "train_model = SimpleTransformer(train_cfg, num_layers=6).to(device).bfloat16()\n",
    "train_optimizer = torch.optim.AdamW(train_model.parameters(), lr=3e-4, weight_decay=0.1, betas=(0.9, 0.95))\n",
    "\n",
    "num_steps = 100\n",
    "scheduler = get_cosine_schedule_with_warmup(train_optimizer, num_warmup_steps=10, num_training_steps=num_steps)\n",
    "\n",
    "# Generate synthetic training data\n",
    "train_data = create_synthetic_dataset(512, train_cfg.max_seq_len, train_cfg.vocab_size)\n",
    "train_batches = [train_data[i:i+4] for i in range(0, len(train_data), 4)]\n",
    "\n",
    "losses = []\n",
    "lrs = []\n",
    "grad_norms = []\n",
    "step_times = []\n",
    "\n",
    "print(\"Training...\")\n",
    "par_cfg = ParallelismConfig(dp=1, tp=1, pp=1, sp=1, ep=1)\n",
    "\n",
    "for step in range(min(num_steps, len(train_batches))):\n",
    "    batch = train_batches[step].to(device)\n",
    "    metrics = distributed_training_step(\n",
    "        train_model, batch, train_optimizer, par_cfg, train_cfg\n",
    "    )\n",
    "    scheduler.step()\n",
    "\n",
    "    losses.append(metrics['loss'])\n",
    "    lrs.append(scheduler.get_last_lr()[0])\n",
    "    grad_norms.append(metrics['grad_norm'])\n",
    "    step_times.append(metrics['step_time'])\n",
    "\n",
    "    if (step + 1) % 20 == 0:\n",
    "        print(f\"  Step {step+1}/{num_steps}: loss={metrics['loss']:.4f}, \"\n",
    "              f\"grad_norm={metrics['grad_norm']:.4f}, \"\n",
    "              f\"time={metrics['step_time']*1000:.1f}ms, \"\n",
    "              f\"lr={lrs[-1]:.2e}\")\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes[0, 0].plot(losses)\n",
    "axes[0, 0].set_title('Training Loss')\n",
    "axes[0, 0].set_xlabel('Step')\n",
    "axes[0, 1].plot(lrs)\n",
    "axes[0, 1].set_title('Learning Rate')\n",
    "axes[0, 1].set_xlabel('Step')\n",
    "axes[1, 0].plot(grad_norms)\n",
    "axes[1, 0].set_title('Gradient Norm')\n",
    "axes[1, 0].set_xlabel('Step')\n",
    "axes[1, 1].plot([t * 1000 for t in step_times])\n",
    "axes[1, 1].set_title('Step Time (ms)')\n",
    "axes[1, 1].set_xlabel('Step')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. Why do we use AdamW instead of Adam? What is the difference in how weight decay is applied, and why does it matter for large Transformers?\n",
    "2. The cosine schedule decays to 10% of the peak LR. What would happen if it decayed to 0? What about 50%?\n",
    "3. Why is gradient clipping especially important in pipeline-parallel training? (Hint: think about gradient magnitudes across different pipeline stages.)\n",
    "\n",
    "---\n",
    "\n",
    "## 3.6 Evaluation"
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate evaluation comparing baseline vs optimized configuration\n",
    "\n",
    "def evaluate_configurations(\n",
    "    model: ModelConfig,\n",
    "    hw: HardwareConfig,\n",
    "    baseline: ParallelismConfig,\n",
    "    optimized: ParallelismConfig,\n",
    "    seq_length: int,\n",
    "    global_batch_size: int = 256,\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Compare baseline (FSDP) and optimized 5D parallelism configurations.\n",
    "\n",
    "    Compute all metrics for both configurations and return a structured comparison.\n",
    "\n",
    "    Args:\n",
    "        model: Model configuration\n",
    "        hw: Hardware configuration\n",
    "        baseline: Baseline parallelism config (pure FSDP)\n",
    "        optimized: Optimized 5D parallelism config\n",
    "        seq_length: Sequence length\n",
    "        global_batch_size: Global batch size\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with 'baseline' and 'optimized' sub-dictionaries,\n",
    "        each containing all performance metrics.\n",
    "\n",
    "    Hints:\n",
    "        1. Call estimate_throughput for both configurations\n",
    "        2. Also compute memory breakdown for both\n",
    "        3. Compute pipeline bubble fraction for the optimized config\n",
    "        4. Compute estimated training days: total_tokens / (tokens_per_sec * 86400)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Implement configuration comparison\")"
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "\n",
    "baseline_cfg = ParallelismConfig(dp=8, tp=1, pp=1, sp=1, ep=1, zero_stage=3)\n",
    "# Use the optimal config found earlier (or a manually specified good config)\n",
    "opt_cfg = best_config  # From the optimizer above\n",
    "\n",
    "comparison = evaluate_configurations(\n",
    "    model_cfg, hw_cfg, baseline_cfg, opt_cfg, seq_length=model_cfg.max_seq_len\n",
    ")\n",
    "\n",
    "# Print comparison table\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<30}{'Baseline (FSDP)':<20}{'Optimized 5D':<20}\")\n",
    "print(\"=\" * 70)\n",
    "for key in comparison['baseline']:\n",
    "    bval = comparison['baseline'][key]\n",
    "    oval = comparison['optimized'][key]\n",
    "    if isinstance(bval, bool):\n",
    "        print(f\"{key:<30}{str(bval):<20}{str(oval):<20}\")\n",
    "    elif 'time' in key:\n",
    "        print(f\"{key:<30}{bval*1000:>15.1f} ms  {oval*1000:>15.1f} ms\")\n",
    "    elif key == 'mfu':\n",
    "        print(f\"{key:<30}{bval*100:>15.1f}%    {oval*100:>15.1f}%\")\n",
    "    elif key == 'memory_gb':\n",
    "        print(f\"{key:<30}{bval:>15.1f} GB   {oval:>15.1f} GB\")\n",
    "    else:\n",
    "        print(f\"{key:<30}{bval:>15,.0f}     {oval:>15,.0f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Bar chart comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "# MFU comparison\n",
    "configs = ['Baseline\\n(FSDP)', 'Optimized\\n(5D)']\n",
    "mfu_vals = [comparison['baseline']['mfu'] * 100, comparison['optimized']['mfu'] * 100]\n",
    "axes[0].bar(configs, mfu_vals, color=['#FF6B6B', '#4CAF50'])\n",
    "axes[0].set_ylabel('MFU (%)')\n",
    "axes[0].set_title('Model FLOPs Utilization')\n",
    "axes[0].axhline(y=45, color='gold', linestyle='--', label='Target (45%)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Throughput comparison\n",
    "tok_vals = [comparison['baseline']['tokens_per_sec'], comparison['optimized']['tokens_per_sec']]\n",
    "axes[1].bar(configs, tok_vals, color=['#FF6B6B', '#4CAF50'])\n",
    "axes[1].set_ylabel('Tokens/sec')\n",
    "axes[1].set_title('Training Throughput')\n",
    "\n",
    "# Memory comparison\n",
    "mem_vals = [comparison['baseline']['memory_gb'], comparison['optimized']['memory_gb']]\n",
    "axes[2].bar(configs, mem_vals, color=['#FF6B6B', '#4CAF50'])\n",
    "axes[2].axhline(y=hw_cfg.gpu_memory_gb, color='red', linestyle='--', label='GPU limit (80 GB)')\n",
    "axes[2].set_ylabel('Memory (GB)')\n",
    "axes[2].set_title('Per-GPU Memory')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. By what factor did the optimized configuration improve MFU over the baseline? Is this improvement consistent with what real systems report?\n",
    "2. Which single parallelism dimension contributed the most to the improvement? How would you determine this experimentally?\n",
    "3. The memory reduction allows us to increase the micro-batch size. How does this affect both throughput and the effective gradient noise?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.7 Error Analysis"
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Inject controlled failures and analyze the results\n",
    "\n",
    "def analyze_failure_modes(model: ModelConfig, hw: HardwareConfig, seq_length: int):\n",
    "    \"\"\"\n",
    "    Systematically test failure modes in parallelism configurations.\n",
    "\n",
    "    Test the following scenarios and explain WHY each fails:\n",
    "\n",
    "    1. TP > GPUs per node: Set TP=16 on an 8-GPU node.\n",
    "       Expected: Cross-node TP has massive communication overhead.\n",
    "\n",
    "    2. Insufficient micro-batches for PP: Set PP=8, M=1.\n",
    "       Expected: 87.5% bubble fraction.\n",
    "\n",
    "    3. Load imbalance with EP: Disable load-balancing loss.\n",
    "       Expected: Expert collapse â€” most tokens go to 1-2 experts.\n",
    "\n",
    "    4. Memory OOM: Set TP=1, PP=1 with long sequence.\n",
    "       Expected: Activation + attention memory exceeds GPU budget.\n",
    "\n",
    "    5. ZeRO-3 communication overhead: ZeRO-3 with TP=1 on 1024 GPUs.\n",
    "       Expected: AllGather communication dominates step time.\n",
    "\n",
    "    For each scenario:\n",
    "    - Compute the relevant metrics\n",
    "    - Explain the root cause of the failure\n",
    "    - Suggest the fix\n",
    "\n",
    "    Args:\n",
    "        model: Model configuration\n",
    "        hw: Hardware configuration\n",
    "        seq_length: Sequence length\n",
    "\n",
    "    Returns:\n",
    "        List of failure analysis dictionaries, each with:\n",
    "        - 'scenario': Description\n",
    "        - 'config': The failing ParallelismConfig\n",
    "        - 'symptom': What goes wrong (metric values)\n",
    "        - 'root_cause': Why it fails\n",
    "        - 'fix': How to fix it\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Implement failure mode analysis\")\n",
    "\n",
    "\n",
    "failures = analyze_failure_modes(model_cfg, hw_cfg, model_cfg.max_seq_len)\n",
    "for i, f in enumerate(failures):\n",
    "    print(f\"\\n--- Failure Mode {i+1}: {f['scenario']} ---\")\n",
    "    print(f\"  Config: DP={f['config'].dp}, TP={f['config'].tp}, PP={f['config'].pp}, EP={f['config'].ep}\")\n",
    "    print(f\"  Symptom: {f['symptom']}\")\n",
    "    print(f\"  Root cause: {f['root_cause']}\")\n",
    "    print(f\"  Fix: {f['fix']}\")"
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. Of the failure modes tested, which one would be hardest to diagnose in a real training run? Why?\n",
    "2. How would you build a monitoring system to detect expert collapse during a multi-day training run?\n",
    "3. Memory fragmentation is a real issue that our analytical model does not capture. How does PyTorch manage GPU memory, and why can the actual memory limit be lower than the theoretical 80 GB?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.8 Scalability and Deployment Considerations"
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scaling analysis â€” how does throughput scale with GPU count?\n",
    "\n",
    "def scaling_analysis(\n",
    "    model: ModelConfig,\n",
    "    seq_length: int,\n",
    "    gpu_counts: List[int] = [128, 256, 512, 1024, 2048, 4096],\n",
    "    global_batch_size: int = 256,\n",
    ") -> Dict[int, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Analyze how training throughput scales with GPU count.\n",
    "\n",
    "    For each GPU count:\n",
    "    1. Find the optimal parallelism configuration\n",
    "    2. Record the throughput and MFU\n",
    "    3. Compute the scaling efficiency: throughput(N) / (N * throughput(1_gpu))\n",
    "\n",
    "    Args:\n",
    "        model: Model configuration\n",
    "        seq_length: Sequence length\n",
    "        gpu_counts: List of GPU counts to evaluate\n",
    "        global_batch_size: Kept constant across scales\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping gpu_count -> performance metrics\n",
    "\n",
    "    Hints:\n",
    "        1. For each GPU count, create a HardwareConfig with appropriate nodes\n",
    "        2. Call find_optimal_config for each\n",
    "        3. Scaling efficiency: normalize throughput by the single-GPU compute rate\n",
    "           (FLOPs / peak_tflops per GPU)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Implement scaling analysis\")\n",
    "\n",
    "\n",
    "results = scaling_analysis(model_cfg, model_cfg.max_seq_len)\n",
    "\n",
    "# Plot scaling curves\n",
    "gpu_counts = sorted(results.keys())\n",
    "throughputs = [results[n]['tokens_per_sec'] for n in gpu_counts]\n",
    "mfus = [results[n]['mfu'] * 100 for n in gpu_counts]\n",
    "\n",
    "# Ideal linear scaling\n",
    "ideal_throughputs = [throughputs[0] * n / gpu_counts[0] for n in gpu_counts]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.plot(gpu_counts, throughputs, 'o-', label='Actual', color='#4CAF50')\n",
    "ax1.plot(gpu_counts, ideal_throughputs, '--', label='Ideal (linear)', color='gray')\n",
    "ax1.set_xlabel('Number of GPUs')\n",
    "ax1.set_ylabel('Tokens/sec')\n",
    "ax1.set_title('Throughput Scaling')\n",
    "ax1.legend()\n",
    "ax1.set_xscale('log', base=2)\n",
    "ax1.set_yscale('log', base=2)\n",
    "\n",
    "ax2.plot(gpu_counts, mfus, 'o-', color='#2196F3')\n",
    "ax2.set_xlabel('Number of GPUs')\n",
    "ax2.set_ylabel('MFU (%)')\n",
    "ax2.set_title('MFU vs Scale')\n",
    "ax2.set_xscale('log', base=2)\n",
    "ax2.axhline(y=45, color='gold', linestyle='--', label='Target (45%)')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Inference benchmarking â€” profile latency for different TP configurations\n",
    "\n",
    "def inference_benchmark(\n",
    "    model: ModelConfig,\n",
    "    input_tokens: int = 50000,\n",
    "    output_tokens: int = 2048,\n",
    "    tp_degrees: List[int] = [1, 2, 4, 8],\n",
    ") -> Dict[int, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Estimate inference latency for different Tensor Parallelism degrees.\n",
    "\n",
    "    For inference, we care about two metrics:\n",
    "    - Time to First Token (TTFT): Time to process the input (prefill)\n",
    "    - Inter-Token Latency (ITL): Time per generated output token (decode)\n",
    "\n",
    "    Args:\n",
    "        model: Model configuration\n",
    "        input_tokens: Number of input tokens (prefill)\n",
    "        output_tokens: Number of output tokens (decode)\n",
    "        tp_degrees: TP degrees to benchmark\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping tp_degree -> latency metrics\n",
    "\n",
    "    Hints:\n",
    "        1. Prefill FLOPs ~ 2 * active_params * input_tokens\n",
    "           (No factor of 3 since no backward pass)\n",
    "        2. TTFT = prefill_flops / (tp_degree * peak_tflops * 1e12 * inference_efficiency)\n",
    "           Use inference_efficiency ~ 0.5 (lower than training due to memory-bound ops)\n",
    "        3. Decode FLOPs per token ~ 2 * active_params\n",
    "           (Each new token attends to all previous tokens, but the compute is dominated\n",
    "            by the linear projections)\n",
    "        4. ITL = decode_flops / (tp_degree * peak_tflops * 1e12 * decode_efficiency)\n",
    "           Use decode_efficiency ~ 0.15 (very memory-bound, limited by KV cache reads)\n",
    "        5. Total latency = TTFT + output_tokens * ITL\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Implement inference benchmarking\")\n",
    "\n",
    "\n",
    "inf_results = inference_benchmark(model_cfg)\n",
    "print(f\"\\nInference Benchmark ({50000} input tokens, {2048} output tokens):\")\n",
    "print(f\"{'TP':<5}{'TTFT (ms)':<15}{'ITL (ms)':<15}{'Total (ms)':<15}\")\n",
    "print('-' * 50)\n",
    "for tp in sorted(inf_results.keys()):\n",
    "    r = inf_results[tp]\n",
    "    print(f\"{tp:<5}{r['ttft_ms']:<15.0f}{r['itl_ms']:<15.1f}{r['total_ms']:<15.0f}\")"
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. At what GPU count does scaling efficiency drop below 80%? What is the main bottleneck (compute, communication, or bubbles)?\n",
    "2. For inference, why is the decode phase much less compute-efficient than prefill? (Hint: think about the arithmetic intensity â€” FLOPs per byte of memory accessed.)\n",
    "3. How does the KV-cache memory scale with sequence length? At what sequence length does KV-cache memory become the bottleneck for inference?\n",
    "\n",
    "---\n",
    "\n",
    "## 3.9 Ethical and Regulatory Analysis\n",
    "\n",
    "Financial AI systems carry unique ethical and regulatory responsibilities. In this section, you will assess the risks and safeguards for MeridianFin-72B."
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write an ethical impact assessment\n",
    "\n",
    "def ethical_impact_assessment() -> str:\n",
    "    \"\"\"\n",
    "    Write a ~500-word ethical impact assessment for MeridianFin-72B.\n",
    "\n",
    "    Cover the following topics:\n",
    "\n",
    "    1. DATA BIAS:\n",
    "       - What biases exist in SEC filing data? (temporal, sector, size, geographic)\n",
    "       - How might these biases affect model outputs?\n",
    "       - What mitigation strategies would you recommend?\n",
    "\n",
    "    2. FAIRNESS ACROSS MARKET SECTORS:\n",
    "       - If the model performs better on tech company filings than energy company\n",
    "         filings, what are the implications for Meridian's clients?\n",
    "       - How would you measure and report sector-specific performance?\n",
    "       - What is the minimum acceptable performance gap between sectors?\n",
    "\n",
    "    3. REGULATORY COMPLIANCE:\n",
    "       - What regulations govern the use of AI in financial services?\n",
    "         (SEC, FINRA, MiFID II, SR 11-7)\n",
    "       - How should the model's outputs be disclosed to end users?\n",
    "       - What are the risks of the model producing outputs that could be\n",
    "         construed as investment advice?\n",
    "\n",
    "    4. SYSTEMIC RISK:\n",
    "       - If multiple financial institutions use the same model for trading\n",
    "         signals, what are the systemic risks?\n",
    "       - How could model failures cascade through financial markets?\n",
    "\n",
    "    Returns:\n",
    "        A string containing the ~500-word assessment.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Write the ethical impact assessment\")\n",
    "\n",
    "\n",
    "assessment = ethical_impact_assessment()\n",
    "print(assessment)"
   ],
   "id": "cell_51"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought Questions:**\n",
    "1. If MeridianFin-72B is used to extract trading signals, and the model makes an error that leads to financial losses, who is liable â€” Meridian AI or the client? How should this be addressed contractually?\n",
    "2. How would you ensure that the model does not memorize and leak confidential information from one client's documents when serving another client?\n",
    "3. What is the environmental impact of training a 72B model on 1,024 H100 GPUs for 30 days? How much CO2 does this produce, and how does it compare to everyday activities?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, you have worked through the complete technical challenge facing Meridian AI:\n",
    "\n",
    "1. **Profiled** the memory and compute requirements of training a large MoE model\n",
    "2. **Established a baseline** using pure Data Parallelism (FSDP) and identified its limitations\n",
    "3. **Implemented** each of the five parallelism dimensions:\n",
    "   - Tensor Parallelism (column/row-parallel linear layers, Megatron MLP)\n",
    "   - Pipeline Parallelism (stage partitioning, 1F1B schedule)\n",
    "   - Sequence/Context Parallelism (Ring Attention)\n",
    "   - Expert Parallelism (MoE with All-to-All dispatch and load balancing)\n",
    "4. **Built a configuration optimizer** that searches the 5D space for the best tradeoff\n",
    "5. **Evaluated** the optimized configuration against the baseline\n",
    "6. **Analyzed failure modes** and scaling behavior\n",
    "7. **Considered ethical and regulatory** implications\n",
    "\n",
    "The key insight is that no single parallelism strategy is sufficient. Each dimension addresses a different bottleneck (memory, compute, sequence length, expert capacity), and the art of distributed training is finding the right combination for your specific model, hardware, and training requirements.\n",
    "\n",
    "For further reading, see Section 4 (Production and System Design Extension) of the case study document, which covers serving infrastructure, monitoring, model versioning, and cost analysis for deploying MeridianFin-72B in production."
   ],
   "id": "cell_52"
  }
 ]
}