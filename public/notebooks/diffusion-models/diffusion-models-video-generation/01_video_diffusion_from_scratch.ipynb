{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Video Diffusion from Scratch ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"18Lc8XC_lV-uzRcNcgg-LLZXfkKSQ0vDX\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Video Diffusion from Scratch: Generating Moving Digits\n",
    "\n",
    "*Part 1 of the Vizuara series on Diffusion Models for Video Generation*\n",
    "*Estimated time: 45 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ü§ñ AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** ‚Äî it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[üëâ Open AI Teaching Assistant](https://course-creator-brown.vercel.app/courses/diffusion-models-video-generation/practice/1/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup ‚Äî Run this cell first\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_why_it_matters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "We have seen how diffusion models can generate stunning **images** from pure noise. But what about **video**?\n",
    "\n",
    "A video is just a sequence of images played in rapid succession. If we can generate one image from noise, surely we can generate a *sequence* of images ‚Äî a video ‚Äî from noise too?\n",
    "\n",
    "That is exactly what we will build in this notebook. By the end, you will have trained a neural network that takes **pure random noise** and transforms it, step by step, into a coherent video of a digit moving across the screen.\n",
    "\n",
    "Let us start by creating our dataset."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Building Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_building_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_building_intuition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition ‚Äî What Is a Video, Really?\n",
    "\n",
    "Think of a flipbook. Each page has a slightly different drawing, and when you flip fast enough, it comes to life as smooth motion.\n",
    "\n",
    "A video works the same way. It is a **4D tensor** with shape `(T, C, H, W)`:\n",
    "- **T** = number of frames (time)\n",
    "- **C** = channels (1 for grayscale, 3 for RGB)\n",
    "- **H, W** = height and width of each frame\n",
    "\n",
    "The key insight for video diffusion: we treat this entire 4D tensor as a *single data point*. We add noise to the entire video at once, and we train a network to denoise the entire video at once. This forces the network to learn not just what individual frames look like, but how they relate to each other over time.\n",
    "\n",
    "### ü§î Think About This\n",
    "\n",
    "If we generated each frame independently using an image diffusion model, what would happen? Why can't we just run an image model 16 times and stitch the results together?\n",
    "\n",
    "*Hint: Think about what \"independently sampled\" means for consistency between frames.*"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Creating Dataset\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_creating_dataset.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_creating_dataset"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Our Dataset: Moving MNIST\n",
    "\n",
    "We will create a simple but powerful dataset: MNIST digits that **move** across a canvas. Each video shows a digit translating smoothly from one position to another."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Download MNIST\n",
    "mnist = datasets.MNIST(root=\"./data\", train=True, download=True,\n",
    "                       transform=transforms.ToTensor())\n",
    "\n",
    "print(f\"MNIST loaded: {len(mnist)} images\")\n",
    "print(f\"Image shape: {mnist[0][0].shape}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_moving_mnist_video(digit_img, canvas_size=32, num_frames=16):\n",
    "    \"\"\"\n",
    "    Create a video of a single MNIST digit moving across a canvas.\n",
    "\n",
    "    Args:\n",
    "        digit_img: (1, 28, 28) MNIST digit tensor\n",
    "        canvas_size: size of the video canvas (square)\n",
    "        num_frames: number of frames in the video\n",
    "\n",
    "    Returns:\n",
    "        video: (num_frames, 1, canvas_size, canvas_size) tensor\n",
    "    \"\"\"\n",
    "    # Resize digit to fit on canvas (14x14)\n",
    "    digit = F.interpolate(digit_img.unsqueeze(0), size=14,\n",
    "                          mode=\"bilinear\", align_corners=False)[0]\n",
    "    dh, dw = digit.shape[1], digit.shape[2]\n",
    "\n",
    "    # Random start and end positions\n",
    "    max_pos = canvas_size - dh\n",
    "    start_y = np.random.randint(0, max_pos)\n",
    "    start_x = np.random.randint(0, max_pos)\n",
    "    end_y = np.random.randint(0, max_pos)\n",
    "    end_x = np.random.randint(0, max_pos)\n",
    "\n",
    "    video = torch.zeros(num_frames, 1, canvas_size, canvas_size)\n",
    "\n",
    "    for t in range(num_frames):\n",
    "        # Linear interpolation of position\n",
    "        frac = t / max(num_frames - 1, 1)\n",
    "        y = int(start_y + frac * (end_y - start_y))\n",
    "        x = int(start_x + frac * (end_x - start_x))\n",
    "        video[t, :, y:y+dh, x:x+dw] = digit\n",
    "\n",
    "    return video\n",
    "\n",
    "\n",
    "def create_dataset(num_videos=2000, num_frames=16, canvas_size=32):\n",
    "    \"\"\"Create a dataset of moving MNIST videos.\"\"\"\n",
    "    videos = []\n",
    "    for i in range(num_videos):\n",
    "        idx = np.random.randint(0, len(mnist))\n",
    "        digit_img = mnist[idx][0]  # (1, 28, 28)\n",
    "        video = create_moving_mnist_video(digit_img, canvas_size, num_frames)\n",
    "        videos.append(video)\n",
    "    return torch.stack(videos)  # (N, T, 1, H, W)\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "NUM_FRAMES = 16\n",
    "CANVAS_SIZE = 32\n",
    "dataset = create_dataset(num_videos=2000, num_frames=NUM_FRAMES,\n",
    "                         canvas_size=CANVAS_SIZE)\n",
    "print(f\"Dataset shape: {dataset.shape}\")\n",
    "print(f\"  {dataset.shape[0]} videos, {dataset.shape[1]} frames each\")\n",
    "print(f\"  Resolution: {dataset.shape[3]}x{dataset.shape[4]}\")\n",
    "print(f\"  Total values per video: {dataset[0].numel():,}\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Dataset Visualization\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_dataset_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_dataset_visualization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize a few videos from our dataset\n",
    "fig, axes = plt.subplots(4, NUM_FRAMES, figsize=(20, 5))\n",
    "for row in range(4):\n",
    "    for t in range(NUM_FRAMES):\n",
    "        axes[row, t].imshow(dataset[row, t, 0].numpy(), cmap=\"gray\",\n",
    "                            vmin=0, vmax=1)\n",
    "        axes[row, t].axis(\"off\")\n",
    "        if row == 0:\n",
    "            axes[row, t].set_title(f\"t={t}\", fontsize=8)\n",
    "fig.suptitle(\"Sample Videos ‚Äî Moving MNIST\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_math"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Mathematics of Video Diffusion\n",
    "\n",
    "Now let us formalize the diffusion process for video. This is **exactly** the same math as image diffusion ‚Äî the only difference is that our data point $\\mathbf{v}$ is a 4D video tensor instead of a 2D image.\n",
    "\n",
    "### Forward Process: Adding Noise\n",
    "\n",
    "At each diffusion timestep $t$, we add Gaussian noise:\n",
    "\n",
    "$$q(\\mathbf{v}_t \\mid \\mathbf{v}_{t-1}) = \\mathcal{N}(\\mathbf{v}_t;\\, \\sqrt{1-\\beta_t}\\,\\mathbf{v}_{t-1},\\, \\beta_t\\,\\mathbf{I})$$\n",
    "\n",
    "Computationally, this means: scale the video down by $\\sqrt{1-\\beta_t}$ and add noise with standard deviation $\\sqrt{\\beta_t}$.\n",
    "\n",
    "Using the reparameterization trick, we can jump directly to any timestep $t$:\n",
    "\n",
    "$$\\mathbf{v}_t = \\sqrt{\\bar{\\alpha}_t}\\,\\mathbf{v}_0 + \\sqrt{1-\\bar{\\alpha}_t}\\,\\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\mathbf{I})$$\n",
    "\n",
    "where $\\bar{\\alpha}_t = \\prod_{s=1}^{t}(1-\\beta_s)$.\n",
    "\n",
    "This is exactly what we want ‚Äî a single formula to noise a clean video to any level."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Noise Schedule\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_noise_schedule.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_noise_schedule"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Let's Build It ‚Äî Component by Component\n",
    "\n",
    "### 5.1 The Noise Schedule\n",
    "\n",
    "The noise schedule $\\{\\beta_t\\}$ controls how quickly we add noise. We will use a cosine schedule, which adds noise more gradually than a linear schedule."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_beta_schedule(num_timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    Cosine noise schedule (Nichol & Dhariwal, 2021).\n",
    "    Produces a smoother noise curve than a linear schedule.\n",
    "    \"\"\"\n",
    "    steps = torch.linspace(0, num_timesteps, num_timesteps + 1)\n",
    "    f = torch.cos((steps / num_timesteps + s) / (1 + s) * math.pi / 2) ** 2\n",
    "    alphas_cumprod = f / f[0]\n",
    "    betas = 1 - alphas_cumprod[1:] / alphas_cumprod[:-1]\n",
    "    return torch.clamp(betas, 0.0001, 0.999)\n",
    "\n",
    "\n",
    "NUM_DIFFUSION_STEPS = 200\n",
    "betas = cosine_beta_schedule(NUM_DIFFUSION_STEPS).to(device)\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
    "\n",
    "print(f\"Noise schedule: {NUM_DIFFUSION_STEPS} timesteps\")\n",
    "print(f\"  alpha_bar at t=0:   {alphas_cumprod[0]:.4f} (almost no noise)\")\n",
    "print(f\"  alpha_bar at t=100: {alphas_cumprod[100]:.4f} (moderate noise)\")\n",
    "print(f\"  alpha_bar at t=199: {alphas_cumprod[-1]:.4f} (almost pure noise)\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize the noise schedule\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "timesteps = range(NUM_DIFFUSION_STEPS)\n",
    "\n",
    "ax1.plot(timesteps, alphas_cumprod.cpu().numpy())\n",
    "ax1.set_xlabel(\"Diffusion Timestep t\")\n",
    "ax1.set_ylabel(\"·æ±_t (signal remaining)\")\n",
    "ax1.set_title(\"Cumulative Signal Retention\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(timesteps, sqrt_one_minus_alphas_cumprod.cpu().numpy())\n",
    "ax2.set_xlabel(\"Diffusion Timestep t\")\n",
    "ax2.set_ylabel(\"‚àö(1-·æ±_t) (noise level)\")\n",
    "ax2.set_title(\"Noise Level Over Time\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Forward Process\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_forward_process.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_forward_process"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 The Forward Noising Process\n",
    "\n",
    "Now let us implement the forward process. Given a clean video $\\mathbf{v}_0$ and a timestep $t$, we compute the noisy video $\\mathbf{v}_t$ and the noise $\\boldsymbol{\\epsilon}$ that was added."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion(v_0, t):\n",
    "    \"\"\"\n",
    "    Add noise to a clean video at timestep t.\n",
    "\n",
    "    Args:\n",
    "        v_0: clean video tensor, shape (B, T, C, H, W)\n",
    "        t: timestep tensor, shape (B,)\n",
    "\n",
    "    Returns:\n",
    "        v_t: noisy video at timestep t\n",
    "        noise: the noise that was added (our training target)\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(v_0)\n",
    "\n",
    "    # Gather the schedule values for each sample in the batch\n",
    "    sqrt_alpha = sqrt_alphas_cumprod[t].view(-1, 1, 1, 1, 1)\n",
    "    sqrt_one_minus_alpha = sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1, 1)\n",
    "\n",
    "    # v_t = sqrt(alpha_bar_t) * v_0 + sqrt(1 - alpha_bar_t) * noise\n",
    "    v_t = sqrt_alpha * v_0 + sqrt_one_minus_alpha * noise\n",
    "\n",
    "    return v_t, noise"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize the forward noising process on a single video\n",
    "sample_video = dataset[0:1].to(device)  # (1, 16, 1, 32, 32)\n",
    "\n",
    "fig, axes = plt.subplots(5, NUM_FRAMES, figsize=(20, 6))\n",
    "timesteps_to_show = [0, 25, 75, 150, 199]\n",
    "\n",
    "for row, ts in enumerate(timesteps_to_show):\n",
    "    t_tensor = torch.tensor([ts], device=device)\n",
    "    noisy, _ = forward_diffusion(sample_video, t_tensor)\n",
    "    for col in range(NUM_FRAMES):\n",
    "        axes[row, col].imshow(noisy[0, col, 0].cpu().numpy(), cmap=\"gray\")\n",
    "        axes[row, col].axis(\"off\")\n",
    "        if col == 0:\n",
    "            axes[row, col].set_ylabel(f\"t={ts}\", fontsize=10)\n",
    "\n",
    "fig.suptitle(\"Forward Diffusion: Clean Video ‚Üí Pure Noise\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Denoising Network\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_denoising_network.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_denoising_network"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly what we want. At $t=0$, the video is perfectly clean. As $t$ increases, the noise gradually destroys the signal. By $t=199$, the video is indistinguishable from random noise.\n",
    "\n",
    "Now we need a neural network that can **reverse** this process ‚Äî take a noisy video and predict the noise that was added.\n",
    "\n",
    "### 5.3 The Denoising Network: A Simple Video U-Net\n",
    "\n",
    "We will build a small 3D U-Net that processes the entire video tensor jointly. It takes the noisy video and the timestep as input, and outputs the predicted noise.\n",
    "\n",
    "The key idea: we use **3D convolutions** that operate across both space and time, so the network naturally learns spatial-temporal patterns."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    \"\"\"Encode the diffusion timestep as a vector.\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t.float().unsqueeze(1) * emb.unsqueeze(0)\n",
    "        return torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock3D(nn.Module):\n",
    "    \"\"\"Residual block with 3D convolutions + timestep conditioning.\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, time_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(in_ch, out_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv3d(out_ch, out_ch, 3, padding=1)\n",
    "        self.time_mlp = nn.Linear(time_dim, out_ch)\n",
    "        self.norm1 = nn.GroupNorm(8, out_ch)\n",
    "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
    "        self.skip = nn.Conv3d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        h = self.norm1(F.silu(self.conv1(x)))\n",
    "        # Add timestep information\n",
    "        t = self.time_mlp(F.silu(t_emb))\n",
    "        h = h + t.view(t.shape[0], t.shape[1], 1, 1, 1)\n",
    "        h = self.norm2(F.silu(self.conv2(h)))\n",
    "        return h + self.skip(x)"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVideoUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A small 3D U-Net for video noise prediction.\n",
    "\n",
    "    Takes: noisy video (B, T, 1, H, W) + timestep (B,)\n",
    "    Returns: predicted noise (B, T, 1, H, W)\n",
    "    \"\"\"\n",
    "    def __init__(self, channels=1, base_dim=32, time_dim=64):\n",
    "        super().__init__()\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalTimeEmbedding(time_dim),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        # Encoder: (B, 1, T, H, W) ‚Üí (B, 32, T, H, W) ‚Üí (B, 64, T/2, H/2, W/2)\n",
    "        self.enc1 = ResBlock3D(channels, base_dim, time_dim)\n",
    "        self.enc2 = ResBlock3D(base_dim, base_dim * 2, time_dim)\n",
    "        self.down = nn.Conv3d(base_dim, base_dim, 3, stride=2, padding=1)\n",
    "        self.down2 = nn.Conv3d(base_dim * 2, base_dim * 2, 3, stride=2, padding=1)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.mid = ResBlock3D(base_dim * 2, base_dim * 2, time_dim)\n",
    "\n",
    "        # Decoder: mirror of encoder with skip connections\n",
    "        self.up2 = nn.ConvTranspose3d(base_dim * 2, base_dim * 2,\n",
    "                                       4, stride=2, padding=1)\n",
    "        self.dec2 = ResBlock3D(base_dim * 4, base_dim, time_dim)\n",
    "        self.up1 = nn.ConvTranspose3d(base_dim, base_dim,\n",
    "                                       4, stride=2, padding=1)\n",
    "        self.dec1 = ResBlock3D(base_dim * 2, base_dim, time_dim)\n",
    "\n",
    "        self.out_conv = nn.Conv3d(base_dim, channels, 1)\n",
    "\n",
    "    def forward(self, v, t):\n",
    "        # v: (B, T, C, H, W) ‚Üí rearrange to (B, C, T, H, W) for Conv3d\n",
    "        B, T, C, H, W = v.shape\n",
    "        x = v.permute(0, 2, 1, 3, 4)  # (B, C, T, H, W)\n",
    "\n",
    "        t_emb = self.time_embed(t)\n",
    "\n",
    "        # Encoder\n",
    "        h1 = self.enc1(x, t_emb)       # (B, 32, T, H, W)\n",
    "        h1_down = self.down(h1)          # (B, 32, T/2, H/2, W/2)\n",
    "        h2 = self.enc2(h1_down, t_emb)  # (B, 64, T/2, H/2, W/2)\n",
    "        h2_down = self.down2(h2)         # (B, 64, T/4, H/4, W/4)\n",
    "\n",
    "        # Bottleneck\n",
    "        mid = self.mid(h2_down, t_emb)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        up2 = self.up2(mid)              # (B, 64, T/2, H/2, W/2)\n",
    "        up2 = torch.cat([up2, h2], dim=1)  # (B, 128, ...)\n",
    "        d2 = self.dec2(up2, t_emb)       # (B, 32, T/2, H/2, W/2)\n",
    "\n",
    "        up1 = self.up1(d2)              # (B, 32, T, H, W)\n",
    "        up1 = torch.cat([up1, h1], dim=1)  # (B, 64, ...)\n",
    "        d1 = self.dec1(up1, t_emb)       # (B, 32, T, H, W)\n",
    "\n",
    "        out = self.out_conv(d1)          # (B, 1, T, H, W)\n",
    "        return out.permute(0, 2, 1, 3, 4)  # (B, T, 1, H, W)\n",
    "\n",
    "\n",
    "model = SimpleVideoUNet(channels=1, base_dim=32, time_dim=64).to(device)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "\n",
    "# Quick shape test\n",
    "test_v = torch.randn(2, NUM_FRAMES, 1, CANVAS_SIZE, CANVAS_SIZE, device=device)\n",
    "test_t = torch.randint(0, NUM_DIFFUSION_STEPS, (2,), device=device)\n",
    "test_out = model(test_v, test_t)\n",
    "print(f\"Input shape:  {test_v.shape}\")\n",
    "print(f\"Output shape: {test_out.shape}\")\n",
    "assert test_v.shape == test_out.shape, \"Shape mismatch!\"\n",
    "print(\"‚úÖ Model works correctly!\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_todo_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_todo_training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üîß Your Turn ‚Äî Implement the Training Loop\n",
    "\n",
    "Now it is your turn. The training objective is simple: predict the noise that was added.\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}_{\\mathbf{v}_0, \\boldsymbol{\\epsilon}, t}\\left[\\|\\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{v}_t, t)\\|^2\\right]$$\n",
    "\n",
    "Computationally: sample a clean video, sample a random timestep, add noise, predict the noise, and minimize the MSE between predicted and actual noise."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_step(model, optimizer, batch):\n",
    "    \"\"\"\n",
    "    Perform one training step of video diffusion.\n",
    "\n",
    "    Args:\n",
    "        model: the denoising network\n",
    "        optimizer: the optimizer\n",
    "        batch: clean video batch, shape (B, T, C, H, W)\n",
    "\n",
    "    Returns:\n",
    "        loss value (float)\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Sample random timesteps for each video in the batch\n",
    "    #         (integers from 0 to NUM_DIFFUSION_STEPS-1)\n",
    "    # Step 2: Use forward_diffusion() to get noisy videos and target noise\n",
    "    # Step 3: Predict the noise using the model\n",
    "    # Step 4: Compute MSE loss between predicted and actual noise\n",
    "    # Step 5: Backpropagate and update weights\n",
    "    # ==============================\n",
    "\n",
    "    t = ???  # YOUR CODE HERE ‚Äî random timesteps, shape (B,)\n",
    "    noisy_video, noise = ???  # YOUR CODE HERE ‚Äî forward diffusion\n",
    "    predicted_noise = ???  # YOUR CODE HERE ‚Äî model prediction\n",
    "\n",
    "    loss = ???  # YOUR CODE HERE ‚Äî MSE loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification ‚Äî check your implementation\n",
    "# We'll run one step and verify the loss is a reasonable number\n",
    "test_model = SimpleVideoUNet(channels=1, base_dim=32, time_dim=64).to(device)\n",
    "test_opt = torch.optim.Adam(test_model.parameters(), lr=1e-3)\n",
    "test_batch = dataset[:4].to(device)\n",
    "\n",
    "try:\n",
    "    test_loss = train_one_step(test_model, test_opt, test_batch)\n",
    "    assert isinstance(test_loss, float), \"Loss should be a float\"\n",
    "    assert 0 < test_loss < 10, f\"Loss {test_loss:.4f} seems wrong (expected 0-10 range)\"\n",
    "    print(f\"‚úÖ Training step works! Loss = {test_loss:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"\\nHint: t should be torch.randint(0, NUM_DIFFUSION_STEPS, (B,), device=device)\")\n",
    "    print(\"Hint: Use F.mse_loss(predicted_noise, noise)\")\n",
    "\n",
    "del test_model, test_opt"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training the Model\n",
    "\n",
    "Now let us train our video diffusion model. With our small dataset and model, this should take about 5 minutes on a T4 GPU."
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "# Create data loader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True\n",
    ")\n",
    "\n",
    "losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_losses = []\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        loss = train_one_step(model, optimizer, batch)\n",
    "        epoch_losses.append(loss)\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    losses.append(avg_loss)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} ‚Äî Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Training curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Training Loss ‚Äî Video Diffusion Model\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo Sampling\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_todo_sampling.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_11_todo_sampling"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sampling ‚Äî Generating Videos from Noise\n",
    "\n",
    "Now for the exciting part: generating videos from pure noise using the trained model!\n",
    "\n",
    "The sampling process reverses the forward diffusion. Starting from pure noise $\\mathbf{v}_T \\sim \\mathcal{N}(0, \\mathbf{I})$, we iteratively denoise using our trained model:\n",
    "\n",
    "$$\\mathbf{v}_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\left(\\mathbf{v}_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\,\\boldsymbol{\\epsilon}_\\theta(\\mathbf{v}_t, t)\\right) + \\sigma_t \\mathbf{z}$$\n",
    "\n",
    "where $\\mathbf{z} \\sim \\mathcal{N}(0, \\mathbf{I})$ and $\\sigma_t = \\sqrt{\\beta_t}$.\n",
    "\n",
    "### üîß TODO: Implement the DDPM Sampling Step\n",
    "\n",
    "The sampling process reverses the forward diffusion using the formula above. Complete the core reverse step inside the loop:"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_videos(model, num_videos=4, num_frames=16, size=32):\n",
    "    \"\"\"\n",
    "    Generate videos by iteratively denoising from pure noise.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    shape = (num_videos, num_frames, 1, size, size)\n",
    "\n",
    "    # Start from pure noise\n",
    "    v = torch.randn(shape, device=device)\n",
    "\n",
    "    # Store intermediate steps for visualization\n",
    "    intermediates = [v.cpu().clone()]\n",
    "\n",
    "    for t in reversed(range(NUM_DIFFUSION_STEPS)):\n",
    "        t_batch = torch.full((num_videos,), t, device=device, dtype=torch.long)\n",
    "\n",
    "        # ============ TODO ============\n",
    "        # Step 1: Use the model to predict the noise: predicted_noise = model(v, t_batch)\n",
    "        # Step 2: Get alpha_t, alpha_bar_t, and beta_t from the schedule\n",
    "        # Step 3: Compute the denoised mean using the DDPM formula:\n",
    "        #         v = (1/sqrt(alpha_t)) * (v - (beta_t/sqrt(1-alpha_bar_t)) * predicted_noise)\n",
    "        # Step 4: If t > 0, add noise: v = v + sqrt(beta_t) * random_noise\n",
    "        # ==============================\n",
    "\n",
    "        predicted_noise = ???  # YOUR CODE HERE (Step 1)\n",
    "        alpha_t = ???  # YOUR CODE HERE (Step 2)\n",
    "        alpha_bar_t = ???  # YOUR CODE HERE (Step 2)\n",
    "        beta_t = ???  # YOUR CODE HERE (Step 2)\n",
    "\n",
    "        v = ???  # YOUR CODE HERE (Step 3 ‚Äî DDPM mean formula)\n",
    "\n",
    "        if t > 0:\n",
    "            v = ???  # YOUR CODE HERE (Step 4 ‚Äî add noise)\n",
    "\n",
    "        # Save snapshots\n",
    "        if t % 40 == 0 or t == 0:\n",
    "            intermediates.append(v.cpu().clone())\n",
    "\n",
    "    model.train()\n",
    "    return v.cpu(), intermediates"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification ‚Äî test sampling\n",
    "try:\n",
    "    test_gen, test_inters = sample_videos(model, num_videos=1, num_frames=NUM_FRAMES, size=CANVAS_SIZE)\n",
    "    assert test_gen.shape == (1, NUM_FRAMES, 1, CANVAS_SIZE, CANVAS_SIZE), f\"Wrong shape: {test_gen.shape}\"\n",
    "    assert len(test_inters) > 1, \"Should have intermediate snapshots\"\n",
    "    print(f\"‚úÖ Sampling works! Generated shape: {test_gen.shape}\")\n",
    "    print(f\"   Saved {len(test_inters)} intermediate snapshots\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Hint: alpha_t = alphas[t], alpha_bar_t = alphas_cumprod[t], beta_t = betas[t]\")\n",
    "    print(\"Hint: For Step 4, use torch.randn_like(v)\")"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Results\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_results.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_12_results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Generate videos!\n",
    "print(\"Generating videos from pure noise...\")\n",
    "generated_videos, intermediates = sample_videos(model, num_videos=8)\n",
    "print(f\"Generated {generated_videos.shape[0]} videos!\")\n",
    "print(f\"Shape: {generated_videos.shape}\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize the denoising process\n",
    "fig, axes = plt.subplots(len(intermediates), NUM_FRAMES, figsize=(20, 2 * len(intermediates)))\n",
    "titles = [f\"t={NUM_DIFFUSION_STEPS}\"] + [f\"t={t}\" for t in range(NUM_DIFFUSION_STEPS-1, -1, -40)][:-1] + [\"t=0\"]\n",
    "\n",
    "for row, (snap, title) in enumerate(zip(intermediates, titles)):\n",
    "    for col in range(NUM_FRAMES):\n",
    "        axes[row, col].imshow(snap[0, col, 0].clamp(0, 1).numpy(),\n",
    "                              cmap=\"gray\", vmin=0, vmax=1)\n",
    "        axes[row, col].axis(\"off\")\n",
    "        if col == 0:\n",
    "            axes[row, col].set_ylabel(title, fontsize=10)\n",
    "\n",
    "fig.suptitle(\"Reverse Diffusion: Noise ‚Üí Video\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Final Gallery\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_final_gallery.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_13_final_gallery"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üéØ Final Output ‚Äî Generated Video Gallery"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display our generated videos as animated GIFs\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(8):\n",
    "    # Show all frames as a montage\n",
    "    frames = generated_videos[i, :, 0].clamp(0, 1).numpy()\n",
    "    # Create a 4x4 grid of frames\n",
    "    montage = np.zeros((4 * CANVAS_SIZE, 4 * CANVAS_SIZE))\n",
    "    for idx in range(min(16, NUM_FRAMES)):\n",
    "        r, c = idx // 4, idx % 4\n",
    "        montage[r*CANVAS_SIZE:(r+1)*CANVAS_SIZE,\n",
    "                c*CANVAS_SIZE:(c+1)*CANVAS_SIZE] = frames[idx]\n",
    "    axes[i].imshow(montage, cmap=\"gray\", vmin=0, vmax=1)\n",
    "    axes[i].set_title(f\"Video {i+1}\", fontsize=11)\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "fig.suptitle(\"üéâ Generated Videos ‚Äî Each 4√ó4 Grid Shows 16 Frames\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéâ Congratulations! You've trained a video diffusion model from scratch!\")\n",
    "print(\"   Each grid above shows a 16-frame video of a moving digit,\")\n",
    "print(\"   generated entirely from random noise.\")"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Reflection\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_reflection.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_14_reflection"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Reflection and Next Steps\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "\n",
    "1. **Why 3D convolutions?** Our model used 3D convolutions that process space and time jointly. What are the advantages and disadvantages of this approach? Would it be cheaper to process spatial and temporal dimensions separately?\n",
    "\n",
    "2. **Temporal coherence:** Look at the generated videos. Are the digits moving smoothly? What would improve temporal consistency?\n",
    "\n",
    "3. **Scaling up:** Our model had ~100K parameters and operated on 32√ó32 grayscale videos. What challenges would we face scaling to 256√ó256 RGB videos at 30fps?\n",
    "\n",
    "### üèÜ Optional Challenges\n",
    "\n",
    "1. **Different motion:** Modify `create_moving_mnist_video` to add rotation or scaling ‚Äî not just translation. How does this affect generation quality?\n",
    "\n",
    "2. **Longer videos:** Try generating 32-frame videos instead of 16. Does quality degrade?\n",
    "\n",
    "3. **Conditional generation:** Add the digit class (0-9) as a conditioning signal. Can the model learn to generate specific digits?\n",
    "\n",
    "**Next notebook:** We will build the **factorized spatial-temporal attention** mechanism that makes modern video diffusion models so much more powerful and efficient than simple 3D convolutions."
   ],
   "id": "cell_33"
  }
 ]
}