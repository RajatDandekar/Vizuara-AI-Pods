{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Dataset and DataLoader â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_04_code_sliding_window",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Code Sliding Window\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_04_code_sliding_window.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_01_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction: Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_01_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and DataLoader -- From Tokens to Training Batches -- Vizuara\n",
    "\n",
    "> **What you will build:** A PyTorch Dataset and DataLoader that converts raw token sequences into shuffled, batched input-target pairs ready for language model training."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_02_why_matter",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why Matter\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_02_why_matter.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "The tokenizer gave us a sequence of numbers. But a language model does not train on a single long sequence -- it needs structured **input-target pairs** organized into **batches**. This is where the Dataset and DataLoader come in.\n",
    "\n",
    "The Dataset creates training pairs using a sliding window: \"given these tokens, predict the next one.\" The DataLoader then organizes these pairs into efficient batches, shuffles them, and streams them to the GPU.\n",
    "\n",
    "Together, they form the conveyor belt of our training factory -- without them, the model has nothing to learn from."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_03_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_03_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Let us start with the core idea. A language model learns by predicting the next token. Given the sequence `[The, cat, sat, on, the, mat]`, we create multiple training examples:\n",
    "\n",
    "- **Input:** `[The, cat, sat, on]` --> **Target:** `[cat, sat, on, the]`\n",
    "- **Input:** `[cat, sat, on, the]` --> **Target:** `[sat, on, the, mat]`\n",
    "\n",
    "Notice: the target is just the input shifted by one position. Each position in the input is trained to predict the next token."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simple demonstration of the sliding window\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "token_ids = list(range(len(tokens)))  # [0, 1, 2, 3, 4, 5]\n",
    "context_length = 4\n",
    "\n",
    "print(\"Sliding Window Examples:\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(len(tokens) - context_length):\n",
    "    input_tokens = tokens[i:i + context_length]\n",
    "    target_tokens = tokens[i + 1:i + context_length + 1]\n",
    "    print(f\"  Input:  {input_tokens}\")\n",
    "    print(f\"  Target: {target_tokens}\")\n",
    "    print(f\"  (Each input token predicts the token directly to its right)\")\n",
    "    print()\n",
    "\n",
    "print(f\"Total training samples: {len(tokens) - context_length}\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_05_viz_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Viz Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_05_viz_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the sliding window mechanism."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_06_viz_sliding_window",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Viz Sliding Window\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_06_viz_sliding_window.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Draw the full token sequence\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(tokens)))\n",
    "for i, (tok, color) in enumerate(zip(tokens, colors)):\n",
    "    rect = plt.Rectangle((i * 1.5, 4), 1.2, 0.8, facecolor=color, edgecolor='black', linewidth=1.5)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(i * 1.5 + 0.6, 4.4, tok, ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(i * 1.5 + 0.6, 3.5, f'[{i}]', ha='center', va='center', fontsize=10, color='gray')\n",
    "\n",
    "ax.text(-0.5, 4.4, 'Full sequence:', ha='right', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Draw sliding windows\n",
    "for win_idx in range(len(tokens) - context_length):\n",
    "    y_offset = 2.2 - win_idx * 1.8\n",
    "\n",
    "    # Input window (blue)\n",
    "    for j in range(context_length):\n",
    "        pos = win_idx + j\n",
    "        rect = plt.Rectangle((pos * 1.5, y_offset), 1.2, 0.8,\n",
    "                             facecolor='#3498db', alpha=0.3, edgecolor='#2980b9', linewidth=1.5)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(pos * 1.5 + 0.6, y_offset + 0.4, tokens[pos],\n",
    "                ha='center', va='center', fontsize=10)\n",
    "\n",
    "    # Target tokens (orange arrows)\n",
    "    for j in range(context_length):\n",
    "        pos = win_idx + j\n",
    "        target_pos = pos + 1\n",
    "        ax.annotate('', xy=(target_pos * 1.5 + 0.6, y_offset - 0.1),\n",
    "                    xytext=(pos * 1.5 + 0.6, y_offset),\n",
    "                    arrowprops=dict(arrowstyle='->', color='#e74c3c', lw=1.5))\n",
    "\n",
    "    ax.text(-0.5, y_offset + 0.4, f'Window {win_idx+1}:', ha='right', va='center', fontsize=10)\n",
    "\n",
    "ax.set_xlim(-2, len(tokens) * 1.5 + 0.5)\n",
    "ax.set_ylim(-1, 5.5)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Sliding Window: Input (blue) predicts next token (red arrows)', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sliding_window.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Checkpoint: Each window creates a training sample where every position predicts the next token.\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_07_math_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Math Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_07_math_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Next-Token Prediction\n",
    "\n",
    "For a sequence of tokens $x_1, x_2, \\ldots, x_T$, the language model learns to maximize:\n",
    "\n",
    "$$P(x_1, x_2, \\ldots, x_T) = \\prod_{t=1}^{T} P(x_t \\mid x_1, \\ldots, x_{t-1})$$\n",
    "\n",
    "The training loss (cross-entropy) for a single sequence of length $T$ is:\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{T} \\sum_{t=1}^{T} \\log P(x_t \\mid x_1, \\ldots, x_{t-1})$$\n",
    "\n",
    "### Context Length and Memory\n",
    "\n",
    "The context length $T$ determines how many tokens the model can see. The self-attention mechanism scales quadratically:\n",
    "\n",
    "$$\\text{Attention Memory} \\propto T^2$$\n",
    "\n",
    "Doubling $T$ from 1,024 to 2,048 means $4\\times$ the attention memory.\n",
    "\n",
    "### Batch Size and Gradient Variance\n",
    "\n",
    "With batch size $B$, the variance of the gradient estimate decreases as:\n",
    "\n",
    "$$\\text{Var}[\\hat{g}_B] = \\frac{\\text{Var}[\\hat{g}_1]}{B}$$\n",
    "\n",
    "So a batch of 32 has $32\\times$ less gradient variance than a single sample."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_08_code_quadratic_scaling",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Code Quadratic Scaling\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_08_code_quadratic_scaling.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the quadratic scaling of attention memory\n",
    "context_lengths = [128, 256, 512, 1024, 2048, 4096, 8192]\n",
    "relative_memory = [(T / 128) ** 2 for T in context_lengths]\n",
    "\n",
    "print(\"Context Length vs Attention Memory:\")\n",
    "print(\"-\" * 40)\n",
    "for T, mem in zip(context_lengths, relative_memory):\n",
    "    bar = \"=\" * int(min(mem, 50))\n",
    "    print(f\"  T = {T:>5,d}  ->  {mem:>7.0f}x  {bar}\")\n",
    "\n",
    "print(\"\\nDoubling context length -> 4x memory!\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_09_transition_build",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Transition Build\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_09_transition_build.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 The TextDataset Class"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_10_code_textdataset",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Code Textdataset\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_10_code_textdataset.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for language model training.\n",
    "    Creates input-target pairs using a sliding window over token sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokens, context_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tokens: List or tensor of token IDs\n",
    "            context_length: Number of tokens in each training sample\n",
    "        \"\"\"\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of training samples we can extract.\"\"\"\n",
    "        return len(self.tokens) - self.context_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single training sample.\n",
    "        Input: tokens[idx : idx + context_length]\n",
    "        Target: tokens[idx + 1 : idx + context_length + 1]\n",
    "        \"\"\"\n",
    "        x = self.tokens[idx : idx + self.context_length]\n",
    "        y = self.tokens[idx + 1 : idx + self.context_length + 1]\n",
    "        return x, y\n",
    "\n",
    "# Test with a small example\n",
    "sample_tokens = list(range(20))  # [0, 1, 2, ..., 19]\n",
    "dataset = TextDataset(sample_tokens, context_length=4)\n",
    "\n",
    "print(f\"Token sequence: {sample_tokens}\")\n",
    "print(f\"Context length: 4\")\n",
    "print(f\"Number of samples: {len(dataset)}\")\n",
    "print()\n",
    "\n",
    "for i in range(min(5, len(dataset))):\n",
    "    x, y = dataset[i]\n",
    "    print(f\"  Sample {i}: Input={x.tolist()}  Target={y.tolist()}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_11_explanation_batching",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Explanation Batching\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_11_explanation_batching.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Understanding Batching\n",
    "\n",
    "Why can we not feed one sample at a time? Because single-sample gradients are extremely noisy. Batching averages the loss across multiple samples, producing smoother, more reliable gradients."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_12_viz_batch_noise",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Viz Batch Noise\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_12_viz_batch_noise.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate gradient noise at different batch sizes\n",
    "np.random.seed(42)\n",
    "true_gradient = 0.5\n",
    "num_experiments = 200\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "batch_sizes = [1, 8, 32]\n",
    "\n",
    "for ax, B in zip(axes, batch_sizes):\n",
    "    # Simulate gradient estimates at this batch size\n",
    "    gradient_estimates = [\n",
    "        np.mean(np.random.normal(true_gradient, 2.0, size=B))\n",
    "        for _ in range(num_experiments)\n",
    "    ]\n",
    "\n",
    "    ax.hist(gradient_estimates, bins=30, color='#3498db', edgecolor='black',\n",
    "            linewidth=0.5, alpha=0.7, density=True)\n",
    "    ax.axvline(true_gradient, color='red', linewidth=2, linestyle='--', label=f'True gradient = {true_gradient}')\n",
    "    ax.set_title(f'Batch Size = {B}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Gradient Estimate', fontsize=10)\n",
    "    ax.set_ylabel('Density', fontsize=10)\n",
    "    variance = np.var(gradient_estimates)\n",
    "    ax.text(0.05, 0.95, f'Var = {variance:.3f}', transform=ax.transAxes,\n",
    "            fontsize=10, verticalalignment='top', fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.set_xlim(-4, 5)\n",
    "\n",
    "plt.suptitle('Larger Batches Reduce Gradient Noise', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('batch_gradient_noise.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Checkpoint: Batch size 32 has ~32x less variance than batch size 1.\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_13_explanation_dataloader",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Explanation Dataloader\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_13_explanation_dataloader.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The DataLoader"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_14_code_dataloader",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Code Dataloader\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_14_code_dataloader.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader with batching and shuffling\n",
    "dataset = TextDataset(list(range(100)), context_length=8)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,       # 4 samples per batch\n",
    "    shuffle=True,       # Randomize order each epoch\n",
    "    drop_last=True,     # Drop incomplete final batch\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} samples\")\n",
    "print(f\"Batch size: 4\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "print(f\"Samples used per epoch: {len(dataloader) * 4}\")\n",
    "print(f\"Dropped samples: {len(dataset) - len(dataloader) * 4}\")\n",
    "\n",
    "print(\"\\nFirst 3 batches:\")\n",
    "for batch_idx, (batch_x, batch_y) in enumerate(dataloader):\n",
    "    if batch_idx >= 3:\n",
    "        break\n",
    "    print(f\"\\n  Batch {batch_idx + 1}: shape = {batch_x.shape}\")\n",
    "    for i in range(len(batch_x)):\n",
    "        print(f\"    Sample {i}: Input={batch_x[i].tolist()[:4]}...  Target={batch_y[i].tolist()[:4]}...\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_15_explanation_shuffling",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Explanation Shuffling\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_15_explanation_shuffling.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Why Shuffling Matters"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_16_code_shuffling",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Code Shuffling\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_16_code_shuffling.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the effect of shuffling\n",
    "dataset = TextDataset(list(range(50)), context_length=4)\n",
    "\n",
    "# Without shuffling\n",
    "no_shuffle_loader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "print(\"WITHOUT shuffling (first batch each epoch):\")\n",
    "for epoch in range(3):\n",
    "    first_batch = next(iter(no_shuffle_loader))\n",
    "    print(f\"  Epoch {epoch+1}: First input = {first_batch[0][0].tolist()}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# With shuffling\n",
    "shuffle_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "print(\"WITH shuffling (first batch each epoch):\")\n",
    "for epoch in range(3):\n",
    "    first_batch = next(iter(shuffle_loader))\n",
    "    print(f\"  Epoch {epoch+1}: First input = {first_batch[0][0].tolist()}\")\n",
    "\n",
    "print(\"\\nShuffling ensures different batch composition each epoch!\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_17_todo1_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo1 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_17_todo1_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Implement a Streaming Dataset\n",
    "\n",
    "Real LLM training data is too large to fit in memory. Implement a dataset that reads tokens from a file lazily."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_18_todo1_streaming_dataset",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo1 Streaming Dataset\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_18_todo1_streaming_dataset.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset that reads tokens from a memory-mapped file\n",
    "    instead of loading everything into RAM.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, token_file_path, context_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_file_path: Path to a file containing token IDs (one per line or binary)\n",
    "            context_length: Number of tokens per sample\n",
    "        \"\"\"\n",
    "        self.context_length = context_length\n",
    "\n",
    "        # TODO: Load token IDs -- for this exercise, simulate with a list\n",
    "        # In production, you would use np.memmap or similar\n",
    "        self.tokens = None  # YOUR CODE HERE: load tokens into a tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO: Return the number of valid samples\n",
    "        pass  # YOUR CODE HERE\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: Return (input, target) pair using sliding window\n",
    "        pass  # YOUR CODE HERE\n",
    "\n",
    "# SOLUTION (uncomment to verify):\n",
    "# class StreamingTextDataset(Dataset):\n",
    "#     def __init__(self, token_file_path, context_length):\n",
    "#         self.context_length = context_length\n",
    "#         # Simulate loading -- in production use np.memmap\n",
    "#         self.tokens = torch.arange(10000, dtype=torch.long)\n",
    "#     def __len__(self):\n",
    "#         return len(self.tokens) - self.context_length\n",
    "#     def __getitem__(self, idx):\n",
    "#         x = self.tokens[idx : idx + self.context_length]\n",
    "#         y = self.tokens[idx + 1 : idx + self.context_length + 1]\n",
    "#         return x, y"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_19_todo2_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo2 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_19_todo2_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement Gradient Accumulation\n",
    "\n",
    "When GPU memory is limited, we cannot use large batch sizes directly. Gradient accumulation simulates a larger batch by accumulating gradients over multiple forward-backward passes."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_20_todo2_gradient_accumulation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo2 Gradient Accumulation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_20_todo2_gradient_accumulation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_gradient_accumulation(model, dataloader, optimizer, accumulation_steps=4):\n",
    "    \"\"\"\n",
    "    Train with gradient accumulation.\n",
    "    Effective batch size = dataloader.batch_size * accumulation_steps\n",
    "\n",
    "    Args:\n",
    "        model: The neural network\n",
    "        dataloader: DataLoader with small batch size\n",
    "        optimizer: The optimizer\n",
    "        accumulation_steps: Number of mini-batches to accumulate before stepping\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, (batch_x, batch_y) in enumerate(dataloader):\n",
    "        # TODO: Forward pass -- compute loss\n",
    "        # loss = ...  # YOUR CODE HERE\n",
    "\n",
    "        # TODO: Scale loss by accumulation_steps (so gradients average correctly)\n",
    "        # scaled_loss = ...  # YOUR CODE HERE\n",
    "        # scaled_loss.backward()\n",
    "\n",
    "        # TODO: Only step the optimizer every accumulation_steps mini-batches\n",
    "        # if (step + 1) % accumulation_steps == 0:\n",
    "        #     optimizer.step()\n",
    "        #     optimizer.zero_grad()\n",
    "\n",
    "        # total_loss += loss.item()\n",
    "        pass\n",
    "\n",
    "    # SOLUTION (uncomment to verify):\n",
    "    # for step, (batch_x, batch_y) in enumerate(dataloader):\n",
    "    #     logits = model(batch_x)\n",
    "    #     loss = F.cross_entropy(logits.view(-1, logits.size(-1)), batch_y.view(-1))\n",
    "    #     scaled_loss = loss / accumulation_steps\n",
    "    #     scaled_loss.backward()\n",
    "    #     if (step + 1) % accumulation_steps == 0:\n",
    "    #         optimizer.step()\n",
    "    #         optimizer.zero_grad()\n",
    "    #     total_loss += loss.item()\n",
    "\n",
    "    print(\"Gradient accumulation simulates larger batch sizes without more memory!\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_21_transition_full_pipeline",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Transition Full Pipeline\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_21_transition_full_pipeline.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us build a complete data pipeline from raw text to training-ready batches."
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_22_code_full_pipeline",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Code Full Pipeline\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_22_code_full_pipeline.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.check_call(['pip', 'install', '-q', 'tiktoken'])\n",
    "import tiktoken\n",
    "\n",
    "# Step 1: Raw text\n",
    "raw_text = \"\"\"\n",
    "The transformer architecture has revolutionized natural language processing.\n",
    "Self-attention allows the model to weigh the importance of different tokens.\n",
    "Training a language model requires tokenization, batching, and optimization.\n",
    "The data loader feeds organized batches to the GPU for efficient training.\n",
    "Each training step computes loss, backpropagates, and updates weights.\n",
    "\"\"\" * 50  # Repeat to get more data\n",
    "\n",
    "# Step 2: Tokenize\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens = enc.encode(raw_text)\n",
    "print(f\"Raw text: {len(raw_text)} characters\")\n",
    "print(f\"Tokens: {len(tokens)} token IDs\")\n",
    "print(f\"Compression ratio: {len(raw_text) / len(tokens):.1f} chars/token\")\n",
    "\n",
    "# Step 3: Create dataset\n",
    "CONTEXT_LENGTH = 32\n",
    "dataset = TextDataset(tokens, context_length=CONTEXT_LENGTH)\n",
    "print(f\"\\nDataset: {len(dataset)} training samples\")\n",
    "print(f\"Context length: {CONTEXT_LENGTH}\")\n",
    "\n",
    "# Step 4: Create dataloader\n",
    "BATCH_SIZE = 8\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "print(f\"Batches per epoch: {len(dataloader)}\")\n",
    "print(f\"Effective training tokens per epoch: {len(dataloader) * BATCH_SIZE * CONTEXT_LENGTH:,}\")\n",
    "\n",
    "# Step 5: Verify shapes\n",
    "batch_x, batch_y = next(iter(dataloader))\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"  Input:  {batch_x.shape}  (batch_size x context_length)\")\n",
    "print(f\"  Target: {batch_y.shape}  (batch_size x context_length)\")\n",
    "\n",
    "# Decode one sample to verify correctness\n",
    "sample_input = enc.decode(batch_x[0].tolist())\n",
    "sample_target = enc.decode(batch_y[0].tolist())\n",
    "print(f\"\\nSample input:  '{sample_input[:60]}...'\")\n",
    "print(f\"Sample target: '{sample_target[:60]}...'\")\n",
    "print(\"\\nTarget is input shifted right by 1 token -- correct!\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_23_transition_benchmark",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Transition Benchmark\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_23_transition_benchmark.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "Let us measure the throughput of our data pipeline -- how fast can it feed data?"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_24_code_benchmark",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Code Benchmark\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_24_code_benchmark.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Benchmark data loading speed\n",
    "dataset_large = TextDataset(list(range(100000)), context_length=128)\n",
    "\n",
    "for batch_size in [8, 32, 128]:\n",
    "    loader = DataLoader(dataset_large, batch_size=batch_size, shuffle=True,\n",
    "                        drop_last=True, num_workers=0)\n",
    "\n",
    "    start = time.time()\n",
    "    tokens_loaded = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        tokens_loaded += batch_x.numel()\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    print(f\"Batch size {batch_size:>4d}:  {tokens_loaded:>10,d} tokens in {elapsed:.3f}s  \"\n",
    "          f\"({tokens_loaded / elapsed / 1e6:.1f}M tokens/sec)\")\n",
    "\n",
    "print(\"\\nThe data pipeline must be fast enough that the GPU never waits for data!\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_25_final_output_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Final Output Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_25_final_output_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_26_viz_pipeline_summary",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Viz Pipeline Summary\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_26_viz_pipeline_summary.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary visualization of the complete data pipeline\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 3))\n",
    "\n",
    "stages = [\n",
    "    ('Raw Text', 'characters\\n\"The cat sat...\"', '#e8f4f8'),\n",
    "    ('Tokenizer', 'token IDs\\n[464, 3797, 3332]', '#fde8e8'),\n",
    "    ('Dataset', 'sliding window\\nInput -> Target', '#e8fde8'),\n",
    "    ('DataLoader', 'shuffled batches\\nShape: (B, T)', '#fdf8e8'),\n",
    "]\n",
    "\n",
    "for ax, (title, desc, color) in zip(axes, stages):\n",
    "    rect = plt.Rectangle((0.05, 0.1), 0.9, 0.8, facecolor=color,\n",
    "                         edgecolor='black', linewidth=2, transform=ax.transAxes)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(0.5, 0.7, title, ha='center', va='center', fontsize=13,\n",
    "            fontweight='bold', transform=ax.transAxes)\n",
    "    ax.text(0.5, 0.35, desc, ha='center', va='center', fontsize=9,\n",
    "            transform=ax.transAxes, style='italic')\n",
    "    ax.axis('off')\n",
    "\n",
    "# Add arrows between stages\n",
    "for i in range(3):\n",
    "    fig.text(0.25 * (i + 1) + 0.02, 0.5, '>>>', ha='center', va='center',\n",
    "             fontsize=16, fontweight='bold', color='#e74c3c')\n",
    "\n",
    "plt.suptitle('The Data Pipeline: Raw Text to Training Batches', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('data_pipeline.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Data pipeline complete! Ready to feed the optimizer.\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_27_closing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Wrap-Up: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_27_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**What we built:** A complete data pipeline -- from raw text through tokenization, dataset creation, and batch loading.\n",
    "\n",
    "**Key takeaways:**\n",
    "- The Dataset implements a sliding window for next-token prediction\n",
    "- Target = Input shifted right by 1 token\n",
    "- Larger batches reduce gradient variance (smoother training)\n",
    "- Shuffling prevents order-dependent memorization\n",
    "- Context length trades off memory (quadratic in attention) for long-range understanding\n",
    "\n",
    "**What is next:** In Notebook 3, we will build the optimizers -- SGD, Momentum, and Adam -- that use these batches to update model weights."
   ],
   "id": "cell_27"
  }
 ]
}