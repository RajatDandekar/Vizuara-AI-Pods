{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "The Complete Training Loop \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Complete Training Loop -- From Raw Text to a Trained Language Model -- Vizuara\n",
    "\n",
    "> **What you will build:** A complete, end-to-end training pipeline that takes raw text and produces a language model that can generate coherent text. Every component from the previous notebooks comes together here."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "This is the culmination of everything we have built. In Notebooks 1-4, we developed each component of the training pipeline individually:\n",
    "\n",
    "1. **Tokenizer** -- converting text to numbers (BPE)\n",
    "2. **Dataset/DataLoader** -- creating batched training samples\n",
    "3. **Optimizer** -- updating weights intelligently (Adam)\n",
    "4. **Scheduler + Clipping** -- stabilizing training\n",
    "\n",
    "Now we assemble all of them into a single, working training loop and actually train a language model. By the end of this notebook, you will have a model that generates text -- trained entirely from scratch.\n",
    "\n",
    "The training loop pattern we build here is the same one used by GPT, LLaMA, Mistral, and every modern LLM. Only the scale differs."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "The training loop is a cycle that repeats thousands of times:\n",
    "\n",
    "1. **Tokenize** raw text once (preprocessing)\n",
    "2. **Load** a batch of input-target pairs\n",
    "3. **Forward pass** -- model predicts next tokens\n",
    "4. **Compute loss** -- cross-entropy between predictions and targets\n",
    "5. **Backward pass** -- compute gradients\n",
    "6. **Clip** gradients to prevent explosions\n",
    "7. **Update** learning rate schedule\n",
    "8. **Step** the optimizer to update weights\n",
    "9. **Repeat** from step 2"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "subprocess.check_call(['pip', 'install', '-q', 'tiktoken'])\n",
    "import tiktoken\n",
    "\n",
    "print(\"All dependencies loaded!\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the training loop as a cycle."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Draw the training cycle\n",
    "stages = [\n",
    "    ('Load\\nBatch', 0),\n",
    "    ('Forward\\nPass', 45),\n",
    "    ('Compute\\nLoss', 90),\n",
    "    ('Backward\\nPass', 135),\n",
    "    ('Clip\\nGradients', 180),\n",
    "    ('Update\\nLR', 225),\n",
    "    ('Optimizer\\nStep', 270),\n",
    "    ('Log\\nMetrics', 315),\n",
    "]\n",
    "\n",
    "radius = 3\n",
    "for name, angle_deg in stages:\n",
    "    angle = np.radians(90 - angle_deg)  # Start from top, go clockwise\n",
    "    x = radius * np.cos(angle)\n",
    "    y = radius * np.sin(angle)\n",
    "\n",
    "    circle = plt.Circle((x, y), 0.7, facecolor='#e8f4f8', edgecolor='#2980b9',\n",
    "                         linewidth=2, zorder=3)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(x, y, name, ha='center', va='center', fontsize=9, fontweight='bold', zorder=4)\n",
    "\n",
    "# Draw arrows between stages\n",
    "for i in range(len(stages)):\n",
    "    angle1 = np.radians(90 - stages[i][1])\n",
    "    angle2 = np.radians(90 - stages[(i + 1) % len(stages)][1])\n",
    "\n",
    "    x1 = radius * np.cos(angle1)\n",
    "    y1 = radius * np.sin(angle1)\n",
    "    x2 = radius * np.cos(angle2)\n",
    "    y2 = radius * np.sin(angle2)\n",
    "\n",
    "    # Shorten arrow to not overlap circles\n",
    "    dx = x2 - x1\n",
    "    dy = y2 - y1\n",
    "    length = np.sqrt(dx**2 + dy**2)\n",
    "    shrink = 0.8 / length\n",
    "\n",
    "    ax.annotate('', xy=(x2 - dx * shrink, y2 - dy * shrink),\n",
    "                xytext=(x1 + dx * shrink, y1 + dy * shrink),\n",
    "                arrowprops=dict(arrowstyle='->', color='#e74c3c', lw=2))\n",
    "\n",
    "ax.text(0, 0, 'Training\\nLoop', ha='center', va='center', fontsize=14,\n",
    "        fontweight='bold', color='#e74c3c')\n",
    "\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_ylim(-5, 5)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('The Training Loop Cycle', fontsize=15, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_cycle.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Checkpoint: Every iteration follows this exact cycle.\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Cross-Entropy Loss for Language Models\n",
    "\n",
    "For a sequence of length $T$ with vocabulary size $V$, the model outputs logits $z \\in \\mathbb{R}^{B \\times T \\times V}$. The cross-entropy loss is:\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{B \\cdot T} \\sum_{b=1}^{B} \\sum_{t=1}^{T} \\log \\frac{e^{z_{b,t,y_{b,t}}}}{\\sum_{v=1}^{V} e^{z_{b,t,v}}}$$\n",
    "\n",
    "where $y_{b,t}$ is the target token at position $t$ in sample $b$.\n",
    "\n",
    "### Perplexity\n",
    "\n",
    "Perplexity measures how \"surprised\" the model is by the data:\n",
    "\n",
    "$$\\text{PPL} = e^{\\mathcal{L}}$$\n",
    "\n",
    "A perplexity of 100 means the model is, on average, as uncertain as if it were choosing uniformly from 100 tokens. Lower is better."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate cross-entropy and perplexity\n",
    "V = 50257  # GPT-2 vocab size\n",
    "\n",
    "# Random model (untrained): loss should be ~log(V)\n",
    "random_loss = math.log(V)\n",
    "random_ppl = math.exp(random_loss)\n",
    "print(f\"Random model (V={V:,}):\")\n",
    "print(f\"  Expected loss: ln({V:,}) = {random_loss:.2f}\")\n",
    "print(f\"  Expected perplexity: {random_ppl:.0f}\")\n",
    "\n",
    "# Good model: much lower loss\n",
    "good_loss = 3.0\n",
    "good_ppl = math.exp(good_loss)\n",
    "print(f\"\\nGood model:\")\n",
    "print(f\"  Loss: {good_loss}\")\n",
    "print(f\"  Perplexity: {good_ppl:.1f}\")\n",
    "\n",
    "# Great model:\n",
    "great_loss = 2.0\n",
    "great_ppl = math.exp(great_loss)\n",
    "print(f\"\\nGreat model:\")\n",
    "print(f\"  Loss: {great_loss}\")\n",
    "print(f\"  Perplexity: {great_ppl:.1f}\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 Data Pipeline"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"Language model dataset with sliding window.\"\"\"\n",
    "\n",
    "    def __init__(self, tokens, context_length):\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.context_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.tokens[idx : idx + self.context_length]\n",
    "        y = self.tokens[idx + 1 : idx + self.context_length + 1]\n",
    "        return x, y\n",
    "\n",
    "# Prepare training data\n",
    "raw_text = \"\"\"\n",
    "In the beginning, the universe was created. This made a lot of people very angry\n",
    "and has been widely regarded as a bad move. Many were increasingly of the opinion\n",
    "that they'd all made a big mistake in coming down from the trees in the first place.\n",
    "And some said that even the trees had been a bad move, and that no one should ever\n",
    "have left the oceans. The ships hung in the sky in much the same way that bricks don't.\n",
    "Ford Prefect knew where his towel was. The answer to the ultimate question of life,\n",
    "the universe, and everything was forty-two. Arthur Dent did not enjoy Thursdays.\n",
    "He never could get the hang of Thursdays. The Vogons were not combative. They were\n",
    "merely thorough. It was not for nothing that the phrase \"as useful as a Vogon guard\n",
    "in an ideological debate\" was so widely used. The great thing about space was that\n",
    "it was so incredibly, amazingly, mind-bogglingly big. Really big. You just won't believe\n",
    "how vastly, hugely, mind-bogglingly big it is. The probability of navigation was remote.\n",
    "\"\"\" * 100  # Repeat for more training data\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens = enc.encode(raw_text)\n",
    "print(f\"Training data: {len(raw_text):,} characters -> {len(tokens):,} tokens\")\n",
    "print(f\"Compression ratio: {len(raw_text) / len(tokens):.1f} chars/token\")\n",
    "\n",
    "CONTEXT_LENGTH = 64\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Split into train/val\n",
    "split_idx = int(0.9 * len(tokens))\n",
    "train_dataset = TextDataset(tokens[:split_idx], CONTEXT_LENGTH)\n",
    "val_dataset = TextDataset(tokens[split_idx:], CONTEXT_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "print(f\"\\nTrain: {len(train_dataset):,} samples, {len(train_loader)} batches\")\n",
    "print(f\"Val:   {len(val_dataset):,} samples, {len(val_loader)} batches\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Transformer Language Model"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLM(nn.Module):\n",
    "    \"\"\"A small Transformer language model for training pipeline demonstration.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model=128, n_heads=4, n_layers=4,\n",
    "                 context_length=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "        # Token + positional embeddings\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(context_length, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Transformer layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # Output head\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        # Weight tying: share token embeddings with output head\n",
    "        self.head.weight = self.token_emb.weight\n",
    "\n",
    "        # Causal mask\n",
    "        self.register_buffer('causal_mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1).bool()\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        pos = torch.arange(T, device=x.device).unsqueeze(0)\n",
    "\n",
    "        tok_emb = self.token_emb(x)\n",
    "        pos_emb = self.pos_emb(pos)\n",
    "        x = self.dropout(tok_emb + pos_emb)\n",
    "\n",
    "        x = self.transformer(x, mask=self.causal_mask[:T, :T], is_causal=True)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# Create model\n",
    "VOCAB_SIZE = enc.n_vocab\n",
    "model = TransformerLM(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=128,\n",
    "    n_heads=4,\n",
    "    n_layers=4,\n",
    "    context_length=CONTEXT_LENGTH,\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "print(f\"Model size: ~{num_params * 4 / 1e6:.1f} MB (float32)\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randint(0, VOCAB_SIZE, (2, CONTEXT_LENGTH)).to(device)\n",
    "test_output = model(test_input)\n",
    "print(f\"\\nForward pass test:\")\n",
    "print(f\"  Input shape:  {test_input.shape}  (batch, seq_len)\")\n",
    "print(f\"  Output shape: {test_output.shape}  (batch, seq_len, vocab)\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training Infrastructure"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(step, warmup_steps, total_steps, lr_max=3e-4, lr_min=1e-5):\n",
    "    \"\"\"Warmup + cosine decay learning rate schedule.\"\"\"\n",
    "    if step < warmup_steps:\n",
    "        return lr_max * step / warmup_steps\n",
    "    progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "    progress = min(progress, 1.0)\n",
    "    return lr_min + 0.5 * (lr_max - lr_min) * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, device):\n",
    "    \"\"\"Compute validation loss and perplexity.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    for x, y in val_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    avg_loss = total_loss / max(num_batches, 1)\n",
    "    model.train()\n",
    "    return avg_loss, math.exp(avg_loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, enc, prompt, max_tokens=100, temperature=0.8, device='cpu'):\n",
    "    \"\"\"Generate text from a prompt.\"\"\"\n",
    "    model.eval()\n",
    "    tokens = enc.encode(prompt)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        # Crop to context length\n",
    "        x = tokens[:, -model.context_length:]\n",
    "        logits = model(x)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "\n",
    "        # Sample from the distribution\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        tokens = torch.cat([tokens, next_token], dim=1)\n",
    "\n",
    "    model.train()\n",
    "    return enc.decode(tokens[0].tolist())\n",
    "\n",
    "# Test generation before training (should be random garbage)\n",
    "print(\"Generation BEFORE training:\")\n",
    "print(generate(model, enc, \"The answer\", max_tokens=50, device=device))"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Complete the Training Loop\n",
    "\n",
    "Fill in the missing parts of the training loop."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs, device,\n",
    "                lr_max=3e-4, warmup_fraction=0.1, max_grad_norm=1.0):\n",
    "    \"\"\"\n",
    "    Complete training loop with all components.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr_max,\n",
    "                                    betas=(0.9, 0.999), weight_decay=0.01)\n",
    "\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    warmup_steps = int(warmup_fraction * total_steps)\n",
    "\n",
    "    # Tracking\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    learning_rates = []\n",
    "    grad_norms = []\n",
    "    step = 0\n",
    "\n",
    "    print(f\"Training for {epochs} epochs ({total_steps} steps, {warmup_steps} warmup)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            # TODO 1a: Forward pass -- compute logits\n",
    "            logits = model(batch_x)   # Shape: (B, T, V)\n",
    "\n",
    "            # TODO 1b: Compute cross-entropy loss\n",
    "            # Reshape logits to (B*T, V) and targets to (B*T)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                batch_y.view(-1)\n",
    "            )\n",
    "\n",
    "            # TODO 1c: Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # TODO 1d: Compute and record gradient norm, then clip\n",
    "            total_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), max_norm=max_grad_norm\n",
    "            )\n",
    "            grad_norms.append(total_norm.item())\n",
    "\n",
    "            # TODO 1e: Update learning rate\n",
    "            lr = get_lr(step, warmup_steps, total_steps, lr_max)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            learning_rates.append(lr)\n",
    "\n",
    "            # TODO 1f: Step the optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            train_losses.append(loss.item())\n",
    "            step += 1\n",
    "\n",
    "        # End of epoch\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_ppl = evaluate(model, val_loader, device)\n",
    "        val_losses.append(val_loss)\n",
    "        elapsed = time.time() - epoch_start\n",
    "\n",
    "        print(f\"Epoch {epoch+1:>2d}/{epochs} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | \"\n",
    "              f\"Val PPL: {val_ppl:.1f} | \"\n",
    "              f\"LR: {lr:.2e} | \"\n",
    "              f\"Time: {elapsed:.1f}s\")\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'learning_rates': learning_rates,\n",
    "        'grad_norms': grad_norms,\n",
    "    }"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Experiment with Hyperparameters\n",
    "\n",
    "After training, try modifying these hyperparameters and observe the effect:"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try different configurations and compare results\n",
    "# Configuration 1 (default): lr_max=3e-4, warmup=10%, clip=1.0\n",
    "# Configuration 2: lr_max=1e-3, warmup=5%, clip=1.0 (higher lr)\n",
    "# Configuration 3: lr_max=3e-4, warmup=10%, clip=0.5 (tighter clipping)\n",
    "# Configuration 4: lr_max=3e-4, no warmup, clip=1.0 (skip warmup)\n",
    "\n",
    "# Which configuration gives the lowest validation loss?\n",
    "# Hypothesis: ________________\n",
    "# Result: ________________\n",
    "\n",
    "print(\"Experiment with hyperparameters after the main training run!\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us run the complete training loop."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model!\n",
    "metrics = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=15,\n",
    "    device=device,\n",
    "    lr_max=3e-4,\n",
    "    warmup_fraction=0.1,\n",
    "    max_grad_norm=1.0,\n",
    ")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Training loss\n",
    "ax = axes[0, 0]\n",
    "ax.plot(metrics['train_losses'], linewidth=0.5, alpha=0.3, color='#3498db')\n",
    "# Smoothed\n",
    "window = min(50, len(metrics['train_losses']) // 10)\n",
    "if window > 1:\n",
    "    smoothed = np.convolve(metrics['train_losses'], np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(metrics['train_losses'])), smoothed, linewidth=2, color='#2c3e50')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Loss', fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Validation loss\n",
    "ax = axes[0, 1]\n",
    "epochs_x = np.arange(1, len(metrics['val_losses']) + 1)\n",
    "ax.plot(epochs_x, metrics['val_losses'], 'o-', linewidth=2, color='#e74c3c', markersize=6)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Validation Loss')\n",
    "ax.set_title('Validation Loss', fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "ax = axes[1, 0]\n",
    "ax.plot(metrics['learning_rates'], linewidth=2, color='#2ecc71')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('Learning Rate Schedule', fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Gradient norms\n",
    "ax = axes[1, 1]\n",
    "ax.plot(metrics['grad_norms'], linewidth=0.5, alpha=0.5, color='#9b59b6')\n",
    "ax.axhline(y=1.0, color='black', linestyle='--', linewidth=1, label='Clip threshold')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Gradient Norm')\n",
    "ax.set_title('Gradient Norms', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Complete Training Dashboard', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_dashboard.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {metrics['train_losses'][-1]:.4f}\")\n",
    "print(f\"Final validation loss: {metrics['val_losses'][-1]:.4f}\")\n",
    "print(f\"Final perplexity: {math.exp(metrics['val_losses'][-1]):.1f}\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output: Text Generation"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with the trained model\n",
    "prompts = [\n",
    "    \"The answer to\",\n",
    "    \"In the beginning\",\n",
    "    \"The ships hung\",\n",
    "    \"Arthur Dent\",\n",
    "    \"The probability\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  TEXT GENERATION FROM TRAINED MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate(model, enc, prompt, max_tokens=80, temperature=0.8, device=device)\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(f\"Output: {generated[:200]}\")\n",
    "    print(\"-\" * 60)"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary: the complete pipeline\n",
    "print(\"=\" * 60)\n",
    "print(\"  THE COMPLETE TRAINING PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"  TOKENIZE  -->  Text to numbers (BPE)\")\n",
    "print(\"  BATCH     -->  Sliding window + DataLoader\")\n",
    "print(\"  FORWARD   -->  Model predicts next tokens\")\n",
    "print(\"  LOSS      -->  Cross-entropy with targets\")\n",
    "print(\"  BACKWARD  -->  Compute gradients\")\n",
    "print(\"  CLIP      -->  Cap gradient norms\")\n",
    "print(\"  SCHEDULE  -->  Warmup + cosine decay\")\n",
    "print(\"  STEP      -->  AdamW updates weights\")\n",
    "print()\n",
    "print(\"  This pattern trains EVERY modern language model.\")\n",
    "print(\"  GPT, LLaMA, Mistral, Gemma, Phi -- same loop,\")\n",
    "print(\"  different scale.\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"  Model size:    {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"  Final loss:    {metrics['val_losses'][-1]:.4f}\")\n",
    "print(f\"  Perplexity:    {math.exp(metrics['val_losses'][-1]):.1f}\")\n",
    "print(f\"  Vocab size:    {VOCAB_SIZE:,}\")\n",
    "print(f\"  Context:       {CONTEXT_LENGTH}\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**What we built:** A complete, working training pipeline that takes raw text and produces a language model capable of generating text. Every component -- tokenizer, dataset, dataloader, optimizer, scheduler, gradient clipping -- works together.\n",
    "\n",
    "**Key takeaways:**\n",
    "- The training loop pattern is universal: tokenize, batch, forward, loss, backward, clip, schedule, step\n",
    "- Cross-entropy loss + next-token prediction is how language models learn\n",
    "- Perplexity = exp(loss) measures model quality\n",
    "- All the components we built individually are essential -- remove any one and training breaks\n",
    "- Scale is the main difference between our model and GPT-4: the same loop, but with trillions of tokens and billions of parameters\n",
    "\n",
    "**The architecture provides the capacity to learn. The training pipeline provides the process of learning. Together, they produce intelligence.**"
   ],
   "id": "cell_26"
  }
 ]
}