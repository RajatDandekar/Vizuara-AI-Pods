{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Training Pipeline Engineering \u2014 Index \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline Engineering -- Notebook Series Index\n",
    "\n",
    "> **Course:** Build LLM from Scratch | **Pod:** Training Pipeline Engineering\n",
    ">\n",
    "> From raw text to a trained language model -- build every component of the training pipeline from scratch."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series Overview\n",
    "\n",
    "This five-notebook series covers the complete LLM training pipeline. Each notebook builds one component from scratch, with the final notebook assembling everything into a working training loop.\n",
    "\n",
    "**Prerequisites:** Basic Python, PyTorch fundamentals, understanding of neural network forward/backward passes.\n",
    "\n",
    "**Hardware:** Google Colab with T4 GPU (free tier is sufficient). All training completes in under 10 minutes."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Index\n",
    "\n",
    "| # | Notebook | Key Concepts | Est. Time |\n",
    "|---|----------|-------------|-----------|\n",
    "| 1 | [Tokenization and BPE](01_tokenization_and_bpe.ipynb) | Character/word/subword tokenization, Byte Pair Encoding algorithm, vocabulary size tradeoffs | 45 min |\n",
    "| 2 | [Dataset and DataLoader](02_dataset_and_dataloader.ipynb) | Sliding window, next-token prediction, batching, shuffling, gradient accumulation | 40 min |\n",
    "| 3 | [Optimization: SGD, Momentum, and Adam](03_optimization_sgd_momentum_adam.ipynb) | SGD, momentum, Adam, AdamW, adaptive learning rates, weight decay | 50 min |\n",
    "| 4 | [LR Scheduling and Gradient Clipping](04_lr_scheduling_and_gradient_clipping.ipynb) | Linear warmup, cosine decay, max-norm gradient clipping | 40 min |\n",
    "| 5 | [The Complete Training Loop](05_complete_training_loop.ipynb) | End-to-end pipeline, cross-entropy loss, perplexity, text generation | 55 min |\n",
    "\n",
    "**Total estimated time: ~3.8 hours**"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Path\n",
    "\n",
    "```\n",
    "Notebook 1: Tokenization (BPE)\n",
    "     |\n",
    "     v\n",
    "Notebook 2: Dataset + DataLoader\n",
    "     |\n",
    "     v\n",
    "Notebook 3: Optimizers (SGD -> Momentum -> Adam)\n",
    "     |\n",
    "     v\n",
    "Notebook 4: LR Schedule + Gradient Clipping\n",
    "     |\n",
    "     v\n",
    "Notebook 5: Complete Training Loop (everything assembled)\n",
    "```\n",
    "\n",
    "Each notebook builds on the previous one. By Notebook 5, every component comes together into a single working system."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use These Notebooks\n",
    "\n",
    "1. Open each notebook in Google Colab (Runtime > Change runtime type > GPU)\n",
    "2. Run all cells top to bottom\n",
    "3. Complete the TODO exercises (solutions are provided as comments)\n",
    "4. Experiment with the hyperparameters\n",
    "5. Move to the next notebook"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ready to start! Open Notebook 01 to begin.\")\n",
    "print()\n",
    "print(\"Notebook 1: Tokenization and BPE\")\n",
    "print(\"  -> Build a BPE tokenizer from scratch\")\n",
    "print(\"  -> Compare against GPT-2's production tokenizer\")"
   ],
   "id": "cell_6"
  }
 ]
}