{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Learning Rate Scheduling and Gradient Clipping \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Scheduling and Gradient Clipping -- Vizuara\n",
    "\n",
    "> **What you will build:** A warmup + cosine decay learning rate scheduler and max-norm gradient clipping from scratch, then watch them stabilize training in real time."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Even with Adam, two things can go catastrophically wrong during training:\n",
    "\n",
    "1. **Learning rate too high at the start:** Random initial weights produce unreliable gradients. A large learning rate amplifies this noise, and the loss can explode to infinity in the first few steps -- a death spiral you cannot recover from.\n",
    "\n",
    "2. **Gradient explosions:** A single unusual batch can produce gradients 100x larger than normal. Without protection, this single step wipes out the progress of thousands of previous steps.\n",
    "\n",
    "Learning rate scheduling (warmup + cosine decay) and gradient clipping are the two safety mechanisms that prevent these failures. Nearly every modern LLM uses both."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Think of training as driving a car on an unfamiliar mountain road at night.\n",
    "\n",
    "**Warmup** is like starting slowly when you first get on the road. You do not know the curves yet, so you drive cautiously. As you learn the road, you speed up.\n",
    "\n",
    "**Cosine decay** is like slowing down as you approach your destination. You need precision to park, not speed.\n",
    "\n",
    "**Gradient clipping** is like a speed limiter. Even if the road briefly becomes a steep downhill, the car's maximum speed is capped so you do not crash."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Visualize the intuition\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Warmup phase\n",
    "steps = np.arange(1000)\n",
    "warmup_lr = 3e-4 * steps / 1000\n",
    "axes[0].plot(steps, warmup_lr, color='#3498db', linewidth=2)\n",
    "axes[0].fill_between(steps, warmup_lr, alpha=0.2, color='#3498db')\n",
    "axes[0].set_title('Warmup: Cautious Start', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].set_ylabel('Learning Rate')\n",
    "axes[0].set_ylim(0, 3.5e-4)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Cosine decay\n",
    "steps = np.arange(9000)\n",
    "cosine_lr = 1e-5 + 0.5 * (3e-4 - 1e-5) * (1 + np.cos(np.pi * steps / 9000))\n",
    "axes[1].plot(steps, cosine_lr, color='#e74c3c', linewidth=2)\n",
    "axes[1].fill_between(steps, cosine_lr, alpha=0.2, color='#e74c3c')\n",
    "axes[1].set_title('Cosine Decay: Gradual Slowdown', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Step')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Gradient clipping\n",
    "grad_norms = np.random.exponential(1.0, 200) * 2\n",
    "clipped = np.minimum(grad_norms, 1.0)\n",
    "axes[2].bar(range(len(grad_norms)), grad_norms, alpha=0.4, color='#e74c3c', label='Original')\n",
    "axes[2].bar(range(len(clipped)), clipped, alpha=0.6, color='#2ecc71', label='Clipped')\n",
    "axes[2].axhline(y=1.0, color='black', linestyle='--', linewidth=1, label='Clip threshold')\n",
    "axes[2].set_title('Gradient Clipping: Safety Net', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Step')\n",
    "axes[2].set_ylabel('Gradient Norm')\n",
    "axes[2].legend(fontsize=9)\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('safety_mechanisms.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Checkpoint: Warmup, cosine decay, and gradient clipping work together to stabilize training.\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Linear Warmup\n",
    "\n",
    "During the first $W$ warmup steps, the learning rate increases linearly from $0$ to $\\eta_{\\max}$:\n",
    "\n",
    "$$\\eta_t = \\eta_{\\max} \\cdot \\frac{t}{W} \\quad \\text{for } t < W$$\n",
    "\n",
    "### Cosine Decay\n",
    "\n",
    "After warmup, the learning rate follows a cosine curve from $\\eta_{\\max}$ to $\\eta_{\\min}$:\n",
    "\n",
    "$$\\eta_t = \\eta_{\\min} + \\frac{1}{2}(\\eta_{\\max} - \\eta_{\\min})\\left(1 + \\cos\\left(\\frac{\\pi \\cdot (t - W)}{T - W}\\right)\\right) \\quad \\text{for } t \\geq W$$\n",
    "\n",
    "where $T$ is the total training steps.\n",
    "\n",
    "### Max-Norm Gradient Clipping\n",
    "\n",
    "Given gradient vector $g$ and threshold $c$:\n",
    "\n",
    "$$g_{\\text{clipped}} = \\begin{cases} g & \\text{if } \\|g\\| \\leq c \\\\ c \\cdot \\frac{g}{\\|g\\|} & \\text{if } \\|g\\| > c \\end{cases}$$\n",
    "\n",
    "This preserves the direction of the gradient but caps its magnitude at $c$."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical trace: cosine decay at key points\n",
    "eta_max, eta_min, T, W = 3e-4, 1e-5, 10000, 1000\n",
    "\n",
    "checkpoints = [0, 500, 1000, 3000, 5000, 7000, 9000, 10000]\n",
    "print(\"Learning Rate at Key Training Steps:\")\n",
    "print(\"-\" * 55)\n",
    "for t in checkpoints:\n",
    "    if t < W:\n",
    "        lr = eta_max * t / W\n",
    "        phase = \"warmup\"\n",
    "    else:\n",
    "        progress = (t - W) / (T - W)\n",
    "        lr = eta_min + 0.5 * (eta_max - eta_min) * (1 + math.cos(math.pi * progress))\n",
    "        phase = \"cosine\"\n",
    "    print(f\"  Step {t:>5d} ({phase:>6s}):  lr = {lr:.2e}\")\n",
    "\n",
    "# Numerical trace: gradient clipping\n",
    "print(\"\\nGradient Clipping Example:\")\n",
    "g = np.array([3.0, 4.0, 0.0])\n",
    "c = 1.0\n",
    "norm = np.linalg.norm(g)\n",
    "g_clipped = c * g / norm if norm > c else g\n",
    "print(f\"  Original gradient: {g},  norm = {norm:.1f}\")\n",
    "print(f\"  Clip threshold:    c = {c}\")\n",
    "print(f\"  Clipped gradient:  {g_clipped},  norm = {np.linalg.norm(g_clipped):.1f}\")\n",
    "print(f\"  Direction preserved: ratios {g[0]/g[1]:.2f} = {g_clipped[0]/g_clipped[1]:.2f}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 Learning Rate Scheduler"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupCosineScheduler:\n",
    "    \"\"\"\n",
    "    Learning rate scheduler with linear warmup + cosine decay.\n",
    "    Used by GPT, LLaMA, Mistral, and virtually every modern LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, warmup_steps, total_steps, lr_max, lr_min=1e-5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            optimizer: The optimizer whose learning rate to adjust\n",
    "            warmup_steps: Number of warmup steps\n",
    "            total_steps: Total training steps\n",
    "            lr_max: Peak learning rate (reached at end of warmup)\n",
    "            lr_min: Minimum learning rate (reached at end of training)\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.lr_max = lr_max\n",
    "        self.lr_min = lr_min\n",
    "\n",
    "    def get_lr(self, step):\n",
    "        \"\"\"Compute the learning rate for a given step.\"\"\"\n",
    "        if step < self.warmup_steps:\n",
    "            # Linear warmup\n",
    "            return self.lr_max * step / self.warmup_steps\n",
    "        else:\n",
    "            # Cosine decay\n",
    "            progress = (step - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
    "            progress = min(progress, 1.0)  # Clamp to [0, 1]\n",
    "            return self.lr_min + 0.5 * (self.lr_max - self.lr_min) * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "    def step(self, current_step):\n",
    "        \"\"\"Update the optimizer's learning rate for the current step.\"\"\"\n",
    "        lr = self.get_lr(current_step)\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        return lr\n",
    "\n",
    "# Test the scheduler\n",
    "model = torch.nn.Linear(10, 1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "scheduler = WarmupCosineScheduler(optimizer, warmup_steps=1000, total_steps=10000,\n",
    "                                   lr_max=3e-4, lr_min=1e-5)\n",
    "\n",
    "# Generate the full schedule\n",
    "steps_range = range(10000)\n",
    "lr_schedule = [scheduler.get_lr(s) for s in steps_range]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(steps_range, lr_schedule, linewidth=2, color='#3498db')\n",
    "ax.axvline(x=1000, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='End of warmup')\n",
    "ax.fill_between(range(1000), [scheduler.get_lr(s) for s in range(1000)], alpha=0.2, color='#3498db')\n",
    "ax.set_xlabel('Training Step', fontsize=12)\n",
    "ax.set_ylabel('Learning Rate', fontsize=12)\n",
    "ax.set_title('Warmup + Cosine Decay Schedule', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Annotate key points\n",
    "ax.annotate(f'Peak: {3e-4:.0e}', xy=(1000, 3e-4), xytext=(2000, 3.2e-4),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'), fontsize=10, color='red')\n",
    "ax.annotate(f'End: {1e-5:.0e}', xy=(9900, 1e-5), xytext=(8000, 5e-5),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'), fontsize=10, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lr_schedule.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Checkpoint: Linear warmup for 1000 steps, then smooth cosine decay.\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Gradient Clipping"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_grad_norm(parameters, max_norm):\n",
    "    \"\"\"\n",
    "    Clip gradients by their total L2 norm.\n",
    "    If the total norm exceeds max_norm, scale all gradients down proportionally.\n",
    "\n",
    "    Args:\n",
    "        parameters: Iterable of model parameters\n",
    "        max_norm: Maximum allowed gradient norm\n",
    "\n",
    "    Returns:\n",
    "        The total gradient norm (before clipping)\n",
    "    \"\"\"\n",
    "    parameters = list(parameters)\n",
    "\n",
    "    # Step 1: Compute total gradient norm\n",
    "    total_norm_sq = 0.0\n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            total_norm_sq += p.grad.data.norm(2).item() ** 2\n",
    "    total_norm = total_norm_sq ** 0.5\n",
    "\n",
    "    # Step 2: If norm exceeds threshold, scale gradients down\n",
    "    if total_norm > max_norm:\n",
    "        clip_coef = max_norm / total_norm\n",
    "        for p in parameters:\n",
    "            if p.grad is not None:\n",
    "                p.grad.data.mul_(clip_coef)\n",
    "\n",
    "    return total_norm\n",
    "\n",
    "# Demonstrate gradient clipping\n",
    "model = torch.nn.Linear(5, 3)\n",
    "\n",
    "# Simulate a large gradient\n",
    "model.weight.grad = torch.randn(3, 5) * 10  # Unusually large\n",
    "model.bias.grad = torch.randn(3) * 10\n",
    "\n",
    "# Before clipping\n",
    "grad_norm_before = sum(p.grad.data.norm(2).item()**2 for p in model.parameters() if p.grad is not None) ** 0.5\n",
    "print(f\"Gradient norm BEFORE clipping: {grad_norm_before:.2f}\")\n",
    "\n",
    "# Apply clipping\n",
    "clip_grad_norm(model.parameters(), max_norm=1.0)\n",
    "\n",
    "# After clipping\n",
    "grad_norm_after = sum(p.grad.data.norm(2).item()**2 for p in model.parameters() if p.grad is not None) ** 0.5\n",
    "print(f\"Gradient norm AFTER clipping:  {grad_norm_after:.2f}\")\n",
    "print(f\"Clip threshold: 1.0\")\n",
    "print(f\"Scale factor: {1.0 / grad_norm_before:.4f}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: What happens without gradient clipping?"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a problem with occasional gradient spikes\n",
    "X = torch.randn(500, 10)\n",
    "y = torch.randn(500, 1)\n",
    "# Add some outliers that will cause gradient spikes\n",
    "X[::50] *= 20\n",
    "y[::50] *= 20\n",
    "\n",
    "class SmallNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(10, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_with_clipping(use_clipping, max_norm=1.0, lr=0.01, epochs=100):\n",
    "    torch.manual_seed(42)\n",
    "    model = SmallNet()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    losses = []\n",
    "    grad_norms = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        pred = model(X)\n",
    "        loss = nn.MSELoss()(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Record gradient norm\n",
    "        total_norm = sum(p.grad.norm(2).item()**2 for p in model.parameters()) ** 0.5\n",
    "        grad_norms.append(total_norm)\n",
    "\n",
    "        if use_clipping:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return losses, grad_norms\n",
    "\n",
    "losses_no_clip, norms_no_clip = train_with_clipping(False)\n",
    "losses_clip, norms_clip = train_with_clipping(True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss comparison\n",
    "axes[0].plot(losses_no_clip, label='No Clipping', linewidth=1.5, color='#e74c3c', alpha=0.7)\n",
    "axes[0].plot(losses_clip, label='With Clipping (c=1.0)', linewidth=2, color='#2ecc71')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Gradient norm comparison\n",
    "axes[1].plot(norms_no_clip, label='No Clipping', linewidth=1, color='#e74c3c', alpha=0.5)\n",
    "axes[1].plot(norms_clip, label='With Clipping', linewidth=1, color='#2ecc71', alpha=0.7)\n",
    "axes[1].axhline(y=1.0, color='black', linestyle='--', linewidth=1, label='Clip threshold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Gradient Norm', fontsize=12)\n",
    "axes[1].set_title('Gradient Norms Over Training', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('clipping_effect.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Checkpoint: Gradient clipping prevents spikes from destabilizing training.\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Implement a Step Decay Scheduler\n",
    "\n",
    "Besides cosine decay, step decay is another common strategy: reduce the learning rate by a factor every N steps."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepDecayScheduler:\n",
    "    \"\"\"\n",
    "    Step decay: multiply lr by gamma every step_size steps.\n",
    "    Example: lr = 0.01, gamma = 0.5, step_size = 1000\n",
    "    Steps 0-999: lr = 0.01\n",
    "    Steps 1000-1999: lr = 0.005\n",
    "    Steps 2000-2999: lr = 0.0025\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, lr_initial, gamma=0.5, step_size=1000):\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_initial = lr_initial\n",
    "        self.gamma = gamma\n",
    "        self.step_size = step_size\n",
    "\n",
    "    def get_lr(self, step):\n",
    "        \"\"\"\n",
    "        TODO: Compute the learning rate at the given step.\n",
    "        Hint: How many times has the lr been decayed?\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # num_decays = step // self.step_size\n",
    "        # return self.lr_initial * (self.gamma ** num_decays)\n",
    "        pass\n",
    "\n",
    "    def step(self, current_step):\n",
    "        lr = self.get_lr(current_step)\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        return lr\n",
    "\n",
    "# SOLUTION (uncomment to verify):\n",
    "# class StepDecayScheduler:\n",
    "#     def __init__(self, optimizer, lr_initial, gamma=0.5, step_size=1000):\n",
    "#         self.optimizer = optimizer\n",
    "#         self.lr_initial = lr_initial\n",
    "#         self.gamma = gamma\n",
    "#         self.step_size = step_size\n",
    "#     def get_lr(self, step):\n",
    "#         num_decays = step // self.step_size\n",
    "#         return self.lr_initial * (self.gamma ** num_decays)\n",
    "#     def step(self, current_step):\n",
    "#         lr = self.get_lr(current_step)\n",
    "#         for param_group in self.optimizer.param_groups:\n",
    "#             param_group['lr'] = lr\n",
    "#         return lr"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement Value-Based Gradient Clipping\n",
    "\n",
    "Instead of clipping by total norm, clip each gradient element independently to a range $[-c, c]$."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_grad_value(parameters, clip_value):\n",
    "    \"\"\"\n",
    "    Clip each gradient element to [-clip_value, clip_value].\n",
    "\n",
    "    This is different from norm clipping:\n",
    "    - Norm clipping scales ALL gradients by the same factor\n",
    "    - Value clipping clips EACH element independently\n",
    "\n",
    "    Args:\n",
    "        parameters: Iterable of model parameters\n",
    "        clip_value: Maximum absolute value for any gradient element\n",
    "    \"\"\"\n",
    "    # TODO: Implement value-based clipping\n",
    "    # Hint: Use torch.clamp_ on each parameter's gradient\n",
    "    pass\n",
    "\n",
    "    # SOLUTION (uncomment to verify):\n",
    "    # for p in parameters:\n",
    "    #     if p.grad is not None:\n",
    "    #         p.grad.data.clamp_(-clip_value, clip_value)"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us combine the learning rate scheduler and gradient clipping in a complete training setup."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training loop with scheduler and clipping\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create data\n",
    "X = torch.randn(1000, 10)\n",
    "y = (X[:, 0:1] * 2 + X[:, 1:2] * -1 + 0.5).detach()\n",
    "X[::20] *= 15  # Add outliers for gradient spikes\n",
    "\n",
    "# Model\n",
    "model = SmallNet()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "# Scheduler\n",
    "total_steps = 200\n",
    "warmup_steps = 20\n",
    "scheduler = WarmupCosineScheduler(optimizer, warmup_steps=warmup_steps,\n",
    "                                   total_steps=total_steps, lr_max=3e-4, lr_min=1e-5)\n",
    "\n",
    "# Training\n",
    "losses = []\n",
    "learning_rates = []\n",
    "gradient_norms = []\n",
    "\n",
    "for step in range(total_steps):\n",
    "    # Forward\n",
    "    pred = model(X)\n",
    "    loss = nn.MSELoss()(pred, y)\n",
    "\n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Record gradient norm BEFORE clipping\n",
    "    total_norm = sum(p.grad.norm(2).item()**2 for p in model.parameters()) ** 0.5\n",
    "    gradient_norms.append(total_norm)\n",
    "\n",
    "    # Gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "    # Update learning rate\n",
    "    lr = scheduler.step(step)\n",
    "    learning_rates.append(lr)\n",
    "\n",
    "    # Step optimizer\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if step % 40 == 0:\n",
    "        print(f\"Step {step:>3d} | Loss: {loss.item():.4f} | LR: {lr:.2e} | Grad Norm: {total_norm:.2f}\")\n",
    "\n",
    "print(f\"\\nFinal loss: {losses[-1]:.6f}\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the complete training dynamics\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(losses, linewidth=1.5, color='#3498db')\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training Dynamics with LR Schedule + Gradient Clipping', fontsize=14, fontweight='bold')\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1].plot(learning_rates, linewidth=1.5, color='#e74c3c')\n",
    "axes[1].axvline(x=warmup_steps, color='gray', linestyle='--', alpha=0.5, label='End warmup')\n",
    "axes[1].set_ylabel('Learning Rate', fontsize=12)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Gradient norms\n",
    "axes[2].plot(gradient_norms, linewidth=1, color='#2ecc71', alpha=0.7)\n",
    "axes[2].axhline(y=1.0, color='black', linestyle='--', linewidth=1, label='Clip threshold')\n",
    "axes[2].set_ylabel('Gradient Norm', fontsize=12)\n",
    "axes[2].set_xlabel('Training Step', fontsize=12)\n",
    "axes[2].legend(fontsize=10)\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('full_training_dynamics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Checkpoint: LR starts low (warmup), peaks, decays (cosine). Gradients stay bounded.\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results: Comparing Schedules"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different scheduling strategies\n",
    "schedules = {\n",
    "    'No Schedule (constant lr)': lambda s: 3e-4,\n",
    "    'Warmup Only': lambda s: 3e-4 * min(1.0, s / 20),\n",
    "    'Cosine Only': lambda s: 1e-5 + 0.5 * (3e-4 - 1e-5) * (1 + math.cos(math.pi * s / 200)),\n",
    "    'Warmup + Cosine': lambda s: (3e-4 * s / 20) if s < 20 else (1e-5 + 0.5 * (3e-4 - 1e-5) * (1 + math.cos(math.pi * (s - 20) / 180))),\n",
    "}\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "colors = ['#e74c3c', '#f39c12', '#3498db', '#2ecc71']\n",
    "\n",
    "for (name, lr_fn), color in zip(schedules.items(), colors):\n",
    "    torch.manual_seed(42)\n",
    "    model = SmallNet()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "    losses = []\n",
    "    lrs = []\n",
    "\n",
    "    for step in range(200):\n",
    "        pred = model(X)\n",
    "        loss = nn.MSELoss()(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        lr = lr_fn(step)\n",
    "        for pg in optimizer.param_groups:\n",
    "            pg['lr'] = lr\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        lrs.append(lr)\n",
    "\n",
    "    ax1.plot(losses, label=name, linewidth=1.5, color=color)\n",
    "    ax2.plot(lrs, label=name, linewidth=1.5, color=color)\n",
    "\n",
    "ax1.set_xlabel('Step', fontsize=12)\n",
    "ax1.set_ylabel('Loss (log)', fontsize=12)\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_title('Loss by Schedule Strategy', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.set_xlabel('Step', fontsize=12)\n",
    "ax2.set_ylabel('Learning Rate', fontsize=12)\n",
    "ax2.set_title('Learning Rate Profiles', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('schedule_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Warmup + Cosine decay gives the best final loss in almost all cases.\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary card\n",
    "print(\"=\" * 60)\n",
    "print(\"  TRAINING SAFETY MECHANISMS -- SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"  1. LINEAR WARMUP\")\n",
    "print(\"     - Start with lr near 0, increase linearly\")\n",
    "print(\"     - Prevents early instability from random weights\")\n",
    "print(\"     - Typical: 5-10% of total training steps\")\n",
    "print()\n",
    "print(\"  2. COSINE DECAY\")\n",
    "print(\"     - Smoothly decrease lr following a cosine curve\")\n",
    "print(\"     - Fast decay in the middle, slow at start and end\")\n",
    "print(\"     - Lands at a small lr_min (e.g. 1e-5)\")\n",
    "print()\n",
    "print(\"  3. GRADIENT CLIPPING\")\n",
    "print(\"     - Cap gradient norm at a threshold (typically 1.0)\")\n",
    "print(\"     - Preserves gradient direction, limits magnitude\")\n",
    "print(\"     - Prevents catastrophic weight updates\")\n",
    "print()\n",
    "print(\"  USED BY: GPT, LLaMA, Mistral, Gemma, Phi, and\")\n",
    "print(\"  virtually every modern language model.\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**What we built:** A complete learning rate scheduler (warmup + cosine decay) and gradient clipping implementation from scratch.\n",
    "\n",
    "**Key takeaways:**\n",
    "- Warmup prevents early-training instability from random weight initialization\n",
    "- Cosine decay provides smooth learning rate annealing with a slow start, fast middle, and slow finish\n",
    "- Gradient clipping caps the gradient norm while preserving direction\n",
    "- These three mechanisms are universal in modern LLM training\n",
    "\n",
    "**What is next:** In Notebook 5, we will combine EVERYTHING -- tokenizer, dataset, dataloader, optimizer, scheduler, and gradient clipping -- into a complete, working training loop and train a small language model from scratch."
   ],
   "id": "cell_24"
  }
 ]
}