{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Energy Functions and the Boltzmann Distribution -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy Functions and the Boltzmann Distribution -- Vizuara\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "Energy-Based Models (EBMs) are one of the most general frameworks in machine learning. Unlike models that directly output probabilities, EBMs assign a single scalar \"energy\" to every possible configuration of data. Lower energy means more likely.\n",
    "\n",
    "This idea comes from statistical physics -- the same equations that describe how gas molecules distribute themselves in a room also describe how we can model complex data distributions.\n",
    "\n",
    "**By the end of this notebook, you will:**\n",
    "- Understand how energy functions map data to scalar values\n",
    "- Implement the Boltzmann distribution to convert energy to probability\n",
    "- Visualize energy landscapes and their corresponding probability distributions\n",
    "- Experience the partition function problem firsthand\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "### The Ball on a Landscape\n",
    "\n",
    "Imagine you are holding a ball at the top of a hilly landscape. When you release it, the ball rolls downhill and settles at the lowest point. This is a fundamental principle from physics: systems naturally settle into configurations that minimize their energy.\n",
    "\n",
    "Now here is the key insight: **what if we could use this same principle to describe data?** Suppose we assign low energy to data points that look like real images and high energy to random noise. Then the \"landscape\" naturally peaks at real data.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "Before we write any code, ask yourself:\n",
    "- If you had an energy function, how would you convert it to a probability?\n",
    "- What properties should the probability function have?\n",
    "- What mathematical function flips \"low energy = good\" into \"high probability\"?\n",
    "\n",
    "## 3. The Mathematics\n",
    "\n",
    "The energy function takes any data point $x$ and outputs a scalar:\n",
    "\n",
    "$$E_\\theta(x) \\in \\mathbb{R}$$\n",
    "\n",
    "To convert energy to probability, we use the **Boltzmann distribution**:\n",
    "\n",
    "$$p(x) = \\frac{\\exp(-E_\\theta(x))}{Z}, \\quad Z = \\int \\exp(-E_\\theta(x)) \\, dx$$\n",
    "\n",
    "Computationally, this means: for each data point, compute the negative energy, exponentiate it (making low energy values large and high energy values small), and then divide by the sum of all such values to normalize.\n",
    "\n",
    "**Numerical example:** Suppose we have three states with energies $E(x_1)=3$, $E(x_2)=1$, $E(x_3)=2$:\n",
    "- $\\exp(-3) = 0.050$, $\\exp(-1) = 0.368$, $\\exp(-2) = 0.135$\n",
    "- $Z = 0.050 + 0.368 + 0.135 = 0.553$\n",
    "- $p(x_1) = 0.090$, $p(x_2) = 0.665$, $p(x_3) = 0.244$\n",
    "\n",
    "The state with the lowest energy ($x_2$) gets the highest probability. This is exactly what we want.\n",
    "\n",
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 Defining Simple Energy Functions"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's define some simple 1D energy functions\n",
    "def quadratic_energy(x):\n",
    "    \"\"\"Simple quadratic energy: E(x) = x^2. Minimum at x=0.\"\"\"\n",
    "    return x ** 2\n",
    "\n",
    "def double_well_energy(x):\n",
    "    \"\"\"Double-well energy: E(x) = (x^2 - 1)^2. Minima at x=-1 and x=1.\"\"\"\n",
    "    return (x ** 2 - 1) ** 2\n",
    "\n",
    "def asymmetric_energy(x):\n",
    "    \"\"\"Asymmetric energy with a deep well on the right.\"\"\"\n",
    "    return 0.5 * (x + 1) ** 2 * (x - 2) ** 2 - 0.3 * x"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize these energy landscapes."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-3, 3, 500)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "energies = [quadratic_energy, double_well_energy, asymmetric_energy]\n",
    "titles = [\"Quadratic Energy\", \"Double-Well Energy\", \"Asymmetric Energy\"]\n",
    "\n",
    "for ax, energy_fn, title in zip(axes, energies, titles):\n",
    "    E = energy_fn(x)\n",
    "    ax.plot(x.numpy(), E.numpy(), 'b-', linewidth=2)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('E(x)')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    # Mark the minimum\n",
    "    min_idx = torch.argmin(E)\n",
    "    ax.plot(x[min_idx], E[min_idx], 'ro', markersize=10, label=f'Min at x={x[min_idx]:.2f}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Converting Energy to Probability (Boltzmann Distribution)"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boltzmann_distribution(energy_fn, x, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Convert an energy function to a probability distribution\n",
    "    using the Boltzmann distribution.\n",
    "\n",
    "    Args:\n",
    "        energy_fn: Function that maps x -> energy scalar\n",
    "        x: Tensor of points to evaluate\n",
    "        temperature: Controls the sharpness (lower = sharper peaks)\n",
    "\n",
    "    Returns:\n",
    "        Normalized probability values at each x\n",
    "    \"\"\"\n",
    "    E = energy_fn(x)\n",
    "    unnormalized = torch.exp(-E / temperature)\n",
    "\n",
    "    # The partition function Z: sum (discrete) or integral (continuous)\n",
    "    # For our discrete grid, we approximate the integral as a sum\n",
    "    dx = x[1] - x[0]  # grid spacing\n",
    "    Z = torch.sum(unnormalized) * dx\n",
    "\n",
    "    probabilities = unnormalized / Z\n",
    "    return probabilities, Z"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize energy -> probability for the quadratic energy\n",
    "x = torch.linspace(-4, 4, 1000)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "\n",
    "# Energy landscape\n",
    "E = quadratic_energy(x)\n",
    "axes[0].plot(x.numpy(), E.numpy(), 'b-', linewidth=2)\n",
    "axes[0].set_ylabel('E(x)', fontsize=12)\n",
    "axes[0].set_title('Energy Landscape', fontsize=14)\n",
    "axes[0].fill_between(x.numpy(), E.numpy(), alpha=0.1, color='blue')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Probability distribution\n",
    "p, Z = boltzmann_distribution(quadratic_energy, x)\n",
    "axes[1].plot(x.numpy(), p.numpy(), 'r-', linewidth=2)\n",
    "axes[1].set_ylabel('p(x)', fontsize=12)\n",
    "axes[1].set_xlabel('x', fontsize=12)\n",
    "axes[1].set_title(f'Probability Distribution (Z = {Z.item():.4f})', fontsize=14)\n",
    "axes[1].fill_between(x.numpy(), p.numpy(), alpha=0.1, color='red')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Notice: where energy is LOW, probability is HIGH. This is exactly what we want.\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn -- Temperature Effects\n",
    "\n",
    "### TODO: Explore Temperature\n",
    "\n",
    "The temperature parameter $T$ controls how \"peaked\" the probability distribution is. Low temperature makes the distribution sharper (more concentrated at the energy minimum), while high temperature makes it flatter (more uniform)."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_temperature_effects(energy_fn, x, temperatures):\n",
    "    \"\"\"\n",
    "    TODO: Complete this function to plot the Boltzmann distribution\n",
    "    at different temperatures.\n",
    "\n",
    "    For each temperature in the list:\n",
    "    1. Compute the Boltzmann distribution using the given energy function\n",
    "    2. Plot the resulting probability distribution\n",
    "\n",
    "    Hint: Use the boltzmann_distribution() function we defined above,\n",
    "    passing the temperature parameter.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    for T in temperatures:\n",
    "        # ============ TODO ============\n",
    "        # Step 1: Compute probabilities at this temperature\n",
    "        # Step 2: Plot the probability curve with a label\n",
    "        # Hint: p, Z = boltzmann_distribution(energy_fn, x, temperature=T)\n",
    "        # ==============================\n",
    "\n",
    "        p, Z = ???  # YOUR CODE HERE\n",
    "        ax.plot(x.numpy(), p.numpy(), linewidth=2, label=f'T={T}')\n",
    "\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('p(x)')\n",
    "    ax.set_title('Effect of Temperature on Boltzmann Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Test with these temperatures\n",
    "x = torch.linspace(-4, 4, 1000)\n",
    "temperatures = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "# plot_temperature_effects(quadratic_energy, x, temperatures)"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: Run this after completing the TODO\n",
    "x = torch.linspace(-4, 4, 1000)\n",
    "p_cold, _ = boltzmann_distribution(quadratic_energy, x, temperature=0.1)\n",
    "p_hot, _ = boltzmann_distribution(quadratic_energy, x, temperature=5.0)\n",
    "\n",
    "# Cold temperature should be more peaked\n",
    "assert p_cold.max() > p_hot.max(), \"Cold temperature should have a higher peak\"\n",
    "# Hot temperature should be flatter\n",
    "assert p_cold.std() < p_hot.std(), \"Hot temperature should be more spread out\"\n",
    "print(\"Correct! Lower temperature = sharper peak, higher temperature = flatter distribution.\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Your Turn -- 2D Energy Landscapes"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_energy_landscape(energy_fn, title=\"2D Energy Landscape\"):\n",
    "    \"\"\"\n",
    "    TODO: Visualize a 2D energy function as a contour plot\n",
    "    and its corresponding 2D Boltzmann distribution.\n",
    "\n",
    "    Steps:\n",
    "    1. Create a meshgrid of x1 and x2 values from -3 to 3\n",
    "    2. Compute E(x1, x2) at every grid point\n",
    "    3. Compute p(x1, x2) = exp(-E) / Z at every grid point\n",
    "    4. Plot both as filled contour plots side by side\n",
    "\n",
    "    Hint: Use torch.meshgrid() and plt.contourf()\n",
    "    \"\"\"\n",
    "    x1 = torch.linspace(-3, 3, 200)\n",
    "    x2 = torch.linspace(-3, 3, 200)\n",
    "    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Compute the energy at each grid point\n",
    "    # Step 2: Compute exp(-E) for each point\n",
    "    # Step 3: Compute Z as the sum of all exp(-E) values\n",
    "    # Step 4: Compute normalized probabilities\n",
    "    # ==============================\n",
    "\n",
    "    E = ???  # YOUR CODE HERE\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    axes[0].contourf(X1.numpy(), X2.numpy(), E.numpy(), levels=30, cmap='viridis')\n",
    "    axes[0].set_title('Energy E(x)')\n",
    "    axes[0].set_xlabel('x1')\n",
    "    axes[0].set_ylabel('x2')\n",
    "\n",
    "    # axes[1].contourf(X1.numpy(), X2.numpy(), p.numpy(), levels=30, cmap='hot')\n",
    "    # axes[1].set_title('Probability p(x)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example 2D energy: E(x1, x2) = x1^2 + x2^2\n",
    "# plot_2d_energy_landscape(lambda x1, x2: x1**2 + x2**2)"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together -- The Partition Function Problem\n",
    "\n",
    "Now let us experience firsthand why the partition function is a problem."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_function_scaling(dims_list):\n",
    "    \"\"\"\n",
    "    Demonstrate how the cost of computing Z scales with dimension.\n",
    "\n",
    "    For a grid of 100 points per dimension, the total number of\n",
    "    evaluations needed grows exponentially.\n",
    "    \"\"\"\n",
    "    grid_points = 100  # points per dimension\n",
    "\n",
    "    print(\"Dimension  |  Grid Points  |  Total Evaluations\")\n",
    "    print(\"-\" * 50)\n",
    "    for d in dims_list:\n",
    "        total = grid_points ** d\n",
    "        print(f\"    {d:3d}     |     {grid_points:5d}     |  {total:.2e}\")\n",
    "\n",
    "    return [grid_points ** d for d in dims_list]\n",
    "\n",
    "dims = [1, 2, 3, 5, 10, 50, 256]\n",
    "counts = partition_function_scaling(dims)"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the exponential blowup\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.semilogy(dims[:5], counts[:5], 'ro-', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Dimensionality', fontsize=12)\n",
    "ax.set_ylabel('Number of Evaluations (log scale)', fontsize=12)\n",
    "ax.set_title('Partition Function: Computational Cost vs Dimension', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotation\n",
    "ax.annotate('For a 16x16 image\\n(dim=256): 100^256\\n= impossible!',\n",
    "            xy=(5, counts[3]), fontsize=10, color='red',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\nThis is why we CANNOT directly compute Z for real-world data.\")\n",
    "print(\"We need a way to train EBMs WITHOUT computing the partition function.\")\n",
    "print(\"This brings us to the score function...\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "Let us train a simple neural network as an energy function on 1D data."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EnergyNetwork(nn.Module):\n",
    "    \"\"\"Neural network that outputs a scalar energy for 1D input.\"\"\"\n",
    "    def __init__(self, hidden=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, hidden), nn.SiLU(),\n",
    "            nn.Linear(hidden, hidden), nn.SiLU(),\n",
    "            nn.Linear(hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "# Create synthetic 1D data from a mixture of Gaussians\n",
    "def sample_mixture(n=1000):\n",
    "    \"\"\"Sample from a mixture of two Gaussians.\"\"\"\n",
    "    mix = torch.rand(n) < 0.5\n",
    "    data = torch.randn(n) * 0.3\n",
    "    data[mix] += 1.5\n",
    "    data[~mix] -= 1.5\n",
    "    return data\n",
    "\n",
    "data = sample_mixture(2000)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.hist(data.numpy(), bins=50, density=True, alpha=0.7, color='steelblue')\n",
    "plt.title('Data Distribution (Mixture of Two Gaussians)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what the energy landscape looks like for this data\n",
    "# A good energy function should have LOW energy where data is dense\n",
    "# and HIGH energy where data is sparse\n",
    "\n",
    "model = EnergyNetwork()\n",
    "x_grid = torch.linspace(-4, 4, 500).unsqueeze(-1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    E_random = model(x_grid).numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(x_grid.squeeze().numpy(), E_random, 'b-', linewidth=2)\n",
    "plt.title('Energy Function (Random Initialization)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('E(x)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "print(\"Before training, the energy landscape is random.\")\n",
    "print(\"We need a training method that does NOT require computing Z.\")\n",
    "print(\"This is what the score function enables -- see the next notebook!\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization: the key insight of this notebook\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "x = torch.linspace(-4, 4, 500)\n",
    "\n",
    "for ax, energy_fn, title in zip(axes,\n",
    "    [quadratic_energy, double_well_energy, asymmetric_energy],\n",
    "    [\"Single Well\", \"Double Well\", \"Asymmetric\"]):\n",
    "\n",
    "    E = energy_fn(x)\n",
    "    p, Z = boltzmann_distribution(energy_fn, x)\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    ax.plot(x.numpy(), E.numpy(), 'b-', linewidth=2, label='Energy')\n",
    "    ax2.plot(x.numpy(), p.numpy(), 'r-', linewidth=2, label='Probability')\n",
    "\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('E(x)', color='blue')\n",
    "    ax2.set_ylabel('p(x)', color='red')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Energy Landscapes and Their Probability Distributions', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. Low energy = high probability (and vice versa)\")\n",
    "print(\"2. The Boltzmann distribution converts energy to probability\")\n",
    "print(\"3. But computing Z is intractable in high dimensions\")\n",
    "print(\"4. We need the SCORE FUNCTION to bypass Z -- next notebook!\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Think About These Questions:\n",
    "1. Why does the temperature parameter matter? What happens in the limit as $T \\to 0$ and $T \\to \\infty$?\n",
    "2. Can you think of other functions besides $\\exp(-E)$ that could convert energy to probability while satisfying the required properties (positive, higher when energy is lower)?\n",
    "3. In physics, the Boltzmann distribution describes thermal equilibrium. What is the analogy in machine learning?\n",
    "4. The partition function is intractable for high-dimensional continuous data. Is it always intractable? When CAN you compute it exactly?\n",
    "\n",
    "### What's Next\n",
    "In the next notebook, we will discover the **score function** -- the gradient of the log probability density -- which completely bypasses the partition function. We will also learn **Langevin dynamics**, a sampling method that uses only the score function to generate data."
   ],
   "id": "cell_21"
  }
 ]
}