{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Full Pipeline: Multi-Scale Score Matching -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Pipeline: Multi-Scale Score Matching and Generation -- Vizuara\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebooks, we trained a score network using a single noise level. This works for simple distributions, but real-world data has complex multi-modal structure. Regions between modes are low-density \"deserts\" where the score is hard to learn.\n",
    "\n",
    "The solution: use **multiple noise scales**. This is the bridge between score matching and modern diffusion models. By training the score network across many noise levels, we get accurate score estimates everywhere -- from the broad global structure (high noise) to fine local details (low noise).\n",
    "\n",
    "**By the end of this notebook, you will:**\n",
    "- Implement multi-scale denoising score matching\n",
    "- Train a noise-conditioned score network\n",
    "- Implement annealed Langevin dynamics\n",
    "- Generate samples from a complex multi-modal distribution\n",
    "- Understand the direct connection to DDPM and Score SDEs\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "### Why Multiple Noise Scales?\n",
    "\n",
    "Imagine you are trying to find a specific house in a city. With a single level of zoom:\n",
    "- If you zoom out too far (high noise), you can see the city but not individual houses\n",
    "- If you zoom in too close (low noise), you can see one house but cannot navigate across neighborhoods\n",
    "\n",
    "The solution: start zoomed out, get to the right neighborhood, then zoom in progressively.\n",
    "\n",
    "This is exactly what annealed Langevin dynamics does: start with high noise (global navigation), then progressively reduce noise (local refinement).\n",
    "\n",
    "### Think About This\n",
    "- Why would a single low noise level fail for a distribution with widely separated modes?\n",
    "- Why would a single high noise level produce blurry samples?\n",
    "\n",
    "## 3. The Mathematics\n",
    "\n",
    "### Noise-Conditioned Score Network\n",
    "\n",
    "Instead of one score network, we train a single network conditioned on the noise level $\\sigma$:\n",
    "\n",
    "$$s_\\theta(x, \\sigma) \\approx \\nabla_x \\log q_\\sigma(x)$$\n",
    "\n",
    "where $q_\\sigma(x)$ is the data distribution convolved with Gaussian noise of standard deviation $\\sigma$.\n",
    "\n",
    "### Multi-Scale DSM Loss\n",
    "\n",
    "$$J(\\theta) = \\sum_{i=1}^{L} \\lambda(\\sigma_i) \\cdot \\mathbb{E}_{p(x)\\, q(\\tilde{x}|x)} \\left[\\left\\|s_\\theta(\\tilde{x}, \\sigma_i) + \\frac{\\tilde{x} - x}{\\sigma_i^2}\\right\\|^2\\right]$$\n",
    "\n",
    "Computationally: for each noise level $\\sigma_i$, add noise to the data, compute the target score, predict the score conditioned on $\\sigma_i$, and take the MSE. Sum across all noise levels.\n",
    "\n",
    "### Annealed Langevin Dynamics\n",
    "\n",
    "Start from pure noise and run Langevin dynamics at decreasing noise levels:\n",
    "\n",
    "For $i = L, L-1, \\ldots, 1$:\n",
    "\n",
    "$$x_{t+1} = x_t + \\eta_i \\cdot s_\\theta(x_t, \\sigma_i) + \\sqrt{2\\eta_i} \\cdot \\epsilon$$\n",
    "\n",
    "where $\\eta_i = c \\cdot \\sigma_i^2 / \\sigma_L^2$ scales the step size with the noise level.\n",
    "\n",
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 Noise-Conditioned Score Network"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ConditionalScoreNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Score network conditioned on noise level sigma.\n",
    "\n",
    "    The noise level is encoded and concatenated with the input,\n",
    "    allowing a single network to predict scores at all noise levels.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=2, hidden=256, n_sigmas=10):\n",
    "        super().__init__()\n",
    "        self.sigma_embed = nn.Embedding(n_sigmas, hidden)\n",
    "        self.input_proj = nn.Linear(dim, hidden)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, sigma_idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input points (batch, dim)\n",
    "            sigma_idx: Index into the noise schedule (batch,) -- integer indices\n",
    "        Returns:\n",
    "            Predicted score (batch, dim)\n",
    "        \"\"\"\n",
    "        h = self.input_proj(x) + self.sigma_embed(sigma_idx)\n",
    "        return self.net(h)"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Geometric Noise Schedule"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_schedule(sigma_min=0.01, sigma_max=5.0, n_levels=10):\n",
    "    \"\"\"\n",
    "    Create a geometric noise schedule from sigma_max to sigma_min.\n",
    "    Geometric spacing ensures good coverage of all scales.\n",
    "    \"\"\"\n",
    "    sigmas = torch.exp(torch.linspace(\n",
    "        np.log(sigma_max), np.log(sigma_min), n_levels\n",
    "    ))\n",
    "    return sigmas\n",
    "\n",
    "sigmas = get_noise_schedule(sigma_min=0.01, sigma_max=5.0, n_levels=10)\n",
    "print(\"Noise schedule (sigma_max -> sigma_min):\")\n",
    "for i, s in enumerate(sigmas):\n",
    "    print(f\"  Level {i}: sigma = {s.item():.4f}\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: effect of each noise level on data\n",
    "data_demo = torch.tensor([[-2.0, 0.0], [2.0, 0.0]]).repeat(200, 1)\n",
    "data_demo = data_demo + torch.randn_like(data_demo) * 0.3\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(18, 7))\n",
    "for i, (ax, sigma) in enumerate(zip(axes.flat, sigmas)):\n",
    "    noisy = data_demo + torch.randn_like(data_demo) * sigma\n",
    "    ax.scatter(noisy[:, 0].numpy(), noisy[:, 1].numpy(), s=3, alpha=0.3, c='steelblue')\n",
    "    ax.set_title(f'sigma={sigma:.3f}', fontsize=11)\n",
    "    ax.set_xlim(-8, 8)\n",
    "    ax.set_ylim(-6, 6)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.2)\n",
    "\n",
    "plt.suptitle('Data at Different Noise Levels', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"High noise: modes merge. Low noise: modes are distinct.\")\n",
    "print(\"The network must learn the score at EVERY level.\")"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Complex Training Data"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_complex_distribution(n, pattern='four_gaussians'):\n",
    "    \"\"\"Generate samples from a more complex distribution.\"\"\"\n",
    "    if pattern == 'four_gaussians':\n",
    "        centers = [[-2, -2], [-2, 2], [2, -2], [2, 2]]\n",
    "    elif pattern == 'ring':\n",
    "        angles = torch.rand(n) * 2 * np.pi\n",
    "        r = 2.0 + torch.randn(n) * 0.2\n",
    "        return torch.stack([r * torch.cos(angles), r * torch.sin(angles)], dim=-1)\n",
    "    elif pattern == 'two_moons':\n",
    "        # Upper moon\n",
    "        n1 = n // 2\n",
    "        angles1 = torch.linspace(0, np.pi, n1)\n",
    "        x1 = torch.cos(angles1) + torch.randn(n1) * 0.1\n",
    "        y1 = torch.sin(angles1) + torch.randn(n1) * 0.1\n",
    "        # Lower moon\n",
    "        n2 = n - n1\n",
    "        angles2 = torch.linspace(0, np.pi, n2)\n",
    "        x2 = 1 - torch.cos(angles2) + torch.randn(n2) * 0.1\n",
    "        y2 = -torch.sin(angles2) + 0.5 + torch.randn(n2) * 0.1\n",
    "        return torch.cat([\n",
    "            torch.stack([x1, y1], dim=-1),\n",
    "            torch.stack([x2, y2], dim=-1)\n",
    "        ], dim=0)\n",
    "    else:\n",
    "        centers = [[-2, 0], [2, 0]]\n",
    "\n",
    "    centers = torch.tensor(centers, dtype=torch.float)\n",
    "    idx = torch.randint(0, len(centers), (n,))\n",
    "    return torch.randn(n, 2) * 0.3 + centers[idx]\n",
    "\n",
    "# Visualize all three patterns\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for ax, pattern in zip(axes, ['four_gaussians', 'ring', 'two_moons']):\n",
    "    data = sample_complex_distribution(2000, pattern)\n",
    "    ax.scatter(data[:, 0].numpy(), data[:, 1].numpy(), s=3, alpha=0.3, c='steelblue')\n",
    "    ax.set_title(pattern.replace('_', ' ').title(), fontsize=13)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "plt.suptitle('Available Training Distributions', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Multi-Scale DSM Training"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_scale_dsm_loss(model, x, sigmas):\n",
    "    \"\"\"\n",
    "    Multi-scale denoising score matching loss.\n",
    "\n",
    "    For each sample in the batch:\n",
    "    1. Randomly pick a noise level\n",
    "    2. Add noise at that level\n",
    "    3. Compute the target score\n",
    "    4. Predict the score (conditioned on noise level)\n",
    "    5. MSE between prediction and target, weighted by sigma^2\n",
    "    \"\"\"\n",
    "    batch_size = x.shape[0]\n",
    "    n_sigmas = len(sigmas)\n",
    "\n",
    "    # Random noise level for each sample\n",
    "    sigma_idx = torch.randint(0, n_sigmas, (batch_size,))\n",
    "    sigma = sigmas[sigma_idx].unsqueeze(-1)  # (batch, 1)\n",
    "\n",
    "    # Add noise\n",
    "    noise = torch.randn_like(x)\n",
    "    x_noisy = x + sigma * noise\n",
    "\n",
    "    # Target score: -(x_noisy - x) / sigma^2 = -noise / sigma\n",
    "    target = -noise / sigma\n",
    "\n",
    "    # Predicted score\n",
    "    pred = model(x_noisy, sigma_idx)\n",
    "\n",
    "    # Weighted MSE (weight by sigma^2 as suggested by Song & Ermon)\n",
    "    weights = sigma.squeeze() ** 2\n",
    "    loss = (weights * ((pred - target) ** 2).sum(dim=-1)).mean()\n",
    "\n",
    "    return loss"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the noise-conditioned score network\n",
    "N_SIGMAS = 10\n",
    "sigmas = get_noise_schedule(sigma_min=0.01, sigma_max=5.0, n_levels=N_SIGMAS)\n",
    "PATTERN = 'four_gaussians'\n",
    "\n",
    "model = ConditionalScoreNet(dim=2, hidden=256, n_sigmas=N_SIGMAS)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "losses = []\n",
    "\n",
    "for epoch in range(3000):\n",
    "    x = sample_complex_distribution(512, PATTERN)\n",
    "    loss = multi_scale_dsm_loss(model, x, sigmas)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"Epoch {epoch+1:5d} | Loss: {loss.item():.4f}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization checkpoint: loss curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.semilogy(losses, alpha=0.3, linewidth=0.5, color='blue')\n",
    "smoothed = np.convolve(losses, np.ones(50)/50, mode='valid')\n",
    "plt.semilogy(smoothed, linewidth=2, color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Multi-Scale DSM Loss (log)')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn -- Implement Annealed Langevin Dynamics"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annealed_langevin_dynamics(model, sigmas, n_samples=500,\n",
    "                                steps_per_level=100, eps=0.01):\n",
    "    \"\"\"\n",
    "    TODO: Implement annealed Langevin dynamics.\n",
    "\n",
    "    For each noise level (from highest to lowest):\n",
    "    1. Set the step size: eta = eps * (sigma_i / sigma_max)^2\n",
    "    2. Run Langevin dynamics for steps_per_level steps at this noise level\n",
    "    3. Use model(x, sigma_idx) to get the score conditioned on sigma_i\n",
    "\n",
    "    Start from random Gaussian noise: x ~ N(0, sigma_max^2 * I)\n",
    "\n",
    "    Args:\n",
    "        model: Noise-conditioned score network\n",
    "        sigmas: Noise schedule (highest to lowest)\n",
    "        n_samples: Number of parallel samples\n",
    "        steps_per_level: Steps per noise level\n",
    "        eps: Base step size\n",
    "\n",
    "    Returns:\n",
    "        x: Final samples (n_samples, 2)\n",
    "        history: List of snapshots for visualization\n",
    "    \"\"\"\n",
    "    dim = 2\n",
    "    sigma_max = sigmas[0]\n",
    "    x = torch.randn(n_samples, dim) * sigma_max\n",
    "    history = [x.clone()]\n",
    "\n",
    "    for i in range(len(sigmas)):\n",
    "        sigma = sigmas[i]\n",
    "        sigma_idx = torch.full((n_samples,), i, dtype=torch.long)\n",
    "\n",
    "        # ============ TODO ============\n",
    "        # Step size scales with noise level\n",
    "        eta = ???  # eps * (sigma / sigma_max) ** 2\n",
    "\n",
    "        for step in range(steps_per_level):\n",
    "            # Compute score and update x\n",
    "            score = ???  # model(x, sigma_idx) -- use torch.no_grad()!\n",
    "            noise = ???  # torch.randn_like(x)\n",
    "            x = ???      # Langevin update\n",
    "        # ==============================\n",
    "\n",
    "        history.append(x.clone())\n",
    "\n",
    "    return x, history"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution and execution\n",
    "def annealed_langevin_solution(model, sigmas, n_samples=500,\n",
    "                                steps_per_level=100, eps=0.005):\n",
    "    dim = 2\n",
    "    sigma_max = sigmas[0]\n",
    "    x = torch.randn(n_samples, dim) * sigma_max\n",
    "    history = [x.clone()]\n",
    "\n",
    "    for i in range(len(sigmas)):\n",
    "        sigma = sigmas[i]\n",
    "        sigma_idx = torch.full((n_samples,), i, dtype=torch.long)\n",
    "        eta = eps * (sigma / sigma_max) ** 2\n",
    "\n",
    "        for step in range(steps_per_level):\n",
    "            with torch.no_grad():\n",
    "                score = model(x, sigma_idx)\n",
    "            noise = torch.randn_like(x)\n",
    "            x = x + eta * score + (2 * eta) ** 0.5 * noise\n",
    "\n",
    "        history.append(x.clone())\n",
    "        print(f\"  Level {i}: sigma={sigma:.4f}, eta={eta:.6f}\")\n",
    "\n",
    "    return x, history\n",
    "\n",
    "print(\"Running annealed Langevin dynamics...\")\n",
    "samples, history = annealed_langevin_solution(\n",
    "    model, sigmas, n_samples=1000, steps_per_level=100, eps=0.005\n",
    ")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Your Turn -- Visualize the Annealing Process"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a visualization showing samples at each noise level\n",
    "# The samples should progressively sharpen from blurry blobs to\n",
    "# well-separated clusters\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(18, 7))\n",
    "\n",
    "for i, (ax, snap) in enumerate(zip(axes.flat, history[:10])):\n",
    "    ax.scatter(snap[:, 0].numpy(), snap[:, 1].numpy(),\n",
    "               s=3, alpha=0.3, c='steelblue')\n",
    "    if i == 0:\n",
    "        ax.set_title('Initial Noise', fontsize=11)\n",
    "    elif i < len(sigmas):\n",
    "        ax.set_title(f'After sigma={sigmas[i-1]:.3f}', fontsize=10)\n",
    "    else:\n",
    "        ax.set_title('Final', fontsize=11)\n",
    "    ax.set_xlim(-6, 6)\n",
    "    ax.set_ylim(-6, 6)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.2)\n",
    "\n",
    "plt.suptitle('Annealed Langevin Dynamics: Progressive Refinement', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Samples start as noise and progressively form the target distribution!\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score fields at different noise levels\n",
    "fig, axes = plt.subplots(2, 5, figsize=(18, 7))\n",
    "n_g = 15\n",
    "g = torch.linspace(-5, 5, n_g)\n",
    "G1, G2 = torch.meshgrid(g, g, indexing='ij')\n",
    "gp = torch.stack([G1.flatten(), G2.flatten()], dim=-1)\n",
    "\n",
    "for i, (ax, sigma) in enumerate(zip(axes.flat, sigmas)):\n",
    "    sigma_idx = torch.full((n_g*n_g,), i, dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        s = model(gp, sigma_idx)\n",
    "\n",
    "    ax.quiver(G1.numpy(), G2.numpy(),\n",
    "              s[:, 0].reshape(n_g, n_g).numpy(),\n",
    "              s[:, 1].reshape(n_g, n_g).numpy(),\n",
    "              color='darkblue', scale=80, width=0.005)\n",
    "    ax.set_title(f'sigma={sigma:.3f}', fontsize=10)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.set_ylim(-5, 5)\n",
    "\n",
    "plt.suptitle('Learned Score Fields at Each Noise Level', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"High noise: broad structure. Low noise: fine details.\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare generated samples with true data\n",
    "true_data = sample_complex_distribution(2000, PATTERN)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].scatter(true_data[:, 0].numpy(), true_data[:, 1].numpy(),\n",
    "                s=5, alpha=0.3, c='steelblue')\n",
    "axes[0].set_title('True Data', fontsize=14)\n",
    "\n",
    "axes[1].scatter(samples[:, 0].numpy(), samples[:, 1].numpy(),\n",
    "                s=5, alpha=0.3, c='coral')\n",
    "axes[1].set_title('Generated Samples', fontsize=14)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.set_ylim(-5, 5)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Multi-Scale Score Matching: True vs Generated', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"The generated samples match the four-mode target distribution!\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grand finale: train on the two-moons dataset and generate\n",
    "print(\"Training on Two Moons dataset...\")\n",
    "model_moons = ConditionalScoreNet(dim=2, hidden=256, n_sigmas=N_SIGMAS)\n",
    "opt_moons = torch.optim.Adam(model_moons.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(3000):\n",
    "    x = sample_complex_distribution(512, 'two_moons')\n",
    "    loss = multi_scale_dsm_loss(model_moons, x, sigmas)\n",
    "    opt_moons.zero_grad()\n",
    "    loss.backward()\n",
    "    opt_moons.step()\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"  Epoch {epoch+1} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Sampling...\")\n",
    "moon_samples, moon_hist = annealed_langevin_solution(\n",
    "    model_moons, sigmas, n_samples=2000, steps_per_level=100, eps=0.005\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# True\n",
    "true_moons = sample_complex_distribution(2000, 'two_moons')\n",
    "axes[0].scatter(true_moons[:, 0], true_moons[:, 1], s=3, alpha=0.3, c='steelblue')\n",
    "axes[0].set_title('True Two Moons', fontsize=14)\n",
    "\n",
    "# Score field\n",
    "n_g = 20\n",
    "g = torch.linspace(-1.5, 2.5, n_g)\n",
    "gy = torch.linspace(-1.5, 1.5, n_g)\n",
    "G1, G2 = torch.meshgrid(g, gy, indexing='ij')\n",
    "gp = torch.stack([G1.flatten(), G2.flatten()], dim=-1)\n",
    "with torch.no_grad():\n",
    "    s = model_moons(gp, torch.zeros(n_g*n_g, dtype=torch.long))  # lowest noise\n",
    "axes[1].quiver(G1.numpy(), G2.numpy(),\n",
    "               s[:, 0].reshape(n_g, n_g).numpy(),\n",
    "               s[:, 1].reshape(n_g, n_g).numpy(),\n",
    "               color='darkblue', scale=80, width=0.004)\n",
    "axes[1].set_title('Learned Score Field', fontsize=14)\n",
    "\n",
    "# Generated\n",
    "axes[2].scatter(moon_samples[:, 0].numpy(), moon_samples[:, 1].numpy(),\n",
    "                s=3, alpha=0.3, c='coral')\n",
    "axes[2].set_title('Generated Samples', fontsize=14)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Score-Based Generation on Two Moons', fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThis is exactly what modern diffusion models do, but with MANY more noise levels\")\n",
    "print(\"and on high-dimensional data (images, audio, video).\")\n",
    "print(\"\\nThe path: EBMs -> Score Function -> DSM -> Multi-Scale DSM -> DDPM -> Score SDE\")\n",
    "print(\"You have now understood the entire intellectual foundation!\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Think About These Questions:\n",
    "1. We used 10 noise levels. What would happen with 100? 1000? (Hint: in the limit, you get a continuous-time SDE)\n",
    "2. How does the noise schedule (geometric vs linear) affect generation quality?\n",
    "3. Modern diffusion models (DDPM, Score SDE) use hundreds or thousands of noise levels and a U-Net architecture. What are the key differences from our simple implementation?\n",
    "4. Could you apply this technique to 1D audio waveforms? What would change?\n",
    "\n",
    "### Challenge Exercise:\n",
    "Try modifying the training to use the 'ring' distribution instead of 'four_gaussians'. Does the model learn the circular structure? How many noise levels do you need?\n",
    "\n",
    "### The Big Picture:\n",
    "You have now traced the complete intellectual path from energy-based models to modern diffusion models:\n",
    "- **Energy functions** assign likelihoods via the Boltzmann distribution\n",
    "- **The score function** bypasses the intractable partition function\n",
    "- **Score matching** learns the score from data alone\n",
    "- **Denoising score matching** simplifies this to noise prediction\n",
    "- **Multi-scale DSM** captures structure at all scales\n",
    "- **DDPM and Score SDEs** are the industrial-strength versions of what we built here\n",
    "\n",
    "Congratulations -- you now understand the foundations of generative AI!"
   ],
   "id": "cell_23"
  }
 ]
}