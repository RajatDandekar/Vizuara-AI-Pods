{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "RL Foundations: The Four Elements -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Foundations -- Vizuara\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "Supervised learning needs labels. Unsupervised learning needs data. But what if your agent has neither -- and must learn entirely from interacting with its environment?\n",
    "\n",
    "This is the core idea behind **reinforcement learning** (RL). It is the closest form of machine learning to how humans and animals actually learn: try something, observe the outcome, adjust, repeat.\n",
    "\n",
    "In this notebook, we will build your intuition for what RL is, how it differs from other ML paradigms, and what the four foundational elements of every RL system are. By the end, you will implement a complete Tic-Tac-Toe agent that learns to play by updating a value function through self-play -- no labeled data required.\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "Think about learning to ride a bicycle. Nobody hands you a dataset of \"correct pedaling patterns.\" You get on, wobble, fall (negative reward), adjust, and eventually learn to balance.\n",
    "\n",
    "Now contrast this with:\n",
    "- **Supervised learning**: \"Here are 1,000 images labeled cat or dog. Learn the mapping.\"\n",
    "- **Unsupervised learning**: \"Here is unlabeled data. Find clusters.\"\n",
    "- **Reinforcement learning**: \"There is no dataset. Interact with the world and maximize reward.\"\n",
    "\n",
    "Let us make this concrete with code. We will simulate all three paradigms side by side."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Supervised Learning: learn from labeled examples\n",
    "# -------------------------------------------------------\n",
    "# Simple linear regression: y = 2x + 1\n",
    "np.random.seed(42)\n",
    "X_supervised = np.random.rand(50)\n",
    "y_supervised = 2 * X_supervised + 1 + np.random.randn(50) * 0.1\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Unsupervised Learning: find structure without labels\n",
    "# -------------------------------------------------------\n",
    "from sklearn.datasets import make_blobs\n",
    "X_unsupervised, _ = make_blobs(n_samples=100, centers=3, cluster_std=0.6, random_state=42)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Reinforcement Learning: learn by interaction\n",
    "# -------------------------------------------------------\n",
    "# Simple bandit: agent picks arms, receives rewards\n",
    "n_arms = 3\n",
    "true_means = [1.0, 2.0, 1.5]  # arm 0, 1, 2\n",
    "rl_rewards = []\n",
    "rl_choices = []\n",
    "\n",
    "# Random agent -- no learning yet\n",
    "for step in range(50):\n",
    "    arm = np.random.randint(n_arms)\n",
    "    reward = np.random.randn() + true_means[arm]\n",
    "    rl_choices.append(arm)\n",
    "    rl_rewards.append(reward)\n",
    "\n",
    "# Visualize all three paradigms\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Panel 1: Supervised\n",
    "axes[0].scatter(X_supervised, y_supervised, c='steelblue', alpha=0.7, s=30)\n",
    "axes[0].plot([0, 1], [1, 3], 'r-', linewidth=2, label='Learned mapping')\n",
    "axes[0].set_title('Supervised Learning\\n(labeled data -> model)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Input X')\n",
    "axes[0].set_ylabel('Label y')\n",
    "axes[0].legend()\n",
    "\n",
    "# Panel 2: Unsupervised\n",
    "axes[1].scatter(X_unsupervised[:, 0], X_unsupervised[:, 1], c='gray', alpha=0.5, s=30)\n",
    "axes[1].set_title('Unsupervised Learning\\n(unlabeled data -> clusters)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "\n",
    "# Panel 3: RL\n",
    "axes[2].plot(rl_rewards, 'o-', markersize=3, alpha=0.7, color='green')\n",
    "axes[2].axhline(y=max(true_means), color='red', linestyle='--', label='Best arm mean')\n",
    "axes[2].set_title('Reinforcement Learning\\n(no data -- learn by interaction)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Time step')\n",
    "axes[2].set_ylabel('Reward received')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: In RL, there is no dataset. The agent generates its own data through interaction.\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "The goal of an RL agent is to find a **policy** $\\pi$ (a strategy for choosing actions) that maximizes the **expected cumulative reward**:\n",
    "\n",
    "$$\\max_{\\pi} \\; \\mathbb{E}\\left[\\sum_{t=0}^{T} r_t\\right]$$\n",
    "\n",
    "Here:\n",
    "- $\\pi$ is the policy (maps states to actions)\n",
    "- $r_t$ is the reward at time step $t$\n",
    "- $T$ is the final time step of the episode\n",
    "\n",
    "Let us compute this by hand for a simple example."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A robot collects cans over 5 time steps\n",
    "rewards = [1, 0, 1, 1, 0]\n",
    "\n",
    "# Compute cumulative return from each time step\n",
    "print(\"Time step | Reward | Return from here onward\")\n",
    "print(\"-\" * 50)\n",
    "for t in range(len(rewards)):\n",
    "    G_t = sum(rewards[t:])\n",
    "    print(f\"    t={t}    |   {rewards[t]}    |   G_{t} = {' + '.join(map(str, rewards[t:]))} = {G_t}\")\n",
    "\n",
    "total = sum(rewards)\n",
    "print(f\"\\nTotal return G_0 = {total}\")\n",
    "print(f\"The agent wants to find a policy that makes G_0 as large as possible.\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "Every RL system has **four elements**. Let us understand each one and build it in code.\n",
    "\n",
    "### Element 1: Policy\n",
    "\n",
    "The policy defines the agent's behavior. It maps states to actions."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple deterministic policy for a grid world\n",
    "# States: positions on a 4x4 grid (0-15)\n",
    "# Actions: 0=up, 1=right, 2=down, 3=left\n",
    "\n",
    "def simple_policy(state):\n",
    "    \"\"\"A hand-coded policy: always go right, then down.\"\"\"\n",
    "    row, col = state // 4, state % 4\n",
    "    if col < 3:\n",
    "        return 1  # go right\n",
    "    else:\n",
    "        return 2  # go down\n",
    "\n",
    "# Test the policy\n",
    "print(\"Policy decisions:\")\n",
    "for state in range(16):\n",
    "    action = simple_policy(state)\n",
    "    action_name = ['up', 'right', 'down', 'left'][action]\n",
    "    row, col = state // 4, state % 4\n",
    "    print(f\"  State {state:2d} (row={row}, col={col}) -> action: {action_name}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Element 2: Reward Signal\n",
    "\n",
    "The reward is the immediate feedback signal -- a single number at each step."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different reward structures for different problems\n",
    "print(\"Example reward structures:\\n\")\n",
    "\n",
    "# Chess\n",
    "print(\"Chess:\")\n",
    "print(\"  Win:  +1\")\n",
    "print(\"  Lose: -1\")\n",
    "print(\"  Draw:  0\")\n",
    "print()\n",
    "\n",
    "# Robot in maze\n",
    "print(\"Robot in maze:\")\n",
    "maze_rewards = [-1, -1, -1, -1, -1, 0]  # -1 per step, 0 at exit\n",
    "print(f\"  Rewards per step: {maze_rewards}\")\n",
    "print(f\"  Total: {sum(maze_rewards)} (faster escape = less negative total)\")\n",
    "print()\n",
    "\n",
    "# Robot collecting cans\n",
    "print(\"Robot collecting cans:\")\n",
    "can_rewards = [1, 0, 0, 1, 1, 0, 1]\n",
    "print(f\"  Rewards per step: {can_rewards}\")\n",
    "print(f\"  Total cans collected: {sum(can_rewards)}\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Element 3: Value Function\n",
    "\n",
    "The value function tells us the **long-term desirability** of a state -- not just the immediate reward, but the total expected future reward."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value function for a simple 5-state chain\n",
    "# States: [0] -> [1] -> [2] -> [3] -> [4=goal]\n",
    "# Reward: +1 at goal, 0 elsewhere\n",
    "\n",
    "# True values (under optimal policy: always move right)\n",
    "# V(s) = probability of reaching goal * reward\n",
    "# With deterministic transitions:\n",
    "# V(4) = 1.0 (at goal)\n",
    "# V(3) = 1.0 (one step to goal)\n",
    "# V(2) = 1.0 (two steps to goal)\n",
    "# V(1) = 1.0 (three steps to goal)\n",
    "# V(0) = 1.0 (four steps to goal)\n",
    "\n",
    "# But with stochastic transitions (50% chance of staying):\n",
    "gamma = 0.9\n",
    "V_stochastic = [0.0] * 5\n",
    "V_stochastic[4] = 1.0  # goal\n",
    "\n",
    "# Work backwards: V(s) = 0.5 * gamma * V(s) + 0.5 * gamma * V(s+1)\n",
    "# Rearranging: V(s) = (0.5 * gamma * V(s+1)) / (1 - 0.5 * gamma)\n",
    "for s in range(3, -1, -1):\n",
    "    V_stochastic[s] = (0.5 * gamma * V_stochastic[s + 1]) / (1 - 0.5 * gamma)\n",
    "\n",
    "print(\"State values (stochastic transitions, gamma=0.9):\")\n",
    "for s in range(5):\n",
    "    bar = \"#\" * int(V_stochastic[s] * 40)\n",
    "    print(f\"  State {s}: V = {V_stochastic[s]:.4f}  {bar}\")\n",
    "\n",
    "print(\"\\nKey insight: States closer to the goal have higher value.\")\n",
    "print(\"The value function captures LONG-TERM desirability, not just immediate reward.\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Element 4: Model of the Environment\n",
    "\n",
    "The model predicts what happens next: given a state and action, what is the next state and reward?"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple environment model (transition table)\n",
    "# Two states: High battery, Low battery\n",
    "# Two actions: Search, Wait\n",
    "\n",
    "model = {\n",
    "    ('High', 'Search'): [('High', 0.7, 2), ('Low', 0.3, 2)],\n",
    "    ('High', 'Wait'):   [('High', 1.0, 1)],\n",
    "    ('Low', 'Search'):  [('Low', 0.4, 2), ('Dead', 0.6, -3)],\n",
    "    ('Low', 'Wait'):    [('Low', 1.0, 1)],\n",
    "    ('Low', 'Recharge'):[('High', 1.0, 0)],\n",
    "}\n",
    "\n",
    "print(\"Environment Model (Recycling Robot):\")\n",
    "print(\"=\" * 60)\n",
    "for (state, action), transitions in model.items():\n",
    "    print(f\"\\n  State: {state}, Action: {action}\")\n",
    "    for next_state, prob, reward in transitions:\n",
    "        print(f\"    -> {next_state} with prob {prob:.1f}, reward = {reward}\")\n",
    "\n",
    "print(\"\\nModel-based: agent can PLAN by simulating the model.\")\n",
    "print(\"Model-free: agent learns directly from experience.\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### Exercise 1: Modify the Bandit\n",
    "\n",
    "The multi-armed bandit below uses a random strategy. Modify it to use an **epsilon-greedy** strategy:\n",
    "- With probability epsilon, choose a random arm (explore)\n",
    "- With probability 1-epsilon, choose the arm with the highest estimated mean (exploit)"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement epsilon-greedy bandit\n",
    "\n",
    "n_arms = 4\n",
    "true_means = [1.0, 2.5, 1.8, 0.5]\n",
    "n_steps = 1000\n",
    "epsilon = 0.1\n",
    "\n",
    "# Track estimates and counts\n",
    "Q = np.zeros(n_arms)       # estimated value of each arm\n",
    "N = np.zeros(n_arms)       # number of times each arm was pulled\n",
    "rewards_history = []\n",
    "\n",
    "for step in range(n_steps):\n",
    "    # TODO: Implement epsilon-greedy action selection\n",
    "    # Hint: Use np.random.random() < epsilon for exploration\n",
    "    # Hint: Use np.argmax(Q) for exploitation\n",
    "    arm = np.random.randint(n_arms)  # REPLACE THIS LINE\n",
    "\n",
    "    # Get reward (noisy version of true mean)\n",
    "    reward = np.random.randn() + true_means[arm]\n",
    "\n",
    "    # TODO: Update Q[arm] using incremental mean formula:\n",
    "    # Q[arm] = Q[arm] + (1 / N[arm]) * (reward - Q[arm])\n",
    "    # Don't forget to increment N[arm] first!\n",
    "    pass  # REPLACE THIS LINE\n",
    "\n",
    "    rewards_history.append(reward)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 4))\n",
    "window = 50\n",
    "smoothed = [np.mean(rewards_history[max(0,i-window):i+1]) for i in range(len(rewards_history))]\n",
    "plt.plot(smoothed, alpha=0.8)\n",
    "plt.axhline(y=max(true_means), color='red', linestyle='--', label=f'Best arm mean = {max(true_means)}')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Average Reward (smoothed)')\n",
    "plt.title('Epsilon-Greedy Bandit')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Estimated values: {Q}\")\n",
    "print(f\"True means:       {true_means}\")\n",
    "print(f\"Arm pull counts:  {N}\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Compute Returns by Hand\n",
    "\n",
    "Given the following reward sequences, compute the return $G_t$ at each time step."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute returns for each reward sequence\n",
    "\n",
    "# Sequence A: episodic (no discounting)\n",
    "rewards_a = [0, 0, 0, 1]\n",
    "print(\"Sequence A (episodic, no discounting):\")\n",
    "for t in range(len(rewards_a)):\n",
    "    G_t = None  # TODO: compute sum of rewards from t to end\n",
    "    print(f\"  G_{t} = {G_t}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Sequence B: discounted with gamma = 0.9\n",
    "rewards_b = [1, 2, 3, 4]\n",
    "gamma = 0.9\n",
    "print(f\"Sequence B (discounted, gamma={gamma}):\")\n",
    "for t in range(len(rewards_b)):\n",
    "    G_t = None  # TODO: compute discounted return from t\n",
    "    # Hint: G_t = r_t + gamma * r_{t+1} + gamma^2 * r_{t+2} + ...\n",
    "    print(f\"  G_{t} = {G_t}\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Now let us build something real: a **Tic-Tac-Toe agent** that learns to play by updating its value function through self-play. This brings together all four elements."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class TicTacToeEnv:\n",
    "    \"\"\"Simple Tic-Tac-Toe environment.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.board = [0] * 9  # 0=empty, 1=X, -1=O\n",
    "        self.current_player = 1  # X goes first\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = [0] * 9\n",
    "        self.current_player = 1\n",
    "        return tuple(self.board)\n",
    "\n",
    "    def get_state(self):\n",
    "        return tuple(self.board)\n",
    "\n",
    "    def available_actions(self):\n",
    "        return [i for i in range(9) if self.board[i] == 0]\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take action (place piece), return (state, reward, done).\"\"\"\n",
    "        self.board[action] = self.current_player\n",
    "        winner = self._check_winner()\n",
    "\n",
    "        if winner != 0:\n",
    "            return tuple(self.board), winner, True\n",
    "        elif 0 not in self.board:\n",
    "            return tuple(self.board), 0, True  # draw\n",
    "        else:\n",
    "            self.current_player *= -1\n",
    "            return tuple(self.board), 0, False\n",
    "\n",
    "    def _check_winner(self):\n",
    "        lines = [\n",
    "            [0,1,2], [3,4,5], [6,7,8],  # rows\n",
    "            [0,3,6], [1,4,7], [2,5,8],  # cols\n",
    "            [0,4,8], [2,4,6]             # diagonals\n",
    "        ]\n",
    "        for line in lines:\n",
    "            s = sum(self.board[i] for i in line)\n",
    "            if s == 3: return 1    # X wins\n",
    "            if s == -3: return -1  # O wins\n",
    "        return 0\n",
    "\n",
    "    def render(self):\n",
    "        symbols = {0: '.', 1: 'X', -1: 'O'}\n",
    "        for i in range(0, 9, 3):\n",
    "            print(' '.join(symbols[self.board[j]] for j in range(i, i+3)))\n",
    "        print()"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunctionAgent:\n",
    "    \"\"\"An RL agent that learns to play Tic-Tac-Toe using a value function.\"\"\"\n",
    "\n",
    "    def __init__(self, player=1, epsilon=0.2, alpha=0.1):\n",
    "        self.player = player         # 1 for X, -1 for O\n",
    "        self.epsilon = epsilon       # exploration rate\n",
    "        self.alpha = alpha           # learning rate (step size)\n",
    "        self.values = defaultdict(lambda: 0.5)  # initial value = 0.5\n",
    "\n",
    "    def choose_action(self, env, training=True):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        actions = env.available_actions()\n",
    "\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            # EXPLORE: random action\n",
    "            return np.random.choice(actions)\n",
    "        else:\n",
    "            # EXPLOIT: choose action leading to highest-value state\n",
    "            best_value = -float('inf')\n",
    "            best_action = actions[0]\n",
    "\n",
    "            for action in actions:\n",
    "                # Simulate the action\n",
    "                test_board = env.board.copy()\n",
    "                test_board[action] = env.current_player\n",
    "                state = tuple(test_board)\n",
    "\n",
    "                if self.values[state] > best_value:\n",
    "                    best_value = self.values[state]\n",
    "                    best_action = action\n",
    "\n",
    "            return best_action\n",
    "\n",
    "    def update(self, state_history, reward):\n",
    "        \"\"\"Update values using temporal difference: back up from final reward.\"\"\"\n",
    "        # Convert reward to this player's perspective\n",
    "        if self.player == -1:\n",
    "            reward = -reward\n",
    "\n",
    "        # Terminal state value\n",
    "        target = 1.0 if reward > 0 else (0.0 if reward < 0 else 0.5)\n",
    "\n",
    "        # Update backwards through the states this player visited\n",
    "        for state in reversed(state_history):\n",
    "            self.values[state] += self.alpha * (target - self.values[state])\n",
    "            target = self.values[state]  # bootstrap\n",
    "\n",
    "print(\"TicTacToe environment and ValueFunctionAgent defined.\")\n",
    "print(\"The agent uses epsilon-greedy for exploration/exploitation balance.\")\n",
    "print(\"It updates its value function by 'backing up' from the game outcome.\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agents(n_episodes=50000):\n",
    "    \"\"\"Train two agents by self-play.\"\"\"\n",
    "    env = TicTacToeEnv()\n",
    "    agent_x = ValueFunctionAgent(player=1, epsilon=0.2, alpha=0.1)\n",
    "    agent_o = ValueFunctionAgent(player=-1, epsilon=0.2, alpha=0.1)\n",
    "\n",
    "    stats = {'X_wins': 0, 'O_wins': 0, 'draws': 0}\n",
    "    win_rates = []  # for plotting\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        x_states = [state]\n",
    "        o_states = []\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if env.current_player == 1:\n",
    "                action = agent_x.choose_action(env)\n",
    "            else:\n",
    "                action = agent_o.choose_action(env)\n",
    "\n",
    "            state, reward, done = env.step(action)\n",
    "\n",
    "            if env.current_player == 1 or done:\n",
    "                o_states.append(state)\n",
    "            if env.current_player == -1 or done:\n",
    "                x_states.append(state)\n",
    "\n",
    "        # Update both agents\n",
    "        agent_x.update(x_states, reward)\n",
    "        agent_o.update(o_states, reward)\n",
    "\n",
    "        # Track stats\n",
    "        if reward == 1: stats['X_wins'] += 1\n",
    "        elif reward == -1: stats['O_wins'] += 1\n",
    "        else: stats['draws'] += 1\n",
    "\n",
    "        # Record win rate every 1000 episodes\n",
    "        if (episode + 1) % 1000 == 0:\n",
    "            recent = 1000\n",
    "            win_rates.append(stats['draws'] / (episode + 1))\n",
    "\n",
    "    return agent_x, agent_o, stats, win_rates\n",
    "\n",
    "print(\"Training two agents through self-play (50,000 games)...\")\n",
    "agent_x, agent_o, stats, win_rates = train_agents(50000)\n",
    "\n",
    "print(f\"\\nResults after 50,000 games:\")\n",
    "print(f\"  X wins: {stats['X_wins']} ({stats['X_wins']/500:.1f}%)\")\n",
    "print(f\"  O wins: {stats['O_wins']} ({stats['O_wins']/500:.1f}%)\")\n",
    "print(f\"  Draws:  {stats['draws']} ({stats['draws']/500:.1f}%)\")\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(1000, 50001, 1000), win_rates, 'b-o', markersize=4)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Cumulative Draw Rate')\n",
    "plt.title('Tic-Tac-Toe Self-Play: Draw Rate Over Training')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "print(\"As training progresses, both agents learn to play optimally, leading to more draws.\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned value function for key states\n",
    "print(\"Learned Value Function (sample states):\\n\")\n",
    "\n",
    "env = TicTacToeEnv()\n",
    "env.reset()\n",
    "\n",
    "# Show values for opening moves\n",
    "print(\"Empty board value:\", f\"{agent_x.values[tuple([0]*9)]:.3f}\")\n",
    "print()\n",
    "\n",
    "# Show value after X plays center\n",
    "test_board = [0]*9\n",
    "test_board[4] = 1  # X in center\n",
    "print(\"After X plays center:\")\n",
    "env.board = test_board.copy()\n",
    "env.render()\n",
    "print(f\"  Value (X perspective): {agent_x.values[tuple(test_board)]:.3f}\")\n",
    "print()\n",
    "\n",
    "# Show value after X plays center, O plays corner\n",
    "test_board[0] = -1  # O in top-left\n",
    "print(\"After X center, O corner:\")\n",
    "env.board = test_board.copy()\n",
    "env.render()\n",
    "print(f\"  Value (X perspective): {agent_x.values[tuple(test_board)]:.3f}\")\n",
    "print()\n",
    "\n",
    "# Show a winning state\n",
    "test_board2 = [1, 1, 1, -1, -1, 0, 0, 0, 0]\n",
    "print(\"X has three in a row (winning):\")\n",
    "env.board = test_board2.copy()\n",
    "env.render()\n",
    "print(f\"  Value (X perspective): {agent_x.values[tuple(test_board2)]:.3f}\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch the trained agent play against a random opponent\n",
    "def play_demo(agent, agent_player=1, n_games=5):\n",
    "    \"\"\"Watch trained agent vs random opponent.\"\"\"\n",
    "    env = TicTacToeEnv()\n",
    "    wins = 0\n",
    "\n",
    "    for game in range(n_games):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        print(f\"--- Game {game+1} ---\")\n",
    "\n",
    "        while not done:\n",
    "            if env.current_player == agent_player:\n",
    "                action = agent.choose_action(env, training=False)\n",
    "                print(f\"Trained agent (X) plays position {action}\")\n",
    "            else:\n",
    "                actions = env.available_actions()\n",
    "                action = np.random.choice(actions)\n",
    "                print(f\"Random agent (O) plays position {action}\")\n",
    "\n",
    "            state, reward, done = env.step(action)\n",
    "            env.render()\n",
    "\n",
    "        if reward == 1:\n",
    "            print(\"Result: X WINS\\n\")\n",
    "            wins += 1\n",
    "        elif reward == -1:\n",
    "            print(\"Result: O WINS\\n\")\n",
    "        else:\n",
    "            print(\"Result: DRAW\\n\")\n",
    "\n",
    "    print(f\"Trained agent won {wins}/{n_games} games against random opponent.\")\n",
    "\n",
    "play_demo(agent_x, agent_player=1, n_games=3)"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output\n",
    "\n",
    "You have built a complete RL agent from scratch. The Tic-Tac-Toe agent demonstrates all four elements:\n",
    "- **Policy**: Epsilon-greedy (explore vs exploit)\n",
    "- **Reward**: +1 for win, -1 for loss, 0 for draw\n",
    "- **Value Function**: Learned probability of winning from each board state\n",
    "- **Model**: Not used (model-free approach)"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY: Reinforcement Learning Foundations\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"What we built:\")\n",
    "print(\"  - Compared RL to supervised and unsupervised learning\")\n",
    "print(\"  - Understood the four elements of every RL system\")\n",
    "print(\"  - Implemented a Tic-Tac-Toe agent that learns from self-play\")\n",
    "print(\"  - Observed exploration vs exploitation in action\")\n",
    "print()\n",
    "print(\"Key takeaway:\")\n",
    "print(\"  RL agents learn from INTERACTION, not from DATA.\")\n",
    "print(\"  The value function captures long-term desirability.\")\n",
    "print(\"  Balancing exploration and exploitation is fundamental.\")\n",
    "print()\n",
    "print(f\"Unique states the agent learned: {len(agent_x.values)}\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**Questions to think about:**\n",
    "1. What happens if you set epsilon to 0 (pure exploitation) during training? Does the agent still learn well?\n",
    "2. Why does the draw rate increase as training progresses?\n",
    "3. Could you design a reward that accidentally teaches the wrong behavior? (Hint: what if you rewarded capturing pieces instead of winning?)\n",
    "\n",
    "**What comes next:**\n",
    "In the next notebook, we will formalize the mathematical framework behind RL: Markov Decision Processes, the Bellman equation, and how to compute optimal value functions. These are the tools that power algorithms like Q-learning and policy gradients."
   ],
   "id": "cell_26"
  }
 ]
}