{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Your First RL Agent with Gymnasium -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your First RL Agent with OpenAI Gymnasium -- Vizuara\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "Theory without practice is incomplete. In the previous notebooks, we built MDPs from scratch and learned the mathematical framework. Now it is time to use **OpenAI Gymnasium** -- the standard library that provides hundreds of pre-built RL environments with a unified interface.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand the Gymnasium API (make, reset, step, render)\n",
    "- Explore the Lunar Lander and CartPole environments\n",
    "- See why a random agent fails miserably\n",
    "- Build a simple heuristic agent that uses domain knowledge\n",
    "- Implement a basic Q-learning agent that learns from scratch\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "Imagine you are given the controls to a lunar lander. You have four buttons: do nothing, fire left thruster, fire main engine, fire right thruster. You can see your position, velocity, angle, and whether your legs are touching the ground. Your goal: land safely on the pad.\n",
    "\n",
    "If you press buttons randomly, you will crash. If you follow some hand-coded rules (\"if tilting left, fire right thruster\"), you might do okay. But the best approach? Let the agent learn the right strategy from experience.\n",
    "\n",
    "This is exactly what Gymnasium lets us do."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Gymnasium (run this once)\n",
    "import subprocess\n",
    "import sys\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'gymnasium[classic_control]', '-q'])\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"Gymnasium installed and imported.\")\n",
    "print(f\"Gymnasium version: {gym.__version__}\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us explore what Gymnasium offers\n",
    "# List some available environments\n",
    "env_categories = {\n",
    "    'Classic Control': ['CartPole-v1', 'MountainCar-v0', 'Acrobot-v1', 'Pendulum-v1'],\n",
    "    'Toy Text': ['FrozenLake-v1', 'Taxi-v3', 'CliffWalking-v0'],\n",
    "}\n",
    "\n",
    "print(\"Sample Gymnasium Environments:\")\n",
    "print(\"=\" * 50)\n",
    "for category, envs in env_categories.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for env_name in envs:\n",
    "        try:\n",
    "            env = gym.make(env_name)\n",
    "            print(f\"  {env_name}\")\n",
    "            print(f\"    Action space:      {env.action_space}\")\n",
    "            print(f\"    Observation space: {env.observation_space}\")\n",
    "            env.close()\n",
    "        except Exception as e:\n",
    "            print(f\"  {env_name}: (not available: {e})\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "The Gymnasium API implements the MDP interface we built in Notebook 2:\n",
    "\n",
    "At each time step, `env.step(action)` returns:\n",
    "- **observation** ($s_{t+1}$) -- the new state\n",
    "- **reward** ($r_{t+1}$) -- the immediate reward\n",
    "- **terminated** -- whether a terminal state was reached\n",
    "- **truncated** -- whether the episode was cut short\n",
    "- **info** -- diagnostic information\n",
    "\n",
    "The agent's goal remains:\n",
    "\n",
    "$$\\max_{\\pi} \\; \\mathbb{E}\\left[\\sum_{t=0}^{T} \\gamma^t r_t\\right]$$\n",
    "\n",
    "For **CartPole**: $r_t = +1$ for every step the pole stays upright (episodic, $\\gamma = 1$).\n",
    "\n",
    "For **Lunar Lander**: $r_t$ is shaped -- you get partial credit for orientation, velocity, and leg contact, plus +100 for landing safely and -100 for crashing."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us examine CartPole in detail\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "print(\"CartPole-v1 Environment Details\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"  - 0: Push cart LEFT\")\n",
    "print(f\"  - 1: Push cart RIGHT\")\n",
    "print()\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"  - obs[0]: Cart position     (range: {env.observation_space.low[0]:.1f} to {env.observation_space.high[0]:.1f})\")\n",
    "print(f\"  - obs[1]: Cart velocity     (range: {env.observation_space.low[1]:.1f} to {env.observation_space.high[1]:.1f})\")\n",
    "print(f\"  - obs[2]: Pole angle (rad)  (range: {env.observation_space.low[2]:.4f} to {env.observation_space.high[2]:.4f})\")\n",
    "print(f\"  - obs[3]: Pole angular vel  (range: {env.observation_space.low[3]:.1f} to {env.observation_space.high[3]:.1f})\")\n",
    "print()\n",
    "print(\"Reward: +1 for every step the pole stays upright\")\n",
    "print(\"Episode ends when: pole angle > 12 deg, cart off screen, or 500 steps\")\n",
    "print(f\"Max possible return: 500\")\n",
    "\n",
    "env.close()"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### Component 1: The Random Agent"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_agent(env_name, n_episodes=20, max_steps=500):\n",
    "    \"\"\"Run a random agent and record rewards.\"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    episode_rewards = []\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        obs, info = env.reset(seed=ep)\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = env.action_space.sample()  # random action\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "    env.close()\n",
    "    return episode_rewards\n",
    "\n",
    "# Test random agent on CartPole\n",
    "random_rewards_cartpole = run_random_agent('CartPole-v1', n_episodes=100)\n",
    "\n",
    "print(\"Random Agent on CartPole-v1:\")\n",
    "print(f\"  Mean reward:   {np.mean(random_rewards_cartpole):.1f}\")\n",
    "print(f\"  Std reward:    {np.std(random_rewards_cartpole):.1f}\")\n",
    "print(f\"  Max reward:    {np.max(random_rewards_cartpole):.1f}\")\n",
    "print(f\"  Min reward:    {np.min(random_rewards_cartpole):.1f}\")\n",
    "print(f\"  Max possible:  500\")\n",
    "print()\n",
    "print(\"The random agent averages about 20 steps -- terrible!\")\n",
    "print(\"The pole falls almost immediately with no intelligent control.\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 2: A Heuristic Agent"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic_cartpole(obs):\n",
    "    \"\"\"\n",
    "    Simple heuristic for CartPole:\n",
    "    If the pole is tilting right (positive angle), push right.\n",
    "    If the pole is tilting left (negative angle), push left.\n",
    "    \"\"\"\n",
    "    cart_pos, cart_vel, pole_angle, pole_angular_vel = obs\n",
    "\n",
    "    # Push in the direction the pole is tilting\n",
    "    if pole_angle + 0.1 * pole_angular_vel > 0:\n",
    "        return 1  # push right\n",
    "    else:\n",
    "        return 0  # push left\n",
    "\n",
    "def run_heuristic_agent(env_name, heuristic_fn, n_episodes=20, max_steps=500):\n",
    "    \"\"\"Run a heuristic agent and record rewards.\"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    episode_rewards = []\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        obs, info = env.reset(seed=ep)\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = heuristic_fn(obs)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "    env.close()\n",
    "    return episode_rewards\n",
    "\n",
    "heuristic_rewards = run_heuristic_agent('CartPole-v1', heuristic_cartpole, n_episodes=100)\n",
    "\n",
    "print(\"Heuristic Agent on CartPole-v1:\")\n",
    "print(f\"  Mean reward:   {np.mean(heuristic_rewards):.1f}\")\n",
    "print(f\"  Std reward:    {np.std(heuristic_rewards):.1f}\")\n",
    "print(f\"  Max reward:    {np.max(heuristic_rewards):.1f}\")\n",
    "print(f\"  Min reward:    {np.min(heuristic_rewards):.1f}\")\n",
    "print()\n",
    "print(\"Much better! But the heuristic was hand-designed.\")\n",
    "print(\"Can an agent LEARN this behavior on its own?\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 3: Visualize the Difference"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare random vs heuristic\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of episode lengths\n",
    "axes[0].hist(random_rewards_cartpole, bins=20, alpha=0.7, label='Random', color='red')\n",
    "axes[0].hist(heuristic_rewards, bins=20, alpha=0.7, label='Heuristic', color='green')\n",
    "axes[0].set_xlabel('Episode Reward (steps survived)', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Distribution of Episode Rewards', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].axvline(x=500, color='gold', linestyle='--', label='Max possible', linewidth=2)\n",
    "axes[0].legend(fontsize=11)\n",
    "\n",
    "# Bar chart of means\n",
    "means = [np.mean(random_rewards_cartpole), np.mean(heuristic_rewards)]\n",
    "stds = [np.std(random_rewards_cartpole), np.std(heuristic_rewards)]\n",
    "bars = axes[1].bar(['Random Agent', 'Heuristic Agent'], means, yerr=stds, capsize=5,\n",
    "                    color=['red', 'green'], alpha=0.7)\n",
    "axes[1].set_ylabel('Mean Episode Reward', fontsize=12)\n",
    "axes[1].set_title('Random vs Heuristic Agent', fontsize=14, fontweight='bold')\n",
    "axes[1].axhline(y=500, color='gold', linestyle='--', label='Max possible')\n",
    "axes[1].legend(fontsize=11)\n",
    "\n",
    "# Add value labels\n",
    "for bar, mean in zip(bars, means):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "                 f'{mean:.0f}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The heuristic agent is dramatically better, but it required human insight.\")\n",
    "print(\"Next: let us build an agent that LEARNS the optimal behavior from scratch.\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### Exercise 1: Explore the Environment\n",
    "\n",
    "Run a single CartPole episode step-by-step and observe how the observation changes with each action."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run a single episode of CartPole and print each step\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "obs, info = env.reset(seed=42)\n",
    "\n",
    "print(\"Step-by-step CartPole episode:\")\n",
    "print(f\"{'Step':>4} | {'Cart Pos':>9} | {'Cart Vel':>9} | {'Pole Angle':>11} | {'Angular Vel':>11} | {'Action':>6} | {'Reward':>6}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for step in range(20):\n",
    "    # TODO: Choose an action (try different strategies)\n",
    "    # Option A: random\n",
    "    # Option B: always push right (action=1)\n",
    "    # Option C: push opposite to pole tilt\n",
    "    action = env.action_space.sample()  # MODIFY THIS\n",
    "\n",
    "    next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    action_name = 'LEFT' if action == 0 else 'RIGHT'\n",
    "    print(f\"{step:>4} | {obs[0]:>9.4f} | {obs[1]:>9.4f} | {obs[2]:>11.6f} | {obs[3]:>11.6f} | {action_name:>6} | {reward:>6.1f}\")\n",
    "\n",
    "    obs = next_obs\n",
    "    if terminated or truncated:\n",
    "        print(f\"\\nEpisode ended at step {step+1}\")\n",
    "        break\n",
    "\n",
    "env.close()"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Improve the Heuristic\n",
    "\n",
    "The heuristic above only considers the pole angle and angular velocity. Modify it to also consider cart position (keep the cart near the center)."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement an improved heuristic that also considers cart position\n",
    "\n",
    "def improved_heuristic(obs):\n",
    "    cart_pos, cart_vel, pole_angle, pole_angular_vel = obs\n",
    "\n",
    "    # TODO: Combine pole angle, angular velocity, AND cart position\n",
    "    # into a better decision rule.\n",
    "    # Hint: if the cart is too far right, you might want to push left\n",
    "    # even if the pole is tilting right.\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    return 1  # REPLACE THIS\n",
    "\n",
    "# Test your improved heuristic\n",
    "improved_rewards = run_heuristic_agent('CartPole-v1', improved_heuristic, n_episodes=100)\n",
    "print(f\"Improved Heuristic: mean={np.mean(improved_rewards):.1f}, max={np.max(improved_rewards):.1f}\")\n",
    "print(f\"Original Heuristic: mean={np.mean(heuristic_rewards):.1f}, max={np.max(heuristic_rewards):.1f}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Now let us build a **Q-learning agent** that learns entirely from interaction with the environment. No hand-coded rules -- pure reinforcement learning."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Q-Learning agent for discrete state spaces.\n",
    "\n",
    "    Q-learning update rule:\n",
    "    Q(s, a) <- Q(s, a) + alpha * [r + gamma * max_a' Q(s', a') - Q(s, a)]\n",
    "\n",
    "    We discretize the continuous CartPole observations into bins\n",
    "    so we can use a simple table-based approach.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_actions, n_bins=10, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_bins = n_bins\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "        # Q-table: maps discretized state -> action values\n",
    "        self.q_table = defaultdict(lambda: np.zeros(n_actions))\n",
    "\n",
    "        # Bin edges for discretizing observations\n",
    "        # CartPole: [cart_pos, cart_vel, pole_angle, pole_angular_vel]\n",
    "        self.bins = [\n",
    "            np.linspace(-2.4, 2.4, n_bins),      # cart position\n",
    "            np.linspace(-3.0, 3.0, n_bins),       # cart velocity\n",
    "            np.linspace(-0.21, 0.21, n_bins),     # pole angle\n",
    "            np.linspace(-3.0, 3.0, n_bins),       # angular velocity\n",
    "        ]\n",
    "\n",
    "    def discretize(self, obs):\n",
    "        \"\"\"Convert continuous observation to discrete state.\"\"\"\n",
    "        state = []\n",
    "        for i, val in enumerate(obs):\n",
    "            idx = np.digitize(val, self.bins[i])\n",
    "            state.append(idx)\n",
    "        return tuple(state)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Q-learning update.\"\"\"\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.gamma * np.max(self.q_table[next_state])\n",
    "\n",
    "        self.q_table[state][action] += self.alpha * (target - self.q_table[state][action])\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Reduce exploration rate over time.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "print(\"Q-Learning Agent defined.\")\n",
    "print()\n",
    "print(\"Key formula:\")\n",
    "print(\"  Q(s,a) <- Q(s,a) + alpha * [r + gamma * max Q(s',a') - Q(s,a)]\")\n",
    "print()\n",
    "print(\"This is the fundamental update rule of Q-learning.\")\n",
    "print(\"The agent learns the value of each (state, action) pair from experience.\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_agent(n_episodes=5000):\n",
    "    \"\"\"Train Q-learning agent on CartPole.\"\"\"\n",
    "    env = gym.make('CartPole-v1')\n",
    "    agent = QLearningAgent(\n",
    "        n_actions=env.action_space.n,\n",
    "        n_bins=12,\n",
    "        alpha=0.1,\n",
    "        gamma=0.99,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.998,\n",
    "        epsilon_min=0.01,\n",
    "    )\n",
    "\n",
    "    episode_rewards = []\n",
    "    best_reward = 0\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        obs, info = env.reset(seed=ep % 100)\n",
    "        state = agent.discretize(obs)\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(500):\n",
    "            action = agent.choose_action(state)\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            next_state = agent.discretize(next_obs)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Penalize falling\n",
    "            if terminated and step < 499:\n",
    "                reward = -10\n",
    "\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += 1  # count steps survived\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "\n",
    "        # Progress report\n",
    "        if (ep + 1) % 500 == 0:\n",
    "            recent = np.mean(episode_rewards[-100:])\n",
    "            print(f\"  Episode {ep+1:>5} | Recent avg: {recent:>6.1f} | Epsilon: {agent.epsilon:.3f} | Best: {best_reward}\")\n",
    "\n",
    "    env.close()\n",
    "    return agent, episode_rewards\n",
    "\n",
    "print(\"Training Q-learning agent on CartPole-v1...\")\n",
    "print(\"This learns from scratch -- no human knowledge built in.\\n\")\n",
    "q_agent, q_rewards = train_q_agent(5000)"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive results visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Learning curve\n",
    "window = 50\n",
    "smoothed = [np.mean(q_rewards[max(0,i-window):i+1]) for i in range(len(q_rewards))]\n",
    "axes[0, 0].plot(smoothed, color='blue', alpha=0.8)\n",
    "axes[0, 0].axhline(y=500, color='gold', linestyle='--', label='Max possible (500)')\n",
    "axes[0, 0].axhline(y=np.mean(random_rewards_cartpole), color='red', linestyle='--', label=f'Random ({np.mean(random_rewards_cartpole):.0f})')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Reward (smoothed)')\n",
    "axes[0, 0].set_title('Q-Learning: Training Progress', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Plot 2: Histogram comparison (all three agents)\n",
    "axes[0, 1].hist(random_rewards_cartpole, bins=20, alpha=0.5, label='Random', color='red', density=True)\n",
    "axes[0, 1].hist(heuristic_rewards, bins=20, alpha=0.5, label='Heuristic', color='green', density=True)\n",
    "axes[0, 1].hist(q_rewards[-100:], bins=20, alpha=0.5, label='Q-Learning (last 100)', color='blue', density=True)\n",
    "axes[0, 1].set_xlabel('Episode Reward')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].set_title('Reward Distribution Comparison', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Plot 3: Bar chart of final performance\n",
    "final_q_mean = np.mean(q_rewards[-100:])\n",
    "means = [np.mean(random_rewards_cartpole), np.mean(heuristic_rewards), final_q_mean]\n",
    "labels = ['Random', 'Heuristic', 'Q-Learning']\n",
    "colors = ['red', 'green', 'blue']\n",
    "bars = axes[1, 0].bar(labels, means, color=colors, alpha=0.7)\n",
    "axes[1, 0].set_ylabel('Mean Reward')\n",
    "axes[1, 0].set_title('Final Performance Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].axhline(y=500, color='gold', linestyle='--')\n",
    "for bar, mean in zip(bars, means):\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "                     f'{mean:.0f}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 4: Q-table size over training\n",
    "q_sizes = []\n",
    "check_points = list(range(0, len(q_rewards), 100))\n",
    "for i in check_points:\n",
    "    q_sizes.append(min(i * 2, len(q_agent.q_table)))  # approximate\n",
    "axes[1, 1].plot(check_points, q_sizes, 'purple', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Episode')\n",
    "axes[1, 1].set_ylabel('Approx. States Explored')\n",
    "axes[1, 1].set_title('State Space Exploration', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Results (last 100 episodes):\")\n",
    "print(f\"  Random Agent:    {np.mean(random_rewards_cartpole):>6.1f} avg reward\")\n",
    "print(f\"  Heuristic Agent: {np.mean(heuristic_rewards):>6.1f} avg reward\")\n",
    "print(f\"  Q-Learning Agent: {final_q_mean:>6.1f} avg reward\")\n",
    "print(f\"\\n  Q-table entries: {len(q_agent.q_table)}\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained Q-learning agent (greedy, no exploration)\n",
    "def evaluate_agent(agent, env_name='CartPole-v1', n_episodes=50):\n",
    "    \"\"\"Evaluate trained agent with no exploration.\"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    rewards = []\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        obs, info = env.reset(seed=ep + 1000)\n",
    "        state = agent.discretize(obs)\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(500):\n",
    "            action = np.argmax(agent.q_table[state])  # greedy\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            state = agent.discretize(obs)\n",
    "            total_reward += 1\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "    env.close()\n",
    "    return rewards\n",
    "\n",
    "eval_rewards = evaluate_agent(q_agent)\n",
    "print(f\"Evaluation (50 episodes, greedy policy):\")\n",
    "print(f\"  Mean: {np.mean(eval_rewards):.1f}\")\n",
    "print(f\"  Std:  {np.std(eval_rewards):.1f}\")\n",
    "print(f\"  Min:  {np.min(eval_rewards)}\")\n",
    "print(f\"  Max:  {np.max(eval_rewards)}\")\n",
    "print(f\"  Solved (>= 475 avg): {'YES' if np.mean(eval_rewards) >= 475 else 'Not yet -- try more training'}\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final interactive demo: step through one episode showing the learned Q-values\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "obs, info = env.reset(seed=99)\n",
    "state = q_agent.discretize(obs)\n",
    "\n",
    "print(\"Watching the trained Q-learning agent play one episode:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Step':>4} | {'Pole Angle':>11} | {'Action':>7} | {'Q(left)':>8} | {'Q(right)':>9} | {'Alive':>5}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for step in range(500):\n",
    "    q_values = q_agent.q_table[state]\n",
    "    action = np.argmax(q_values)\n",
    "    action_name = 'LEFT' if action == 0 else 'RIGHT'\n",
    "\n",
    "    print(f\"{step:>4} | {obs[2]:>11.6f} | {action_name:>7} | {q_values[0]:>8.2f} | {q_values[1]:>9.2f} | {'yes':>5}\")\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    state = q_agent.discretize(obs)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(f\"\\nEpisode ended at step {step+1}\")\n",
    "        break\n",
    "\n",
    "    # Only show first 15 and last 5 steps to avoid clutter\n",
    "    if step == 15:\n",
    "        print(f\"  ... (skipping middle steps for brevity) ...\")\n",
    "    if 15 < step < max(0, step - 5):\n",
    "        continue\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(f\"\\nThe agent survived {step+1} steps by choosing actions based on learned Q-values.\")\n",
    "print(\"Notice how Q(left) and Q(right) change based on the pole angle --\")\n",
    "print(\"the agent learned which action is better in each state, purely from experience.\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY: Your First RL Agent with Gymnasium\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"What we built:\")\n",
    "print(\"  - Explored the Gymnasium API (make, reset, step)\")\n",
    "print(\"  - Tested a random agent (terrible: ~20 steps)\")\n",
    "print(\"  - Built a heuristic agent (good but hand-designed)\")\n",
    "print(\"  - Trained a Q-learning agent from scratch\")\n",
    "print(\"  - Compared all three approaches quantitatively\")\n",
    "print()\n",
    "print(\"Key takeaway:\")\n",
    "print(\"  The Q-learning agent learned to balance the pole\")\n",
    "print(\"  WITHOUT any human knowledge of physics or control theory.\")\n",
    "print(\"  It discovered the right behavior purely through trial and error.\")\n",
    "print()\n",
    "print(\"The Q-learning update rule:\")\n",
    "print(\"  Q(s,a) <- Q(s,a) + alpha * [r + gamma * max Q(s',a') - Q(s,a)]\")\n",
    "print()\n",
    "print(\"This is the foundation of modern RL. Deep Q-Networks (DQN)\")\n",
    "print(\"replace the Q-table with a neural network to handle\")\n",
    "print(\"high-dimensional state spaces like Atari game pixels.\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**Questions to think about:**\n",
    "1. What happens if you increase n_bins (finer discretization)? Does the agent learn faster or slower? Why?\n",
    "2. Why did we penalize the agent with -10 when it fails, instead of just 0? What happens without this penalty?\n",
    "3. The Q-learning agent uses a table with finite entries. How would you handle an environment with continuous states and continuous actions?\n",
    "\n",
    "**What comes next:**\n",
    "You now have the complete foundation of RL: the problem definition, the mathematical framework (MDPs), and a working learning algorithm (Q-learning). In the next pod, we will go deeper into value functions, the Bellman equation, and build towards Deep Q-Networks (DQN) that can play Atari games directly from pixels."
   ],
   "id": "cell_25"
  }
 ]
}