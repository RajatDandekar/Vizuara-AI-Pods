{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "MDPs, Rewards, and the Markov Property -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDPs, Rewards, and the Markov Property -- Vizuara\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebook, we built a Tic-Tac-Toe agent that learns from experience. But we did not have a formal mathematical framework to describe the environment. We just hard-coded the rules.\n",
    "\n",
    "In real RL problems -- controlling robots, optimizing chemical reactors, playing Atari -- we need a precise language for describing states, actions, transitions, and rewards. That language is the **Markov Decision Process** (MDP).\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Implement a complete MDP from scratch (the recycling robot)\n",
    "- Compute returns with and without discounting\n",
    "- Verify the Markov property computationally\n",
    "- Solve a small MDP to find optimal behavior\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "Think about a recycling robot. At each moment, it has a battery level (High or Low) and must choose between searching for cans, waiting, or recharging. Each choice has consequences: searching finds more cans but drains the battery. If the battery dies, the robot must be rescued (a large penalty).\n",
    "\n",
    "This is a classic MDP: the next state depends only on the current state and action, not on how you got there. The battery does not \"remember\" what the robot did three steps ago -- it only knows its current level."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Visualize the recycling robot decision problem\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "ax.set_xlim(-1, 11)\n",
    "ax.set_ylim(-1, 7)\n",
    "ax.axis('off')\n",
    "ax.set_title('The Recycling Robot MDP', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Draw states\n",
    "circle_high = plt.Circle((3, 4), 1.2, fill=False, linewidth=2, color='green')\n",
    "circle_low = plt.Circle((8, 4), 1.2, fill=False, linewidth=2, color='orange')\n",
    "ax.add_patch(circle_high)\n",
    "ax.add_patch(circle_low)\n",
    "ax.text(3, 4, 'HIGH', ha='center', va='center', fontsize=14, fontweight='bold', color='green')\n",
    "ax.text(8, 4, 'LOW', ha='center', va='center', fontsize=14, fontweight='bold', color='orange')\n",
    "\n",
    "# Annotations\n",
    "ax.annotate('search (alpha)\\nreward = r_search', xy=(3, 5.5), fontsize=9, ha='center',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow'))\n",
    "ax.annotate('wait\\nreward = r_wait', xy=(3, 2.2), fontsize=9, ha='center',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='lightcyan'))\n",
    "\n",
    "# Arrow from High to Low\n",
    "ax.annotate('', xy=(6.8, 4.3), xytext=(4.2, 4.3),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=1.5))\n",
    "ax.text(5.5, 4.8, 'search\\n(1-alpha)', fontsize=8, ha='center', color='red')\n",
    "\n",
    "# Arrow from Low to High (recharge)\n",
    "ax.annotate('', xy=(4.2, 3.7), xytext=(6.8, 3.7),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue', lw=1.5))\n",
    "ax.text(5.5, 3.0, 'recharge\\nreward = 0', fontsize=8, ha='center', color='blue')\n",
    "\n",
    "# Dead state\n",
    "ax.text(8, 1, 'DEAD\\n(rescue: -3)', ha='center', fontsize=10, color='red',\n",
    "        bbox=dict(boxstyle='round,pad=0.3', facecolor='mistyrose'))\n",
    "ax.annotate('', xy=(8, 1.6), xytext=(8, 2.8),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=1.5))\n",
    "ax.text(8.8, 2.2, 'search\\n(1-beta)', fontsize=8, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The robot must balance short-term reward (searching) against long-term survival (recharging).\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### The Agent-Environment Interface\n",
    "\n",
    "At each time step $t$:\n",
    "1. Agent observes state $s_t$\n",
    "2. Agent takes action $a_t$\n",
    "3. Environment returns reward $r_{t+1}$ and next state $s_{t+1}$\n",
    "\n",
    "### Formal MDP Definition\n",
    "\n",
    "An MDP is defined by the tuple:\n",
    "\n",
    "$$\\text{MDP} = (S, A, P, R, \\gamma)$$\n",
    "\n",
    "where:\n",
    "- $S$ = set of states\n",
    "- $A$ = set of actions\n",
    "- $P(s' | s, a)$ = transition probability\n",
    "- $R(s, a)$ = reward function\n",
    "- $\\gamma$ = discount factor\n",
    "\n",
    "### Returns and Discounting\n",
    "\n",
    "**Episodic return** (no discounting):\n",
    "$$G_t = r_t + r_{t+1} + r_{t+2} + \\cdots + r_T$$\n",
    "\n",
    "**Discounted return**:\n",
    "$$G_t = r_t + \\gamma \\, r_{t+1} + \\gamma^2 \\, r_{t+2} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k \\, r_{t+k}$$\n",
    "\n",
    "### The Markov Property\n",
    "\n",
    "A state is Markov if:\n",
    "$$P(s_{t+1} | s_t, a_t) = P(s_{t+1} | s_0, a_0, \\ldots, s_t, a_t)$$\n",
    "\n",
    "The future depends only on the present, not the past."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us verify each of these mathematically with code\n",
    "\n",
    "# 1. Compute episodic vs discounted returns\n",
    "rewards = [1, 2, 3, 4, 5]\n",
    "gammas = [0.0, 0.5, 0.9, 0.99, 1.0]\n",
    "\n",
    "print(\"Reward sequence:\", rewards)\n",
    "print()\n",
    "print(f\"{'gamma':>6} | {'G_0':>8} | Interpretation\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for gamma in gammas:\n",
    "    G = sum(gamma**k * rewards[k] for k in range(len(rewards)))\n",
    "    if gamma == 0.0:\n",
    "        interp = \"Only immediate reward matters\"\n",
    "    elif gamma == 1.0:\n",
    "        interp = \"All rewards equally weighted\"\n",
    "    else:\n",
    "        interp = f\"Future rewards discounted by {gamma} per step\"\n",
    "    print(f\"{gamma:>6.2f} | {G:>8.3f} | {interp}\")\n",
    "\n",
    "print(\"\\nKey insight: gamma controls the agent's 'patience' for future rewards.\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Visualize how discounting affects the weight of future rewards\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "timesteps = np.arange(20)\n",
    "for gamma in [0.5, 0.8, 0.9, 0.95, 0.99]:\n",
    "    weights = gamma ** timesteps\n",
    "    ax.plot(timesteps, weights, 'o-', markersize=4, label=f'gamma={gamma}')\n",
    "\n",
    "ax.set_xlabel('Time steps into the future', fontsize=12)\n",
    "ax.set_ylabel('Weight (gamma^k)', fontsize=12)\n",
    "ax.set_title('How Discount Factor Affects Future Reward Weights', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"With gamma=0.5, rewards 10 steps away are worth only 0.001 of immediate reward.\")\n",
    "print(\"With gamma=0.99, rewards 10 steps away are still worth 0.90 of immediate reward.\")"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### Component 1: Building an MDP Class"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    \"\"\"A complete Markov Decision Process implementation.\"\"\"\n",
    "\n",
    "    def __init__(self, states, actions, transitions, rewards, gamma=0.9):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            states: list of state names\n",
    "            actions: dict mapping state -> list of available actions\n",
    "            transitions: dict mapping (state, action) -> list of (next_state, probability)\n",
    "            rewards: dict mapping (state, action) -> immediate reward\n",
    "            gamma: discount factor\n",
    "        \"\"\"\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.transitions = transitions\n",
    "        self.rewards = rewards\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def step(self, state, action):\n",
    "        \"\"\"Take action in state, return (next_state, reward).\"\"\"\n",
    "        possible = self.transitions[(state, action)]\n",
    "        probs = [p for _, p in possible]\n",
    "        next_states = [s for s, _ in possible]\n",
    "\n",
    "        # Sample next state according to transition probabilities\n",
    "        idx = np.random.choice(len(possible), p=probs)\n",
    "        next_state = next_states[idx]\n",
    "        reward = self.rewards[(state, action)]\n",
    "\n",
    "        return next_state, reward\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        \"\"\"Get available actions in a state.\"\"\"\n",
    "        return self.actions.get(state, [])\n",
    "\n",
    "    def print_model(self):\n",
    "        \"\"\"Display the full MDP model.\"\"\"\n",
    "        print(\"MDP Model\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"States: {self.states}\")\n",
    "        print(f\"Gamma: {self.gamma}\")\n",
    "        print()\n",
    "        for state in self.states:\n",
    "            if state not in self.actions:\n",
    "                continue\n",
    "            print(f\"State: {state}\")\n",
    "            for action in self.actions[state]:\n",
    "                reward = self.rewards[(state, action)]\n",
    "                print(f\"  Action: {action} (reward = {reward})\")\n",
    "                for next_state, prob in self.transitions[(state, action)]:\n",
    "                    print(f\"    -> {next_state} with probability {prob:.2f}\")\n",
    "            print()\n",
    "\n",
    "print(\"MDP class defined. Let us create the recycling robot MDP.\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 2: The Recycling Robot MDP"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the recycling robot MDP\n",
    "alpha = 0.7  # prob of staying high when searching from high\n",
    "beta = 0.4   # prob of staying low when searching from low\n",
    "r_search = 2\n",
    "r_wait = 1\n",
    "\n",
    "recycling_robot = MDP(\n",
    "    states=['High', 'Low', 'Dead'],\n",
    "    actions={\n",
    "        'High': ['search', 'wait'],\n",
    "        'Low': ['search', 'wait', 'recharge'],\n",
    "    },\n",
    "    transitions={\n",
    "        ('High', 'search'):   [('High', alpha), ('Low', 1-alpha)],\n",
    "        ('High', 'wait'):     [('High', 1.0)],\n",
    "        ('Low', 'search'):    [('Low', beta), ('Dead', 1-beta)],\n",
    "        ('Low', 'wait'):      [('Low', 1.0)],\n",
    "        ('Low', 'recharge'):  [('High', 1.0)],\n",
    "    },\n",
    "    rewards={\n",
    "        ('High', 'search'):   r_search,\n",
    "        ('High', 'wait'):     r_wait,\n",
    "        ('Low', 'search'):    r_search,  # expected if not dead\n",
    "        ('Low', 'wait'):      r_wait,\n",
    "        ('Low', 'recharge'):  0,\n",
    "    },\n",
    "    gamma=0.9\n",
    ")\n",
    "\n",
    "recycling_robot.print_model()"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 3: Simulating Episodes"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_episode(mdp, policy, start_state, max_steps=20):\n",
    "    \"\"\"\n",
    "    Run one episode following a policy.\n",
    "\n",
    "    Args:\n",
    "        policy: dict mapping state -> action\n",
    "        start_state: initial state\n",
    "        max_steps: maximum steps before truncation\n",
    "    \"\"\"\n",
    "    state = start_state\n",
    "    trajectory = []\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        if state == 'Dead' or state not in mdp.actions:\n",
    "            break\n",
    "\n",
    "        action = policy[state]\n",
    "        next_state, reward = mdp.step(state, action)\n",
    "\n",
    "        # Special case: if battery dies during search from Low\n",
    "        if state == 'Low' and action == 'search' and next_state == 'Dead':\n",
    "            reward = -3  # rescue penalty\n",
    "\n",
    "        trajectory.append((t, state, action, reward, next_state))\n",
    "        total_reward += (mdp.gamma ** t) * reward\n",
    "        state = next_state\n",
    "\n",
    "    return trajectory, total_reward\n",
    "\n",
    "# Define two different policies\n",
    "policy_aggressive = {'High': 'search', 'Low': 'search'}   # always search\n",
    "policy_cautious = {'High': 'search', 'Low': 'recharge'}    # recharge when low\n",
    "\n",
    "# Simulate both\n",
    "np.random.seed(42)\n",
    "print(\"AGGRESSIVE POLICY (always search):\")\n",
    "print(\"-\" * 60)\n",
    "traj_agg, reward_agg = simulate_episode(recycling_robot, policy_aggressive, 'High')\n",
    "for t, s, a, r, ns in traj_agg:\n",
    "    print(f\"  t={t}: state={s:5s} action={a:10s} reward={r:+.0f} -> {ns}\")\n",
    "print(f\"  Discounted return: {reward_agg:.2f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"CAUTIOUS POLICY (recharge when low):\")\n",
    "print(\"-\" * 60)\n",
    "traj_cau, reward_cau = simulate_episode(recycling_robot, policy_cautious, 'High')\n",
    "for t, s, a, r, ns in traj_cau:\n",
    "    print(f\"  t={t}: state={s:5s} action={a:10s} reward={r:+.0f} -> {ns}\")\n",
    "print(f\"  Discounted return: {reward_cau:.2f}\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### Exercise 1: Verify the Markov Property\n",
    "\n",
    "Run the cell below and fill in the TODOs to verify that the recycling robot is indeed Markov. The key test: does the transition probability from a state depend on how you arrived there?"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Verify the Markov property empirically\n",
    "\n",
    "# Run many episodes and track:\n",
    "# 1. P(next_state | current_state=Low, action=search) regardless of history\n",
    "# 2. P(next_state | current_state=Low, action=search, previous_state=High)\n",
    "# 3. These should be equal if the Markov property holds\n",
    "\n",
    "n_trials = 10000\n",
    "# Count transitions from Low with search action\n",
    "count_total = {'Low': 0, 'Dead': 0}        # overall counts\n",
    "count_from_high = {'Low': 0, 'Dead': 0}    # counts when previous state was High\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    state = 'High'\n",
    "    prev_state = None\n",
    "\n",
    "    for step in range(50):\n",
    "        if state == 'Dead' or state not in recycling_robot.actions:\n",
    "            break\n",
    "\n",
    "        # Always search (to collect more data)\n",
    "        action = 'search' if state in ['High', 'Low'] else 'recharge'\n",
    "        next_state, _ = recycling_robot.step(state, action)\n",
    "\n",
    "        if state == 'Low' and action == 'search' and next_state != state:\n",
    "            next_state = 'Dead' if np.random.random() > beta else 'Low'\n",
    "\n",
    "        # TODO: If current state is Low and action is search,\n",
    "        # record the transition in count_total\n",
    "        # Also, if prev_state was High, record in count_from_high\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "        prev_state = state\n",
    "        state = next_state\n",
    "\n",
    "# TODO: Compute and compare the probabilities\n",
    "# P(Dead | Low, search) overall vs P(Dead | Low, search, came from High)\n",
    "# They should be approximately equal (both close to 1-beta = 0.6)\n",
    "\n",
    "total_sum = count_total['Low'] + count_total['Dead']\n",
    "from_high_sum = count_from_high['Low'] + count_from_high['Dead']\n",
    "\n",
    "if total_sum > 0:\n",
    "    print(f\"P(Dead | Low, search) overall:         {count_total['Dead']/total_sum:.3f}\")\n",
    "if from_high_sum > 0:\n",
    "    print(f\"P(Dead | Low, search, came from High): {count_from_high['Dead']/from_high_sum:.3f}\")\n",
    "print(f\"Theoretical:                            {1-beta:.3f}\")\n",
    "print()\n",
    "print(\"If these are approximately equal, the Markov property holds!\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Design Your Own MDP\n",
    "\n",
    "Create an MDP for a simple scenario: a student deciding between studying, partying, or sleeping. Define states, actions, transitions, and rewards."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a Student MDP\n",
    "# States: 'Rested', 'Tired', 'Exhausted'\n",
    "# Actions: 'study' (reward but tiring), 'party' (fun but very tiring), 'sleep' (no reward but restoring)\n",
    "# Transitions: studying from Rested -> 60% stay Rested, 40% become Tired, etc.\n",
    "\n",
    "student_mdp = MDP(\n",
    "    states=['Rested', 'Tired', 'Exhausted'],\n",
    "    actions={\n",
    "        'Rested':    ['study', 'party', 'sleep'],\n",
    "        'Tired':     ['study', 'party', 'sleep'],\n",
    "        'Exhausted': ['sleep'],  # can only sleep when exhausted\n",
    "    },\n",
    "    transitions={\n",
    "        # TODO: Fill in realistic transition probabilities\n",
    "        ('Rested', 'study'):     [('Rested', 0.6), ('Tired', 0.4)],\n",
    "        ('Rested', 'party'):     [('Tired', 0.7), ('Exhausted', 0.3)],\n",
    "        ('Rested', 'sleep'):     [('Rested', 1.0)],\n",
    "        ('Tired', 'study'):      [('Tired', 0.5), ('Exhausted', 0.5)],\n",
    "        ('Tired', 'party'):      [('Exhausted', 1.0)],\n",
    "        ('Tired', 'sleep'):      [('Rested', 0.8), ('Tired', 0.2)],\n",
    "        ('Exhausted', 'sleep'):  [('Tired', 0.7), ('Exhausted', 0.3)],\n",
    "    },\n",
    "    rewards={\n",
    "        # TODO: Fill in rewards that capture studying > partying > sleeping\n",
    "        ('Rested', 'study'):     3,\n",
    "        ('Rested', 'party'):     2,\n",
    "        ('Rested', 'sleep'):     0,\n",
    "        ('Tired', 'study'):      2,\n",
    "        ('Tired', 'party'):      1,\n",
    "        ('Tired', 'sleep'):      0,\n",
    "        ('Exhausted', 'sleep'):  0,\n",
    "    },\n",
    "    gamma=0.9\n",
    ")\n",
    "\n",
    "student_mdp.print_model()\n",
    "\n",
    "# TODO: Test different policies and compare discounted returns\n",
    "# Which is better: always study, or study when rested + sleep when tired?"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Now let us compute the **value of each state** under a given policy. This is the expected discounted return starting from that state."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_state_values(mdp, policy, start_state, n_episodes=5000, max_steps=50):\n",
    "    \"\"\"Estimate V(s) for each state by averaging returns over many episodes.\"\"\"\n",
    "    returns = defaultdict(list)\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        state = start_state\n",
    "        episode = []\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            if state == 'Dead' or state not in mdp.actions:\n",
    "                break\n",
    "            action = policy[state]\n",
    "            next_state, reward = mdp.step(state, action)\n",
    "            if state == 'Low' and action == 'search' and next_state == 'Dead':\n",
    "                reward = -3\n",
    "            episode.append((state, reward))\n",
    "            state = next_state\n",
    "\n",
    "        # Compute discounted returns for each state visited\n",
    "        G = 0\n",
    "        for state, reward in reversed(episode):\n",
    "            G = reward + mdp.gamma * G\n",
    "            returns[state].append(G)\n",
    "\n",
    "    # Average returns for each state\n",
    "    values = {}\n",
    "    for state in mdp.states:\n",
    "        if state in returns and len(returns[state]) > 0:\n",
    "            values[state] = np.mean(returns[state])\n",
    "        else:\n",
    "            values[state] = 0.0\n",
    "\n",
    "    return values\n",
    "\n",
    "# Compare policies\n",
    "policy_aggressive = {'High': 'search', 'Low': 'search'}\n",
    "policy_cautious = {'High': 'search', 'Low': 'recharge'}\n",
    "policy_lazy = {'High': 'wait', 'Low': 'wait'}\n",
    "\n",
    "policies = {\n",
    "    'Aggressive (always search)': policy_aggressive,\n",
    "    'Cautious (search if high, recharge if low)': policy_cautious,\n",
    "    'Lazy (always wait)': policy_lazy,\n",
    "}\n",
    "\n",
    "print(\"State Values Under Different Policies\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "all_values = {}\n",
    "for name, policy in policies.items():\n",
    "    values = estimate_state_values(recycling_robot, policy, 'High', n_episodes=10000)\n",
    "    all_values[name] = values\n",
    "    print(f\"\\n{name}:\")\n",
    "    for state in ['High', 'Low']:\n",
    "        print(f\"  V({state}) = {values[state]:.2f}\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize state values comparison\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "policy_names = list(all_values.keys())\n",
    "x = np.arange(len(policy_names))\n",
    "width = 0.35\n",
    "\n",
    "high_vals = [all_values[name]['High'] for name in policy_names]\n",
    "low_vals = [all_values[name]['Low'] for name in policy_names]\n",
    "\n",
    "bars1 = ax.bar(x - width/2, high_vals, width, label='V(High)', color='green', alpha=0.7)\n",
    "bars2 = ax.bar(x + width/2, low_vals, width, label='V(Low)', color='orange', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Policy', fontsize=12)\n",
    "ax.set_ylabel('State Value V(s)', fontsize=12)\n",
    "ax.set_title('State Values Under Different Policies\\n(Recycling Robot MDP)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Aggressive', 'Cautious', 'Lazy'], fontsize=11)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "            f'{bar.get_height():.1f}', ha='center', fontsize=10)\n",
    "for bar in bars2:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "            f'{bar.get_height():.1f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The cautious policy yields the highest state values -- it balances reward and risk.\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "Let us now find the optimal policy using **Monte Carlo policy evaluation with exploration**."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_policy(mdp, n_iterations=20000, max_steps=50):\n",
    "    \"\"\"Find the best policy by trying all state-action combinations.\"\"\"\n",
    "    # For this small MDP, we can enumerate all possible policies\n",
    "    active_states = [s for s in mdp.states if s in mdp.actions]\n",
    "\n",
    "    best_policy = None\n",
    "    best_value = -float('inf')\n",
    "    all_policies_results = []\n",
    "\n",
    "    # Generate all possible policies\n",
    "    from itertools import product\n",
    "    action_lists = [mdp.actions[s] for s in active_states]\n",
    "\n",
    "    for combo in product(*action_lists):\n",
    "        policy = dict(zip(active_states, combo))\n",
    "\n",
    "        # Evaluate this policy\n",
    "        values = estimate_state_values(mdp, policy, 'High', n_episodes=5000)\n",
    "        total_value = values.get('High', 0)\n",
    "\n",
    "        all_policies_results.append((policy.copy(), total_value))\n",
    "\n",
    "        if total_value > best_value:\n",
    "            best_value = total_value\n",
    "            best_policy = policy.copy()\n",
    "\n",
    "    return best_policy, best_value, all_policies_results\n",
    "\n",
    "print(\"Searching for optimal policy (evaluating all possible policies)...\")\n",
    "best_policy, best_value, all_results = find_best_policy(recycling_robot)\n",
    "\n",
    "print(f\"\\nOptimal Policy Found:\")\n",
    "for state, action in best_policy.items():\n",
    "    print(f\"  State: {state:5s} -> Action: {action}\")\n",
    "print(f\"\\n  V(High) under optimal policy: {best_value:.2f}\")\n",
    "\n",
    "# Show all policies ranked\n",
    "print(\"\\nAll Policies Ranked:\")\n",
    "all_results.sort(key=lambda x: x[1], reverse=True)\n",
    "for i, (policy, value) in enumerate(all_results):\n",
    "    actions_str = ', '.join(f\"{s}:{a}\" for s, a in policy.items())\n",
    "    marker = \" <-- BEST\" if i == 0 else \"\"\n",
    "    print(f\"  {i+1}. V(High)={value:>7.2f}  |  {actions_str}{marker}\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration: simulate the optimal policy for 100 steps\n",
    "np.random.seed(0)\n",
    "state = 'High'\n",
    "total_cans = 0\n",
    "steps_alive = 0\n",
    "\n",
    "state_history = []\n",
    "reward_history = []\n",
    "\n",
    "for t in range(100):\n",
    "    if state == 'Dead' or state not in recycling_robot.actions:\n",
    "        break\n",
    "\n",
    "    action = best_policy[state]\n",
    "    next_state, reward = recycling_robot.step(state, action)\n",
    "    if state == 'Low' and action == 'search' and next_state == 'Dead':\n",
    "        reward = -3\n",
    "\n",
    "    state_history.append(state)\n",
    "    reward_history.append(reward)\n",
    "    total_cans += max(0, reward)\n",
    "    steps_alive += 1\n",
    "    state = next_state\n",
    "\n",
    "# Plot the simulation\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
    "\n",
    "# State over time\n",
    "state_numeric = [1 if s == 'High' else 0 for s in state_history]\n",
    "ax1.step(range(len(state_history)), state_numeric, 'b-', linewidth=2)\n",
    "ax1.set_yticks([0, 1])\n",
    "ax1.set_yticklabels(['Low', 'High'])\n",
    "ax1.set_ylabel('Battery State', fontsize=12)\n",
    "ax1.set_title(f'Optimal Policy Simulation ({steps_alive} steps, {total_cans:.0f} cans collected)', fontsize=14, fontweight='bold')\n",
    "ax1.fill_between(range(len(state_history)), state_numeric, alpha=0.2)\n",
    "\n",
    "# Cumulative reward\n",
    "cumulative = np.cumsum(reward_history)\n",
    "ax2.plot(cumulative, 'g-', linewidth=2)\n",
    "ax2.set_xlabel('Time step', fontsize=12)\n",
    "ax2.set_ylabel('Cumulative Reward', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"The optimal policy kept the robot alive for {steps_alive} steps\")\n",
    "print(f\"and collected {total_cans:.0f} cans (cumulative reward: {cumulative[-1]:.1f}).\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY: MDPs, Rewards, and the Markov Property\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"What we built:\")\n",
    "print(\"  - A complete MDP class with states, actions, transitions, rewards\")\n",
    "print(\"  - The recycling robot MDP from Sutton & Barto\")\n",
    "print(\"  - Episodic and discounted return computation\")\n",
    "print(\"  - Monte Carlo state value estimation\")\n",
    "print(\"  - Exhaustive policy search for the optimal policy\")\n",
    "print()\n",
    "print(\"Key equations:\")\n",
    "print(\"  MDP = (S, A, P, R, gamma)\")\n",
    "print(\"  G_t = r_t + gamma * r_{t+1} + gamma^2 * r_{t+2} + ...\")\n",
    "print(\"  P(s' | s, a) -- the Markov property\")\n",
    "print()\n",
    "print(\"Key insight: The optimal policy balances immediate reward\")\n",
    "print(\"against long-term consequences. In our recycling robot,\")\n",
    "print(\"this means knowing when to recharge instead of searching.\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**Questions to think about:**\n",
    "1. What happens to the optimal policy if we increase the rescue penalty from -3 to -10? Does the robot become more cautious?\n",
    "2. How does changing gamma affect the optimal policy? Try gamma = 0.5 vs 0.99.\n",
    "3. Can you think of a real-world problem where the Markov property does NOT hold? What would you do in that case?\n",
    "\n",
    "**What comes next:**\n",
    "We now have the mathematical framework. In the next notebook, we will use OpenAI Gymnasium to interact with pre-built environments, and you will see how these MDP concepts map to real simulations. We will also explore why a random policy fails miserably, motivating the need for proper learning algorithms."
   ],
   "id": "cell_24"
  }
 ]
}