{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Basics of RL -- Notebook Index -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Reinforcement Learning -- Notebook Index\n",
    "\n",
    "## Vizuara -- RL From Scratch Course\n",
    "\n",
    "Welcome to the **Basics of Reinforcement Learning** notebook series. These three notebooks take you from zero RL knowledge to implementing a Q-learning agent that learns to balance a pole -- all from first principles.\n",
    "\n",
    "### Notebook 1: RL Foundations\n",
    "**File:** `01_rl_foundations.ipynb`\n",
    "\n",
    "- What makes RL different from supervised and unsupervised learning\n",
    "- The four elements: policy, reward, value function, model\n",
    "- Exploration vs exploitation\n",
    "- Build a Tic-Tac-Toe agent that learns through self-play\n",
    "\n",
    "### Notebook 2: MDPs, Rewards, and the Markov Property\n",
    "**File:** `02_mdps_rewards_and_markov_property.ipynb`\n",
    "\n",
    "- The agent-environment interface\n",
    "- Formal MDP definition: (S, A, P, R, gamma)\n",
    "- Episodic vs discounted returns\n",
    "- The Markov property and why it matters\n",
    "- Build and solve the recycling robot MDP\n",
    "\n",
    "### Notebook 3: Your First RL Agent with Gymnasium\n",
    "**File:** `03_gymnasium_first_rl_agent.ipynb`\n",
    "\n",
    "- OpenAI Gymnasium API (make, reset, step)\n",
    "- CartPole environment deep dive\n",
    "- Random agent vs heuristic agent vs Q-learning agent\n",
    "- Implement Q-learning from scratch\n",
    "- Quantitative comparison of all three approaches\n",
    "\n",
    "### Prerequisites\n",
    "- Python basics (variables, loops, functions, classes)\n",
    "- NumPy fundamentals\n",
    "- Basic probability (what a probability distribution is)\n",
    "\n",
    "### How to Use\n",
    "1. Open each notebook in Google Colab (no local setup needed)\n",
    "2. Run cells in order from top to bottom\n",
    "3. Complete the TODO exercises before checking solutions\n",
    "4. Reflect on the questions at the end of each notebook"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Basics of Reinforcement Learning -- Vizuara\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"Notebooks in this series:\")\n",
    "print(\"  1. RL Foundations (Tic-Tac-Toe agent)\")\n",
    "print(\"  2. MDPs, Rewards, and the Markov Property (Recycling Robot)\")\n",
    "print(\"  3. Your First RL Agent with Gymnasium (Q-Learning on CartPole)\")\n",
    "print()\n",
    "print(\"Start with Notebook 1 and work through in order.\")\n",
    "print(\"Each notebook builds on concepts from the previous one.\")"
   ],
   "id": "cell_2"
  }
 ]
}