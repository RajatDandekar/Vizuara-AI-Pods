{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Scaled Dot-Product Attention â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1RJjttCvltRK-j5XaI_Tp752cibGKRYMf\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/02_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_06_setting_up_inputs",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Setting Up Inputs\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_06_setting_up_inputs.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product Attention: Computing Attention from Scratch\n",
    "\n",
    "*Part 2 of the Vizuara series on Self-Attention from First Principles*\n",
    "*Estimated time: 45 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_02_why_it_matters",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_02_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebook, we learned how to project word embeddings into Query, Key, and Value vectors. But computing Q, K, and V is just the preparation. The real magic happens when we use them to compute **attention scores** â€” a mechanism that lets every word in a sentence look at every other word and decide how much to pay attention to it.\n",
    "\n",
    "The scaled dot-product attention formula is the single most important equation in modern NLP:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Implement every step of this formula from scratch\n",
    "- Work through a complete numerical example by hand and verify it with code\n",
    "- Understand *why* we scale by $\\sqrt{d_k}$ (and what goes wrong if we do not)\n",
    "- Visualize attention weight heatmaps on real sentences"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_03_building_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Building Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_03_building_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Here is the core idea of attention in three sentences:\n",
    "\n",
    "1. **Compare**: For every word, measure how relevant every other word is by comparing the word's Query against all other words' Keys (via dot product).\n",
    "2. **Normalize**: Convert these raw relevance scores into a probability distribution using softmax â€” so they sum to 1.\n",
    "3. **Aggregate**: Take a weighted average of all words' Values, using those probabilities as weights.\n",
    "\n",
    "The result? Each word's output is a **context-enriched representation** â€” it now contains information from other words, weighted by how relevant those words are.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "Why a weighted *average* of Values rather than just picking the single most relevant word? Because in natural language, meaning is often distributed across multiple words. The word \"it\" might need information from both \"cat\" (its referent) and \"tired\" (its state) to be fully understood."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_04_the_mathematics",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: The Mathematics\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_04_the_mathematics.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "The attention formula has four steps. Let us write them out explicitly.\n",
    "\n",
    "**Step 1: Raw attention scores** â€” $S = QK^T$\n",
    "\n",
    "This computes the dot product between every Query and every Key. If we have $n$ words each with $d_k$-dimensional Q and K vectors, $S$ is an $n \\times n$ matrix where $S_{ij}$ measures how much word $i$ should attend to word $j$.\n",
    "\n",
    "**What this says computationally:** Each element $S_{ij} = q_i \\cdot k_j = \\sum_{d=1}^{d_k} q_{i,d} \\cdot k_{j,d}$. When two vectors point in similar directions, their dot product is large (high attention). When they are orthogonal, it is zero (no attention).\n",
    "\n",
    "**Step 2: Scaling** â€” $S_{\\text{scaled}} = S / \\sqrt{d_k}$\n",
    "\n",
    "**What this says computationally:** Divide every element of $S$ by the square root of the key dimension. This prevents the dot products from becoming too large when $d_k$ is large.\n",
    "\n",
    "**Step 3: Softmax** â€” $A = \\text{softmax}(S_{\\text{scaled}})$\n",
    "\n",
    "**What this says computationally:** For each row (each Query word), apply softmax so the attention weights become a probability distribution that sums to 1:\n",
    "\n",
    "$$A_{ij} = \\frac{e^{S_{ij} / \\sqrt{d_k}}}{\\sum_{j'=1}^{n} e^{S_{ij'} / \\sqrt{d_k}}}$$\n",
    "\n",
    "**Step 4: Weighted sum** â€” $\\text{Output} = A \\cdot V$\n",
    "\n",
    "**What this says computationally:** For each word $i$, the output is a weighted combination of all Value vectors: $\\text{output}_i = \\sum_{j=1}^{n} A_{ij} \\cdot v_j$. Words with higher attention weights contribute more."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_05_build_it_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Build It Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_05_build_it_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It â€” Component by Component\n",
    "\n",
    "### 4.1 Setting Up the Inputs\n",
    "\n",
    "Let us use the \"I love math\" example from the article, with known Q, K, V matrices."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# The sentence\n",
    "words = [\"I\", \"love\", \"math\"]\n",
    "n = len(words)\n",
    "d_k = 2  # Dimension of Q, K, V vectors\n",
    "\n",
    "# Queries (one per word)\n",
    "Q = torch.tensor([\n",
    "    [1.0, 0.0],  # \"I\"\n",
    "    [0.0, 1.0],  # \"love\"\n",
    "    [1.0, 1.0],  # \"math\"\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Keys (one per word)\n",
    "K = torch.tensor([\n",
    "    [0.0, 1.0],  # \"I\"\n",
    "    [1.0, 0.0],  # \"love\"\n",
    "    [1.0, 1.0],  # \"math\"\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Values (one per word)\n",
    "V = torch.tensor([\n",
    "    [1.0, 2.0],  # \"I\"\n",
    "    [3.0, 4.0],  # \"love\"\n",
    "    [5.0, 6.0],  # \"math\"\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(\"Q (Queries):\\n\", Q)\n",
    "print(\"\\nK (Keys):\\n\", K)\n",
    "print(\"\\nV (Values):\\n\", V)"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_07_step1_heading",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Step1 Heading\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_07_step1_heading.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Step 1: Compute Raw Attention Scores ($QK^T$)"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_08_step1_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Step1 Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_08_step1_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: QK^T\n",
    "# Q has shape (3, 2), K^T has shape (2, 3)\n",
    "# Result has shape (3, 3) â€” one score for every word pair\n",
    "\n",
    "raw_scores = Q @ K.T  # Matrix multiplication\n",
    "\n",
    "print(\"Raw attention scores (QK^T):\")\n",
    "print(raw_scores)\n",
    "print(f\"\\nShape: {raw_scores.shape} â€” a {n}x{n} matrix\")\n",
    "\n",
    "# Let's verify one element manually:\n",
    "# Q[0] dot K[0] = [1, 0] dot [0, 1] = 0\n",
    "# Q[0] dot K[1] = [1, 0] dot [1, 0] = 1\n",
    "# Q[0] dot K[2] = [1, 0] dot [1, 1] = 1\n",
    "print(f\"\\nManual check row 0:\")\n",
    "print(f\"  Q['I'] Â· K['I']    = {(Q[0] * K[0]).sum().item()}\")\n",
    "print(f\"  Q['I'] Â· K['love'] = {(Q[0] * K[1]).sum().item()}\")\n",
    "print(f\"  Q['I'] Â· K['math'] = {(Q[0] * K[2]).sum().item()}\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_09_step1_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Step1 Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_09_step1_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize raw attention scores\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "im = ax.imshow(raw_scores.numpy(), cmap='YlOrRd', aspect='equal')\n",
    "ax.set_xticks(range(n))\n",
    "ax.set_xticklabels(words, fontsize=12)\n",
    "ax.set_yticks(range(n))\n",
    "ax.set_yticklabels(words, fontsize=12)\n",
    "ax.set_xlabel('Keys (attending TO)', fontsize=12)\n",
    "ax.set_ylabel('Queries (attending FROM)', fontsize=12)\n",
    "ax.set_title('Raw Attention Scores (QK^T)', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        ax.text(j, i, f'{raw_scores[i,j].item():.1f}',\n",
    "                ha='center', va='center', fontsize=14, fontweight='bold',\n",
    "                color='white' if raw_scores[i,j].item() > 1 else 'black')\n",
    "\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_10_step2_heading",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Step2 Heading\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_10_step2_heading.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Step 2: Scaling by $\\sqrt{d_k}$"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_11_step2_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Step2 Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_11_step2_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Scale by sqrt(d_k)\n",
    "scale = math.sqrt(d_k)\n",
    "scaled_scores = raw_scores / scale\n",
    "\n",
    "print(f\"Scale factor: sqrt({d_k}) = {scale:.3f}\")\n",
    "print(f\"\\nScaled scores:\")\n",
    "print(scaled_scores)"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_12_why_scaling_matters_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why Scaling Matters Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_12_why_scaling_matters_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see *why* scaling matters with a demonstration."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_13_scaling_demo_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Scaling Demo Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_13_scaling_demo_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_32_training_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Training Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_32_training_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHY SCALING MATTERS: demonstration\n",
    "# With large d_k, dot products get very large, making softmax nearly one-hot\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, d_k_demo in zip(axes, [2, 64, 512]):\n",
    "    torch.manual_seed(42)\n",
    "    # Random Q and K of dimension d_k_demo\n",
    "    Q_demo = torch.randn(1, d_k_demo)\n",
    "    K_demo = torch.randn(5, d_k_demo)\n",
    "\n",
    "    # Without scaling\n",
    "    scores_unscaled = (Q_demo @ K_demo.T).squeeze()\n",
    "    probs_unscaled = F.softmax(scores_unscaled, dim=0)\n",
    "\n",
    "    # With scaling\n",
    "    scores_scaled = scores_unscaled / math.sqrt(d_k_demo)\n",
    "    probs_scaled = F.softmax(scores_scaled, dim=0)\n",
    "\n",
    "    x_pos = np.arange(5)\n",
    "    width = 0.35\n",
    "    ax.bar(x_pos - width/2, probs_unscaled.numpy(), width, label='Unscaled', color='#e74c3c', alpha=0.8)\n",
    "    ax.bar(x_pos + width/2, probs_scaled.numpy(), width, label='Scaled', color='#3498db', alpha=0.8)\n",
    "    ax.set_title(f'd_k = {d_k_demo}', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Softmax probability')\n",
    "    ax.set_xlabel('Key index')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "plt.suptitle('Scaling Prevents Softmax from Becoming One-Hot', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Notice: without scaling, as d_k grows, softmax collapses to nearly one-hot.\")\n",
    "print(\"Scaling keeps the distribution smoother, allowing gradients to flow.\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_14_step3_heading",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Step3 Heading\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_14_step3_heading.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Step 3: Softmax"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_15_step3_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Step3 Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_15_step3_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply softmax row-wise\n",
    "attention_weights = F.softmax(scaled_scores, dim=-1)\n",
    "\n",
    "print(\"Attention weights (after softmax):\")\n",
    "print(attention_weights)\n",
    "print(f\"\\nRow sums (should all be 1.0): {attention_weights.sum(dim=1).tolist()}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_16_verify_softmax_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Verify Softmax Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_16_verify_softmax_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify one row manually:"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_17_verify_softmax_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Verify Softmax Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_17_verify_softmax_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual softmax for Row 1 (word \"I\")\n",
    "row1_scaled = scaled_scores[0]  # [0, 0.707, 0.707]\n",
    "exp_values = torch.exp(row1_scaled)\n",
    "softmax_manual = exp_values / exp_values.sum()\n",
    "\n",
    "print(f\"Scaled scores for 'I': {row1_scaled.tolist()}\")\n",
    "print(f\"exp() values: {exp_values.tolist()}\")\n",
    "print(f\"Sum of exp(): {exp_values.sum().item():.3f}\")\n",
    "print(f\"Manual softmax: {softmax_manual.tolist()}\")\n",
    "print(f\"PyTorch softmax: {attention_weights[0].tolist()}\")\n",
    "print(f\"Match: {torch.allclose(softmax_manual, attention_weights[0], atol=1e-4)}\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_18_step4_heading",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Step4 Heading\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_18_step4_heading.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Step 4: Weighted Sum of Values"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_19_step4_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Step4 Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_19_step4_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Output = Attention weights * V\n",
    "output = attention_weights @ V\n",
    "\n",
    "print(\"Output (context-enriched representations):\")\n",
    "print(output)\n",
    "print(f\"\\nShape: {output.shape} â€” one output vector per word\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_20_verify_weighted_sum_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Verify Weighted Sum Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_20_verify_weighted_sum_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify one output row manually:"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_21_verify_weighted_sum_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Verify Weighted Sum Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_21_verify_weighted_sum_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual computation for Row 3 (\"math\")\n",
    "a_math = attention_weights[2]  # attention weights for \"math\"\n",
    "print(f\"Attention weights for 'math': {a_math.tolist()}\")\n",
    "print(f\"\\nWeighted sum:\")\n",
    "print(f\"  {a_math[0].item():.3f} * V['I']    = {a_math[0].item():.3f} * {V[0].tolist()} = {(a_math[0]*V[0]).tolist()}\")\n",
    "print(f\"  {a_math[1].item():.3f} * V['love']  = {a_math[1].item():.3f} * {V[1].tolist()} = {(a_math[1]*V[1]).tolist()}\")\n",
    "print(f\"  {a_math[2].item():.3f} * V['math']  = {a_math[2].item():.3f} * {V[2].tolist()} = {(a_math[2]*V[2]).tolist()}\")\n",
    "output_math_manual = a_math[0]*V[0] + a_math[1]*V[1] + a_math[2]*V[2]\n",
    "print(f\"\\n  Sum = {output_math_manual.tolist()}\")\n",
    "print(f\"  PyTorch output: {output[2].tolist()}\")\n",
    "print(f\"  Match: {torch.allclose(output_math_manual, output[2], atol=1e-3)}\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_22_comprehensive_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Comprehensive Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_22_comprehensive_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive attention visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Attention heatmap\n",
    "ax = axes[0]\n",
    "im = ax.imshow(attention_weights.detach().numpy(), cmap='Blues', vmin=0, vmax=0.6)\n",
    "ax.set_xticks(range(n))\n",
    "ax.set_xticklabels(words, fontsize=12)\n",
    "ax.set_yticks(range(n))\n",
    "ax.set_yticklabels(words, fontsize=12)\n",
    "ax.set_xlabel('Keys (attending TO)')\n",
    "ax.set_ylabel('Queries (attending FROM)')\n",
    "ax.set_title('Attention Weights', fontsize=13, fontweight='bold')\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        ax.text(j, i, f'{attention_weights[i,j].item():.3f}',\n",
    "                ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "# 2. Values\n",
    "ax = axes[1]\n",
    "im2 = ax.imshow(V.numpy(), cmap='RdBu_r', aspect='auto')\n",
    "ax.set_xticks(range(d_k))\n",
    "ax.set_xticklabels([f'Dim {i}' for i in range(d_k)])\n",
    "ax.set_yticks(range(n))\n",
    "ax.set_yticklabels(words, fontsize=12)\n",
    "ax.set_title('Value Vectors', fontsize=13, fontweight='bold')\n",
    "for i in range(n):\n",
    "    for j in range(d_k):\n",
    "        ax.text(j, i, f'{V[i,j].item():.1f}', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im2, ax=ax, shrink=0.8)\n",
    "\n",
    "# 3. Output\n",
    "ax = axes[2]\n",
    "im3 = ax.imshow(output.detach().numpy(), cmap='RdBu_r', aspect='auto')\n",
    "ax.set_xticks(range(d_k))\n",
    "ax.set_xticklabels([f'Dim {i}' for i in range(d_k)])\n",
    "ax.set_yticks(range(n))\n",
    "ax.set_yticklabels(words, fontsize=12)\n",
    "ax.set_title('Output (Weighted Sum)', fontsize=13, fontweight='bold')\n",
    "for i in range(n):\n",
    "    for j in range(d_k):\n",
    "        ax.text(j, i, f'{output[i,j].item():.2f}', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im3, ax=ax, shrink=0.8)\n",
    "\n",
    "plt.suptitle('From Attention Weights to Context-Enriched Outputs', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_23_todo_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_23_todo_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Implement Scaled Dot-Product Attention from Scratch"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_24_todo1_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo1 Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_24_todo1_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "\n",
    "    Args:\n",
    "        Q: Query tensor of shape (seq_len, d_k)\n",
    "        K: Key tensor of shape (seq_len, d_k)\n",
    "        V: Value tensor of shape (seq_len, d_v)\n",
    "\n",
    "    Returns:\n",
    "        output: Weighted sum of V, shape (seq_len, d_v)\n",
    "        attention_weights: Softmax attention weights, shape (seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Compute raw scores: Q @ K^T\n",
    "    # Step 2: Scale by sqrt(d_k)\n",
    "    # Step 3: Apply softmax along the last dimension\n",
    "    # Step 4: Multiply by V\n",
    "    # ==============================\n",
    "\n",
    "    scores = ???       # YOUR CODE HERE\n",
    "    scaled = ???       # YOUR CODE HERE\n",
    "    attn_weights = ??? # YOUR CODE HERE\n",
    "    output = ???       # YOUR CODE HERE\n",
    "\n",
    "    return output, attn_weights"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_25_todo1_verification",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Todo1 Verification\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_25_todo1_verification.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "out_test, weights_test = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "# Check shapes\n",
    "assert out_test.shape == (3, 2), f\"Expected output shape (3, 2), got {out_test.shape}\"\n",
    "assert weights_test.shape == (3, 3), f\"Expected weights shape (3, 3), got {weights_test.shape}\"\n",
    "\n",
    "# Check that weights sum to 1 per row\n",
    "row_sums = weights_test.sum(dim=-1)\n",
    "assert torch.allclose(row_sums, torch.ones(3), atol=1e-5), f\"Weights don't sum to 1: {row_sums}\"\n",
    "\n",
    "# Check against known output\n",
    "expected_output = attention_weights @ V\n",
    "assert torch.allclose(out_test, expected_output, atol=1e-3), \"Output doesn't match expected values\"\n",
    "\n",
    "print(\"All assertions passed! Your attention implementation is correct!\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_26_todo2_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo2 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_26_todo2_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Add Causal Masking"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_27_todo2_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo2 Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_27_todo2_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention with optional causal mask.\n",
    "\n",
    "    A causal mask prevents words from attending to future words.\n",
    "    This is essential for autoregressive models like GPT.\n",
    "\n",
    "    Args:\n",
    "        Q: Query tensor of shape (seq_len, d_k)\n",
    "        K: Key tensor of shape (seq_len, d_k)\n",
    "        V: Value tensor of shape (seq_len, d_v)\n",
    "        mask: Boolean mask of shape (seq_len, seq_len).\n",
    "              True = position is allowed, False = position is masked.\n",
    "\n",
    "    Returns:\n",
    "        output: Shape (seq_len, d_v)\n",
    "        attention_weights: Shape (seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = Q @ K.T / math.sqrt(d_k)\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # If mask is provided:\n",
    "    #   Replace masked positions (where mask is False) with -1e9\n",
    "    #   This makes softmax assign ~0 probability to those positions\n",
    "    #\n",
    "    # Hint: scores = scores.masked_fill(~mask, float('-inf'))\n",
    "    # ==============================\n",
    "\n",
    "    if mask is not None:\n",
    "        pass  # YOUR CODE HERE\n",
    "\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = attn_weights @ V\n",
    "    return output, attn_weights"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_28_todo2_verification",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Todo2 Verification\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_28_todo2_verification.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with causal mask\n",
    "# Causal mask: word i can only attend to words j <= i\n",
    "causal_mask = torch.tril(torch.ones(3, 3, dtype=torch.bool))\n",
    "print(\"Causal mask (lower triangular):\")\n",
    "print(causal_mask.int())\n",
    "\n",
    "out_masked, weights_masked = masked_scaled_dot_product_attention(Q, K, V, mask=causal_mask)\n",
    "\n",
    "print(\"\\nMasked attention weights:\")\n",
    "print(weights_masked)\n",
    "print(\"\\nNotice: word 0 ('I') only attends to itself\")\n",
    "print(\"Word 1 ('love') attends to 'I' and 'love' but not 'math'\")\n",
    "print(\"Word 2 ('math') attends to all words\")\n",
    "\n",
    "# Verify masking worked\n",
    "assert weights_masked[0, 1].item() < 1e-6, \"Word 0 should not attend to word 1\"\n",
    "assert weights_masked[0, 2].item() < 1e-6, \"Word 0 should not attend to word 2\"\n",
    "assert weights_masked[1, 2].item() < 1e-6, \"Word 1 should not attend to word 2\"\n",
    "print(\"\\nAll masking assertions passed!\")"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_29_putting_it_together",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Putting It Together\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_29_putting_it_together.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_30_attention_module",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Attention Module\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_30_attention_module.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete scaled dot-product attention module.\n",
    "    Supports optional causal masking.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        d_k = Q.shape[-1]\n",
    "\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(~mask, float('-inf'))\n",
    "\n",
    "        # Softmax normalization\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "# Test the module\n",
    "attn_module = ScaledDotProductAttention()\n",
    "\n",
    "# Without mask\n",
    "out1, w1 = attn_module(Q, K, V)\n",
    "print(\"Without mask â€” attention weights:\")\n",
    "print(w1)\n",
    "\n",
    "# With causal mask\n",
    "out2, w2 = attn_module(Q, K, V, mask=causal_mask)\n",
    "print(\"\\nWith causal mask â€” attention weights:\")\n",
    "print(w2)"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_31_training_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Training Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_31_training_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "Let us apply self-attention to a real task: next-word prediction on a tiny corpus. We will train embeddings and QKV projections end-to-end."
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Tiny corpus\n",
    "vocab = ['<pad>', 'the', 'cat', 'sat', 'on', 'mat', 'dog', 'ran', 'in', 'park']\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "d_model = 16\n",
    "d_k = 8\n",
    "\n",
    "# Training data: predict last word from context\n",
    "# \"the cat sat\" -> \"on\", \"the dog ran\" -> \"in\"\n",
    "sequences = [\n",
    "    ([1, 2, 3], 4),    # the cat sat -> on\n",
    "    ([1, 6, 7], 8),    # the dog ran -> in\n",
    "    ([4, 1, 5], 3),    # on the mat -> sat\n",
    "    ([8, 1, 9], 7),    # in the park -> ran\n",
    "]\n",
    "\n",
    "# Model components\n",
    "embedding = nn.Embedding(vocab_size, d_model)\n",
    "W_q = nn.Linear(d_model, d_k, bias=False)\n",
    "W_k = nn.Linear(d_model, d_k, bias=False)\n",
    "W_v = nn.Linear(d_model, d_k, bias=False)\n",
    "output_proj = nn.Linear(d_k, vocab_size)\n",
    "\n",
    "params = list(embedding.parameters()) + list(W_q.parameters()) + \\\n",
    "         list(W_k.parameters()) + list(W_v.parameters()) + list(output_proj.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.01)\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(300):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for seq, target in sequences:\n",
    "        x = embedding(torch.tensor(seq))  # (3, d_model)\n",
    "\n",
    "        Q_t = W_q(x)\n",
    "        K_t = W_k(x)\n",
    "        V_t = W_v(x)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = Q_t @ K_t.T / math.sqrt(d_k)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        context = attn @ V_t  # (3, d_k)\n",
    "\n",
    "        # Use last position for prediction\n",
    "        logits = output_proj(context[-1])\n",
    "        loss = F.cross_entropy(logits.unsqueeze(0), torch.tensor([target]))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if logits.argmax().item() == target:\n",
    "            correct += 1\n",
    "\n",
    "    losses.append(total_loss / len(sequences))\n",
    "    accuracies.append(correct / len(sequences))\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"Final accuracy: {accuracies[-1]*100:.0f}%\")"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_33_training_plots",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Training Plots\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_33_training_plots.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(losses, color='#e74c3c', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(accuracies, color='#2ecc71', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "ax2.set_title('Training Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylim(-0.05, 1.05)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Self-Attention Learning Next-Word Prediction', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_34_final_output_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Final Output Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_34_final_output_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_35_final_output_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Final Output Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_35_final_output_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned attention patterns for each sequence\n",
    "fig, axes = plt.subplots(1, len(sequences), figsize=(5*len(sequences), 4))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (seq, target) in enumerate(sequences):\n",
    "        x = embedding(torch.tensor(seq))\n",
    "        Q_t = W_q(x)\n",
    "        K_t = W_k(x)\n",
    "        scores = Q_t @ K_t.T / math.sqrt(d_k)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        ax = axes[idx]\n",
    "        im = ax.imshow(attn.numpy(), cmap='Blues', vmin=0, vmax=1)\n",
    "        seq_words = [vocab[i] for i in seq]\n",
    "        ax.set_xticks(range(3))\n",
    "        ax.set_xticklabels(seq_words, fontsize=11)\n",
    "        ax.set_yticks(range(3))\n",
    "        ax.set_yticklabels(seq_words, fontsize=11)\n",
    "\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                ax.text(j, i, f'{attn[i,j].item():.2f}',\n",
    "                        ha='center', va='center', fontsize=10,\n",
    "                        color='white' if attn[i,j].item() > 0.5 else 'black')\n",
    "\n",
    "        ax.set_title(f'{\" \".join(seq_words)} -> {vocab[target]}', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Learned Attention Patterns', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCongratulations! You have built scaled dot-product attention from scratch!\")\n",
    "print(\"You can see how the model learns different attention patterns for different sequences.\")\n",
    "print(\"Next up: Multi-Head Attention â€” running multiple attention heads in parallel.\")"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_36_closing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Wrap-Up: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_36_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "1. Looking at the attention heatmaps, which word positions does the model attend to most when predicting the next word? Does this make intuitive sense?\n",
    "2. What would happen if we removed the scaling by $\\sqrt{d_k}$? Try it â€” comment out the scaling in the training loop and observe how training behaves.\n",
    "3. How does causal masking change the information available to each position? Why is this essential for autoregressive (left-to-right) generation?\n",
    "\n",
    "### Optional Challenges\n",
    "1. Implement **dropout** on the attention weights (a common technique in Transformers). Add `nn.Dropout(p=0.1)` after softmax and observe its effect on training stability.\n",
    "2. Extend the training to sequences of length 5 or more. Does the model still learn meaningful attention patterns?"
   ],
   "id": "cell_36"
  }
 ]
}