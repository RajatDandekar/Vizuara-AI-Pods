{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Multi-Head Attention & Positional Encoding â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1_yOuaRupWcvvBB5tNnjVrtDllXqg6x4Q\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/03_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_02_why_matter_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why Matter Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_02_why_matter_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention and Positional Encoding from Scratch\n",
    "\n",
    "*Part 3 of the Vizuara series on Self-Attention from First Principles*\n",
    "*Estimated time: 50 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_03_why_matter_pe",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why Matter Pe\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_03_why_matter_pe.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebook, we built scaled dot-product attention â€” a mechanism that lets every word attend to every other word. But a single attention head can only learn **one** pattern. What if the word \"it\" needs to simultaneously attend to \"animal\" (coreference), \"was\" (syntax), and \"tired\" (semantics)?\n",
    "\n",
    "This is where **Multi-Head Attention** comes in: we run multiple attention operations in parallel, each with its own learned weight matrices, so the model can capture multiple types of relationships simultaneously.\n",
    "\n",
    "There is also a subtle problem we have been ignoring: self-attention has **no concept of word order**. The mechanism treats \"dog bites man\" and \"man bites dog\" identically. **Positional Encoding** fixes this by injecting position information into the embeddings.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Implement Multi-Head Attention from scratch\n",
    "- Visualize what different heads learn (they specialize!)\n",
    "- Build sinusoidal positional encodings and understand their frequency structure\n",
    "- See how position information changes attention patterns"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_04_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_04_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Imagine you are an editor reviewing a manuscript. You need to check for:\n",
    "- **Grammar:** Does the subject agree with the verb?\n",
    "- **Coherence:** Do pronouns refer to the right nouns?\n",
    "- **Flow:** Does each paragraph connect logically to the next?\n",
    "\n",
    "No single pass can catch everything. You need to read the text multiple times, each time focusing on a different aspect. Multi-Head Attention does exactly this â€” it reads the sentence multiple times in parallel, each \"head\" looking for a different type of relationship.\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "Now imagine reading the same manuscript, but someone has shuffled all the words. Could you still understand it? Of course not â€” word order matters enormously. Self-attention, by itself, is like reading a bag of shuffled words. Positional encoding is the page numbering that tells the model where each word belongs.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "If we had infinite computational budget, would we prefer more heads with smaller dimensions, or fewer heads with larger dimensions? What are the trade-offs?"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_05_math",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_05_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_06_math_pe",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Math Pe\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_06_math_pe.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Given $h$ attention heads, the multi-head attention mechanism works as follows:\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(X W_i^Q, \\; X W_i^K, \\; X W_i^V)$$\n",
    "\n",
    "$$\\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) \\, W^O$$\n",
    "\n",
    "**What this says computationally:** Split the model dimension $d_{\\text{model}}$ into $h$ heads, each operating on $d_k = d_{\\text{model}} / h$ dimensions. Each head has its own $W_i^Q, W_i^K, W_i^V$ matrices. Run attention independently in each head, concatenate the results, and project back to $d_{\\text{model}}$ dimensions with $W^O$.\n",
    "\n",
    "### Sinusoidal Positional Encoding\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
    "\n",
    "**What this says computationally:** For each position $pos$ and each dimension index $i$, compute a sinusoidal value. Even dimensions use sine, odd dimensions use cosine. The frequency decreases as $i$ increases â€” early dimensions oscillate rapidly (capturing fine position differences) while later dimensions oscillate slowly (capturing coarse position differences)."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_07_single_head_recap",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Single Head Recap\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_07_single_head_recap.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It â€” Component by Component\n",
    "\n",
    "### 4.1 Single-Head Attention (Recap)"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def single_head_attention(Q, K, V, mask=None):\n",
    "    \"\"\"Scaled dot-product attention for a single head.\"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(~mask, float('-inf'))\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(weights, V)\n",
    "    return output, weights\n",
    "\n",
    "# Quick test\n",
    "Q_test = torch.randn(3, 4)\n",
    "K_test = torch.randn(3, 4)\n",
    "V_test = torch.randn(3, 4)\n",
    "out, w = single_head_attention(Q_test, K_test, V_test)\n",
    "print(f\"Single head output shape: {out.shape}\")\n",
    "print(f\"Attention weights shape: {w.shape}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_08_mha_class",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Mha Class\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_08_mha_class.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Multi-Head Attention â€” The Reshaping Trick\n",
    "\n",
    "The key insight in multi-head attention is that instead of creating separate weight matrices for each head, we use a single large weight matrix and **reshape** the output into multiple heads. This is mathematically equivalent but much more efficient on GPUs."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # Single large projections (equivalent to separate per-head projections)\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Step 1: Project to Q, K, V (all heads at once)\n",
    "        Q = self.W_q(x)  # (batch, seq, d_model)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        # Step 2: Reshape into multiple heads\n",
    "        # (batch, seq, d_model) -> (batch, seq, num_heads, d_k) -> (batch, num_heads, seq, d_k)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Step 3: Scaled dot-product attention (per head)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(~mask, float('-inf'))\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "\n",
    "        # Step 4: Concatenate heads\n",
    "        # (batch, num_heads, seq, d_k) -> (batch, seq, num_heads, d_k) -> (batch, seq, d_model)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        # Step 5: Output projection\n",
    "        output = self.W_o(context)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "# Create and test\n",
    "torch.manual_seed(42)\n",
    "d_model = 32\n",
    "num_heads = 4\n",
    "mha = MultiHeadSelfAttention(d_model, num_heads)\n",
    "\n",
    "x = torch.randn(1, 6, d_model)  # batch=1, seq=6, d_model=32\n",
    "output, attn_weights = mha(x)\n",
    "\n",
    "print(f\"Input shape:            {x.shape}\")\n",
    "print(f\"Output shape:           {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"  -> {num_heads} heads, each with a 6x6 attention matrix\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_09_mha_viz_random",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Mha Viz Random\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_09_mha_viz_random.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns from each head\n",
    "fig, axes = plt.subplots(1, num_heads, figsize=(4*num_heads, 4))\n",
    "words_demo = ['The', 'cat', 'sat', 'on', 'the', 'mat']\n",
    "\n",
    "for head_idx in range(num_heads):\n",
    "    ax = axes[head_idx]\n",
    "    weights_np = attn_weights[0, head_idx].detach().numpy()\n",
    "    im = ax.imshow(weights_np, cmap='Blues', vmin=0, vmax=weights_np.max())\n",
    "    ax.set_xticks(range(6))\n",
    "    ax.set_xticklabels(words_demo, rotation=45, fontsize=9)\n",
    "    ax.set_yticks(range(6))\n",
    "    ax.set_yticklabels(words_demo, fontsize=9)\n",
    "    ax.set_title(f'Head {head_idx+1}', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Attention Patterns Per Head (Random Init)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"These are random patterns because the model is untrained.\")\n",
    "print(\"After training, each head would specialize in a different relationship type.\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_10_reshape_viz_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Reshape Viz Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_10_reshape_viz_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The Reshaping Step â€” Visualized"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's trace the exact reshaping that happens\n",
    "batch_size, seq_len = 1, 4\n",
    "d_model_demo = 8\n",
    "num_heads_demo = 2\n",
    "d_k_demo = d_model_demo // num_heads_demo  # = 4\n",
    "\n",
    "# Start with a single projected Q (before reshape)\n",
    "Q_flat = torch.arange(1, batch_size * seq_len * d_model_demo + 1, dtype=torch.float32)\n",
    "Q_flat = Q_flat.view(batch_size, seq_len, d_model_demo)\n",
    "\n",
    "print(\"Q before reshape (batch=1, seq=4, d_model=8):\")\n",
    "print(Q_flat[0])\n",
    "\n",
    "# Step 1: View into (batch, seq, num_heads, d_k)\n",
    "Q_reshaped = Q_flat.view(batch_size, seq_len, num_heads_demo, d_k_demo)\n",
    "print(f\"\\nAfter view (batch=1, seq=4, heads=2, d_k=4):\")\n",
    "for h in range(num_heads_demo):\n",
    "    print(f\"  Head {h}: {Q_reshaped[0, :, h, :].tolist()}\")\n",
    "\n",
    "# Step 2: Transpose to (batch, num_heads, seq, d_k)\n",
    "Q_heads = Q_reshaped.transpose(1, 2)\n",
    "print(f\"\\nAfter transpose (batch=1, heads=2, seq=4, d_k=4):\")\n",
    "for h in range(num_heads_demo):\n",
    "    print(f\"  Head {h}:\")\n",
    "    for s in range(seq_len):\n",
    "        print(f\"    Position {s}: {Q_heads[0, h, s, :].tolist()}\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_11_reshape_viz_plot",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Reshape Viz Plot\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_11_reshape_viz_plot.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual diagram of the reshaping\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "# Before reshape\n",
    "ax = axes[0]\n",
    "data = Q_flat[0].numpy()\n",
    "im = ax.imshow(data, cmap='viridis', aspect='auto')\n",
    "ax.set_title('Before Reshape\\n(seq=4, d_model=8)', fontsize=11, fontweight='bold')\n",
    "ax.set_xlabel('d_model dimension')\n",
    "ax.set_ylabel('Sequence position')\n",
    "ax.set_xticks(range(8))\n",
    "ax.set_yticks(range(4))\n",
    "\n",
    "# Head 0\n",
    "ax = axes[1]\n",
    "data_h0 = Q_heads[0, 0].numpy()\n",
    "im = ax.imshow(data_h0, cmap='viridis', aspect='auto', vmin=1, vmax=32)\n",
    "ax.set_title('Head 0\\n(seq=4, d_k=4)', fontsize=11, fontweight='bold')\n",
    "ax.set_xlabel('d_k dimension')\n",
    "ax.set_ylabel('Sequence position')\n",
    "ax.set_xticks(range(4))\n",
    "ax.set_yticks(range(4))\n",
    "\n",
    "# Head 1\n",
    "ax = axes[2]\n",
    "data_h1 = Q_heads[0, 1].numpy()\n",
    "im = ax.imshow(data_h1, cmap='viridis', aspect='auto', vmin=1, vmax=32)\n",
    "ax.set_title('Head 1\\n(seq=4, d_k=4)', fontsize=11, fontweight='bold')\n",
    "ax.set_xlabel('d_k dimension')\n",
    "ax.set_ylabel('Sequence position')\n",
    "ax.set_xticks(range(4))\n",
    "ax.set_yticks(range(4))\n",
    "\n",
    "plt.suptitle('Multi-Head Reshaping: Splitting d_model into h Heads', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_12_sinusoidal_pe_class",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Sinusoidal Pe Class\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_12_sinusoidal_pe_class.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Sinusoidal Positional Encoding"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encoding as described in 'Attention Is All You Need'.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create a matrix of shape (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model)\n",
    "        )  # (d_model/2,)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even dimensions\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd dimensions\n",
    "\n",
    "        # Register as a buffer (not a parameter â€” not trained)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# Create and inspect\n",
    "pe_module = SinusoidalPositionalEncoding(d_model=64, max_len=100)\n",
    "print(f\"PE buffer shape: {pe_module.pe.shape}\")\n",
    "\n",
    "# Show the PE values for position 0 and position 3\n",
    "print(f\"\\nPE[0, :4] = {pe_module.pe[0, 0, :4].tolist()}\")\n",
    "print(f\"PE[3, :4] = {pe_module.pe[0, 3, :4].tolist()}\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_13_sinusoidal_pe_verify",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Sinusoidal Pe Verify\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_13_sinusoidal_pe_verify.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify the worked example from the article."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the hand-computed PE from the article\n",
    "# PE at position 3 with d_model=4\n",
    "d_model_small = 4\n",
    "pe_small = SinusoidalPositionalEncoding(d_model_small, max_len=10)\n",
    "\n",
    "pe_pos3 = pe_small.pe[0, 3, :].tolist()\n",
    "print(\"Positional encoding for position 3 (d_model=4):\")\n",
    "print(f\"  PE[3] = {[round(v, 3) for v in pe_pos3]}\")\n",
    "print(f\"\\nExpected from article:\")\n",
    "print(f\"  PE[3] = [sin(3), cos(3), sin(0.03), cos(0.03)]\")\n",
    "print(f\"        = [{math.sin(3):.3f}, {math.cos(3):.3f}, {math.sin(0.03):.3f}, {math.cos(0.03):.3f}]\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_14_sinusoidal_pe_heatmap",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Sinusoidal Pe Heatmap\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_14_sinusoidal_pe_heatmap.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the positional encoding heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "pe_data = pe_module.pe[0, :50, :64].numpy()  # First 50 positions, first 64 dims\n",
    "im = ax.imshow(pe_data, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "ax.set_xlabel('Dimension Index', fontsize=12)\n",
    "ax.set_ylabel('Position', fontsize=12)\n",
    "ax.set_title('Sinusoidal Positional Encoding', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax, label='Encoding Value')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left side: rapidly oscillating (fine-grained position)\")\n",
    "print(\"Right side: slowly oscillating (coarse position)\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_15_sinusoidal_pe_frequencies",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Sinusoidal Pe Frequencies\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_15_sinusoidal_pe_frequencies.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key property: relative positions can be represented as linear transformations\n",
    "# Let's show that PE(pos+k) can be expressed as a rotation of PE(pos)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "positions = list(range(20))\n",
    "pe_values = pe_module.pe[0, :20, :].numpy()\n",
    "\n",
    "# Plot sinusoidal waves at different frequencies\n",
    "for ax, dim_pair, title in zip(axes,\n",
    "                                [(0, 1), (10, 11), (30, 31)],\n",
    "                                ['Dimensions 0-1\\n(High Frequency)',\n",
    "                                 'Dimensions 10-11\\n(Medium Frequency)',\n",
    "                                 'Dimensions 30-31\\n(Low Frequency)']):\n",
    "    ax.plot(positions, pe_values[:, dim_pair[0]], 'o-', label=f'sin (dim {dim_pair[0]})',\n",
    "            color='#3498db', linewidth=2, markersize=4)\n",
    "    ax.plot(positions, pe_values[:, dim_pair[1]], 's-', label=f'cos (dim {dim_pair[1]})',\n",
    "            color='#e74c3c', linewidth=2, markersize=4)\n",
    "    ax.set_xlabel('Position', fontsize=11)\n",
    "    ax.set_ylabel('Value', fontsize=11)\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-1.2, 1.2)\n",
    "\n",
    "plt.suptitle('Sinusoidal Waves at Different Frequencies', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_16_pe_affect_attn_setup",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Pe Affect Attn Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_16_pe_affect_attn_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 How Positional Encoding Affects Attention"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_17_pe_affect_attn_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Pe Affect Attn Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_17_pe_affect_attn_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate that positional encoding changes attention patterns\n",
    "torch.manual_seed(42)\n",
    "\n",
    "d_model_test = 32\n",
    "seq_len_test = 8\n",
    "num_heads_test = 4\n",
    "\n",
    "# Create identical embeddings for all positions (same word repeated)\n",
    "# Without PE: attention should be uniform\n",
    "# With PE: attention should show positional bias\n",
    "embedding_test = nn.Embedding(1, d_model_test)\n",
    "same_word_input = torch.zeros(1, seq_len_test, dtype=torch.long)  # all same word\n",
    "x_no_pe = embedding_test(same_word_input)  # (1, 8, 32)\n",
    "\n",
    "# Create PE\n",
    "pe_test = SinusoidalPositionalEncoding(d_model_test)\n",
    "x_with_pe = pe_test(x_no_pe.clone())\n",
    "\n",
    "# Run through MHA\n",
    "mha_test = MultiHeadSelfAttention(d_model_test, num_heads_test)\n",
    "\n",
    "_, attn_no_pe = mha_test(x_no_pe)\n",
    "_, attn_with_pe = mha_test(x_with_pe)\n",
    "\n",
    "fig, axes = plt.subplots(2, num_heads_test, figsize=(4*num_heads_test, 8))\n",
    "\n",
    "for h in range(num_heads_test):\n",
    "    # Without PE\n",
    "    ax = axes[0][h]\n",
    "    im = ax.imshow(attn_no_pe[0, h].detach().numpy(), cmap='Blues', vmin=0)\n",
    "    ax.set_title(f'Head {h+1}', fontsize=11, fontweight='bold')\n",
    "    if h == 0:\n",
    "        ax.set_ylabel('Without PE', fontsize=12, fontweight='bold')\n",
    "\n",
    "    # With PE\n",
    "    ax = axes[1][h]\n",
    "    im = ax.imshow(attn_with_pe[0, h].detach().numpy(), cmap='Blues', vmin=0)\n",
    "    ax.set_title(f'Head {h+1}', fontsize=11, fontweight='bold')\n",
    "    if h == 0:\n",
    "        ax.set_ylabel('With PE', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Attention Patterns: Same Word at Every Position\\n'\n",
    "             'Without PE (top) vs With PE (bottom)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Without PE: attention is nearly uniform (no position info).\")\n",
    "print(\"With PE: each position attends differently based on relative distance.\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_18_todo1_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo1 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_18_todo1_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Implement Multi-Head Attention Without the Reshaping Trick"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_19_todo1_guidance",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo1 Guidance\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_19_todo1_guidance.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionExplicit(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention implemented with explicit separate heads.\n",
    "    This is less efficient but makes the logic crystal clear.\n",
    "\n",
    "    Args:\n",
    "        d_model: Total model dimension\n",
    "        num_heads: Number of attention heads\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # ============ TODO ============\n",
    "        # Create separate W_q, W_k, W_v for EACH head\n",
    "        # Use nn.ModuleList to hold them\n",
    "        # Each projects from d_model to d_k\n",
    "        #\n",
    "        # Also create the output projection W_o\n",
    "        # from d_model to d_model\n",
    "        # ==============================\n",
    "\n",
    "        self.W_q_heads = ???  # YOUR CODE HERE: nn.ModuleList of nn.Linear\n",
    "        self.W_k_heads = ???  # YOUR CODE HERE\n",
    "        self.W_v_heads = ???  # YOUR CODE HERE\n",
    "        self.W_o = ???        # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input of shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            output: Shape (batch_size, seq_len, d_model)\n",
    "            attention_weights: Shape (batch_size, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        head_outputs = []\n",
    "        all_weights = []\n",
    "\n",
    "        # ============ TODO ============\n",
    "        # For each head i:\n",
    "        #   1. Compute Q_i = self.W_q_heads[i](x)\n",
    "        #   2. Compute K_i = self.W_k_heads[i](x)\n",
    "        #   3. Compute V_i = self.W_v_heads[i](x)\n",
    "        #   4. Compute attention: scores = Q_i @ K_i.T / sqrt(d_k)\n",
    "        #   5. Apply softmax\n",
    "        #   6. Compute output = weights @ V_i\n",
    "        #   7. Collect the output and weights\n",
    "        #\n",
    "        # After the loop:\n",
    "        #   8. Concatenate all head outputs along the last dimension\n",
    "        #   9. Apply W_o\n",
    "        # ==============================\n",
    "\n",
    "        for i in range(self.num_heads):\n",
    "            pass  # YOUR CODE HERE\n",
    "\n",
    "        # Concatenate and project\n",
    "        output = ???  # YOUR CODE HERE\n",
    "        attn = ???    # YOUR CODE HERE\n",
    "\n",
    "        return output, attn"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_20_todo1_verify",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Todo1 Verify\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_20_todo1_verify.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_23_todo2_verify",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Todo2 Verify\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_23_todo2_verify.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: compare explicit and efficient implementations\n",
    "torch.manual_seed(42)\n",
    "d_m = 16\n",
    "n_h = 4\n",
    "x_verify = torch.randn(2, 5, d_m)\n",
    "\n",
    "mha_efficient = MultiHeadSelfAttention(d_m, n_h)\n",
    "out_eff, w_eff = mha_efficient(x_verify)\n",
    "\n",
    "# If you implemented the explicit version, test it:\n",
    "# mha_explicit = MultiHeadAttentionExplicit(d_m, n_h)\n",
    "# out_exp, w_exp = mha_explicit(x_verify)\n",
    "# print(f\"Efficient output shape: {out_eff.shape}\")\n",
    "# print(f\"Explicit output shape: {out_exp.shape}\")\n",
    "\n",
    "print(f\"Output shape: {out_eff.shape}\")\n",
    "assert out_eff.shape == (2, 5, d_m), f\"Expected (2, 5, {d_m}), got {out_eff.shape}\"\n",
    "assert w_eff.shape == (2, n_h, 5, 5), f\"Expected (2, {n_h}, 5, 5), got {w_eff.shape}\"\n",
    "print(\"Shape assertions passed!\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_21_todo2_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo2 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_21_todo2_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement Learned Positional Encoding"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_22_todo2_guidance",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo2 Guidance\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_22_todo2_guidance.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Learned (trainable) positional encoding.\n",
    "    Unlike sinusoidal, these are trained along with the model.\n",
    "    Used in BERT and GPT-2.\n",
    "\n",
    "    Args:\n",
    "        d_model: Embedding dimension\n",
    "        max_len: Maximum sequence length\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        # ============ TODO ============\n",
    "        # Create a learnable embedding table for positions\n",
    "        # Hint: nn.Embedding(max_len, d_model)\n",
    "        # ==============================\n",
    "\n",
    "        self.position_embedding = ???  # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input of shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            x + positional encoding\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # ============ TODO ============\n",
    "        # 1. Create position indices: [0, 1, 2, ..., seq_len-1]\n",
    "        # 2. Look up the positional embeddings\n",
    "        # 3. Add them to x\n",
    "        # ==============================\n",
    "\n",
    "        positions = ???  # YOUR CODE HERE\n",
    "        pe = ???         # YOUR CODE HERE\n",
    "        return x + pe"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "lpe = LearnedPositionalEncoding(d_model=32, max_len=100)\n",
    "test_x = torch.randn(2, 10, 32)\n",
    "out_lpe = lpe(test_x)\n",
    "\n",
    "assert out_lpe.shape == test_x.shape, f\"Shape mismatch: {out_lpe.shape} vs {test_x.shape}\"\n",
    "assert not torch.allclose(out_lpe, test_x), \"Output should differ from input (PE added)\"\n",
    "print(\"Learned PE assertions passed!\")\n",
    "\n",
    "# Compare: sinusoidal is deterministic, learned is random at init\n",
    "spe = SinusoidalPositionalEncoding(d_model=32)\n",
    "out_spe = spe(test_x)\n",
    "print(f\"\\nSinusoidal PE (deterministic): first 4 values = {out_spe[0, 0, :4].tolist()}\")\n",
    "print(f\"Learned PE (random init): first 4 values = {out_lpe[0, 0, :4].tolist()}\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_24_putting_together",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Putting Together\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_24_putting_together.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerInput(nn.Module):\n",
    "    \"\"\"\n",
    "    Full input processing pipeline for a Transformer:\n",
    "    Embedding + Positional Encoding -> Multi-Head Self-Attention\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, num_heads, max_len=512):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = SinusoidalPositionalEncoding(d_model, max_len)\n",
    "        self.attention = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, token_ids, mask=None):\n",
    "        # Step 1: Token embedding\n",
    "        x = self.embedding(token_ids) * math.sqrt(self.d_model)  # Scale by sqrt(d_model)\n",
    "\n",
    "        # Step 2: Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        # Step 3: Multi-Head Self-Attention\n",
    "        output, attention_weights = self.attention(x, mask)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "# Create and test\n",
    "torch.manual_seed(42)\n",
    "model = TransformerInput(vocab_size=100, d_model=32, num_heads=4)\n",
    "\n",
    "# Fake token IDs for \"The cat sat on the mat\"\n",
    "token_ids = torch.tensor([[1, 5, 23, 8, 1, 42]])  # batch=1, seq=6\n",
    "output, attn_weights = model(token_ids)\n",
    "\n",
    "print(f\"Token IDs shape:        {token_ids.shape}\")\n",
    "print(f\"Output shape:           {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_25_combined_attn_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Combined Attn Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_25_combined_attn_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the complete pipeline\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "word_labels = ['The', 'cat', 'sat', 'on', 'the', 'mat']\n",
    "\n",
    "for h in range(4):\n",
    "    ax = axes[h // 2][h % 2]\n",
    "    w = attn_weights[0, h].detach().numpy()\n",
    "    im = ax.imshow(w, cmap='Blues', vmin=0, vmax=w.max())\n",
    "    ax.set_xticks(range(6))\n",
    "    ax.set_xticklabels(word_labels, rotation=45, fontsize=10)\n",
    "    ax.set_yticks(range(6))\n",
    "    ax.set_yticklabels(word_labels, fontsize=10)\n",
    "    ax.set_title(f'Head {h+1}', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "plt.suptitle('Multi-Head Attention: \"The cat sat on the mat\"\\n'\n",
    "             '(With Positional Encoding, Random Init)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_26_training_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Training Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_26_training_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "Let us train the model on a word-ordering task to see heads specialize."
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Task: given a scrambled sentence, predict the correct order\n",
    "# We train the attention to learn positional relationships\n",
    "\n",
    "vocab = ['<pad>', 'the', 'cat', 'sat', 'on', 'mat', 'dog', 'ran', 'to', 'park']\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "d_model = 32\n",
    "num_heads = 4\n",
    "\n",
    "# Full model\n",
    "model = TransformerInput(vocab_size, d_model, num_heads)\n",
    "classifier = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "params = list(model.parameters()) + list(classifier.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.005)\n",
    "\n",
    "# Training sequences (predict next word)\n",
    "sequences = [\n",
    "    ([1, 2, 3, 4, 1], 5),    # the cat sat on the -> mat\n",
    "    ([1, 6, 7, 8, 1], 9),    # the dog ran to the -> park\n",
    "    ([4, 1, 5, 1, 2], 3),    # on the mat the cat -> sat\n",
    "    ([8, 1, 9, 1, 6], 7),    # to the park the dog -> ran\n",
    "]\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(400):\n",
    "    total_loss = 0\n",
    "    for seq, target in sequences:\n",
    "        token_ids = torch.tensor([seq])\n",
    "        out, _ = model(token_ids)\n",
    "        logits = classifier(out[0, -1])  # Last position\n",
    "        loss = F.cross_entropy(logits.unsqueeze(0), torch.tensor([target]))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    losses.append(total_loss / len(sequences))\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_27_training_curve_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Training Curve Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_27_training_curve_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(losses, color='#3498db', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('Training Multi-Head Attention for Next-Word Prediction', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_28_final_output_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Final Output Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_28_final_output_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_29_final_output_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Final Output Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_29_final_output_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned attention patterns after training\n",
    "fig, axes = plt.subplots(len(sequences), num_heads, figsize=(4*num_heads, 4*len(sequences)))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for s_idx, (seq, target) in enumerate(sequences):\n",
    "        token_ids = torch.tensor([seq])\n",
    "        _, attn = model(token_ids)\n",
    "        seq_words = [vocab[i] for i in seq]\n",
    "\n",
    "        for h in range(num_heads):\n",
    "            ax = axes[s_idx][h]\n",
    "            w = attn[0, h].numpy()\n",
    "            im = ax.imshow(w, cmap='Blues', vmin=0, vmax=w.max())\n",
    "            ax.set_xticks(range(5))\n",
    "            ax.set_xticklabels(seq_words, rotation=45, fontsize=8)\n",
    "            ax.set_yticks(range(5))\n",
    "            ax.set_yticklabels(seq_words, fontsize=8)\n",
    "            if s_idx == 0:\n",
    "                ax.set_title(f'Head {h+1}', fontsize=11, fontweight='bold')\n",
    "            if h == 0:\n",
    "                ax.set_ylabel(f'{\" \".join(seq_words)}\\n->{vocab[target]}',\n",
    "                            fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Learned Multi-Head Attention Patterns\\n'\n",
    "             'Each Head Specializes in Different Relationships',\n",
    "             fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCongratulations! You have built Multi-Head Attention and Positional Encoding from scratch!\")\n",
    "print(\"Notice how different heads have learned different attention patterns.\")\n",
    "print(\"Next up: assembling the full Transformer block with residual connections,\")\n",
    "print(\"layer normalization, and the feed-forward network.\")"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_30_closing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Wrap-Up: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_30_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "1. Why is the standard practice to set $d_k = d_{\\text{model}} / h$ rather than having each head use the full $d_{\\text{model}}$ dimensions? What happens to computation cost if each head uses the full dimension?\n",
    "2. Compare sinusoidal and learned positional encodings. What are the advantages of each? (Hint: think about generalization to longer sequences.)\n",
    "3. In the trained attention patterns, do different heads appear to focus on different things? What patterns can you identify?\n",
    "\n",
    "### Optional Challenges\n",
    "1. Implement **Rotary Positional Encoding (RoPE)** â€” the positional encoding used in LLaMA and modern LLMs. It encodes relative position directly into the attention computation.\n",
    "2. Try training with different numbers of heads (1, 2, 4, 8) and compare the final loss. Is there a sweet spot?"
   ],
   "id": "cell_35"
  }
 ]
}