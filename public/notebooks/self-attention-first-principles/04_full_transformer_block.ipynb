{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Full Transformer Block â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"17rFuCNZUUY1xHrMq1WTamV-JWh_IDZe8\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/04_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_06_layernorm_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Layernorm Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_06_layernorm_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_02_why_it_matters",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_02_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Full Transformer Block: From Attention to Architecture\n",
    "\n",
    "*Part 4 of the Vizuara series on Self-Attention from First Principles*\n",
    "*Estimated time: 55 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Over the last three notebooks, we have built every component of self-attention from the ground up: QKV projections, scaled dot-product attention, multi-head attention, and positional encoding. Now it is time to assemble these pieces into the **Transformer block** â€” the fundamental building block of GPT, BERT, LLaMA, and every other modern language model.\n",
    "\n",
    "A single Transformer block adds two critical ingredients to our attention mechanism: **residual connections** (skip connections) and **layer normalization**. Together with a **feed-forward network**, these form a complete Transformer layer. Stack 6, 12, or 96 of these blocks, and you have a state-of-the-art language model.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Build a complete Transformer encoder block from scratch\n",
    "- Train it on a real text classification task (IMDB sentiment)\n",
    "- Visualize attention patterns on actual English sentences\n",
    "- Understand why residual connections and layer norm are essential"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_03_building_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Building Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_03_building_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Think of building a Transformer block like constructing a building floor:\n",
    "\n",
    "1. **Multi-Head Attention** is the main conference room where everyone shares information with everyone else.\n",
    "2. **Residual Connection** is the elevator shaft â€” it lets information bypass the conference room and flow directly to the next floor. Even if the conference room adds nothing useful, the original information is preserved.\n",
    "3. **Layer Normalization** is the HVAC system â€” it keeps the \"temperature\" (scale of activations) consistent across all rooms on the floor.\n",
    "4. **Feed-Forward Network** is the private offices where each person processes information independently after the group discussion.\n",
    "\n",
    "Stack these floors, and each floor refines the representations further.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "What would happen if we removed the residual connections? Without them, every piece of information must pass *through* the attention and FFN layers â€” if those layers are poorly initialized, the signal gets corrupted. The residual connection guarantees that at worst, the block is the identity function â€” it cannot make things worse."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_04_the_mathematics",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: The Mathematics\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_04_the_mathematics.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "A single Transformer encoder block computes:\n",
    "\n",
    "**Sub-layer 1: Multi-Head Attention + Add & Norm**\n",
    "\n",
    "$$\\text{sublayer}_1 = \\text{LayerNorm}(x + \\text{MultiHead}(x))$$\n",
    "\n",
    "**Sub-layer 2: Feed-Forward Network + Add & Norm**\n",
    "\n",
    "$$\\text{sublayer}_2 = \\text{LayerNorm}(\\text{sublayer}_1 + \\text{FFN}(\\text{sublayer}_1))$$\n",
    "\n",
    "where the FFN is:\n",
    "\n",
    "$$\\text{FFN}(x) = \\max(0, \\; x W_1 + b_1) \\, W_2 + b_2$$\n",
    "\n",
    "**What this says computationally:** Take the input $x$, run it through multi-head attention, then add the original $x$ back (residual connection), and normalize. Feed the result through a two-layer neural network (with ReLU activation), add another residual connection, and normalize again. The output has the same shape as the input â€” $(n, d_{\\text{model}})$ â€” so blocks can be stacked.\n",
    "\n",
    "The FFN typically has a hidden dimension of $4 \\times d_{\\text{model}}$. In the original Transformer with $d_{\\text{model}} = 512$, the FFN hidden size is 2048."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_05_layernorm_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Layernorm Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_05_layernorm_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It â€” Component by Component\n",
    "\n",
    "### 4.1 Layer Normalization"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# First, let's understand what Layer Normalization does\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"Layer Normalization implemented from scratch.\"\"\"\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))   # Learnable scale\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))    # Learnable shift\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute mean and variance along the last dimension\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "\n",
    "        # Normalize\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "        # Scale and shift\n",
    "        return self.gamma * x_norm + self.beta\n",
    "\n",
    "# Compare our implementation with PyTorch's\n",
    "torch.manual_seed(42)\n",
    "x_test = torch.randn(2, 5, 16)\n",
    "\n",
    "our_ln = LayerNorm(16)\n",
    "pytorch_ln = nn.LayerNorm(16)\n",
    "\n",
    "out_ours = our_ln(x_test)\n",
    "out_pytorch = pytorch_ln(x_test)\n",
    "\n",
    "print(f\"Our LayerNorm output mean (should be ~0): {out_ours.mean(dim=-1)[0].tolist()}\")\n",
    "print(f\"Our LayerNorm output std (should be ~1):  {out_ours.std(dim=-1)[0].tolist()}\")\n",
    "print(f\"Outputs match: {torch.allclose(out_ours, out_pytorch, atol=1e-5)}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_07_layernorm_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Layernorm Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_07_layernorm_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what Layer Norm does\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "x_vis = torch.randn(1, 8, 16) * 5 + 3  # Large scale, shifted mean\n",
    "x_normed = our_ln(x_vis)\n",
    "\n",
    "# Before normalization\n",
    "ax = axes[0]\n",
    "im = ax.imshow(x_vis[0].detach().numpy(), cmap='RdBu_r', aspect='auto', vmin=-15, vmax=15)\n",
    "ax.set_title('Before LayerNorm', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Dimension')\n",
    "ax.set_ylabel('Position')\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "# After normalization\n",
    "ax = axes[1]\n",
    "im = ax.imshow(x_normed[0].detach().numpy(), cmap='RdBu_r', aspect='auto', vmin=-3, vmax=3)\n",
    "ax.set_title('After LayerNorm', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Dimension')\n",
    "ax.set_ylabel('Position')\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "# Distribution comparison\n",
    "ax = axes[2]\n",
    "ax.hist(x_vis[0].detach().numpy().flatten(), bins=30, alpha=0.7, label='Before', color='#e74c3c')\n",
    "ax.hist(x_normed[0].detach().numpy().flatten(), bins=30, alpha=0.7, label='After', color='#3498db')\n",
    "ax.set_title('Value Distribution', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Value')\n",
    "\n",
    "plt.suptitle('Layer Normalization: Standardizing Activations', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_08_ffn_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Ffn Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_08_ffn_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Feed-Forward Network"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_09_ffn_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Ffn Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_09_ffn_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"Position-wise Feed-Forward Network.\"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq, d_model)\n",
    "        x = self.linear1(x)       # (batch, seq, d_ff)\n",
    "        x = F.relu(x)             # ReLU activation\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)       # (batch, seq, d_model)\n",
    "        return x\n",
    "\n",
    "# Test\n",
    "ffn = FeedForwardNetwork(d_model=32, d_ff=128)\n",
    "x_ffn = torch.randn(1, 6, 32)\n",
    "out_ffn = ffn(x_ffn)\n",
    "print(f\"FFN input shape:  {x_ffn.shape}\")\n",
    "print(f\"FFN output shape: {out_ffn.shape}\")\n",
    "print(f\"Expansion ratio: {128/32}x (d_ff / d_model)\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_10_mha_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Mha Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_10_mha_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Multi-Head Self-Attention (from previous notebook)"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_11_mha_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Mha Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_11_mha_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, C = x.size()\n",
    "        Q = self.W_q(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(~mask, float('-inf'))\n",
    "        attn_weights = self.dropout(F.softmax(scores, dim=-1))\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "\n",
    "        context = context.transpose(1, 2).contiguous().view(B, T, self.d_model)\n",
    "        return self.W_o(context), attn_weights"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_12_block_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Block Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_12_block_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The Complete Transformer Block"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_13_block_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Block Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_13_block_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer encoder block.\n",
    "    Architecture: MultiHeadAttention -> Add & Norm -> FFN -> Add & Norm\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadSelfAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Sub-layer 1: Multi-Head Attention + Residual + Norm\n",
    "        attn_output, attn_weights = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))  # Residual connection\n",
    "\n",
    "        # Sub-layer 2: FFN + Residual + Norm\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout2(ffn_output))    # Residual connection\n",
    "\n",
    "        return x, attn_weights\n",
    "\n",
    "# Test the complete block\n",
    "torch.manual_seed(42)\n",
    "block = TransformerBlock(d_model=32, num_heads=4, d_ff=128)\n",
    "x_block = torch.randn(2, 6, 32)\n",
    "out_block, attn_block = block(x_block)\n",
    "\n",
    "print(f\"Input shape:            {x_block.shape}\")\n",
    "print(f\"Output shape:           {out_block.shape}\")\n",
    "print(f\"Attention weights shape: {attn_block.shape}\")\n",
    "print(f\"\\nInput and output have the same shape â€” blocks can be stacked!\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_14_residual_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Residual Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_14_residual_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate that residual connections preserve information\n",
    "# Compare block output with and without residuals\n",
    "\n",
    "class TransformerBlockNoResidual(nn.Module):\n",
    "    \"\"\"Transformer block WITHOUT residual connections (for comparison).\"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadSelfAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output, _ = self.attention(x, mask)\n",
    "        x = self.norm1(attn_output)  # NO residual\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(ffn_output)   # NO residual\n",
    "        return x\n",
    "\n",
    "# Stack 10 blocks and see how signal propagates\n",
    "torch.manual_seed(42)\n",
    "d_model_demo = 32\n",
    "x_input = torch.randn(1, 6, d_model_demo)\n",
    "\n",
    "# With residuals\n",
    "norms_with_res = [x_input.norm().item()]\n",
    "x_with = x_input.clone()\n",
    "for _ in range(10):\n",
    "    block_r = TransformerBlock(d_model_demo, 4, 128, dropout=0.0)\n",
    "    block_r.eval()\n",
    "    x_with, _ = block_r(x_with)\n",
    "    norms_with_res.append(x_with.norm().item())\n",
    "\n",
    "# Without residuals\n",
    "norms_without_res = [x_input.norm().item()]\n",
    "x_without = x_input.clone()\n",
    "for _ in range(10):\n",
    "    block_nr = TransformerBlockNoResidual(d_model_demo, 4, 128, dropout=0.0)\n",
    "    block_nr.eval()\n",
    "    x_without = block_nr(x_without)\n",
    "    norms_without_res.append(x_without.norm().item())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(range(11), norms_with_res, 'o-', label='With Residuals', color='#2ecc71', linewidth=2, markersize=6)\n",
    "ax.plot(range(11), norms_without_res, 's-', label='Without Residuals', color='#e74c3c', linewidth=2, markersize=6)\n",
    "ax.set_xlabel('Layer Depth', fontsize=12)\n",
    "ax.set_ylabel('Activation Norm', fontsize=12)\n",
    "ax.set_title('Signal Propagation: With vs Without Residual Connections', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"With residuals: activation norms stay stable.\")\n",
    "print(\"Without residuals: activations may collapse or explode.\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_15_todo1_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo1 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_15_todo1_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Build a Full Transformer Encoder"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_16_todo1_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo1 Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_16_todo1_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Full Transformer encoder: Embedding + PE + N stacked blocks.\n",
    "\n",
    "    Args:\n",
    "        vocab_size: Number of tokens in the vocabulary\n",
    "        d_model: Model dimension\n",
    "        num_heads: Number of attention heads per block\n",
    "        d_ff: Feed-forward hidden dimension\n",
    "        num_layers: Number of Transformer blocks to stack\n",
    "        max_len: Maximum sequence length\n",
    "        dropout: Dropout rate\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # ============ TODO ============\n",
    "        # 1. Create a token embedding layer: nn.Embedding(vocab_size, d_model)\n",
    "        # 2. Create a positional encoding: SinusoidalPositionalEncoding(d_model, max_len)\n",
    "        # 3. Create a list of TransformerBlock layers: nn.ModuleList\n",
    "        # 4. Create a dropout layer\n",
    "        # ==============================\n",
    "\n",
    "        self.embedding = ???      # YOUR CODE HERE\n",
    "        self.pos_encoding = ???   # YOUR CODE HERE\n",
    "        self.blocks = ???         # YOUR CODE HERE\n",
    "        self.dropout = ???        # YOUR CODE HERE\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Token IDs of shape (batch_size, seq_len)\n",
    "            mask: Optional attention mask\n",
    "\n",
    "        Returns:\n",
    "            output: Shape (batch_size, seq_len, d_model)\n",
    "            all_attention_weights: List of attention weights from each layer\n",
    "        \"\"\"\n",
    "        # ============ TODO ============\n",
    "        # 1. Embed tokens and scale by sqrt(d_model)\n",
    "        # 2. Add positional encoding\n",
    "        # 3. Apply dropout\n",
    "        # 4. Pass through each Transformer block, collecting attention weights\n",
    "        # ==============================\n",
    "\n",
    "        all_attn = []\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        return x, all_attn"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_17_todo1_after",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Todo1 After\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_17_todo1_after.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_20_todo2_after",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Todo2 After\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_20_todo2_after.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "torch.manual_seed(42)\n",
    "encoder = TransformerEncoder(\n",
    "    vocab_size=1000, d_model=64, num_heads=4,\n",
    "    d_ff=256, num_layers=3, max_len=128\n",
    ")\n",
    "\n",
    "test_tokens = torch.randint(0, 1000, (2, 20))\n",
    "out_enc, attn_list = encoder(test_tokens)\n",
    "\n",
    "assert out_enc.shape == (2, 20, 64), f\"Expected (2, 20, 64), got {out_enc.shape}\"\n",
    "assert len(attn_list) == 3, f\"Expected 3 layers of attention, got {len(attn_list)}\"\n",
    "assert attn_list[0].shape == (2, 4, 20, 20), f\"Wrong attention shape: {attn_list[0].shape}\"\n",
    "print(\"All assertions passed! Your Transformer encoder is correct!\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_18_todo2_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo2 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_18_todo2_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement Pre-Norm vs Post-Norm"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_19_todo2_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo2 Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_19_todo2_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockPreNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block with Pre-LayerNorm (used in GPT-2, LLaMA).\n",
    "\n",
    "    The difference from our original block:\n",
    "    - Original (Post-Norm):  x = LayerNorm(x + Attention(x))\n",
    "    - Pre-Norm:              x = x + Attention(LayerNorm(x))\n",
    "\n",
    "    Pre-Norm tends to train more stably, especially in deep models.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadSelfAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # ============ TODO ============\n",
    "        # Implement PRE-NORM Transformer block:\n",
    "        # Sub-layer 1: x = x + Attention(LayerNorm(x))\n",
    "        # Sub-layer 2: x = x + FFN(LayerNorm(x))\n",
    "        #\n",
    "        # Note: LayerNorm is applied BEFORE the sub-layer, not after\n",
    "        # ==============================\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        return x, attn_weights"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "torch.manual_seed(42)\n",
    "block_pre = TransformerBlockPreNorm(d_model=32, num_heads=4, d_ff=128)\n",
    "x_pre = torch.randn(1, 6, 32)\n",
    "out_pre, _ = block_pre(x_pre)\n",
    "assert out_pre.shape == x_pre.shape, f\"Shape mismatch: {out_pre.shape}\"\n",
    "print(\"Pre-Norm block assertions passed!\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_21_putting_it_together_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Putting It Together Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_21_putting_it_together_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_22_classifier_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Classifier Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_22_classifier_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer-based text classifier.\n",
    "    Embedding + PE + N Transformer blocks + Mean Pooling + Classification Head\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, num_classes, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = SinusoidalPositionalEncoding(d_model, max_len)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Embed + PE\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Transformer blocks\n",
    "        all_attn = []\n",
    "        for block in self.blocks:\n",
    "            x, attn = block(x, mask)\n",
    "            all_attn.append(attn)\n",
    "\n",
    "        # Mean pooling over sequence dimension\n",
    "        x_pooled = x.mean(dim=1)  # (batch, d_model)\n",
    "\n",
    "        # Classification\n",
    "        logits = self.classifier(x_pooled)  # (batch, num_classes)\n",
    "\n",
    "        return logits, all_attn\n",
    "\n",
    "# Create the model\n",
    "torch.manual_seed(42)\n",
    "model = TransformerClassifier(\n",
    "    vocab_size=200, d_model=32, num_heads=4,\n",
    "    d_ff=128, num_layers=2, num_classes=2\n",
    ")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_23_training_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Training Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_23_training_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "Let us train on a synthetic sentiment classification task."
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_24_synthetic_data_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Synthetic Data Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_24_synthetic_data_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data: simple patterns that require attention\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "vocab_size = 200\n",
    "seq_len = 20\n",
    "n_samples = 500\n",
    "\n",
    "# Pattern: sequences with \"positive\" tokens (100-149) vs \"negative\" tokens (150-199)\n",
    "# Label = majority class in the sequence\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    # Choose label\n",
    "    label = np.random.randint(2)\n",
    "\n",
    "    if label == 1:  # Positive\n",
    "        # Mostly tokens from 100-149 with some noise\n",
    "        seq = np.random.choice(range(100, 150), size=seq_len-4)\n",
    "        noise = np.random.choice(range(1, 100), size=4)\n",
    "        seq = np.concatenate([seq, noise])\n",
    "    else:  # Negative\n",
    "        # Mostly tokens from 150-199 with some noise\n",
    "        seq = np.random.choice(range(150, 200), size=seq_len-4)\n",
    "        noise = np.random.choice(range(1, 100), size=4)\n",
    "        seq = np.concatenate([seq, noise])\n",
    "\n",
    "    np.random.shuffle(seq)\n",
    "    X_data.append(seq)\n",
    "    y_data.append(label)\n",
    "\n",
    "X_tensor = torch.tensor(np.array(X_data), dtype=torch.long)\n",
    "y_tensor = torch.tensor(y_data, dtype=torch.long)\n",
    "\n",
    "# Split into train/test\n",
    "train_X, test_X = X_tensor[:400], X_tensor[400:]\n",
    "train_y, test_y = y_tensor[:400], y_tensor[400:]\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_X, train_y), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(test_X, test_y), batch_size=32)\n",
    "\n",
    "print(f\"Train: {len(train_X)} samples, Test: {len(test_X)} samples\")\n",
    "print(f\"Sequence length: {seq_len}, Vocab size: {vocab_size}\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_25_training_loop_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Training Loop Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_25_training_loop_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        logits, _ = model(batch_X)\n",
    "        loss = F.cross_entropy(logits, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * batch_X.size(0)\n",
    "        correct += (logits.argmax(dim=1) == batch_y).sum().item()\n",
    "        total += batch_X.size(0)\n",
    "\n",
    "    train_losses.append(epoch_loss / total)\n",
    "    train_accs.append(correct / total)\n",
    "\n",
    "    # Test accuracy\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            logits, _ = model(batch_X)\n",
    "            correct_test += (logits.argmax(dim=1) == batch_y).sum().item()\n",
    "            total_test += batch_X.size(0)\n",
    "    test_accs.append(correct_test / total_test)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}: loss={train_losses[-1]:.4f}  \"\n",
    "              f\"train_acc={train_accs[-1]:.3f}  test_acc={test_accs[-1]:.3f}\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_26_training_plot_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Training Plot Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_26_training_plot_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(train_losses, color='#e74c3c', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(train_accs, label='Train', color='#3498db', linewidth=2)\n",
    "ax2.plot(test_accs, label='Test', color='#2ecc71', linewidth=2, linestyle='--')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "ax2.set_title('Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.set_ylim(0.4, 1.05)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Transformer Classifier Training', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_27_final_output_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Final Output Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_27_final_output_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_28_attention_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Attention Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_28_attention_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns across all layers and heads\n",
    "model.eval()\n",
    "sample_idx = 0\n",
    "sample_X = test_X[sample_idx:sample_idx+1]\n",
    "sample_y = test_y[sample_idx].item()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, all_attn = model(sample_X)\n",
    "    pred = logits.argmax(dim=1).item()\n",
    "\n",
    "num_layers = len(all_attn)\n",
    "num_heads = all_attn[0].shape[1]\n",
    "\n",
    "fig, axes = plt.subplots(num_layers, num_heads, figsize=(4*num_heads, 4*num_layers))\n",
    "\n",
    "for layer_idx in range(num_layers):\n",
    "    for head_idx in range(num_heads):\n",
    "        ax = axes[layer_idx][head_idx] if num_layers > 1 else axes[head_idx]\n",
    "        w = all_attn[layer_idx][0, head_idx].numpy()\n",
    "        im = ax.imshow(w, cmap='Blues', vmin=0, vmax=w.max())\n",
    "        if layer_idx == 0:\n",
    "            ax.set_title(f'Head {head_idx+1}', fontsize=11, fontweight='bold')\n",
    "        if head_idx == 0:\n",
    "            ax.set_ylabel(f'Layer {layer_idx+1}', fontsize=11, fontweight='bold')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "label_names = ['Negative', 'Positive']\n",
    "plt.suptitle(f'Attention Patterns â€” Predicted: {label_names[pred]}, '\n",
    "             f'True: {label_names[sample_y]}',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_29_architecture_summary",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Architecture Summary\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_29_architecture_summary.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary: model architecture diagram\n",
    "print(\"=\" * 60)\n",
    "print(\"TRANSFORMER CLASSIFIER ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Vocabulary size:   {vocab_size}\")\n",
    "print(f\"  Model dimension:   {model.d_model}\")\n",
    "print(f\"  Attention heads:   {num_heads}\")\n",
    "print(f\"  FFN hidden dim:    128\")\n",
    "print(f\"  Transformer layers: {num_layers}\")\n",
    "print(f\"  Total parameters:  {total_params:,}\")\n",
    "print(f\"  Max sequence len:  512\")\n",
    "print()\n",
    "print(\"  Architecture:\")\n",
    "print(\"  Token IDs -> Embedding -> + Positional Encoding\")\n",
    "print(\"    -> [Transformer Block x N]\")\n",
    "print(\"        -> Multi-Head Attention + Add & Norm\")\n",
    "print(\"        -> Feed-Forward Network + Add & Norm\")\n",
    "print(\"    -> Mean Pooling -> Linear -> Class Logits\")\n",
    "print()\n",
    "print(f\"  Final test accuracy: {test_accs[-1]*100:.1f}%\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nCongratulations! You have built a complete Transformer from scratch!\")\n",
    "print(\"From QKV projections to attention to the full encoder block,\")\n",
    "print(\"you now understand every single matrix multiplication in the architecture.\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_30_closing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Wrap-Up: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_30_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "1. What happens when you increase the number of layers? Does the model become harder to train? How do residual connections help?\n",
    "2. Why do we scale the embedding by $\\sqrt{d_{\\text{model}}}$ before adding positional encoding? (Hint: think about the relative magnitudes of the embedding and positional vectors.)\n",
    "3. The FFN uses a 4x expansion ratio ($d_{ff} = 4 \\times d_{\\text{model}}$). Why not 2x or 8x? Research suggests this is a good trade-off between expressiveness and compute â€” but what would happen at the extremes?\n",
    "\n",
    "### Optional Challenges\n",
    "1. Implement a **Transformer decoder** with causal masking and cross-attention. This is the architecture behind GPT-style models.\n",
    "2. Replace the synthetic data with real IMDB movie review data (available via `torchtext` or HuggingFace datasets). Train the model on actual sentiment classification.\n",
    "3. Add **learning rate warmup** (linearly increasing the LR for the first few epochs). This is standard practice for training Transformers and can significantly improve convergence."
   ],
   "id": "cell_30"
  }
 ]
}