{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Self-Attention & The Transformer Encoder ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1RJjttCvltRK-j5XaI_Tp752cibGKRYMf\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/02_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Self-Attention & The Transformer Encoder from First Principles\n",
    "\n",
    "*Part 2 of the Vizuara series on Understanding BERT from Scratch*\n",
    "*Estimated time: 60 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_02_ai_assistant",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Ai Assistant\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_02_ai_assistant.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ü§ñ AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** ‚Äî it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[üëâ Open AI Teaching Assistant](https://pods.vizuara.ai/courses/understanding-bert-from-scratch/practice/2/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_03_why_it_matters",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_03_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "The Transformer encoder is the engine that powers BERT, GPT, and virtually every modern language model. At its heart is a mechanism called **self-attention** ‚Äî a way for every word to look at every other word and decide which ones matter most.\n",
    "\n",
    "In this notebook, we will build the entire Transformer encoder **from scratch** ‚Äî no HuggingFace, no pre-built modules. By the end, you will have:\n",
    "\n",
    "1. A working **scaled dot-product attention** mechanism\n",
    "2. A **multi-head attention** module\n",
    "3. A complete **Transformer encoder block** with residual connections\n",
    "4. Beautiful **attention heatmaps** showing which words attend to which"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup ‚Äî run this cell first\n",
    "!pip install -q torch matplotlib numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_05_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_05_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Imagine you are reading the sentence: *\"The delivery arrived late, but **it** was in perfect condition.\"*\n",
    "\n",
    "When you read \"it,\" your brain instantly jumps back to \"delivery\" ‚Äî that is what \"it\" refers to. Your brain does not give equal weight to every word; it **selectively attends** to the relevant ones.\n",
    "\n",
    "Self-attention does exactly this ‚Äî for every word, it computes an attention score with every other word, then uses those scores to create a weighted combination.\n",
    "\n",
    "Think of it like a library:\n",
    "- The **Query** is your search question: \"What am I looking for?\"\n",
    "- The **Key** is the label on each book: \"What information does this book contain?\"\n",
    "- The **Value** is the actual content of the book: \"Here is the information.\"\n",
    "\n",
    "You match your Query against all Keys, and the best-matching Keys point you to the most relevant Values.\n",
    "\n",
    "### ü§î Think About This\n",
    "Why do we need THREE separate matrices (Q, K, V) instead of just using the embeddings directly? Think about what would happen if Q = K = V = the embedding itself."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_06_mathematics_attention",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Mathematics Attention\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_06_mathematics_attention.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_07_mathematics_multihead",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Mathematics Multihead\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_07_mathematics_multihead.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Scaled Dot-Product Attention\n",
    "\n",
    "Given queries $Q$, keys $K$, and values $V$, the attention output is:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Let us break this down computationally:\n",
    "- $QK^T$ computes the dot product between every query and every key. This gives us a matrix of \"relevance scores\" ‚Äî how much each word should attend to each other word.\n",
    "- Dividing by $\\sqrt{d_k}$ prevents the dot products from becoming too large (which would make softmax saturate and produce very peaked distributions).\n",
    "- Softmax normalizes each row so the attention weights sum to 1.\n",
    "- Multiplying by $V$ computes a weighted combination of the value vectors.\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Instead of computing attention once, we compute it $h$ times in parallel with different learned projections:\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W^O$$\n",
    "\n",
    "where $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$\n",
    "\n",
    "Computationally: each head learns to look for a different type of relationship. One head might learn syntax (subject-verb), another might learn coreference (what does \"it\" refer to?), and another might learn semantic similarity."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It ‚Äî Component by Component\n",
    "\n",
    "### 4.1 Scaled Dot-Product Attention"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_08_sdpa_implementation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Sdpa Implementation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_08_sdpa_implementation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "\n",
    "    Args:\n",
    "        Q: Query tensor (batch, num_heads, seq_len, d_k)\n",
    "        K: Key tensor (batch, num_heads, seq_len, d_k)\n",
    "        V: Value tensor (batch, num_heads, seq_len, d_v)\n",
    "        mask: Optional mask tensor\n",
    "\n",
    "    Returns:\n",
    "        output: Weighted values (batch, num_heads, seq_len, d_v)\n",
    "        attention_weights: Softmax attention weights (batch, num_heads, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "\n",
    "    # Step 1: Compute attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch, heads, seq, seq)\n",
    "\n",
    "    # Step 2: Scale by sqrt(d_k)\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "\n",
    "    # Step 3: Apply mask (if provided)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # Step 4: Apply softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # Step 5: Multiply by values\n",
    "    output = torch.matmul(attention_weights, V)  # (batch, heads, seq, d_v)\n",
    "\n",
    "    return output, attention_weights"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify this with the numerical example from the article."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_09_sdpa_verification",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Sdpa Verification\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_09_sdpa_verification.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical example from the article\n",
    "Q = torch.tensor([[[1.0, 0.0],\n",
    "                    [0.0, 1.0],\n",
    "                    [1.0, 1.0]]]).unsqueeze(1)  # (1, 1, 3, 2)\n",
    "\n",
    "K = torch.tensor([[[1.0, 1.0],\n",
    "                    [0.0, 1.0],\n",
    "                    [1.0, 0.0]]]).unsqueeze(1)\n",
    "\n",
    "V = torch.tensor([[[1.0, 2.0],\n",
    "                    [3.0, 4.0],\n",
    "                    [5.0, 6.0]]]).unsqueeze(1)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(\"Attention weights (row = query word, col = key word):\")\n",
    "print(weights[0, 0].detach().numpy().round(2))\n",
    "print(f\"\\nOutput for first word: {output[0, 0, 0].detach().numpy().round(2)}\")\n",
    "print(f\"Expected (from article): ~[3.0, 4.0]\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_10_sdpa_visualization",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Sdpa Visualization\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_10_sdpa_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize the attention weights\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(weights[0, 0].detach().numpy(), cmap='Blues', vmin=0, vmax=1)\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xticks([0, 1, 2], ['Word 1', 'Word 2', 'Word 3'])\n",
    "plt.yticks([0, 1, 2], ['Word 1', 'Word 2', 'Word 3'])\n",
    "plt.xlabel(\"Key (attending TO)\")\n",
    "plt.ylabel(\"Query (attending FROM)\")\n",
    "plt.title(\"Attention Weights Matrix\")\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        val = weights[0, 0, i, j].item()\n",
    "        plt.text(j, i, f\"{val:.2f}\", ha='center', va='center',\n",
    "                 color='white' if val > 0.5 else 'black', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Multi-Head Attention"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_11_mha_implementation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Mha Implementation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_11_mha_implementation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention mechanism.\n",
    "\n",
    "    Instead of one attention function, we run h parallel attention \"heads\",\n",
    "    each with its own learned Q, K, V projections.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "\n",
    "        # Learned projection matrices\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)  # Output projection\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_Q(x)  # (batch, seq, d_model)\n",
    "        K = self.W_K(x)\n",
    "        V = self.W_V(x)\n",
    "\n",
    "        # Reshape to (batch, num_heads, seq_len, d_k)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Apply scaled dot-product attention\n",
    "        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Concatenate heads: (batch, seq_len, d_model)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "\n",
    "        # Final linear projection\n",
    "        output = self.W_O(attn_output)\n",
    "\n",
    "        return output, attn_weights"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_12_mha_verification",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Mha Verification\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_12_mha_verification.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-head attention\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Random input: batch=1, seq_len=5, d_model=64\n",
    "x = torch.randn(1, 5, d_model)\n",
    "output, attn_weights = mha(x)\n",
    "\n",
    "print(f\"Input shape:           {x.shape}\")\n",
    "print(f\"Output shape:          {output.shape}\")\n",
    "print(f\"Attention weights:     {attn_weights.shape}\")\n",
    "print(f\"  (batch, heads, seq, seq)\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_13_mha_visualization",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Mha Visualization\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_13_mha_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize attention patterns across all heads\n",
    "fig, axes = plt.subplots(1, num_heads, figsize=(16, 4))\n",
    "words = ['Word‚ÇÅ', 'Word‚ÇÇ', 'Word‚ÇÉ', 'Word‚ÇÑ', 'Word‚ÇÖ']\n",
    "\n",
    "for head_idx in range(num_heads):\n",
    "    ax = axes[head_idx]\n",
    "    weights_np = attn_weights[0, head_idx].detach().numpy()\n",
    "    im = ax.imshow(weights_np, cmap='Blues', vmin=0, vmax=weights_np.max())\n",
    "    ax.set_title(f\"Head {head_idx + 1}\", fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(range(5))\n",
    "    ax.set_xticklabels(words, rotation=45, fontsize=8)\n",
    "    ax.set_yticks(range(5))\n",
    "    ax.set_yticklabels(words, fontsize=8)\n",
    "\n",
    "plt.suptitle(\"Multi-Head Attention ‚Äî Each Head Learns Different Patterns\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"üí° Notice how each head has a DIFFERENT attention pattern!\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Feed-Forward Network\n",
    "\n",
    "The Transformer encoder block also contains a position-wise feed-forward network: two linear layers with a ReLU activation in between.\n",
    "\n",
    "$$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "Computationally: this acts as a \"thinking\" step. After attention gathers relevant information from other words, the FFN processes that information independently at each position."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_14_ffn_implementation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Ffn Implementation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_14_ffn_implementation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network.\n",
    "\n",
    "    Two linear transformations with ReLU in between.\n",
    "    The inner dimension is typically 4x the model dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.relu(self.linear1(x)))"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Layer Normalization"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_15_layernorm_implementation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Layernorm Implementation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_15_layernorm_implementation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer Normalization (Ba et al., 2016).\n",
    "\n",
    "    Normalizes the last dimension of the input to have\n",
    "    zero mean and unit variance, then applies learned\n",
    "    scale (gamma) and shift (beta).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 The Complete Transformer Encoder Block"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_16_encoder_block_implementation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Encoder Block Implementation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_16_encoder_block_implementation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    One Transformer encoder block:\n",
    "      1. Multi-Head Self-Attention + Residual + LayerNorm\n",
    "      2. Feed-Forward Network + Residual + LayerNorm\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection and layer norm\n",
    "        attn_output, attn_weights = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # Feed-forward with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "\n",
    "        return x, attn_weights"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_17_encoder_block_verification",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Encoder Block Verification\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_17_encoder_block_verification.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the full encoder block\n",
    "encoder_block = TransformerEncoderBlock(d_model=64, num_heads=4, d_ff=256)\n",
    "\n",
    "x = torch.randn(1, 5, 64)\n",
    "output, weights = encoder_block(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"‚úÖ Same shape in and out ‚Äî this is key for stacking blocks!\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîß Your Turn\n",
    "\n",
    "### TODO: Stack Multiple Encoder Blocks\n",
    "\n",
    "BERT-Base uses 12 encoder blocks stacked on top of each other. Implement the `TransformerEncoder` class that stacks N blocks."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stack of N Transformer encoder blocks.\n",
    "\n",
    "    Args:\n",
    "        num_layers: Number of encoder blocks to stack\n",
    "        d_model: Model dimension\n",
    "        num_heads: Number of attention heads\n",
    "        d_ff: Feed-forward inner dimension\n",
    "        dropout: Dropout rate\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # ============ TODO ============\n",
    "        # Create a nn.ModuleList containing num_layers\n",
    "        # TransformerEncoderBlock instances\n",
    "        # ==============================\n",
    "        self.layers = ???  # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Pass input through all encoder blocks sequentially.\n",
    "        Return the final output and attention weights from the LAST layer.\n",
    "        \"\"\"\n",
    "        attn_weights = None\n",
    "\n",
    "        # ============ TODO ============\n",
    "        # Loop through self.layers and pass x through each\n",
    "        # ==============================\n",
    "        for layer in ???:  # YOUR CODE HERE\n",
    "            x, attn_weights = ???  # YOUR CODE HERE\n",
    "\n",
    "        return x, attn_weights"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_19_todo_stack_verification",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo Stack Verification\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_19_todo_stack_verification.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_21_todo_pe_verification",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo Pe Verification\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_21_todo_pe_verification.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification\n",
    "encoder = TransformerEncoder(num_layers=6, d_model=64, num_heads=4, d_ff=256)\n",
    "test_input = torch.randn(2, 10, 64)  # batch=2, seq_len=10\n",
    "test_output, test_weights = encoder(test_input)\n",
    "\n",
    "assert test_output.shape == (2, 10, 64), f\"‚ùå Expected shape (2, 10, 64), got {test_output.shape}\"\n",
    "assert test_weights.shape == (2, 4, 10, 10), f\"‚ùå Expected attention shape (2, 4, 10, 10), got {test_weights.shape}\"\n",
    "print(f\"‚úÖ TransformerEncoder works! Output shape: {test_output.shape}\")\n",
    "print(f\"   6 layers stacked, each with 4 attention heads\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in encoder.parameters()):,}\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_20_todo_pe_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo Pe Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_20_todo_pe_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implement Positional Encoding\n",
    "\n",
    "The Transformer has no built-in notion of word order. We need to inject positional information. Implement sinusoidal positional encoding:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
    "\n",
    "Computationally: even dimensions use sine, odd dimensions use cosine, with frequencies that decrease as the dimension increases. This creates a unique \"fingerprint\" for each position."
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encoding.\n",
    "\n",
    "    Adds a fixed positional signal to the input embeddings\n",
    "    so the model knows the order of words.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # ============ TODO ============\n",
    "        # Step 1: Compute the division term: 10000^(2i/d_model)\n",
    "        #         Hint: use torch.exp and torch.arange\n",
    "        # Step 2: Apply sin to even indices (0, 2, 4, ...)\n",
    "        # Step 3: Apply cos to odd indices (1, 3, 5, ...)\n",
    "        # ==============================\n",
    "\n",
    "        div_term = ???  # YOUR CODE HERE\n",
    "        pe[:, 0::2] = ???  # YOUR CODE HERE (even indices)\n",
    "        pe[:, 1::2] = ???  # YOUR CODE HERE (odd indices)\n",
    "\n",
    "        # Register as buffer (not a parameter ‚Äî it's fixed)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
    "        return x + self.pe[:, :x.size(1)]"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification\n",
    "pos_enc = PositionalEncoding(d_model=64, max_len=100)\n",
    "test_x = torch.zeros(1, 50, 64)\n",
    "encoded = pos_enc(test_x)\n",
    "\n",
    "assert encoded.shape == (1, 50, 64), f\"‚ùå Expected shape (1, 50, 64), got {encoded.shape}\"\n",
    "assert not torch.allclose(encoded[0, 0], encoded[0, 1]), \"‚ùå Different positions should have different encodings\"\n",
    "print(\"‚úÖ Positional encoding works!\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_22_pe_visualization",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Pe Visualization\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_22_pe_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize the positional encoding\n",
    "pe = pos_enc.pe[0, :50, :64].numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.imshow(pe.T, cmap='RdBu', aspect='auto', interpolation='nearest')\n",
    "plt.colorbar(label='Encoding Value')\n",
    "plt.xlabel(\"Position in Sequence\")\n",
    "plt.ylabel(\"Embedding Dimension\")\n",
    "plt.title(\"Sinusoidal Positional Encoding\\n(each position has a unique pattern)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together: Attention on Real Text\n",
    "\n",
    "Let us run our Transformer encoder on a real sentence and visualize what the attention heads learn."
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_23_real_text_demo",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Real Text Demo\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_23_real_text_demo.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple tokenizer for our demo\n",
    "sentence = \"the cat sat on it\"\n",
    "words = sentence.split()\n",
    "word_to_idx = {w: i for i, w in enumerate(set(words))}\n",
    "idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "\n",
    "# Build a simple model\n",
    "class SimpleTransformerDemo(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        self.encoder = TransformerEncoder(num_layers, d_model, num_heads, d_ff)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.pos_encoding(x)\n",
    "        output, attn_weights = self.encoder(x)\n",
    "        return output, attn_weights\n",
    "\n",
    "vocab_size = len(word_to_idx)\n",
    "demo_model = SimpleTransformerDemo(\n",
    "    vocab_size=vocab_size, d_model=32, num_heads=4, d_ff=128, num_layers=2\n",
    ")\n",
    "\n",
    "# Encode the sentence\n",
    "input_ids = torch.tensor([[word_to_idx[w] for w in words]])\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, attn_weights = demo_model(input_ids)\n",
    "\n",
    "print(f\"Sentence: '{sentence}'\")\n",
    "print(f\"Input shape:            {input_ids.shape}\")\n",
    "print(f\"Contextual output:      {output.shape}\")\n",
    "print(f\"Attention weights:      {attn_weights.shape}\")"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_24_real_text_visualization",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Real Text Visualization\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_24_real_text_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Attention heatmap ‚Äî which words attend to which?\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "for head_idx in range(4):\n",
    "    ax = axes[head_idx]\n",
    "    weights_np = attn_weights[0, head_idx].detach().numpy()\n",
    "    im = ax.imshow(weights_np, cmap='Purples', vmin=0)\n",
    "    ax.set_title(f\"Head {head_idx + 1}\", fontsize=13, fontweight='bold')\n",
    "    ax.set_xticks(range(len(words)))\n",
    "    ax.set_xticklabels(words, rotation=45, fontsize=11)\n",
    "    ax.set_yticks(range(len(words)))\n",
    "    ax.set_yticklabels(words, fontsize=11)\n",
    "\n",
    "    # Annotate values\n",
    "    for i in range(len(words)):\n",
    "        for j in range(len(words)):\n",
    "            val = weights_np[i, j]\n",
    "            ax.text(j, i, f\"{val:.2f}\", ha='center', va='center',\n",
    "                    fontsize=8, color='white' if val > 0.4 else 'black')\n",
    "\n",
    "plt.suptitle(f'Self-Attention Heatmaps: \"{sentence}\"', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Each head develops a DIFFERENT attention pattern.\")\n",
    "print(\"   In a trained model, 'it' would strongly attend to 'cat' (coreference).\")"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üéØ Final Output: Interactive Attention Visualization"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_25_final_visualization",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Final Visualization\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_25_final_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a slightly larger demo to show richer patterns\n",
    "sentences = [\n",
    "    \"the cat sat on the mat and purred\",\n",
    "    \"the dog chased the cat around the yard\",\n",
    "    \"she went to the bank to deposit money\",\n",
    "]\n",
    "\n",
    "# Build a combined vocabulary\n",
    "all_words_list = list(set(w for s in sentences for w in s.split()))\n",
    "demo_vocab = {w: i for i, w in enumerate(all_words_list)}\n",
    "demo_idx_to_word = {i: w for w, i in demo_vocab.items()}\n",
    "\n",
    "demo_model_2 = SimpleTransformerDemo(\n",
    "    vocab_size=len(demo_vocab), d_model=32, num_heads=4, d_ff=128, num_layers=3\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(len(sentences), 1, figsize=(14, 4 * len(sentences)))\n",
    "\n",
    "for sent_idx, sentence in enumerate(sentences):\n",
    "    words = sentence.split()\n",
    "    ids = torch.tensor([[demo_vocab[w] for w in words]])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, attn = demo_model_2(ids)\n",
    "\n",
    "    # Average attention across all heads\n",
    "    avg_attn = attn[0].mean(dim=0).numpy()\n",
    "\n",
    "    ax = axes[sent_idx]\n",
    "    im = ax.imshow(avg_attn, cmap='viridis', vmin=0)\n",
    "    ax.set_title(f'\"{sentence}\"', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(range(len(words)))\n",
    "    ax.set_xticklabels(words, rotation=45, fontsize=10)\n",
    "    ax.set_yticks(range(len(words)))\n",
    "    ax.set_yticklabels(words, fontsize=10)\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.suptitle(\"üéØ Self-Attention Patterns (Averaged Across Heads)\", fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéâ Congratulations! You've built a complete Transformer encoder from scratch!\")\n",
    "print(\"   Next up: BERT's architecture ‚Äî input representation and pre-training objectives.\")"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_26_closing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Wrap-Up: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_26_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reflection and Next Steps\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "1. Why do we divide by $\\sqrt{d_k}$ in the attention formula? What would happen without it? (Hint: think about the variance of dot products as dimension grows.)\n",
    "2. If we have 12 attention heads with $d_k = 64$ each, what is the total model dimension $d_{\\text{model}}$? Why is this more efficient than having one head with $d_k = 768$?\n",
    "3. The residual connections in the encoder block add the input directly to the output. Why is this important for training deep networks?\n",
    "\n",
    "### üèÜ Optional Challenges\n",
    "1. **Masked Self-Attention**: Modify the attention function to apply a causal mask that prevents words from attending to future words. This is how GPT works (decoder-style).\n",
    "2. **Relative Position Encoding**: Instead of fixed sinusoidal encodings, implement relative position encodings where attention scores are modified based on the distance between tokens.\n",
    "3. **Attention Dropout**: Add dropout to the attention weights (after softmax, before multiplying by V). How does this affect the model?"
   ],
   "id": "cell_34"
  }
 ]
}