{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Ablation Studies ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1bBtLgEU0TdHr55w7SYOhyDG1BKljPaRc\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/04_00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Ablation Studies: What Really Matters in Recursive Reasoning?\n",
    "\n",
    "*Part 4 of the Vizuara series on Tiny Recursive Models*\n",
    "*Estimated time: 35 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ü§ñ AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** ‚Äî it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[üëâ Open AI Teaching Assistant](https://course-creator-brown.vercel.app/courses/tiny-recursive-models/practice/4/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "We have built and trained a Tiny Recursive Model. It works. But **why** does it work?\n",
    "\n",
    "The TRM paper's ablation study is one of its most valuable contributions. It systematically removes or changes components to reveal which design choices matter and which are just noise. The findings are surprising:\n",
    "\n",
    "- Full backpropagation: **+30.9 percentage points**\n",
    "- MLP over attention (for small grids): **+13.0 points**\n",
    "- More recursion over more layers: **+7.9 points**\n",
    "- EMA: **+7.5 points**\n",
    "\n",
    "In this notebook, you will run these ablation experiments yourself and see firsthand what makes recursive reasoning tick."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_01_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_01_intuition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### What Is an Ablation Study?\n",
    "\n",
    "In medicine, \"ablation\" means removing a specific tissue or structure. In machine learning, an ablation study removes or changes one component at a time to measure its contribution.\n",
    "\n",
    "Think of it like debugging a recipe. If your cake tastes amazing, you want to know: Is it the vanilla extract? The extra egg? The longer baking time? You bake the cake multiple times, each time leaving out one ingredient, and compare results.\n",
    "\n",
    "### The Paper's Key Findings (Sudoku-Extreme)\n",
    "\n",
    "| Change | Accuracy Impact |\n",
    "|--------|----------------|\n",
    "| Full backprop (vs 1-step approx) | **+30.9%** |\n",
    "| MLP mixing (vs attention) | **+13.0%** |\n",
    "| 2 layers + more recursion (vs 4 layers) | **+7.9%** |\n",
    "| EMA (vs no EMA) | **+7.5%** |\n",
    "| Two features y+z (vs y only) | Essential |\n",
    "\n",
    "### ü§î Think About This\n",
    "\n",
    "Before running the experiments, make your predictions:\n",
    "1. Will more recursion really beat more layers? It seems counterintuitive...\n",
    "2. Why would a simple MLP beat powerful self-attention?\n",
    "3. How much does the reasoning scratchpad z actually help?\n",
    "\n",
    "Write down your predictions. Then compare them to the experimental results."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_02_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_02_math"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics (Quick Reference)\n",
    "\n",
    "All the math was covered in Notebooks 2 and 3. Here is a quick reference:\n",
    "\n",
    "**Recursion:**\n",
    "$$z \\leftarrow \\text{net}(x, y, z), \\quad y \\leftarrow \\text{net}(y, z)$$\n",
    "\n",
    "**Prediction loss:** $\\mathcal{L}_{\\text{pred}} = -\\sum_i y_i^{\\text{true}} \\log(\\hat{y}_i)$\n",
    "\n",
    "**Effective depth:** $T \\times (n+1) \\times n_{\\text{layers}}$\n",
    "\n",
    "The key insight for ablations: we keep total training time roughly constant and vary one factor at a time."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_03_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_03_setup"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It ‚Äî The Ablation Framework\n",
    "\n",
    "### 4.1 Model and Data Setup"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse components from previous notebooks\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + self.eps)\n",
    "        return (x / rms) * self.weight\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim=None):\n",
    "        super().__init__()\n",
    "        hidden_dim = hidden_dim or dim * 4\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w3 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "    def forward(self, x):\n",
    "        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n",
    "\n",
    "class MLPMixer(nn.Module):\n",
    "    def __init__(self, seq_len, dim):\n",
    "        super().__init__()\n",
    "        self.token_mix = nn.Linear(seq_len, seq_len, bias=False)\n",
    "    def forward(self, x):\n",
    "        return self.token_mix(x.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.qkv = nn.Linear(dim, 3 * dim, bias=False)\n",
    "        self.out_proj = nn.Linear(dim, dim, bias=False)\n",
    "    def forward(self, x):\n",
    "        B, L, D = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, L, 3, self.n_heads, self.head_dim)\n",
    "        q, k, v = qkv.unbind(dim=2)\n",
    "        q, k, v = q.transpose(1,2), k.transpose(1,2), v.transpose(1,2)\n",
    "        attn = (q @ k.transpose(-2,-1)) * (self.head_dim ** -0.5)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = (attn @ v).transpose(1,2).reshape(B, L, D)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "class TRMLayer(nn.Module):\n",
    "    def __init__(self, dim, seq_len, use_attention=False, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(dim)\n",
    "        self.norm2 = RMSNorm(dim)\n",
    "        self.mixer = SelfAttention(dim, n_heads) if use_attention else MLPMixer(seq_len, dim)\n",
    "        self.ffn = SwiGLU(dim, hidden_dim=dim * 4)\n",
    "    def forward(self, x):\n",
    "        x = x + self.mixer(self.norm1(x))\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AblationTRM(nn.Module):\n",
    "    \"\"\"\n",
    "    TRM with configurable components for ablation studies.\n",
    "\n",
    "    Configurable:\n",
    "    - n_layers: number of layers in the recursive block\n",
    "    - use_attention: MLP vs self-attention mixing\n",
    "    - use_z: whether to use the latent reasoning feature z\n",
    "    - full_backprop: whether to backprop through all recursions or use stop-grad\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes=4, grid_size=4, dim=48,\n",
    "                 n_layers=2, use_attention=False, use_z=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.grid_size = grid_size\n",
    "        self.n_classes = n_classes\n",
    "        self.use_z = use_z\n",
    "        seq_len = grid_size * grid_size\n",
    "\n",
    "        # Feature dimension depends on whether we use z\n",
    "        feat_mult = 3 if use_z else 2  # [x, y, z] or [x, y]\n",
    "\n",
    "        self.input_embed = nn.Linear(n_classes + 1, dim, bias=False)\n",
    "        self.y_init = nn.Parameter(torch.randn(1, 1, dim) * 0.02)\n",
    "        if use_z:\n",
    "            self.z_init = nn.Parameter(torch.randn(1, 1, dim) * 0.02)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TRMLayer(dim * feat_mult, seq_len=seq_len, use_attention=use_attention)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.split_proj_y = nn.Linear(dim * feat_mult, dim, bias=False)\n",
    "        if use_z:\n",
    "            self.split_proj_z = nn.Linear(dim * feat_mult, dim, bias=False)\n",
    "        self.output_head = nn.Linear(dim, n_classes)\n",
    "\n",
    "    def embed_input(self, x):\n",
    "        B = x.shape[0]\n",
    "        x_flat = x.reshape(B, -1)\n",
    "        x_onehot = F.one_hot(x_flat.long(), num_classes=self.n_classes + 1).float()\n",
    "        return self.input_embed(x_onehot)\n",
    "\n",
    "    def recurse(self, x_emb, y, z=None):\n",
    "        if self.use_z and z is not None:\n",
    "            combined = torch.cat([x_emb, y, z], dim=-1)\n",
    "        else:\n",
    "            combined = torch.cat([x_emb, y], dim=-1)\n",
    "        for layer in self.layers:\n",
    "            combined = layer(combined)\n",
    "        y_new = self.split_proj_y(combined)\n",
    "        z_new = self.split_proj_z(combined) if self.use_z else None\n",
    "        return y_new, z_new\n",
    "\n",
    "    def forward_with_supervision(self, x, T=3, n=4, full_backprop=True):\n",
    "        B = x.shape[0]\n",
    "        seq_len = self.grid_size * self.grid_size\n",
    "        x_emb = self.embed_input(x)\n",
    "        y = self.y_init.expand(B, seq_len, -1)\n",
    "        z = self.z_init.expand(B, seq_len, -1) if self.use_z else None\n",
    "\n",
    "        checkpoints = []\n",
    "        for t in range(T):\n",
    "            for i in range(n - 1):\n",
    "                if full_backprop:\n",
    "                    y, z = self.recurse(x_emb, y, z)\n",
    "                else:\n",
    "                    # Stop-grad: detach y and z to prevent gradient flow\n",
    "                    y_d = y.detach()\n",
    "                    z_d = z.detach() if z is not None else None\n",
    "                    y, z = self.recurse(x_emb, y_d, z_d)\n",
    "\n",
    "            # Last recursion always has gradients\n",
    "            y, z = self.recurse(x_emb, y, z)\n",
    "            logits = self.output_head(y)\n",
    "            checkpoints.append(logits)\n",
    "\n",
    "        return checkpoints\n",
    "\n",
    "print(\"Ablation model ready!\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data Generation"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_4x4(grid, r, c, num):\n",
    "    if num in grid[r, :]: return False\n",
    "    if num in grid[:, c]: return False\n",
    "    box_r, box_c = 2*(r//2), 2*(c//2)\n",
    "    if num in grid[box_r:box_r+2, box_c:box_c+2]: return False\n",
    "    return True\n",
    "\n",
    "def solve_4x4(grid):\n",
    "    for r in range(4):\n",
    "        for c in range(4):\n",
    "            if grid[r, c] == 0:\n",
    "                nums = list(range(1, 5))\n",
    "                np.random.shuffle(nums)\n",
    "                for num in nums:\n",
    "                    if is_valid_4x4(grid, r, c, num):\n",
    "                        grid[r, c] = num\n",
    "                        if solve_4x4(grid): return True\n",
    "                        grid[r, c] = 0\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def generate_dataset(n, n_remove=8, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    puzzles, solutions = [], []\n",
    "    for _ in range(n):\n",
    "        grid = np.zeros((4,4), dtype=np.int64)\n",
    "        solve_4x4(grid)\n",
    "        sol = grid.copy()\n",
    "        idxs = np.random.choice(16, size=n_remove, replace=False)\n",
    "        puz = sol.copy()\n",
    "        for idx in idxs: puz[idx//4, idx%4] = 0\n",
    "        puzzles.append(puz); solutions.append(sol)\n",
    "    return np.array(puzzles), np.array(solutions)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_puz, train_sol = generate_dataset(1000, n_remove=8, seed=42)\n",
    "test_puz, test_sol = generate_dataset(200, n_remove=8, seed=999)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(train_puz), torch.tensor(train_sol)),\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(test_puz), torch.tensor(test_sol)),\n",
    "    batch_size=64, shuffle=False\n",
    ")\n",
    "print(f\"Train: {len(train_puz)} puzzles | Test: {len(test_puz)} puzzles\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Unified Training Function"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(config, n_epochs=25, verbose=True):\n",
    "    \"\"\"\n",
    "    Run a single ablation experiment.\n",
    "\n",
    "    config: dict with keys:\n",
    "        name, dim, n_layers, use_attention, use_z, T, n, full_backprop, use_ema\n",
    "    \"\"\"\n",
    "    name = config['name']\n",
    "    dim = config.get('dim', 48)\n",
    "    n_layers = config.get('n_layers', 2)\n",
    "    use_attention = config.get('use_attention', False)\n",
    "    use_z = config.get('use_z', True)\n",
    "    T = config.get('T', 3)\n",
    "    n = config.get('n', 4)\n",
    "    full_backprop = config.get('full_backprop', True)\n",
    "    use_ema = config.get('use_ema', True)\n",
    "\n",
    "    model = AblationTRM(\n",
    "        n_classes=4, grid_size=4, dim=dim,\n",
    "        n_layers=n_layers, use_attention=use_attention, use_z=use_z\n",
    "    ).to(device)\n",
    "\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "\n",
    "    # EMA setup\n",
    "    ema_shadow = None\n",
    "    if use_ema:\n",
    "        ema_shadow = {n: p.data.clone() for n, p in model.named_parameters() if p.requires_grad}\n",
    "\n",
    "    history = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for bx, by in train_loader:\n",
    "            bx, by = bx.to(device), by.to(device)\n",
    "            mask = (bx.reshape(bx.shape[0], -1) == 0)\n",
    "            targets = by.reshape(by.shape[0], -1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            checkpoints = model.forward_with_supervision(bx, T=T, n=n, full_backprop=full_backprop)\n",
    "\n",
    "            loss = 0\n",
    "            for logits in checkpoints:\n",
    "                targets_0idx = (targets - 1).clamp(min=0)\n",
    "                per_elem = F.cross_entropy(logits.reshape(-1, 4), targets_0idx.reshape(-1), reduction='none')\n",
    "                loss = loss + (per_elem * mask.reshape(-1).float()).sum() / mask.float().sum().clamp(min=1)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if use_ema and ema_shadow:\n",
    "                with torch.no_grad():\n",
    "                    for n_param, p in model.named_parameters():\n",
    "                        if p.requires_grad:\n",
    "                            ema_shadow[n_param] = 0.999 * ema_shadow[n_param] + 0.001 * p.data\n",
    "\n",
    "        # Evaluate (with EMA if enabled)\n",
    "        if use_ema and ema_shadow:\n",
    "            backup = {n: p.data.clone() for n, p in model.named_parameters() if p.requires_grad}\n",
    "            for n_param, p in model.named_parameters():\n",
    "                if p.requires_grad: p.data = ema_shadow[n_param]\n",
    "\n",
    "        model.eval()\n",
    "        correct, total, puzzles_correct, puzzles_total = 0, 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for bx, by in test_loader:\n",
    "                bx, by = bx.to(device), by.to(device)\n",
    "                mask = (bx.reshape(bx.shape[0], -1) == 0)\n",
    "                targets = by.reshape(by.shape[0], -1)\n",
    "                checkpoints = model.forward_with_supervision(bx, T=T, n=n, full_backprop=True)\n",
    "                preds = checkpoints[-1].argmax(dim=-1) + 1\n",
    "                c = (preds == targets) & mask\n",
    "                correct += c.sum().item()\n",
    "                total += mask.sum().item()\n",
    "                for i in range(bx.shape[0]):\n",
    "                    m = mask[i]\n",
    "                    if m.sum() > 0:\n",
    "                        puzzles_correct += c[i][m].all().item()\n",
    "                    puzzles_total += 1\n",
    "\n",
    "        if use_ema and ema_shadow:\n",
    "            for n_param, p in model.named_parameters():\n",
    "                if p.requires_grad: p.data = backup[n_param]\n",
    "\n",
    "        cell_acc = 100 * correct / max(total, 1)\n",
    "        puzzle_acc = 100 * puzzles_correct / max(puzzles_total, 1)\n",
    "        history.append(puzzle_acc)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    final_acc = history[-1]\n",
    "\n",
    "    if verbose:\n",
    "        eff_depth = T * (n+1) * n_layers if full_backprop else T * n_layers\n",
    "        print(f\"  {name:<35s} | Params: {n_params:>7,} | Eff.Depth: {eff_depth:>3} | \"\n",
    "              f\"Puzzle Acc: {final_acc:>5.1f}% | Time: {elapsed:.0f}s\")\n",
    "\n",
    "    return {\n",
    "        'name': name, 'final_acc': final_acc, 'n_params': n_params,\n",
    "        'history': history, 'time': elapsed, 'config': config\n",
    "    }"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo Rankings\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_04_todo_rankings.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_04_todo_rankings"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Your Turn: Predict the Rankings\n",
    "\n",
    "Before running the experiments, make your predictions!\n",
    "\n",
    "### TODO: Rank the ablation impacts"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO ============\n",
    "# Based on your understanding from Notebooks 1-3, rank these ablations\n",
    "# from MOST impactful to LEAST impactful (which removal hurts the most?).\n",
    "#\n",
    "# Assign ranks 1-5 (1 = most impactful removal, 5 = least impactful):\n",
    "# ==============================\n",
    "\n",
    "your_rankings = {\n",
    "    'No full backprop':     ???,  # Rank 1-5\n",
    "    'Attention vs MLP':     ???,  # Rank 1-5\n",
    "    '4 layers vs 2+recur':  ???,  # Rank 1-5\n",
    "    'No z feature':         ???,  # Rank 1-5\n",
    "    'No EMA':               ???,  # Rank 1-5\n",
    "}\n",
    "\n",
    "print(\"Your predicted impact rankings (1=most impactful removal):\")\n",
    "for name, rank in sorted(your_rankings.items(), key=lambda x: x[1]):\n",
    "    print(f\"  #{rank}: {name}\")\n",
    "print(\"\\nLet's see if you're right! Running experiments now...\\n\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Experiments\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_05_experiments.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_05_experiments"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Running the Ablation Experiments\n",
    "\n",
    "Now let us run the experiments. We will test 6 configurations:"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 90)\n",
    "print(\"  ABLATION STUDY ‚Äî 4√ó4 Sudoku\")\n",
    "print(\"=\" * 90)\n",
    "print()\n",
    "\n",
    "experiments = [\n",
    "    # Baseline: full TRM configuration\n",
    "    {'name': '1. Full TRM (baseline)',\n",
    "     'n_layers': 2, 'use_attention': False, 'use_z': True,\n",
    "     'T': 3, 'n': 4, 'full_backprop': True, 'use_ema': True},\n",
    "\n",
    "    # Ablation: stop-grad (no full backprop)\n",
    "    {'name': '2. No full backprop (stop-grad)',\n",
    "     'n_layers': 2, 'use_attention': False, 'use_z': True,\n",
    "     'T': 3, 'n': 4, 'full_backprop': False, 'use_ema': True},\n",
    "\n",
    "    # Ablation: attention instead of MLP\n",
    "    {'name': '3. Attention (instead of MLP)',\n",
    "     'n_layers': 2, 'use_attention': True, 'use_z': True,\n",
    "     'T': 3, 'n': 4, 'full_backprop': True, 'use_ema': True},\n",
    "\n",
    "    # Ablation: 4 layers with less recursion\n",
    "    {'name': '4. 4 layers, n=2 recursions',\n",
    "     'n_layers': 4, 'use_attention': False, 'use_z': True,\n",
    "     'T': 3, 'n': 2, 'full_backprop': True, 'use_ema': True},\n",
    "\n",
    "    # Ablation: no z feature\n",
    "    {'name': '5. No z (solution only)',\n",
    "     'n_layers': 2, 'use_attention': False, 'use_z': False,\n",
    "     'T': 3, 'n': 4, 'full_backprop': True, 'use_ema': True},\n",
    "\n",
    "    # Ablation: no EMA\n",
    "    {'name': '6. No EMA',\n",
    "     'n_layers': 2, 'use_attention': False, 'use_z': True,\n",
    "     'T': 3, 'n': 4, 'full_backprop': True, 'use_ema': False},\n",
    "]\n",
    "\n",
    "results = []\n",
    "for config in experiments:\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    result = run_experiment(config, n_epochs=25)\n",
    "    results.append(result)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 90)"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Bar chart of ablation results\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "names = [r['name'] for r in results]\n",
    "accs = [r['final_acc'] for r in results]\n",
    "colors = ['#2e7d32'] + ['#ef5350', '#ff9800', '#42a5f5', '#ab47bc', '#78909c']\n",
    "\n",
    "bars = ax.barh(range(len(names)), accs, color=colors, edgecolor='gray', linewidth=1.5, height=0.6)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, acc) in enumerate(zip(bars, accs)):\n",
    "    ax.text(acc + 0.5, i, f'{acc:.1f}%', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_yticks(range(len(names)))\n",
    "ax.set_yticklabels(names, fontsize=11)\n",
    "ax.set_xlabel('Test Puzzle Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Ablation Study: What Matters in TRM?', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlim(0, max(accs) + 10)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Curves\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_06_curves.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_06_curves"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Training curves for all experiments\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "colors_line = ['#2e7d32', '#ef5350', '#ff9800', '#42a5f5', '#ab47bc', '#78909c']\n",
    "for result, color in zip(results, colors_line):\n",
    "    ax.plot(result['history'], label=result['name'], linewidth=2, color=color)\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Test Puzzle Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Training Progression Across Ablation Experiments', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=9, loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo Custom\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_07_todo_custom.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_07_todo_custom"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîß Your Turn\n",
    "\n",
    "### TODO: Design Your Own Ablation\n",
    "\n",
    "Choose one hypothesis and test it by designing a new configuration."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO ============\n",
    "# Pick ONE hypothesis and create a config to test it:\n",
    "#\n",
    "# Hypothesis A: \"More recursion always helps\"\n",
    "#   ‚Üí Test n=8 vs n=4 vs n=2 (keep everything else the same)\n",
    "#\n",
    "# Hypothesis B: \"Bigger hidden dimension helps\"\n",
    "#   ‚Üí Test dim=32 vs dim=48 vs dim=64\n",
    "#\n",
    "# Hypothesis C: \"Deep supervision steps matter\"\n",
    "#   ‚Üí Test T=1 vs T=3 vs T=5\n",
    "#\n",
    "# Write your config below:\n",
    "# ==============================\n",
    "\n",
    "my_configs = [\n",
    "    # Example: testing more recursion\n",
    "    {'name': 'Custom: n=2 recursions',\n",
    "     'n_layers': 2, 'use_attention': False, 'use_z': True,\n",
    "     'T': 3, 'n': 2, 'full_backprop': True, 'use_ema': True},\n",
    "\n",
    "    {'name': 'Custom: n=4 recursions (baseline)',\n",
    "     'n_layers': 2, 'use_attention': False, 'use_z': True,\n",
    "     'T': 3, 'n': 4, 'full_backprop': True, 'use_ema': True},\n",
    "\n",
    "    # YOUR CONFIG HERE ‚Äî test n=8 or dim=64 or T=5\n",
    "    ???\n",
    "]\n",
    "\n",
    "print(\"Running your custom ablation...\\n\")\n",
    "my_results = []\n",
    "for config in my_configs:\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    result = run_experiment(config, n_epochs=25)\n",
    "    my_results.append(result)"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification: Plot your results\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "for result in my_results:\n",
    "    ax.plot(result['history'], label=result['name'], linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Test Puzzle Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Your Custom Ablation Results', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nYour ablation results:\")\n",
    "for r in my_results:\n",
    "    print(f\"  {r['name']}: {r['final_acc']:.1f}%\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Big Picture\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_08_big_picture.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_08_big_picture"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together ‚Äî The Parameter vs Compute Trade-off"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä The big picture: parameters vs accuracy\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Parameters vs accuracy\n",
    "params = [r['n_params'] for r in results]\n",
    "accs = [r['final_acc'] for r in results]\n",
    "colors_scatter = ['#2e7d32', '#ef5350', '#ff9800', '#42a5f5', '#ab47bc', '#78909c']\n",
    "\n",
    "for i, (p, a, c, r) in enumerate(zip(params, accs, colors_scatter, results)):\n",
    "    axes[0].scatter(p, a, c=c, s=200, edgecolors='gray', linewidth=1.5, zorder=5)\n",
    "    axes[0].annotate(r['name'].split('.')[0] + '.',\n",
    "                     (p, a), textcoords=\"offset points\",\n",
    "                     xytext=(10, 5), fontsize=9)\n",
    "\n",
    "axes[0].set_xlabel('Parameters', fontsize=12)\n",
    "axes[0].set_ylabel('Test Puzzle Accuracy (%)', fontsize=12)\n",
    "axes[0].set_title('Parameters vs Accuracy', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Effective depth vs accuracy\n",
    "eff_depths = []\n",
    "for r in results:\n",
    "    c = r['config']\n",
    "    if c['full_backprop']:\n",
    "        ed = c['T'] * (c['n'] + 1) * c['n_layers']\n",
    "    else:\n",
    "        ed = c['T'] * c['n_layers']\n",
    "    eff_depths.append(ed)\n",
    "\n",
    "for i, (d, a, c, r) in enumerate(zip(eff_depths, accs, colors_scatter, results)):\n",
    "    axes[1].scatter(d, a, c=c, s=200, edgecolors='gray', linewidth=1.5, zorder=5)\n",
    "    axes[1].annotate(r['name'].split('.')[0] + '.',\n",
    "                     (d, a), textcoords=\"offset points\",\n",
    "                     xytext=(10, 5), fontsize=9)\n",
    "\n",
    "axes[1].set_xlabel('Effective Depth (layers)', fontsize=12)\n",
    "axes[1].set_ylabel('Test Puzzle Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Effective Depth vs Accuracy', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('The Parameters vs Compute Trade-off', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Key insight: Effective depth (recursion) matters MORE than parameter count!\")\n",
    "print(\"   The baseline has fewer parameters than config 4 but higher accuracy,\")\n",
    "print(\"   because it achieves greater effective depth through more recursion.\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Report\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_09_report.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_09_report"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üéØ Final Output ‚Äî Complete Ablation Report"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a comprehensive ablation report\n",
    "print(\"=\" * 80)\n",
    "print(\"  ABLATION STUDY REPORT ‚Äî Tiny Recursive Models on 4√ó4 Sudoku\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "baseline_acc = results[0]['final_acc']\n",
    "print(f\"  Baseline accuracy: {baseline_acc:.1f}%\")\n",
    "print(f\"  (2 layers, MLP mixing, with z, T=3, n=4, full backprop, EMA)\")\n",
    "print()\n",
    "print(f\"  {'Experiment':<35s} | {'Accuracy':>8s} | {'Œî vs baseline':>13s} | {'Key finding'}\")\n",
    "print(\"  \" + \"-\" * 100)\n",
    "\n",
    "findings = [\n",
    "    \"Reference configuration\",\n",
    "    \"Full backprop is critical for learning\",\n",
    "    \"MLP beats attention on small fixed grids\",\n",
    "    \"More recursion > more layers\",\n",
    "    \"Reasoning scratchpad z is essential\",\n",
    "    \"EMA stabilizes training\"\n",
    "]\n",
    "\n",
    "for r, finding in zip(results, findings):\n",
    "    delta = r['final_acc'] - baseline_acc\n",
    "    delta_str = f\"{'+'if delta>=0 else ''}{delta:.1f}%\"\n",
    "    print(f\"  {r['name']:<35s} | {r['final_acc']:>7.1f}% | {delta_str:>13s} | {finding}\")\n",
    "\n",
    "print()\n",
    "print(\"  \" + \"=\" * 100)\n",
    "print()\n",
    "\n",
    "# Rank by impact\n",
    "impacts = [(r['name'], baseline_acc - r['final_acc'], findings[i])\n",
    "           for i, r in enumerate(results[1:])]\n",
    "impacts.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"  Components ranked by impact (removing each one):\")\n",
    "for name, impact, finding in impacts:\n",
    "    if impact > 0:\n",
    "        print(f\"    {impact:+.1f}% ‚Äî {name}: {finding}\")\n",
    "    else:\n",
    "        print(f\"    {impact:+.1f}% ‚Äî {name}: {finding}\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Final visualization: impact bar chart\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "names_short = [\n",
    "    'No full backprop',\n",
    "    'Attention\\n(vs MLP)',\n",
    "    '4 layers\\n(vs 2+recursion)',\n",
    "    'No z feature',\n",
    "    'No EMA'\n",
    "]\n",
    "impacts_val = [baseline_acc - r['final_acc'] for r in results[1:]]\n",
    "bar_colors = ['#ef5350' if v > 0 else '#66bb6a' for v in impacts_val]\n",
    "\n",
    "bars = ax.bar(range(len(names_short)), impacts_val, color=bar_colors,\n",
    "              edgecolor='gray', linewidth=1.5, width=0.6)\n",
    "\n",
    "for bar, val in zip(bars, impacts_val):\n",
    "    y_pos = bar.get_height() + 0.3 if val > 0 else bar.get_height() - 1\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, y_pos,\n",
    "            f'{val:+.1f}%', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_xticks(range(len(names_short)))\n",
    "ax.set_xticklabels(names_short, fontsize=10)\n",
    "ax.set_ylabel('Accuracy Drop (percentage points)', fontsize=12)\n",
    "ax.set_title('Impact of Removing Each Component\\n(Higher = More Important)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéâ Congratulations! You have completed the ablation study.\")\n",
    "print(\"   You now understand WHY each component of TRM matters, not just WHAT it does.\")\n",
    "print(f\"\\n   The key lesson: recursive depth (thinking longer) beats parameter count (thinking bigger).\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_10_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_10_closing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reflection and Next Steps\n",
    "\n",
    "### üí° Key Takeaways\n",
    "\n",
    "1. **Full backpropagation** is the single most important factor ‚Äî it enables the model to learn from the full recursion chain\n",
    "2. **MLP mixing** beats attention for small, fixed-size grids ‚Äî simpler is better when the context fits in a small matrix\n",
    "3. **More recursion with fewer layers** beats more layers with less recursion ‚Äî computational depth without parameter growth\n",
    "4. **The reasoning scratchpad z** is essential for generalization ‚Äî the model needs internal state that is not directly part of the answer\n",
    "5. **EMA** provides stability, especially on small datasets\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "\n",
    "1. Why does removing full backpropagation hurt so much? (Hint: think about what the model can learn from 1-step gradients vs multi-step gradients.)\n",
    "2. The paper's Sudoku-Extreme results show +30.9% from full backprop ‚Äî much larger than our 4√ó4 result. Why would the impact be larger on harder puzzles?\n",
    "3. If you had unlimited compute but only 1,000 training examples, how would you design the TRM? What T, n, and n_layers would you choose?\n",
    "\n",
    "### üèÜ Optional Challenges\n",
    "\n",
    "1. **Recursion budget ablation:** Fix the total number of forward passes (e.g., 12) and vary the split between T and n. Is T=3, n=4 optimal, or would T=2, n=6 be better?\n",
    "2. **Data efficiency:** Train with 100, 500, and 1000 examples. At what data size does EMA stop helping?\n",
    "3. **Transfer learning:** Train on easy puzzles (4 empty cells), then fine-tune on hard puzzles (10 empty cells). Does pre-training help?\n",
    "\n",
    "### Series Summary\n",
    "\n",
    "Across these 4 notebooks, you have:\n",
    "1. **Understood** recursive reasoning through constraint propagation\n",
    "2. **Built** the complete TRM architecture from scratch (RMSNorm, SwiGLU, RoPE, MLP/Attention variants)\n",
    "3. **Trained** TRM with deep supervision, prediction loss, halting loss, and EMA\n",
    "4. **Validated** each design choice through systematic ablation experiments\n",
    "\n",
    "The central lesson of Tiny Recursive Models: **Less is More.** A tiny model that thinks recursively outperforms giant models that think once. Recursive depth beats parameter count. Thinking longer beats thinking bigger."
   ],
   "id": "cell_27"
  }
 ]
}