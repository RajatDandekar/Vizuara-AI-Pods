{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Building the TRM Architecture ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1TR8B7sH0a1GpUcd-1quRGmL_zhGcJtec\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/02_00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Building the Tiny Recursive Model Architecture from Scratch\n",
    "\n",
    "*Part 2 of the Vizuara series on Tiny Recursive Models*\n",
    "*Estimated time: 40 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ü§ñ AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** ‚Äî it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[üëâ Open AI Teaching Assistant](https://pods.vizuara.ai/courses/tiny-recursive-models/practice/2/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebook, we saw that recursive constraint propagation can solve Sudoku puzzles that single-pass approaches cannot. But we hand-coded the rules ‚Äî \"eliminate values in the same row, column, box.\"\n",
    "\n",
    "What if the model could **learn** its own constraint-propagation rules from data?\n",
    "\n",
    "That is exactly what the Tiny Recursive Model (TRM) does. With just **7 million parameters** and a **2-layer network** applied recursively, it learns to reason about abstract patterns ‚Äî achieving 87.4% on extreme Sudoku and 44.6% on ARC-AGI-1.\n",
    "\n",
    "By the end of this notebook, you will have built the complete TRM architecture from scratch:\n",
    "- RMSNorm normalization\n",
    "- SwiGLU gated activation\n",
    "- Rotary Position Embeddings (RoPE)\n",
    "- The 2-layer recursive block (both MLP and Attention variants)\n",
    "- The full recursion loop with solution (y) and reasoning (z) features"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_01_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_01_intuition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### The Architecture at a Glance\n",
    "\n",
    "Think of TRM as a very small brain running in a loop:\n",
    "\n",
    "1. **Inputs:** The puzzle (x), current solution guess (y), and reasoning scratchpad (z)\n",
    "2. **Processing:** A tiny 2-layer network processes all three together\n",
    "3. **Outputs:** Updated solution (y) and updated reasoning state (z)\n",
    "4. **Loop:** Feed the outputs back as inputs and repeat\n",
    "\n",
    "The key insight: **the same 2-layer network is used for every pass**. No new weights are learned between passes ‚Äî the model just gets better because each pass produces new context for the next.\n",
    "\n",
    "### Why 2 Layers?\n",
    "\n",
    "You might think more layers = better. But the TRM paper showed that **2 layers with more recursion** outperforms **4 layers with less recursion** by 7.9 percentage points. Why?\n",
    "\n",
    "- More layers = more parameters = more overfitting risk (especially on small datasets)\n",
    "- More recursion = more computational depth **without** adding parameters\n",
    "- With T=3 supervision steps and n=6 recursions: effective depth = 3 √ó 7 √ó 2 = **42 layers**\n",
    "\n",
    "The model gets the depth of a 42-layer transformer using only 2 layers of weights.\n",
    "\n",
    "### ü§î Think About This\n",
    "\n",
    "If you had a budget of 7 million parameters, would you rather have:\n",
    "- (A) A 14-layer network, each layer with 500K parameters, processing input once?\n",
    "- (B) A 2-layer network with 3.5M parameters each, processing input 21 times?\n",
    "\n",
    "The TRM paper shows (B) wins dramatically. Can you intuit why?"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_02_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_02_math"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### The Recursion Equations\n",
    "\n",
    "The TRM recursion has two phases per step:\n",
    "\n",
    "**Phase 1 ‚Äî Update reasoning state** (repeat $n$ times):\n",
    "$$z \\leftarrow \\text{net}(x, y, z)$$\n",
    "\n",
    "**Phase 2 ‚Äî Refine solution:**\n",
    "$$y \\leftarrow \\text{net}(y, z)$$\n",
    "\n",
    "Where $\\text{net}$ is the same 2-layer network. Computationally: concatenate the inputs along the feature dimension, pass through two transformer-like layers, and split the output back into the appropriate feature sizes.\n",
    "\n",
    "### Inside Each Layer\n",
    "\n",
    "Each of the 2 layers contains:\n",
    "\n",
    "**RMSNorm** ‚Äî normalizes activations using root-mean-square:\n",
    "$$\\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d}\\sum_{i=1}^{d} x_i^2 + \\epsilon}} \\odot \\gamma$$\n",
    "\n",
    "This is simpler than LayerNorm (no mean subtraction) but equally effective. Computationally: compute the RMS of the vector, divide each element by it, then scale by learnable parameter $\\gamma$.\n",
    "\n",
    "**SwiGLU activation** ‚Äî a gated activation function:\n",
    "$$\\text{SwiGLU}(x) = (\\text{Swish}(xW_1) \\odot xW_2) W_3$$\n",
    "\n",
    "Where $\\text{Swish}(x) = x \\cdot \\sigma(x)$. Computationally: project the input through two parallel linear layers, apply Swish to one of them, multiply element-wise, then project back down. The \"gate\" ($xW_2$) learns which features to pass through.\n",
    "\n",
    "**Rotary Position Embeddings (RoPE):**\n",
    "$$\\text{RoPE}(x_m, m) = x_m \\cdot \\cos(m\\theta) + \\text{rotate}(x_m) \\cdot \\sin(m\\theta)$$\n",
    "\n",
    "This encodes position $m$ directly into the query/key vectors via rotation in 2D subspaces. Computationally: pair up consecutive dimensions, rotate each pair by an angle that depends on the position.\n",
    "\n",
    "### Effective Depth\n",
    "\n",
    "With $T$ supervision steps, $n$ recursions per step, and $n_\\text{layers}$ layers per recursion:\n",
    "\n",
    "$$\\text{Effective depth} = T \\times (n + 1) \\times n_\\text{layers}$$\n",
    "\n",
    "For TRM: $3 \\times 7 \\times 2 = 42$ effective layers."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Rmsnorm\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_03_rmsnorm.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_03_rmsnorm"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It ‚Äî Component by Component\n",
    "\n",
    "### 4.1 RMSNorm\n",
    "\n",
    "RMSNorm is the simplest component. It normalizes the input by its root-mean-square value."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Root Mean Square Layer Normalization.\n",
    "    Simpler than LayerNorm ‚Äî no mean subtraction, just scale by RMS.\n",
    "\n",
    "    Used in: LLaMA, Gemini, TRM\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))  # Learnable scale (gamma)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape: (batch, seq_len, dim)\n",
    "        # Compute RMS along the last dimension\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        # Normalize and scale\n",
    "        return (x / rms) * self.weight\n",
    "\n",
    "# Test it\n",
    "norm = RMSNorm(dim=8)\n",
    "test_input = torch.randn(2, 4, 8)  # batch=2, seq_len=4, dim=8\n",
    "output = norm(test_input)\n",
    "print(f\"Input shape:  {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Input RMS:    {torch.sqrt(torch.mean(test_input**2, dim=-1))}\")\n",
    "print(f\"Output RMS:   {torch.sqrt(torch.mean(output**2, dim=-1))}\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize: RMSNorm stabilizes activations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Generate data with varying scales\n",
    "x = torch.randn(1, 10, 32)\n",
    "x[:, 5:, :] *= 5  # Make some positions have much larger magnitude\n",
    "\n",
    "x_normed = norm(x[:, :, :8])  # Apply to first 8 dims (matching norm dim)\n",
    "\n",
    "axes[0].imshow(x[0].numpy(), cmap='RdBu', vmin=-5, vmax=5, aspect='auto')\n",
    "axes[0].set_title(\"Before RMSNorm\", fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel(\"Feature dimension\")\n",
    "axes[0].set_ylabel(\"Sequence position\")\n",
    "\n",
    "# For comparison, normalize all 32 dims\n",
    "norm32 = RMSNorm(dim=32)\n",
    "x_full_normed = norm32(x)\n",
    "axes[1].imshow(x_full_normed[0].detach().numpy(), cmap='RdBu', vmin=-5, vmax=5, aspect='auto')\n",
    "axes[1].set_title(\"After RMSNorm\", fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel(\"Feature dimension\")\n",
    "axes[1].set_ylabel(\"Sequence position\")\n",
    "\n",
    "plt.suptitle(\"RMSNorm Stabilizes Activation Magnitudes\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Swiglu\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_04_swiglu.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_04_swiglu"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 SwiGLU Activation\n",
    "\n",
    "SwiGLU is a gated activation function. The \"gate\" learns which features to let through ‚Äî it acts like a learned filter."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    SwiGLU gated activation function.\n",
    "    Projects input to 2 * hidden_dim, splits into two halves,\n",
    "    applies Swish to one half, element-wise multiply, project back.\n",
    "\n",
    "    Swish(x) = x * sigmoid(x)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, hidden_dim: int = None):\n",
    "        super().__init__()\n",
    "        hidden_dim = hidden_dim or dim * 4  # Standard 4x expansion\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)  # For Swish path\n",
    "        self.w2 = nn.Linear(dim, hidden_dim, bias=False)  # For gate path\n",
    "        self.w3 = nn.Linear(hidden_dim, dim, bias=False)   # Project back\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Swish gate: x * sigmoid(x) applied to w1 projection\n",
    "        swish_out = F.silu(self.w1(x))  # silu = Swish = x * sigmoid(x)\n",
    "        # Gate: linear projection through w2\n",
    "        gate = self.w2(x)\n",
    "        # Element-wise multiply and project back\n",
    "        return self.w3(swish_out * gate)\n",
    "\n",
    "# Test it\n",
    "swiglu = SwiGLU(dim=16, hidden_dim=32)\n",
    "test_x = torch.randn(2, 4, 16)\n",
    "out = swiglu(test_x)\n",
    "print(f\"Input shape:  {test_x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "n_params = sum(p.numel() for p in swiglu.parameters())\n",
    "print(f\"Parameters:   {n_params:,}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize: Swish activation function\n",
    "x_plot = torch.linspace(-5, 5, 200)\n",
    "relu_y = F.relu(x_plot)\n",
    "gelu_y = F.gelu(x_plot)\n",
    "swish_y = F.silu(x_plot)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x_plot.numpy(), relu_y.numpy(), label='ReLU', linewidth=2, alpha=0.7)\n",
    "plt.plot(x_plot.numpy(), gelu_y.numpy(), label='GELU', linewidth=2, alpha=0.7)\n",
    "plt.plot(x_plot.numpy(), swish_y.numpy(), label='Swish (SiLU)', linewidth=2.5, color='#e65100')\n",
    "plt.xlabel('Input', fontsize=12)\n",
    "plt.ylabel('Output', fontsize=12)\n",
    "plt.title('Swish vs ReLU vs GELU ‚Äî Swish is Smooth and Allows Negative Signal', fontsize=13, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='black', linewidth=0.5)\n",
    "plt.axvline(x=0, color='black', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Rope\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_05_rope.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_05_rope"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Rotary Position Embeddings (RoPE)\n",
    "\n",
    "RoPE encodes position information by rotating pairs of dimensions. Position $m$ gets a rotation angle $m\\theta_i$ for each dimension pair $i$."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary Position Embedding (RoPE).\n",
    "    Encodes position by rotating consecutive dimension pairs.\n",
    "    The rotation angle depends on the position ‚Äî closer positions have similar rotations.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, max_seq_len: int = 1024, base: float = 10000.0):\n",
    "        super().__init__()\n",
    "        # Compute rotation frequencies for each dimension pair\n",
    "        # theta_i = 1 / (base^(2i/dim)) for i = 0, 1, ..., dim/2 - 1\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "        # Precompute sin/cos for all positions\n",
    "        positions = torch.arange(max_seq_len).float()\n",
    "        # angles shape: (max_seq_len, dim/2)\n",
    "        angles = positions.unsqueeze(1) * inv_freq.unsqueeze(0)\n",
    "        self.register_buffer('cos_cache', angles.cos())\n",
    "        self.register_buffer('sin_cache', angles.sin())\n",
    "\n",
    "    def forward(self, x: torch.Tensor, seq_len: int = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply rotary embeddings to input tensor.\n",
    "        x shape: (batch, seq_len, dim)\n",
    "        \"\"\"\n",
    "        if seq_len is None:\n",
    "            seq_len = x.shape[1]\n",
    "        cos = self.cos_cache[:seq_len]  # (seq_len, dim/2)\n",
    "        sin = self.sin_cache[:seq_len]  # (seq_len, dim/2)\n",
    "\n",
    "        # Split x into pairs of consecutive dimensions\n",
    "        x1 = x[..., 0::2]  # Even indices\n",
    "        x2 = x[..., 1::2]  # Odd indices\n",
    "\n",
    "        # Apply rotation: [x1, x2] -> [x1*cos - x2*sin, x1*sin + x2*cos]\n",
    "        rotated_x1 = x1 * cos - x2 * sin\n",
    "        rotated_x2 = x1 * sin + x2 * cos\n",
    "\n",
    "        # Interleave back\n",
    "        out = torch.stack([rotated_x1, rotated_x2], dim=-1).flatten(-2)\n",
    "        return out\n",
    "\n",
    "# Test it\n",
    "rope = RotaryPositionEmbedding(dim=16, max_seq_len=81)\n",
    "test_x = torch.randn(1, 9, 16)\n",
    "out = rope(test_x)\n",
    "print(f\"Input shape:  {test_x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize: How RoPE rotations change with position\n",
    "rope_vis = RotaryPositionEmbedding(dim=8, max_seq_len=20)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Show rotation angles for different dimension pairs\n",
    "positions = np.arange(20)\n",
    "for i in range(4):\n",
    "    angles = positions * rope_vis.inv_freq[i].item()\n",
    "    axes[0].plot(positions, np.cos(angles), label=f'Dim pair {i}', linewidth=2)\n",
    "\n",
    "axes[0].set_xlabel('Position', fontsize=12)\n",
    "axes[0].set_ylabel('cos(position √ó frequency)', fontsize=12)\n",
    "axes[0].set_title('RoPE Rotation Patterns per Dimension Pair', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Show how similar positions get similar embeddings\n",
    "x_same = torch.ones(1, 20, 8)  # Same content at every position\n",
    "x_rotated = rope_vis(x_same)\n",
    "\n",
    "# Compute cosine similarity between all position pairs\n",
    "sims = F.cosine_similarity(\n",
    "    x_rotated[0].unsqueeze(0).expand(20, -1, -1),\n",
    "    x_rotated[0].unsqueeze(1).expand(-1, 20, -1),\n",
    "    dim=-1\n",
    ")\n",
    "im = axes[1].imshow(sims.detach().numpy(), cmap='viridis', vmin=-1, vmax=1)\n",
    "axes[1].set_xlabel('Position', fontsize=12)\n",
    "axes[1].set_ylabel('Position', fontsize=12)\n",
    "axes[1].set_title('Position Similarity (Closer = More Similar)', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(im, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Mixer Attn\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_06_mixer_attn.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_06_mixer_attn"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The MLP Mixing Layer\n",
    "\n",
    "For small, fixed-size grids (like 9√ó9 Sudoku = 81 tokens), TRM uses a simple linear layer instead of self-attention. This is a matrix multiplication of size [L, L] that learns how tokens should communicate."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPMixer(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP token mixing layer ‚Äî replaces self-attention for fixed-size contexts.\n",
    "    A simple [L, L] matrix that learns pairwise token interactions.\n",
    "    Much cheaper than attention when seq_len is small and fixed.\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len: int, dim: int):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        # This is the key: a learnable [L, L] mixing matrix\n",
    "        self.token_mix = nn.Linear(seq_len, seq_len, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape: (batch, seq_len, dim)\n",
    "        # Transpose to mix across token dimension, then transpose back\n",
    "        x = x.transpose(1, 2)       # (batch, dim, seq_len)\n",
    "        x = self.token_mix(x)        # (batch, dim, seq_len) ‚Äî mixing happens here\n",
    "        x = x.transpose(1, 2)       # (batch, seq_len, dim)\n",
    "        return x\n",
    "\n",
    "# Test it\n",
    "mixer = MLPMixer(seq_len=16, dim=32)\n",
    "test_x = torch.randn(2, 16, 32)\n",
    "out = mixer(test_x)\n",
    "print(f\"Input shape:  {test_x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "n_params = sum(p.numel() for p in mixer.parameters())\n",
    "print(f\"Parameters:   {n_params:,} (just a {16}√ó{16} matrix!)\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Self-Attention (for variable-size contexts)\n",
    "\n",
    "For larger, variable-size grids, TRM uses standard self-attention with RoPE."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard multi-head self-attention with RoPE.\n",
    "    Used for variable-length contexts (e.g., 30x30 mazes).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, n_heads: int = 4, max_seq_len: int = 1024):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = dim // n_heads\n",
    "        assert dim % n_heads == 0\n",
    "\n",
    "        self.qkv = nn.Linear(dim, 3 * dim, bias=False)\n",
    "        self.out_proj = nn.Linear(dim, dim, bias=False)\n",
    "        self.rope = RotaryPositionEmbedding(self.head_dim, max_seq_len)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, L, D = x.shape\n",
    "\n",
    "        # Project to Q, K, V\n",
    "        qkv = self.qkv(x).reshape(B, L, 3, self.n_heads, self.head_dim)\n",
    "        q, k, v = qkv.unbind(dim=2)  # Each: (B, L, n_heads, head_dim)\n",
    "\n",
    "        # Apply RoPE to Q and K\n",
    "        q = q.transpose(1, 2)  # (B, n_heads, L, head_dim)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # Apply RoPE per head\n",
    "        q_rope = self.rope(q.reshape(B * self.n_heads, L, self.head_dim))\n",
    "        k_rope = self.rope(k.reshape(B * self.n_heads, L, self.head_dim))\n",
    "        q = q_rope.reshape(B, self.n_heads, L, self.head_dim)\n",
    "        k = k_rope.reshape(B, self.n_heads, L, self.head_dim)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scale = self.head_dim ** -0.5\n",
    "        attn = (q @ k.transpose(-2, -1)) * scale  # (B, n_heads, L, L)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        # Apply attention to values\n",
    "        out = (attn @ v)  # (B, n_heads, L, head_dim)\n",
    "        out = out.transpose(1, 2).reshape(B, L, D)  # (B, L, D)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "# Test it\n",
    "attn = SelfAttention(dim=32, n_heads=4)\n",
    "test_x = torch.randn(2, 16, 32)\n",
    "out = attn(test_x)\n",
    "print(f\"Input shape:  {test_x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Trm Layer\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_07_trm_layer.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_07_trm_layer"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 The TRM Layer\n",
    "\n",
    "Now we combine everything into a single TRM layer: RMSNorm ‚Üí Mixing (MLP or Attention) ‚Üí RMSNorm ‚Üí SwiGLU FFN."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRMLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    One layer of the Tiny Recursive Model.\n",
    "    Architecture: RMSNorm ‚Üí Token Mixing ‚Üí Residual ‚Üí RMSNorm ‚Üí SwiGLU FFN ‚Üí Residual\n",
    "\n",
    "    This is inspired by the Pre-Norm Transformer block but can use either\n",
    "    MLP mixing (for fixed-size) or self-attention (for variable-size).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, seq_len: int = None, use_attention: bool = False,\n",
    "                 n_heads: int = 4, ffn_mult: int = 4):\n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(dim)\n",
    "        self.norm2 = RMSNorm(dim)\n",
    "\n",
    "        # Token mixing: MLP or Attention\n",
    "        if use_attention:\n",
    "            self.mixer = SelfAttention(dim, n_heads=n_heads, max_seq_len=seq_len or 1024)\n",
    "        else:\n",
    "            assert seq_len is not None, \"MLP mixer requires fixed seq_len\"\n",
    "            self.mixer = MLPMixer(seq_len, dim)\n",
    "\n",
    "        # Feed-forward network with SwiGLU\n",
    "        self.ffn = SwiGLU(dim, hidden_dim=dim * ffn_mult)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Pre-norm architecture with residual connections\n",
    "        x = x + self.mixer(self.norm1(x))  # Mixing + residual\n",
    "        x = x + self.ffn(self.norm2(x))    # FFN + residual\n",
    "        return x\n",
    "\n",
    "# Test it\n",
    "layer = TRMLayer(dim=32, seq_len=16, use_attention=False)\n",
    "test_x = torch.randn(2, 16, 32)\n",
    "out = layer(test_x)\n",
    "print(f\"Input shape:  {test_x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "n_params = sum(p.numel() for p in layer.parameters())\n",
    "print(f\"Layer parameters: {n_params:,}\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Full Trm\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_08_full_trm.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_08_full_trm"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 The Full TRM\n",
    "\n",
    "Now for the main event ‚Äî the complete Tiny Recursive Model with the recursion loop."
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyRecursiveModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The complete Tiny Recursive Model (TRM).\n",
    "\n",
    "    Architecture:\n",
    "    - Input embedding: projects x into hidden dimension\n",
    "    - Solution embedding: projects initial y (zeros) into hidden dimension\n",
    "    - Reasoning embedding: projects initial z (zeros) into hidden dimension\n",
    "    - 2-layer recursive block (applied n times per supervision step)\n",
    "    - Output head: decodes y into class predictions\n",
    "\n",
    "    Recursion loop:\n",
    "    1. Concatenate [x, y, z] along feature dimension\n",
    "    2. Pass through 2-layer block ‚Üí get updated features\n",
    "    3. Split back into y and z\n",
    "    4. Repeat n times (this is one supervision step)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes: int, grid_size: int, dim: int = 64,\n",
    "                 n_layers: int = 2, use_attention: bool = False, n_heads: int = 4):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.grid_size = grid_size\n",
    "        seq_len = grid_size * grid_size\n",
    "\n",
    "        # Input embedding: one-hot class ‚Üí dim\n",
    "        self.input_embed = nn.Linear(n_classes + 1, dim, bias=False)  # +1 for \"empty\" token\n",
    "        # Solution and reasoning are initialized as learned embeddings\n",
    "        self.y_init = nn.Parameter(torch.randn(1, 1, dim) * 0.02)\n",
    "        self.z_init = nn.Parameter(torch.randn(1, 1, dim) * 0.02)\n",
    "\n",
    "        # The 2-layer recursive block\n",
    "        self.layers = nn.ModuleList([\n",
    "            TRMLayer(dim * 3, seq_len=seq_len, use_attention=use_attention, n_heads=n_heads)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # Projection to split 3*dim back into dim for y and z\n",
    "        self.split_proj_y = nn.Linear(dim * 3, dim, bias=False)\n",
    "        self.split_proj_z = nn.Linear(dim * 3, dim, bias=False)\n",
    "\n",
    "        # Output head: y ‚Üí class predictions\n",
    "        self.output_head = nn.Linear(dim, n_classes)\n",
    "\n",
    "        # Halting head: y ‚Üí confidence score\n",
    "        self.halt_head = nn.Linear(dim, 1)\n",
    "\n",
    "    def embed_input(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Convert integer grid to embedded features.\"\"\"\n",
    "        # x shape: (batch, grid_size, grid_size) with integer values 0..n_classes\n",
    "        B = x.shape[0]\n",
    "        x_flat = x.reshape(B, -1)  # (batch, seq_len)\n",
    "        # One-hot encode\n",
    "        x_onehot = F.one_hot(x_flat.long(), num_classes=self.output_head.out_features + 1).float()\n",
    "        return self.input_embed(x_onehot)  # (batch, seq_len, dim)\n",
    "\n",
    "    def recurse(self, x_emb: torch.Tensor, y: torch.Tensor, z: torch.Tensor) -> tuple:\n",
    "        \"\"\"\n",
    "        One recursion step: z ‚Üê net(x, y, z) and y ‚Üê net(y, z).\n",
    "        In practice, we concatenate [x, y, z], pass through layers, and split.\n",
    "        \"\"\"\n",
    "        # Concatenate along feature dimension\n",
    "        combined = torch.cat([x_emb, y, z], dim=-1)  # (batch, seq_len, 3*dim)\n",
    "\n",
    "        # Pass through the 2-layer block\n",
    "        for layer in self.layers:\n",
    "            combined = layer(combined)\n",
    "\n",
    "        # Split back into y and z updates\n",
    "        y_new = self.split_proj_y(combined)\n",
    "        z_new = self.split_proj_z(combined)\n",
    "\n",
    "        return y_new, z_new\n",
    "\n",
    "    def forward(self, x: torch.Tensor, n_recursions: int = 6) -> dict:\n",
    "        \"\"\"\n",
    "        Full forward pass with n recursions.\n",
    "\n",
    "        Args:\n",
    "            x: input grid, shape (batch, grid_size, grid_size), integer values\n",
    "            n_recursions: number of recursion iterations\n",
    "\n",
    "        Returns:\n",
    "            dict with 'logits', 'halt_logits', and 'y_history' (for visualization)\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        seq_len = self.grid_size * self.grid_size\n",
    "\n",
    "        # Embed input\n",
    "        x_emb = self.embed_input(x)  # (batch, seq_len, dim)\n",
    "\n",
    "        # Initialize y and z\n",
    "        y = self.y_init.expand(B, seq_len, -1)  # (batch, seq_len, dim)\n",
    "        z = self.z_init.expand(B, seq_len, -1)  # (batch, seq_len, dim)\n",
    "\n",
    "        # Track y at each recursion step (for visualization)\n",
    "        y_history = []\n",
    "\n",
    "        # Recursion loop\n",
    "        for step in range(n_recursions):\n",
    "            y, z = self.recurse(x_emb, y, z)\n",
    "            y_history.append(y.detach())\n",
    "\n",
    "        # Decode final y into class predictions\n",
    "        logits = self.output_head(y)  # (batch, seq_len, n_classes)\n",
    "        halt_logits = self.halt_head(y).squeeze(-1)  # (batch, seq_len)\n",
    "\n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'halt_logits': halt_logits,\n",
    "            'y_history': y_history\n",
    "        }\n",
    "\n",
    "# Create a TRM for 4x4 Sudoku (values 1-4, plus 0 for empty)\n",
    "model = TinyRecursiveModel(\n",
    "    n_classes=4,\n",
    "    grid_size=4,\n",
    "    dim=32,\n",
    "    n_layers=2,\n",
    "    use_attention=False\n",
    ")\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params:,}\")\n",
    "print(f\"\\nFor comparison:\")\n",
    "print(f\"  TRM paper (Sudoku):  ~5M parameters\")\n",
    "print(f\"  TRM paper (ARC-AGI): ~7M parameters\")\n",
    "print(f\"  Our mini model:      {n_params:,} parameters\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "test_grid = torch.randint(0, 5, (2, 4, 4))  # batch=2, 4x4 grid\n",
    "output = model(test_grid, n_recursions=6)\n",
    "\n",
    "print(f\"Input shape:       {test_grid.shape}\")\n",
    "print(f\"Logits shape:      {output['logits'].shape}  (batch, seq_len, n_classes)\")\n",
    "print(f\"Halt logits shape: {output['halt_logits'].shape}  (batch, seq_len)\")\n",
    "print(f\"Recursion steps:   {len(output['y_history'])}\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize: How y evolves across recursion steps\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "for step_idx in range(6):\n",
    "    ax = axes[step_idx // 3][step_idx % 3]\n",
    "    y_step = output['y_history'][step_idx][0]  # First sample\n",
    "\n",
    "    # Show the prediction at this step\n",
    "    logits_step = model.output_head(y_step)\n",
    "    probs = F.softmax(logits_step, dim=-1).detach().numpy()\n",
    "\n",
    "    # Show probability distribution as a heatmap\n",
    "    im = ax.imshow(probs, cmap='Blues', aspect='auto', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Recursion Step {step_idx + 1}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Class (1-4)')\n",
    "    ax.set_ylabel('Cell position')\n",
    "    ax.set_xticks(range(4))\n",
    "    ax.set_xticklabels(['1', '2', '3', '4'])\n",
    "\n",
    "plt.suptitle('Prediction Confidence Evolves Across Recursion Steps\\n(untrained model ‚Äî random predictions)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_09_todo.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_09_todo"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîß Your Turn\n",
    "\n",
    "### TODO: Implement the Attention Variant\n",
    "\n",
    "The model above uses MLP mixing. Now implement the attention variant by modifying the model creation."
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO ============\n",
    "# Create a TRM that uses self-attention instead of MLP mixing.\n",
    "# Hint: just change one parameter in the constructor.\n",
    "# Then compare the parameter counts.\n",
    "# ==============================\n",
    "\n",
    "model_attn = TinyRecursiveModel(\n",
    "    n_classes=4,\n",
    "    grid_size=4,\n",
    "    dim=32,\n",
    "    n_layers=2,\n",
    "    use_attention=???,  # YOUR CODE HERE\n",
    "    n_heads=4\n",
    ")\n",
    "\n",
    "n_params_attn = sum(p.numel() for p in model_attn.parameters())\n",
    "print(f\"MLP variant parameters:       {n_params:,}\")\n",
    "print(f\"Attention variant parameters: {n_params_attn:,}\")\n",
    "print(f\"Difference: {n_params_attn - n_params:,} more parameters with attention\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification\n",
    "assert n_params_attn > n_params, \"‚ùå Attention variant should have more parameters than MLP variant\"\n",
    "print(\"‚úÖ Correct! Attention adds parameters for Q, K, V projections.\")\n",
    "print(f\"   For a 4√ó4 grid (16 tokens), MLP mixing uses a 16√ó16={16*16} param matrix\")\n",
    "print(f\"   Attention uses QKV projections: 3 √ó dim √ó dim = {3 * 32 * 32} params per head group\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Count Effective Depth"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO ============\n",
    "# Calculate the effective depth for different configurations.\n",
    "# Formula: effective_depth = T * (n + 1) * n_layers\n",
    "#\n",
    "# Fill in the values:\n",
    "# ==============================\n",
    "\n",
    "T = 3          # supervision steps\n",
    "n = 6          # recursions per step\n",
    "n_layers = 2   # layers per recursion\n",
    "\n",
    "effective_depth = ???  # YOUR CODE HERE\n",
    "\n",
    "print(f\"Configuration: T={T}, n={n}, n_layers={n_layers}\")\n",
    "print(f\"Effective depth: {effective_depth}\")\n",
    "print(f\"This is equivalent to a {effective_depth}-layer transformer!\")\n",
    "print(f\"But with only {n_layers} layers of unique weights.\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification\n",
    "assert effective_depth == 42, f\"‚ùå Expected 42, got {effective_depth}. Check formula: T √ó (n+1) √ó n_layers\"\n",
    "print(\"‚úÖ Correct! TRM achieves 42-layer depth with just 2 layers of weights.\")"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Final\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_10_final.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_10_final"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us verify the complete model works end-to-end with a proper forward pass."
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count total and per-component parameters.\"\"\"\n",
    "    total = 0\n",
    "    components = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        n = param.numel()\n",
    "        total += n\n",
    "        component = name.split('.')[0]\n",
    "        components[component] = components.get(component, 0) + n\n",
    "    return total, components\n",
    "\n",
    "total, components = count_parameters(model)\n",
    "print(\"Parameter breakdown:\")\n",
    "print(f\"{'Component':<20} {'Parameters':>12} {'Percentage':>10}\")\n",
    "print(\"-\" * 44)\n",
    "for comp, n in sorted(components.items(), key=lambda x: -x[1]):\n",
    "    print(f\"{comp:<20} {n:>12,} {100*n/total:>9.1f}%\")\n",
    "print(\"-\" * 44)\n",
    "print(f\"{'TOTAL':<20} {total:>12,} {'100.0%':>10}\")"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full forward pass with a real-looking puzzle\n",
    "# Create a batch of 4x4 grids with some zeros (empty cells)\n",
    "batch_size = 4\n",
    "grids = torch.zeros(batch_size, 4, 4, dtype=torch.long)\n",
    "for b in range(batch_size):\n",
    "    # Fill with a valid grid pattern then mask some cells\n",
    "    base = torch.tensor([[1,2,3,4],[3,4,1,2],[2,1,4,3],[4,3,2,1]])\n",
    "    mask = torch.rand(4, 4) > 0.4  # Keep ~60% of cells\n",
    "    grids[b] = base * mask.long()\n",
    "\n",
    "print(\"Input grids (0 = empty):\")\n",
    "for b in range(batch_size):\n",
    "    print(f\"\\nGrid {b+1}:\")\n",
    "    print(grids[b].numpy())\n",
    "\n",
    "# Forward pass with 6 recursions\n",
    "output = model(grids, n_recursions=6)\n",
    "predictions = output['logits'].argmax(dim=-1) + 1  # +1 because classes are 1-indexed\n",
    "\n",
    "print(f\"\\nPredictions shape: {predictions.shape}\")\n",
    "print(f\"(These are random since the model is untrained)\")"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Final architecture summary\n",
    "print(\"=\" * 60)\n",
    "print(\"  TINY RECURSIVE MODEL ‚Äî ARCHITECTURE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "  Grid size:        4√ó4 (16 cells)\n",
    "  Classes:          4 (values 1-4)\n",
    "  Hidden dim:       32\n",
    "  Layers:           2 (shared across all recursions)\n",
    "  Recursions:       6 per supervision step\n",
    "  Supervision:      3 steps\n",
    "  Effective depth:  42 layers\n",
    "\n",
    "  Total parameters: {total:,}\n",
    "\n",
    "  The same 2 layers process the input 21 times.\n",
    "  No new weights ‚Äî just new information from each pass.\n",
    "\"\"\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üéØ Final Output ‚Äî Architecture Visualization"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual summary of the TRM architecture\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Draw the recursion loop\n",
    "# Input box\n",
    "rect = mpatches.FancyBboxPatch((0.5, 6), 2.5, 1.5, boxstyle=\"round,pad=0.2\",\n",
    "                                facecolor='#e3f2fd', edgecolor='#1565c0', linewidth=2)\n",
    "ax.add_patch(rect)\n",
    "ax.text(1.75, 6.75, 'Input (x)', ha='center', va='center', fontsize=12, fontweight='bold', color='#1565c0')\n",
    "\n",
    "# y box\n",
    "rect = mpatches.FancyBboxPatch((0.5, 4), 2.5, 1.5, boxstyle=\"round,pad=0.2\",\n",
    "                                facecolor='#c8e6c9', edgecolor='#2e7d32', linewidth=2)\n",
    "ax.add_patch(rect)\n",
    "ax.text(1.75, 4.75, 'Solution (y)', ha='center', va='center', fontsize=12, fontweight='bold', color='#2e7d32')\n",
    "\n",
    "# z box\n",
    "rect = mpatches.FancyBboxPatch((0.5, 2), 2.5, 1.5, boxstyle=\"round,pad=0.2\",\n",
    "                                facecolor='#fff3e0', edgecolor='#e65100', linewidth=2)\n",
    "ax.add_patch(rect)\n",
    "ax.text(1.75, 2.75, 'Reasoning (z)', ha='center', va='center', fontsize=12, fontweight='bold', color='#e65100')\n",
    "\n",
    "# Arrows to network\n",
    "for y_pos in [6.75, 4.75, 2.75]:\n",
    "    ax.annotate('', xy=(4, 4.75), xytext=(3, y_pos),\n",
    "                arrowprops=dict(arrowstyle='->', color='#555', lw=1.5))\n",
    "\n",
    "# Network box\n",
    "rect = mpatches.FancyBboxPatch((4, 2.5), 3.5, 5, boxstyle=\"round,pad=0.3\",\n",
    "                                facecolor='#e8eaf6', edgecolor='#3949ab', linewidth=2.5)\n",
    "ax.add_patch(rect)\n",
    "ax.text(5.75, 7, '2-Layer Network', ha='center', va='center', fontsize=13, fontweight='bold', color='#283593')\n",
    "ax.text(5.75, 6.2, 'RMSNorm', ha='center', va='center', fontsize=10, color='#555')\n",
    "ax.text(5.75, 5.5, 'Token Mixing', ha='center', va='center', fontsize=10, color='#555')\n",
    "ax.text(5.75, 4.8, '(MLP or Attention)', ha='center', va='center', fontsize=9, color='#888')\n",
    "ax.text(5.75, 4, 'RMSNorm', ha='center', va='center', fontsize=10, color='#555')\n",
    "ax.text(5.75, 3.3, 'SwiGLU FFN', ha='center', va='center', fontsize=10, color='#555')\n",
    "\n",
    "# Output arrows\n",
    "ax.annotate('', xy=(9, 5.75), xytext=(7.5, 5.75),\n",
    "            arrowprops=dict(arrowstyle='->', color='#2e7d32', lw=2))\n",
    "ax.annotate('', xy=(9, 3.75), xytext=(7.5, 3.75),\n",
    "            arrowprops=dict(arrowstyle='->', color='#e65100', lw=2))\n",
    "\n",
    "# Updated y\n",
    "rect = mpatches.FancyBboxPatch((9, 5), 3, 1.5, boxstyle=\"round,pad=0.2\",\n",
    "                                facecolor='#c8e6c9', edgecolor='#2e7d32', linewidth=2)\n",
    "ax.add_patch(rect)\n",
    "ax.text(10.5, 5.75, 'Updated y', ha='center', va='center', fontsize=12, fontweight='bold', color='#2e7d32')\n",
    "\n",
    "# Updated z\n",
    "rect = mpatches.FancyBboxPatch((9, 3), 3, 1.5, boxstyle=\"round,pad=0.2\",\n",
    "                                facecolor='#fff3e0', edgecolor='#e65100', linewidth=2)\n",
    "ax.add_patch(rect)\n",
    "ax.text(10.5, 3.75, 'Updated z', ha='center', va='center', fontsize=12, fontweight='bold', color='#e65100')\n",
    "\n",
    "# Recursion arrow (loop back)\n",
    "ax.annotate('', xy=(1.75, 3.5), xytext=(10.5, 2.8),\n",
    "            arrowprops=dict(arrowstyle='->', color='#3949ab', lw=2.5,\n",
    "                           connectionstyle='arc3,rad=0.3'))\n",
    "ax.text(6, 1.5, 'Repeat n times', ha='center', va='center', fontsize=14,\n",
    "        fontweight='bold', color='#3949ab', style='italic')\n",
    "\n",
    "# Title\n",
    "ax.text(7, 9.3, 'Tiny Recursive Model ‚Äî Architecture', ha='center', va='center',\n",
    "        fontsize=16, fontweight='bold')\n",
    "ax.text(7, 8.7, f'{total:,} parameters ‚Ä¢ 2 layers ‚Ä¢ applied {6} times per step',\n",
    "        ha='center', va='center', fontsize=11, color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéâ You have built the complete TRM architecture from scratch!\")\n",
    "print(\"   In the next notebook, we will train it with deep supervision.\")"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_11_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_11_closing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reflection and Next Steps\n",
    "\n",
    "### üí° Key Takeaways\n",
    "\n",
    "1. **RMSNorm** normalizes by root-mean-square (simpler than LayerNorm, equally effective)\n",
    "2. **SwiGLU** is a gated activation ‚Äî learns which features to pass through\n",
    "3. **RoPE** encodes position via rotation ‚Äî nearby positions get similar encodings\n",
    "4. **MLP mixing** is cheaper than attention for fixed-size contexts\n",
    "5. **Weight sharing** across recursions gives 42-layer depth from 2 layers of weights\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "\n",
    "1. Why is the MLP mixer limited to fixed-size inputs, while attention handles variable sizes?\n",
    "2. What role does the **z** (reasoning) feature play that **y** (solution) alone cannot handle? (Hint: think about information that is useful for reasoning but should not appear in the final answer.)\n",
    "3. If we increased the hidden dimension from 32 to 256, how would the parameter count change? Would this help or hurt on a small dataset?\n",
    "\n",
    "### üèÜ Optional Challenges\n",
    "\n",
    "1. **Add skip connections** between recursion steps (connect y from step $i$ to step $i+2$)\n",
    "2. **Implement a multi-head version** of the MLP mixer where different heads attend to different spatial neighborhoods\n",
    "3. **Profile memory usage** ‚Äî how does memory scale with the number of recursions?\n",
    "\n",
    "### What's Next\n",
    "\n",
    "In Notebook 3, we will implement the **deep supervision training loop** ‚Äî the secret sauce that makes TRM work. We will train our model on 4√ó4 Sudoku and watch the recursion steps progressively solve puzzles."
   ],
   "id": "cell_37"
  }
 ]
}