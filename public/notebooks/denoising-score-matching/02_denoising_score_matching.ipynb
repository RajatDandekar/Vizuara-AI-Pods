{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Denoising Score Matching -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Score Matching -- Vizuara\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebook, we learned how to train a score function using the tractable score matching loss. It worked, but it required computing the Jacobian trace -- an operation that scales as $O(D^2)$, making it infeasible for images or any high-dimensional data.\n",
    "\n",
    "**Denoising Score Matching (DSM)** solves this problem with a remarkably simple idea: instead of matching the score of the clean data distribution, we match the score of a noisy version. Since we control the noise, we know the target score in closed form -- no Jacobian needed.\n",
    "\n",
    "This single insight underpins ALL modern diffusion models (DDPM, Stable Diffusion, DALL-E, Sora).\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand the DSM objective and derive it step by step\n",
    "- Implement DSM training from scratch\n",
    "- Train a score network on 2D data and visualize the learned score field\n",
    "- See the direct equivalence between DSM and DDPM's noise prediction\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "### The Magnet Analogy\n",
    "\n",
    "Imagine a tabletop with invisible magnets hidden at specific spots. You want to map the magnetic field (which direction each point is pulled toward), but you cannot see or measure the magnets directly.\n",
    "\n",
    "Here is the trick:\n",
    "\n",
    "1. **Place a metal ball on a magnet.** This is your clean data point $x$.\n",
    "2. **Flick the ball in a random direction.** It lands at a new spot $\\tilde{x}$. This is your noisy data.\n",
    "3. **Ask a student (neural network):** \"Which direction would pull this ball back to its starting position?\"\n",
    "\n",
    "The student does not know where the ball came from. But YOU do -- because you placed it. So you can give the student feedback: \"You guessed this direction, but the true direction is that way.\"\n",
    "\n",
    "After enough practice with many balls on many magnets, the student learns the entire magnetic field -- the score function.\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "The key insight: **we do not need to know the score of the clean distribution.** We only need the score of the transition from clean to noisy, which is a simple Gaussian. The direction from noisy back to clean is:\n",
    "\n",
    "$$\\text{target} = \\frac{x - \\tilde{x}}{\\sigma^2}$$\n",
    "\n",
    "This is just \"the direction from the noisy point back to the clean point, divided by the noise variance.\" No Jacobians, no intractable integrals.\n",
    "\n",
    "## 3. The Mathematics\n",
    "\n",
    "### Step 1: Add Noise\n",
    "\n",
    "We corrupt each clean data point $x$ with Gaussian noise:\n",
    "\n",
    "$$\\tilde{x} = x + \\sigma \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\n",
    "\n",
    "The conditional distribution is:\n",
    "\n",
    "$$q_\\sigma(\\tilde{x}|x) = \\mathcal{N}(\\tilde{x}; x, \\sigma^2 I)$$\n",
    "\n",
    "**Computationally:** sample a noise vector $\\epsilon$ from a standard normal, scale it by $\\sigma$, and add it to $x$.\n",
    "\n",
    "### Step 2: Compute the Target Score\n",
    "\n",
    "Since the conditional is Gaussian, we can compute its score in closed form:\n",
    "\n",
    "$$\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}|x) = \\frac{x - \\tilde{x}}{\\sigma^2} = -\\frac{\\epsilon}{\\sigma}$$\n",
    "\n",
    "**Computationally:** this is just the negative noise divided by $\\sigma$, or equivalently, the vector pointing from the noisy point back to the clean point, scaled by $1/\\sigma^2$.\n",
    "\n",
    "### Step 3: The DSM Loss\n",
    "\n",
    "Train the score network to predict this target:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{DSM}} = \\mathbb{E}_{x \\sim p(x)} \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)} \\left[ \\left\\| s_\\theta(x + \\sigma\\epsilon) - \\frac{-\\epsilon}{\\sigma} \\right\\|^2 \\right]$$\n",
    "\n",
    "Or equivalently, written in terms of $\\tilde{x}$:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{DSM}} = \\mathbb{E} \\left[ \\left\\| s_\\theta(\\tilde{x}) - \\frac{x - \\tilde{x}}{\\sigma^2} \\right\\|^2 \\right]$$\n",
    "\n",
    "**Computationally:** for each training step, sample a batch of clean data, add noise, compute the target direction, predict with the network, and minimize the MSE.\n",
    "\n",
    "### Numerical Walkthrough\n",
    "\n",
    "Let us trace through one training sample.\n",
    "\n",
    "Clean data point: $x = [3.0, 4.0]$, noise level: $\\sigma = 0.5$\n",
    "\n",
    "We sample noise: $\\epsilon = [0.4, -0.6]$\n",
    "\n",
    "Noisy point: $\\tilde{x} = [3.0 + 0.5 \\times 0.4, \\; 4.0 + 0.5 \\times (-0.6)] = [3.2, 3.7]$\n",
    "\n",
    "Target score: $\\frac{x - \\tilde{x}}{\\sigma^2} = \\frac{[3.0, 4.0] - [3.2, 3.7]}{0.25} = \\frac{[-0.2, 0.3]}{0.25} = [-0.8, 1.2]$\n",
    "\n",
    "Alternative (using noise): $-\\frac{\\epsilon}{\\sigma} = -\\frac{[0.4, -0.6]}{0.5} = [-0.8, 1.2]$\n",
    "\n",
    "Both give the same answer. This is exactly what we want.\n",
    "\n",
    "If the network predicts $s_\\theta(\\tilde{x}) = [-0.7, 1.0]$, the loss is:\n",
    "\n",
    "$$\\|[-0.7, 1.0] - [-0.8, 1.2]\\|^2 = \\|[0.1, -0.2]\\|^2 = 0.01 + 0.04 = 0.05$$\n",
    "\n",
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 Data Generation"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate 2D data from a mixture of Gaussians\n",
    "def generate_data(n_samples=2000):\n",
    "    \"\"\"Create a bimodal 2D dataset with two clusters.\"\"\"\n",
    "    cluster1 = torch.randn(n_samples // 2, 2) * 0.5 + torch.tensor([2.0, 2.0])\n",
    "    cluster2 = torch.randn(n_samples // 2, 2) * 0.5 + torch.tensor([-2.0, -2.0])\n",
    "    data = torch.cat([cluster1, cluster2], dim=0)\n",
    "    return data\n",
    "\n",
    "data = generate_data(2000)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(data[:, 0].numpy(), data[:, 1].numpy(), alpha=0.3, s=5)\n",
    "plt.title('Training Data: Mixture of 2 Gaussians')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Cluster 1 center: ~(2, 2), Cluster 2 center: ~(-2, -2)\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Noise Corruption"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the noise corruption process\n",
    "sigma = 0.5  # Noise level\n",
    "\n",
    "# Take 5 samples to show the process\n",
    "n_demo = 5\n",
    "clean_samples = data[:n_demo]\n",
    "noise = torch.randn_like(clean_samples) * sigma\n",
    "noisy_samples = clean_samples + noise\n",
    "target_scores = (clean_samples - noisy_samples) / (sigma ** 2)\n",
    "\n",
    "print(\"Clean -> Noisy -> Target Score\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(n_demo):\n",
    "    x = clean_samples[i].numpy()\n",
    "    x_tilde = noisy_samples[i].numpy()\n",
    "    target = target_scores[i].numpy()\n",
    "    print(f\"x={x} -> x_tilde={x_tilde} -> target={target}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(data[:, 0], data[:, 1], alpha=0.1, s=3, c='blue', label='Clean data')\n",
    "noisy_all = data + torch.randn_like(data) * sigma\n",
    "plt.scatter(noisy_all[:, 0], noisy_all[:, 1], alpha=0.1, s=3, c='red', label='Noisy data')\n",
    "plt.legend()\n",
    "plt.title(f'Clean vs Noisy Data (sigma={sigma})')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nThe noisy data is a slightly blurred version of the clean data.\")\n",
    "print(f\"Each noisy point was displaced by noise with std={sigma}\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Score Network Architecture"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreNetwork(nn.Module):\n",
    "    \"\"\"Simple MLP that predicts the score (gradient of log density).\"\"\"\n",
    "    def __init__(self, input_dim=2, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.SiLU(),  # Smooth activation\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)  # Same dimension as input\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, input_dim) -- noisy data points\n",
    "        Returns:\n",
    "            scores: (batch_size, input_dim) -- predicted score vectors\n",
    "        \"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "model = ScoreNetwork()\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(4, 2)\n",
    "test_output = model(test_input)\n",
    "print(f\"Input shape:  {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "print(\"Output is the same dimension as input -- each point gets a score vector.\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The DSM Training Loop"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsm_loss(model, clean_data, sigma):\n",
    "    \"\"\"\n",
    "    Compute the Denoising Score Matching loss.\n",
    "\n",
    "    Steps:\n",
    "    1. Add Gaussian noise to create noisy data\n",
    "    2. Compute target score: (clean - noisy) / sigma^2\n",
    "    3. Predict score with the model\n",
    "    4. Return MSE between prediction and target\n",
    "    \"\"\"\n",
    "    # Step 1: Add noise\n",
    "    noise = torch.randn_like(clean_data) * sigma\n",
    "    noisy_data = clean_data + noise\n",
    "\n",
    "    # Step 2: Target score\n",
    "    target_score = (clean_data - noisy_data) / (sigma ** 2)\n",
    "\n",
    "    # Step 3: Predict\n",
    "    predicted_score = model(noisy_data)\n",
    "\n",
    "    # Step 4: MSE loss\n",
    "    loss = ((predicted_score - target_score) ** 2).sum(dim=-1).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Verify loss computation\n",
    "test_loss = dsm_loss(model, data[:32], sigma=0.5)\n",
    "print(f\"Initial DSM loss: {test_loss.item():.4f}\")\n",
    "print(\"(This should decrease during training)\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn -- TODO Exercises\n",
    "\n",
    "### TODO 1: Implement DSM Loss from Noise Perspective"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsm_loss_noise_form(model, clean_data, sigma):\n",
    "    \"\"\"\n",
    "    Compute DSM loss using the noise prediction form.\n",
    "\n",
    "    Instead of target = (x - x_tilde) / sigma^2,\n",
    "    use target = -epsilon / sigma\n",
    "\n",
    "    These are mathematically equivalent!\n",
    "\n",
    "    Args:\n",
    "        model: ScoreNetwork\n",
    "        clean_data: (N, D) clean data points\n",
    "        sigma: noise standard deviation\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Sample epsilon ~ N(0, I) with same shape as clean_data\n",
    "    # Step 2: Compute noisy_data = clean_data + sigma * epsilon\n",
    "    # Step 3: Compute target_score = -epsilon / sigma\n",
    "    # Step 4: Get predicted_score from model(noisy_data)\n",
    "    # Step 5: Return MSE between predicted and target\n",
    "    # ==============================\n",
    "\n",
    "    loss = None  # YOUR CODE HERE\n",
    "\n",
    "    return loss"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: both forms should give similar loss values\n",
    "torch.manual_seed(42)\n",
    "loss_original = dsm_loss(model, data[:256], sigma=0.5)\n",
    "torch.manual_seed(42)\n",
    "loss_noise = dsm_loss_noise_form(model, data[:256], sigma=0.5)\n",
    "\n",
    "if loss_noise is not None:\n",
    "    print(f\"Original DSM loss: {loss_original.item():.4f}\")\n",
    "    print(f\"Noise-form DSM loss: {loss_noise.item():.4f}\")\n",
    "    # They use different random noise samples so won't be identical,\n",
    "    # but should be in the same ballpark\n",
    "    print(\"Both forms should give similar magnitudes (same order)\")\n",
    "else:\n",
    "    print(\"Implement the function first!\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Effect of Noise Level"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_sigma(sigma, n_epochs=1000):\n",
    "    \"\"\"\n",
    "    Train a score model with a specific noise level and return\n",
    "    the final loss and the trained model.\n",
    "\n",
    "    Args:\n",
    "        sigma: noise standard deviation\n",
    "        n_epochs: number of training epochs\n",
    "\n",
    "    Returns:\n",
    "        model: trained ScoreNetwork\n",
    "        losses: list of loss values\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Create a fresh ScoreNetwork and Adam optimizer (lr=1e-3)\n",
    "    # Step 2: Train for n_epochs using dsm_loss()\n",
    "    # Step 3: Record losses\n",
    "    # ==============================\n",
    "\n",
    "    model = None  # YOUR CODE HERE\n",
    "    losses = []   # YOUR CODE HERE\n",
    "\n",
    "    return model, losses\n",
    "\n",
    "# Train with different noise levels\n",
    "# sigmas = [0.1, 0.5, 1.0, 2.0]\n",
    "# models = {}\n",
    "# all_losses = {}\n",
    "# for s in sigmas:\n",
    "#     print(f\"\\nTraining with sigma={s}...\")\n",
    "#     m, l = train_with_sigma(s)\n",
    "#     models[s] = m\n",
    "#     all_losses[s] = l"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: compare score fields for different noise levels\n",
    "# (Uncomment after implementing train_with_sigma)\n",
    "\n",
    "# fig, axes = plt.subplots(1, 4, figsize=(24, 6))\n",
    "# n_grid = 20\n",
    "# x_range = torch.linspace(-5, 5, n_grid)\n",
    "# y_range = torch.linspace(-5, 5, n_grid)\n",
    "# xx, yy = torch.meshgrid(x_range, y_range, indexing='ij')\n",
    "# grid = torch.stack([xx.flatten(), yy.flatten()], dim=1)\n",
    "#\n",
    "# for ax, s in zip(axes, sigmas):\n",
    "#     with torch.no_grad():\n",
    "#         scores = models[s](grid)\n",
    "#     ax.scatter(data[:, 0], data[:, 1], alpha=0.1, s=3, c='blue')\n",
    "#     ax.quiver(grid[:, 0], grid[:, 1], scores[:, 0], scores[:, 1],\n",
    "#               color='red', alpha=0.7, scale=50)\n",
    "#     ax.set_title(f'sigma = {s}')\n",
    "#     ax.set_aspect('equal')\n",
    "#     ax.grid(True, alpha=0.3)\n",
    "# plt.suptitle('Effect of Noise Level on Learned Score Field')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full DSM training pipeline\n",
    "model = ScoreNetwork()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "sigma = 0.5\n",
    "n_epochs = 2000\n",
    "\n",
    "losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    # Shuffle data\n",
    "    idx = torch.randperm(len(data))\n",
    "    batch = data[idx[:512]]\n",
    "\n",
    "    loss = dsm_loss(model, batch, sigma)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, alpha=0.7)\n",
    "plt.title('DSM Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned score field\n",
    "n_grid = 25\n",
    "x_range = torch.linspace(-5, 5, n_grid)\n",
    "y_range = torch.linspace(-5, 5, n_grid)\n",
    "xx, yy = torch.meshgrid(x_range, y_range, indexing='ij')\n",
    "grid = torch.stack([xx.flatten(), yy.flatten()], dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    scores = model(grid)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(data[:, 0], data[:, 1], alpha=0.15, s=5, c='blue')\n",
    "plt.quiver(grid[:, 0].numpy(), grid[:, 1].numpy(),\n",
    "           scores[:, 0].numpy(), scores[:, 1].numpy(),\n",
    "           color='red', alpha=0.7, scale=50)\n",
    "plt.title('Learned Score Field via Denoising Score Matching', fontsize=14)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"All arrows point toward the data clusters!\")\n",
    "print(\"The score field acts as a compass guiding us toward high-density regions.\")\n",
    "print(\"And we achieved this WITHOUT computing any Jacobian traces!\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to DDPM: show that score = -noise/sigma\n",
    "print(\"=\" * 60)\n",
    "print(\"DSM <-> DDPM Equivalence\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Take a sample\n",
    "x_clean = data[0:1]\n",
    "eps = torch.randn_like(x_clean)\n",
    "x_noisy = x_clean + sigma * eps\n",
    "\n",
    "# DSM target\n",
    "dsm_target = (x_clean - x_noisy) / (sigma ** 2)\n",
    "\n",
    "# DDPM-style target (scaled)\n",
    "ddpm_target = -eps / sigma\n",
    "\n",
    "print(f\"\\nClean point:    {x_clean[0].numpy()}\")\n",
    "print(f\"Noise (eps):    {eps[0].numpy()}\")\n",
    "print(f\"Noisy point:    {x_noisy[0].numpy()}\")\n",
    "print(f\"\\nDSM target:     {dsm_target[0].numpy()}\")\n",
    "print(f\"DDPM-style:     {ddpm_target[0].numpy()}\")\n",
    "print(f\"\\nThey are identical! Score prediction = Noise prediction (up to scaling)\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization: DSM score field with score magnitude heatmap\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Left: Score field\n",
    "with torch.no_grad():\n",
    "    scores = model(grid)\n",
    "    score_magnitudes = torch.norm(scores, dim=-1).reshape(n_grid, n_grid)\n",
    "\n",
    "axes[0].scatter(data[:, 0], data[:, 1], alpha=0.1, s=3, c='blue')\n",
    "axes[0].quiver(grid[:, 0].numpy(), grid[:, 1].numpy(),\n",
    "               scores[:, 0].numpy(), scores[:, 1].numpy(),\n",
    "               color='red', alpha=0.7, scale=50)\n",
    "axes[0].set_title('Learned Score Field (DSM)', fontsize=14)\n",
    "axes[0].set_xlabel('x1')\n",
    "axes[0].set_ylabel('x2')\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Score magnitude heatmap\n",
    "im = axes[1].imshow(score_magnitudes.numpy().T,\n",
    "                     extent=[-5, 5, -5, 5], origin='lower',\n",
    "                     cmap='viridis', aspect='equal')\n",
    "axes[1].scatter(data[:, 0], data[:, 1], alpha=0.15, s=3, c='white')\n",
    "axes[1].set_title('Score Magnitude ||s(x)||', fontsize=14)\n",
    "axes[1].set_xlabel('x1')\n",
    "axes[1].set_ylabel('x2')\n",
    "plt.colorbar(im, ax=axes[1])\n",
    "\n",
    "plt.suptitle('Denoising Score Matching: Learned Score Function', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey takeaway: DSM replaces the intractable Jacobian trace with a\")\n",
    "print(\"simple noise-direction target. This is why diffusion models work!\")\n",
    "print(f\"\\nCompare: Tractable SM for a 784-dim image needs a 784x784 Jacobian.\")\n",
    "print(f\"DSM needs only a forward pass through the network. Night and day.\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Think About This\n",
    "\n",
    "1. **What happens if sigma is too small?** The noisy distribution is nearly identical to the clean distribution. Does this help or hurt training?\n",
    "\n",
    "2. **What happens if sigma is too large?** The noisy points are far from the clean data. Does the score function still capture the data structure?\n",
    "\n",
    "3. **Why does DSM work even though we match the score of the NOISY distribution, not the clean one?** (Hint: Vincent proved that as sigma approaches 0, the DSM objective converges to the true score matching objective.)\n",
    "\n",
    "4. **How does multi-scale noise (many sigmas at once) solve the problems from questions 1 and 2?** This is what NCSN (Noise Conditional Score Networks) and DDPM do!\n",
    "\n",
    "### Extension Challenge\n",
    "\n",
    "Modify the code to train with MULTIPLE noise levels simultaneously. For each training step, randomly sample a sigma from a set of values (e.g., [0.1, 0.3, 0.5, 1.0, 2.0]) and condition the network on sigma by concatenating it to the input. This is the bridge to full diffusion models.\n",
    "\n",
    "### What's Next\n",
    "\n",
    "In the next notebook, we will use our trained score function to actually **generate new data** using Langevin Dynamics -- turning our score compass into a generative model."
   ],
   "id": "cell_22"
  }
 ]
}