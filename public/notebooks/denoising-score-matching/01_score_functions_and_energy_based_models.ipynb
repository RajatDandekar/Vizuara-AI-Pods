{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Score Functions and Energy-Based Models -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Functions and Energy-Based Models -- Vizuara\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "In generative modeling, we want to learn the probability distribution of our data so that we can generate new, realistic samples. But there is a fundamental problem: for complex, high-dimensional data, computing the probability density directly is **intractable** because of the dreaded partition function.\n",
    "\n",
    "The **score function** offers an elegant escape. Instead of modeling the density itself, we model its gradient -- a vector field that points toward regions of high probability. This simple shift unlocks an entire family of powerful generative models, including the diffusion models behind DALL-E, Stable Diffusion, and Sora.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand what the score function is and why it bypasses the partition function\n",
    "- Implement and visualize score functions for simple distributions\n",
    "- Build a neural network that learns the score function using the **tractable score matching** objective\n",
    "- See why this objective is computationally expensive and why we need something better\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "### The Compass Analogy\n",
    "\n",
    "Imagine you are standing in a vast, foggy landscape. You know there are treasure chests hidden somewhere, but you cannot see them. All you have is a magical compass that, at any point, tells you the direction where the treasure density is highest.\n",
    "\n",
    "That compass is the **score function**. It does not tell you the exact probability of finding treasure at your location. Instead, it tells you which direction to walk to increase your chances the most.\n",
    "\n",
    "### Why Not Just Model the Probability?\n",
    "\n",
    "If we use an energy function $E_\\theta(x)$ to model how \"unlikely\" a data point is, the probability becomes:\n",
    "\n",
    "$$p_\\theta(x) = \\frac{1}{Z(\\theta)} \\exp(-E_\\theta(x))$$\n",
    "\n",
    "The normalization constant $Z(\\theta) = \\int \\exp(-E_\\theta(x)) dx$ requires integrating over ALL possible data points. For a 28x28 image, that is an integral over a 784-dimensional space. Completely intractable.\n",
    "\n",
    "But the score function -- the gradient of the log density -- eliminates $Z$ entirely!\n",
    "\n",
    "## 3. The Mathematics\n",
    "\n",
    "### Definition of the Score Function\n",
    "\n",
    "The score function is defined as:\n",
    "\n",
    "$$s(x) = \\nabla_x \\log p(x)$$\n",
    "\n",
    "This gives us a vector at every point in space, pointing in the direction of steepest increase in log-probability.\n",
    "\n",
    "**Computationally, this means:** for each dimension of x, compute the partial derivative of the log-density. The result is a vector of the same dimension as x.\n",
    "\n",
    "### Why the Partition Function Vanishes\n",
    "\n",
    "Starting from the energy-based density:\n",
    "\n",
    "$$\\log p_\\theta(x) = -E_\\theta(x) - \\log Z(\\theta)$$\n",
    "\n",
    "Taking the gradient with respect to x:\n",
    "\n",
    "$$\\nabla_x \\log p_\\theta(x) = -\\nabla_x E_\\theta(x) - \\nabla_x \\log Z(\\theta)$$\n",
    "\n",
    "Since $Z(\\theta)$ does not depend on x, $\\nabla_x \\log Z(\\theta) = 0$, so:\n",
    "\n",
    "$$s_\\theta(x) = -\\nabla_x E_\\theta(x)$$\n",
    "\n",
    "The partition function has vanished completely.\n",
    "\n",
    "### Numerical Example\n",
    "\n",
    "Let us verify this with concrete numbers. Suppose $E_\\theta(x) = (x - 2)^2$ and $Z = 10$.\n",
    "\n",
    "$$\\log p_\\theta(x) = -(x-2)^2 - \\log(10)$$\n",
    "\n",
    "$$s_\\theta(x) = \\nabla_x \\log p_\\theta(x) = -2(x-2)$$\n",
    "\n",
    "At $x = 5$: $s_\\theta(5) = -2(5-2) = -6$ (points toward the minimum at $x=2$).\n",
    "\n",
    "At $x = 0$: $s_\\theta(0) = -2(0-2) = 4$ (points toward $x=2$ from the left).\n",
    "\n",
    "The $\\log(10)$ from the partition function disappeared. This is exactly what we want.\n",
    "\n",
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 Score Function for a 1D Gaussian\n",
    "\n",
    "Let us start by computing and visualizing the score function for a simple Gaussian distribution."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a 1D Gaussian distribution\n",
    "mu, sigma = 0.0, 1.0\n",
    "\n",
    "# Create a grid of x values\n",
    "x = torch.linspace(-4, 4, 200)\n",
    "\n",
    "# Compute the probability density\n",
    "p_x = (1 / (sigma * np.sqrt(2 * np.pi))) * torch.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "# Compute the score function analytically: s(x) = -(x - mu) / sigma^2\n",
    "score = -(x - mu) / (sigma ** 2)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Probability density\n",
    "axes[0].plot(x.numpy(), p_x.numpy(), 'b-', linewidth=2)\n",
    "axes[0].set_title('Gaussian Probability Density p(x)')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('p(x)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Score function\n",
    "axes[1].plot(x.numpy(), score.numpy(), 'r-', linewidth=2)\n",
    "axes[1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1].set_title('Score Function s(x) = -x')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('s(x)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add arrows showing score direction at key points\n",
    "for xi in [-3, -2, -1, 1, 2, 3]:\n",
    "    si = -xi\n",
    "    axes[1].annotate('', xy=(xi + 0.3 * np.sign(si), si),\n",
    "                     xytext=(xi, si),\n",
    "                     arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Score at x=-3: {-(-3):.1f} (points right, toward center)\")\n",
    "print(f\"Score at x= 0: {-(0):.1f} (zero, already at peak)\")\n",
    "print(f\"Score at x= 3: {-(3):.1f} (points left, toward center)\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Score Field for a 2D Mixture of Gaussians\n",
    "\n",
    "Now let us compute and visualize the score field for a more interesting distribution -- a mixture of two Gaussians."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Mixture of Gaussians\n",
    "def mixture_log_prob(x, means, covs, weights):\n",
    "    \"\"\"Compute log probability of a mixture of Gaussians.\"\"\"\n",
    "    log_probs = []\n",
    "    for mean, cov, w in zip(means, covs, weights):\n",
    "        diff = x - mean\n",
    "        # For diagonal covariance\n",
    "        log_p = -0.5 * (diff ** 2 / cov).sum(dim=-1)\n",
    "        log_p += np.log(w) - 0.5 * torch.log(cov).sum() - np.log(2 * np.pi)\n",
    "        log_probs.append(log_p)\n",
    "    log_probs = torch.stack(log_probs, dim=-1)\n",
    "    return torch.logsumexp(log_probs, dim=-1)\n",
    "\n",
    "def compute_score_field(x, means, covs, weights):\n",
    "    \"\"\"Compute the score using autograd.\"\"\"\n",
    "    x_grad = x.clone().requires_grad_(True)\n",
    "    log_p = mixture_log_prob(x_grad, means, covs, weights)\n",
    "    log_p_sum = log_p.sum()\n",
    "    log_p_sum.backward()\n",
    "    return x_grad.grad.clone()\n",
    "\n",
    "# Define mixture parameters\n",
    "means = [torch.tensor([2.0, 2.0]), torch.tensor([-2.0, -2.0])]\n",
    "covs = [torch.tensor([0.25, 0.25]), torch.tensor([0.25, 0.25])]\n",
    "weights = [0.5, 0.5]\n",
    "\n",
    "# Create grid\n",
    "n_grid = 20\n",
    "x_range = torch.linspace(-5, 5, n_grid)\n",
    "y_range = torch.linspace(-5, 5, n_grid)\n",
    "xx, yy = torch.meshgrid(x_range, y_range, indexing='ij')\n",
    "grid = torch.stack([xx.flatten(), yy.flatten()], dim=1)\n",
    "\n",
    "# Compute score field\n",
    "scores = compute_score_field(grid, means, covs, weights)\n",
    "\n",
    "# Also compute density for contour plot\n",
    "with torch.no_grad():\n",
    "    n_dense = 100\n",
    "    x_dense = torch.linspace(-5, 5, n_dense)\n",
    "    y_dense = torch.linspace(-5, 5, n_dense)\n",
    "    xx_d, yy_d = torch.meshgrid(x_dense, y_dense, indexing='ij')\n",
    "    grid_dense = torch.stack([xx_d.flatten(), yy_d.flatten()], dim=1)\n",
    "    log_p = mixture_log_prob(grid_dense, means, covs, weights)\n",
    "    density = torch.exp(log_p).reshape(n_dense, n_dense)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.contourf(xx_d.numpy(), yy_d.numpy(), density.numpy(), levels=20, cmap='Blues', alpha=0.5)\n",
    "plt.colorbar(label='p(x)')\n",
    "plt.quiver(grid[:, 0].numpy(), grid[:, 1].numpy(),\n",
    "           scores[:, 0].numpy(), scores[:, 1].numpy(),\n",
    "           color='red', alpha=0.8, scale=40)\n",
    "plt.title('Score Field of a 2D Mixture of Gaussians')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.2)\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: All arrows point TOWARD the high-density regions (blue peaks)\")\n",
    "print(\"This is the score function acting as a compass!\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn -- TODO Exercises\n",
    "\n",
    "### TODO 1: Score Function for a Different Distribution"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score_uniform_mixture(x, centers, radius=1.0):\n",
    "    \"\"\"\n",
    "    Compute the score function for a mixture of 3 Gaussians\n",
    "    centered at the given positions.\n",
    "\n",
    "    Args:\n",
    "        x: tensor of shape (N, 2) -- query points\n",
    "        centers: list of 3 tensors, each shape (2,)\n",
    "        radius: standard deviation of each Gaussian\n",
    "\n",
    "    Returns:\n",
    "        scores: tensor of shape (N, 2)\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Define covariances (use radius^2 for diagonal)\n",
    "    # Step 2: Define equal weights (1/3 each)\n",
    "    # Step 3: Call compute_score_field() to get scores\n",
    "    # ==============================\n",
    "\n",
    "    scores = None  # YOUR CODE HERE\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Test with 3 clusters at triangle vertices\n",
    "centers = [\n",
    "    torch.tensor([0.0, 2.0]),\n",
    "    torch.tensor([-1.73, -1.0]),\n",
    "    torch.tensor([1.73, -1.0])\n",
    "]"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification cell\n",
    "test_point = torch.tensor([[0.0, 0.0]])\n",
    "test_score = compute_score_uniform_mixture(test_point, centers, radius=1.0)\n",
    "assert test_score is not None, \"You need to implement the function!\"\n",
    "assert test_score.shape == (1, 2), f\"Expected shape (1, 2), got {test_score.shape}\"\n",
    "# The score at the origin should be small since it is equidistant from all 3 centers\n",
    "assert torch.norm(test_score) < 2.0, \"Score at center should be small\"\n",
    "print(\"Your implementation works! Now visualize it below.\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize your result\n",
    "n_grid = 20\n",
    "x_range = torch.linspace(-4, 4, n_grid)\n",
    "y_range = torch.linspace(-4, 4, n_grid)\n",
    "xx, yy = torch.meshgrid(x_range, y_range, indexing='ij')\n",
    "grid = torch.stack([xx.flatten(), yy.flatten()], dim=1)\n",
    "\n",
    "your_scores = compute_score_uniform_mixture(grid, centers)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.quiver(grid[:, 0].numpy(), grid[:, 1].numpy(),\n",
    "           your_scores[:, 0].numpy(), your_scores[:, 1].numpy(),\n",
    "           color='red', alpha=0.8, scale=40)\n",
    "for c in centers:\n",
    "    plt.plot(c[0], c[1], 'b*', markersize=15)\n",
    "plt.title('Your Score Field for 3-Cluster Mixture')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Tractable Score Matching Loss"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tractable_score_matching_loss(model, data):\n",
    "    \"\"\"\n",
    "    Compute the tractable score matching loss (Hyvarinen 2005).\n",
    "\n",
    "    L = E[ tr(J_s(x)) + 0.5 * ||s(x)||^2 ]\n",
    "\n",
    "    Args:\n",
    "        model: nn.Module that takes (N, D) and returns (N, D) scores\n",
    "        data: tensor of shape (N, D)\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar tensor\n",
    "    \"\"\"\n",
    "    data = data.clone().requires_grad_(True)\n",
    "    scores = model(data)\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Compute ||s(x)||^2 for each sample\n",
    "    #         Hint: (scores ** 2).sum(dim=-1)\n",
    "    #\n",
    "    # Step 2: Compute the trace of the Jacobian\n",
    "    #         Hint: For each output dimension d, compute\n",
    "    #         d(s_d) / d(x_d) using torch.autograd.grad\n",
    "    #         Sum these diagonal elements to get the trace\n",
    "    #\n",
    "    # Step 3: Combine: loss = mean(trace + 0.5 * norm_sq)\n",
    "    # ==============================\n",
    "\n",
    "    loss = None  # YOUR CODE HERE\n",
    "\n",
    "    return loss"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "class SimpleLinearScore(torch.nn.Module):\n",
    "    \"\"\"Score model that outputs s(x) = -x (perfect for standard normal).\"\"\"\n",
    "    def forward(self, x):\n",
    "        return -x\n",
    "\n",
    "simple_model = SimpleLinearScore()\n",
    "test_data = torch.randn(100, 2)\n",
    "test_loss = tractable_score_matching_loss(simple_model, test_data)\n",
    "assert test_loss is not None, \"Implement the loss function first!\"\n",
    "print(f\"Loss for perfect score model: {test_loss.item():.4f}\")\n",
    "print(\"(Should be close to -1.0 for 2D standard normal)\")\n",
    "# For s(x)=-x on N(0,I) in 2D: trace = -2, norm_sq = ||x||^2 ~ 2\n",
    "# Loss = -2 + 0.5*2 = -1.0"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us train a neural network score model using the tractable score matching loss on our 2D mixture data."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Generate training data\n",
    "def generate_mixture_data(n_samples=2000):\n",
    "    cluster1 = torch.randn(n_samples // 2, 2) * 0.5 + torch.tensor([2.0, 2.0])\n",
    "    cluster2 = torch.randn(n_samples // 2, 2) * 0.5 + torch.tensor([-2.0, -2.0])\n",
    "    return torch.cat([cluster1, cluster2], dim=0)\n",
    "\n",
    "# Score network\n",
    "class ScoreNet(nn.Module):\n",
    "    def __init__(self, dim=2, hidden=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden), nn.SiLU(),\n",
    "            nn.Linear(hidden, hidden), nn.SiLU(),\n",
    "            nn.Linear(hidden, dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Training with tractable score matching\n",
    "data = generate_mixture_data(2000)\n",
    "model = ScoreNet()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(500):\n",
    "    # Shuffle data\n",
    "    idx = torch.randperm(len(data))\n",
    "    batch = data[idx[:256]]\n",
    "    batch.requires_grad_(True)\n",
    "\n",
    "    scores = model(batch)\n",
    "\n",
    "    # Compute ||s||^2\n",
    "    score_norm = (scores ** 2).sum(dim=-1)\n",
    "\n",
    "    # Compute trace of Jacobian (diagonal elements only)\n",
    "    trace = torch.zeros(len(batch))\n",
    "    for d in range(2):  # For each dimension\n",
    "        grad_d = torch.autograd.grad(\n",
    "            scores[:, d].sum(), batch, create_graph=True\n",
    "        )[0][:, d]\n",
    "        trace += grad_d\n",
    "\n",
    "    loss = (trace + 0.5 * score_norm).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, alpha=0.7)\n",
    "plt.title('Tractable Score Matching Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned score field\n",
    "n_grid = 20\n",
    "x_range = torch.linspace(-5, 5, n_grid)\n",
    "y_range = torch.linspace(-5, 5, n_grid)\n",
    "xx, yy = torch.meshgrid(x_range, y_range, indexing='ij')\n",
    "grid = torch.stack([xx.flatten(), yy.flatten()], dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    learned_scores = model(grid)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(data[:, 0], data[:, 1], alpha=0.1, s=3, c='blue')\n",
    "plt.quiver(grid[:, 0].numpy(), grid[:, 1].numpy(),\n",
    "           learned_scores[:, 0].numpy(), learned_scores[:, 1].numpy(),\n",
    "           color='red', alpha=0.7, scale=50)\n",
    "plt.title('Learned Score Field (Tractable Score Matching)')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"The arrows should point toward the two data clusters.\")\n",
    "print(\"But notice: this required computing the Jacobian trace,\")\n",
    "print(\"which is O(D^2) per sample. For images, this is too expensive!\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison: true vs learned score fields\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# True score field\n",
    "true_scores = compute_score_field(grid.clone(), means, covs, weights)\n",
    "axes[0].scatter(data[:, 0], data[:, 1], alpha=0.1, s=3, c='blue')\n",
    "axes[0].quiver(grid[:, 0].numpy(), grid[:, 1].numpy(),\n",
    "               true_scores[:, 0].numpy(), true_scores[:, 1].numpy(),\n",
    "               color='green', alpha=0.7, scale=50)\n",
    "axes[0].set_title('True Score Field', fontsize=14)\n",
    "axes[0].set_xlabel('x1')\n",
    "axes[0].set_ylabel('x2')\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learned score field\n",
    "axes[1].scatter(data[:, 0], data[:, 1], alpha=0.1, s=3, c='blue')\n",
    "axes[1].quiver(grid[:, 0].numpy(), grid[:, 1].numpy(),\n",
    "               learned_scores[:, 0].numpy(), learned_scores[:, 1].numpy(),\n",
    "               color='red', alpha=0.7, scale=50)\n",
    "axes[1].set_title('Learned Score Field (Tractable SM)', fontsize=14)\n",
    "axes[1].set_xlabel('x1')\n",
    "axes[1].set_ylabel('x2')\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Score Function Learning via Tractable Score Matching', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The learned field closely matches the true field!\")\n",
    "print(\"But the Jacobian trace made this SLOW for just 2 dimensions.\")\n",
    "print(\"Imagine doing this for 784 dimensions (28x28 image)...\")\n",
    "print(\"This motivates Denoising Score Matching, which we cover next.\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Think About This\n",
    "\n",
    "1. **Why does the score function point toward high-probability regions?** Think about what the gradient of log-probability means geometrically.\n",
    "\n",
    "2. **What happens to the score at a local maximum of the density?** What about at the boundary between two modes?\n",
    "\n",
    "3. **Why is the Jacobian trace computation O(D^2)?** How many second derivatives do we need to compute?\n",
    "\n",
    "4. **Can you think of a way to avoid computing the full Jacobian?** (Hint: what if we added some noise to our data first?)\n",
    "\n",
    "### Extension Challenge\n",
    "\n",
    "Try modifying the mixture to have 4 clusters arranged in a square pattern. Does the learned score field still capture the structure? What happens if you reduce the number of hidden units in the network?\n",
    "\n",
    "### What's Next\n",
    "\n",
    "The tractable score matching loss works but is computationally prohibitive for high-dimensional data. In the next notebook, we will discover **Denoising Score Matching** -- Pascal Vincent's elegant trick that replaces the expensive Jacobian with a simple noise prediction target. This is the foundation of modern diffusion models."
   ],
   "id": "cell_19"
  }
 ]
}