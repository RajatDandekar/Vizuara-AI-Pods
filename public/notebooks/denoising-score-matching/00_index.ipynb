{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Denoising Score Matching -- Learning Path"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Score Matching -- Learning Path\n",
    "\n",
    "## Welcome to the Vizuara Denoising Score Matching Course!\n",
    "\n",
    "This series of notebooks will take you from the fundamentals of score functions to implementing a complete score-based generative model. Each notebook builds on the previous one, forming a cohesive learning journey.\n",
    "\n",
    "## Learning Path\n",
    "\n",
    "### Notebook 1: Score Functions and Energy-Based Models\n",
    "- **What you'll learn:** What the score function is, why it bypasses the partition function, and how to compute and visualize score fields\n",
    "- **Key concepts:** Energy-based models, partition function, score function, tractable score matching\n",
    "- **Final output:** Side-by-side comparison of true vs learned score fields\n",
    "\n",
    "### Notebook 2: Denoising Score Matching\n",
    "- **What you'll learn:** Pascal Vincent's key insight -- adding noise creates a known target, eliminating the expensive Jacobian computation\n",
    "- **Key concepts:** DSM loss, noise corruption, score-noise equivalence, connection to DDPM\n",
    "- **Final output:** Trained score network with magnitude heatmap visualization\n",
    "\n",
    "### Notebook 3: Langevin Dynamics and Sampling\n",
    "- **What you'll learn:** How to use a trained score function to generate new data samples\n",
    "- **Key concepts:** Langevin update rule, stochastic sampling, convergence, mode coverage\n",
    "- **Final output:** Complete generative pipeline from noise to realistic samples\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic Python and PyTorch\n",
    "- Familiarity with neural networks and gradient descent\n",
    "- Understanding of probability distributions (Gaussian, mixture models)\n",
    "\n",
    "## How to Use These Notebooks\n",
    "\n",
    "1. Run each notebook sequentially (01, 02, 03)\n",
    "2. Read the markdown cells carefully before running code\n",
    "3. Complete the TODO exercises -- they reinforce key concepts\n",
    "4. Experiment with the extension challenges"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ready to start! Open Notebook 01 to begin.\")\n",
    "print()\n",
    "print(\"Notebook 01: Score Functions and Energy-Based Models\")\n",
    "print(\"Notebook 02: Denoising Score Matching\")\n",
    "print(\"Notebook 03: Langevin Dynamics and Sampling\")"
   ],
   "id": "cell_2"
  }
 ]
}