{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Latent Variables & Autoencoders â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_01_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction: Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_01_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Variables & Autoencoders: Understanding the Foundation of VAEs\n",
    "\n",
    "*Part 1 of the Vizuara series on Variational Autoencoders From Scratch*\n",
    "*Estimated time: 35 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Imagine you want to build a machine that can *draw new faces it has never seen before*. Not copy-paste existing faces â€” actually **generate** brand-new, realistic faces from scratch.\n",
    "\n",
    "How would you even begin?\n",
    "\n",
    "The key insight is that images are not random collections of pixels. Behind every face lies a **hidden recipe** â€” the shape of the jaw, the color of the eyes, whether the person is smiling. If we could somehow learn these hidden recipes, we could mix and match them to create entirely new faces.\n",
    "\n",
    "These hidden recipes are what we call **latent variables**, and learning to work with them is the foundation of some of the most powerful generative models in AI â€” including Variational Autoencoders (VAEs), which are the focus of this series.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Build intuition for what latent variables are using a simple thought experiment\n",
    "2. Build a regular (deterministic) autoencoder on handwritten digits\n",
    "3. Visualize its latent space and discover a critical flaw â€” the **dead zone problem**\n",
    "4. Understand why this flaw motivates the probabilistic approach we will explore in Notebook 2\n",
    "\n",
    "By the end, you will see with your own eyes *why* regular autoencoders fail at generation â€” and why we need something better."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_02_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_02_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition: The Hidden Recipe\n",
    "\n",
    "Let us start with a thought experiment. No code, no math â€” just intuition.\n",
    "\n",
    "### The Cat Drawing Factory\n",
    "\n",
    "Imagine you run a factory that produces cat drawings. Every drawing that rolls off the assembly line looks different â€” some cats are fat, some are skinny, some have pointy ears, some have round ears.\n",
    "\n",
    "Now here is the crucial question: **what controls how each cat looks?**\n",
    "\n",
    "Behind every cat drawing, there are *hidden factors* that determine its appearance:\n",
    "- **Body roundness**: How chubby is the cat? (scale of 0 to 1)\n",
    "- **Ear pointiness**: Are the ears sharp triangles or soft curves? (scale of 0 to 1)\n",
    "\n",
    "These two numbers â€” body roundness and ear pointiness â€” are the **latent variables**. We call them \"latent\" because they are *hidden*. You never observe them directly. You only see the final drawing (the pixels), not the recipe that produced it.\n",
    "\n",
    "Now picture a 2D plane where the x-axis is \"body roundness\" and the y-axis is \"ear pointiness.\" Every possible cat drawing lives somewhere on this plane. A fat cat with pointy ears sits in the top-right corner. A skinny cat with round ears sits in the bottom-left.\n",
    "\n",
    "This 2D plane is the **latent space** â€” the space of all possible hidden recipes.\n",
    "\n",
    "Here is the magic: if we could learn a machine that takes *any point* on this 2D plane and produces a convincing cat drawing, we could generate infinite new cats just by picking random points.\n",
    "\n",
    "That is exactly what autoencoders (and later, VAEs) try to do.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "If you had to describe ANY cat drawing using just 2 numbers, what would those numbers represent? There is no single right answer â€” the point is that *some* compact description must exist, and we want our model to discover it automatically."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_03_math_of_latent_variables",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Math Of Latent Variables\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_03_math_of_latent_variables.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics of Latent Variables\n",
    "\n",
    "Now let us put some notation on these ideas. Do not worry â€” we will keep it simple.\n",
    "\n",
    "We use two variables:\n",
    "\n",
    "- $\\mathbf{x}$ = the **observed data** (the image you can see â€” e.g., a 28x28 pixel image flattened into 784 numbers)\n",
    "- $\\mathbf{z}$ = the **latent variables** (the hidden recipe â€” e.g., 2 numbers that capture the \"essence\" of the image)\n",
    "\n",
    "The **encoder** is a function that compresses the image into the hidden recipe:\n",
    "\n",
    "$$\\mathbf{z} = f_{\\text{encoder}}(\\mathbf{x})$$\n",
    "\n",
    "In plain English: \"Give me an image, and I will tell you its hidden recipe.\"\n",
    "\n",
    "The **decoder** is a function that reconstructs the image from the hidden recipe:\n",
    "\n",
    "$$\\hat{\\mathbf{x}} = f_{\\text{decoder}}(\\mathbf{z})$$\n",
    "\n",
    "In plain English: \"Give me a hidden recipe, and I will draw you the image.\"\n",
    "\n",
    "Together, the encoder and decoder form an **autoencoder**. The training objective is simple: make the reconstructed image $\\hat{\\mathbf{x}}$ as close as possible to the original image $\\mathbf{x}$.\n",
    "\n",
    "We measure \"closeness\" using the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{recon}} = \\frac{1}{N} \\sum_{i=1}^{N} \\| \\mathbf{x}_i - \\hat{\\mathbf{x}}_i \\|^2$$\n",
    "\n",
    "In plain English: \"For each pixel, compute the squared difference between the original and the reconstruction, then average over all pixels.\" The lower this number, the better our reconstruction.\n",
    "\n",
    "### A Concrete Example\n",
    "\n",
    "Suppose our image is just 4 pixels: $\\mathbf{x} = [0.9, 0.1, 0.8, 0.2]$ and our reconstruction is $\\hat{\\mathbf{x}} = [0.85, 0.15, 0.75, 0.25]$.\n",
    "\n",
    "The MSE would be:\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{4}\\left[(0.9-0.85)^2 + (0.1-0.15)^2 + (0.8-0.75)^2 + (0.2-0.25)^2\\right] = \\frac{1}{4}(0.01) = 0.0025$$\n",
    "\n",
    "That is a small number, which means the reconstruction is quite good."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_04_build_autoencoder_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Build Autoencoder Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_04_build_autoencoder_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let Us Build It â€” A Regular Autoencoder\n",
    "\n",
    "Time to get our hands dirty. We will build a regular autoencoder and train it on MNIST handwritten digits â€” a dataset of 70,000 grayscale images of digits 0 through 9, each 28x28 pixels.\n",
    "\n",
    "Why MNIST? It is small enough to train quickly, yet complex enough to reveal interesting patterns in the latent space.\n",
    "\n",
    "### 4.1 Setup and Data"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_05_data_setup",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Data Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_05_data_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab already has most of these, but just in case)\n",
    "!pip install -q torch torchvision matplotlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_data):,}\")\n",
    "print(f\"Test samples:     {len(test_data):,}\")\n",
    "print(f\"Image shape:      {train_data[0][0].shape} (channels x height x width)\")\n",
    "print(f\"Pixel range:      [{train_data[0][0].min():.1f}, {train_data[0][0].max():.1f}]\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_06_visualize_mnist",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Visualize Mnist\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_06_visualize_mnist.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some samples so we know what we are working with\n",
    "fig, axes = plt.subplots(2, 8, figsize=(14, 4))\n",
    "for i in range(16):\n",
    "    ax = axes[i // 8, i % 8]\n",
    "    ax.imshow(train_data[i][0].squeeze(), cmap='gray')\n",
    "    ax.set_title(str(train_data[i][1]), fontsize=10)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('MNIST Handwritten Digits â€” Our Training Data', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each image is 28x28 = 784 pixels.\")\n",
    "print(\"Our goal: compress these 784 numbers into just 2 latent numbers,\")\n",
    "print(\"then reconstruct the image from those 2 numbers alone.\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_07_the_encoder",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: The Encoder\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_07_the_encoder.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Encoder\n",
    "\n",
    "The encoder takes a 784-dimensional image and squeezes it through progressively smaller layers until we reach our tiny 2-dimensional latent space.\n",
    "\n",
    "Think of it like summarizing a full novel into a single sentence â€” we lose details, but the *essence* should survive."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim=512, latent_dim=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)   # 784 -> 512\n",
    "        self.fc2 = nn.Linear(hidden_dim, 256)          # 512 -> 256\n",
    "        self.fc_out = nn.Linear(256, latent_dim)       # 256 -> 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc_out(h)  # A single point in latent space\n",
    "\n",
    "# Key observation: this encoder outputs a SINGLE POINT (two numbers).\n",
    "# Later, when we build a VAE, the encoder will output a DISTRIBUTION instead.\n",
    "# That one change is what makes all the difference."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_08_deterministic_mapping",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Deterministic Mapping\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_08_deterministic_mapping.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice something important: this encoder maps each image to *exactly one point* in latent space. There is no randomness, no uncertainty â€” just a deterministic mapping. We will see why this is a problem later."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_09_the_decoder",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: The Decoder\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_09_the_decoder.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The Decoder\n",
    "\n",
    "The decoder does the reverse: it takes our 2 latent numbers and expands them back to 784 pixels."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=2, hidden_dim=512, output_dim=784):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 256)          # 2 -> 256\n",
    "        self.fc2 = nn.Linear(256, hidden_dim)          # 256 -> 512\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)   # 512 -> 784\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = F.relu(self.fc1(z))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return torch.sigmoid(self.fc3(h))  # Sigmoid keeps output in [0, 1]\n",
    "\n",
    "# The sigmoid at the end ensures our pixel values stay between 0 and 1,\n",
    "# matching the range of our input images."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_10_full_autoencoder",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Full Autoencoder\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_10_full_autoencoder.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The Full Autoencoder\n",
    "\n",
    "Now we combine the encoder and decoder into a single model. Data flows in one direction: image -> encoder -> latent code -> decoder -> reconstructed image."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim=512, latent_dim=2):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)        # Compress: 784 -> 2\n",
    "        x_recon = self.decoder(z)  # Reconstruct: 2 -> 784\n",
    "        return x_recon, z          # Return both for visualization"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us inspect what we have built\n",
    "model = Autoencoder(latent_dim=2).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(f\"  Input:   784 pixels (28x28 image)\")\n",
    "print(f\"  Encoder: 784 -> 512 -> 256 -> 2\")\n",
    "print(f\"  Decoder: 2 -> 256 -> 512 -> 784\")\n",
    "print(f\"  Output:  784 pixels (reconstructed image)\")\n",
    "print(f\"\\nBottleneck: 784 dimensions compressed to just 2!\")\n",
    "print(f\"That is a {784/2:.0f}x compression ratio.\")\n",
    "print(f\"\\nThis extreme compression forces the encoder to learn\")\n",
    "print(f\"only the most important features of each digit.\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_11_todo_training_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo Training Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_11_todo_training_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn: Implement the Training Loop\n",
    "\n",
    "Now it is time for you to write some code. The training loop for an autoencoder is straightforward:\n",
    "\n",
    "1. **Forward pass**: Feed an image through the model, get the reconstruction\n",
    "2. **Compute loss**: Measure how different the reconstruction is from the original\n",
    "3. **Backward pass**: Compute gradients\n",
    "4. **Update weights**: Take a step with the optimizer\n",
    "\n",
    "We use **MSE loss with sum reduction** â€” this sums the squared errors across all 784 pixels for each image in the batch, then we divide by the dataset size to get the average per-image loss.\n",
    "\n",
    "### TODO: Complete the training function\n",
    "\n",
    "Read through the code below. The five key steps are marked with comments. The solution is filled in, but make sure you understand *why* each step is there before running it."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_12_todo_training_loop_details",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo Training Loop Details\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_12_todo_training_loop_details.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model, train_loader, epochs=15, lr=1e-3):\n",
    "    \"\"\"Train the autoencoder using MSE reconstruction loss.\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            # Flatten 28x28 images to 784-dim vectors\n",
    "            data = data.view(-1, 784).to(device)\n",
    "\n",
    "            # ============ THE FIVE STEPS ============\n",
    "            # Step 1: Zero the gradients from the previous iteration\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Step 2: Forward pass â€” get reconstruction and latent code\n",
    "            x_recon, z = model(data)\n",
    "\n",
    "            # Step 3: Compute reconstruction loss (MSE, summed over pixels)\n",
    "            loss = F.mse_loss(x_recon, data, reduction='sum')\n",
    "\n",
    "            # Step 4: Backward pass â€” compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Step 5: Update the weights\n",
    "            optimizer.step()\n",
    "            # =========================================\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        losses.append(avg_loss)\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}')\n",
    "\n",
    "    return losses"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Before running the next cell, predict what will happen to the loss over time. Will it go up, go down, or stay flat? Why?"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_13_run_training",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Run Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_13_run_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the autoencoder!\n",
    "print(\"Training the autoencoder (this takes about 1-2 minutes)...\")\n",
    "print(\"=\" * 50)\n",
    "losses = train_autoencoder(model, train_loader, epochs=15)\n",
    "print(\"=\" * 50)\n",
    "print(\"Training complete!\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_14_training_curve_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Training Curve Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_14_training_curve_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses, 'b-', linewidth=2, marker='o', markersize=4)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Average Reconstruction Loss (MSE)', fontsize=12)\n",
    "plt.title('Autoencoder Training Progress', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Starting loss: {losses[0]:.4f}\")\n",
    "print(f\"Final loss:    {losses[-1]:.4f}\")\n",
    "print(f\"Improvement:   {(1 - losses[-1]/losses[0])*100:.1f}%\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss should decrease quickly at first (the model learns the broad strokes) and then plateau (the remaining details are hard to capture with only 2 latent dimensions)."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_15_reconstructions_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Reconstructions Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_15_reconstructions_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing Reconstructions\n",
    "\n",
    "Let us see how well our autoencoder actually works. We will feed in test images it has never seen before and compare the originals to the reconstructions."
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_16_reconstruction_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Reconstruction Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_16_reconstruction_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs reconstructed images\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_batch = next(iter(test_loader))\n",
    "    test_images = test_batch[0][:8].view(-1, 784).to(device)\n",
    "    test_labels = test_batch[1][:8]\n",
    "    recon_images, latent_codes = model(test_images)\n",
    "\n",
    "fig, axes = plt.subplots(3, 8, figsize=(14, 6))\n",
    "\n",
    "for i in range(8):\n",
    "    # Original\n",
    "    axes[0, i].imshow(test_images[i].cpu().view(28, 28), cmap='gray')\n",
    "    axes[0, i].set_title(f'Digit {test_labels[i].item()}', fontsize=9)\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    # Reconstructed\n",
    "    axes[1, i].imshow(recon_images[i].cpu().view(28, 28), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "    # Difference (error map)\n",
    "    diff = torch.abs(test_images[i] - recon_images[i]).cpu().view(28, 28)\n",
    "    axes[2, i].imshow(diff, cmap='Reds', vmin=0, vmax=0.5)\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Original', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Reconstructed', fontsize=11, fontweight='bold')\n",
    "axes[2, 0].set_ylabel('Error', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Original vs Reconstructed Digits (2D Latent Space)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The reconstructions capture the overall shape but lose fine details.\")\n",
    "print(\"This is expected â€” we compressed 784 dimensions into just 2!\")\n",
    "print(f\"\\nLatent codes for these images (each is just 2 numbers):\")\n",
    "for i in range(8):\n",
    "    z = latent_codes[i].cpu().numpy()\n",
    "    print(f\"  Digit {test_labels[i].item()}: z = [{z[0]:.2f}, {z[1]:.2f}]\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how each digit is represented by just two numbers. Different instances of the same digit tend to have similar latent codes â€” that is the encoder learning a meaningful representation."
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_17_dead_zone_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Dead Zone Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_17_dead_zone_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. The Dead Zone Problem\n",
    "\n",
    "This is the most important section of the entire notebook. Everything we have built so far *works* â€” the autoencoder can compress and reconstruct digits. But there is a fatal flaw that prevents it from being a good **generative** model.\n",
    "\n",
    "Let us see it with our own eyes.\n",
    "\n",
    "### 7.1 Mapping the Latent Space\n",
    "\n",
    "We will encode every image in the test set and plot where it lands in the 2D latent space, colored by digit class."
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_18_latent_space_map_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Latent Space Map Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_18_latent_space_map_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the entire test set and visualize the latent space\n",
    "model.eval()\n",
    "all_z = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        data = data.view(-1, 784).to(device)\n",
    "        _, z = model(data)\n",
    "        all_z.append(z.cpu())\n",
    "        all_labels.append(labels)\n",
    "\n",
    "all_z = torch.cat(all_z).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(all_z[:, 0], all_z[:, 1], c=all_labels, cmap='tab10',\n",
    "                       s=3, alpha=0.5)\n",
    "cbar = plt.colorbar(scatter, label='Digit Class')\n",
    "cbar.set_ticks(range(10))\n",
    "plt.xlabel('Latent Dimension 1 ($z_1$)', fontsize=12)\n",
    "plt.ylabel('Latent Dimension 2 ($z_2$)', fontsize=12)\n",
    "plt.title('Regular Autoencoder Latent Space â€” Notice the Gaps!',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Look at this plot carefully.\")\n",
    "print(\"Each color is a different digit (0-9).\")\n",
    "print(\"Notice the EMPTY SPACES between clusters.\")\n",
    "print(\"These gaps are the 'dead zones' â€” regions where NO training data lives.\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study this plot carefully. You should see distinct clusters for each digit, with **empty gaps** between them. These gaps are the crux of the problem."
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_19_sample_dead_zones",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Sample Dead Zones\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_19_sample_dead_zones.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 What Happens When We Sample from Dead Zones?\n",
    "\n",
    "If we want to use our autoencoder as a **generator**, we would pick random points in the latent space and feed them through the decoder. But what happens when we pick a point that falls in one of those empty gaps?"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from dead zones vs. from actual cluster regions\n",
    "model.eval()\n",
    "\n",
    "# Points from empty regions (dead zones)\n",
    "# We pick coordinates that are likely in the gaps between clusters\n",
    "dead_zone_points = torch.tensor([\n",
    "    [0.0, 0.0],       # center of the space â€” likely a gap\n",
    "    [-5.0, 5.0],      # far corner â€” probably empty\n",
    "    [10.0, -10.0],    # another remote region\n",
    "    [3.0, 3.0],       # midpoint between clusters\n",
    "], dtype=torch.float32).to(device)\n",
    "\n",
    "# Points from actual cluster centers (near real encodings)\n",
    "# We pick indices of different digits for variety\n",
    "cluster_indices = []\n",
    "for digit in [0, 3, 7, 9]:\n",
    "    idx = np.where(all_labels == digit)[0][0]\n",
    "    cluster_indices.append(idx)\n",
    "cluster_points = torch.tensor(all_z[cluster_indices], dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    dead_decoded = model.decoder(dead_zone_points).cpu()\n",
    "    cluster_decoded = model.decoder(cluster_points).cpu()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "digit_names = ['0', '3', '7', '9']\n",
    "dead_labels = ['(0, 0)', '(-5, 5)', '(10, -10)', '(3, 3)']\n",
    "\n",
    "for i in range(4):\n",
    "    # Top row: good samples from cluster centers\n",
    "    axes[0, i].imshow(cluster_decoded[i].view(28, 28), cmap='gray')\n",
    "    axes[0, i].set_title(f'Near digit {digit_names[i]}\\nz = [{all_z[cluster_indices[i]][0]:.1f}, {all_z[cluster_indices[i]][1]:.1f}]',\n",
    "                          fontsize=9)\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    # Bottom row: garbage from dead zones\n",
    "    axes[1, i].imshow(dead_decoded[i].view(28, 28), cmap='gray')\n",
    "    axes[1, i].set_title(f'Dead zone\\nz = {dead_labels[i]}', fontsize=9)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Good regions\\n(clear digits)', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Dead zones\\n(garbage!)', fontsize=11, fontweight='bold')\n",
    "plt.suptitle('The Dead Zone Problem: Why Regular Autoencoders Fail at Generation',\n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top row: Points sampled near real data produce clean digits.\")\n",
    "print(\"Bottom row: Points sampled from gaps produce meaningless noise.\")\n",
    "print(\"\\nThis is the DEAD ZONE PROBLEM.\")\n",
    "print(\"A generative model MUST produce valid outputs for ANY point in the latent space.\")\n",
    "print(\"Regular autoencoders simply do not guarantee this.\")"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_20_dead_zone_explanation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Dead Zone Explanation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_20_dead_zone_explanation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the fundamental limitation. The decoder only learned to produce good outputs for the specific regions where training data exists. Everywhere else is uncharted territory â€” and the decoder produces garbage."
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_21_grid_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Grid Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_21_grid_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Visualizing Dead Zones on a Grid\n",
    "\n",
    "Let us make this even more concrete. We will create a grid of points covering the entire latent space and decode every single one. This gives us a \"map\" of what the decoder produces everywhere."
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode a grid of points across the latent space\n",
    "n_grid = 20\n",
    "z1_range = np.linspace(all_z[:, 0].min() - 2, all_z[:, 0].max() + 2, n_grid)\n",
    "z2_range = np.linspace(all_z[:, 1].min() - 2, all_z[:, 1].max() + 2, n_grid)\n",
    "\n",
    "# Decode each grid point\n",
    "figure = np.zeros((28 * n_grid, 28 * n_grid))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, z1 in enumerate(z1_range):\n",
    "        for j, z2 in enumerate(z2_range):\n",
    "            z = torch.tensor([[z1, z2]], dtype=torch.float32).to(device)\n",
    "            decoded = model.decoder(z).cpu().view(28, 28).numpy()\n",
    "            figure[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = decoded\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(figure, cmap='gray', origin='lower',\n",
    "           extent=[z1_range[0], z1_range[-1], z2_range[0], z2_range[-1]])\n",
    "plt.xlabel('Latent Dimension 1 ($z_1$)', fontsize=12)\n",
    "plt.ylabel('Latent Dimension 2 ($z_2$)', fontsize=12)\n",
    "plt.title('Decoder Output Across the Entire Latent Space\\n(Blurry/empty regions = dead zones)',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.colorbar(label='Pixel intensity')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"This grid shows what the decoder produces for every point in latent space.\")\n",
    "print(\"Clear digits appear where training data existed.\")\n",
    "print(\"Blurry, meaningless blobs appear in the dead zones.\")\n",
    "print(\"\\nA good generative model would produce clear, meaningful images EVERYWHERE.\")"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_22_todo_latent_dim_experiment",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo Latent Dim Experiment\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_22_todo_latent_dim_experiment.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Your Turn: Experiment with Latent Dimensions\n",
    "\n",
    "We used `latent_dim=2` so we could visualize the latent space on a 2D plot. But what happens if we change the latent dimension? Does the dead zone problem get better or worse?\n",
    "\n",
    "### TODO: Train autoencoders with different latent dimensions and compare"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_23_run_latent_dim_experiment",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Run Latent Dim Experiment\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_23_run_latent_dim_experiment.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_train_and_evaluate(latent_dim, epochs=10):\n",
    "    \"\"\"Train an autoencoder with a given latent dimension and measure quality.\"\"\"\n",
    "    ae = Autoencoder(latent_dim=latent_dim).to(device)\n",
    "    optimizer = optim.Adam(ae.parameters(), lr=1e-3)\n",
    "\n",
    "    # Train\n",
    "    ae.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data, _ in train_loader:\n",
    "            data = data.view(-1, 784).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            x_recon, z = ae(data)\n",
    "            loss = F.mse_loss(x_recon, data, reduction='sum')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate reconstruction quality on test set\n",
    "    ae.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.view(-1, 784).to(device)\n",
    "            x_recon, _ = ae(data)\n",
    "            total_loss += F.mse_loss(x_recon, data, reduction='sum').item()\n",
    "\n",
    "    avg_loss = total_loss / len(test_data)\n",
    "    return ae, avg_loss\n",
    "\n",
    "\n",
    "# ============ EXPERIMENT ============\n",
    "# Try different latent dimensions\n",
    "dims_to_try = [2, 5, 10, 20]\n",
    "results = {}\n",
    "\n",
    "print(\"Training autoencoders with different latent dimensions...\")\n",
    "print(\"=\" * 50)\n",
    "for dim in dims_to_try:\n",
    "    print(f\"\\nLatent dim = {dim}:\")\n",
    "    ae, loss = quick_train_and_evaluate(dim, epochs=10)\n",
    "    results[dim] = loss\n",
    "    print(f\"  Test reconstruction loss: {loss:.4f}\")\n",
    "print(\"=\" * 50)"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_24_latent_dim_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Latent Dim Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_24_latent_dim_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart of reconstruction loss\n",
    "dims = list(results.keys())\n",
    "losses_by_dim = list(results.values())\n",
    "colors = ['#e74c3c' if d == 2 else '#3498db' for d in dims]\n",
    "axes[0].bar([str(d) for d in dims], losses_by_dim, color=colors, edgecolor='black')\n",
    "axes[0].set_xlabel('Latent Dimension', fontsize=12)\n",
    "axes[0].set_ylabel('Reconstruction Loss (lower = better)', fontsize=12)\n",
    "axes[0].set_title('More Dimensions = Better Reconstruction', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Compression ratio\n",
    "ratios = [784 / d for d in dims]\n",
    "axes[1].bar([str(d) for d in dims], ratios, color='#2ecc71', edgecolor='black')\n",
    "axes[1].set_xlabel('Latent Dimension', fontsize=12)\n",
    "axes[1].set_ylabel('Compression Ratio (784 / latent_dim)', fontsize=12)\n",
    "axes[1].set_title('The Compression-Quality Tradeoff', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: more latent dimensions = better reconstruction.\")\n",
    "print(\"But more dimensions also means a HIGHER-dimensional latent space,\")\n",
    "print(\"which makes the dead zone problem WORSE (exponentially more empty space).\")\n",
    "print(\"\\nThis is why VAEs are so important: they solve the dead zone problem\")\n",
    "print(\"regardless of how many latent dimensions we use.\")"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Think about this tradeoff. With 2 latent dimensions, reconstruction is poor but we can visualize the space. With 20 dimensions, reconstruction is great but the dead zones become an even bigger problem (there is exponentially more empty space in higher dimensions). How would you solve this dilemma?"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_25_quantify_dead_zones",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Quantify Dead Zones\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_25_quantify_dead_zones.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quantifying the Dead Zones\n",
    "\n",
    "Let us put a number on the dead zone problem. We will estimate what fraction of the latent space is \"dead\" (produces garbage when decoded)."
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_dead_zones(model, test_loader, device, grid_size=20):\n",
    "    \"\"\"Estimate how much of the latent space is 'dead' (produces garbage).\"\"\"\n",
    "    model.eval()\n",
    "    all_z = []\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.view(-1, 784).to(device)\n",
    "            _, z = model(data)\n",
    "            all_z.append(z.cpu())\n",
    "    all_z = torch.cat(all_z).numpy()\n",
    "\n",
    "    # Create a grid over the latent space (with some padding)\n",
    "    z1_min, z1_max = all_z[:, 0].min() - 1, all_z[:, 0].max() + 1\n",
    "    z2_min, z2_max = all_z[:, 1].min() - 1, all_z[:, 1].max() + 1\n",
    "    z1_range = np.linspace(z1_min, z1_max, grid_size)\n",
    "    z2_range = np.linspace(z2_min, z2_max, grid_size)\n",
    "\n",
    "    # Count grid cells that have NO nearby encodings\n",
    "    dead_count = 0\n",
    "    total_count = grid_size ** 2\n",
    "    for z1 in z1_range:\n",
    "        for z2 in z2_range:\n",
    "            distances = np.sqrt((all_z[:, 0] - z1)**2 + (all_z[:, 1] - z2)**2)\n",
    "            if distances.min() > 1.0:  # No encoding within radius 1.0\n",
    "                dead_count += 1\n",
    "\n",
    "    return dead_count / total_count * 100\n",
    "\n",
    "\n",
    "dead_pct = count_dead_zones(model, test_loader, device, grid_size=25)\n",
    "print(f\"Estimated dead zone coverage: {dead_pct:.1f}% of the latent space\")\n",
    "print(f\"\\nThis means roughly {dead_pct:.0f}% of the latent space produces garbage!\")\n",
    "print(\"If you randomly sample a point, you have about a {:.0f}% chance\".format(dead_pct))\n",
    "print(\"of getting a meaningless output.\")\n",
    "print(\"\\nA VAE fixes this by forcing the encoder to spread its encodings\")\n",
    "print(\"smoothly across the latent space, filling in the gaps.\")"
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_26_interpolation_test",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Interpolation Test\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_26_interpolation_test.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. The Motivation for VAEs: Interpolation Test\n",
    "\n",
    "Let us do one final experiment that really drives the point home. We will **interpolate** between two digits by drawing a straight line through the latent space. In a good generative model, every point along this line should produce a valid, meaningful image."
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear interpolation between two different digits\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Find a '3' and an '8' in the test set\n",
    "    for data, labels in test_loader:\n",
    "        idx_3 = (labels == 3).nonzero(as_tuple=True)[0]\n",
    "        idx_8 = (labels == 8).nonzero(as_tuple=True)[0]\n",
    "        if len(idx_3) > 0 and len(idx_8) > 0:\n",
    "            img_3 = data[idx_3[0]].view(-1, 784).to(device)\n",
    "            img_8 = data[idx_8[0]].view(-1, 784).to(device)\n",
    "            break\n",
    "\n",
    "    z_3 = model.encoder(img_3)\n",
    "    z_8 = model.encoder(img_8)\n",
    "\n",
    "    # Interpolate between the two latent codes\n",
    "    n_steps = 10\n",
    "    alphas = torch.linspace(0, 1, n_steps).to(device)\n",
    "    interpolated = []\n",
    "    z_points = []\n",
    "    for alpha in alphas:\n",
    "        z_interp = (1 - alpha) * z_3 + alpha * z_8\n",
    "        decoded = model.decoder(z_interp)\n",
    "        interpolated.append(decoded.cpu().view(28, 28))\n",
    "        z_points.append(z_interp.cpu().numpy().flatten())\n",
    "\n",
    "# Plot the interpolation\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 5), gridspec_kw={'height_ratios': [3, 1]})\n",
    "\n",
    "# Top: the decoded images\n",
    "ax_imgs = fig.add_subplot(2, 1, 1)\n",
    "combined = torch.cat([img.unsqueeze(0) for img in interpolated], dim=0)\n",
    "grid_img = np.concatenate([img.numpy() for img in interpolated], axis=1)\n",
    "ax_imgs.imshow(grid_img, cmap='gray')\n",
    "ax_imgs.set_title('Interpolation from Digit 3 to Digit 8 in Regular Autoencoder',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "ax_imgs.set_xticks([14 + 28*i for i in range(n_steps)])\n",
    "ax_imgs.set_xticklabels([f'{a:.1f}' for a in alphas.cpu().numpy()], fontsize=9)\n",
    "ax_imgs.set_xlabel('Interpolation factor (alpha)', fontsize=11)\n",
    "ax_imgs.set_yticks([])\n",
    "\n",
    "# Bottom: the latent space trajectory\n",
    "z_points = np.array(z_points)\n",
    "ax_traj = fig.add_subplot(2, 1, 2)\n",
    "ax_traj.scatter(all_z[:, 0], all_z[:, 1], c=all_labels, cmap='tab10', s=1, alpha=0.2)\n",
    "ax_traj.plot(z_points[:, 0], z_points[:, 1], 'r-o', linewidth=2, markersize=5, label='Interpolation path')\n",
    "ax_traj.set_xlabel('$z_1$', fontsize=11)\n",
    "ax_traj.set_ylabel('$z_2$', fontsize=11)\n",
    "ax_traj.set_title('Interpolation Path Through Latent Space', fontsize=12)\n",
    "ax_traj.legend(fontsize=10)\n",
    "ax_traj.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how some intermediate images look blurry or distorted?\")\n",
    "print(\"That is because the interpolation path passes through dead zones\")\n",
    "print(\"where the decoder has never seen any training data.\")\n",
    "print(\"\\nIn Notebook 2, we will learn the mathematical framework (the ELBO)\")\n",
    "print(\"that fixes this by forcing the latent space to be smooth and complete.\")\n",
    "print(\"In Notebook 3, we will build a VAE and see beautiful, smooth interpolations.\")"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_27_closing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Wrap-Up: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_27_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Takeaways and What is Next\n",
    "\n",
    "Let us summarize what we have learned:\n",
    "\n",
    "### What We Built\n",
    "- An **autoencoder** with an encoder (784 -> 2) and a decoder (2 -> 784)\n",
    "- The encoder compresses images into a compact latent representation\n",
    "- The decoder reconstructs images from this representation\n",
    "\n",
    "### What We Discovered\n",
    "1. **Latent variables** ($\\mathbf{z}$) are hidden factors that capture the essence of our data\n",
    "2. A regular autoencoder maps each image to a **single point** in latent space\n",
    "3. This creates **dead zones** â€” empty regions where the decoder produces garbage\n",
    "4. The dead zone problem makes regular autoencoders **poor generators**\n",
    "\n",
    "### Why This Matters\n",
    "The dead zone problem is not just a minor inconvenience â€” it is a fundamental limitation. To build a true generative model, we need **every point** in the latent space to produce a valid output. This requires a fundamentally different approach: instead of mapping each image to a single point, we need to map it to a **distribution**.\n",
    "\n",
    "### Reflection Questions\n",
    "1. Why does compressing 784 dimensions to just 2 still preserve the digit identity? What does this tell us about the \"true dimensionality\" of handwritten digits?\n",
    "2. If you increased the latent dimension from 2 to 20, would the dead zone problem get better or worse? (Hint: think about how volume scales with dimension.)\n",
    "3. How is this encoder-decoder setup similar to how your brain processes visual information? When you close your eyes and imagine a \"3\", what is your brain's \"latent code\"?\n",
    "\n",
    "### Optional Challenges\n",
    "1. **Convolutional encoder**: Replace the fully-connected layers with `nn.Conv2d` and `nn.ConvTranspose2d`. Does reconstruction improve?\n",
    "2. **Batch normalization**: Add `nn.BatchNorm1d` after each hidden layer. How does it affect training speed and final quality?\n",
    "3. **Deeper network**: Add more hidden layers (e.g., 784 -> 512 -> 256 -> 128 -> 64 -> 2). Does it help?\n",
    "\n",
    "### What is Next\n",
    "\n",
    "In **Notebook 2: The ELBO and KL Divergence**, we will learn the mathematical framework that transforms our regular autoencoder into a *Variational* Autoencoder:\n",
    "- The **Evidence Lower Bound (ELBO)** â€” the loss function that makes VAEs work\n",
    "- **KL Divergence** â€” how we measure the \"distance\" between two probability distributions\n",
    "- The **reparameterization trick** â€” the clever hack that makes it all trainable\n",
    "\n",
    "The key idea: instead of mapping each image to a single point, the encoder will output a *probability distribution*, and the KL divergence term will ensure these distributions overlap, filling in the dead zones.\n",
    "\n",
    "See you in Notebook 2!"
   ],
   "id": "cell_42"
  }
 ]
}