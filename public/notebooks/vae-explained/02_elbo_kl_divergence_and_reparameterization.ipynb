{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "The ELBO, KL Divergence & Reparameterization \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ELBO, KL Divergence & Reparameterization Trick\n",
    "\n",
    "**Notebook 2 of 4** in the Vizuara series on *Variational Autoencoders From Scratch*\n",
    "\n",
    "In this notebook, we will derive and implement the three pillars of VAE training: the Evidence Lower Bound (ELBO), the KL divergence regularizer, and the reparameterization trick that makes it all trainable with gradient descent.\n",
    "\n",
    "By the end, you will have a deep, hands-on understanding of every term in the VAE loss function -- and you will have built each piece from scratch."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Before we dive in, let us see *why* we need the ELBO at all.\n",
    "\n",
    "In Notebook 1, we built a regular autoencoder that maps images to single points in a latent space. The problem? When we sample random points to generate new images, we get garbage -- because the latent space is full of \"dead zones\" that the decoder has never seen.\n",
    "\n",
    "The VAE fixes this by making the encoder output a *distribution* instead of a point. But this creates a new question: **how do we train this probabilistic model?**\n",
    "\n",
    "We cannot just minimize pixel error any more. We need an objective function that simultaneously:\n",
    "1. Makes reconstructions accurate (decoded images should match originals)\n",
    "2. Keeps the latent space well-organized (no dead zones)\n",
    "\n",
    "The **ELBO** is that objective function. It elegantly balances both goals with exactly two terms -- and we will derive, visualize, and implement every piece of it.\n",
    "\n",
    "Let us start with a quick preview of what the ELBO looks like in action."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- Preview: The two forces of the ELBO ---\n",
    "# Reconstruction wants the decoder to perfectly reconstruct the input.\n",
    "# KL divergence wants the encoder to stay close to a standard normal.\n",
    "\n",
    "# Simulate a training run: reconstruction loss drops while KL rises, then both settle\n",
    "epochs = np.arange(1, 51)\n",
    "recon_loss = 250 * np.exp(-0.08 * epochs) + 90 + np.random.normal(0, 3, len(epochs))\n",
    "kl_loss = 25 * (1 - np.exp(-0.12 * epochs)) + np.random.normal(0, 1, len(epochs))\n",
    "total_loss = recon_loss + kl_loss\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(epochs, recon_loss, color='#2196F3', linewidth=2)\n",
    "axes[0].set_title('Reconstruction Loss', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_ylim(0, 350)\n",
    "\n",
    "axes[1].plot(epochs, kl_loss, color='#FF5722', linewidth=2)\n",
    "axes[1].set_title('KL Divergence', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_ylim(0, 40)\n",
    "\n",
    "axes[2].plot(epochs, total_loss, color='#4CAF50', linewidth=2)\n",
    "axes[2].set_title('Total ELBO Loss', fontsize=13, fontweight='bold')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Loss')\n",
    "axes[2].set_ylim(0, 350)\n",
    "\n",
    "plt.suptitle('The Two Forces of VAE Training', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Reconstruction loss DECREASES as the decoder improves.\")\n",
    "print(\"KL divergence INCREASES as the encoder learns to use the latent space.\")\n",
    "print(\"The ELBO balances both -- and that balance is what makes VAEs work.\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition -- Why We Need a Training Objective\n",
    "\n",
    "Let us think about this carefully before touching any math.\n",
    "\n",
    "In a regular autoencoder, the training objective is simple: minimize the difference between the input image and the reconstructed image. This is just mean squared error or binary cross-entropy between pixels. Easy.\n",
    "\n",
    "But a VAE is different. The encoder does not output a single point -- it outputs a **probability distribution** described by a mean $\\mu$ and standard deviation $\\sigma$. We then *sample* a latent code $z$ from this distribution and pass it to the decoder.\n",
    "\n",
    "This creates two competing pressures:\n",
    "\n",
    "**Pressure 1: Reconstruction quality.** The decoder wants precise, informative latent codes so it can reconstruct the image perfectly. If the encoder's distribution is very narrow (small $\\sigma$), then every sample is close to $\\mu$, and the decoder gets a consistent, predictable input. This is great for reconstruction.\n",
    "\n",
    "**Pressure 2: Latent space regularity.** But if every image's distribution is narrow and far from the origin, the latent space is just a regular autoencoder in disguise -- full of dead zones. We need the distributions to overlap and cover the space smoothly. This means pushing them toward a standard normal distribution $\\mathcal{N}(0, I)$.\n",
    "\n",
    "Think of it as a GPS analogy:\n",
    "- **Reconstruction loss** is like \"arrive at the destination as accurately as possible.\"\n",
    "- **KL divergence** is like \"but stay on the main roads\" (the standard normal prior).\n",
    "\n",
    "If you only cared about reaching the destination, you would take bizarre shortcuts through fields and alleys. If you only cared about staying on main roads, you would never reach your specific destination. The ELBO combines both: **reach your destination while staying on well-traveled paths.**\n",
    "\n",
    "This tension between reconstruction and regularization is the heart of VAE training. Now let us formalize it mathematically."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics -- Deriving the ELBO Step by Step\n",
    "\n",
    "We will derive the ELBO from first principles. Every step will be followed by a plain-English explanation and a numerical example.\n",
    "\n",
    "### 3.1 What Are We Trying to Maximize?\n",
    "\n",
    "We want our model to assign high probability to real data. Mathematically, we want to maximize the **log-likelihood** of the data:\n",
    "\n",
    "$$\\log p_\\theta(x)$$\n",
    "\n",
    "In plain English: \"How likely is data point $x$ under our model?\" We want this to be as large as possible.\n",
    "\n",
    "But computing this directly requires an intractable integral:\n",
    "\n",
    "$$p_\\theta(x) = \\int p_\\theta(x|z) \\, p(z) \\, dz$$\n",
    "\n",
    "This says: to find the probability of image $x$, we would need to try *every possible* latent code $z$, decode each one, and sum up the probabilities. This is impossible for continuous, high-dimensional $z$.\n",
    "\n",
    "**Numerical intuition:** Imagine a 2D latent space. Even if we discretized it into a 100x100 grid, we would need 10,000 forward passes through the decoder. For a 20-dimensional latent space? That is $100^{20}$ evaluations -- far more than atoms in the universe."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Step 1 -- Introduce the Encoder\n",
    "\n",
    "Since we cannot compute the integral directly, we introduce our encoder $q_\\phi(z|x)$ as a helper. We multiply and divide by it inside the integral:\n",
    "\n",
    "$$\\log p_\\theta(x) = \\log \\int p_\\theta(x, z) \\, dz = \\log \\int \\frac{p_\\theta(x, z)}{q_\\phi(z|x)} \\cdot q_\\phi(z|x) \\, dz$$\n",
    "\n",
    "In plain English: \"We have not changed anything mathematically -- we just multiplied by 1 in a clever way. But now the integral looks like an *expectation* under $q_\\phi(z|x)$, which we can approximate by sampling.\""
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical illustration: multiplying and dividing by the same thing = identity\n",
    "# This is the mathematical trick that gets the derivation started.\n",
    "\n",
    "p_xz = 0.03   # p(x, z) -- joint probability for some specific z\n",
    "q_zx = 0.15   # q(z|x) -- encoder probability for that same z\n",
    "\n",
    "# Original: just p(x,z)\n",
    "print(f\"p(x, z) = {p_xz}\")\n",
    "\n",
    "# Multiply and divide by q(z|x):\n",
    "print(f\"p(x, z) / q(z|x) * q(z|x) = {p_xz / q_zx} * {q_zx} = {(p_xz / q_zx) * q_zx}\")\n",
    "print(f\"\\nSame number! But now we can treat q(z|x) as a sampling distribution.\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Step 2 -- Apply Jensen's Inequality\n",
    "\n",
    "Jensen's inequality states: for a concave function like $\\log$, the log of an expectation is at least as large as the expectation of the log.\n",
    "\n",
    "$$\\log \\mathbb{E}[X] \\geq \\mathbb{E}[\\log X]$$\n",
    "\n",
    "Applying this to our integral:\n",
    "\n",
    "$$\\log p_\\theta(x) \\geq \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log \\frac{p_\\theta(x, z)}{q_\\phi(z|x)}\\right]$$\n",
    "\n",
    "In plain English: \"The true log-likelihood is *at least as large as* the right-hand side. So if we maximize the right-hand side (the ELBO), we are pushing up the true objective as well.\""
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Jensen's inequality with concrete numbers.\n",
    "# For the concave function log: log(E[X]) >= E[log(X)]\n",
    "\n",
    "values = np.array([1.0, 2.0, 5.0, 8.0])\n",
    "weights = np.array([0.25, 0.25, 0.25, 0.25])  # uniform weights\n",
    "\n",
    "# Left side: log of the expectation\n",
    "expectation = np.sum(weights * values)\n",
    "log_of_expectation = np.log(expectation)\n",
    "\n",
    "# Right side: expectation of the log\n",
    "log_values = np.log(values)\n",
    "expectation_of_log = np.sum(weights * log_values)\n",
    "\n",
    "print(\"Jensen's Inequality Demonstration\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Values: {values}\")\n",
    "print(f\"Weights: {weights}\")\n",
    "print()\n",
    "print(f\"E[X]      = {expectation:.4f}\")\n",
    "print(f\"log(E[X]) = {log_of_expectation:.4f}   <-- LEFT side\")\n",
    "print()\n",
    "print(f\"log(X)    = {np.round(log_values, 4)}\")\n",
    "print(f\"E[log(X)] = {expectation_of_log:.4f}   <-- RIGHT side\")\n",
    "print()\n",
    "print(f\"log(E[X]) >= E[log(X)]  =>  {log_of_expectation:.4f} >= {expectation_of_log:.4f}  =>  {log_of_expectation >= expectation_of_log}\")\n",
    "print()\n",
    "print(\"The gap between these two is related to the KL divergence.\")\n",
    "print(\"By maximizing the lower bound (ELBO), we push up the true log-likelihood.\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Step 3 -- Expand the Joint Probability\n",
    "\n",
    "We can factorize the joint probability as $p_\\theta(x, z) = p_\\theta(x|z) \\cdot p(z)$. Substituting:\n",
    "\n",
    "$$\\log p_\\theta(x) \\geq \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log p_\\theta(x|z) + \\log p(z) - \\log q_\\phi(z|x)\\right]$$\n",
    "\n",
    "In plain English: \"We split the joint probability into two parts -- the likelihood of $x$ given $z$ (how well the decoder reconstructs), and the prior probability of $z$ (how 'normal' is this latent code).\""
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Step 4 -- Rearrange into Two Beautiful Terms\n",
    "\n",
    "Grouping the $p(z)$ and $q_\\phi(z|x)$ terms together, we arrive at:\n",
    "\n",
    "$$\\log p_\\theta(x) \\geq \\underbrace{\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]}_{\\text{Reconstruction}} - \\underbrace{D_{KL}(q_\\phi(z|x) \\,\\|\\, p(z))}_{\\text{Regularization}}$$\n",
    "\n",
    "**This is the Evidence Lower Bound (ELBO).** It has exactly two terms:\n",
    "\n",
    "**Term 1: Reconstruction Loss** -- $\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]$\n",
    "\n",
    "\"Sample a latent code $z$ from the encoder, pass it through the decoder, and measure how well the output matches the input.\" Higher is better. In practice, we use the negative binary cross-entropy (for binary images) or negative MSE (for continuous images).\n",
    "\n",
    "**Term 2: KL Divergence** -- $D_{KL}(q_\\phi(z|x) \\,\\|\\, p(z))$\n",
    "\n",
    "\"How different is the encoder's output distribution from the standard normal prior?\" Lower is better. This term acts as a regularizer -- it prevents the encoder from collapsing to a trivial mapping.\n",
    "\n",
    "We *maximize* the ELBO, which means we *maximize* reconstruction (make the first term large) while *minimizing* KL divergence (make the second term small). In practice, we negate the ELBO and *minimize* it as a loss:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{VAE}} = -\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] + D_{KL}(q_\\phi(z|x) \\,\\|\\, p(z))$$\n",
    "\n",
    "Let us now implement both terms with numerical examples."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Implementing Each Component\n",
    "\n",
    "### 4.1 Reconstruction Loss (Binary Cross-Entropy)\n",
    "\n",
    "For binary images (like MNIST), the reconstruction loss is the **binary cross-entropy** (BCE) between the true pixel values $x_i$ and the decoder's predicted probabilities $p_i$:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{recon}} = -\\sum_{i=1}^{D} \\left[ x_i \\log(p_i) + (1 - x_i) \\log(1 - p_i) \\right]$$\n",
    "\n",
    "In plain English: for each pixel, we ask \"if the true value is 1, how confident was the decoder that it is 1? If the true value is 0, how confident was the decoder that it is 0?\" We sum up these log-probabilities across all pixels.\n",
    "\n",
    "Let us compute this by hand for a tiny 4-pixel image."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# --- Reconstruction Loss: Step-by-step computation ---\n",
    "\n",
    "# A tiny 4-pixel image and the decoder's predictions\n",
    "x = torch.tensor([1.0, 0.0, 1.0, 1.0])          # true pixel values\n",
    "p = torch.tensor([0.9, 0.1, 0.8, 0.95])          # decoder's predicted probabilities\n",
    "\n",
    "print(\"Reconstruction Loss (Binary Cross-Entropy) -- Per Pixel\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compute BCE for each pixel: -[x_i * log(p_i) + (1 - x_i) * log(1 - p_i)]\n",
    "for i in range(len(x)):\n",
    "    bce_i = -(x[i] * torch.log(p[i]) + (1 - x[i]) * torch.log(1 - p[i]))\n",
    "    print(f\"Pixel {i+1}: x={x[i]:.0f}, p={p[i]:.2f}\")\n",
    "    print(f\"  BCE = -[{x[i]:.0f} * log({p[i]:.2f}) + {1-x[i]:.0f} * log({1-p[i]:.2f})]\")\n",
    "    print(f\"      = -[{x[i] * torch.log(p[i]):.4f} + {(1-x[i]) * torch.log(1-p[i]):.4f}]\")\n",
    "    print(f\"      = {bce_i:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Total reconstruction loss\n",
    "total_bce = F.binary_cross_entropy(p, x, reduction='sum')\n",
    "print(f\"Total reconstruction loss = {total_bce:.4f}\")\n",
    "print()\n",
    "print(\"Interpretation: a loss of 0.484 means the decoder is doing a GOOD job.\")\n",
    "print(\"Perfect predictions (p = x exactly) would give a loss of 0.\")\n",
    "print(\"Random predictions (p = 0.5 for all) would give a much higher loss.\")\n",
    "\n",
    "# Verify: what would random predictions give?\n",
    "p_random = torch.tensor([0.5, 0.5, 0.5, 0.5])\n",
    "random_bce = F.binary_cross_entropy(p_random, x, reduction='sum')\n",
    "print(f\"\\nWith random predictions: total loss = {random_bce:.4f}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 KL Divergence -- Closed-Form Solution\n",
    "\n",
    "The KL divergence measures how different the encoder's distribution $q_\\phi(z|x) = \\mathcal{N}(\\mu, \\sigma^2)$ is from the standard normal prior $p(z) = \\mathcal{N}(0, 1)$.\n",
    "\n",
    "The beautiful thing is that for two Gaussians, KL divergence has a **closed-form** solution -- no sampling needed:\n",
    "\n",
    "$$D_{KL} = -\\frac{1}{2}\\left(1 + \\log(\\sigma^2) - \\mu^2 - \\sigma^2\\right)$$\n",
    "\n",
    "In plain English: \"This formula measures the 'distance' between the encoder's Gaussian and the standard normal. It is zero only when $\\mu = 0$ and $\\sigma = 1$ -- meaning the encoder perfectly matches the prior.\"\n",
    "\n",
    "For a latent space of dimension $J$, we sum over all dimensions:\n",
    "\n",
    "$$D_{KL} = -\\frac{1}{2} \\sum_{j=1}^{J}\\left(1 + \\log(\\sigma_j^2) - \\mu_j^2 - \\sigma_j^2\\right)$$\n",
    "\n",
    "Let us compute two concrete examples."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# --- KL Divergence: Closed-form computation ---\n",
    "\n",
    "def kl_divergence_manual(mu, sigma):\n",
    "    \"\"\"Compute KL divergence between N(mu, sigma^2) and N(0, 1) for a single dimension.\"\"\"\n",
    "    return -0.5 * (1 + torch.log(sigma**2) - mu**2 - sigma**2)\n",
    "\n",
    "\n",
    "print(\"KL Divergence -- Closed Form Examples\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Example 1: mu = 0.5, sigma = 1.2  (slightly off from standard normal)\n",
    "mu1, sigma1 = torch.tensor(0.5), torch.tensor(1.2)\n",
    "kl1 = kl_divergence_manual(mu1, sigma1)\n",
    "print(f\"\\nExample 1: mu = {mu1:.1f}, sigma = {sigma1:.1f}\")\n",
    "print(f\"  D_KL = -0.5 * (1 + log({sigma1**2:.2f}) - {mu1**2:.2f} - {sigma1**2:.2f})\")\n",
    "print(f\"       = -0.5 * (1 + {torch.log(sigma1**2):.4f} - {mu1**2:.4f} - {sigma1**2:.4f})\")\n",
    "inner = 1 + torch.log(sigma1**2) - mu1**2 - sigma1**2\n",
    "print(f\"       = -0.5 * ({inner:.4f})\")\n",
    "print(f\"       = {kl1:.4f}\")\n",
    "print(f\"  This means the encoder's distribution is SLIGHTLY off from the standard normal.\")\n",
    "\n",
    "# Example 2: mu = 0, sigma = 1  (perfect standard normal)\n",
    "mu2, sigma2 = torch.tensor(0.0), torch.tensor(1.0)\n",
    "kl2 = kl_divergence_manual(mu2, sigma2)\n",
    "print(f\"\\nExample 2: mu = {mu2:.1f}, sigma = {sigma2:.1f}\")\n",
    "print(f\"  D_KL = -0.5 * (1 + log({sigma2**2:.2f}) - {mu2**2:.2f} - {sigma2**2:.2f})\")\n",
    "print(f\"       = -0.5 * (1 + {torch.log(sigma2**2):.4f} - {mu2**2:.4f} - {sigma2**2:.4f})\")\n",
    "inner2 = 1 + torch.log(sigma2**2) - mu2**2 - sigma2**2\n",
    "print(f\"       = -0.5 * ({inner2:.4f})\")\n",
    "print(f\"       = {kl2:.4f}\")\n",
    "print(f\"  Perfect match! KL = 0 when the encoder IS the standard normal.\")\n",
    "\n",
    "# Example 3: mu = 3.0, sigma = 0.1  (far away, very narrow -- bad!)\n",
    "mu3, sigma3 = torch.tensor(3.0), torch.tensor(0.1)\n",
    "kl3 = kl_divergence_manual(mu3, sigma3)\n",
    "print(f\"\\nExample 3: mu = {mu3:.1f}, sigma = {sigma3:.1f}\")\n",
    "print(f\"  D_KL = {kl3:.4f}\")\n",
    "print(f\"  Very high! The encoder is far from standard normal -- strong penalty.\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualize how the KL divergence changes as we vary $\\mu$ and $\\sigma$. This gives us an intuitive map of the \"penalty landscape.\""
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# --- Visualization 1: KL Divergence as a function of mu and sigma ---\n",
    "\n",
    "mu_range = np.linspace(-3, 3, 100)\n",
    "sigma_range = np.linspace(0.1, 3.0, 100)\n",
    "MU, SIGMA = np.meshgrid(mu_range, sigma_range)\n",
    "\n",
    "# KL divergence: -0.5 * (1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "KL = -0.5 * (1 + np.log(SIGMA**2) - MU**2 - SIGMA**2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Heatmap\n",
    "im = axes[0].pcolormesh(MU, SIGMA, KL, cmap='YlOrRd', shading='auto', vmin=0, vmax=5)\n",
    "axes[0].set_xlabel('$\\\\mu$ (mean)', fontsize=12)\n",
    "axes[0].set_ylabel('$\\\\sigma$ (std dev)', fontsize=12)\n",
    "axes[0].set_title('KL Divergence Heatmap', fontsize=14, fontweight='bold')\n",
    "axes[0].plot(0, 1, 'k*', markersize=15, label='$\\\\mu$=0, $\\\\sigma$=1 (KL=0)')\n",
    "axes[0].legend(fontsize=11)\n",
    "plt.colorbar(im, ax=axes[0], label='$D_{KL}$')\n",
    "\n",
    "# Cross-sections\n",
    "sigmas_to_plot = [0.5, 1.0, 1.5, 2.0]\n",
    "colors = ['#E91E63', '#2196F3', '#4CAF50', '#FF9800']\n",
    "for sigma_val, color in zip(sigmas_to_plot, colors):\n",
    "    kl_vals = -0.5 * (1 + np.log(sigma_val**2) - mu_range**2 - sigma_val**2)\n",
    "    axes[1].plot(mu_range, kl_vals, color=color, linewidth=2, label=f'$\\\\sigma$={sigma_val}')\n",
    "\n",
    "axes[1].set_xlabel('$\\\\mu$ (mean)', fontsize=12)\n",
    "axes[1].set_ylabel('$D_{KL}$', fontsize=12)\n",
    "axes[1].set_title('KL Divergence vs $\\\\mu$ (for fixed $\\\\sigma$)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].set_ylim(0, 5)\n",
    "axes[1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observations:\")\n",
    "print(\"1. KL = 0 only at mu=0, sigma=1 (the star) -- perfect match to prior\")\n",
    "print(\"2. Moving mu away from 0 increases KL (mean penalty)\")\n",
    "print(\"3. Making sigma too small or too large increases KL (spread penalty)\")\n",
    "print(\"4. The minimum for each sigma curve is always at mu=0\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The Reparameterization Trick\n",
    "\n",
    "We have the ELBO and we know how to compute both terms. But there is one more technical hurdle before we can train.\n",
    "\n",
    "**The Problem:** During the forward pass, we need to *sample* $z$ from the encoder's distribution $\\mathcal{N}(\\mu, \\sigma^2)$. But sampling is a random operation, and **random operations are not differentiable**. If gradients cannot flow through the sampling step, backpropagation breaks down and we cannot train the encoder.\n",
    "\n",
    "**The Solution:** Instead of sampling $z$ directly, we reparameterize it as a *deterministic* function of $\\mu$, $\\sigma$, and a random noise variable $\\epsilon$:\n",
    "\n",
    "$$z = \\mu + \\sigma \\odot \\epsilon, \\quad \\text{where } \\epsilon \\sim \\mathcal{N}(0, I)$$\n",
    "\n",
    "In plain English: \"We move the randomness into $\\epsilon$, which does not depend on any learnable parameters. The quantities $\\mu$ and $\\sigma$ are deterministic outputs of the encoder, so gradients flow through them freely.\"\n",
    "\n",
    "**Numerical example:**\n",
    "\n",
    "Suppose $\\mu = [2.1, -0.5]$, $\\sigma = [0.3, 0.2]$, and we sample $\\epsilon = [0.7, -1.1]$.\n",
    "\n",
    "$$z = [2.1 + 0.3 \\times 0.7, \\; -0.5 + 0.2 \\times (-1.1)]$$\n",
    "$$z = [2.1 + 0.21, \\; -0.5 + (-0.22)]$$\n",
    "$$z = [2.31, -0.72]$$\n",
    "\n",
    "The gradient $\\frac{\\partial z}{\\partial \\mu} = 1$ and $\\frac{\\partial z}{\\partial \\sigma} = \\epsilon$, so gradients are well-defined."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# --- Reparameterization Trick: Step by step ---\n",
    "\n",
    "mu = torch.tensor([2.1, -0.5], requires_grad=True)\n",
    "sigma = torch.tensor([0.3, 0.2], requires_grad=True)\n",
    "\n",
    "# The \"wrong\" way: sample directly (cannot backprop)\n",
    "print(\"=\" * 60)\n",
    "print(\"THE REPARAMETERIZATION TRICK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# The \"right\" way: reparameterize\n",
    "epsilon = torch.tensor([0.7, -1.1])  # fixed for demonstration\n",
    "z = mu + sigma * epsilon\n",
    "\n",
    "print(f\"\\nInputs:\")\n",
    "print(f\"  mu    = {mu.data.tolist()}\")\n",
    "print(f\"  sigma = {sigma.data.tolist()}\")\n",
    "print(f\"  epsilon (sampled from N(0,I)) = {epsilon.tolist()}\")\n",
    "print(f\"\\nComputation: z = mu + sigma * epsilon\")\n",
    "print(f\"  z[0] = {mu[0].item():.1f} + {sigma[0].item():.1f} * {epsilon[0].item():.1f} = {z[0].item():.2f}\")\n",
    "print(f\"  z[1] = {mu[1].item():.1f} + {sigma[1].item():.1f} * {epsilon[1].item():.1f} = {z[1].item():.2f}\")\n",
    "print(f\"  z    = [{z[0].item():.2f}, {z[1].item():.2f}]\")\n",
    "\n",
    "# Now show that gradients flow through mu and sigma\n",
    "loss = z.sum()  # dummy loss to trigger backward\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nGradient check (this is the key!):\")\n",
    "print(f\"  dz/d(mu)    = {mu.grad.tolist()}  (always 1 -- gradient flows!)\")\n",
    "print(f\"  dz/d(sigma) = {sigma.grad.tolist()}  (= epsilon -- gradient flows!)\")\n",
    "print(f\"  dz/d(epsilon) = not tracked (epsilon has no grad)\")\n",
    "print(f\"\\nThe randomness is in epsilon, which is NOT a learnable parameter.\")\n",
    "print(f\"Gradients flow through mu and sigma, so the encoder can be trained.\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now visualize the reparameterization trick more concretely by showing that gradients *do* flow through $\\mu$ and $\\sigma$ but *do not* flow through $\\epsilon$."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# --- Visualization 2: Gradient flow in reparameterization ---\n",
    "\n",
    "# Run the reparameterization trick multiple times and collect gradients\n",
    "n_samples = 200\n",
    "mu = torch.tensor([1.5], requires_grad=True)\n",
    "sigma = torch.tensor([0.8], requires_grad=True)\n",
    "\n",
    "z_values = []\n",
    "grad_mu_values = []\n",
    "grad_sigma_values = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Fresh computation graph each time\n",
    "    mu_val = mu.detach().requires_grad_(True)\n",
    "    sigma_val = sigma.detach().requires_grad_(True)\n",
    "\n",
    "    eps = torch.randn(1)\n",
    "    z = mu_val + sigma_val * eps\n",
    "\n",
    "    # Compute gradient of z w.r.t. mu and sigma\n",
    "    z.backward()\n",
    "\n",
    "    z_values.append(z.item())\n",
    "    grad_mu_values.append(mu_val.grad.item())\n",
    "    grad_sigma_values.append(sigma_val.grad.item())\n",
    "\n",
    "z_values = np.array(z_values)\n",
    "grad_mu_values = np.array(grad_mu_values)\n",
    "grad_sigma_values = np.array(grad_sigma_values)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4.5))\n",
    "\n",
    "# Panel 1: Distribution of z samples\n",
    "axes[0].hist(z_values, bins=30, color='#7E57C2', alpha=0.7, edgecolor='white')\n",
    "axes[0].axvline(x=1.5, color='red', linewidth=2, linestyle='--', label=f'$\\\\mu$ = 1.5')\n",
    "axes[0].set_xlabel('z value', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Samples from $z = \\\\mu + \\\\sigma\\\\epsilon$', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "\n",
    "# Panel 2: dz/d(mu) is always 1\n",
    "axes[1].scatter(range(n_samples), grad_mu_values, s=5, color='#2196F3', alpha=0.7)\n",
    "axes[1].axhline(y=1.0, color='red', linewidth=2, linestyle='--', label='Expected: 1.0')\n",
    "axes[1].set_xlabel('Sample index', fontsize=12)\n",
    "axes[1].set_ylabel('$\\\\partial z / \\\\partial \\\\mu$', fontsize=12)\n",
    "axes[1].set_title('Gradient w.r.t. $\\\\mu$ (always 1)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylim(0.5, 1.5)\n",
    "axes[1].legend(fontsize=11)\n",
    "\n",
    "# Panel 3: dz/d(sigma) = epsilon (varies)\n",
    "axes[2].scatter(range(n_samples), grad_sigma_values, s=5, color='#FF5722', alpha=0.7)\n",
    "axes[2].axhline(y=0.0, color='gray', linewidth=1, linestyle='--')\n",
    "axes[2].set_xlabel('Sample index', fontsize=12)\n",
    "axes[2].set_ylabel('$\\\\partial z / \\\\partial \\\\sigma$', fontsize=12)\n",
    "axes[2].set_title('Gradient w.r.t. $\\\\sigma$ (= $\\\\epsilon$, varies)', fontsize=13, fontweight='bold')\n",
    "axes[2].legend(['$\\\\epsilon \\\\sim \\\\mathcal{N}(0,1)$'], fontsize=11)\n",
    "\n",
    "plt.suptitle('Reparameterization Trick: Gradients Flow Through $\\\\mu$ and $\\\\sigma$',\n",
    "             fontsize=14, fontweight='bold', y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key takeaway:\")\n",
    "print(\"  dz/d(mu) = 1.0 always -- clean, constant gradient signal\")\n",
    "print(\"  dz/d(sigma) = epsilon -- noisy but well-defined gradient signal\")\n",
    "print(\"  Both are differentiable, so backpropagation works!\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The ELBO Balance -- Reconstruction vs KL Tradeoff\n",
    "\n",
    "One of the most important insights about the ELBO is the *tension* between its two terms. Let us visualize this tradeoff directly."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# --- Visualization 3: ELBO balance as sigma varies ---\n",
    "# For a fixed mu, show how reconstruction and KL trade off as sigma changes\n",
    "\n",
    "mu_fixed = torch.tensor([1.0])\n",
    "sigma_range = torch.linspace(0.01, 3.0, 200)\n",
    "\n",
    "# Simulated reconstruction loss: as sigma grows, z samples are more spread out,\n",
    "# so the decoder gets noisier inputs and reconstruction worsens.\n",
    "# We model this as: recon_loss ~ base + scale * sigma^2\n",
    "recon_base = 80.0\n",
    "recon_scale = 30.0\n",
    "recon_losses = recon_base + recon_scale * sigma_range**2\n",
    "\n",
    "# KL divergence: closed form\n",
    "kl_losses = -0.5 * (1 + torch.log(sigma_range**2) - mu_fixed**2 - sigma_range**2)\n",
    "\n",
    "# Total ELBO loss (negated ELBO = recon_loss + KL)\n",
    "total_losses = recon_losses + kl_losses\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(sigma_range.numpy(), recon_losses.numpy(), color='#2196F3', linewidth=2.5, label='Reconstruction Loss')\n",
    "ax.plot(sigma_range.numpy(), kl_losses.numpy(), color='#FF5722', linewidth=2.5, label='KL Divergence')\n",
    "ax.plot(sigma_range.numpy(), total_losses.numpy(), color='#4CAF50', linewidth=2.5, linestyle='--', label='Total Loss (Recon + KL)')\n",
    "\n",
    "# Mark the optimal sigma (minimum total loss)\n",
    "min_idx = torch.argmin(total_losses)\n",
    "opt_sigma = sigma_range[min_idx].item()\n",
    "opt_loss = total_losses[min_idx].item()\n",
    "ax.plot(opt_sigma, opt_loss, 'k*', markersize=15, zorder=5)\n",
    "ax.annotate(f'Optimal $\\\\sigma$ = {opt_sigma:.2f}', xy=(opt_sigma, opt_loss),\n",
    "            xytext=(opt_sigma + 0.5, opt_loss + 20), fontsize=12,\n",
    "            arrowprops=dict(arrowstyle='->', color='black'))\n",
    "\n",
    "ax.set_xlabel('$\\\\sigma$ (encoder standard deviation)', fontsize=13)\n",
    "ax.set_ylabel('Loss', fontsize=13)\n",
    "ax.set_title(f'The ELBO Balancing Act (fixed $\\\\mu$ = {mu_fixed.item():.1f})', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.set_ylim(0, 200)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"At the optimal sigma = {opt_sigma:.2f}:\")\n",
    "print(f\"  Reconstruction loss = {recon_losses[min_idx].item():.2f}\")\n",
    "print(f\"  KL divergence       = {kl_losses[min_idx].item():.2f}\")\n",
    "print(f\"  Total ELBO loss     = {opt_loss:.2f}\")\n",
    "print()\n",
    "print(\"If sigma is too small: KL is high (far from N(0,1)), but reconstruction is good.\")\n",
    "print(\"If sigma is too large: KL might decrease, but reconstruction suffers.\")\n",
    "print(\"The ELBO finds the sweet spot that balances both pressures.\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn -- TODO Exercises\n",
    "\n",
    "Now it is your turn to implement these components from scratch. We provide scaffolding and verification cells to check your work.\n",
    "\n",
    "### TODO 1: Implement KL Divergence from Scratch\n",
    "\n",
    "Given the closed-form formula, compute the KL divergence for a batch of encoder outputs. The formula for a single latent dimension is:\n",
    "\n",
    "$$D_{KL} = -\\frac{1}{2}\\left(1 + \\log(\\sigma^2) - \\mu^2 - \\sigma^2\\right)$$\n",
    "\n",
    "For a latent space of dimension $J$ and a batch of $N$ samples, sum over all dimensions and all samples.\n",
    "\n",
    "**Hint:** In practice, neural networks output $\\log(\\sigma^2)$ (log-variance) instead of $\\sigma$ directly. So the formula becomes:\n",
    "\n",
    "$$D_{KL} = -\\frac{1}{2}\\left(1 + \\log\\sigma^2 - \\mu^2 - e^{\\log\\sigma^2}\\right)$$"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def kl_divergence_todo(mu, logvar):\n",
    "    \"\"\"\n",
    "    Compute the KL divergence between q(z|x) = N(mu, exp(logvar)) and p(z) = N(0, I).\n",
    "\n",
    "    Args:\n",
    "        mu: Tensor of shape (batch_size, latent_dim) -- encoder means\n",
    "        logvar: Tensor of shape (batch_size, latent_dim) -- encoder log-variances\n",
    "\n",
    "    Returns:\n",
    "        Scalar tensor: total KL divergence summed over batch and dimensions\n",
    "    \"\"\"\n",
    "    # ============================================================\n",
    "    # TODO: Implement the KL divergence formula here.\n",
    "    #\n",
    "    # The closed-form KL for one dimension is:\n",
    "    #   -0.5 * (1 + logvar - mu^2 - exp(logvar))\n",
    "    #\n",
    "    # Sum over all latent dimensions AND all batch samples.\n",
    "    #\n",
    "    # Replace the line below with your implementation:\n",
    "    # ============================================================\n",
    "    kl = None  # YOUR CODE HERE\n",
    "\n",
    "    return kl\n",
    "\n",
    "\n",
    "# --- Test data ---\n",
    "# Batch of 4 samples, each with 3 latent dimensions\n",
    "mu_test = torch.tensor([\n",
    "    [0.0, 0.0, 0.0],     # perfect standard normal\n",
    "    [0.5, -0.3, 0.1],    # slightly off\n",
    "    [2.0, 1.5, -1.0],    # far from standard normal\n",
    "    [0.0, 0.0, 0.0],     # perfect again, but with sigma != 1\n",
    "])\n",
    "logvar_test = torch.tensor([\n",
    "    [0.0, 0.0, 0.0],           # sigma = 1 (log(1) = 0)\n",
    "    [0.3646, 0.1823, -0.2231], # sigma ~ [1.2, 1.1, 0.9]\n",
    "    [0.0, 0.0, 0.0],           # sigma = 1 but mu is far\n",
    "    [-2.0, -1.0, 1.0],         # sigma = [0.37, 0.61, 1.65]\n",
    "])\n",
    "\n",
    "kl_result = kl_divergence_todo(mu_test, logvar_test)\n",
    "print(f\"Your KL divergence result: {kl_result}\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification Cell for TODO 1\n",
    "\n",
    "Run this cell to check your implementation. If you see all green, you got it right!"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verification for TODO 1 ---\n",
    "\n",
    "# Reference implementation\n",
    "def kl_divergence_reference(mu, logvar):\n",
    "    return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "expected = kl_divergence_reference(mu_test, logvar_test)\n",
    "\n",
    "print(\"Verification for TODO 1: KL Divergence\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if kl_result is None:\n",
    "    print(\"You have not implemented kl_divergence_todo yet!\")\n",
    "    print(f\"Expected result: {expected.item():.4f}\")\n",
    "else:\n",
    "    try:\n",
    "        assert torch.isclose(kl_result, expected, atol=1e-3), \\\n",
    "            f\"Mismatch! Got {kl_result.item():.4f}, expected {expected.item():.4f}\"\n",
    "        print(f\"Your result:     {kl_result.item():.4f}\")\n",
    "        print(f\"Expected result: {expected.item():.4f}\")\n",
    "        print(\"PASSED! Your KL divergence implementation is correct.\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"FAILED: {e}\")\n",
    "\n",
    "# Break down by sample to help debug\n",
    "print(\"\\nBreakdown by sample:\")\n",
    "for i in range(mu_test.shape[0]):\n",
    "    kl_i = -0.5 * torch.sum(1 + logvar_test[i] - mu_test[i].pow(2) - logvar_test[i].exp())\n",
    "    print(f\"  Sample {i+1}: mu={mu_test[i].tolist()}, logvar={logvar_test[i].tolist()}\")\n",
    "    print(f\"    KL = {kl_i.item():.4f}\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement the Reparameterization Trick and Verify Gradients\n",
    "\n",
    "Implement the reparameterization trick: given $\\mu$ and $\\log(\\sigma^2)$, sample $z$ such that gradients flow back to $\\mu$ and $\\sigma$.\n",
    "\n",
    "Remember: $z = \\mu + \\sigma \\odot \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, I)$ and $\\sigma = \\exp(0.5 \\cdot \\log\\sigma^2)$."
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def reparameterize_todo(mu, logvar):\n",
    "    \"\"\"\n",
    "    Sample z from q(z|x) = N(mu, exp(logvar)) using the reparameterization trick.\n",
    "\n",
    "    Args:\n",
    "        mu: Tensor of shape (batch_size, latent_dim) -- encoder means\n",
    "        logvar: Tensor of shape (batch_size, latent_dim) -- encoder log-variances\n",
    "\n",
    "    Returns:\n",
    "        z: Tensor of shape (batch_size, latent_dim) -- sampled latent codes\n",
    "    \"\"\"\n",
    "    # ============================================================\n",
    "    # TODO: Implement the reparameterization trick.\n",
    "    #\n",
    "    # Step 1: Compute std = exp(0.5 * logvar)\n",
    "    # Step 2: Sample epsilon from N(0, I) with the same shape as std\n",
    "    #         Use torch.randn_like(std) for this.\n",
    "    # Step 3: Compute z = mu + std * epsilon\n",
    "    #\n",
    "    # Replace the line below with your implementation:\n",
    "    # ============================================================\n",
    "    z = None  # YOUR CODE HERE\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "# --- Test ---\n",
    "mu_test2 = torch.tensor([[2.1, -0.5]], requires_grad=True)\n",
    "logvar_test2 = torch.tensor([[2 * torch.log(torch.tensor(0.3)).item(),\n",
    "                               2 * torch.log(torch.tensor(0.2)).item()]])\n",
    "\n",
    "z_sample = reparameterize_todo(mu_test2, logvar_test2)\n",
    "print(f\"Sampled z: {z_sample}\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification Cell for TODO 2\n",
    "\n",
    "Run this cell to verify that gradients flow correctly through your reparameterization."
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verification for TODO 2 ---\n",
    "\n",
    "print(\"Verification for TODO 2: Reparameterization Trick\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "if z_sample is None:\n",
    "    print(\"You have not implemented reparameterize_todo yet!\")\n",
    "else:\n",
    "    # Check 1: Output shape is correct\n",
    "    assert z_sample.shape == mu_test2.shape, \\\n",
    "        f\"Shape mismatch: got {z_sample.shape}, expected {mu_test2.shape}\"\n",
    "    print(\"Shape check: PASSED\")\n",
    "\n",
    "    # Check 2: Gradients flow through mu\n",
    "    try:\n",
    "        dummy_loss = z_sample.sum()\n",
    "        dummy_loss.backward()\n",
    "        assert mu_test2.grad is not None, \"No gradient for mu!\"\n",
    "        assert torch.all(mu_test2.grad == 1.0), \\\n",
    "            f\"Expected dz/dmu = 1, got {mu_test2.grad}\"\n",
    "        print(f\"Gradient check (dz/dmu = {mu_test2.grad.tolist()}): PASSED\")\n",
    "    except Exception as e:\n",
    "        print(f\"Gradient check FAILED: {e}\")\n",
    "\n",
    "    # Check 3: z is different from mu (sampling adds noise)\n",
    "    is_different = not torch.allclose(z_sample.detach(), mu_test2.detach())\n",
    "    print(f\"Stochasticity check (z != mu): {'PASSED' if is_different else 'WARNING: z equals mu exactly'}\")\n",
    "\n",
    "    # Check 4: Multiple samples produce different results\n",
    "    torch.manual_seed(0)\n",
    "    z1 = mu_test2.detach() + torch.exp(0.5 * logvar_test2) * torch.randn_like(mu_test2)\n",
    "    torch.manual_seed(1)\n",
    "    z2 = mu_test2.detach() + torch.exp(0.5 * logvar_test2) * torch.randn_like(mu_test2)\n",
    "    print(f\"Different seeds give different z: {'PASSED' if not torch.allclose(z1, z2) else 'FAILED'}\")\n",
    "\n",
    "    print(\"\\nAll checks passed! Your reparameterization trick works correctly.\")"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together -- The Full VAE Loss Function\n",
    "\n",
    "Now let us combine all three components into a single, clean VAE loss function. This is the complete training objective."
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def vae_loss(x_recon, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    Complete VAE loss function = Reconstruction Loss + KL Divergence.\n",
    "\n",
    "    Args:\n",
    "        x_recon: Decoder output probabilities, shape (batch_size, D)\n",
    "        x: Original input, shape (batch_size, D)\n",
    "        mu: Encoder means, shape (batch_size, latent_dim)\n",
    "        logvar: Encoder log-variances, shape (batch_size, latent_dim)\n",
    "\n",
    "    Returns:\n",
    "        total_loss: Scalar tensor\n",
    "        recon_loss: Reconstruction component (for logging)\n",
    "        kl_loss: KL divergence component (for logging)\n",
    "    \"\"\"\n",
    "    # Term 1: Reconstruction loss (binary cross-entropy, summed over pixels)\n",
    "    recon_loss = F.binary_cross_entropy(x_recon, x, reduction='sum')\n",
    "\n",
    "    # Term 2: KL divergence (closed-form for Gaussian)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    # Total ELBO loss (negated ELBO -- we minimize this)\n",
    "    total_loss = recon_loss + kl_loss\n",
    "\n",
    "    return total_loss, recon_loss, kl_loss\n",
    "\n",
    "\n",
    "# --- Demo: compute the loss for a synthetic mini-batch ---\n",
    "batch_size = 4\n",
    "D = 16  # 4x4 pixel images\n",
    "latent_dim = 2\n",
    "\n",
    "# Simulate encoder outputs\n",
    "mu = torch.randn(batch_size, latent_dim)\n",
    "logvar = torch.randn(batch_size, latent_dim)\n",
    "\n",
    "# Simulate original images (binary)\n",
    "x = torch.bernoulli(torch.ones(batch_size, D) * 0.5)\n",
    "\n",
    "# Simulate decoder predictions (probabilities between 0 and 1)\n",
    "x_recon = torch.sigmoid(torch.randn(batch_size, D))\n",
    "\n",
    "# Compute loss\n",
    "total, recon, kl = vae_loss(x_recon, x, mu, logvar)\n",
    "\n",
    "print(\"Full VAE Loss Computation\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Image dimension: {D} pixels\")\n",
    "print(f\"Latent dimension: {latent_dim}\")\n",
    "print()\n",
    "print(f\"Reconstruction loss: {recon.item():.4f}\")\n",
    "print(f\"KL divergence:       {kl.item():.4f}\")\n",
    "print(f\"Total ELBO loss:     {total.item():.4f}\")\n",
    "print()\n",
    "print(f\"Reconstruction share: {100 * recon.item() / total.item():.1f}%\")\n",
    "print(f\"KL share:             {100 * kl.item() / total.item():.1f}%\")\n",
    "print()\n",
    "print(\"In early training, reconstruction dominates (the decoder is learning).\")\n",
    "print(\"As training progresses, KL grows (the encoder learns structure).\")"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Exploration -- The Beta-VAE\n",
    "\n",
    "In a standard VAE, both loss terms are weighted equally. But what if we could control the balance? The **beta-VAE** introduces a weighting factor $\\beta$:\n",
    "\n",
    "$$\\mathcal{L}_{\\beta\\text{-VAE}} = \\mathcal{L}_{\\text{recon}} + \\beta \\cdot D_{KL}$$\n",
    "\n",
    "- $\\beta = 1$: Standard VAE\n",
    "- $\\beta < 1$: Emphasize reconstruction (sharper images, less organized latent space)\n",
    "- $\\beta > 1$: Emphasize regularization (blurrier images, more disentangled latent space)\n",
    "\n",
    "Let us explore how different values of $\\beta$ change the loss landscape."
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# --- Visualization 4: Beta-VAE loss landscape ---\n",
    "\n",
    "# We will simulate how the total loss changes as a function of mu\n",
    "# for different beta values, with sigma fixed at 1.0\n",
    "\n",
    "mu_range = np.linspace(-4, 4, 300)\n",
    "sigma_fixed = 1.0\n",
    "\n",
    "# Base reconstruction loss (quadratic in mu for simplicity -- moving away from data)\n",
    "recon_loss_base = 50.0  # constant base\n",
    "recon_loss_mu = 5.0 * mu_range**2  # worse reconstruction as mu moves away\n",
    "\n",
    "# KL divergence for fixed sigma=1.0: -0.5 * (1 + 0 - mu^2 - 1) = 0.5 * mu^2\n",
    "kl_base = 0.5 * mu_range**2\n",
    "\n",
    "betas = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "colors = ['#E91E63', '#FF9800', '#4CAF50', '#2196F3', '#7E57C2']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Panel 1: Total loss for different betas\n",
    "for beta, color in zip(betas, colors):\n",
    "    total = recon_loss_base + recon_loss_mu + beta * kl_base\n",
    "    axes[0].plot(mu_range, total, color=color, linewidth=2, label=f'$\\\\beta$ = {beta}')\n",
    "\n",
    "axes[0].set_xlabel('$\\\\mu$ (encoder mean)', fontsize=12)\n",
    "axes[0].set_ylabel('Total Loss', fontsize=12)\n",
    "axes[0].set_title('Total Loss for Different $\\\\beta$ Values', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].set_ylim(0, 250)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: Component breakdown for beta = 0.1, 1.0, 5.0\n",
    "beta_showcase = [0.1, 1.0, 5.0]\n",
    "linestyles = ['--', '-', ':']\n",
    "for beta, ls in zip(beta_showcase, linestyles):\n",
    "    recon = recon_loss_base + recon_loss_mu\n",
    "    kl = beta * kl_base\n",
    "    axes[1].plot(mu_range, recon, color='#2196F3', linewidth=1.5, linestyle=ls, alpha=0.6)\n",
    "    axes[1].plot(mu_range, kl, color='#FF5722', linewidth=1.5, linestyle=ls, alpha=0.6)\n",
    "\n",
    "# Add labels\n",
    "axes[1].plot([], [], color='#2196F3', linewidth=2, label='Reconstruction (same for all $\\\\beta$)')\n",
    "axes[1].plot([], [], color='#FF5722', linewidth=2, linestyle='--', label='$\\\\beta$ KL ($\\\\beta$=0.1)')\n",
    "axes[1].plot([], [], color='#FF5722', linewidth=2, linestyle='-', label='$\\\\beta$ KL ($\\\\beta$=1.0)')\n",
    "axes[1].plot([], [], color='#FF5722', linewidth=2, linestyle=':', label='$\\\\beta$ KL ($\\\\beta$=5.0)')\n",
    "axes[1].set_xlabel('$\\\\mu$ (encoder mean)', fontsize=12)\n",
    "axes[1].set_ylabel('Loss Component', fontsize=12)\n",
    "axes[1].set_title('Reconstruction vs $\\\\beta$-scaled KL', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].set_ylim(0, 150)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Beta-VAE: Controlling the Reconstruction-Regularization Tradeoff',\n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"What beta controls:\")\n",
    "print(\"  beta < 1: The model prioritizes reconstruction. Sharper images, but\")\n",
    "print(\"            the latent space may have dead zones.\")\n",
    "print(\"  beta = 1: Standard VAE. Equal weight to both terms.\")\n",
    "print(\"  beta > 1: The model prioritizes a well-organized latent space.\")\n",
    "print(\"            Images may be blurrier, but latent factors are more disentangled.\")"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make this even more interactive. We will compute the actual loss components for a range of $\\beta$ values and show how the balance shifts."
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# --- Interactive beta sweep ---\n",
    "\n",
    "# Simulate a \"trained\" encoder output for a batch of images\n",
    "batch_size = 100\n",
    "latent_dim = 2\n",
    "\n",
    "# Encoder outputs: means clustered around different digit-like regions\n",
    "mu_batch = torch.randn(batch_size, latent_dim) * 1.5\n",
    "logvar_batch = torch.randn(batch_size, latent_dim) * 0.3 - 0.5  # sigma around 0.7-0.9\n",
    "\n",
    "# Simulated reconstruction quality\n",
    "x_batch = torch.bernoulli(torch.ones(batch_size, 784) * 0.3)\n",
    "x_recon_batch = torch.sigmoid(torch.randn(batch_size, 784) * 0.5 +\n",
    "                               (x_batch - 0.5) * 3)  # correlated with true x\n",
    "\n",
    "# Compute per-sample losses\n",
    "recon_per_sample = torch.stack([\n",
    "    F.binary_cross_entropy(x_recon_batch[i], x_batch[i], reduction='sum')\n",
    "    for i in range(batch_size)\n",
    "])\n",
    "kl_per_sample = torch.stack([\n",
    "    -0.5 * torch.sum(1 + logvar_batch[i] - mu_batch[i].pow(2) - logvar_batch[i].exp())\n",
    "    for i in range(batch_size)\n",
    "])\n",
    "\n",
    "# Sweep beta\n",
    "betas = np.logspace(-1, 1, 50)  # 0.1 to 10\n",
    "mean_recon = recon_per_sample.mean().item()\n",
    "mean_kl = kl_per_sample.mean().item()\n",
    "total_losses = [mean_recon + b * mean_kl for b in betas]\n",
    "recon_fractions = [mean_recon / (mean_recon + b * mean_kl) * 100 for b in betas]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Panel 1: Total loss vs beta\n",
    "axes[0].semilogx(betas, total_losses, color='#4CAF50', linewidth=2.5)\n",
    "axes[0].axvline(x=1.0, color='gray', linestyle='--', alpha=0.5, label='$\\\\beta$=1 (standard VAE)')\n",
    "axes[0].set_xlabel('$\\\\beta$', fontsize=13)\n",
    "axes[0].set_ylabel('Mean Total Loss', fontsize=13)\n",
    "axes[0].set_title('Total Loss vs $\\\\beta$', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: Recon fraction vs beta\n",
    "axes[1].semilogx(betas, recon_fractions, color='#2196F3', linewidth=2.5, label='Recon %')\n",
    "axes[1].semilogx(betas, [100 - r for r in recon_fractions], color='#FF5722', linewidth=2.5, label='KL %')\n",
    "axes[1].axvline(x=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].axhline(y=50, color='gray', linestyle=':', alpha=0.3)\n",
    "axes[1].set_xlabel('$\\\\beta$', fontsize=13)\n",
    "axes[1].set_ylabel('Percentage of Total Loss', fontsize=13)\n",
    "axes[1].set_title('Loss Composition vs $\\\\beta$', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=12)\n",
    "axes[1].set_ylim(0, 100)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean reconstruction loss: {mean_recon:.2f}\")\n",
    "print(f\"Mean KL divergence:       {mean_kl:.2f}\")\n",
    "print(f\"\\nAt beta=0.1:  Recon is {recon_fractions[0]:.0f}% of total loss\")\n",
    "print(f\"At beta=1.0:  Recon is ~{mean_recon/(mean_recon + mean_kl)*100:.0f}% of total loss\")\n",
    "print(f\"At beta=10.0: Recon is {recon_fractions[-1]:.0f}% of total loss\")"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output -- Visualization Dashboard\n",
    "\n",
    "Let us bring everything together in a single visualization dashboard that shows all three components of the VAE loss and how they interact."
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 14))\n",
    "gs = gridspec.GridSpec(3, 3, hspace=0.4, wspace=0.35)\n",
    "\n",
    "# ================================================================\n",
    "# Panel 1 (top-left): Reconstruction Loss Surface\n",
    "# ================================================================\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "# BCE loss as a function of predicted probability for x=1 and x=0\n",
    "p_range = np.linspace(0.01, 0.99, 200)\n",
    "bce_x1 = -np.log(p_range)       # when true label = 1\n",
    "bce_x0 = -np.log(1 - p_range)   # when true label = 0\n",
    "\n",
    "ax1.plot(p_range, bce_x1, color='#2196F3', linewidth=2.5, label='True pixel = 1')\n",
    "ax1.plot(p_range, bce_x0, color='#FF5722', linewidth=2.5, label='True pixel = 0')\n",
    "ax1.set_xlabel('Predicted probability $p$', fontsize=11)\n",
    "ax1.set_ylabel('BCE Loss', fontsize=11)\n",
    "ax1.set_title('Reconstruction Loss\\n(Binary Cross-Entropy)', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.set_ylim(0, 5)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate key points\n",
    "ax1.annotate('Perfect\\nprediction', xy=(0.95, -np.log(0.95)), xytext=(0.7, 1.5),\n",
    "             fontsize=9, arrowprops=dict(arrowstyle='->', color='#2196F3'),\n",
    "             color='#2196F3')\n",
    "\n",
    "# ================================================================\n",
    "# Panel 2 (top-center): KL Divergence Heatmap\n",
    "# ================================================================\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "mu_grid = np.linspace(-3, 3, 100)\n",
    "sigma_grid = np.linspace(0.1, 3.0, 100)\n",
    "MU, SIGMA = np.meshgrid(mu_grid, sigma_grid)\n",
    "KL = -0.5 * (1 + np.log(SIGMA**2) - MU**2 - SIGMA**2)\n",
    "\n",
    "im = ax2.pcolormesh(MU, SIGMA, KL, cmap='magma_r', shading='auto', vmin=0, vmax=4)\n",
    "ax2.plot(0, 1, 'w*', markersize=12)\n",
    "ax2.set_xlabel('$\\\\mu$', fontsize=11)\n",
    "ax2.set_ylabel('$\\\\sigma$', fontsize=11)\n",
    "ax2.set_title('KL Divergence\\n$D_{KL}(\\\\mathcal{N}(\\\\mu,\\\\sigma^2) \\\\| \\\\mathcal{N}(0,1))$',\n",
    "              fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax2, label='$D_{KL}$', shrink=0.8)\n",
    "\n",
    "# ================================================================\n",
    "# Panel 3 (top-right): Reparameterization Gradient Flow\n",
    "# ================================================================\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "\n",
    "# Show multiple reparameterized samples from same distribution\n",
    "mu_val = 1.5\n",
    "sigma_val = 0.6\n",
    "n_eps = 500\n",
    "epsilons = np.random.randn(n_eps)\n",
    "z_samples = mu_val + sigma_val * epsilons\n",
    "\n",
    "ax3.hist(z_samples, bins=40, density=True, alpha=0.6, color='#7E57C2', edgecolor='white',\n",
    "         label=f'$z = \\\\mu + \\\\sigma\\\\epsilon$')\n",
    "\n",
    "# Overlay the theoretical Gaussian\n",
    "z_theory = np.linspace(mu_val - 3*sigma_val, mu_val + 3*sigma_val, 200)\n",
    "pdf = (1 / (sigma_val * np.sqrt(2*np.pi))) * np.exp(-0.5 * ((z_theory - mu_val)/sigma_val)**2)\n",
    "ax3.plot(z_theory, pdf, color='#FF5722', linewidth=2.5, label=f'$\\\\mathcal{{N}}({mu_val}, {sigma_val}^2)$')\n",
    "ax3.axvline(mu_val, color='red', linestyle='--', linewidth=1.5, label=f'$\\\\mu = {mu_val}$')\n",
    "\n",
    "# Show gradient arrows\n",
    "for offset in [-1.0, -0.3, 0.5, 1.2]:\n",
    "    z_pt = mu_val + sigma_val * offset\n",
    "    ax3.annotate('', xy=(z_pt, 0.05), xytext=(mu_val, 0.05),\n",
    "                arrowprops=dict(arrowstyle='->', color='#4CAF50', lw=1.5))\n",
    "\n",
    "ax3.set_xlabel('$z$', fontsize=11)\n",
    "ax3.set_ylabel('Density', fontsize=11)\n",
    "ax3.set_title('Reparameterization\\nGradient Flow', fontsize=12, fontweight='bold')\n",
    "ax3.legend(fontsize=9, loc='upper left')\n",
    "\n",
    "# ================================================================\n",
    "# Panel 4 (middle, spanning full width): ELBO Training Dynamics\n",
    "# ================================================================\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "\n",
    "epochs = np.arange(1, 101)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate realistic training curves\n",
    "recon_curve = 550 * np.exp(-0.04 * epochs) + 85 + np.random.normal(0, 5, len(epochs))\n",
    "kl_curve = 22 * (1 - np.exp(-0.06 * epochs)) + 3 + np.random.normal(0, 1, len(epochs))\n",
    "kl_curve = np.maximum(kl_curve, 0)\n",
    "total_curve = recon_curve + kl_curve\n",
    "\n",
    "ax4.plot(epochs, recon_curve, color='#2196F3', linewidth=2, alpha=0.8, label='Reconstruction Loss')\n",
    "ax4.plot(epochs, kl_curve, color='#FF5722', linewidth=2, alpha=0.8, label='KL Divergence')\n",
    "ax4.plot(epochs, total_curve, color='#4CAF50', linewidth=2.5, label='Total ELBO Loss')\n",
    "\n",
    "# Add phases\n",
    "ax4.axvspan(1, 15, alpha=0.08, color='blue')\n",
    "ax4.axvspan(15, 50, alpha=0.08, color='orange')\n",
    "ax4.axvspan(50, 100, alpha=0.08, color='green')\n",
    "ax4.text(7, max(total_curve) * 0.9, 'Phase 1:\\nDecoder\\nlearning', fontsize=9,\n",
    "         ha='center', color='#1565C0', fontweight='bold')\n",
    "ax4.text(32, max(total_curve) * 0.9, 'Phase 2:\\nEncoder\\norganizing', fontsize=9,\n",
    "         ha='center', color='#E65100', fontweight='bold')\n",
    "ax4.text(75, max(total_curve) * 0.9, 'Phase 3:\\nEquilibrium', fontsize=9,\n",
    "         ha='center', color='#2E7D32', fontweight='bold')\n",
    "\n",
    "ax4.set_xlabel('Epoch', fontsize=12)\n",
    "ax4.set_ylabel('Loss', fontsize=12)\n",
    "ax4.set_title('VAE Training Dynamics: How the ELBO Evolves', fontsize=14, fontweight='bold')\n",
    "ax4.legend(fontsize=11, loc='center right')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# ================================================================\n",
    "# Panel 5 (bottom-left): Reconstruction quality vs KL for different beta\n",
    "# ================================================================\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "\n",
    "betas_demo = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "# Simulated: as beta increases, recon gets worse but KL gets better\n",
    "recon_at_beta = [80, 90, 105, 130, 180, 250]\n",
    "kl_at_beta = [15, 8, 5, 3, 1.5, 0.5]\n",
    "colors_beta = plt.cm.coolwarm(np.linspace(0, 1, len(betas_demo)))\n",
    "\n",
    "for i, (b, r, k) in enumerate(zip(betas_demo, recon_at_beta, kl_at_beta)):\n",
    "    ax5.scatter(k, r, color=colors_beta[i], s=120, zorder=5, edgecolors='black', linewidth=1)\n",
    "    ax5.annotate(f'$\\\\beta$={b}', xy=(k, r), xytext=(k + 0.8, r + 5), fontsize=9)\n",
    "\n",
    "ax5.set_xlabel('KL Divergence', fontsize=11)\n",
    "ax5.set_ylabel('Reconstruction Loss', fontsize=11)\n",
    "ax5.set_title('Recon-KL Tradeoff\\n(Pareto frontier)', fontsize=12, fontweight='bold')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# ================================================================\n",
    "# Panel 6 (bottom-center): Encoder distributions at different KLs\n",
    "# ================================================================\n",
    "ax6 = fig.add_subplot(gs[2, 1])\n",
    "\n",
    "z_range = np.linspace(-5, 5, 300)\n",
    "\n",
    "# Standard normal prior\n",
    "prior = (1 / np.sqrt(2*np.pi)) * np.exp(-0.5 * z_range**2)\n",
    "ax6.fill_between(z_range, prior, alpha=0.15, color='gray')\n",
    "ax6.plot(z_range, prior, color='gray', linewidth=2, linestyle='--', label='Prior $p(z) = \\\\mathcal{N}(0,1)$')\n",
    "\n",
    "# Encoder distributions with different KL values\n",
    "configs = [\n",
    "    (0.0, 1.0, '#4CAF50', 'KL $\\\\approx$ 0'),\n",
    "    (0.5, 1.2, '#FF9800', 'KL = 0.16'),\n",
    "    (2.0, 0.5, '#E91E63', 'KL = 2.12'),\n",
    "]\n",
    "for mu_c, sigma_c, color_c, label_c in configs:\n",
    "    pdf_c = (1 / (sigma_c * np.sqrt(2*np.pi))) * np.exp(-0.5 * ((z_range - mu_c)/sigma_c)**2)\n",
    "    ax6.plot(z_range, pdf_c, color=color_c, linewidth=2, label=label_c)\n",
    "\n",
    "ax6.set_xlabel('$z$', fontsize=11)\n",
    "ax6.set_ylabel('Density', fontsize=11)\n",
    "ax6.set_title('Encoder Distributions\\nat Different KL Values', fontsize=12, fontweight='bold')\n",
    "ax6.legend(fontsize=9)\n",
    "ax6.set_xlim(-5, 5)\n",
    "\n",
    "# ================================================================\n",
    "# Panel 7 (bottom-right): ELBO decomposition pie charts\n",
    "# ================================================================\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "\n",
    "# Show ELBO composition at epoch 5, 30, 100\n",
    "epochs_show = ['Epoch 5', 'Epoch 30', 'Epoch 100']\n",
    "recon_vals = [95, 70, 55]\n",
    "kl_vals = [5, 30, 45]\n",
    "\n",
    "x_pos = np.arange(len(epochs_show))\n",
    "width = 0.6\n",
    "\n",
    "ax7.bar(x_pos, recon_vals, width, color='#2196F3', label='Reconstruction %')\n",
    "ax7.bar(x_pos, kl_vals, width, bottom=recon_vals, color='#FF5722', label='KL Divergence %')\n",
    "\n",
    "for i in range(len(epochs_show)):\n",
    "    ax7.text(x_pos[i], recon_vals[i]/2, f'{recon_vals[i]}%', ha='center', va='center',\n",
    "             fontsize=11, fontweight='bold', color='white')\n",
    "    ax7.text(x_pos[i], recon_vals[i] + kl_vals[i]/2, f'{kl_vals[i]}%', ha='center',\n",
    "             va='center', fontsize=11, fontweight='bold', color='white')\n",
    "\n",
    "ax7.set_xticks(x_pos)\n",
    "ax7.set_xticklabels(epochs_show, fontsize=11)\n",
    "ax7.set_ylabel('Share of Total Loss (%)', fontsize=11)\n",
    "ax7.set_title('ELBO Composition\\nOver Training', fontsize=12, fontweight='bold')\n",
    "ax7.legend(fontsize=10, loc='lower right')\n",
    "ax7.set_ylim(0, 110)\n",
    "\n",
    "plt.suptitle('VAE Loss Dashboard: ELBO, KL Divergence & Reparameterization',\n",
    "             fontsize=16, fontweight='bold', y=1.01)\n",
    "plt.show()\n",
    "\n",
    "print(\"This dashboard shows the complete picture of VAE training:\")\n",
    "print(\"  1. Reconstruction loss penalizes bad pixel predictions (top-left)\")\n",
    "print(\"  2. KL divergence penalizes deviations from the standard normal (top-center)\")\n",
    "print(\"  3. The reparameterization trick enables gradient flow (top-right)\")\n",
    "print(\"  4. Training balances reconstruction and regularization (middle)\")\n",
    "print(\"  5. Beta controls the tradeoff (bottom-left)\")\n",
    "print(\"  6. Different KL values produce different encoder shapes (bottom-center)\")\n",
    "print(\"  7. The relative importance of each term shifts during training (bottom-right)\")"
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection Questions\n",
    "\n",
    "Take a moment to reflect on what we have learned. Try to answer these questions *before* looking at the hints.\n",
    "\n",
    "**Question 1:** If we set the KL divergence weight to zero (effectively removing the regularization term), what would happen to the latent space? Would the model still be able to generate new samples?\n",
    "\n",
    "> *Hint: Think about what happens to a regular autoencoder. Without the KL term, what prevents the encoder from mapping each image to an isolated, arbitrary point?*\n",
    "\n",
    "**Question 2:** The reparameterization trick separates randomness ($\\epsilon$) from learned parameters ($\\mu$, $\\sigma$). Can you think of another domain where a similar \"separation of randomness\" technique is used?\n",
    "\n",
    "> *Hint: Think about dropout in neural networks, or REINFORCE vs. the Gumbel-Softmax trick in reinforcement learning.*\n",
    "\n",
    "**Question 3:** In the KL divergence formula $D_{KL} = -\\frac{1}{2}(1 + \\log\\sigma^2 - \\mu^2 - \\sigma^2)$, what happens when $\\sigma \\to 0$? What does this mean for the encoder's behavior?\n",
    "\n",
    "> *Hint: Compute $\\lim_{\\sigma \\to 0} \\log(\\sigma^2)$. What does an infinitely narrow distribution mean?*"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Challenges\n",
    "\n",
    "If you want to go deeper, try these:\n",
    "\n",
    "1. **Derive the KL divergence yourself.** Start from the definition $D_{KL}(q \\| p) = \\mathbb{E}_q[\\log q(z) - \\log p(z)]$ with $q = \\mathcal{N}(\\mu, \\sigma^2)$ and $p = \\mathcal{N}(0, 1)$. Substitute the Gaussian PDFs and simplify. You should arrive at $-\\frac{1}{2}(1 + \\log\\sigma^2 - \\mu^2 - \\sigma^2)$.\n",
    "\n",
    "2. **Implement MSE reconstruction loss** instead of BCE. When would you use MSE instead of BCE? (Hint: think about continuous vs. binary data.)\n",
    "\n",
    "3. **Explore the \"posterior collapse\" problem.** What happens when the KL divergence goes to zero everywhere? Is this always a good thing? (Hint: it means the decoder ignores $z$ entirely.)"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered the three pillars of VAE training:\n",
    "\n",
    "| Component | What it does | Formula |\n",
    "|-----------|-------------|---------|\n",
    "| **Reconstruction Loss** | Ensures decoded images match originals | $-\\sum_i [x_i \\log p_i + (1-x_i)\\log(1-p_i)]$ |\n",
    "| **KL Divergence** | Keeps latent space organized near $\\mathcal{N}(0,I)$ | $-\\frac{1}{2}\\sum_j (1 + \\log\\sigma_j^2 - \\mu_j^2 - \\sigma_j^2)$ |\n",
    "| **Reparameterization** | Enables gradient-based training through sampling | $z = \\mu + \\sigma \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,I)$ |\n",
    "\n",
    "The ELBO combines reconstruction and regularization into a single objective:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{VAE}} = \\underbrace{-\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]}_{\\text{Reconstruction}} + \\underbrace{D_{KL}(q_\\phi(z|x) \\| p(z))}_{\\text{Regularization}}$$\n",
    "\n",
    "**In the next notebook**, we will put all of this into action by building a complete VAE in PyTorch, training it on MNIST, and visualizing the latent space.\n",
    "\n",
    "---\n",
    "\n",
    "*Vizuara -- Making AI Intuitive*"
   ],
   "id": "cell_42"
  }
 ]
}