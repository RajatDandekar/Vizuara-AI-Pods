{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "VAE From Scratch \u2014 Notebook Series Index \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders From Scratch \u2014 Notebook Series\n",
    "\n",
    "*A hands-on journey through one of the most elegant frameworks in generative AI*\n",
    "\n",
    "**By Vizuara**"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "\n",
    "This series of 4 interactive Google Colab notebooks takes you from zero to a fully working Variational Autoencoder. Each notebook builds on the previous one, but is also **self-contained** \u2014 you can run any notebook independently.\n",
    "\n",
    "By the end of this series, you will:\n",
    "- Understand what latent variables are and why they matter\n",
    "- Derive the ELBO training objective from first principles\n",
    "- Build a complete VAE in PyTorch from scratch\n",
    "- Generate new handwritten digits and smoothly morph between them"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Path"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The VAE learning journey\n",
    "notebooks = {\n",
    "    \"01\": {\n",
    "        \"title\": \"Latent Variables & Autoencoders\",\n",
    "        \"time\": \"35 min\",\n",
    "        \"concepts\": [\n",
    "            \"What are latent variables (the 'hidden recipe')\",\n",
    "            \"The encoder-decoder framework\",\n",
    "            \"Building a regular autoencoder on MNIST\",\n",
    "            \"The 'dead zone' problem \u2014 why regular autoencoders fail at generation\"\n",
    "        ],\n",
    "        \"final_output\": \"Latent space visualization showing dead zones\"\n",
    "    },\n",
    "    \"02\": {\n",
    "        \"title\": \"The ELBO, KL Divergence & Reparameterization\",\n",
    "        \"time\": \"40 min\",\n",
    "        \"concepts\": [\n",
    "            \"Why we need a probabilistic framework\",\n",
    "            \"Deriving the Evidence Lower Bound (ELBO) step by step\",\n",
    "            \"Reconstruction loss and KL divergence \u2014 the two forces\",\n",
    "            \"The reparameterization trick for gradient flow\",\n",
    "            \"Numerical examples for every equation\"\n",
    "        ],\n",
    "        \"final_output\": \"Interactive visualization dashboard of all VAE math components\"\n",
    "    },\n",
    "    \"03\": {\n",
    "        \"title\": \"Building a VAE from Scratch\",\n",
    "        \"time\": \"40 min\",\n",
    "        \"concepts\": [\n",
    "            \"Encoder with two-headed output (mu and log-variance)\",\n",
    "            \"Decoder with sigmoid pixel probabilities\",\n",
    "            \"Full VAE class with reparameterization\",\n",
    "            \"The ELBO loss function in code\",\n",
    "            \"Training on MNIST for 20 epochs\"\n",
    "        ],\n",
    "        \"final_output\": \"Trained VAE with reconstruction comparison grid\"\n",
    "    },\n",
    "    \"04\": {\n",
    "        \"title\": \"Exploring the Latent Space\",\n",
    "        \"time\": \"35 min\",\n",
    "        \"concepts\": [\n",
    "            \"Latent space scatter plot (10 digit classes)\",\n",
    "            \"Generating new digits from random noise\",\n",
    "            \"Smooth interpolation between digits (3 \u2192 8)\",\n",
    "            \"The full digit manifold visualization\",\n",
    "            \"Why VAEs produce blurry images\"\n",
    "        ],\n",
    "        \"final_output\": \"Beautiful digit manifold and smooth morphing animations\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  VARIATIONAL AUTOENCODERS FROM SCRATCH\")\n",
    "print(\"  A Vizuara Notebook Series (4 notebooks)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for num, info in notebooks.items():\n",
    "    print(f\"\\n\ud83d\udcd3 Notebook {num}: {info['title']}\")\n",
    "    print(f\"   \u23f1\ufe0f  {info['time']}\")\n",
    "    print(f\"   \ud83c\udfaf Final output: {info['final_output']}\")\n",
    "    for concept in info['concepts']:\n",
    "        print(f\"      \u2022 {concept}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  Total estimated time: ~2.5 hours\")\n",
    "print(\"  Prerequisites: Basic Python, some PyTorch familiarity\")\n",
    "print(\"  Hardware: Google Colab with T4 GPU (free tier works!)\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- **Python**: Comfortable with classes, functions, and loops\n",
    "- **PyTorch**: Basic familiarity with tensors and `nn.Module` (we explain everything as we go)\n",
    "- **Math**: High school calculus is helpful but not required \u2014 we build intuition before equations\n",
    "- **Hardware**: Google Colab free tier with a T4 GPU is all you need"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use These Notebooks\n",
    "\n",
    "Each notebook follows a consistent structure:\n",
    "\n",
    "1. **Why Does This Matter?** \u2014 Motivation and a teaser of the final output\n",
    "2. **Building Intuition** \u2014 No code, just analogies and plain-English explanations\n",
    "3. **The Mathematics** \u2014 Equations with plain-English explanations and numerical examples\n",
    "4. **Let's Build It** \u2014 Incremental code with visualizations at every step\n",
    "5. **Your Turn** \u2014 TODO exercises where you implement key components\n",
    "6. **Final Output** \u2014 A satisfying, visual result you can be proud of\n",
    "7. **Reflection** \u2014 Questions to deepen understanding + optional challenges"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "\n",
    "Pick any notebook below to begin. We recommend starting with Notebook 01 for the full experience, but each notebook is self-contained."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\ude80 Ready to start?\")\n",
    "print()\n",
    "print(\"Open any notebook in this folder to begin!\")\n",
    "print()\n",
    "print(\"  \ud83d\udcd3 01_latent_variables_and_autoencoders.ipynb\")\n",
    "print(\"  \ud83d\udcd3 02_elbo_kl_divergence_and_reparameterization.ipynb\")\n",
    "print(\"  \ud83d\udcd3 03_building_a_vae_from_scratch.ipynb\")\n",
    "print(\"  \ud83d\udcd3 04_exploring_the_latent_space.ipynb\")\n",
    "print()\n",
    "print(\"Recommended order: 01 \u2192 02 \u2192 03 \u2192 04\")\n",
    "print()\n",
    "print(\"Happy learning! \ud83c\udf93\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Vizuara\n",
    "\n",
    "Vizuara teaches AI and machine learning from first principles. We believe that understanding *why* something works is just as important as knowing *how* to use it. Our articles and notebooks build everything from scratch \u2014 no black boxes, no hand-waving.\n",
    "\n",
    "**Read the full article:** [Variational Autoencoders From Scratch](https://vizuara.substack.com)\n",
    "\n",
    "**References:**\n",
    "- Kingma & Welling, \"Auto-Encoding Variational Bayes\" (2013)\n",
    "- Doersch, \"Tutorial on Variational Autoencoders\" (2016)\n",
    "- Blei, Kucukelbir & McAuliffe, \"Variational Inference: A Review for Statisticians\" (2017)"
   ],
   "id": "cell_9"
  }
 ]
}