{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Building a VAE from Scratch â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_01_setup_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Setup Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_01_setup_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1_yOuaRupWcvvBB5tNnjVrtDllXqg6x4Q\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/03_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a VAE from Scratch in PyTorch\n",
    "\n",
    "*Part 3 of 4 in the Vizuara series on Variational Autoencoders From Scratch*\n",
    "\n",
    "*Estimated time: 50 minutes*\n",
    "\n",
    "In Notebook 1, we explored why regular autoencoders fail as generative models. In Notebook 2, we built the mathematical foundations â€” the ELBO, KL divergence, and the reparameterization trick. Now we put all of that theory into working code.\n",
    "\n",
    "By the end of this notebook, you will have:\n",
    "- A fully functional VAE that generates new handwritten digits from pure noise\n",
    "- Clear understanding of every design choice in the architecture\n",
    "- Training curves showing the delicate balance between reconstruction and regularization\n",
    "- Side-by-side comparisons of original digits and their reconstructions\n",
    "\n",
    "Let us build it."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_02_why_matter",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why Matter\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_02_why_matter.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Why Does This Matter?\n",
    "\n",
    "Here is what we are about to accomplish: we will write approximately 60 lines of PyTorch that, after 20 epochs of training on MNIST, can do something remarkable. We will sample a random point from a standard normal distribution â€” pure noise â€” feed it through a neural network, and out comes a handwritten digit that looks like a human wrote it.\n",
    "\n",
    "This is fundamentally different from a regular autoencoder. A regular autoencoder compresses and reconstructs. A VAE *generates*. It learns the underlying distribution of handwritten digits so well that it can create new ones that have never existed before.\n",
    "\n",
    "The key ingredient that makes this possible? Instead of encoding each image to a single point in latent space, we encode it to a *distribution*. And instead of a simple reconstruction loss, we add a regularization term (KL divergence) that shapes the latent space into something we can sample from.\n",
    "\n",
    "Let us build each piece, one component at a time."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_03_vae_diff_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Vae Diff Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_03_vae_diff_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_04_diff1_dist",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Diff1 Dist\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_04_diff1_dist.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_05_diff2_reparam",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Diff2 Reparam\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_05_diff2_reparam.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_06_diff3_elbo",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Diff3 Elbo\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_06_diff3_elbo.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Building Intuition â€” What Makes a VAE Different?\n",
    "\n",
    "Before we write a single line of code, let us recap the three critical differences between a regular autoencoder and a VAE. Each one directly affects how we build the architecture.\n",
    "\n",
    "### Difference 1: Distributions, Not Points\n",
    "\n",
    "A regular autoencoder maps input $x$ to a single latent vector $z$. The encoder is deterministic â€” the same input always produces the same code.\n",
    "\n",
    "A VAE encoder outputs two vectors: a mean $\\mu$ and a log-variance $\\log(\\sigma^2)$. Together, these define a Gaussian distribution in latent space. The same input maps to a *cloud* of possible latent codes.\n",
    "\n",
    "**Architecture implication:** The encoder needs two output heads instead of one. We will build `fc_mu` and `fc_logvar` as separate linear layers.\n",
    "\n",
    "### Difference 2: The Reparameterization Trick\n",
    "\n",
    "We cannot backpropagate through a random sampling operation. If we just did `z = torch.randn(...)`, gradients would not flow from the decoder back to the encoder. The reparameterization trick separates the randomness from the learnable parameters:\n",
    "\n",
    "$$z = \\mu + \\sigma \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\n",
    "\n",
    "The randomness is in $\\epsilon$, which has no learnable parameters. The encoder outputs ($\\mu$ and $\\sigma$) are deterministic functions of the input, so gradients flow through them normally.\n",
    "\n",
    "**Architecture implication:** We need a `reparameterize` method that takes $\\mu$ and $\\log(\\sigma^2)$ and returns a sampled $z$.\n",
    "\n",
    "### Difference 3: The ELBO Loss\n",
    "\n",
    "A regular autoencoder minimizes reconstruction error alone. A VAE minimizes the **Evidence Lower Bound (ELBO)**, which has two terms:\n",
    "\n",
    "$$\\mathcal{L} = \\underbrace{\\text{BCE}(x, \\hat{x})}_{\\text{reconstruction}} + \\underbrace{D_{KL}(q(z|x) \\| p(z))}_{\\text{regularization}}$$\n",
    "\n",
    "The reconstruction term asks: \"How well did we recreate the input?\" The KL term asks: \"How close is the encoder distribution to a standard normal?\" These two forces pull in opposite directions, and their balance determines the quality of the generative model.\n",
    "\n",
    "**Architecture implication:** Our loss function must compute and return both terms separately so we can monitor the balance during training."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_07_math_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Math Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_07_math_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_08_math_logvar",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Math Logvar\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_08_math_logvar.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_09_math_reparam",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Math Reparam\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_09_math_reparam.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_10_math_loss",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Math Loss\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_10_math_loss.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: The Mathematics for Implementation\n",
    "\n",
    "Let us write down the three equations we will implement, focusing on the choices that matter for code.\n",
    "\n",
    "### 3.1 The Encoder Distribution\n",
    "\n",
    "The encoder defines a Gaussian posterior over the latent space:\n",
    "\n",
    "$$q(z|x) = \\mathcal{N}(\\mu(x), \\text{diag}(\\sigma^2(x)))$$\n",
    "\n",
    "where $\\mu(x)$ and $\\sigma^2(x)$ are neural network outputs.\n",
    "\n",
    "**Why log-variance instead of $\\sigma$ directly?**\n",
    "\n",
    "This is a crucial design choice. The standard deviation $\\sigma$ must be positive. If we output $\\sigma$ directly, we would need to constrain the network output (e.g., with a softplus). Instead, we output $\\log(\\sigma^2)$, which can be any real number â€” positive or negative. To recover $\\sigma$, we compute:\n",
    "\n",
    "$$\\sigma = \\exp\\left(\\frac{1}{2} \\log(\\sigma^2)\\right)$$\n",
    "\n",
    "This is numerically stable because the exponential naturally ensures positivity, and operating in log-space avoids very large or very small variance values that could destabilize training.\n",
    "\n",
    "### 3.2 The Reparameterization\n",
    "\n",
    "$$z = \\mu + \\exp\\left(\\frac{1}{2} \\log(\\sigma^2)\\right) \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\n",
    "\n",
    "In code, this is just three lines. But those three lines are what make a VAE trainable.\n",
    "\n",
    "### 3.3 The Loss Function\n",
    "\n",
    "**Reconstruction loss** â€” We use Binary Cross-Entropy (BCE) because MNIST pixels are values between 0 and 1. Our decoder outputs pixel probabilities through a sigmoid, and BCE measures how well these probabilities match the actual pixel values:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{recon}} = -\\sum_{i=1}^{784} \\left[ x_i \\log(\\hat{x}_i) + (1 - x_i) \\log(1 - \\hat{x}_i) \\right]$$\n",
    "\n",
    "**KL divergence** â€” For two Gaussians (our posterior vs. the standard normal prior), the KL divergence has a closed-form solution:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{KL}} = -\\frac{1}{2} \\sum_{j=1}^{d} \\left( 1 + \\log(\\sigma_j^2) - \\mu_j^2 - \\sigma_j^2 \\right)$$\n",
    "\n",
    "**Why `reduction='sum'`?** We sum over all 784 pixels and all latent dimensions rather than averaging. This keeps the reconstruction and KL terms at comparable scales. If we averaged the reconstruction term (dividing by 784) but summed the KL term (over just 2 dimensions), the KL term would dominate and the model would ignore reconstruction entirely."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_11_build_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Build Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_11_build_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Let Us Build It â€” Component by Component\n",
    "\n",
    "### 4.1 Setup and Data Loading"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_12_imports_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Imports Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_12_imports_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_13_data_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Data Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_13_data_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "# We normalize to [0, 1] â€” important for BCE loss!\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts to [0, 1] range\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset):,}\")\n",
    "print(f\"Test samples:     {len(test_dataset):,}\")\n",
    "print(f\"Image shape:      {train_dataset[0][0].shape}\")\n",
    "print(f\"Pixel range:      [{train_dataset[0][0].min():.1f}, {train_dataset[0][0].max():.1f}]\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_14_visualize_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Visualize Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_14_visualize_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize some training images so we know what our VAE will be learning to reconstruct and generate."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_15_visualize_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Visualize Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_15_visualize_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample training images\n",
    "fig, axes = plt.subplots(2, 8, figsize=(14, 4))\n",
    "fig.suptitle('MNIST Training Samples â€” What Our VAE Will Learn', fontsize=14, fontweight='bold')\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img, label = train_dataset[i]\n",
    "    ax.imshow(img.squeeze(), cmap='gray')\n",
    "    ax.set_title(str(label), fontsize=10)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nEach image is 28x28 = 784 pixels\")\n",
    "print(f\"Our VAE will compress these 784 values into just 2 latent dimensions\")\n",
    "print(f\"That is a compression ratio of {784/2:.0f}:1!\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_16_encoder_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Encoder Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_16_encoder_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Encoder\n",
    "\n",
    "The encoder takes a flattened 784-dimensional image and maps it to two vectors: the mean $\\mu$ and the log-variance $\\log(\\sigma^2)$ of the latent distribution. Notice the **two-headed output** â€” this is what makes it a VAE encoder rather than a regular autoencoder encoder.\n",
    "\n",
    "Why two separate linear layers at the end? Because $\\mu$ and $\\log(\\sigma^2)$ represent fundamentally different quantities. The mean tells us *where* in latent space this image lives. The log-variance tells us *how uncertain* the encoder is about that location. These should be independent outputs that the network learns separately."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_17_encoder_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Encoder Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_17_encoder_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"VAE Encoder: maps input x to latent distribution parameters (mu, logvar).\n",
    "\n",
    "    Architecture:\n",
    "        784 -> 512 -> 256 -> (mu: latent_dim, logvar: latent_dim)\n",
    "\n",
    "    The two-headed output is the key difference from a regular autoencoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=784, hidden_dim=512, latent_dim=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 256)\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)       # Mean of q(z|x)\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)   # Log-variance of q(z|x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_18_test_encoder_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Test Encoder Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_18_test_encoder_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify the encoder and inspect its outputs."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_19_test_encoder_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Test Encoder Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_19_test_encoder_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the encoder\n",
    "encoder = Encoder(input_dim=784, hidden_dim=512, latent_dim=2).to(device)\n",
    "dummy_input = torch.randn(4, 784).to(device)  # Batch of 4 flattened images\n",
    "\n",
    "mu, logvar = encoder(dummy_input)\n",
    "print(\"Encoder Test\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Input shape:   {dummy_input.shape}\")    # (4, 784)\n",
    "print(f\"Mu shape:      {mu.shape}\")              # (4, 2)\n",
    "print(f\"Logvar shape:  {logvar.shape}\")          # (4, 2)\n",
    "print(f\"\\nSample mu:     {mu[0].detach().cpu().numpy()}\")\n",
    "print(f\"Sample logvar: {logvar[0].detach().cpu().numpy()}\")\n",
    "\n",
    "# Parameter count\n",
    "enc_params = sum(p.numel() for p in encoder.parameters())\n",
    "print(f\"\\nEncoder parameters: {enc_params:,}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_20_logvar_interpret",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Logvar Interpret\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_20_logvar_interpret.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `logvar` values can be positive or negative â€” they are not constrained. A logvar of -2 means $\\sigma^2 = e^{-2} \\approx 0.14$ (the encoder is fairly certain), while a logvar of 2 means $\\sigma^2 = e^{2} \\approx 7.4$ (high uncertainty). The network learns these values through backpropagation."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_21_decoder_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Decoder Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_21_decoder_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The Decoder\n",
    "\n",
    "The decoder takes a latent vector $z$ and maps it back to a 784-dimensional output â€” a reconstructed image. The critical detail is the **sigmoid activation** at the output.\n",
    "\n",
    "Why sigmoid? Because MNIST pixels are in the range $[0, 1]$, and we use Binary Cross-Entropy (BCE) as our reconstruction loss. BCE expects both the target and the prediction to be probabilities in $[0, 1]$. The sigmoid squashes the raw network output into exactly this range. You can think of each output value as the *probability* that the corresponding pixel is \"on.\""
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_22_decoder_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Decoder Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_22_decoder_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"VAE Decoder: maps latent vector z back to image space.\n",
    "\n",
    "    Architecture:\n",
    "        latent_dim -> 256 -> 512 -> 784 (with sigmoid)\n",
    "\n",
    "    The sigmoid output gives pixel probabilities in [0, 1],\n",
    "    which is required for Binary Cross-Entropy loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=2, hidden_dim=512, output_dim=784):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = F.relu(self.fc1(z))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return torch.sigmoid(self.fc3(h))"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_23_test_decoder_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Test Decoder Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_23_test_decoder_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the decoder\n",
    "decoder = Decoder(latent_dim=2, hidden_dim=512, output_dim=784).to(device)\n",
    "dummy_z = torch.randn(4, 2).to(device)  # Batch of 4 latent vectors\n",
    "\n",
    "x_recon = decoder(dummy_z)\n",
    "print(\"Decoder Test\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Input shape (z): {dummy_z.shape}\")      # (4, 2)\n",
    "print(f\"Output shape:    {x_recon.shape}\")       # (4, 784)\n",
    "print(f\"Output range:    [{x_recon.min().item():.4f}, {x_recon.max().item():.4f}]\")\n",
    "print(f\"  (Should be in [0, 1] thanks to sigmoid)\")\n",
    "\n",
    "# Parameter count\n",
    "dec_params = sum(p.numel() for p in decoder.parameters())\n",
    "print(f\"\\nDecoder parameters: {dec_params:,}\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_24_vae_class_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Vae Class Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_24_vae_class_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The VAE Class â€” Connecting Everything with Reparameterization\n",
    "\n",
    "Now we assemble the encoder and decoder into the full VAE. The `reparameterize` method is the bridge between them â€” it takes the encoder's distribution parameters and produces a concrete latent vector that we can feed to the decoder, while keeping the computation graph intact for backpropagation."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_25_vae_class_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Vae Class Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_25_vae_class_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"Complete Variational Autoencoder.\n",
    "\n",
    "    Pipeline:\n",
    "        x -> Encoder -> (mu, logvar) -> Reparameterize -> z -> Decoder -> x_recon\n",
    "\n",
    "    The reparameterization trick is what makes this trainable:\n",
    "        z = mu + exp(0.5 * logvar) * epsilon,  epsilon ~ N(0, I)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=784, hidden_dim=512, latent_dim=2):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Sample z from q(z|x) using the reparameterization trick.\n",
    "\n",
    "        Instead of sampling z ~ N(mu, sigma^2), we compute:\n",
    "            z = mu + sigma * epsilon,  where epsilon ~ N(0, I)\n",
    "\n",
    "        This makes z a deterministic function of mu, logvar, and epsilon,\n",
    "        so gradients can flow through mu and logvar.\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)   # sigma = exp(0.5 * log(sigma^2))\n",
    "        eps = torch.randn_like(std)      # epsilon ~ N(0, I)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, mu, logvar"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_26_test_vae_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Test Vae Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_26_test_vae_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the full VAE pipeline\n",
    "vae_test = VAE(input_dim=784, hidden_dim=512, latent_dim=2).to(device)\n",
    "dummy_x = torch.randn(4, 784).clamp(0, 1).to(device)\n",
    "\n",
    "x_recon, mu, logvar = vae_test(dummy_x)\n",
    "\n",
    "print(\"Full VAE Pipeline Test\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Input shape:   {dummy_x.shape}\")       # (4, 784)\n",
    "print(f\"Recon shape:   {x_recon.shape}\")        # (4, 784)\n",
    "print(f\"Mu shape:      {mu.shape}\")              # (4, 2)\n",
    "print(f\"Logvar shape:  {logvar.shape}\")          # (4, 2)\n",
    "print(f\"Recon range:   [{x_recon.min().item():.4f}, {x_recon.max().item():.4f}]\")\n",
    "\n",
    "total_params = sum(p.numel() for p in vae_test.parameters())\n",
    "print(f\"\\nTotal VAE parameters: {total_params:,}\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_27_arch_summary_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Arch Summary Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_27_arch_summary_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the architecture to make sure we understand the full data flow."
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_28_arch_summary_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Arch Summary Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_28_arch_summary_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture summary\n",
    "print(\"=\" * 60)\n",
    "print(\"              VAE ARCHITECTURE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"  ENCODER\")\n",
    "print(\"  -------\")\n",
    "print(\"  Input (784) â”€â”€> Linear(784, 512) â”€â”€> ReLU\")\n",
    "print(\"                  Linear(512, 256) â”€â”€> ReLU\")\n",
    "print(\"                       â”œâ”€â”€> Linear(256, 2) â”€â”€> mu\")\n",
    "print(\"                       â””â”€â”€> Linear(256, 2) â”€â”€> logvar\")\n",
    "print()\n",
    "print(\"  REPARAMETERIZE\")\n",
    "print(\"  --------------\")\n",
    "print(\"  z = mu + exp(0.5 * logvar) * epsilon\")\n",
    "print(\"  epsilon ~ N(0, I)\")\n",
    "print()\n",
    "print(\"  DECODER\")\n",
    "print(\"  -------\")\n",
    "print(\"  z (2) â”€â”€> Linear(2, 256) â”€â”€> ReLU\")\n",
    "print(\"            Linear(256, 512) â”€â”€> ReLU\")\n",
    "print(\"            Linear(512, 784) â”€â”€> Sigmoid â”€â”€> x_recon\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Detailed parameter breakdown\n",
    "enc_params = sum(p.numel() for p in vae_test.encoder.parameters())\n",
    "dec_params = sum(p.numel() for p in vae_test.decoder.parameters())\n",
    "\n",
    "print(f\"  Encoder parameters:  {enc_params:>10,}\")\n",
    "print(f\"  Decoder parameters:  {dec_params:>10,}\")\n",
    "print(f\"  Total parameters:    {total_params:>10,}\")\n",
    "print(f\"  Model size:          {total_params * 4 / 1024 / 1024:.2f} MB (float32)\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_29_loss_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Loss Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_29_loss_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 The Loss Function\n",
    "\n",
    "The VAE loss has two terms that work in tension:\n",
    "\n",
    "1. **Reconstruction loss (BCE):** Pushes the model to faithfully reproduce the input. This term wants the encoder to memorize each image precisely â€” use the full capacity of the latent space, spread points apart, make each code unique.\n",
    "\n",
    "2. **KL divergence:** Pushes the encoder distribution toward a standard normal $\\mathcal{N}(0, I)$. This term wants all codes to cluster near the origin with unit variance â€” the opposite of what reconstruction wants.\n",
    "\n",
    "The balance between these forces is what gives a VAE its generative power. The reconstruction term ensures the latent space is *informative* (different digits map to different regions). The KL term ensures it is *regular* (no gaps, smooth transitions, easy to sample from)."
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_30_loss_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Loss Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_30_loss_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x_recon, x, mu, logvar):\n",
    "    \"\"\"Compute the VAE loss = Reconstruction + KL Divergence.\n",
    "\n",
    "    Args:\n",
    "        x_recon: Reconstructed images from decoder (B, 784), values in [0, 1]\n",
    "        x: Original images (B, 784), values in [0, 1]\n",
    "        mu: Encoder mean (B, latent_dim)\n",
    "        logvar: Encoder log-variance (B, latent_dim)\n",
    "\n",
    "    Returns:\n",
    "        total_loss: Scalar, the sum of reconstruction and KL losses\n",
    "        recon_loss: Scalar, BCE reconstruction term\n",
    "        kl_loss: Scalar, KL divergence term\n",
    "\n",
    "    Note: We use reduction='sum' to keep recon and KL at comparable scales.\n",
    "    \"\"\"\n",
    "    # Reconstruction: how well did we recreate the input?\n",
    "    recon_loss = F.binary_cross_entropy(x_recon, x, reduction='sum')\n",
    "\n",
    "    # KL divergence: how far is q(z|x) from N(0, I)?\n",
    "    # Closed-form for Gaussian: -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return recon_loss + kl_loss, recon_loss, kl_loss"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_31_test_loss_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Test Loss Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_31_test_loss_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the loss function\n",
    "x_test = torch.rand(4, 784).to(device)  # Fake input in [0, 1]\n",
    "x_recon_test, mu_test, logvar_test = vae_test(x_test)\n",
    "\n",
    "total, recon, kl = vae_loss(x_recon_test, x_test, mu_test, logvar_test)\n",
    "print(\"Loss Function Test\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Reconstruction loss: {recon.item():>10.2f}\")\n",
    "print(f\"KL divergence:       {kl.item():>10.2f}\")\n",
    "print(f\"Total loss:          {total.item():>10.2f}\")\n",
    "print(f\"\\nRecon / Total:       {recon.item()/total.item():>10.1%}\")\n",
    "print(f\"KL / Total:          {kl.item()/total.item():>10.1%}\")\n",
    "print(\"\\n(At initialization, KL should be small â€” the encoder\")\n",
    "print(\" hasn't learned to use the latent space yet)\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_32_todo1_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo1 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_32_todo1_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Your Turn!\n",
    "\n",
    "Before we run the full training, let us make sure you deeply understand two of the most important pieces. Implement them yourself.\n",
    "\n",
    "### TODO 1: Implement the Reparameterization Trick\n",
    "\n",
    "Given $\\mu$ and $\\log(\\sigma^2)$ from the encoder, sample a latent vector $z$ using the reparameterization trick.\n",
    "\n",
    "**Recall the formula:**\n",
    "\n",
    "$$z = \\mu + \\sigma \\cdot \\epsilon, \\quad \\text{where } \\sigma = \\exp\\left(\\frac{1}{2} \\log(\\sigma^2)\\right), \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_33_todo1_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo1 Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_33_todo1_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize_todo(mu, logvar):\n",
    "    \"\"\"\n",
    "    Sample z from q(z|x) using the reparameterization trick.\n",
    "\n",
    "    Steps:\n",
    "      1. Compute std from logvar:  std = exp(0.5 * logvar)\n",
    "      2. Sample epsilon:           eps = torch.randn_like(std)\n",
    "      3. Compute z:                z = mu + std * eps\n",
    "\n",
    "    Args:\n",
    "        mu: (B, latent_dim) â€” mean of the encoder distribution\n",
    "        logvar: (B, latent_dim) â€” log-variance of the encoder distribution\n",
    "\n",
    "    Returns:\n",
    "        z: (B, latent_dim) â€” sampled latent vector\n",
    "\n",
    "    Hints:\n",
    "        - torch.exp() computes element-wise exponential\n",
    "        - torch.randn_like(tensor) samples from N(0,1) with same shape as tensor\n",
    "        - All operations are element-wise\n",
    "    \"\"\"\n",
    "    # ============ YOUR CODE HERE ============\n",
    "    # TODO: Implement the 3 steps above\n",
    "\n",
    "    raise NotImplementedError(\"Implement the reparameterization trick!\")\n",
    "    # ========================================="
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_34_todo1_after",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo1 After\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_34_todo1_after.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verification for TODO 1 ---\n",
    "# Uncomment after implementing:\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "# test_mu = torch.tensor([[1.0, -1.0], [0.5, 0.5]])\n",
    "# test_logvar = torch.tensor([[0.0, 0.0], [1.0, -1.0]])  # sigma^2 = 1, 1, e, 1/e\n",
    "#\n",
    "# z = reparameterize_todo(test_mu, test_logvar)\n",
    "# print(f\"Mu:     {test_mu}\")\n",
    "# print(f\"Logvar: {test_logvar}\")\n",
    "# print(f\"z:      {z}\")\n",
    "# print(f\"z shape: {z.shape}\")\n",
    "# assert z.shape == (2, 2), f\"Expected shape (2, 2), got {z.shape}\"\n",
    "#\n",
    "# # Check that z is close to mu (on average) â€” run many samples\n",
    "# torch.manual_seed(0)\n",
    "# zs = torch.stack([reparameterize_todo(test_mu, test_logvar) for _ in range(10000)])\n",
    "# mean_z = zs.mean(dim=0)\n",
    "# print(f\"\\nMean of 10,000 samples: {mean_z}\")\n",
    "# print(f\"Should be close to mu:  {test_mu}\")\n",
    "# assert torch.allclose(mean_z, test_mu, atol=0.1), \"Mean of samples should be close to mu!\"\n",
    "# print(\"\\nTODO 1: PASSED!\")"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_35_todo2_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo2 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_35_todo2_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement the VAE Loss Function\n",
    "\n",
    "Given the reconstruction, original input, $\\mu$, and $\\log(\\sigma^2)$, compute the two loss terms.\n",
    "\n",
    "**Recall the formulas:**\n",
    "\n",
    "Reconstruction: $\\mathcal{L}_{\\text{recon}} = \\text{BCE}(\\hat{x}, x, \\text{reduction}=\\text{sum})$\n",
    "\n",
    "KL divergence: $\\mathcal{L}_{\\text{KL}} = -\\frac{1}{2} \\sum (1 + \\log(\\sigma^2) - \\mu^2 - \\sigma^2)$"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_36_todo2_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo2 Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_36_todo2_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_todo(x_recon, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    Compute the VAE loss: Reconstruction + KL Divergence.\n",
    "\n",
    "    Steps:\n",
    "      1. Compute recon_loss using F.binary_cross_entropy with reduction='sum'\n",
    "      2. Compute kl_loss: -0.5 * sum(1 + logvar - mu^2 - exp(logvar))\n",
    "      3. Return total_loss, recon_loss, kl_loss\n",
    "\n",
    "    Args:\n",
    "        x_recon: (B, 784) â€” decoder output, values in [0, 1]\n",
    "        x: (B, 784) â€” original input, values in [0, 1]\n",
    "        mu: (B, latent_dim) â€” encoder mean\n",
    "        logvar: (B, latent_dim) â€” encoder log-variance\n",
    "\n",
    "    Returns:\n",
    "        total_loss: recon_loss + kl_loss\n",
    "        recon_loss: BCE reconstruction term\n",
    "        kl_loss: KL divergence term\n",
    "\n",
    "    Hints:\n",
    "        - F.binary_cross_entropy(pred, target, reduction='sum')\n",
    "        - mu.pow(2) computes element-wise square\n",
    "        - logvar.exp() computes element-wise exponential\n",
    "        - torch.sum() sums all elements\n",
    "    \"\"\"\n",
    "    # ============ YOUR CODE HERE ============\n",
    "    # TODO: Implement the 3 steps above\n",
    "\n",
    "    raise NotImplementedError(\"Implement the VAE loss function!\")\n",
    "    # ========================================="
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_37_todo2_after",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo2 After\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_37_todo2_after.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verification for TODO 2 ---\n",
    "# Uncomment after implementing:\n",
    "\n",
    "# # Create known inputs\n",
    "# test_x = torch.tensor([[0.8, 0.2, 0.5], [0.1, 0.9, 0.3]])\n",
    "# test_x_recon = torch.tensor([[0.7, 0.3, 0.6], [0.2, 0.8, 0.4]])\n",
    "# test_mu = torch.tensor([[0.5, -0.5], [1.0, 0.0]])\n",
    "# test_logvar = torch.tensor([[0.0, 0.0], [0.5, -0.5]])\n",
    "#\n",
    "# total, recon, kl = vae_loss_todo(test_x_recon, test_x, test_mu, test_logvar)\n",
    "#\n",
    "# # Check against reference implementation\n",
    "# ref_recon = F.binary_cross_entropy(test_x_recon, test_x, reduction='sum')\n",
    "# ref_kl = -0.5 * torch.sum(1 + test_logvar - test_mu.pow(2) - test_logvar.exp())\n",
    "#\n",
    "# print(f\"Your recon loss:     {recon.item():.4f}\")\n",
    "# print(f\"Reference recon:     {ref_recon.item():.4f}\")\n",
    "# print(f\"Your KL loss:        {kl.item():.4f}\")\n",
    "# print(f\"Reference KL:        {ref_kl.item():.4f}\")\n",
    "# print(f\"Your total:          {total.item():.4f}\")\n",
    "# print(f\"Reference total:     {(ref_recon + ref_kl).item():.4f}\")\n",
    "#\n",
    "# assert torch.allclose(recon, ref_recon, atol=1e-4), \"Reconstruction loss mismatch!\"\n",
    "# assert torch.allclose(kl, ref_kl, atol=1e-4), \"KL divergence mismatch!\"\n",
    "# print(\"\\nTODO 2: PASSED!\")"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_38_training_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Training Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_38_training_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Training\n",
    "\n",
    "Now let us train our VAE. We will track three losses separately â€” total, reconstruction, and KL â€” so we can observe the balance between them. This balance is one of the most important things to monitor when training a VAE."
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_39_training_setup_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Training Setup Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_39_training_setup_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "torch.manual_seed(42)\n",
    "model = VAE(input_dim=784, hidden_dim=512, latent_dim=2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"=\" * 55)\n",
    "print(\"           VAE TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"  Model:         VAE (latent_dim=2)\")\n",
    "print(f\"  Parameters:    {total_params:,}\")\n",
    "print(f\"  Optimizer:     Adam (lr=1e-3)\")\n",
    "print(f\"  Batch size:    128\")\n",
    "print(f\"  Epochs:        20\")\n",
    "print(f\"  Device:        {device}\")\n",
    "print(f\"  Dataset:       MNIST (60,000 training images)\")\n",
    "print(\"=\" * 55)"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_40_recon_snapshots_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Recon Snapshots Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_40_recon_snapshots_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also save sample reconstructions at regular intervals during training, so we can visualize how the model improves over time."
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_41_training_loop_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Training Loop Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_41_training_loop_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a fixed batch of test images for tracking reconstruction quality\n",
    "fixed_test_images, fixed_test_labels = next(iter(test_loader))\n",
    "fixed_test_images = fixed_test_images[:16].to(device)  # 16 images for visualization\n",
    "fixed_test_flat = fixed_test_images.view(-1, 784)\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 20\n",
    "history = {\n",
    "    'total_loss': [], 'recon_loss': [], 'kl_loss': []\n",
    "}\n",
    "reconstruction_snapshots = {}  # Save reconstructions at certain epochs\n",
    "\n",
    "print(f\"{'Epoch':>5} | {'Total Loss':>10} | {'Recon Loss':>10} | {'KL Loss':>10} | {'Recon %':>7}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_total = 0\n",
    "    epoch_recon = 0\n",
    "    epoch_kl = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    for batch_images, _ in train_loader:\n",
    "        # Flatten images: (B, 1, 28, 28) -> (B, 784)\n",
    "        batch_images = batch_images.view(-1, 784).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        x_recon, mu, logvar = model(batch_images)\n",
    "\n",
    "        # Compute loss\n",
    "        total, recon, kl = vae_loss(x_recon, batch_images, mu, logvar)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track losses (normalize by batch size for cleaner numbers)\n",
    "        epoch_total += total.item() / len(batch_images)\n",
    "        epoch_recon += recon.item() / len(batch_images)\n",
    "        epoch_kl += kl.item() / len(batch_images)\n",
    "        n_batches += 1\n",
    "\n",
    "    # Average over batches\n",
    "    avg_total = epoch_total / n_batches\n",
    "    avg_recon = epoch_recon / n_batches\n",
    "    avg_kl = epoch_kl / n_batches\n",
    "\n",
    "    history['total_loss'].append(avg_total)\n",
    "    history['recon_loss'].append(avg_recon)\n",
    "    history['kl_loss'].append(avg_kl)\n",
    "\n",
    "    recon_pct = avg_recon / avg_total * 100\n",
    "\n",
    "    print(f\"{epoch:>5d} | {avg_total:>10.2f} | {avg_recon:>10.2f} | {avg_kl:>10.2f} | {recon_pct:>6.1f}%\")\n",
    "\n",
    "    # Save reconstruction snapshots at key epochs\n",
    "    if epoch in [1, 5, 10, 15, 20]:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            recon_snap, _, _ = model(fixed_test_flat)\n",
    "            reconstruction_snapshots[epoch] = recon_snap.cpu().view(-1, 1, 28, 28)\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Final total loss: {history['total_loss'][-1]:.2f}\")"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_42_visualize_recon_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Visualize Recon Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_42_visualize_recon_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize how reconstructions improve across training. This gives us a concrete sense of what the model is learning at each stage."
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_43_visualize_recon_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Visualize Recon Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_43_visualize_recon_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstruction quality across training\n",
    "snapshot_epochs = sorted(reconstruction_snapshots.keys())\n",
    "n_show = 8  # Number of images to show\n",
    "\n",
    "fig, axes = plt.subplots(len(snapshot_epochs) + 1, n_show, figsize=(14, 3 * (len(snapshot_epochs) + 1)))\n",
    "fig.suptitle('Reconstruction Quality Across Training', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "# Row 0: Original images\n",
    "for j in range(n_show):\n",
    "    axes[0, j].imshow(fixed_test_images[j].cpu().squeeze(), cmap='gray')\n",
    "    axes[0, j].axis('off')\n",
    "    if j == 0:\n",
    "        axes[0, j].set_ylabel('Original', fontsize=12, fontweight='bold', rotation=0, labelpad=60, va='center')\n",
    "\n",
    "# Remaining rows: Reconstructions at each snapshot\n",
    "for i, ep in enumerate(snapshot_epochs):\n",
    "    for j in range(n_show):\n",
    "        axes[i+1, j].imshow(reconstruction_snapshots[ep][j].squeeze(), cmap='gray')\n",
    "        axes[i+1, j].axis('off')\n",
    "        if j == 0:\n",
    "            axes[i+1, j].set_ylabel(f'Epoch {ep}', fontsize=12, fontweight='bold', rotation=0, labelpad=60, va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_44_recon_progression",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Recon Progression\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_44_recon_progression.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the progression:\n",
    "- **Epoch 1:** The reconstructions are blurry blobs â€” the model has barely learned anything.\n",
    "- **Epoch 5:** Rough digit shapes emerge. The model is learning the most important structures.\n",
    "- **Epoch 10-15:** Digits become recognizable. Details sharpen as the model refines its latent representations.\n",
    "- **Epoch 20:** Reconstructions are reasonably clear, though still softer than the originals. This \"blurriness\" is a well-known characteristic of VAEs â€” a consequence of the probabilistic decoding."
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_45_results_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Results Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_45_results_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Results\n",
    "\n",
    "### 7.1 Training Curves\n",
    "\n",
    "Let us plot the three loss components together. The relationship between reconstruction loss and KL divergence tells us a lot about what the model has learned."
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_46_training_curves_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Training Curves Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_46_training_curves_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves: total, reconstruction, and KL loss\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "epochs_range = range(1, EPOCHS + 1)\n",
    "\n",
    "# Plot 1: Total loss\n",
    "axes[0].plot(epochs_range, history['total_loss'], color='#1565C0', linewidth=2.5, marker='o', markersize=4)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss (per sample)', fontsize=12)\n",
    "axes[0].set_title('Total Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Reconstruction vs KL (stacked or overlaid)\n",
    "axes[1].plot(epochs_range, history['recon_loss'], color='#E65100', linewidth=2.5, marker='s', markersize=4, label='Reconstruction (BCE)')\n",
    "axes[1].plot(epochs_range, history['kl_loss'], color='#2E7D32', linewidth=2.5, marker='^', markersize=4, label='KL Divergence')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss (per sample)', fontsize=12)\n",
    "axes[1].set_title('Loss Components', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: KL as percentage of total\n",
    "kl_pct = [kl / total * 100 for kl, total in zip(history['kl_loss'], history['total_loss'])]\n",
    "axes[2].plot(epochs_range, kl_pct, color='#6A1B9A', linewidth=2.5, marker='D', markersize=4)\n",
    "axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "axes[2].set_ylabel('KL / Total (%)', fontsize=12)\n",
    "axes[2].set_title('KL Divergence Share', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].axhline(y=kl_pct[-1], color='gray', linestyle='--', alpha=0.5)\n",
    "axes[2].annotate(f'{kl_pct[-1]:.1f}%', xy=(EPOCHS, kl_pct[-1]),\n",
    "                 fontsize=10, fontweight='bold', color='#6A1B9A',\n",
    "                 xytext=(EPOCHS - 3, kl_pct[-1] + 2))\n",
    "\n",
    "plt.suptitle('VAE Training Dynamics', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_47_curves_analysis",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Curves Analysis\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_47_curves_analysis.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to notice about these curves:\n",
    "\n",
    "- **The KL divergence increases during early training.** At initialization, the encoder outputs are near zero (close to the prior), so KL is low. As the model learns to use the latent space meaningfully, the encoder distributions move away from the prior, and KL rises. This is healthy â€” it means the model is learning to encode information.\n",
    "\n",
    "- **Reconstruction loss drops rapidly then plateaus.** The biggest visual improvements happen in the first few epochs. Later epochs mainly refine details.\n",
    "\n",
    "- **The KL share stabilizes.** In a healthy VAE, the KL term settles at a relatively small fraction of the total loss (typically 5-20%). If KL were zero, it would mean the encoder ignores the input entirely (posterior collapse). If KL dominated, it would mean the model cannot reconstruct anything."
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_48_recon_quality_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Recon Quality Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_48_recon_quality_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Reconstruction Quality\n",
    "\n",
    "Let us do a thorough comparison: 16 test images side-by-side with their reconstructions."
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_49_recon_quality_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Recon Quality Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_49_recon_quality_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction comparison on test set\n",
    "model.eval()\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "test_flat = test_images[:16].view(-1, 784).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    recons, _, _ = model(test_flat)\n",
    "    recons = recons.view(-1, 1, 28, 28).cpu()\n",
    "\n",
    "fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "fig.suptitle('Original (Top) vs Reconstruction (Bottom) â€” 16 Test Digits',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "for i in range(16):\n",
    "    row = (i // 8) * 2\n",
    "    col = i % 8\n",
    "\n",
    "    # Original\n",
    "    axes[row, col].imshow(test_images[i].squeeze(), cmap='gray')\n",
    "    axes[row, col].axis('off')\n",
    "    axes[row, col].set_title(f'{test_labels[i].item()}', fontsize=10, color='#1565C0')\n",
    "\n",
    "    # Reconstruction\n",
    "    axes[row+1, col].imshow(recons[i].squeeze(), cmap='gray')\n",
    "    axes[row+1, col].axis('off')\n",
    "\n",
    "# Add row labels\n",
    "axes[0, 0].set_ylabel('Original', fontsize=12, fontweight='bold', rotation=0, labelpad=55, va='center')\n",
    "axes[1, 0].set_ylabel('VAE', fontsize=12, fontweight='bold', rotation=0, labelpad=55, va='center')\n",
    "axes[2, 0].set_ylabel('Original', fontsize=12, fontweight='bold', rotation=0, labelpad=55, va='center')\n",
    "axes[3, 0].set_ylabel('VAE', fontsize=12, fontweight='bold', rotation=0, labelpad=55, va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_50_blurriness_explain",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Blurriness Explain\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_50_blurriness_explain.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reconstructions capture the overall structure of each digit â€” the right number, roughly the right style, the correct position. But they are noticeably softer and blurrier than the originals. This is not a bug; it is a fundamental property of VAEs.\n",
    "\n",
    "**Why the blurriness?** The decoder must produce an output that is a good reconstruction for *any* latent vector in the neighborhood of the encoded point (because of the noise injected by reparameterization). It hedges its bets by producing the average of plausible outputs, which looks blurry. This is the price we pay for having a smooth, sample-able latent space.\n",
    "\n",
    "We will explore techniques to address this blurriness in Notebook 4."
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_51_final_output_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Final Output Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_51_final_output_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Final Output\n",
    "\n",
    "Let us create the definitive figure for this notebook: a beautiful grid comparing originals and reconstructions, plus a summary of what we built."
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_52_final_recon_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Final Recon Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_52_final_recon_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final reconstruction comparison grid\n",
    "model.eval()\n",
    "\n",
    "# Get diverse digits: pick one of each 0-9\n",
    "final_images = []\n",
    "final_labels = []\n",
    "for digit in range(10):\n",
    "    for img, label in test_dataset:\n",
    "        if label == digit:\n",
    "            final_images.append(img)\n",
    "            final_labels.append(label)\n",
    "            break\n",
    "\n",
    "final_images = torch.stack(final_images).to(device)\n",
    "final_flat = final_images.view(-1, 784)\n",
    "\n",
    "with torch.no_grad():\n",
    "    final_recons, _, _ = model(final_flat)\n",
    "    final_recons = final_recons.view(-1, 1, 28, 28).cpu()\n",
    "\n",
    "fig, axes = plt.subplots(2, 10, figsize=(16, 4))\n",
    "fig.suptitle('VAE Reconstructions â€” One of Each Digit (0-9)', fontsize=16, fontweight='bold', y=1.05)\n",
    "\n",
    "for i in range(10):\n",
    "    axes[0, i].imshow(final_images[i].cpu().squeeze(), cmap='gray')\n",
    "    axes[0, i].set_title(str(i), fontsize=12, fontweight='bold')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    axes[1, i].imshow(final_recons[i].squeeze(), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Original', fontsize=12, fontweight='bold', rotation=0, labelpad=55, va='center')\n",
    "axes[1, 0].set_ylabel('VAE', fontsize=12, fontweight='bold', rotation=0, labelpad=55, va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_53_save_model",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Save Model\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_53_save_model.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model for Notebook 4\n",
    "torch.save(model.state_dict(), 'vae_mnist.pth')\n",
    "print(f\"Model saved to 'vae_mnist.pth'\")\n",
    "print(f\"File size: {sum(p.numel() for p in model.parameters()) * 4 / 1024:.1f} KB\")"
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_54_summary_print",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Summary Print\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_54_summary_print.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 65)\n",
    "print(\"  Congratulations! You have built a Variational Autoencoder\")\n",
    "print(\"  from scratch!\")\n",
    "print(\"=\" * 65)\n",
    "print()\n",
    "print(\"  What we built:\")\n",
    "print(\"    - Encoder: 784 -> 512 -> 256 -> (mu, logvar) with 2 latent dims\")\n",
    "print(\"    - Decoder: 2 -> 256 -> 512 -> 784 with sigmoid output\")\n",
    "print(\"    - Reparameterization: z = mu + sigma * epsilon\")\n",
    "print(\"    - Loss: BCE reconstruction + KL divergence\")\n",
    "print()\n",
    "print(f\"  Training results:\")\n",
    "print(f\"    - Final total loss:  {history['total_loss'][-1]:.2f}\")\n",
    "print(f\"    - Final recon loss:  {history['recon_loss'][-1]:.2f}\")\n",
    "print(f\"    - Final KL loss:     {history['kl_loss'][-1]:.2f}\")\n",
    "print()\n",
    "print(\"  The model is saved to 'vae_mnist.pth' for Notebook 4.\")\n",
    "print()\n",
    "print(\"=\" * 65)"
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_55_reflection_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Reflection Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_55_reflection_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "\n",
    "Take a moment to think about what we have built.\n",
    "\n",
    "**1. Why log-variance and not variance directly?**\n",
    "\n",
    "We output $\\log(\\sigma^2)$ instead of $\\sigma^2$ because the logarithm maps the positive-only variance to the full real line, which neural networks handle naturally. To recover $\\sigma$ for the reparameterization trick, we compute $\\exp(0.5 \\cdot \\log(\\sigma^2))$. This exponential guarantees positivity without any additional constraints. It also provides better numerical stability â€” very small and very large variances are represented equally well in log-space.\n",
    "\n",
    "**2. Why does the KL term prevent posterior collapse... but also risk it?**\n",
    "\n",
    "The KL term penalizes the encoder for deviating from the prior $\\mathcal{N}(0, I)$. If the KL weight is too strong (or the decoder is too powerful), the encoder might find it easier to just output $\\mu = 0, \\log(\\sigma^2) = 0$ for every input, achieving zero KL. This is *posterior collapse* â€” the latent space carries no information, and the decoder must generate everything from scratch. Monitoring the KL loss during training helps catch this: if KL drops to near zero while reconstruction stays high, the model has collapsed.\n",
    "\n",
    "**3. Why sum reduction instead of mean?**\n",
    "\n",
    "With `reduction='sum'`, the reconstruction loss sums over all 784 pixels, producing a large number. The KL loss sums over 2 latent dimensions, producing a smaller number. If we used `reduction='mean'` for reconstruction (dividing by 784), the KL term would be relatively much larger and would dominate. The sum keeps the two terms at natural scales where the model balances them well."
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_56_next_steps",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Next Steps\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_56_next_steps.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Comes Next â€” Notebook 4\n",
    "\n",
    "We have trained a VAE, but we have only used it for reconstruction so far. The real power of a VAE is **generation** â€” creating new images that have never been seen before. In Notebook 4, we will:\n",
    "\n",
    "1. **Explore the latent space** â€” Visualize where each digit class lives in the 2D latent space\n",
    "2. **Generate new digits** â€” Sample from $\\mathcal{N}(0, I)$ and decode to create novel images\n",
    "3. **Interpolate between digits** â€” Walk smoothly through latent space and watch digits morph\n",
    "4. **Build a latent space grid** â€” Systematically sample the 2D space to see what the decoder has learned at every location\n",
    "\n",
    "The model we saved (`vae_mnist.pth`) will be loaded directly into Notebook 4, so everything we trained here carries forward."
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_57_closing_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Closing Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_57_closing_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_58_closing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Wrap-Up: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_58_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Next up: Notebook 4 â€” Exploring the Latent Space\")\n",
    "print()\n",
    "print(\"We will load the model we just saved and use it to:\")\n",
    "print(\"  1. Visualize the latent space (where do 0s, 1s, 2s... live?)\")\n",
    "print(\"  2. Generate brand-new digits from random noise\")\n",
    "print(\"  3. Interpolate between digits (watch a 3 morph into an 8)\")\n",
    "print(\"  4. Build a complete grid of the latent space\")\n",
    "print()\n",
    "print(\"See you in Notebook 4!\")"
   ],
   "id": "cell_51"
  }
 ]
}