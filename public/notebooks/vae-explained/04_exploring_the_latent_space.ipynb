{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Exploring the Latent Space â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"17rFuCNZUUY1xHrMq1WTamV-JWh_IDZe8\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/04_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_05_imports_device",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Imports Device\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_05_imports_device.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_02_why_it_matters",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_02_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Latent Space\n",
    "\n",
    "*Part 4 of the Vizuara series on Variational Autoencoders From Scratch*\n",
    "*Estimated time: 45 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "We have built the machine. We derived the ELBO, implemented the reparameterization trick, and trained a working VAE on MNIST. The loss went down, the reconstructions looked reasonable.\n",
    "\n",
    "But here is the thing: **training loss is just a number.** The real question is: *what has the model actually learned?*\n",
    "\n",
    "In this notebook, we are going to open the hood and look inside. Specifically, we will:\n",
    "\n",
    "1. **Visualize the latent space** -- see how the VAE organizes different digits into a coherent 2D map\n",
    "2. **Generate brand-new digits** -- sample random points from the latent space and watch the decoder turn them into handwritten digits that never existed before\n",
    "3. **Morph between digits** -- smoothly interpolate from a 3 to an 8 and see every intermediate shape along the way\n",
    "4. **Map the entire manifold** -- create a sweeping grid over the 2D latent space and decode every point, producing one of the most beautiful visualizations in all of machine learning\n",
    "\n",
    "This is the payoff. This is where everything clicks."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_03_building_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Building Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_03_building_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Think of the latent space as a **map**. Not a map of a physical place, but a map of *digit styles*.\n",
    "\n",
    "Every location on this map corresponds to a specific type of handwritten digit. If you walk north on the map, maybe the digits get rounder. Walk east, and maybe they develop sharper strokes. Walk from the \"3 region\" to the \"8 region,\" and you see a smooth, continuous transition -- first the 3 grows a top loop, then fills in, until it becomes an 8.\n",
    "\n",
    "This is the VAE's superpower. A regular autoencoder would scatter digits randomly across the space, with dead zones in between that decode to garbage. But the VAE, thanks to the KL divergence term, is forced to organize everything smoothly around the origin. **Every** point on the map decodes to something meaningful.\n",
    "\n",
    "Let us verify this claim with actual experiments."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_04_setup_vae",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Setup Vae\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_04_setup_vae.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup -- Building and Training the VAE\n",
    "\n",
    "Since each notebook in this series must run independently in Google Colab, we will include the complete VAE model code and train it from scratch on MNIST. This takes roughly 2-3 minutes on a Colab T4 GPU.\n",
    "\n",
    "If you have already completed Notebook 3, the code below will look familiar -- it is the same architecture and loss function."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"Device: {device}\")"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_06_model_definition_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Model Definition Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_06_model_definition_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model Definition\n",
    "\n",
    "We use a 2-dimensional latent space so that we can directly visualize it as a scatter plot. The encoder outputs $\\mu$ and $\\log(\\sigma^2)$ for each input, and the decoder maps a 2D latent code back to a 784-dimensional pixel-probability vector."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_07_model_definition_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Model Definition Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_07_model_definition_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim=512, latent_dim=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 256)\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=2, hidden_dim=512, output_dim=784):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = F.relu(self.fc1(z))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return torch.sigmoid(self.fc3(h))\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim=512, latent_dim=2):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "\n",
    "def vae_loss(x_recon, x, mu, logvar):\n",
    "    \"\"\"ELBO loss = reconstruction (BCE) + KL divergence.\"\"\"\n",
    "    recon_loss = F.binary_cross_entropy(x_recon, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_loss\n",
    "\n",
    "\n",
    "print(\"Model classes defined.\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_08_data_loading_training_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Data Loading Training Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_08_data_loading_training_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Loading and Training"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_09_training_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Training Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_09_training_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=128, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "model = VAE(latent_dim=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "print(\"Training VAE on MNIST (20 epochs)...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    for data, _ in train_loader:\n",
    "        data = data.view(-1, 784).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, mu, logvar = model(data)\n",
    "        loss = vae_loss(x_recon, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"  Epoch {epoch+1:2d}/20  |  Loss: {avg_loss:.2f}\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"Training complete!\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_10_sanity_check_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Sanity Check Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_10_sanity_check_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do a quick sanity check before diving into exploration. We will reconstruct a few test images to confirm the model is working properly."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_11_sanity_check_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Sanity Check Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_11_sanity_check_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check: show original vs reconstruction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_data, _ = next(iter(test_loader))\n",
    "    sample_data = sample_data[:8].view(-1, 784).to(device)\n",
    "    recon, _, _ = model(sample_data)\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(14, 3.5))\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(sample_data[i].cpu().view(28, 28), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    axes[1, i].imshow(recon[i].cpu().view(28, 28), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Original', fontsize=12, rotation=0, labelpad=60)\n",
    "axes[1, 0].set_ylabel('Reconstructed', fontsize=12, rotation=0, labelpad=60)\n",
    "fig.suptitle('Sanity Check: Original vs. Reconstructed Digits', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Reconstructions look reasonable -- we are ready to explore!\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_12_explore_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Explore Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_12_explore_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let Us Explore -- Component by Component\n",
    "\n",
    "Now for the fun part. We have a trained VAE with a 2-dimensional latent space. Let us systematically explore what this model has learned."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_13_latent_scatter_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Latent Scatter Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_13_latent_scatter_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Latent Space Scatter Plot\n",
    "\n",
    "The first thing we want to know is: **how does the VAE organize different digits in latent space?**\n",
    "\n",
    "We will pass every image in the 10,000-image test set through the encoder, extract the mean $\\mu$ (ignoring the variance for visualization purposes), and plot each point colored by its true digit label.\n",
    "\n",
    "If the VAE has learned a meaningful representation, we should see digits of the same class clustering together, and digits that look visually similar (like 4 and 9, or 3 and 8) placed near each other."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_14_encode_test_set",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Encode Test Set\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_14_encode_test_set.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the entire test set\n",
    "model.eval()\n",
    "all_mu = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        data = data.view(-1, 784).to(device)\n",
    "        mu, _ = model.encoder(data)\n",
    "        all_mu.append(mu.cpu())\n",
    "        all_labels.append(labels)\n",
    "\n",
    "all_mu = torch.cat(all_mu, dim=0).numpy()\n",
    "all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "\n",
    "print(f\"Encoded {len(all_mu)} test images into 2D latent space.\")\n",
    "print(f\"Latent code range: [{all_mu.min():.2f}, {all_mu.max():.2f}]\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_15_latent_scatter_plot_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Latent Scatter Plot Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_15_latent_scatter_plot_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of the latent space\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    all_mu[:, 0], all_mu[:, 1],\n",
    "    c=all_labels, cmap='tab10',\n",
    "    s=3, alpha=0.5, edgecolors='none'\n",
    ")\n",
    "\n",
    "cbar = plt.colorbar(scatter, ax=ax, ticks=range(10))\n",
    "cbar.set_label('Digit Class', fontsize=12)\n",
    "cbar.ax.set_yticklabels([str(i) for i in range(10)])\n",
    "\n",
    "ax.set_xlabel('$z_1$', fontsize=14)\n",
    "ax.set_ylabel('$z_2$', fontsize=14)\n",
    "ax.set_title('VAE Latent Space â€” 10,000 Test Digits', fontsize=15, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_16_latent_scatter_explanation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Latent Scatter Explanation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_16_latent_scatter_explanation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to study this plot carefully. Notice several things:\n",
    "\n",
    "- **Each digit class forms a recognizable cluster.** The 0s live in one region, the 1s in another, the 7s in yet another.\n",
    "- **Visually similar digits are placed close together.** Look for 4 and 9 -- they often overlap, which makes sense because a sloppy 4 can look very much like a 9. Similarly, 3 and 8 tend to be neighbors.\n",
    "- **The overall distribution is roughly centered at the origin** and approximately Gaussian-shaped. This is the KL divergence doing its job -- it pushes all encoder distributions toward $\\mathcal{N}(0, I)$.\n",
    "- **There are smooth transitions between clusters**, not hard boundaries. This is what makes generation and interpolation work.\n",
    "\n",
    "This is a *structured, meaningful* latent space. Proximity implies similarity."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_17_generate_digits_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Generate Digits Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_17_generate_digits_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Generating New Digits\n",
    "\n",
    "Now for generation. The beauty of a VAE is that we can create digits that have never existed before. The recipe is simple:\n",
    "\n",
    "1. Sample a random vector $z \\sim \\mathcal{N}(0, I)$\n",
    "2. Pass it through the decoder\n",
    "3. Out comes a new digit\n",
    "\n",
    "Because the KL term trained the encoder to place latent codes near $\\mathcal{N}(0, I)$, random samples from this distribution will land in meaningful regions of the latent space."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_18_generate_digits_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Generate Digits Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_18_generate_digits_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new digits from random latent codes\n",
    "model.eval()\n",
    "n_samples = 25  # 5x5 grid\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Sample z from standard normal\n",
    "    z_random = torch.randn(n_samples, 2).to(device)\n",
    "    generated = model.decoder(z_random).cpu().view(-1, 28, 28)\n",
    "\n",
    "fig, axes = plt.subplots(5, 5, figsize=(8, 8))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        idx = i * 5 + j\n",
    "        axes[i, j].imshow(generated[idx], cmap='gray')\n",
    "        axes[i, j].axis('off')\n",
    "\n",
    "fig.suptitle('Generated Digits â€” Sampled from $z \\\\sim \\\\mathcal{N}(0, I)$',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each of these digits was generated from a random 2D latent code.\")\n",
    "print(\"They never existed in the training set -- the VAE invented them.\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_19_generate_digits_explanation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Generate Digits Explanation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_19_generate_digits_explanation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the diversity. Some digits are cleaner than others, and some might be ambiguous -- is that a 4 or a 9? But the remarkable thing is that they are all *plausible* handwritten digits, synthesized from nothing but random noise passed through a learned decoder.\n",
    "\n",
    "This is the power of a well-organized latent space. **Any** random point decodes to something meaningful."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_20_interpolation_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Interpolation Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_20_interpolation_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Latent Space Interpolation\n",
    "\n",
    "This is one of the most elegant demonstrations in generative modeling. We will take two specific digits -- a **3** and an **8** -- encode them into latent space, and then walk along the straight line connecting them, decoding every step along the way.\n",
    "\n",
    "If the latent space is truly smooth and continuous, we should see a gradual transformation with no sudden jumps, no garbage, and no dead zones."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_21_find_digits_encode",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Find Digits Encode\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_21_find_digits_encode.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a nice 3 and a nice 8 from the test set\n",
    "model.eval()\n",
    "\n",
    "# Get a specific 3 and 8\n",
    "digit_3_img = None\n",
    "digit_8_img = None\n",
    "\n",
    "for data, labels in test_loader:\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == 3 and digit_3_img is None:\n",
    "            digit_3_img = data[i]\n",
    "        if labels[i] == 8 and digit_8_img is None:\n",
    "            digit_8_img = data[i]\n",
    "        if digit_3_img is not None and digit_8_img is not None:\n",
    "            break\n",
    "    if digit_3_img is not None and digit_8_img is not None:\n",
    "        break\n",
    "\n",
    "# Encode both digits\n",
    "with torch.no_grad():\n",
    "    x_3 = digit_3_img.view(1, 784).to(device)\n",
    "    x_8 = digit_8_img.view(1, 784).to(device)\n",
    "    mu_3, _ = model.encoder(x_3)\n",
    "    mu_8, _ = model.encoder(x_8)\n",
    "\n",
    "print(f\"Latent code for digit 3: z = [{mu_3[0, 0]:.3f}, {mu_3[0, 1]:.3f}]\")\n",
    "print(f\"Latent code for digit 8: z = [{mu_8[0, 0]:.3f}, {mu_8[0, 1]:.3f}]\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_22_interpolation_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Interpolation Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_22_interpolation_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate between the two latent codes\n",
    "n_steps = 12\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Linear interpolation: z(t) = (1-t)*z_3 + t*z_8\n",
    "    alphas = torch.linspace(0, 1, n_steps)\n",
    "    z_interp = torch.stack([\n",
    "        (1 - a) * mu_3 + a * mu_8 for a in alphas\n",
    "    ]).squeeze(1).to(device)\n",
    "\n",
    "    # Decode every interpolated point\n",
    "    interp_images = model.decoder(z_interp).cpu().view(-1, 28, 28)\n",
    "\n",
    "# Display as a filmstrip\n",
    "fig, axes = plt.subplots(1, n_steps, figsize=(18, 2.5))\n",
    "for i in range(n_steps):\n",
    "    axes[i].imshow(interp_images[i], cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[i].set_title('3', fontsize=13, fontweight='bold', color='#2196F3')\n",
    "    elif i == n_steps - 1:\n",
    "        axes[i].set_title('8', fontsize=13, fontweight='bold', color='#F44336')\n",
    "    else:\n",
    "        axes[i].set_title(f'{alphas[i]:.2f}', fontsize=9, color='gray')\n",
    "\n",
    "fig.suptitle('Latent Space Interpolation: 3 $\\\\rightarrow$ 8', fontsize=14, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice the smooth, gradual transition -- no jumps, no garbage.\")\n",
    "print(\"The 3 grows a top loop, then fills in to become an 8.\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_23_interpolation_explanation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Interpolation Explanation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_23_interpolation_explanation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is genuinely beautiful. Watch the progression carefully:\n",
    "\n",
    "- The **3** starts with its two open curves\n",
    "- Gradually, the top curve begins to close\n",
    "- The shape fills in, gaining a rounder body\n",
    "- By the end, it has become a full **8**\n",
    "\n",
    "Every intermediate image is a plausible digit -- there are no sudden switches, no nonsense, no noise artifacts. This tells us that the VAE has learned a *continuous manifold* in latent space, where nearby points produce similar images.\n",
    "\n",
    "**Why would this fail with a regular autoencoder?** In a regular autoencoder, the 3 and the 8 might be encoded as isolated points with nothing meaningful in between. Walking along the line connecting them would pass through dead zones -- regions the decoder has never seen during training -- producing garbage output."
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_24_manifold_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Manifold Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_24_manifold_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The Digit Manifold\n",
    "\n",
    "This is the crown jewel of VAE visualization. We are going to create a systematic grid over the entire 2D latent space, decode every single grid point, and assemble the results into one large composite image.\n",
    "\n",
    "The result is a *map of all possible digits* that the decoder can produce.\n",
    "\n",
    "We will sample a 15x15 grid spanning $[-3, 3]$ in both dimensions. Why $[-3, 3]$? Because the KL term encourages the latent distribution to be close to $\\mathcal{N}(0, I)$, which means most of the probability mass lives within about 3 standard deviations of the origin."
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_25_manifold_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Manifold Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_25_manifold_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the digit manifold\n",
    "model.eval()\n",
    "n = 15  # 15x15 grid\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "\n",
    "# Grid of z values spanning [-3, 3] in both dimensions\n",
    "# We use the inverse CDF (ppf) of the normal distribution for even spacing\n",
    "# in probability space, but linear spacing works well too\n",
    "grid_x = np.linspace(-3, 3, n)\n",
    "grid_y = np.linspace(3, -3, n)  # Flip y so top of image = high z2\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z = torch.tensor([[xi, yi]], dtype=torch.float32).to(device)\n",
    "            decoded = model.decoder(z).cpu().view(28, 28).numpy()\n",
    "            figure[i * digit_size:(i + 1) * digit_size,\n",
    "                   j * digit_size:(j + 1) * digit_size] = decoded\n",
    "\n",
    "# Display\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.imshow(figure, cmap='gray_r', extent=[-3, 3, -3, 3])\n",
    "ax.set_xlabel('$z_1$', fontsize=16)\n",
    "ax.set_ylabel('$z_2$', fontsize=16)\n",
    "ax.set_title('The Digit Manifold â€” Every Point in Latent Space Decoded',\n",
    "             fontsize=15, fontweight='bold')\n",
    "ax.grid(True, alpha=0.15, color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"This image contains {n*n} = {n*n} decoded digits arranged on a {n}x{n} grid.\")\n",
    "print(\"Each position corresponds to a specific (z1, z2) coordinate in latent space.\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_26_manifold_explanation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Manifold Explanation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_26_manifold_explanation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spend a moment studying this manifold. It is one of the most revealing visualizations in generative modeling.\n",
    "\n",
    "**What you should notice:**\n",
    "\n",
    "- **Different digits occupy different regions.** You can find 0s, 1s, 7s, and other digit classes concentrated in specific areas of the grid.\n",
    "- **Smooth transitions.** As you move your eyes across the image, digit shapes morph gradually into each other. A 1 might slowly widen and curve until it becomes a 7, then a 9.\n",
    "- **The center is densely populated.** Near $(0, 0)$, the digits tend to be more \"average-looking\" -- the decoder is hedging across multiple possibilities.\n",
    "- **The edges get sparser.** At the fringes ($|z| > 2.5$), the decoder produces less confident outputs, because very few training images were encoded this far from the origin.\n",
    "\n",
    "This manifold demonstrates that the VAE has learned a *continuous, structured* representation where the geometry of the latent space mirrors the visual similarity of the digits."
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_27_blurry_images_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Blurry Images Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_27_blurry_images_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Why Are the Images Blurry?\n",
    "\n",
    "You may have noticed that our generated and reconstructed digits look softer and less crisp than the original MNIST images. This is not a bug -- it is a fundamental property of how VAEs work.\n",
    "\n",
    "Let us see this clearly by placing a sharp original next to its VAE reconstruction."
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_28_blurry_images_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Blurry Images Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_28_blurry_images_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sharp originals vs. blurry reconstructions\n",
    "model.eval()\n",
    "fig, axes = plt.subplots(2, 6, figsize=(14, 5))\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_data, sample_labels = next(iter(test_loader))\n",
    "    sample_data_flat = sample_data[:6].view(-1, 784).to(device)\n",
    "    recon, _, _ = model(sample_data_flat)\n",
    "\n",
    "for i in range(6):\n",
    "    # Original (sharp)\n",
    "    axes[0, i].imshow(sample_data[i].squeeze(), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    axes[0, i].set_title(f'Digit {sample_labels[i].item()}', fontsize=11)\n",
    "\n",
    "    # Reconstruction (blurry)\n",
    "    axes[1, i].imshow(recon[i].cpu().view(28, 28), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Original\\n(sharp)', fontsize=12, rotation=0, labelpad=55, va='center')\n",
    "axes[1, 0].set_ylabel('VAE\\n(blurry)', fontsize=12, rotation=0, labelpad=55, va='center')\n",
    "\n",
    "fig.suptitle('Why VAEs Produce Blurry Images', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_29_blurry_images_explanation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Blurry Images Explanation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_29_blurry_images_explanation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why does this happen?**\n",
    "\n",
    "The culprit is the **pixel-independence assumption** in the decoder. Our decoder outputs a Bernoulli probability for each pixel independently. This means the model cannot express correlations between pixels -- it must make each pixel prediction on its own.\n",
    "\n",
    "Consider a specific pixel that is black in 60% of training images of a \"7\" and white in the other 40%. What should the decoder predict?\n",
    "\n",
    "- If it predicts **black** (probability 0.9), it will be wrong 40% of the time.\n",
    "- If it predicts **white** (probability 0.9), it will be wrong 60% of the time.\n",
    "- The optimal strategy is to predict a **middle value** (probability ~0.6) -- hedging its bets.\n",
    "\n",
    "This averaging over modes produces the characteristic grayish, soft look of VAE outputs. The model literally predicts the *mean* of all possible pixel values, rather than committing to a sharp black-or-white image.\n",
    "\n",
    "This limitation motivated the development of **diffusion models**, which use a fundamentally different approach (iterative denoising) and can generate much sharper images. But the *conceptual foundation* of VAEs -- learning a latent space and sampling from it -- carries forward into virtually every modern generative model."
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_30_todo_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_30_todo_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn -- TODO Sections\n",
    "\n",
    "Now it is your turn. The two exercises below will deepen your understanding of how latent space operations work."
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_31_todo1_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo1 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_31_todo1_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1: Implement the Interpolation Function\n",
    "\n",
    "Write a function that takes two latent codes and returns a sequence of interpolated codes between them. This is the engine behind the filmstrip visualization we saw in Section 4.3."
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_32_todo1_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo1 Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_32_todo1_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_latent(z_start, z_end, n_steps=10):\n",
    "    \"\"\"\n",
    "    Linearly interpolate between two latent codes.\n",
    "\n",
    "    Args:\n",
    "        z_start: Starting latent code, shape (1, latent_dim) or (latent_dim,)\n",
    "        z_end: Ending latent code, shape (1, latent_dim) or (latent_dim,)\n",
    "        n_steps: Number of interpolation steps (including endpoints)\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (n_steps, latent_dim) with interpolated codes\n",
    "    \"\"\"\n",
    "    # Ensure 1D\n",
    "    z_start = z_start.view(-1)\n",
    "    z_end = z_end.view(-1)\n",
    "\n",
    "    # ============================================================\n",
    "    # TODO: Create a tensor of n_steps evenly spaced points\n",
    "    # between z_start and z_end (inclusive).\n",
    "    #\n",
    "    # Hint: Use torch.linspace to create interpolation weights\n",
    "    # alpha = 0, 1/(n_steps-1), 2/(n_steps-1), ..., 1\n",
    "    # Then compute z(alpha) = (1 - alpha) * z_start + alpha * z_end\n",
    "    # ============================================================\n",
    "\n",
    "    interpolated = None  # Replace with your implementation\n",
    "\n",
    "    # ============================================================\n",
    "    # END TODO\n",
    "    # ============================================================\n",
    "\n",
    "    return interpolated"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_33_todo1_verification",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo1 Verification\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_33_todo1_verification.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_36_todo2_verification",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo2 Verification\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_36_todo2_verification.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verification ---\n",
    "# Test with known values\n",
    "z_a = torch.tensor([0.0, 0.0])\n",
    "z_b = torch.tensor([4.0, 2.0])\n",
    "result = interpolate_latent(z_a, z_b, n_steps=5)\n",
    "\n",
    "print(\"Testing interpolation from [0, 0] to [4, 2] with 5 steps:\")\n",
    "if result is not None:\n",
    "    print(f\"  Shape: {result.shape} (expected: torch.Size([5, 2]))\")\n",
    "    print(f\"  First point:  {result[0].tolist()} (expected: [0.0, 0.0])\")\n",
    "    print(f\"  Middle point: {result[2].tolist()} (expected: [2.0, 1.0])\")\n",
    "    print(f\"  Last point:   {result[4].tolist()} (expected: [4.0, 2.0])\")\n",
    "\n",
    "    # Visualize: interpolate between a 1 and a 0\n",
    "    model.eval()\n",
    "    digit_1_img = None\n",
    "    digit_0_img = None\n",
    "    for data, labels in test_loader:\n",
    "        for i in range(len(labels)):\n",
    "            if labels[i] == 1 and digit_1_img is None:\n",
    "                digit_1_img = data[i]\n",
    "            if labels[i] == 0 and digit_0_img is None:\n",
    "                digit_0_img = data[i]\n",
    "            if digit_1_img is not None and digit_0_img is not None:\n",
    "                break\n",
    "        if digit_1_img is not None and digit_0_img is not None:\n",
    "            break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mu_1, _ = model.encoder(digit_1_img.view(1, 784).to(device))\n",
    "        mu_0, _ = model.encoder(digit_0_img.view(1, 784).to(device))\n",
    "        z_path = interpolate_latent(mu_1.cpu(), mu_0.cpu(), n_steps=10).to(device)\n",
    "        decoded_path = model.decoder(z_path).cpu().view(-1, 28, 28)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(16, 2.2))\n",
    "    for i in range(10):\n",
    "        axes[i].imshow(decoded_path[i], cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "    axes[0].set_title('1', fontweight='bold', color='#2196F3')\n",
    "    axes[-1].set_title('0', fontweight='bold', color='#F44336')\n",
    "    fig.suptitle('Your Interpolation: 1 $\\\\rightarrow$ 0', fontsize=13, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"If you see a smooth transition from 1 to 0, your implementation is correct!\")\n",
    "else:\n",
    "    print(\"  interpolated is None -- implement the TODO above!\")"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_34_todo2_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo2 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_34_todo2_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Generate the Digit Manifold Grid\n",
    "\n",
    "Write a function that creates the digit manifold visualization. Given the decoder, a grid size, and a range, it should decode every point on the grid and assemble the results into one large image."
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_35_todo2_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo2 Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_35_todo2_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_manifold(decoder, n=20, z_range=3.0, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate a digit manifold by decoding a grid of latent codes.\n",
    "\n",
    "    Args:\n",
    "        decoder: Trained decoder network\n",
    "        n: Grid size (n x n points)\n",
    "        z_range: Range of z values, grid spans [-z_range, z_range]\n",
    "        device: torch device\n",
    "\n",
    "    Returns:\n",
    "        numpy array of shape (n*28, n*28) containing the assembled image\n",
    "    \"\"\"\n",
    "    digit_size = 28\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "\n",
    "    # Create grid coordinates\n",
    "    grid_x = np.linspace(-z_range, z_range, n)\n",
    "    grid_y = np.linspace(z_range, -z_range, n)  # Flip y-axis\n",
    "\n",
    "    # ============================================================\n",
    "    # TODO: Fill in the figure array by decoding each grid point.\n",
    "    #\n",
    "    # For each (i, j) position in the grid:\n",
    "    #   1. Create a latent code z = [grid_x[j], grid_y[i]]\n",
    "    #   2. Pass z through the decoder to get a 784-dim output\n",
    "    #   3. Reshape to 28x28 and place it in the correct position\n",
    "    #      in the figure array\n",
    "    #\n",
    "    # Hint: Use torch.no_grad() to avoid unnecessary computation.\n",
    "    # The decoded image goes at:\n",
    "    #   figure[i*28:(i+1)*28, j*28:(j+1)*28]\n",
    "    # ============================================================\n",
    "\n",
    "    # Your code here\n",
    "\n",
    "    # ============================================================\n",
    "    # END TODO\n",
    "    # ============================================================\n",
    "\n",
    "    return figure"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verification ---\n",
    "manifold = generate_manifold(model.decoder, n=20, z_range=3.0, device=device)\n",
    "\n",
    "if manifold.max() > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    ax.imshow(manifold, cmap='gray_r', extent=[-3, 3, -3, 3])\n",
    "    ax.set_xlabel('$z_1$', fontsize=16)\n",
    "    ax.set_ylabel('$z_2$', fontsize=16)\n",
    "    ax.set_title('Your Digit Manifold (20x20 Grid)', fontsize=15, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.15, color='white')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"Manifold generated: {manifold.shape[0]}x{manifold.shape[1]} pixels\")\n",
    "    print(f\"That is {20*20} = 400 decoded digits arranged in a grid!\")\n",
    "    print(\"If you see smooth transitions across the grid, your implementation is correct!\")\n",
    "else:\n",
    "    print(\"Manifold is all zeros -- implement the TODO above!\")"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_37_gallery_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Narration: Gallery Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_37_gallery_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together -- The VAE Gallery\n",
    "\n",
    "Let us create a single composite figure that showcases everything we have explored: the latent space scatter plot, generated samples, the interpolation filmstrip, and the digit manifold, all in one place."
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_38_gallery_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Gallery Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_38_gallery_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Gallery: everything in one figure\n",
    "model.eval()\n",
    "\n",
    "fig = plt.figure(figsize=(18, 14))\n",
    "\n",
    "# ---- Panel 1: Latent Space Scatter ----\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "scatter = ax1.scatter(all_mu[:, 0], all_mu[:, 1], c=all_labels, cmap='tab10',\n",
    "                      s=2, alpha=0.4, edgecolors='none')\n",
    "cbar = plt.colorbar(scatter, ax=ax1, ticks=range(10), shrink=0.8)\n",
    "cbar.set_label('Digit')\n",
    "ax1.set_xlabel('$z_1$')\n",
    "ax1.set_ylabel('$z_2$')\n",
    "ax1.set_title('(a) Latent Space', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ---- Panel 2: Generated Samples ----\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "with torch.no_grad():\n",
    "    z_grid = torch.randn(36, 2).to(device)\n",
    "    gen_grid = model.decoder(z_grid).cpu().view(-1, 28, 28)\n",
    "\n",
    "grid_img = np.zeros((6 * 28, 6 * 28))\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        grid_img[i*28:(i+1)*28, j*28:(j+1)*28] = gen_grid[i*6+j].numpy()\n",
    "ax2.imshow(grid_img, cmap='gray')\n",
    "ax2.axis('off')\n",
    "ax2.set_title('(b) Generated Samples', fontsize=13, fontweight='bold')\n",
    "\n",
    "# ---- Panel 3: Interpolation Filmstrip ----\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "with torch.no_grad():\n",
    "    alphas_gallery = torch.linspace(0, 1, 10)\n",
    "    z_interp_gallery = torch.stack([\n",
    "        (1 - a) * mu_3 + a * mu_8 for a in alphas_gallery\n",
    "    ]).squeeze(1).to(device)\n",
    "    interp_gallery = model.decoder(z_interp_gallery).cpu().view(-1, 28, 28)\n",
    "\n",
    "strip = np.concatenate([interp_gallery[i].numpy() for i in range(10)], axis=1)\n",
    "ax3.imshow(strip, cmap='gray')\n",
    "ax3.axis('off')\n",
    "ax3.set_title('(c) Interpolation: 3 $\\\\rightarrow$ 8', fontsize=13, fontweight='bold')\n",
    "\n",
    "# ---- Panel 4: Digit Manifold ----\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "n_gallery = 12\n",
    "digit_size = 28\n",
    "manifold_gallery = np.zeros((digit_size * n_gallery, digit_size * n_gallery))\n",
    "grid_x_g = np.linspace(-3, 3, n_gallery)\n",
    "grid_y_g = np.linspace(3, -3, n_gallery)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, yi in enumerate(grid_y_g):\n",
    "        for j, xi in enumerate(grid_x_g):\n",
    "            z = torch.tensor([[xi, yi]], dtype=torch.float32).to(device)\n",
    "            decoded = model.decoder(z).cpu().view(28, 28).numpy()\n",
    "            manifold_gallery[i*digit_size:(i+1)*digit_size,\n",
    "                             j*digit_size:(j+1)*digit_size] = decoded\n",
    "\n",
    "ax4.imshow(manifold_gallery, cmap='gray_r')\n",
    "ax4.axis('off')\n",
    "ax4.set_title('(d) Digit Manifold', fontsize=13, fontweight='bold')\n",
    "\n",
    "fig.suptitle('The VAE Gallery -- Everything Our Model Has Learned',\n",
    "             fontsize=16, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Four views of the same model, four windows into the latent space.\")"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_39_final_output_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Final Output Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_39_final_output_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Output\n",
    "\n",
    "Let us produce the two signature outputs of this notebook series: a high-resolution digit manifold and a multi-pair interpolation showcase."
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_40_final_manifold_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Final Manifold Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_40_final_manifold_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Output 1: High-resolution digit manifold\n",
    "model.eval()\n",
    "n_final = 20\n",
    "digit_size = 28\n",
    "manifold_final = np.zeros((digit_size * n_final, digit_size * n_final))\n",
    "grid_x_f = np.linspace(-3, 3, n_final)\n",
    "grid_y_f = np.linspace(3, -3, n_final)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, yi in enumerate(grid_y_f):\n",
    "        for j, xi in enumerate(grid_x_f):\n",
    "            z = torch.tensor([[xi, yi]], dtype=torch.float32).to(device)\n",
    "            decoded = model.decoder(z).cpu().view(28, 28).numpy()\n",
    "            manifold_final[i*digit_size:(i+1)*digit_size,\n",
    "                           j*digit_size:(j+1)*digit_size] = decoded\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "ax.imshow(manifold_final, cmap='gray_r', extent=[-3, 3, -3, 3])\n",
    "ax.set_xlabel('$z_1$', fontsize=18)\n",
    "ax.set_ylabel('$z_2$', fontsize=18)\n",
    "ax.set_title('The Complete Digit Manifold (20x20)',\n",
    "             fontsize=17, fontweight='bold')\n",
    "ax.tick_params(labelsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"400 unique digits decoded from a uniform grid over the latent space.\")"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_41_final_interpolation_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Final Interpolation Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_41_final_interpolation_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Output 2: Multi-pair interpolation\n",
    "model.eval()\n",
    "\n",
    "# Find one example of each digit pair we want to interpolate\n",
    "pairs = [(0, 1), (2, 7), (3, 8), (4, 9), (5, 6)]\n",
    "pair_images = {}\n",
    "\n",
    "for data, labels in test_loader:\n",
    "    for i in range(len(labels)):\n",
    "        lab = labels[i].item()\n",
    "        if lab not in pair_images:\n",
    "            pair_images[lab] = data[i]\n",
    "    if len(pair_images) == 10:\n",
    "        break\n",
    "\n",
    "fig, axes = plt.subplots(len(pairs), 12, figsize=(18, 7.5))\n",
    "\n",
    "for row, (d1, d2) in enumerate(pairs):\n",
    "    with torch.no_grad():\n",
    "        mu_a, _ = model.encoder(pair_images[d1].view(1, 784).to(device))\n",
    "        mu_b, _ = model.encoder(pair_images[d2].view(1, 784).to(device))\n",
    "\n",
    "        alphas_final = torch.linspace(0, 1, 12)\n",
    "        z_path = torch.stack([\n",
    "            (1 - a) * mu_a + a * mu_b for a in alphas_final\n",
    "        ]).squeeze(1).to(device)\n",
    "        decoded_path = model.decoder(z_path).cpu().view(-1, 28, 28)\n",
    "\n",
    "    for col in range(12):\n",
    "        axes[row, col].imshow(decoded_path[col], cmap='gray')\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "    axes[row, 0].set_ylabel(f'{d1}$\\\\rightarrow${d2}', fontsize=12,\n",
    "                             rotation=0, labelpad=35, va='center')\n",
    "\n",
    "fig.suptitle('Smooth Interpolations Between Digit Pairs',\n",
    "             fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Every row shows a smooth morphing between two different digits.\")\n",
    "print(\"No jumps, no garbage, no dead zones -- just continuous transformation.\")"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_42_congratulations",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Congratulations\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_42_congratulations.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"=\" * 65)\n",
    "print(\"  Congratulations! You have built a Variational Autoencoder\")\n",
    "print(\"  completely from scratch!\")\n",
    "print(\"=\" * 65)\n",
    "print()\n",
    "print(\"  From latent variables to the ELBO to a working generative\")\n",
    "print(\"  model -- you now understand one of the most elegant\")\n",
    "print(\"  frameworks in machine learning.\")\n",
    "print()\n",
    "print(\"  Key insight: generation = learning a latent space\")\n",
    "print(\"                            + sampling from it\")\n",
    "print(\"=\" * 65)"
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_43_reflection_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Reflection Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_43_reflection_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reflection and Next Steps\n",
    "\n",
    "We have come a long way. Let us reflect on what we accomplished across all four notebooks:\n",
    "\n",
    "1. **Notebook 1**: We built the intuition -- latent variables as hidden recipes, the encoder-decoder framework, and why making it probabilistic matters.\n",
    "2. **Notebook 2**: We derived the math -- the ELBO, the KL divergence closed form, and the reparameterization trick.\n",
    "3. **Notebook 3**: We implemented everything in PyTorch and trained a working VAE on MNIST.\n",
    "4. **Notebook 4 (this one)**: We explored the latent space and saw the payoff -- structured representations, generation, interpolation, and the digit manifold."
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_44_reflection_questions",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Reflection Questions\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_44_reflection_questions.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Questions\n",
    "\n",
    "Take a few minutes to think about these:\n",
    "\n",
    "1. **Why does the manifold look different at the center vs. the edges?** What role does the KL divergence play in shaping this distribution?\n",
    "\n",
    "2. **If we increased the latent dimension from 2 to 20**, what would change? The reconstructions would improve (more capacity), but we could no longer make scatter plots or 2D manifolds. What is the trade-off between latent dimension and interpretability?\n",
    "\n",
    "3. **Why does interpolation work in the VAE's latent space but not in pixel space?** If you averaged the pixel values of a 3 and an 8 directly (without encoding), what would you get? Try it and compare."
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_45_optional_challenges",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Optional Challenges\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_45_optional_challenges.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Challenges\n",
    "\n",
    "If you want to push further, here are some exciting directions:\n",
    "\n",
    "- **Try a higher latent dimension** (e.g., 10 or 20). Train the model and compare reconstruction quality. You will not be able to visualize the latent space as a 2D scatter plot, but you can use t-SNE or UMAP to project it down to 2D for visualization.\n",
    "\n",
    "- **Add convolutional layers.** Replace the fully-connected encoder and decoder with convolutional and transposed-convolutional layers. This should improve image quality noticeably while keeping the same ELBO training objective.\n",
    "\n",
    "- **Try Fashion-MNIST or CIFAR-10.** Fashion-MNIST is a drop-in replacement for MNIST but with clothing items instead of digits. CIFAR-10 (32x32 color images) is significantly harder and will expose the blurriness limitation more clearly.\n",
    "\n",
    "- **Implement a conditional VAE (CVAE).** Pass the digit label as an additional input to both the encoder and decoder. This lets you control *which* digit to generate while the latent code controls the *style*."
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_46_bigger_picture",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Bigger Picture\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_46_bigger_picture.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bigger Picture\n",
    "\n",
    "The blurriness of VAE outputs is not a failure -- it is a *design trade-off*. By using independent pixel distributions, the model gains a tractable training objective (the ELBO) and a beautifully structured latent space. The cost is soft, averaged outputs.\n",
    "\n",
    "This trade-off directly motivated the development of **diffusion models**, which bypass the encoder-decoder bottleneck entirely and instead learn to iteratively denoise random noise into sharp images. But at their core, diffusion models still rely on the same fundamental insight that VAEs taught us:\n",
    "\n",
    "**Generation = learning a structured representation of data + sampling from it.**\n",
    "\n",
    "This idea is the thread that connects VAEs, GANs, diffusion models, normalizing flows, and even large language models. You now understand one of the most foundational frameworks in all of generative AI.\n",
    "\n",
    "Thank you for working through this series. We hope you enjoyed the journey.\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- Kingma & Welling, \"Auto-Encoding Variational Bayes\" (2013)\n",
    "- Doersch, \"Tutorial on Variational Autoencoders\" (2016)\n",
    "- Blei, Kucukelbir & McAuliffe, \"Variational Inference: A Review for Statisticians\" (2017)"
   ],
   "id": "cell_46"
  }
 ]
}