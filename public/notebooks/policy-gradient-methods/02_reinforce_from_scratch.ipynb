{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "REINFORCE from Scratch \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE from Scratch: Variance, Baselines, and Convergence\n",
    "\n",
    "*Part 2 of the Vizuara series on Policy Gradient Methods*\n",
    "*Estimated time: 50 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebook, we built a policy gradient agent. It worked \u2014 but you may have noticed that the training was noisy. The reward jumped around wildly between episodes. Sometimes the agent would seem to learn, then suddenly forget everything.\n",
    "\n",
    "This is the **variance problem** \u2014 the central challenge in policy gradient methods. It is the reason why vanilla REINFORCE took over 100,000 steps to solve CartPole, while value-based methods can do it in under 10,000.\n",
    "\n",
    "In this notebook, we will:\n",
    "- Diagnose the variance problem with visualizations\n",
    "- Build the REINFORCE algorithm from scratch\n",
    "- Implement variance reduction through baselines\n",
    "- Compare convergence speed with and without baselines\n",
    "- See Q-value variance drop by more than 3x\n",
    "\n",
    "The techniques we build here \u2014 baselines and advantage estimation \u2014 are the foundation of every modern RL algorithm including PPO."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Imagine you want to estimate the average age of people in your country. You go out and sample 100 people.\n",
    "\n",
    "If you happen to sample mostly from a retirement community, you will estimate the average age is 70. If you happen to sample mostly from a university, you will estimate it is 22. Each sample gives a wildly different answer \u2014 this is high variance.\n",
    "\n",
    "Now imagine subtracting the national average (say, 35) from each person's age before computing your estimate. The values you work with are now much smaller in magnitude: +35 for the 70-year-old, -13 for the 22-year-old. The estimate is more stable.\n",
    "\n",
    "This is exactly what a baseline does in REINFORCE. Instead of weighting actions by raw returns (which can be very large and noisy), we subtract a baseline to center the gradient around zero. The expected gradient does not change, but the variance drops dramatically.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "If every trajectory in your batch has a positive return (say, returns of 90, 95, 100, 105), the gradient says \"increase the probability of ALL actions.\" But some actions were clearly better than others. How does subtracting the mean (97.5) help differentiate between them?"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### 3.1 The REINFORCE Update Rule\n",
    "\n",
    "The REINFORCE update is:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t$$\n",
    "\n",
    "This says: for each action taken, adjust the policy parameters proportionally to the log-probability gradient times the return from that timestep forward.\n",
    "\n",
    "Let us plug in numbers. Suppose $\\alpha = 0.01$, $\\nabla_\\theta \\log \\pi = 0.8$, and $G_t = 100$:\n",
    "\n",
    "$$\\Delta\\theta = 0.01 \\times 0.8 \\times 100 = 0.8$$\n",
    "\n",
    "That is a large update! Now suppose a different trajectory has $G_t = 90$ and $\\nabla_\\theta \\log \\pi = -0.3$:\n",
    "\n",
    "$$\\Delta\\theta = 0.01 \\times (-0.3) \\times 90 = -0.27$$\n",
    "\n",
    "The updates swing between +0.8 and -0.27. High variance.\n",
    "\n",
    "### 3.2 REINFORCE with Baseline\n",
    "\n",
    "With a baseline $b(s_t)$:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot (G_t - b(s_t))$$\n",
    "\n",
    "Using $b = 95$ (the mean return):\n",
    "\n",
    "$$\\Delta\\theta_1 = 0.01 \\times 0.8 \\times (100 - 95) = 0.04$$\n",
    "$$\\Delta\\theta_2 = 0.01 \\times (-0.3) \\times (90 - 95) = 0.015$$\n",
    "\n",
    "Now the updates are +0.04 and +0.015. Both positive (both trajectories were reasonably good), but much smaller and more stable. This is exactly what we want.\n",
    "\n",
    "### 3.3 The Advantage Function\n",
    "\n",
    "The best baseline is the value function $V(s)$. The quantity $G_t - V(s_t)$ is called the **advantage**:\n",
    "\n",
    "$$A(s_t, a_t) = G_t - V(s_t)$$\n",
    "\n",
    "The advantage tells us: \"How much better was this action compared to what we expected?\" Positive advantage means better than average, negative means worse."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It \u2014 Component by Component\n",
    "\n",
    "### 4.1 Computing Discounted Returns"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "def compute_returns(rewards, gamma=GAMMA):\n",
    "    \"\"\"\n",
    "    Compute discounted returns working backwards from the end.\n",
    "    G_t = r_t + gamma * r_{t+1} + gamma^2 * r_{t+2} + ...\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    return returns\n",
    "\n",
    "# Example: rewards = [1, 1, 1, 1, 1] with gamma = 0.99\n",
    "example_rewards = [1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "example_returns = compute_returns(example_rewards)\n",
    "\n",
    "print(\"Rewards:\", example_rewards)\n",
    "print(\"Returns:\", [f\"{r:.4f}\" for r in example_returns])\n",
    "print(\"\\nNotice: earlier timesteps have higher returns because they\")\n",
    "print(\"receive more future discounted rewards.\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Policy Network"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Policy network that maps states to action logits.\"\"\"\n",
    "    def __init__(self, state_dim, n_actions, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        logits = self.forward(state)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Episode Collection"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episode(env, policy):\n",
    "    \"\"\"Run one full episode and collect experiences.\"\"\"\n",
    "    states, actions, rewards, log_probs = [], [], [], []\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        state_t = torch.as_tensor(state, dtype=torch.float32)\n",
    "        action, log_prob = policy.sample_action(state_t)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        state = next_state\n",
    "\n",
    "    return states, actions, rewards, log_probs"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Show what a trajectory looks like\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "policy = PolicyNetwork(state_dim=4, n_actions=2)\n",
    "\n",
    "states, actions, rewards, log_probs = collect_episode(env, policy)\n",
    "returns = compute_returns(rewards)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
    "\n",
    "axes[0].plot(rewards, 'g-', alpha=0.7, label='Reward per step')\n",
    "axes[0].set_ylabel('Reward')\n",
    "axes[0].set_title('Trajectory Anatomy')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(returns, 'b-', linewidth=2, label='Discounted Return G_t')\n",
    "axes[1].set_xlabel('Time Step')\n",
    "axes[1].set_ylabel('Return')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Episode length: {len(rewards)} | Total return: {returns[0]:.2f}\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 REINFORCE Training Loop"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce(env, policy, optimizer, num_episodes=500, use_baseline=False):\n",
    "    \"\"\"\n",
    "    Train using REINFORCE, optionally with a mean-return baseline.\n",
    "    \"\"\"\n",
    "    reward_history = []\n",
    "    return_variance_history = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        states, actions, rewards, log_probs = collect_episode(env, policy)\n",
    "        returns = compute_returns(rewards)\n",
    "\n",
    "        returns_t = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "        # Apply baseline: subtract mean return\n",
    "        if use_baseline:\n",
    "            baseline = returns_t.mean()\n",
    "            advantages = returns_t - baseline\n",
    "        else:\n",
    "            advantages = returns_t\n",
    "\n",
    "        # Track variance\n",
    "        return_variance_history.append(returns_t.var().item())\n",
    "\n",
    "        # Policy gradient loss\n",
    "        log_probs_t = torch.stack(log_probs)\n",
    "        loss = -(log_probs_t * advantages).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        reward_history.append(sum(rewards))\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg = np.mean(reward_history[-100:])\n",
    "            var = np.mean(return_variance_history[-100:])\n",
    "            print(f\"Ep {episode+1:4d} | Avg Reward: {avg:.1f} | Return Var: {var:.1f}\")\n",
    "\n",
    "    return reward_history, return_variance_history"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO: Implement REINFORCE with a Learned Value Baseline"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network that estimates V(s) \u2014 the expected return from state s.\n",
    "    This serves as the baseline (the critic).\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        # ============ TODO ============\n",
    "        # Create a neural network with:\n",
    "        # - Linear layer: state_dim -> hidden_dim\n",
    "        # - ReLU activation\n",
    "        # - Linear layer: hidden_dim -> 1 (single value output)\n",
    "        # ==============================\n",
    "        self.net = ???  # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state).squeeze(-1)"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "value_net = ValueNetwork(state_dim=4)\n",
    "test_state = torch.randn(4)\n",
    "value = value_net(test_state)\n",
    "assert value.shape == torch.Size([]), f\"Expected scalar output, got {value.shape}\"\n",
    "\n",
    "# Test with batch\n",
    "test_batch = torch.randn(10, 4)\n",
    "values = value_net(test_batch)\n",
    "assert values.shape == torch.Size([10]), f\"Expected batch output, got {values.shape}\"\n",
    "print(\"Correct! ValueNetwork produces scalar state-value estimates.\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Compute Advantage with the Value Baseline"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages(returns, states, value_net):\n",
    "    \"\"\"\n",
    "    Compute advantages A_t = G_t - V(s_t) using the value network.\n",
    "\n",
    "    Args:\n",
    "        returns: list of discounted returns\n",
    "        states: list of state observations\n",
    "        value_net: the value network\n",
    "\n",
    "    Returns:\n",
    "        advantages: torch tensor of advantage values\n",
    "        value_loss: MSE loss for training the value network\n",
    "    \"\"\"\n",
    "    returns_t = torch.tensor(returns, dtype=torch.float32)\n",
    "    states_t = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Get value predictions V(s) from value_net (detach for advantage, not for loss)\n",
    "    # Step 2: Compute advantages: A_t = G_t - V(s_t)   [detach V for this]\n",
    "    # Step 3: Compute value loss: MSE between returns and value predictions\n",
    "    # ==============================\n",
    "\n",
    "    advantages = ???  # YOUR CODE HERE\n",
    "    value_loss = ???  # YOUR CODE HERE\n",
    "\n",
    "    return advantages, value_loss"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "dummy_returns = [5.0, 4.0, 3.0, 2.0, 1.0]\n",
    "dummy_states = [np.random.randn(4) for _ in range(5)]\n",
    "v_net = ValueNetwork(state_dim=4)\n",
    "\n",
    "advs, v_loss = compute_advantages(dummy_returns, dummy_states, v_net)\n",
    "assert advs.shape == torch.Size([5]), f\"Expected 5 advantages, got {advs.shape}\"\n",
    "assert v_loss.shape == torch.Size([]), f\"Expected scalar loss, got {v_loss.shape}\"\n",
    "print(\"Correct! Advantage computation works.\")\n",
    "print(f\"Advantages: {advs.detach().numpy().round(2)}\")\n",
    "print(f\"Value loss: {v_loss.item():.4f}\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us train both methods and compare them head to head."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train REINFORCE without baseline\n",
    "print(\"=\" * 60)\n",
    "print(\"Training: REINFORCE (no baseline)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "env1 = gym.make(\"CartPole-v1\")\n",
    "policy1 = PolicyNetwork(state_dim=4, n_actions=2)\n",
    "opt1 = torch.optim.Adam(policy1.parameters(), lr=0.01)\n",
    "\n",
    "rewards_no_baseline, var_no_baseline = train_reinforce(\n",
    "    env1, policy1, opt1, num_episodes=500, use_baseline=False\n",
    ")\n",
    "env1.close()\n",
    "\n",
    "# Train REINFORCE with baseline\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training: REINFORCE with Baseline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "env2 = gym.make(\"CartPole-v1\")\n",
    "policy2 = PolicyNetwork(state_dim=4, n_actions=2)\n",
    "opt2 = torch.optim.Adam(policy2.parameters(), lr=0.01)\n",
    "\n",
    "rewards_with_baseline, var_with_baseline = train_reinforce(\n",
    "    env2, policy2, opt2, num_episodes=500, use_baseline=True\n",
    ")\n",
    "env2.close()"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare training curves\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Reward comparison\n",
    "window = 20\n",
    "for data, label, color in [\n",
    "    (rewards_no_baseline, 'REINFORCE', '#ef4444'),\n",
    "    (rewards_with_baseline, 'REINFORCE + Baseline', '#3b82f6')\n",
    "]:\n",
    "    if len(data) >= window:\n",
    "        smoothed = np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "        axes[0].plot(range(window-1, len(data)), smoothed, label=label, color=color, linewidth=2)\n",
    "\n",
    "axes[0].axhline(y=500, color='gray', linestyle='--', alpha=0.5, label='Max Reward')\n",
    "axes[0].set_ylabel('Episode Reward', fontsize=12)\n",
    "axes[0].set_title('REINFORCE vs REINFORCE with Baseline', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Variance comparison\n",
    "for data, label, color in [\n",
    "    (var_no_baseline, 'REINFORCE', '#ef4444'),\n",
    "    (var_with_baseline, 'REINFORCE + Baseline', '#3b82f6')\n",
    "]:\n",
    "    if len(data) >= window:\n",
    "        smoothed = np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "        axes[1].plot(range(window-1, len(data)), smoothed, label=label, color=color, linewidth=2)\n",
    "\n",
    "axes[1].set_xlabel('Episode', fontsize=12)\n",
    "axes[1].set_ylabel('Return Variance', fontsize=12)\n",
    "axes[1].set_title('Return Variance Comparison', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(\"\\n--- Convergence Statistics ---\")\n",
    "for name, data in [(\"REINFORCE\", rewards_no_baseline), (\"+ Baseline\", rewards_with_baseline)]:\n",
    "    first_500 = next((i for i, r in enumerate(data) if np.mean(data[max(0,i-20):i+1]) > 450), len(data))\n",
    "    avg_final = np.mean(data[-50:]) if len(data) >= 50 else np.mean(data)\n",
    "    print(f\"  {name:12s}: First 450+ avg at ep {first_500:4d} | Final 50-ep avg: {avg_final:.1f}\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison summary\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT SUMMARY: REINFORCE vs Baseline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Key metrics\n",
    "avg_no_bl = np.mean(rewards_no_baseline[-100:])\n",
    "avg_bl = np.mean(rewards_with_baseline[-100:])\n",
    "var_no_bl = np.mean(var_no_baseline[-100:])\n",
    "var_bl = np.mean(var_with_baseline[-100:])\n",
    "\n",
    "print(f\"\\nFinal 100-episode average reward:\")\n",
    "print(f\"  REINFORCE:            {avg_no_bl:.1f}\")\n",
    "print(f\"  REINFORCE + Baseline: {avg_bl:.1f}\")\n",
    "print(f\"\\nFinal 100-episode average return variance:\")\n",
    "print(f\"  REINFORCE:            {var_no_bl:.1f}\")\n",
    "print(f\"  REINFORCE + Baseline: {var_bl:.1f}\")\n",
    "print(f\"  Variance reduction:   {(1 - var_bl/max(var_no_bl, 1))*100:.1f}%\")\n",
    "print(f\"\\nConclusion: Baseline reduces variance and improves convergence.\")\n",
    "print(\"Congratulations! You have built REINFORCE with baseline from scratch!\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "1. Why does subtracting a baseline NOT change the expected gradient? (Hint: the baseline does not depend on the action, so its gradient contribution is zero in expectation.)\n",
    "2. If you used a very bad baseline (e.g., a constant of 1,000,000), would it help or hurt? Why?\n",
    "3. The advantage function $A(s,a) = G_t - V(s)$ can be negative. What does a negative advantage tell us about the action taken?\n",
    "\n",
    "### Optional Challenges\n",
    "1. Implement a learned value baseline (use a separate neural network to estimate $V(s)$) instead of the mean return baseline.\n",
    "2. Try normalizing the advantages to have zero mean and unit variance before computing the gradient. Does this help?\n",
    "3. Experiment with different discount factors (gamma = 0.9, 0.99, 0.999) and compare convergence."
   ],
   "id": "cell_26"
  }
 ]
}