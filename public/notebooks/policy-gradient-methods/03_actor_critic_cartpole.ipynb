{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Actor-Critic Methods \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Methods: The Student-Teacher Framework\n",
    "\n",
    "*Part 3 of the Vizuara series on Policy Gradient Methods*\n",
    "*Estimated time: 55 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In REINFORCE with baseline, we used the mean return as a simple baseline. But we can do much better. What if the baseline could **learn** to predict how good each state is?\n",
    "\n",
    "This is the Actor-Critic architecture \u2014 one of the most important ideas in modern reinforcement learning. The \"actor\" is the policy (it picks actions), and the \"critic\" is a value network (it evaluates how good each state is). Together, they form a powerful learning system.\n",
    "\n",
    "Actor-Critic methods are the foundation of PPO, the algorithm used to fine-tune ChatGPT. They are also used in robotics (learning to walk, manipulate objects) and game playing.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Build an Actor-Critic agent with separate policy and value networks\n",
    "- Train both networks simultaneously on CartPole\n",
    "- Compare Actor-Critic against vanilla REINFORCE\n",
    "- Visualize the learned value function\n",
    "- See training converge significantly faster"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Think of a student and a teacher. The student (actor) takes an exam and gives answers. The teacher (critic) grades the exam \u2014 but not just right/wrong. The teacher says \"this answer was 3 points above average\" or \"this was 2 points below average.\"\n",
    "\n",
    "This relative feedback is far more useful than raw scores. If every student scores between 80-100, saying \"you got 90\" does not tell you much. But saying \"you were 5 points above the class average\" tells you exactly how well you did relative to expectations.\n",
    "\n",
    "The advantage function is this relative feedback:\n",
    "\n",
    "$A(s, a) = Q(s, a) - V(s)$\n",
    "\n",
    "\"How much better (or worse) was action $a$ compared to what we expected in state $s$?\"\n",
    "\n",
    "The critic learns $V(s)$ (what to expect). The actor uses the advantage (actual outcome minus expectation) to improve.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "In CartPole, a state where the pole is nearly vertical is \"good\" (high V). A state where the pole is almost horizontal is \"bad\" (low V). If the actor takes an action that keeps the pole balanced from a bad state, what sign would the advantage have? What would this tell the actor?"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### 3.1 The Actor-Critic Update\n",
    "\n",
    "The actor update uses the advantage:\n",
    "\n",
    "$$\\theta_{\\text{actor}} \\leftarrow \\theta_{\\text{actor}} + \\alpha_a \\sum_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot A_t$$\n",
    "\n",
    "where $A_t = G_t - V_\\phi(s_t)$.\n",
    "\n",
    "The critic update minimizes the prediction error:\n",
    "\n",
    "$$\\theta_{\\text{critic}} \\leftarrow \\theta_{\\text{critic}} - \\alpha_c \\nabla_\\phi \\sum_t (G_t - V_\\phi(s_t))^2$$\n",
    "\n",
    "Let us plug in numbers. Suppose at time step $t$:\n",
    "- The actual return is $G_t = 15.0$\n",
    "- The critic predicts $V(s_t) = 12.0$\n",
    "- So the advantage is $A_t = 15.0 - 12.0 = 3.0$\n",
    "\n",
    "This positive advantage tells the actor: \"This action was 3 units better than expected. Increase its probability.\"\n",
    "\n",
    "The critic sees its error: it predicted 12 but the truth was 15. So it adjusts to predict higher values for similar states.\n",
    "\n",
    "### 3.2 Why Two Networks?\n",
    "\n",
    "The actor and critic have different jobs:\n",
    "- **Actor** outputs a probability distribution over actions (softmax output)\n",
    "- **Critic** outputs a single scalar value estimate\n",
    "\n",
    "They share the same input (state) but produce fundamentally different outputs. Using two networks lets each specialize."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It \u2014 Component by Component\n",
    "\n",
    "### 4.1 The Actor Network"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"The policy network \u2014 selects actions.\"\"\"\n",
    "    def __init__(self, state_dim, n_actions, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        logits = self.net(state)\n",
    "        return F.softmax(logits, dim=-1)\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        probs = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "\n",
    "# Test actor\n",
    "actor = Actor(state_dim=4, n_actions=2)\n",
    "test_state = torch.randn(4)\n",
    "probs = actor(test_state)\n",
    "print(f\"Actor output (action probs): {probs.detach().numpy().round(4)}\")\n",
    "print(f\"Sum: {probs.sum().item():.4f}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Critic Network"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"The value network \u2014 evaluates states.\"\"\"\n",
    "    def __init__(self, state_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state).squeeze(-1)\n",
    "\n",
    "# Test critic\n",
    "critic = Critic(state_dim=4)\n",
    "test_state = torch.randn(4)\n",
    "value = critic(test_state)\n",
    "print(f\"Critic output (state value): {value.item():.4f}\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Actor and Critic architecture\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Actor diagram\n",
    "layers_actor = ['State\\n(4 dims)', 'Hidden\\n(128 ReLU)', 'Action Probs\\n(2 softmax)']\n",
    "colors_actor = ['#f97316', '#fb923c', '#fdba74']\n",
    "for i, (label, color) in enumerate(zip(layers_actor, colors_actor)):\n",
    "    rect = plt.Rectangle((i*2, 0), 1.5, 1, facecolor=color, edgecolor='black', linewidth=1.5)\n",
    "    ax1.add_patch(rect)\n",
    "    ax1.text(i*2 + 0.75, 0.5, label, ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    if i < len(layers_actor) - 1:\n",
    "        ax1.annotate('', xy=(i*2+2, 0.5), xytext=(i*2+1.5, 0.5),\n",
    "                     arrowprops=dict(arrowstyle='->', lw=2, color='gray'))\n",
    "ax1.set_xlim(-0.5, 6.5)\n",
    "ax1.set_ylim(-0.5, 1.5)\n",
    "ax1.set_title('Actor Network (Policy)', fontsize=14, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Critic diagram\n",
    "layers_critic = ['State\\n(4 dims)', 'Hidden\\n(128 ReLU)', 'Value\\n(1 scalar)']\n",
    "colors_critic = ['#3b82f6', '#60a5fa', '#93c5fd']\n",
    "for i, (label, color) in enumerate(zip(layers_critic, colors_critic)):\n",
    "    rect = plt.Rectangle((i*2, 0), 1.5, 1, facecolor=color, edgecolor='black', linewidth=1.5)\n",
    "    ax2.add_patch(rect)\n",
    "    ax2.text(i*2 + 0.75, 0.5, label, ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    if i < len(layers_critic) - 1:\n",
    "        ax2.annotate('', xy=(i*2+2, 0.5), xytext=(i*2+1.5, 0.5),\n",
    "                     arrowprops=dict(arrowstyle='->', lw=2, color='gray'))\n",
    "ax2.set_xlim(-0.5, 6.5)\n",
    "ax2.set_ylim(-0.5, 1.5)\n",
    "ax2.set_title('Critic Network (Value)', fontsize=14, fontweight='bold')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"The Actor selects actions. The Critic evaluates states.\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Computing Returns and Advantages"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "\n",
    "def compute_returns(rewards, gamma=GAMMA):\n",
    "    \"\"\"Compute discounted returns G_t for each timestep.\"\"\"\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    return returns\n",
    "\n",
    "def compute_advantages(returns, states, critic):\n",
    "    \"\"\"Compute advantages A_t = G_t - V(s_t).\"\"\"\n",
    "    returns_t = torch.tensor(returns, dtype=torch.float32)\n",
    "    states_t = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        values = critic(states_t)\n",
    "\n",
    "    advantages = returns_t - values\n",
    "    return advantages, returns_t, values\n",
    "\n",
    "# Example\n",
    "dummy_returns = [5.0, 4.0, 3.0]\n",
    "dummy_states = [np.zeros(4) for _ in range(3)]\n",
    "advs, rets, vals = compute_advantages(dummy_returns, dummy_states, critic)\n",
    "print(f\"Returns:     {rets.numpy().round(2)}\")\n",
    "print(f\"Values V(s): {vals.numpy().round(2)}\")\n",
    "print(f\"Advantages:  {advs.numpy().round(2)}\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO: Implement the Actor-Critic Training Step"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic_step(actor, critic, actor_optimizer, critic_optimizer,\n",
    "                      states, log_probs, rewards):\n",
    "    \"\"\"\n",
    "    Perform one Actor-Critic update step after collecting an episode.\n",
    "\n",
    "    Args:\n",
    "        actor: Actor network\n",
    "        critic: Critic network\n",
    "        actor_optimizer: Optimizer for actor\n",
    "        critic_optimizer: Optimizer for critic\n",
    "        states: list of state observations\n",
    "        log_probs: list of log probabilities (from action sampling)\n",
    "        rewards: list of rewards\n",
    "\n",
    "    Returns:\n",
    "        actor_loss: float, the actor loss value\n",
    "        critic_loss: float, the critic loss value\n",
    "    \"\"\"\n",
    "    # Step 1: Compute returns\n",
    "    returns = compute_returns(rewards)\n",
    "    returns_t = torch.tensor(returns, dtype=torch.float32)\n",
    "    states_t = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 2: Get value predictions from critic\n",
    "    # values = critic(states_t)\n",
    "    #\n",
    "    # Step 3: Compute advantages (returns - values.detach())\n",
    "    # advantages = ???\n",
    "    #\n",
    "    # Step 4: Actor loss = -sum(log_probs * advantages)\n",
    "    # log_probs_t = torch.stack(log_probs)\n",
    "    # actor_loss = ???\n",
    "    #\n",
    "    # Step 5: Critic loss = MSE(returns, values)\n",
    "    # critic_loss = ???\n",
    "    #\n",
    "    # Step 6: Update actor\n",
    "    # actor_optimizer.zero_grad()\n",
    "    # actor_loss.backward()\n",
    "    # actor_optimizer.step()\n",
    "    #\n",
    "    # Step 7: Update critic\n",
    "    # critic_optimizer.zero_grad()\n",
    "    # critic_loss.backward()\n",
    "    # critic_optimizer.step()\n",
    "    # ==============================\n",
    "\n",
    "    return actor_loss.item(), critic_loss.item()"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "test_actor = Actor(4, 2)\n",
    "test_critic = Critic(4)\n",
    "test_a_opt = torch.optim.Adam(test_actor.parameters(), lr=0.01)\n",
    "test_c_opt = torch.optim.Adam(test_critic.parameters(), lr=0.01)\n",
    "\n",
    "# Collect a test episode\n",
    "states, actions, rewards, log_probs = [], [], [], []\n",
    "state, _ = env.reset()\n",
    "for _ in range(10):\n",
    "    st = torch.as_tensor(state, dtype=torch.float32)\n",
    "    action, lp = test_actor.sample_action(st)\n",
    "    ns, r, term, trunc, _ = env.step(action)\n",
    "    states.append(state)\n",
    "    log_probs.append(lp)\n",
    "    rewards.append(r)\n",
    "    state = ns\n",
    "    if term or trunc:\n",
    "        break\n",
    "\n",
    "a_loss, c_loss = actor_critic_step(test_actor, test_critic, test_a_opt, test_c_opt,\n",
    "                                    states, log_probs, rewards)\n",
    "assert isinstance(a_loss, float), \"Actor loss should be a float\"\n",
    "assert isinstance(c_loss, float), \"Critic loss should be a float\"\n",
    "print(f\"Actor loss: {a_loss:.4f}\")\n",
    "print(f\"Critic loss: {c_loss:.4f}\")\n",
    "print(\"Correct! Actor-Critic training step works.\")\n",
    "env.close()"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implement Episode Collection with Value Tracking"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episode_with_values(env, actor, critic):\n",
    "    \"\"\"\n",
    "    Collect an episode and also record the critic's value estimates.\n",
    "\n",
    "    Returns:\n",
    "        states, actions, rewards, log_probs, values\n",
    "    \"\"\"\n",
    "    states, actions, rewards, log_probs, values = [], [], [], [], []\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        state_t = torch.as_tensor(state, dtype=torch.float32)\n",
    "\n",
    "        # ============ TODO ============\n",
    "        # Step 1: Get action and log_prob from actor\n",
    "        # Step 2: Get value estimate from critic (detached)\n",
    "        # Step 3: Step the environment\n",
    "        # Step 4: Store everything\n",
    "        # ==============================\n",
    "\n",
    "        pass  # YOUR CODE HERE\n",
    "\n",
    "    return states, actions, rewards, log_probs, values"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "actor = Actor(4, 2)\n",
    "critic = Critic(4)\n",
    "\n",
    "s, a, r, lp, v = collect_episode_with_values(env, actor, critic)\n",
    "assert len(s) == len(a) == len(r) == len(lp) == len(v)\n",
    "assert all(isinstance(vi, float) for vi in v)\n",
    "print(f\"Collected episode with {len(s)} steps\")\n",
    "print(f\"First 5 values: {[f'{vi:.2f}' for vi in v[:5]]}\")\n",
    "print(\"Correct! Episode collection with value tracking works.\")\n",
    "env.close()"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor_critic(env_name=\"CartPole-v1\", num_episodes=500, lr_actor=0.01, lr_critic=0.01):\n",
    "    \"\"\"Full Actor-Critic training loop.\"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    actor = Actor(state_dim, n_actions)\n",
    "    critic = Critic(state_dim)\n",
    "    actor_opt = torch.optim.Adam(actor.parameters(), lr=lr_actor)\n",
    "    critic_opt = torch.optim.Adam(critic.parameters(), lr=lr_critic)\n",
    "\n",
    "    reward_history = []\n",
    "    actor_losses = []\n",
    "    critic_losses = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Collect episode\n",
    "        states, actions, rewards, log_probs = [], [], [], []\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            state_t = torch.as_tensor(state, dtype=torch.float32)\n",
    "            action, log_prob = actor.sample_action(state_t)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            state = next_state\n",
    "\n",
    "        # Compute returns and update\n",
    "        returns = compute_returns(rewards)\n",
    "        returns_t = torch.tensor(returns, dtype=torch.float32)\n",
    "        states_t = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "\n",
    "        # Critic values and advantages\n",
    "        values = critic(states_t)\n",
    "        advantages = (returns_t - values.detach())\n",
    "\n",
    "        # Normalize advantages\n",
    "        if len(advantages) > 1:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # Actor loss\n",
    "        log_probs_t = torch.stack(log_probs)\n",
    "        actor_loss = -(log_probs_t * advantages).sum()\n",
    "\n",
    "        # Critic loss\n",
    "        critic_loss = F.mse_loss(values, returns_t)\n",
    "\n",
    "        # Update actor\n",
    "        actor_opt.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_opt.step()\n",
    "\n",
    "        # Update critic\n",
    "        critic_opt.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_opt.step()\n",
    "\n",
    "        ep_reward = sum(rewards)\n",
    "        reward_history.append(ep_reward)\n",
    "        actor_losses.append(actor_loss.item())\n",
    "        critic_losses.append(critic_loss.item())\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg = np.mean(reward_history[-100:])\n",
    "            print(f\"Ep {episode+1:4d} | Avg Reward: {avg:.1f} | \"\n",
    "                  f\"Actor Loss: {np.mean(actor_losses[-100:]):.2f} | \"\n",
    "                  f\"Critic Loss: {np.mean(critic_losses[-100:]):.2f}\")\n",
    "\n",
    "    env.close()\n",
    "    return reward_history, actor_losses, critic_losses, actor, critic\n",
    "\n",
    "# Train Actor-Critic\n",
    "print(\"=\" * 60)\n",
    "print(\"Training: Actor-Critic\")\n",
    "print(\"=\" * 60)\n",
    "ac_rewards, ac_actor_losses, ac_critic_losses, trained_actor, trained_critic = \\\n",
    "    train_actor_critic(num_episodes=500)"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three methods\n",
    "# First, train REINFORCE variants for comparison\n",
    "def train_reinforce_simple(num_episodes=500, use_baseline=False):\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    policy = Actor(4, 2)  # Reuse Actor class\n",
    "    opt = torch.optim.Adam(policy.parameters(), lr=0.01)\n",
    "    rewards = []\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        states, actions, rews, log_probs = [], [], [], []\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            st = torch.as_tensor(state, dtype=torch.float32)\n",
    "            action, lp = policy.sample_action(st)\n",
    "            ns, r, term, trunc, _ = env.step(action)\n",
    "            done = term or trunc\n",
    "            states.append(state); rews.append(r); log_probs.append(lp)\n",
    "            state = ns\n",
    "\n",
    "        rets = compute_returns(rews)\n",
    "        rets_t = torch.tensor(rets, dtype=torch.float32)\n",
    "        if use_baseline:\n",
    "            rets_t = rets_t - rets_t.mean()\n",
    "\n",
    "        loss = -(torch.stack(log_probs) * rets_t).sum()\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        rewards.append(sum(rews))\n",
    "    env.close()\n",
    "    return rewards\n",
    "\n",
    "print(\"Training REINFORCE (no baseline)...\")\n",
    "reinforce_rewards = train_reinforce_simple(500, use_baseline=False)\n",
    "print(\"Training REINFORCE + Baseline...\")\n",
    "baseline_rewards = train_reinforce_simple(500, use_baseline=True)"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grand comparison plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "window = 30\n",
    "\n",
    "for data, label, color, ls in [\n",
    "    (reinforce_rewards, 'REINFORCE', '#ef4444', '-'),\n",
    "    (baseline_rewards, 'REINFORCE + Baseline', '#f59e0b', '--'),\n",
    "    (ac_rewards, 'Actor-Critic', '#3b82f6', '-'),\n",
    "]:\n",
    "    if len(data) >= window:\n",
    "        smoothed = np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "        ax.plot(range(window-1, len(data)), smoothed, label=label,\n",
    "                color=color, linewidth=2.5, linestyle=ls)\n",
    "\n",
    "ax.axhline(y=500, color='gray', linestyle=':', alpha=0.5, label='Max Reward')\n",
    "ax.set_xlabel('Episode', fontsize=13)\n",
    "ax.set_ylabel('Episode Reward', fontsize=13)\n",
    "ax.set_title('Policy Gradient Methods: Head-to-Head Comparison', fontsize=15)\n",
    "ax.legend(fontsize=12, loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 550)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n--- Final 100-Episode Average ---\")\n",
    "for name, data in [(\"REINFORCE\", reinforce_rewards),\n",
    "                    (\"+ Baseline\", baseline_rewards),\n",
    "                    (\"Actor-Critic\", ac_rewards)]:\n",
    "    avg = np.mean(data[-100:])\n",
    "    print(f\"  {name:14s}: {avg:.1f}\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor-Critic loss curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "window = 30\n",
    "\n",
    "# Actor loss\n",
    "smoothed_actor = np.convolve(ac_actor_losses, np.ones(window)/window, mode='valid')\n",
    "ax1.plot(range(window-1, len(ac_actor_losses)), smoothed_actor, color='#f97316', linewidth=2)\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Actor Loss', fontsize=12)\n",
    "ax1.set_title('Actor Loss Over Training', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Critic loss\n",
    "smoothed_critic = np.convolve(ac_critic_losses, np.ones(window)/window, mode='valid')\n",
    "ax2.plot(range(window-1, len(ac_critic_losses)), smoothed_critic, color='#3b82f6', linewidth=2)\n",
    "ax2.set_xlabel('Episode', fontsize=12)\n",
    "ax2.set_ylabel('Critic Loss (MSE)', fontsize=12)\n",
    "ax2.set_title('Critic Loss Over Training', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Watch how the critic loss decreases as V(s) gets more accurate,\")\n",
    "print(\"which in turn provides better advantage estimates for the actor.\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned value function\n",
    "print(\"=\" * 60)\n",
    "print(\"VISUALIZING THE LEARNED VALUE FUNCTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a grid of states varying cart position and pole angle\n",
    "cart_positions = np.linspace(-2.4, 2.4, 50)\n",
    "pole_angles = np.linspace(-0.2, 0.2, 50)\n",
    "\n",
    "value_grid = np.zeros((50, 50))\n",
    "for i, cp in enumerate(cart_positions):\n",
    "    for j, pa in enumerate(pole_angles):\n",
    "        state = torch.tensor([cp, 0.0, pa, 0.0], dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            value_grid[j, i] = trained_critic(state).item()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Value function heatmap\n",
    "im = axes[0].imshow(value_grid, origin='lower', aspect='auto',\n",
    "                     extent=[-2.4, 2.4, -0.2, 0.2],\n",
    "                     cmap='RdYlGn')\n",
    "axes[0].set_xlabel('Cart Position', fontsize=12)\n",
    "axes[0].set_ylabel('Pole Angle (rad)', fontsize=12)\n",
    "axes[0].set_title('Learned Value Function V(s)', fontsize=14)\n",
    "plt.colorbar(im, ax=axes[0], label='Estimated Value')\n",
    "axes[0].axhline(y=0, color='white', linestyle='--', alpha=0.5)\n",
    "axes[0].axvline(x=0, color='white', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Policy heatmap (probability of pushing right)\n",
    "policy_grid = np.zeros((50, 50))\n",
    "for i, cp in enumerate(cart_positions):\n",
    "    for j, pa in enumerate(pole_angles):\n",
    "        state = torch.tensor([cp, 0.0, pa, 0.0], dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            probs = trained_actor(state)\n",
    "            policy_grid[j, i] = probs[1].item()  # P(push right)\n",
    "\n",
    "im2 = axes[1].imshow(policy_grid, origin='lower', aspect='auto',\n",
    "                      extent=[-2.4, 2.4, -0.2, 0.2],\n",
    "                      cmap='coolwarm', vmin=0, vmax=1)\n",
    "axes[1].set_xlabel('Cart Position', fontsize=12)\n",
    "axes[1].set_ylabel('Pole Angle (rad)', fontsize=12)\n",
    "axes[1].set_title('Learned Policy: P(push right)', fontsize=14)\n",
    "plt.colorbar(im2, ax=axes[1], label='P(right)')\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[1].axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLeft plot: States near center (cart at 0, pole upright) have highest value.\")\n",
    "print(\"Right plot: Policy pushes right when pole tilts right, left when pole tilts left.\")\n",
    "print(\"\\nCongratulations! You have built Actor-Critic from scratch!\")\n",
    "print(\"This is the foundation of PPO, the algorithm used to fine-tune ChatGPT.\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "1. Why do we normalize advantages before computing the actor loss? What happens if advantages are all large and positive?\n",
    "2. The critic loss is MSE between predicted values and actual returns. Why is this a good loss function? What alternatives exist?\n",
    "3. In Actor-Critic, the actor and critic share no parameters. What would happen if they shared the early layers of the network? What are the trade-offs?\n",
    "\n",
    "### Optional Challenges\n",
    "1. Implement a shared backbone: one network with shared hidden layers and two separate output heads (policy and value).\n",
    "2. Add entropy regularization to encourage exploration: subtract $\\beta H(\\pi)$ from the actor loss.\n",
    "3. Try the LunarLander-v3 environment (4 discrete actions, 8-dimensional state). Does Actor-Critic still outperform REINFORCE?"
   ],
   "id": "cell_26"
  }
 ]
}