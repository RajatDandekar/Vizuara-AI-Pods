{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Policy Gradient Foundations \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Foundations: From Preferences to Gradient Ascent\n",
    "\n",
    "*Part 1 of the Vizuara series on Policy Gradient Methods*\n",
    "*Estimated time: 45 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Policy gradient methods are the backbone of modern reinforcement learning. Every time you hear about RLHF (Reinforcement Learning from Human Feedback), PPO (Proximal Policy Optimization), or how ChatGPT was fine-tuned \u2014 policy gradients are at the core.\n",
    "\n",
    "But here is the key insight: instead of learning a value function and deriving a policy from it (like Q-learning), policy gradient methods **directly optimize the policy itself**. This is a paradigm shift. It is like the difference between memorizing an answer key versus learning how to solve problems.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Build a softmax policy from scratch\n",
    "- Implement the performance measure $J(\\theta)$\n",
    "- Derive and code the policy gradient theorem step by step\n",
    "- See gradient ascent move a policy toward higher returns\n",
    "\n",
    "Let us begin."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Let us think about a simple problem. You are learning to throw darts at a dartboard. You could try to memorize the value of every hand position (this is what Q-learning does). Or you could directly adjust your throwing motion based on where the darts land.\n",
    "\n",
    "Policy gradient methods take the second approach. Instead of estimating values for every state-action pair, we directly parameterize a policy \u2014 a function that outputs the probability of each action \u2014 and then use gradient ascent to make it better.\n",
    "\n",
    "Think of it this way: imagine a mountain landscape where the height at each point represents how good your policy is. Your policy parameters $\\theta$ determine where you stand on this landscape. Gradient ascent tells you which direction to walk to go uphill.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "If you have a continuous action space (like robot joint angles), why would a lookup table approach fail? How many entries would you need for a robot arm with 7 joints, each with 360 possible angles?"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### 3.1 Policy Parameterization\n",
    "\n",
    "We write our policy as $\\pi(a|s, \\theta)$ \u2014 the probability of action $a$ given state $s$ and parameters $\\theta$.\n",
    "\n",
    "For discrete actions, we use the softmax function to convert raw preferences into probabilities:\n",
    "\n",
    "$$\\pi(a|s, \\theta) = \\frac{e^{h(s, a, \\theta)}}{\\sum_{a'} e^{h(s, a', \\theta)}}$$\n",
    "\n",
    "This equation says: compute a preference score $h$ for each action, exponentiate them, and normalize. Actions with higher preferences get higher probabilities, but every action retains some probability \u2014 this is how the agent explores.\n",
    "\n",
    "Let us plug in simple numbers. Suppose we have 3 actions with preferences $h(a_1) = 2.0$, $h(a_2) = 1.0$, $h(a_3) = 0.5$:\n",
    "\n",
    "$$e^{2.0} = 7.39, \\quad e^{1.0} = 2.72, \\quad e^{0.5} = 1.65$$\n",
    "$$\\text{sum} = 7.39 + 2.72 + 1.65 = 11.76$$\n",
    "$$\\pi(a_1) = 0.63, \\quad \\pi(a_2) = 0.23, \\quad \\pi(a_3) = 0.14$$\n",
    "\n",
    "Action $a_1$ gets 63% probability because it has the highest preference. This is exactly what we want.\n",
    "\n",
    "### 3.2 The Performance Measure\n",
    "\n",
    "What are we optimizing? The expected total return starting from the initial state:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[G(\\tau)] = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T-1} \\gamma^t r_t\\right]$$\n",
    "\n",
    "This equation says: sample many trajectories $\\tau$ under policy $\\pi_\\theta$, compute the discounted return $G$ for each, and take the average. We want to maximize this.\n",
    "\n",
    "### 3.3 The Policy Gradient Theorem\n",
    "\n",
    "The gradient of $J(\\theta)$ is:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G(\\tau)\\right]$$\n",
    "\n",
    "This is the most important equation. It says: for each trajectory, multiply the gradient of the log-probability of each action by the total return. Good trajectories (high $G$) push the policy to make those actions more likely. Bad trajectories push the policy away."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It \u2014 Component by Component\n",
    "\n",
    "### 4.1 Softmax Policy\n",
    "\n",
    "Let us build a softmax policy from scratch. We start by implementing the softmax function and a simple linear policy."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def manual_softmax(preferences):\n",
    "    \"\"\"Compute softmax probabilities from raw preferences.\"\"\"\n",
    "    # Subtract max for numerical stability\n",
    "    shifted = preferences - np.max(preferences)\n",
    "    exp_prefs = np.exp(shifted)\n",
    "    return exp_prefs / np.sum(exp_prefs)\n",
    "\n",
    "# Example: 3 actions with preferences\n",
    "preferences = np.array([2.0, 1.0, 0.5])\n",
    "probabilities = manual_softmax(preferences)\n",
    "\n",
    "print(\"Action preferences:\", preferences)\n",
    "print(\"Action probabilities:\", probabilities)\n",
    "print(\"Sum of probabilities:\", np.sum(probabilities))  # Should be 1.0"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization checkpoint: See how preferences become probabilities\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Left: preferences\n",
    "axes[0].bar(['a1', 'a2', 'a3'], preferences, color=['#2563eb', '#3b82f6', '#93c5fd'])\n",
    "axes[0].set_title('Action Preferences h(a, \u03b8)', fontsize=14)\n",
    "axes[0].set_ylabel('Preference Value')\n",
    "\n",
    "# Right: probabilities after softmax\n",
    "axes[1].bar(['a1', 'a2', 'a3'], probabilities, color=['#2563eb', '#3b82f6', '#93c5fd'])\n",
    "axes[1].set_title('Action Probabilities \u03c0(a|s, \u03b8)', fontsize=14)\n",
    "axes[1].set_ylabel('Probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Notice how the highest preference gets the highest probability!\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Policy Neural Network\n",
    "\n",
    "Now let us build the actual policy network that maps states to action probabilities:"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"A neural network that parameterizes the policy.\"\"\"\n",
    "    def __init__(self, state_dim, n_actions, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Return action logits (preferences) for the given state.\"\"\"\n",
    "        return self.net(state)\n",
    "\n",
    "    def get_action_probs(self, state):\n",
    "        \"\"\"Return action probabilities (softmax of logits).\"\"\"\n",
    "        logits = self.forward(state)\n",
    "        return F.softmax(logits, dim=-1)\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        \"\"\"Sample an action from the policy and return (action, log_prob).\"\"\"\n",
    "        probs = self.get_action_probs(state)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "# Create a policy for CartPole (4-dim state, 2 actions)\n",
    "policy = PolicyNetwork(state_dim=4, n_actions=2)\n",
    "print(f\"Policy parameters: {sum(p.numel() for p in policy.parameters())}\")\n",
    "\n",
    "# Test with a dummy state\n",
    "dummy_state = torch.randn(4)\n",
    "probs = policy.get_action_probs(dummy_state)\n",
    "print(f\"State: {dummy_state.numpy().round(2)}\")\n",
    "print(f\"Action probs: {probs.detach().numpy().round(4)}\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The Log-Derivative Trick\n",
    "\n",
    "The key insight behind policy gradients is the log-derivative trick. Let us verify it numerically:"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The log-derivative trick: \u2207P(x)/P(x) = \u2207log(P(x))\n",
    "# Or equivalently: \u2207P(x) = P(x) * \u2207log(P(x))\n",
    "\n",
    "# Let us verify this with a concrete example\n",
    "theta = torch.tensor([1.5], requires_grad=True)\n",
    "\n",
    "# f(theta) = theta^2\n",
    "f = theta ** 2\n",
    "f.backward()\n",
    "grad_f = theta.grad.item()\n",
    "\n",
    "# Reset\n",
    "theta = torch.tensor([1.5], requires_grad=True)\n",
    "log_f = torch.log(theta ** 2)\n",
    "log_f.backward()\n",
    "grad_log_f = theta.grad.item()\n",
    "\n",
    "print(f\"f(\u03b8) = \u03b8\u00b2 at \u03b8 = 1.5\")\n",
    "print(f\"\u2207f(\u03b8) = 2\u03b8 = {grad_f:.4f}\")\n",
    "print(f\"\u2207log(f(\u03b8)) = {grad_log_f:.4f}\")\n",
    "print(f\"f(\u03b8) \u00d7 \u2207log(f(\u03b8)) = {1.5**2 * grad_log_f:.4f}\")\n",
    "print(f\"These should match: \u2207f = {grad_f:.4f} \u2248 f \u00d7 \u2207log(f) = {1.5**2 * grad_log_f:.4f}\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: How the gradient direction depends on the return\n",
    "returns_range = np.linspace(-5, 5, 100)\n",
    "log_prob = -0.5  # Fixed log probability\n",
    "\n",
    "gradients = log_prob * returns_range\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(returns_range, gradients, 'b-', linewidth=2)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.fill_between(returns_range, gradients, where=(returns_range > 0), alpha=0.2, color='green', label='Positive return: reinforce')\n",
    "plt.fill_between(returns_range, gradients, where=(returns_range < 0), alpha=0.2, color='red', label='Negative return: penalize')\n",
    "plt.xlabel('Return G(\u03c4)', fontsize=12)\n",
    "plt.ylabel('Gradient contribution', fontsize=12)\n",
    "plt.title('How Returns Shape the Policy Gradient', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Positive returns push the gradient to increase action probability.\")\n",
    "print(\"Negative returns push the gradient to decrease action probability.\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO: Implement the Policy Gradient Estimator"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_policy_gradient(log_probs, returns):\n",
    "    \"\"\"\n",
    "    Estimate the policy gradient from a batch of trajectories.\n",
    "\n",
    "    This implements: \u2207J(\u03b8) \u2248 (1/N) \u03a3_i \u03a3_t \u2207log \u03c0(a_t|s_t) * G(\u03c4_i)\n",
    "\n",
    "    Args:\n",
    "        log_probs: List of lists. log_probs[i][t] = log \u03c0(a_t|s_t) for trajectory i, step t\n",
    "        returns: List of floats. returns[i] = G(\u03c4_i) for trajectory i\n",
    "\n",
    "    Returns:\n",
    "        Estimated policy gradient (scalar for simplicity)\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: For each trajectory, sum up all the log_probs\n",
    "    # Step 2: Multiply each sum by the corresponding return\n",
    "    # Step 3: Average over all trajectories\n",
    "    # ==============================\n",
    "\n",
    "    gradient = ???  # YOUR CODE HERE\n",
    "\n",
    "    return gradient"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "test_log_probs = [\n",
    "    [-0.5, -1.2],   # Trajectory 1: two actions\n",
    "    [-0.3, -0.8],   # Trajectory 2: two actions\n",
    "]\n",
    "test_returns = [3.0, -1.0]  # Traj 1 was good, Traj 2 was bad\n",
    "\n",
    "result = estimate_policy_gradient(test_log_probs, test_returns)\n",
    "expected = ((-0.5 + -1.2) * 3.0 + (-0.3 + -0.8) * (-1.0)) / 2.0\n",
    "# = (-1.7 * 3.0 + -1.1 * -1.0) / 2.0 = (-5.1 + 1.1) / 2.0 = -2.0\n",
    "\n",
    "assert abs(result - expected) < 1e-6, f\"Expected {expected}, got {result}\"\n",
    "print(\"Correct! Your policy gradient estimator works.\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implement Temperature-Scaled Softmax"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_softmax(preferences, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Compute softmax with temperature scaling.\n",
    "\n",
    "    Higher temperature -> more uniform (more exploration)\n",
    "    Lower temperature -> more peaked (more exploitation)\n",
    "    Temperature = 1.0 -> standard softmax\n",
    "\n",
    "    Args:\n",
    "        preferences: numpy array of preference values\n",
    "        temperature: float > 0, controls the sharpness\n",
    "\n",
    "    Returns:\n",
    "        numpy array of probabilities\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Divide preferences by temperature\n",
    "    # Step 2: Apply softmax (subtract max for stability)\n",
    "    # ==============================\n",
    "\n",
    "    probabilities = ???  # YOUR CODE HERE\n",
    "\n",
    "    return probabilities"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "prefs = np.array([2.0, 1.0, 0.5])\n",
    "\n",
    "low_temp = temperature_softmax(prefs, temperature=0.1)\n",
    "mid_temp = temperature_softmax(prefs, temperature=1.0)\n",
    "high_temp = temperature_softmax(prefs, temperature=10.0)\n",
    "\n",
    "assert np.argmax(low_temp) == 0, \"Low temp should heavily favor the best action\"\n",
    "assert abs(np.sum(mid_temp) - 1.0) < 1e-6, \"Probabilities must sum to 1\"\n",
    "assert np.max(high_temp) - np.min(high_temp) < 0.1, \"High temp should be nearly uniform\"\n",
    "print(\"Correct! Temperature scaling works as expected.\")\n",
    "print(f\"Low temp (0.1):  {low_temp.round(4)} \u2014 almost deterministic\")\n",
    "print(f\"Mid temp (1.0):  {mid_temp.round(4)} \u2014 standard softmax\")\n",
    "print(f\"High temp (10.0): {high_temp.round(4)} \u2014 nearly uniform\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us combine everything into a simple demonstration. We will create a policy, compute gradients for sample trajectories, and show how gradient ascent works."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Create CartPole environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Initialize policy\n",
    "policy = PolicyNetwork(state_dim, n_actions)\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "def collect_episode(env, policy):\n",
    "    \"\"\"Collect one full episode using the current policy.\"\"\"\n",
    "    states, actions, rewards, log_probs = [], [], [], []\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        state_t = torch.as_tensor(state, dtype=torch.float32)\n",
    "        action, log_prob = policy.sample_action(state_t)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    return states, actions, rewards, log_probs\n",
    "\n",
    "def compute_returns(rewards, gamma=0.99):\n",
    "    \"\"\"Compute discounted returns working backwards from the end.\"\"\"\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    return returns\n",
    "\n",
    "# Collect one episode and examine it\n",
    "states, actions, rewards, log_probs = collect_episode(env, policy)\n",
    "returns = compute_returns(rewards)\n",
    "\n",
    "print(f\"Episode length: {len(rewards)} steps\")\n",
    "print(f\"Total reward: {sum(rewards):.1f}\")\n",
    "print(f\"First 5 returns: {[f'{r:.2f}' for r in returns[:5]]}\")\n",
    "print(f\"Last 5 returns: {[f'{r:.2f}' for r in returns[-5:]]}\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with vanilla policy gradient for a few episodes\n",
    "GAMMA = 0.99\n",
    "NUM_EPISODES = 300\n",
    "reward_history = []\n",
    "\n",
    "policy = PolicyNetwork(state_dim, n_actions)\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    # Collect episode\n",
    "    states, actions, rewards, log_probs = collect_episode(env, policy)\n",
    "    returns = compute_returns(rewards, GAMMA)\n",
    "\n",
    "    # Compute policy gradient loss\n",
    "    returns_t = torch.tensor(returns, dtype=torch.float32)\n",
    "    log_probs_t = torch.stack(log_probs)\n",
    "\n",
    "    # Loss = -\u03a3 log_prob * return (negative because we minimize)\n",
    "    loss = -(log_probs_t * returns_t).sum()\n",
    "\n",
    "    # Update\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    episode_reward = sum(rewards)\n",
    "    reward_history.append(episode_reward)\n",
    "\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        avg = np.mean(reward_history[-50:])\n",
    "        print(f\"Episode {episode+1:4d} | Reward: {episode_reward:6.1f} | Avg(50): {avg:.1f}\")\n",
    "\n",
    "env.close()"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(reward_history, alpha=0.3, color='steelblue', label='Per-episode')\n",
    "# Smoothed average\n",
    "window = 20\n",
    "if len(reward_history) >= window:\n",
    "    smoothed = np.convolve(reward_history, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(reward_history)), smoothed, color='navy', linewidth=2, label=f'{window}-episode average')\n",
    "\n",
    "ax.axhline(y=500, color='gray', linestyle='--', alpha=0.5, label='Max reward (500)')\n",
    "ax.set_xlabel('Episode', fontsize=12)\n",
    "ax.set_ylabel('Episode Reward', fontsize=12)\n",
    "ax.set_title('Policy Gradient Training on CartPole', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Watch how the reward climbs as the policy improves!\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trained policy's action probabilities across different states\n",
    "states_to_test = [\n",
    "    (\"Cart centered, pole upright\", [0.0, 0.0, 0.0, 0.0]),\n",
    "    (\"Cart left, pole tilting right\", [-1.0, -0.5, 0.1, 0.5]),\n",
    "    (\"Cart right, pole tilting left\", [1.0, 0.5, -0.1, -0.5]),\n",
    "    (\"Pole falling right fast\", [0.0, 0.0, 0.2, 1.5]),\n",
    "    (\"Pole falling left fast\", [0.0, 0.0, -0.2, -1.5]),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(states_to_test), figsize=(16, 4))\n",
    "\n",
    "for idx, (desc, state) in enumerate(states_to_test):\n",
    "    state_t = torch.tensor(state, dtype=torch.float32)\n",
    "    probs = policy.get_action_probs(state_t).detach().numpy()\n",
    "\n",
    "    colors = ['#ef4444', '#3b82f6']  # Red for left, Blue for right\n",
    "    axes[idx].bar(['Left', 'Right'], probs, color=colors)\n",
    "    axes[idx].set_title(desc, fontsize=9, wrap=True)\n",
    "    axes[idx].set_ylim(0, 1)\n",
    "    if idx == 0:\n",
    "        axes[idx].set_ylabel('Probability')\n",
    "\n",
    "plt.suptitle('Trained Policy: Action Probabilities for Different States', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Congratulations! You have built a policy gradient agent from scratch!\")\n",
    "print(\"Notice how the policy learned to push right when the pole tilts right, and left when it tilts left.\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "1. Why do we use the log-derivative trick instead of directly differentiating the trajectory probability? What makes direct differentiation difficult?\n",
    "2. If the return for all trajectories is positive (e.g., rewards are always non-negative), what happens to the gradient estimate? Why might this be a problem?\n",
    "3. How does the number of sampled trajectories affect the quality of the gradient estimate?\n",
    "\n",
    "### Optional Challenges\n",
    "1. Modify the policy network to use 2 hidden layers instead of 1. Does it learn faster?\n",
    "2. Experiment with different learning rates (0.001, 0.01, 0.1). Plot the training curves for each.\n",
    "3. Replace CartPole with LunarLander-v3 (4 actions instead of 2). Does the same approach work?"
   ],
   "id": "cell_26"
  }
 ]
}