{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Sampling Strategies & Speed ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1rhbO-3WtvU0YjQYQrCebpE_IATSZyRCX\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Motivation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_motivation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_motivation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Strategies & Speed Tradeoffs in Diffusion LLMs\n",
    "\n",
    "*Part 3 of the Vizuara series on Diffusion Language Models*\n",
    "*Estimated time: 35 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Autoregressive models have a hard speed limit: to generate $L$ tokens, you need $L$ sequential forward passes. Period.\n",
    "\n",
    "Diffusion models break this limit. You choose the number of denoising steps $S$, and each step processes *all* tokens in parallel. But this raises critical questions:\n",
    "\n",
    "- **How many steps do we actually need?** Is 5 enough? Do we need 50?\n",
    "- **Does the unmasking order matter?** Should we unmask the most confident tokens first, or use random order?\n",
    "- **How much faster is diffusion?** Can we quantify the speedup?\n",
    "\n",
    "In this notebook, we will train a diffusion model and then systematically explore these tradeoffs. The punchline: you can get 80% of the quality with 20% of the steps.\n",
    "\n",
    "**Teaser ‚Äî what you will build:**\n",
    "\n",
    "A quality-vs-speed curve showing how generation quality degrades gracefully as we reduce the number of steps, and a side-by-side comparison of different remasking strategies."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_intuition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### The Speed-Quality Tradeoff\n",
    "\n",
    "Think of painting a wall. If you make one careful pass with the roller, you get good coverage. Two passes give you even coverage. By the third pass, the wall looks great.\n",
    "\n",
    "But what if you only have time for **half a pass**? You would focus on the most visible areas first ‚Äî the spots at eye level, near the door. You would skip the corners and behind the furniture.\n",
    "\n",
    "This is exactly how confidence-based remasking works. With limited steps, the model focuses on the \"easy\" tokens first (function words, common patterns) and resolves the harder tokens (content words, rare combinations) in later steps. If you give it fewer steps, it still gets the easy tokens right but may stumble on the hard ones.\n",
    "\n",
    "### ü§î Think About This\n",
    "\n",
    "With $S = 1$ step (single-shot generation), the diffusion model predicts all tokens independently and commits to all of them at once. With $S = \\infty$ steps, it unmasks one token at a time, always with full context.\n",
    "\n",
    "- Which extreme gives better quality?\n",
    "- Which extreme is faster?\n",
    "- Where is the sweet spot?"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_math"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Remasking Strategies\n",
    "\n",
    "At each denoising step $s$, the model predicts tokens at all masked positions. The **remasking strategy** determines which predictions to keep and which to mask again:\n",
    "\n",
    "**1. Confidence-based remasking:**\n",
    "Keep the top $k$ predictions ranked by confidence $p_\\theta(x^i \\mid x_t)$, where:\n",
    "\n",
    "$$k = \\left\\lfloor \\frac{N_{\\text{masked}}}{s} \\right\\rfloor$$\n",
    "\n",
    "**Numerical example:** If 12 tokens are masked and we are at step $s = 4$, we unmask $\\lfloor 12/4 \\rfloor = 3$ tokens (the 3 most confident predictions). The other 9 get remasked.\n",
    "\n",
    "**2. Random remasking:**\n",
    "Keep $k$ predictions chosen uniformly at random (ignoring confidence). Same schedule for $k$.\n",
    "\n",
    "**3. Linear schedule:**\n",
    "Unmask a fixed fraction at each step: $k = N_{\\text{total}} / S$ tokens per step, regardless of confidence.\n",
    "\n",
    "**What this means computationally:** Confidence-based is like a student answering exam questions easiest-first. Random is like answering in random order. Linear is like answering one per minute, regardless of difficulty."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Build Model\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_build_model.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_build_model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It ‚Äî Component by Component\n",
    "\n",
    "### 4.1 Model and Data (Reused from Notebook 1)"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 16\n",
    "SEQ_LEN = 24\n",
    "MASK_TOKEN = 0\n",
    "D_MODEL = 64\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 3\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def generate_pattern_data(batch_size, seq_len, vocab_size):\n",
    "    \"\"\"Generate sequences with repeating patterns.\"\"\"\n",
    "    sequences = []\n",
    "    for _ in range(batch_size):\n",
    "        pattern_len = np.random.randint(2, 6)\n",
    "        pattern = np.random.randint(1, vocab_size, size=pattern_len)\n",
    "        seq = np.tile(pattern, seq_len // pattern_len + 1)[:seq_len]\n",
    "        sequences.append(seq)\n",
    "    return torch.tensor(np.array(sequences), dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "class DiffusionLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(SEQ_LEN, d_model)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, d_model), nn.SiLU(), nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=d_model*4,\n",
    "            dropout=0.1, batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(enc_layer, n_layers)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x_t, t):\n",
    "        positions = torch.arange(x_t.size(1), device=x_t.device)\n",
    "        h = self.embed(x_t) + self.pos_embed(positions)\n",
    "        h = h + self.time_mlp(t).unsqueeze(1)\n",
    "        h = self.transformer(h)\n",
    "        return self.head(h)\n",
    "\n",
    "\n",
    "def mask_tokens(x_0, t):\n",
    "    mask = torch.rand_like(x_0.float()) < t\n",
    "    x_t = x_0.clone()\n",
    "    x_t[mask] = MASK_TOKEN\n",
    "    return x_t, mask\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model = DiffusionLM(VOCAB_SIZE, D_MODEL, N_HEADS, N_LAYERS).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "print(\"Training diffusion model...\")\n",
    "for step in range(2500):\n",
    "    x_0 = generate_pattern_data(BATCH_SIZE, SEQ_LEN, VOCAB_SIZE)\n",
    "    t = torch.rand(BATCH_SIZE, 1, device=device) * 0.98 + 0.02\n",
    "    x_t, mask = mask_tokens(x_0, t)\n",
    "    logits = model(x_t, t)\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    loss = F.cross_entropy(logits[mask], x_0[mask])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    if (step+1) % 500 == 0:\n",
    "        print(f\"  Step {step+1}/2500 | Loss: {loss.item():.4f}\")\n",
    "\n",
    "model.eval()\n",
    "print(\"Done!\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Strategies\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_strategies.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_strategies"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Three Remasking Strategies"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_confidence(model, seq_len=SEQ_LEN, n_steps=10):\n",
    "    \"\"\"Confidence-based remasking: unmask most confident predictions first.\"\"\"\n",
    "    x = torch.full((1, seq_len), MASK_TOKEN, dtype=torch.long, device=device)\n",
    "    history = [x[0].cpu().clone()]\n",
    "\n",
    "    for s in range(n_steps, 0, -1):\n",
    "        t = torch.tensor([[s / n_steps]], device=device, dtype=torch.float)\n",
    "        logits = model(x, t)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        sampled = torch.multinomial(probs.view(-1, VOCAB_SIZE), 1).view(1, -1)\n",
    "        confidence = probs.gather(-1, sampled.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        is_masked = (x == MASK_TOKEN)\n",
    "        n_unmask = max(1, int(is_masked.sum().item() / s))\n",
    "\n",
    "        conf = confidence.clone()\n",
    "        conf[~is_masked] = -float('inf')\n",
    "        _, top_idx = conf.topk(min(n_unmask, is_masked.sum().item()), dim=-1)\n",
    "        x.scatter_(1, top_idx, sampled.gather(1, top_idx))\n",
    "        history.append(x[0].cpu().clone())\n",
    "\n",
    "    return x, history"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_random(model, seq_len=SEQ_LEN, n_steps=10):\n",
    "    \"\"\"Random remasking: unmask random masked positions each step.\"\"\"\n",
    "    x = torch.full((1, seq_len), MASK_TOKEN, dtype=torch.long, device=device)\n",
    "    history = [x[0].cpu().clone()]\n",
    "\n",
    "    for s in range(n_steps, 0, -1):\n",
    "        t = torch.tensor([[s / n_steps]], device=device, dtype=torch.float)\n",
    "        logits = model(x, t)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        sampled = torch.multinomial(probs.view(-1, VOCAB_SIZE), 1).view(1, -1)\n",
    "\n",
    "        is_masked = (x == MASK_TOKEN)\n",
    "        n_unmask = max(1, int(is_masked.sum().item() / s))\n",
    "\n",
    "        # Randomly select among masked positions\n",
    "        masked_indices = is_masked[0].nonzero(as_tuple=True)[0]\n",
    "        if len(masked_indices) > 0:\n",
    "            perm = torch.randperm(len(masked_indices), device=device)[:n_unmask]\n",
    "            selected = masked_indices[perm]\n",
    "            x[0, selected] = sampled[0, selected]\n",
    "\n",
    "        history.append(x[0].cpu().clone())\n",
    "\n",
    "    return x, history"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_single_shot(model, seq_len=SEQ_LEN):\n",
    "    \"\"\"Single-shot: predict all tokens at once (S=1).\"\"\"\n",
    "    x = torch.full((1, seq_len), MASK_TOKEN, dtype=torch.long, device=device)\n",
    "    t = torch.tensor([[1.0]], device=device, dtype=torch.float)\n",
    "    logits = model(x, t)\n",
    "    x = logits.argmax(dim=-1)\n",
    "    return x, [torch.full((seq_len,), MASK_TOKEN), x[0].cpu().clone()]"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Quality Metric\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_quality_metric.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_quality_metric"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Quality Metric"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_pattern_quality(sequence):\n",
    "    \"\"\"Measure how well a generated sequence follows a repeating pattern.\n",
    "\n",
    "    Returns a score from 0 to 1, where 1 means a perfect repeating pattern.\n",
    "    We check all possible pattern lengths (2-6) and return the best score.\n",
    "    \"\"\"\n",
    "    seq = sequence.cpu().numpy() if isinstance(sequence, torch.Tensor) else sequence\n",
    "    best_score = 0\n",
    "\n",
    "    for pat_len in range(2, 7):\n",
    "        # Check if the sequence repeats with this period\n",
    "        matches = 0\n",
    "        total = 0\n",
    "        for i in range(pat_len, len(seq)):\n",
    "            if seq[i] == seq[i % pat_len]:\n",
    "                matches += 1\n",
    "            total += 1\n",
    "        if total > 0:\n",
    "            score = matches / total\n",
    "            best_score = max(best_score, score)\n",
    "\n",
    "    return best_score"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Quick test: generate with each strategy and compare\n",
    "print(\"Testing generation strategies...\\n\")\n",
    "\n",
    "for name, gen_fn in [(\"Confidence-based\", generate_confidence),\n",
    "                      (\"Random\", generate_random),\n",
    "                      (\"Single-shot\", generate_single_shot)]:\n",
    "    scores = []\n",
    "    for _ in range(20):\n",
    "        if name == \"Single-shot\":\n",
    "            seq, _ = gen_fn(model)\n",
    "        else:\n",
    "            seq, _ = gen_fn(model, n_steps=10)\n",
    "        scores.append(measure_pattern_quality(seq[0]))\n",
    "\n",
    "    print(f\"{name:20s} | Quality: {np.mean(scores):.3f} ¬± {np.std(scores):.3f}\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_todo.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_todo"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîß Your Turn: Run the Step-Count Sweep\n",
    "\n",
    "### TODO: Measure quality at different step counts"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_step_counts(model, strategy_fn, step_counts, n_samples=30):\n",
    "    \"\"\"Measure generation quality at different numbers of denoising steps.\n",
    "\n",
    "    Args:\n",
    "        model: Trained DiffusionLM\n",
    "        strategy_fn: Generation function (generate_confidence or generate_random)\n",
    "        step_counts: List of step counts to test\n",
    "        n_samples: Number of sequences to generate per step count\n",
    "\n",
    "    Returns:\n",
    "        mean_scores: List of mean quality scores\n",
    "        std_scores: List of standard deviations\n",
    "    \"\"\"\n",
    "    mean_scores = []\n",
    "    std_scores = []\n",
    "\n",
    "    for n_steps in step_counts:\n",
    "        # ============ TODO ============\n",
    "        # For each step count:\n",
    "        # 1. Generate n_samples sequences using strategy_fn(model, n_steps=n_steps)\n",
    "        # 2. Compute the pattern quality score for each\n",
    "        # 3. Store the mean and std of scores\n",
    "\n",
    "        scores = []\n",
    "        for _ in range(n_samples):\n",
    "            seq, _ = ???  # YOUR CODE: call strategy_fn\n",
    "            score = ???   # YOUR CODE: measure quality\n",
    "            scores.append(score)\n",
    "\n",
    "        mean_scores.append(???)  # YOUR CODE\n",
    "        std_scores.append(???)   # YOUR CODE\n",
    "        # ==============================\n",
    "\n",
    "        print(f\"  Steps={n_steps:3d} | Quality: {mean_scores[-1]:.3f} ¬± {std_scores[-1]:.3f}\")\n",
    "\n",
    "    return mean_scores, std_scores\n",
    "\n",
    "# Test\n",
    "step_counts = [1, 2, 3, 5, 8, 12, 16, 24, 32]\n",
    "print(\"Confidence-based remasking:\")\n",
    "# conf_means, conf_stds = sweep_step_counts(model, generate_confidence, step_counts)"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification\n",
    "# Uncomment and run after completing the TODO above:\n",
    "# assert len(conf_means) == len(step_counts), \"Wrong number of results\"\n",
    "# assert all(0 <= s <= 1 for s in conf_means), \"Scores should be between 0 and 1\"\n",
    "# print(\"‚úÖ Sweep function works!\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Post Todo\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_post_todo.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_post_todo"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_step_counts(model, strategy_fn, step_counts, n_samples=30):\n",
    "    \"\"\"Measure generation quality at different step counts.\"\"\"\n",
    "    mean_scores = []\n",
    "    std_scores = []\n",
    "\n",
    "    for n_steps in step_counts:\n",
    "        scores = []\n",
    "        for _ in range(n_samples):\n",
    "            seq, _ = strategy_fn(model, n_steps=n_steps)\n",
    "            score = measure_pattern_quality(seq[0])\n",
    "            scores.append(score)\n",
    "        mean_scores.append(np.mean(scores))\n",
    "        std_scores.append(np.std(scores))\n",
    "        print(f\"  Steps={n_steps:3d} | Quality: {mean_scores[-1]:.3f} ¬± {std_scores[-1]:.3f}\")\n",
    "\n",
    "    return mean_scores, std_scores\n",
    "\n",
    "\n",
    "step_counts = [1, 2, 3, 5, 8, 12, 16, 24, 32]\n",
    "\n",
    "print(\"Confidence-based remasking:\")\n",
    "conf_means, conf_stds = sweep_step_counts(model, generate_confidence, step_counts)\n",
    "\n",
    "print(\"\\nRandom remasking:\")\n",
    "rand_means, rand_stds = sweep_step_counts(model, generate_random, step_counts)"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Quality Curve\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_quality_curve.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_quality_curve"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together ‚Äî The Quality vs Speed Curve"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Quality vs Number of Steps\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(step_counts, conf_means, 'o-', color='#1565c0', linewidth=2.5,\n",
    "        markersize=8, label='Confidence-based', zorder=5)\n",
    "ax.fill_between(step_counts,\n",
    "                np.array(conf_means) - np.array(conf_stds),\n",
    "                np.array(conf_means) + np.array(conf_stds),\n",
    "                color='#1565c0', alpha=0.15)\n",
    "\n",
    "ax.plot(step_counts, rand_means, 's--', color='#e65100', linewidth=2.5,\n",
    "        markersize=8, label='Random', zorder=5)\n",
    "ax.fill_between(step_counts,\n",
    "                np.array(rand_means) - np.array(rand_stds),\n",
    "                np.array(rand_means) + np.array(rand_stds),\n",
    "                color='#e65100', alpha=0.15)\n",
    "\n",
    "# Mark the sweet spot\n",
    "best_idx = np.argmax(np.array(conf_means) > 0.9 * max(conf_means))\n",
    "if best_idx > 0:\n",
    "    ax.axvline(x=step_counts[best_idx], color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.annotate(f'Sweet spot\\n~{step_counts[best_idx]} steps',\n",
    "                xy=(step_counts[best_idx], conf_means[best_idx]),\n",
    "                xytext=(step_counts[best_idx] + 5, conf_means[best_idx] - 0.1),\n",
    "                fontsize=11, arrowprops=dict(arrowstyle='->', color='gray'))\n",
    "\n",
    "ax.set_xlabel('Number of Denoising Steps', fontsize=13)\n",
    "ax.set_ylabel('Pattern Quality Score', fontsize=13)\n",
    "ax.set_title('Quality vs Speed: How Many Steps Do We Need?', fontsize=15)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Speed\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_speed.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_speed"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed Benchmark"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_speed(model, step_counts, n_runs=20):\n",
    "    \"\"\"Measure tokens/second at different step counts.\"\"\"\n",
    "    tokens_per_second = []\n",
    "\n",
    "    for n_steps in step_counts:\n",
    "        times = []\n",
    "        for _ in range(n_runs):\n",
    "            start = time.time()\n",
    "            _ = generate_confidence(model, n_steps=n_steps)\n",
    "            elapsed = time.time() - start\n",
    "            times.append(elapsed)\n",
    "\n",
    "        avg_time = np.mean(times)\n",
    "        tps = SEQ_LEN / avg_time\n",
    "        tokens_per_second.append(tps)\n",
    "\n",
    "    return tokens_per_second\n",
    "\n",
    "\n",
    "print(\"Benchmarking speed...\")\n",
    "speed_results = benchmark_speed(model, step_counts)\n",
    "\n",
    "for steps, tps in zip(step_counts, speed_results):\n",
    "    print(f\"  Steps={steps:3d} | {tps:,.0f} tokens/sec\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Speed vs Quality combined plot\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "line1 = ax1.plot(step_counts, conf_means, 'o-', color='#1565c0',\n",
    "                  linewidth=2.5, markersize=8, label='Quality')\n",
    "line2 = ax2.plot(step_counts, speed_results, 's--', color='#2e7d32',\n",
    "                  linewidth=2.5, markersize=8, label='Speed')\n",
    "\n",
    "ax1.set_xlabel('Number of Denoising Steps', fontsize=13)\n",
    "ax1.set_ylabel('Quality Score', fontsize=13, color='#1565c0')\n",
    "ax2.set_ylabel('Tokens / Second', fontsize=13, color='#2e7d32')\n",
    "ax1.tick_params(axis='y', labelcolor='#1565c0')\n",
    "ax2.tick_params(axis='y', labelcolor='#2e7d32')\n",
    "\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, fontsize=12, loc='center right')\n",
    "ax1.set_title('The Quality‚ÄìSpeed Tradeoff', fontsize=15)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key takeaway: Quality plateaus quickly, but speed drops linearly.\")\n",
    "print(\"   The sweet spot is usually 5-15 steps for this model.\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Trajectories\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_trajectories.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_11_trajectories"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üìä Visualizing the Unmasking Trajectory"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_trajectories(model, n_steps=10):\n",
    "    \"\"\"Show side-by-side unmasking trajectories for different strategies.\"\"\"\n",
    "    _, hist_conf = generate_confidence(model, n_steps=n_steps)\n",
    "    _, hist_rand = generate_random(model, n_steps=n_steps)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 6))\n",
    "\n",
    "    for ax, history, title, color in [\n",
    "        (ax1, hist_conf, 'Confidence-Based (Easy tokens first)', '#1565c0'),\n",
    "        (ax2, hist_rand, 'Random Remasking', '#e65100')\n",
    "    ]:\n",
    "        # Create a 2D grid: rows = steps, columns = positions\n",
    "        n_rows = len(history)\n",
    "        grid = np.zeros((n_rows, SEQ_LEN, 3))\n",
    "\n",
    "        for row, seq in enumerate(history):\n",
    "            tokens = seq.numpy()\n",
    "            for pos in range(SEQ_LEN):\n",
    "                if tokens[pos] == MASK_TOKEN:\n",
    "                    grid[row, pos] = [0.15, 0.15, 0.15]\n",
    "                else:\n",
    "                    c = plt.cm.Set2(tokens[pos] / VOCAB_SIZE)[:3]\n",
    "                    grid[row, pos] = c\n",
    "\n",
    "        ax.imshow(grid, aspect='auto', interpolation='nearest')\n",
    "        ax.set_ylabel('Step', fontsize=11)\n",
    "        ax.set_xlabel('Token Position', fontsize=11)\n",
    "        ax.set_title(title, fontsize=13, color=color)\n",
    "        ax.set_yticks(range(0, n_rows, max(1, n_rows//5)))\n",
    "\n",
    "    plt.suptitle('Unmasking Trajectories: Which Tokens Appear First?', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_trajectories(model, n_steps=12)"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Final\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_final.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_12_final"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üéØ Final Output: Generation Quality Grid"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_grid(model, step_counts_display=[1, 3, 5, 10, 20], n_sequences=6):\n",
    "    \"\"\"Show generated sequences at different step counts side by side.\"\"\"\n",
    "    fig, axes = plt.subplots(n_sequences, len(step_counts_display),\n",
    "                              figsize=(3 * len(step_counts_display), n_sequences * 0.9))\n",
    "\n",
    "    for col, n_steps in enumerate(step_counts_display):\n",
    "        for row in range(n_sequences):\n",
    "            ax = axes[row, col]\n",
    "            seq, _ = generate_confidence(model, n_steps=n_steps)\n",
    "            tokens = seq[0].cpu().numpy()\n",
    "            quality = measure_pattern_quality(seq[0])\n",
    "\n",
    "            img = np.zeros((1, SEQ_LEN, 3))\n",
    "            for pos in range(SEQ_LEN):\n",
    "                if tokens[pos] == MASK_TOKEN:\n",
    "                    img[0, pos] = [0.15, 0.15, 0.15]\n",
    "                else:\n",
    "                    img[0, pos] = plt.cm.Set2(tokens[pos] / VOCAB_SIZE)[:3]\n",
    "\n",
    "            ax.imshow(img, aspect='auto', interpolation='nearest')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "            if row == 0:\n",
    "                ax.set_title(f'{n_steps} steps', fontsize=12)\n",
    "            if col == len(step_counts_display) - 1:\n",
    "                ax.text(SEQ_LEN + 0.5, 0.5, f'{quality:.0%}',\n",
    "                        va='center', fontsize=9, color='gray')\n",
    "\n",
    "    plt.suptitle('Generation Quality at Different Step Counts\\n'\n",
    "                 '(Dark = leftover masks, Colors = tokens, Right = quality score)',\n",
    "                 fontsize=14, y=1.04)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"üéâ Notice how even 5 steps produce recognizable patterns!\")\n",
    "    print(\"   This is why Mercury achieves 10x speed over autoregressive models.\")\n",
    "\n",
    "quality_grid(model)"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_13_closing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "\n",
    "1. **Diminishing returns:** Why does quality plateau after a certain number of steps? What is the model doing in those extra steps that does not help much?\n",
    "\n",
    "2. **Confidence vs random:** Why does confidence-based remasking outperform random? Think about what information the model has after each step.\n",
    "\n",
    "3. **Real-world scaling:** Mercury achieves 1,109 tokens/sec on H100 with ~25 steps. If we reduce to 10 steps, we get ~2.5x more speed. What applications would benefit from this tradeoff?\n",
    "\n",
    "### üèÜ Optional Challenges\n",
    "\n",
    "1. **Cosine schedule:** Instead of unmasking $N/s$ tokens per step, try a cosine schedule: unmask more tokens in early steps (when predictions are coarser) and fewer in late steps (when fine details matter). Does this improve quality?\n",
    "\n",
    "2. **Temperature annealing:** Start with high temperature (more randomness) in early steps and reduce it in later steps. Compare to fixed temperature.\n",
    "\n",
    "3. **Semi-autoregressive:** Generate text in blocks of 8 tokens from left to right. Within each block, use diffusion. Implement this hybrid and measure quality vs pure diffusion.\n",
    "\n",
    "**Next notebook:** We will build a **real diffusion language model on the TinyStories dataset** ‚Äî training it to generate coherent short stories through iterative unmasking!"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "chatbot"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üí¨ AI Teaching Assistant ‚Äî Click ‚ñ∂ to start\n",
    "#@markdown This AI chatbot reads your notebook and can answer questions about any concept, code, or exercise.\n",
    "\n",
    "import json as _json\n",
    "import requests as _requests\n",
    "from google.colab import output as _output\n",
    "from IPython.display import display, HTML as _HTML, Markdown as _Markdown\n",
    "\n",
    "# --- Read notebook content for context ---\n",
    "def _get_notebook_context():\n",
    "    try:\n",
    "        from google.colab import _message\n",
    "        nb = _message.blocking_request(\"get_ipynb\", request=\"\", timeout_sec=10)\n",
    "        cells = nb.get(\"ipynb\", {}).get(\"cells\", [])\n",
    "        parts = []\n",
    "        for cell in cells:\n",
    "            src = \"\".join(cell.get(\"source\", []))\n",
    "            tags = cell.get(\"metadata\", {}).get(\"tags\", [])\n",
    "            if \"chatbot\" in tags:\n",
    "                continue\n",
    "            if src.strip():\n",
    "                ct = cell.get(\"cell_type\", \"unknown\")\n",
    "                parts.append(f\"[{ct.upper()}]\\n{src}\")\n",
    "        return \"\\n\\n---\\n\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"Notebook content unavailable.\"\n",
    "\n",
    "_NOTEBOOK_CONTEXT = _get_notebook_context()\n",
    "_CHAT_HISTORY = []\n",
    "_API_URL = \"https://course-creator-brown.vercel.app/api/chat\"\n",
    "\n",
    "def _notebook_chat(question):\n",
    "    global _CHAT_HISTORY\n",
    "    try:\n",
    "        resp = _requests.post(_API_URL, json={\n",
    "            'question': question,\n",
    "            'context': _NOTEBOOK_CONTEXT[:100000],\n",
    "            'history': _CHAT_HISTORY[-10:],\n",
    "        }, timeout=60)\n",
    "        data = resp.json()\n",
    "        answer = data.get('answer', 'Sorry, I could not generate a response.')\n",
    "        _CHAT_HISTORY.append({'role': 'user', 'content': question})\n",
    "        _CHAT_HISTORY.append({'role': 'assistant', 'content': answer})\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f'Error connecting to teaching assistant: {str(e)}'\n",
    "\n",
    "_output.register_callback('notebook_chat', _notebook_chat)\n",
    "\n",
    "def ask(question):\n",
    "    \"\"\"Ask the AI teaching assistant a question about this notebook.\"\"\"\n",
    "    answer = _notebook_chat(question)\n",
    "    display(_Markdown(answer))\n",
    "\n",
    "print(\"\\u2705 AI Teaching Assistant is ready!\")\n",
    "print(\"\\U0001f4a1 Use the chat below, or call ask(\\'your question\\') in any cell.\")\n",
    "\n",
    "# --- Display chat widget ---\n",
    "display(_HTML('''<style>\n  .vc-wrap{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;max-width:100%;border-radius:16px;overflow:hidden;box-shadow:0 4px 24px rgba(0,0,0,.12);background:#fff;border:1px solid #e5e7eb}\n  .vc-hdr{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;padding:16px 20px;display:flex;align-items:center;gap:12px}\n  .vc-avatar{width:42px;height:42px;background:rgba(255,255,255,.2);border-radius:50%;display:flex;align-items:center;justify-content:center;font-size:22px}\n  .vc-hdr h3{font-size:16px;font-weight:600;margin:0}\n  .vc-hdr p{font-size:12px;opacity:.85;margin:2px 0 0}\n  .vc-msgs{height:420px;overflow-y:auto;padding:16px;background:#f8f9fb;display:flex;flex-direction:column;gap:10px}\n  .vc-msg{display:flex;flex-direction:column;animation:vc-fade .25s ease}\n  .vc-msg.user{align-items:flex-end}\n  .vc-msg.bot{align-items:flex-start}\n  .vc-bbl{max-width:85%;padding:10px 14px;border-radius:16px;font-size:14px;line-height:1.55;word-wrap:break-word}\n  .vc-msg.user .vc-bbl{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border-bottom-right-radius:4px}\n  .vc-msg.bot .vc-bbl{background:#fff;color:#1a1a2e;border:1px solid #e8e8e8;border-bottom-left-radius:4px}\n  .vc-bbl code{background:rgba(0,0,0,.07);padding:2px 6px;border-radius:4px;font-size:13px;font-family:'Fira Code',monospace}\n  .vc-bbl pre{background:#1e1e2e;color:#cdd6f4;padding:12px;border-radius:8px;overflow-x:auto;margin:8px 0;font-size:13px}\n  .vc-bbl pre code{background:none;padding:0;color:inherit}\n  .vc-bbl h3,.vc-bbl h4{margin:10px 0 4px;font-size:15px}\n  .vc-bbl ul,.vc-bbl ol{margin:4px 0;padding-left:20px}\n  .vc-bbl li{margin:2px 0}\n  .vc-chips{display:flex;flex-wrap:wrap;gap:8px;padding:0 16px 12px;background:#f8f9fb}\n  .vc-chip{background:#fff;border:1px solid #d1d5db;border-radius:20px;padding:6px 14px;font-size:12px;cursor:pointer;transition:all .15s;color:#4b5563}\n  .vc-chip:hover{border-color:#667eea;color:#667eea;background:#f0f0ff}\n  .vc-input{display:flex;padding:12px 16px;background:#fff;border-top:1px solid #eee;gap:8px}\n  .vc-input input{flex:1;padding:10px 16px;border:2px solid #e8e8e8;border-radius:24px;font-size:14px;outline:none;transition:border-color .2s}\n  .vc-input input:focus{border-color:#667eea}\n  .vc-input button{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border:none;border-radius:50%;width:42px;height:42px;cursor:pointer;display:flex;align-items:center;justify-content:center;font-size:18px;transition:transform .1s}\n  .vc-input button:hover{transform:scale(1.05)}\n  .vc-input button:disabled{opacity:.5;cursor:not-allowed;transform:none}\n  .vc-typing{display:flex;gap:5px;padding:4px 0}\n  .vc-typing span{width:8px;height:8px;background:#667eea;border-radius:50%;animation:vc-bounce 1.4s infinite ease-in-out}\n  .vc-typing span:nth-child(2){animation-delay:.2s}\n  .vc-typing span:nth-child(3){animation-delay:.4s}\n  @keyframes vc-bounce{0%,80%,100%{transform:scale(0)}40%{transform:scale(1)}}\n  @keyframes vc-fade{from{opacity:0;transform:translateY(8px)}to{opacity:1;transform:translateY(0)}}\n  .vc-note{text-align:center;font-size:11px;color:#9ca3af;padding:8px 16px 12px;background:#fff}\n</style>\n<div class=\"vc-wrap\">\n  <div class=\"vc-hdr\">\n    <div class=\"vc-avatar\">&#129302;</div>\n    <div>\n      <h3>Vizuara Teaching Assistant</h3>\n      <p>Ask me anything about this notebook</p>\n    </div>\n  </div>\n  <div class=\"vc-msgs\" id=\"vcMsgs\">\n    <div class=\"vc-msg bot\">\n      <div class=\"vc-bbl\">&#128075; Hi! I've read through this entire notebook. Ask me about any concept, code block, or exercise &mdash; I'm here to help you learn!</div>\n    </div>\n  </div>\n  <div class=\"vc-chips\" id=\"vcChips\">\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Explain the main concept</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Help with the TODO exercise</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Summarize what I learned</span>\n  </div>\n  <div class=\"vc-input\">\n    <input type=\"text\" id=\"vcIn\" placeholder=\"Ask about concepts, code, exercises...\" />\n    <button id=\"vcSend\" onclick=\"vcSendMsg()\">&#10148;</button>\n  </div>\n  <div class=\"vc-note\">AI-generated &middot; Verify important information &middot; <a href=\"#\" onclick=\"vcClear();return false\" style=\"color:#667eea\">Clear chat</a></div>\n</div>\n<script>\n(function(){\n  var msgs=document.getElementById('vcMsgs'),inp=document.getElementById('vcIn'),\n      btn=document.getElementById('vcSend'),chips=document.getElementById('vcChips');\n\n  function esc(s){var d=document.createElement('div');d.textContent=s;return d.innerHTML}\n\n  function md(t){\n    return t\n      .replace(/```(\\w*)\\n([\\s\\S]*?)```/g,function(_,l,c){return '<pre><code>'+esc(c)+'</code></pre>'})\n      .replace(/`([^`]+)`/g,'<code>$1</code>')\n      .replace(/\\*\\*([^*]+)\\*\\*/g,'<strong>$1</strong>')\n      .replace(/\\*([^*]+)\\*/g,'<em>$1</em>')\n      .replace(/^#### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^## (.+)$/gm,'<h3>$1</h3>')\n      .replace(/^\\d+\\. (.+)$/gm,'<li>$1</li>')\n      .replace(/^- (.+)$/gm,'<li>$1</li>')\n      .replace(/\\n\\n/g,'<br><br>')\n      .replace(/\\n/g,'<br>');\n  }\n\n  function addMsg(text,isUser){\n    var m=document.createElement('div');m.className='vc-msg '+(isUser?'user':'bot');\n    var b=document.createElement('div');b.className='vc-bbl';\n    b.innerHTML=isUser?esc(text):md(text);\n    m.appendChild(b);msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function showTyping(){\n    var m=document.createElement('div');m.className='vc-msg bot';m.id='vcTyping';\n    m.innerHTML='<div class=\"vc-bbl\"><div class=\"vc-typing\"><span></span><span></span><span></span></div></div>';\n    msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function hideTyping(){var e=document.getElementById('vcTyping');if(e)e.remove()}\n\n  window.vcSendMsg=function(){\n    var q=inp.value.trim();if(!q)return;\n    inp.value='';chips.style.display='none';\n    addMsg(q,true);showTyping();btn.disabled=true;\n    google.colab.kernel.invokeFunction('notebook_chat',[q],{})\n      .then(function(r){\n        hideTyping();\n        var a=r.data['application/json'];\n        addMsg(typeof a==='string'?a:JSON.stringify(a),false);\n      })\n      .catch(function(){\n        hideTyping();\n        addMsg('Sorry, I encountered an error. Please check your internet connection and try again.',false);\n      })\n      .finally(function(){btn.disabled=false;inp.focus()});\n  };\n\n  window.vcAsk=function(q){inp.value=q;vcSendMsg()};\n  window.vcClear=function(){\n    msgs.innerHTML='<div class=\"vc-msg bot\"><div class=\"vc-bbl\">&#128075; Chat cleared. Ask me anything!</div></div>';\n    chips.style.display='flex';\n  };\n\n  inp.addEventListener('keypress',function(e){if(e.key==='Enter')vcSendMsg()});\n  inp.focus();\n})();\n</script>'''))"
   ],
   "id": "vizuara_chatbot"
  }
 ]
}