{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Masked Diffusion from First Principles ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"18Lc8XC_lV-uzRcNcgg-LLZXfkKSQ0vDX\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_setup"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Diffusion from First Principles\n",
    "\n",
    "*Part 1 of the Vizuara series on Diffusion Language Models*\n",
    "*Estimated time: 45 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Every large language model you have used ‚Äî GPT-4, Claude, LLaMA ‚Äî generates text **one token at a time**, left to right. This is called **autoregressive** generation, and it works like a typewriter: once a word is committed, there is no going back.\n",
    "\n",
    "But what if a language model could work like a **painter** instead? Start with a blank canvas, sketch a rough outline, and then progressively refine every part simultaneously?\n",
    "\n",
    "This is exactly what **Diffusion Language Models** do. Instead of generating text left-to-right, they start with a fully masked sequence and iteratively reveal tokens ‚Äî unmasking the ones they are most confident about first.\n",
    "\n",
    "By the end of this notebook, you will have built a complete masked diffusion language model from scratch and watched it generate text through iterative unmasking. Here is a preview of what the generation process looks like:\n",
    "\n",
    "```\n",
    "Step 1: [M] [M] [M] [M] [M] [M] [M] [M]\n",
    "Step 2: [M] [M]  c  [M] [M]  a  [M] [M]\n",
    "Step 3:  a  [M]  c  [M]  b   a  [M]  c\n",
    "Step 4:  a   b   c   a   b   a   b   c\n",
    "```\n",
    "\n",
    "The model fills in the easiest tokens first, then uses that context to resolve the harder ones. Let us build this from scratch."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup ‚Äî Run this cell first\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_intuition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### The Core Problem: How Do You \"Noise\" Text?\n",
    "\n",
    "In image diffusion, the forward process gradually adds Gaussian noise to pixels. A pixel value of 127.0 becomes 128.1 ‚Äî still a valid pixel.\n",
    "\n",
    "But text is **discrete**. The word \"cat\" is a symbol, not a number. You cannot add \"a little bit of noise\" to \"cat\" and get something between \"cat\" and \"dog.\" It simply does not make sense.\n",
    "\n",
    "The solution? **Masking.** Instead of adding noise, we replace tokens with a special `[MASK]` token. This is the text equivalent of corrupting an image with noise:\n",
    "\n",
    "| Image Diffusion | Text Diffusion |\n",
    "|---|---|\n",
    "| Add Gaussian noise | Replace tokens with [MASK] |\n",
    "| Pure noise at t=1 | All [MASK] at t=1 |\n",
    "| Predict noise to remove | Predict original token at [MASK] |\n",
    "| Continuous | Discrete |\n",
    "\n",
    "### ü§î Think About This\n",
    "\n",
    "Before we write any code, consider this question: **In what order should a diffusion language model reveal tokens during generation?**\n",
    "\n",
    "An autoregressive model has no choice ‚Äî it must go left to right. But a diffusion model can unmask tokens in *any* order. Should it unmask left to right? Random order? Or something smarter?\n",
    "\n",
    "*Think about this for a moment. We will see the answer emerge naturally from the math.*"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_math"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### The Forward Process\n",
    "\n",
    "Given a clean sequence $x_0 = [x_0^1, x_0^2, \\ldots, x_0^L]$ of $L$ tokens, the forward process at time $t \\in [0, 1]$ independently masks each token with probability $t$:\n",
    "\n",
    "$$q(x_t^i \\mid x_0^i) = (1 - t) \\cdot \\mathbb{1}[x_t^i = x_0^i] + t \\cdot \\mathbb{1}[x_t^i = \\texttt{[MASK]}]$$\n",
    "\n",
    "**What this says computationally:** For each token, flip a biased coin with probability $t$. Heads ‚Üí replace with [MASK]. Tails ‚Üí keep the original.\n",
    "\n",
    "At $t = 0$, nothing is masked (clean text). At $t = 1$, everything is masked (pure noise). At $t = 0.5$, about half the tokens are masked.\n",
    "\n",
    "### The Training Objective\n",
    "\n",
    "The loss function asks the model to predict the original tokens at masked positions:\n",
    "\n",
    "$$\\mathcal{L} = -\\mathbb{E}_{t \\sim U(0,1)} \\left[ \\frac{1}{t \\cdot L} \\sum_{i:\\, x_t^i = \\texttt{[MASK]}} \\log p_\\theta(x_0^i \\mid x_t) \\right]$$\n",
    "\n",
    "**What this says computationally:**\n",
    "1. Pick a random masking ratio $t$ between 0 and 1\n",
    "2. Mask that fraction of tokens\n",
    "3. Feed the partially masked sequence into the Transformer\n",
    "4. Compute cross-entropy loss at masked positions only\n",
    "5. Divide by $t \\cdot L$ (the expected number of masks) to normalize\n",
    "\n",
    "The $1/t$ weighting is crucial ‚Äî it upweights low masking ratios (where few tokens are masked but each prediction is harder because there is less context).\n",
    "\n",
    "**Numerical example:** Suppose $L = 4$, $t = 0.5$, and 2 tokens are masked. The model predicts the correct token at position 2 with probability 0.8 and at position 3 with probability 0.6:\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{0.5 \\times 4}[\\log(0.8) + \\log(0.6)] = -\\frac{1}{2}[-0.223 + (-0.511)] = 0.367$$\n",
    "\n",
    "The loss is 0.367. If the model had predicted both tokens perfectly (probability 1.0), the loss would be 0. Training pushes the model toward more confident, accurate predictions."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Data Masking\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_data_masking.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_data_masking"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It ‚Äî Component by Component\n",
    "\n",
    "### 4.1 The Synthetic Dataset\n",
    "\n",
    "We will train on a simple synthetic dataset: sequences built from repeating patterns. This is small enough to train in minutes but complex enough to test whether our diffusion model truly learns structure."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "VOCAB_SIZE = 8       # Tokens: [MASK]=0, then 1-7 are our vocabulary\n",
    "SEQ_LEN = 16         # Sequence length\n",
    "MASK_TOKEN = 0       # Token ID for [MASK]\n",
    "BATCH_SIZE = 64\n",
    "D_MODEL = 64         # Embedding dimension\n",
    "N_HEADS = 4          # Attention heads\n",
    "N_LAYERS = 3         # Transformer layers\n",
    "\n",
    "# --- Synthetic Data Generator ---\n",
    "def generate_pattern_data(batch_size, seq_len, vocab_size):\n",
    "    \"\"\"Generate sequences with learnable repeating patterns.\n",
    "\n",
    "    Each sequence picks a random short pattern (length 2-4) from the\n",
    "    vocabulary and tiles it to fill the sequence length. The model\n",
    "    must learn to recognize and complete these patterns.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for _ in range(batch_size):\n",
    "        pattern_len = np.random.randint(2, 5)  # pattern of length 2-4\n",
    "        # Tokens 1 to vocab_size-1 (avoid 0 which is MASK)\n",
    "        pattern = np.random.randint(1, vocab_size, size=pattern_len)\n",
    "        # Tile to fill sequence\n",
    "        seq = np.tile(pattern, seq_len // pattern_len + 1)[:seq_len]\n",
    "        sequences.append(seq)\n",
    "    return torch.tensor(np.array(sequences), dtype=torch.long).to(device)\n",
    "\n",
    "# Let's see some example patterns\n",
    "examples = generate_pattern_data(5, SEQ_LEN, VOCAB_SIZE)\n",
    "for i, seq in enumerate(examples):\n",
    "    tokens = seq.tolist()\n",
    "    print(f\"Pattern {i+1}: {tokens}\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Forward Masking Process\n",
    "\n",
    "This is the heart of the diffusion forward process ‚Äî randomly masking tokens with probability $t$."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_tokens(x_0, t):\n",
    "    \"\"\"Apply the forward masking process.\n",
    "\n",
    "    Args:\n",
    "        x_0: Clean token sequences, shape (B, L)\n",
    "        t: Masking ratio for each sample, shape (B, 1)\n",
    "\n",
    "    Returns:\n",
    "        x_t: Masked sequences, shape (B, L)\n",
    "        mask: Boolean mask indicating which positions were masked, shape (B, L)\n",
    "    \"\"\"\n",
    "    # For each token, independently mask with probability t\n",
    "    random_vals = torch.rand_like(x_0.float())           # (B, L)\n",
    "    mask = random_vals < t                                # (B, L) ‚Äî True where masked\n",
    "    x_t = x_0.clone()\n",
    "    x_t[mask] = MASK_TOKEN\n",
    "    return x_t, mask"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization: The forward process at different timesteps\n",
    "fig, axes = plt.subplots(1, 5, figsize=(18, 3))\n",
    "sample = generate_pattern_data(1, SEQ_LEN, VOCAB_SIZE)\n",
    "\n",
    "timesteps = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "for ax, t_val in zip(axes, timesteps):\n",
    "    t = torch.tensor([[t_val]]).to(device)\n",
    "    masked, _ = mask_tokens(sample, t)\n",
    "\n",
    "    # Visualize as colored grid\n",
    "    colors = plt.cm.Set2(sample[0].cpu().numpy() / VOCAB_SIZE)\n",
    "    display = masked[0].cpu().numpy()\n",
    "\n",
    "    for pos in range(SEQ_LEN):\n",
    "        if display[pos] == MASK_TOKEN:\n",
    "            ax.add_patch(plt.Rectangle((pos, 0), 1, 1, color='black', alpha=0.8))\n",
    "            ax.text(pos + 0.5, 0.5, 'M', ha='center', va='center',\n",
    "                    color='white', fontsize=9, fontweight='bold')\n",
    "        else:\n",
    "            ax.add_patch(plt.Rectangle((pos, 0), 1, 1,\n",
    "                         color=plt.cm.Set2(display[pos] / VOCAB_SIZE)))\n",
    "            ax.text(pos + 0.5, 0.5, str(display[pos]), ha='center',\n",
    "                    va='center', fontsize=9)\n",
    "\n",
    "    ax.set_xlim(0, SEQ_LEN)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(f't = {t_val}', fontsize=13)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.suptitle('Forward Process: Gradually Masking Tokens', fontsize=15, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Transformer\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_transformer.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_transformer"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The Bidirectional Transformer\n",
    "\n",
    "Our model is a standard Transformer encoder ‚Äî crucially, with **no causal mask**. This means every position can attend to every other position, including both masked and unmasked tokens. We also add a **time embedding** so the model knows the current masking ratio."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Standard sinusoidal positional encoding.\"\"\"\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionLM(nn.Module):\n",
    "    \"\"\"A bidirectional Transformer for masked diffusion language modeling.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, max_len=512):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, d_model),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_model, d_model),\n",
    "        )\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)\n",
    "        self.output_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x_t, t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_t: Masked token IDs, shape (B, L)\n",
    "            t: Masking ratio, shape (B, 1)\n",
    "        Returns:\n",
    "            Logits over vocabulary at every position, shape (B, L, V)\n",
    "        \"\"\"\n",
    "        # Token embeddings + positional encoding\n",
    "        h = self.token_embed(x_t)           # (B, L, D)\n",
    "        h = self.pos_enc(h)\n",
    "\n",
    "        # Add time conditioning (broadcast to all positions)\n",
    "        t_emb = self.time_mlp(t).unsqueeze(1)  # (B, 1, D)\n",
    "        h = h + t_emb\n",
    "\n",
    "        # Bidirectional Transformer ‚Äî NO causal mask!\n",
    "        h = self.transformer(h)\n",
    "\n",
    "        # Project to vocabulary\n",
    "        return self.output_head(h)           # (B, L, V)\n",
    "\n",
    "\n",
    "model = DiffusionLM(VOCAB_SIZE, D_MODEL, N_HEADS, N_LAYERS).to(device)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params:,}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Key Insight: Why No Causal Mask?\n",
    "\n",
    "In GPT-style models, the attention mask prevents each position from seeing future tokens (causal masking). This enforces left-to-right generation.\n",
    "\n",
    "In our diffusion model, we **want** every position to see every other position ‚Äî both masked and unmasked. This bidirectional attention is what allows the model to:\n",
    "- Use unmasked tokens on the *right* to predict masked tokens on the *left*\n",
    "- Fill in tokens in any order, not just left-to-right\n",
    "- Overcome the \"reversal curse\" that plagues autoregressive models"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The Training Loop"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_diffusion_lm(model, n_steps=2000, lr=3e-4):\n",
    "    \"\"\"Train the masked diffusion language model.\"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_steps)\n",
    "    losses = []\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        # Generate a fresh batch of pattern data\n",
    "        x_0 = generate_pattern_data(BATCH_SIZE, SEQ_LEN, VOCAB_SIZE)\n",
    "\n",
    "        # Sample random masking ratio t ~ U(epsilon, 1) for each sample\n",
    "        # We use epsilon > 0 to avoid division by zero in the weighting\n",
    "        t = torch.rand(BATCH_SIZE, 1, device=device) * 0.98 + 0.02  # (B, 1)\n",
    "\n",
    "        # Apply forward masking process\n",
    "        x_t, mask = mask_tokens(x_0, t)\n",
    "\n",
    "        # Forward pass: predict original tokens\n",
    "        logits = model(x_t, t)                # (B, L, V)\n",
    "\n",
    "        # Compute loss ONLY at masked positions\n",
    "        # Flatten for cross-entropy\n",
    "        logits_masked = logits[mask]           # (N_masked, V)\n",
    "        targets_masked = x_0[mask]             # (N_masked,)\n",
    "\n",
    "        if logits_masked.shape[0] == 0:\n",
    "            continue  # Skip if nothing was masked\n",
    "\n",
    "        loss = F.cross_entropy(logits_masked, targets_masked)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if (step + 1) % 200 == 0:\n",
    "            print(f\"Step {step+1}/{n_steps} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "print(\"Training the diffusion language model...\")\n",
    "losses = train_diffusion_lm(model, n_steps=2000)\n",
    "print(\"Done!\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Training curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "# Smooth with moving average\n",
    "window = 50\n",
    "smoothed = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "plt.plot(smoothed, color='#1565c0', linewidth=2)\n",
    "plt.xlabel('Training Step', fontsize=12)\n",
    "plt.ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "plt.title('Diffusion LM Training Loss', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_todo.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_todo"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîß Your Turn: Implement the Generation Algorithm\n",
    "\n",
    "Now comes the exciting part ‚Äî generation! The model starts with all [MASK] tokens and iteratively unmasks them.\n",
    "\n",
    "The key idea: at each step, predict all masked tokens, but only **keep the most confident predictions**. Remask the uncertain ones for the next step.\n",
    "\n",
    "### TODO: Complete the `generate` function"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, seq_len=SEQ_LEN, n_steps=8):\n",
    "    \"\"\"Generate a sequence using iterative confidence-based unmasking.\n",
    "\n",
    "    Args:\n",
    "        model: Trained DiffusionLM\n",
    "        seq_len: Length of sequence to generate\n",
    "        n_steps: Number of denoising steps\n",
    "\n",
    "    Returns:\n",
    "        final_sequence: Generated token IDs, shape (1, seq_len)\n",
    "        history: List of (sequence, step) tuples for visualization\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Start fully masked\n",
    "    x = torch.full((1, seq_len), MASK_TOKEN, dtype=torch.long, device=device)\n",
    "    history = [(x[0].cpu().clone(), 'Start')]\n",
    "\n",
    "    for s in range(n_steps, 0, -1):\n",
    "        t = torch.tensor([[s / n_steps]], device=device, dtype=torch.float)\n",
    "        logits = model(x, t)\n",
    "        probs = F.softmax(logits, dim=-1)  # (1, L, V)\n",
    "\n",
    "        # ============ TODO ============\n",
    "        # Step 1: Sample a token from the predicted distribution at every position\n",
    "        #         Use torch.multinomial on the flattened probs\n",
    "        #         Hint: probs.view(-1, VOCAB_SIZE) gives shape (L, V)\n",
    "        sampled = ???  # YOUR CODE: shape should be (1, seq_len)\n",
    "\n",
    "        # Step 2: Compute confidence = probability of the sampled token\n",
    "        #         Use probs.gather(-1, ...) to look up the probability of each sampled token\n",
    "        confidence = ???  # YOUR CODE: shape should be (1, seq_len)\n",
    "\n",
    "        # Step 3: Only consider masked positions\n",
    "        #         Set confidence of already-unmasked positions to infinity\n",
    "        #         so they are never selected for unmasking (they're already done)\n",
    "        is_masked = (x == MASK_TOKEN)\n",
    "        confidence[~is_masked] = ???  # YOUR CODE\n",
    "\n",
    "        # Step 4: Determine how many tokens to unmask this step\n",
    "        #         We want to unmask roughly (1/s) of the remaining masked tokens\n",
    "        n_to_unmask = max(1, int(is_masked.sum().item() * (1.0 / s)))\n",
    "\n",
    "        # Step 5: Find the n_to_unmask positions with HIGHEST confidence among masked ones\n",
    "        #         But we set non-masked to inf, so we need the LOWEST confidence values\n",
    "        #         actually ‚Äî we want highest confidence among masked tokens\n",
    "        #         Re-think: set non-masked to -inf so topk picks the masked ones with highest conf\n",
    "        # ==============================\n",
    "\n",
    "        # Fix: set non-masked to -inf for topk selection\n",
    "        conf_for_selection = confidence.clone()\n",
    "        conf_for_selection[~is_masked] = -float('inf')\n",
    "\n",
    "        _, top_idx = conf_for_selection.topk(n_to_unmask, dim=-1)\n",
    "        x.scatter_(1, top_idx, sampled.gather(1, top_idx))\n",
    "\n",
    "        history.append((x[0].cpu().clone(), f'Step {n_steps - s + 1}'))\n",
    "\n",
    "    return x, history"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification: Test your implementation\n",
    "try:\n",
    "    generated, history = generate(model, seq_len=SEQ_LEN, n_steps=8)\n",
    "    assert generated.shape == (1, SEQ_LEN), f\"Wrong shape: {generated.shape}\"\n",
    "    assert (generated != MASK_TOKEN).all(), \"Some positions still masked!\"\n",
    "    print(\"‚úÖ Generation works! Here's the output:\")\n",
    "    print(f\"   Generated sequence: {generated[0].tolist()}\")\n",
    "    print(f\"   Steps recorded: {len(history)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"   Check your TODO implementation above.\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Post Todo\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_post_todo.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_post_todo"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ‚úã Stop and Think\n",
    "Before running the solution, try to answer:\n",
    "1. Why do we set confidence of already-unmasked positions to negative infinity?\n",
    "2. What happens if we unmask ALL tokens in one step instead of gradually?\n",
    "3. Why do function words (common tokens) tend to appear first?\n",
    "\n",
    "*Take a minute. Then scroll down for the solution.*\n",
    "\n",
    "---"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, seq_len=SEQ_LEN, n_steps=8):\n",
    "    \"\"\"Generate a sequence using iterative confidence-based unmasking.\"\"\"\n",
    "    model.eval()\n",
    "    x = torch.full((1, seq_len), MASK_TOKEN, dtype=torch.long, device=device)\n",
    "    history = [(x[0].cpu().clone(), 'Start')]\n",
    "\n",
    "    for s in range(n_steps, 0, -1):\n",
    "        t = torch.tensor([[s / n_steps]], device=device, dtype=torch.float)\n",
    "        logits = model(x, t)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Step 1: Sample tokens from predicted distributions\n",
    "        sampled = torch.multinomial(\n",
    "            probs.view(-1, VOCAB_SIZE), num_samples=1\n",
    "        ).view(1, -1)  # (1, L)\n",
    "\n",
    "        # Step 2: Confidence = probability of the sampled token\n",
    "        confidence = probs.gather(-1, sampled.unsqueeze(-1)).squeeze(-1)  # (1, L)\n",
    "\n",
    "        # Step 3: Only unmask among currently masked positions\n",
    "        is_masked = (x == MASK_TOKEN)\n",
    "\n",
    "        # Step 4: How many to unmask this step\n",
    "        n_to_unmask = max(1, int(is_masked.sum().item() * (1.0 / s)))\n",
    "\n",
    "        # Step 5: Pick the most confident masked positions\n",
    "        conf_for_selection = confidence.clone()\n",
    "        conf_for_selection[~is_masked] = -float('inf')\n",
    "\n",
    "        _, top_idx = conf_for_selection.topk(n_to_unmask, dim=-1)\n",
    "        x.scatter_(1, top_idx, sampled.gather(1, top_idx))\n",
    "\n",
    "        history.append((x[0].cpu().clone(), f'Step {n_steps - s + 1}'))\n",
    "\n",
    "    return x, history\n",
    "\n",
    "# Generate and show\n",
    "generated, history = generate(model, seq_len=SEQ_LEN, n_steps=8)\n",
    "print(\"Generated:\", generated[0].tolist())"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Visualization\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_visualization"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize the full generation process step by step\n",
    "def visualize_generation(history, vocab_size):\n",
    "    \"\"\"Show how tokens are revealed step by step.\"\"\"\n",
    "    n_steps = len(history)\n",
    "    fig, axes = plt.subplots(n_steps, 1, figsize=(14, n_steps * 0.8))\n",
    "\n",
    "    if n_steps == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (seq, label) in zip(axes, history):\n",
    "        tokens = seq.numpy()\n",
    "        for pos in range(len(tokens)):\n",
    "            if tokens[pos] == MASK_TOKEN:\n",
    "                ax.add_patch(plt.Rectangle((pos, 0), 1, 1,\n",
    "                             color='#333333', alpha=0.85))\n",
    "                ax.text(pos + 0.5, 0.5, 'M', ha='center', va='center',\n",
    "                        color='white', fontsize=10, fontweight='bold')\n",
    "            else:\n",
    "                color = plt.cm.Set2(tokens[pos] / vocab_size)\n",
    "                ax.add_patch(plt.Rectangle((pos, 0), 1, 1, color=color))\n",
    "                ax.text(pos + 0.5, 0.5, str(tokens[pos]), ha='center',\n",
    "                        va='center', fontsize=10)\n",
    "\n",
    "        ax.set_xlim(0, len(tokens))\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_ylabel(label, fontsize=10, rotation=0, ha='right', va='center')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    plt.suptitle('Generation Process: Iterative Unmasking',\n",
    "                 fontsize=15, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "generated, history = generate(model, seq_len=SEQ_LEN, n_steps=8)\n",
    "visualize_generation(history, VOCAB_SIZE)"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluating the Model\n",
    "\n",
    "Let us check whether the model actually learned the repeating patterns. We can test this by masking part of a known pattern and seeing if the model recovers it."
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pattern_completion(model, n_tests=100):\n",
    "    \"\"\"Test if the model can complete partially masked patterns.\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for _ in range(n_tests):\n",
    "        # Generate a clean pattern\n",
    "        x_0 = generate_pattern_data(1, SEQ_LEN, VOCAB_SIZE)\n",
    "\n",
    "        # Mask the last half\n",
    "        x_t = x_0.clone()\n",
    "        x_t[0, SEQ_LEN // 2:] = MASK_TOKEN\n",
    "        t = torch.tensor([[0.5]], device=device)\n",
    "\n",
    "        # Predict\n",
    "        logits = model(x_t, t)\n",
    "        preds = logits[0, SEQ_LEN // 2:].argmax(dim=-1)\n",
    "        targets = x_0[0, SEQ_LEN // 2:]\n",
    "\n",
    "        correct += (preds == targets).sum().item()\n",
    "        total += len(targets)\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate_pattern_completion(model)\n",
    "print(f\"Pattern completion accuracy: {accuracy:.1f}%\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Generate multiple sequences and display them\n",
    "print(\"Generated sequences (should show repeating patterns):\\n\")\n",
    "for i in range(8):\n",
    "    generated, _ = generate(model, seq_len=SEQ_LEN, n_steps=10)\n",
    "    seq = generated[0].tolist()\n",
    "    print(f\"  Sequence {i+1}: {seq}\")"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Final\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_final.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_final"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üéØ Final Output: Animated Generation"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animated_generation_grid(model, n_sequences=6, n_steps=10):\n",
    "    \"\"\"Generate multiple sequences and show the unmasking process as a grid.\"\"\"\n",
    "    fig, axes = plt.subplots(n_sequences, n_steps + 1, figsize=(18, n_sequences * 1.2))\n",
    "\n",
    "    for row in range(n_sequences):\n",
    "        _, history = generate(model, seq_len=SEQ_LEN, n_steps=n_steps)\n",
    "\n",
    "        # Pad history if needed\n",
    "        while len(history) < n_steps + 1:\n",
    "            history.append(history[-1])\n",
    "\n",
    "        for col in range(n_steps + 1):\n",
    "            ax = axes[row, col]\n",
    "            seq = history[col][0].numpy()\n",
    "\n",
    "            # Create colored visualization\n",
    "            img = np.zeros((1, SEQ_LEN, 3))\n",
    "            for pos in range(SEQ_LEN):\n",
    "                if seq[pos] == MASK_TOKEN:\n",
    "                    img[0, pos] = [0.2, 0.2, 0.2]  # dark gray for MASK\n",
    "                else:\n",
    "                    c = plt.cm.Set2(seq[pos] / VOCAB_SIZE)[:3]\n",
    "                    img[0, pos] = c\n",
    "\n",
    "            ax.imshow(img, aspect='auto', interpolation='nearest')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "            if row == 0:\n",
    "                ax.set_title(history[col][1], fontsize=9)\n",
    "            if col == 0:\n",
    "                ax.set_ylabel(f'Seq {row+1}', fontsize=9)\n",
    "\n",
    "    plt.suptitle('Diffusion LM Generation: From Masked to Revealed',\n",
    "                 fontsize=15, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"üéâ Each row shows one sequence being generated through iterative unmasking!\")\n",
    "    print(\"   Dark = [MASK], Colors = revealed tokens. Notice how easy tokens appear first.\")\n",
    "\n",
    "animated_generation_grid(model)"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_11_closing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "\n",
    "1. **Order of unmasking:** Did you notice which tokens tend to be revealed first? Why might common tokens (those that appear frequently in patterns) be unmasked earlier?\n",
    "\n",
    "2. **Number of steps:** What happens if you set `n_steps=1` (unmask everything in one shot)? Why is the quality worse? What about `n_steps=50`?\n",
    "\n",
    "3. **Comparison to autoregressive:** If we had built an autoregressive model on the same data, it would generate left-to-right. What advantage does our diffusion model have for pattern completion?\n",
    "\n",
    "### üèÜ Optional Challenges\n",
    "\n",
    "1. **Temperature sampling:** Add a `temperature` parameter to the generation function. How does temperature affect the diversity vs quality tradeoff?\n",
    "\n",
    "2. **Different data:** Swap the synthetic patterns for a character-level text dataset. Does the model learn to generate readable text?\n",
    "\n",
    "3. **Masking schedules:** Instead of uniform random $t$, try a cosine schedule where more training time is spent at high masking ratios. Does this improve generation quality?\n",
    "\n",
    "**Next notebook:** We will demonstrate the **reversal curse** ‚Äî training both an autoregressive model and a diffusion model on the same data, and showing that only the diffusion model can reason bidirectionally."
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "chatbot"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üí¨ AI Teaching Assistant ‚Äî Click ‚ñ∂ to start\n",
    "#@markdown This AI chatbot reads your notebook and can answer questions about any concept, code, or exercise.\n",
    "\n",
    "import json as _json\n",
    "import requests as _requests\n",
    "from google.colab import output as _output\n",
    "from IPython.display import display, HTML as _HTML, Markdown as _Markdown\n",
    "\n",
    "# --- Read notebook content for context ---\n",
    "def _get_notebook_context():\n",
    "    try:\n",
    "        from google.colab import _message\n",
    "        nb = _message.blocking_request(\"get_ipynb\", request=\"\", timeout_sec=10)\n",
    "        cells = nb.get(\"ipynb\", {}).get(\"cells\", [])\n",
    "        parts = []\n",
    "        for cell in cells:\n",
    "            src = \"\".join(cell.get(\"source\", []))\n",
    "            tags = cell.get(\"metadata\", {}).get(\"tags\", [])\n",
    "            if \"chatbot\" in tags:\n",
    "                continue\n",
    "            if src.strip():\n",
    "                ct = cell.get(\"cell_type\", \"unknown\")\n",
    "                parts.append(f\"[{ct.upper()}]\\n{src}\")\n",
    "        return \"\\n\\n---\\n\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"Notebook content unavailable.\"\n",
    "\n",
    "_NOTEBOOK_CONTEXT = _get_notebook_context()\n",
    "_CHAT_HISTORY = []\n",
    "_API_URL = \"https://course-creator-brown.vercel.app/api/chat\"\n",
    "\n",
    "def _notebook_chat(question):\n",
    "    global _CHAT_HISTORY\n",
    "    try:\n",
    "        resp = _requests.post(_API_URL, json={\n",
    "            'question': question,\n",
    "            'context': _NOTEBOOK_CONTEXT[:100000],\n",
    "            'history': _CHAT_HISTORY[-10:],\n",
    "        }, timeout=60)\n",
    "        data = resp.json()\n",
    "        answer = data.get('answer', 'Sorry, I could not generate a response.')\n",
    "        _CHAT_HISTORY.append({'role': 'user', 'content': question})\n",
    "        _CHAT_HISTORY.append({'role': 'assistant', 'content': answer})\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f'Error connecting to teaching assistant: {str(e)}'\n",
    "\n",
    "_output.register_callback('notebook_chat', _notebook_chat)\n",
    "\n",
    "def ask(question):\n",
    "    \"\"\"Ask the AI teaching assistant a question about this notebook.\"\"\"\n",
    "    answer = _notebook_chat(question)\n",
    "    display(_Markdown(answer))\n",
    "\n",
    "print(\"\\u2705 AI Teaching Assistant is ready!\")\n",
    "print(\"\\U0001f4a1 Use the chat below, or call ask(\\'your question\\') in any cell.\")\n",
    "\n",
    "# --- Display chat widget ---\n",
    "display(_HTML('''<style>\n  .vc-wrap{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;max-width:100%;border-radius:16px;overflow:hidden;box-shadow:0 4px 24px rgba(0,0,0,.12);background:#fff;border:1px solid #e5e7eb}\n  .vc-hdr{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;padding:16px 20px;display:flex;align-items:center;gap:12px}\n  .vc-avatar{width:42px;height:42px;background:rgba(255,255,255,.2);border-radius:50%;display:flex;align-items:center;justify-content:center;font-size:22px}\n  .vc-hdr h3{font-size:16px;font-weight:600;margin:0}\n  .vc-hdr p{font-size:12px;opacity:.85;margin:2px 0 0}\n  .vc-msgs{height:420px;overflow-y:auto;padding:16px;background:#f8f9fb;display:flex;flex-direction:column;gap:10px}\n  .vc-msg{display:flex;flex-direction:column;animation:vc-fade .25s ease}\n  .vc-msg.user{align-items:flex-end}\n  .vc-msg.bot{align-items:flex-start}\n  .vc-bbl{max-width:85%;padding:10px 14px;border-radius:16px;font-size:14px;line-height:1.55;word-wrap:break-word}\n  .vc-msg.user .vc-bbl{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border-bottom-right-radius:4px}\n  .vc-msg.bot .vc-bbl{background:#fff;color:#1a1a2e;border:1px solid #e8e8e8;border-bottom-left-radius:4px}\n  .vc-bbl code{background:rgba(0,0,0,.07);padding:2px 6px;border-radius:4px;font-size:13px;font-family:'Fira Code',monospace}\n  .vc-bbl pre{background:#1e1e2e;color:#cdd6f4;padding:12px;border-radius:8px;overflow-x:auto;margin:8px 0;font-size:13px}\n  .vc-bbl pre code{background:none;padding:0;color:inherit}\n  .vc-bbl h3,.vc-bbl h4{margin:10px 0 4px;font-size:15px}\n  .vc-bbl ul,.vc-bbl ol{margin:4px 0;padding-left:20px}\n  .vc-bbl li{margin:2px 0}\n  .vc-chips{display:flex;flex-wrap:wrap;gap:8px;padding:0 16px 12px;background:#f8f9fb}\n  .vc-chip{background:#fff;border:1px solid #d1d5db;border-radius:20px;padding:6px 14px;font-size:12px;cursor:pointer;transition:all .15s;color:#4b5563}\n  .vc-chip:hover{border-color:#667eea;color:#667eea;background:#f0f0ff}\n  .vc-input{display:flex;padding:12px 16px;background:#fff;border-top:1px solid #eee;gap:8px}\n  .vc-input input{flex:1;padding:10px 16px;border:2px solid #e8e8e8;border-radius:24px;font-size:14px;outline:none;transition:border-color .2s}\n  .vc-input input:focus{border-color:#667eea}\n  .vc-input button{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border:none;border-radius:50%;width:42px;height:42px;cursor:pointer;display:flex;align-items:center;justify-content:center;font-size:18px;transition:transform .1s}\n  .vc-input button:hover{transform:scale(1.05)}\n  .vc-input button:disabled{opacity:.5;cursor:not-allowed;transform:none}\n  .vc-typing{display:flex;gap:5px;padding:4px 0}\n  .vc-typing span{width:8px;height:8px;background:#667eea;border-radius:50%;animation:vc-bounce 1.4s infinite ease-in-out}\n  .vc-typing span:nth-child(2){animation-delay:.2s}\n  .vc-typing span:nth-child(3){animation-delay:.4s}\n  @keyframes vc-bounce{0%,80%,100%{transform:scale(0)}40%{transform:scale(1)}}\n  @keyframes vc-fade{from{opacity:0;transform:translateY(8px)}to{opacity:1;transform:translateY(0)}}\n  .vc-note{text-align:center;font-size:11px;color:#9ca3af;padding:8px 16px 12px;background:#fff}\n</style>\n<div class=\"vc-wrap\">\n  <div class=\"vc-hdr\">\n    <div class=\"vc-avatar\">&#129302;</div>\n    <div>\n      <h3>Vizuara Teaching Assistant</h3>\n      <p>Ask me anything about this notebook</p>\n    </div>\n  </div>\n  <div class=\"vc-msgs\" id=\"vcMsgs\">\n    <div class=\"vc-msg bot\">\n      <div class=\"vc-bbl\">&#128075; Hi! I've read through this entire notebook. Ask me about any concept, code block, or exercise &mdash; I'm here to help you learn!</div>\n    </div>\n  </div>\n  <div class=\"vc-chips\" id=\"vcChips\">\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Explain the main concept</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Help with the TODO exercise</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Summarize what I learned</span>\n  </div>\n  <div class=\"vc-input\">\n    <input type=\"text\" id=\"vcIn\" placeholder=\"Ask about concepts, code, exercises...\" />\n    <button id=\"vcSend\" onclick=\"vcSendMsg()\">&#10148;</button>\n  </div>\n  <div class=\"vc-note\">AI-generated &middot; Verify important information &middot; <a href=\"#\" onclick=\"vcClear();return false\" style=\"color:#667eea\">Clear chat</a></div>\n</div>\n<script>\n(function(){\n  var msgs=document.getElementById('vcMsgs'),inp=document.getElementById('vcIn'),\n      btn=document.getElementById('vcSend'),chips=document.getElementById('vcChips');\n\n  function esc(s){var d=document.createElement('div');d.textContent=s;return d.innerHTML}\n\n  function md(t){\n    return t\n      .replace(/```(\\w*)\\n([\\s\\S]*?)```/g,function(_,l,c){return '<pre><code>'+esc(c)+'</code></pre>'})\n      .replace(/`([^`]+)`/g,'<code>$1</code>')\n      .replace(/\\*\\*([^*]+)\\*\\*/g,'<strong>$1</strong>')\n      .replace(/\\*([^*]+)\\*/g,'<em>$1</em>')\n      .replace(/^#### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^## (.+)$/gm,'<h3>$1</h3>')\n      .replace(/^\\d+\\. (.+)$/gm,'<li>$1</li>')\n      .replace(/^- (.+)$/gm,'<li>$1</li>')\n      .replace(/\\n\\n/g,'<br><br>')\n      .replace(/\\n/g,'<br>');\n  }\n\n  function addMsg(text,isUser){\n    var m=document.createElement('div');m.className='vc-msg '+(isUser?'user':'bot');\n    var b=document.createElement('div');b.className='vc-bbl';\n    b.innerHTML=isUser?esc(text):md(text);\n    m.appendChild(b);msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function showTyping(){\n    var m=document.createElement('div');m.className='vc-msg bot';m.id='vcTyping';\n    m.innerHTML='<div class=\"vc-bbl\"><div class=\"vc-typing\"><span></span><span></span><span></span></div></div>';\n    msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function hideTyping(){var e=document.getElementById('vcTyping');if(e)e.remove()}\n\n  window.vcSendMsg=function(){\n    var q=inp.value.trim();if(!q)return;\n    inp.value='';chips.style.display='none';\n    addMsg(q,true);showTyping();btn.disabled=true;\n    google.colab.kernel.invokeFunction('notebook_chat',[q],{})\n      .then(function(r){\n        hideTyping();\n        var a=r.data['application/json'];\n        addMsg(typeof a==='string'?a:JSON.stringify(a),false);\n      })\n      .catch(function(){\n        hideTyping();\n        addMsg('Sorry, I encountered an error. Please check your internet connection and try again.',false);\n      })\n      .finally(function(){btn.disabled=false;inp.focus()});\n  };\n\n  window.vcAsk=function(q){inp.value=q;vcSendMsg()};\n  window.vcClear=function(){\n    msgs.innerHTML='<div class=\"vc-msg bot\"><div class=\"vc-bbl\">&#128075; Chat cleared. Ask me anything!</div></div>';\n    chips.style.display='flex';\n  };\n\n  inp.addEventListener('keypress',function(e){if(e.key==='Enter')vcSendMsg()});\n  inp.focus();\n})();\n</script>'''))"
   ],
   "id": "vizuara_chatbot"
  }
 ]
}