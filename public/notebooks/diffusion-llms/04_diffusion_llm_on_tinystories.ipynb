{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Diffusion LLM on TinyStories â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1b5CC6G-W_RszA4l3kZ1pQHVSrFdbh7aI\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Motivation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_motivation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_motivation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Diffusion LLM on TinyStories\n",
    "\n",
    "*Part 4 of the Vizuara series on Diffusion Language Models*\n",
    "*Estimated time: 50 minutes (includes ~10 min training)*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://pods.vizuara.ai/courses/diffusion-llms/practice/4/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebooks, we built diffusion language models on synthetic patterns. That was great for understanding the mechanics. But the real test of any language model is: **can it generate coherent, readable text?**\n",
    "\n",
    "In this notebook, we will train a masked diffusion language model on the **TinyStories** dataset â€” a collection of short stories written in simple English that a 3-4 year old could understand. By the end, our model will generate short stories through iterative unmasking, and you will watch words materialize from a sea of [MASK] tokens into a readable narrative.\n",
    "\n",
    "**Teaser â€” what our trained model will produce:**\n",
    "\n",
    "```\n",
    "Step 1:  [M] [M] [M] [M] [M] [M] [M] [M] [M] [M] [M] [M] ...\n",
    "Step 3:  [M] was [M] [M] [M] . [M] [M] a [M] [M] the [M] ...\n",
    "Step 6:  she was [M] happy [M] . [M] had a [M] big the dog ...\n",
    "Step 10: she was very happy today . she had a new big the dog .\n",
    "```\n",
    "\n",
    "Not Shakespeare â€” but a real diffusion model generating real English, trained in under 10 minutes on a single GPU."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install datasets library if needed\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "    print(\"datasets library ready\")\n",
    "except ImportError:\n",
    "    !pip install datasets -q\n",
    "    from datasets import load_dataset\n",
    "    print(\"datasets library installed\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_intuition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### Why TinyStories?\n",
    "\n",
    "Training a language model that generates coherent text usually requires billions of parameters and weeks of compute. But the TinyStories dataset (Eldan & Li, 2023) is special:\n",
    "\n",
    "- Stories use only vocabulary a **3-4 year old** would understand\n",
    "- Each story is short (50-200 words)\n",
    "- Grammar is simple but correct\n",
    "- Stories have narrative structure (beginning, middle, end)\n",
    "\n",
    "This means a **small model** (a few million parameters) can learn meaningful patterns in **minutes**. Perfect for our Colab notebook.\n",
    "\n",
    "### How Diffusion Generation Differs from Autoregressive\n",
    "\n",
    "When GPT generates a story, it writes one word at a time, left to right. It commits to \"Once\" before knowing the story is about a dog. It commits to \"Once upon\" before knowing the ending.\n",
    "\n",
    "Our diffusion model will generate stories differently:\n",
    "1. Start: `[M] [M] [M] [M] [M] [M] [M] [M] [M] [M]`\n",
    "2. Structure words appear first: `[M] was [M] [M] . [M] had [M] [M] .`\n",
    "3. Content fills in: `She was very happy . She had a dog .`\n",
    "\n",
    "The model \"plans\" the whole story simultaneously, committing to easy structural tokens first and content words later.\n",
    "\n",
    "### ðŸ¤” Think About This\n",
    "\n",
    "In a TinyStory like \"Once upon a time, there was a little girl named Lily. She liked to play in the park.\"\n",
    "\n",
    "- Which words would be **easiest** for the model to predict (appear first during generation)?\n",
    "- Which words would be **hardest** (appear last)?\n",
    "\n",
    "*Hint: Think about which words are most predictable from context vs which carry the most unique information.*"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_math"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics â€” Quick Recap\n",
    "\n",
    "We use the same masked diffusion framework from Notebook 1:\n",
    "\n",
    "**Forward process:** Mask each token independently with probability $t$:\n",
    "$$q(x_t^i \\mid x_0^i) = (1 - t) \\cdot \\mathbb{1}[x_t^i = x_0^i] + t \\cdot \\mathbb{1}[x_t^i = \\texttt{[MASK]}]$$\n",
    "\n",
    "**Training loss:** Cross-entropy at masked positions, weighted by $1/(t \\cdot L)$:\n",
    "$$\\mathcal{L} = -\\mathbb{E}_{t} \\left[ \\frac{1}{t \\cdot L} \\sum_{i:\\, x_t^i = \\texttt{[MASK]}} \\log p_\\theta(x_0^i \\mid x_t) \\right]$$\n",
    "\n",
    "**What this says computationally:** At each training step, we randomly mask some fraction of the story, then ask the Transformer to predict the missing words. The model sees the whole partially-masked story bidirectionally and must fill in the blanks."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Data Loading\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_data_loading.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_data_loading"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It â€” Component by Component\n",
    "\n",
    "### 4.1 Load and Prepare TinyStories"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading TinyStories dataset...\")\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "print(f\"Total stories: {len(dataset):,}\")\n",
    "\n",
    "# Peek at a few stories\n",
    "for i in range(3):\n",
    "    story = dataset[i]['text']\n",
    "    print(f\"\\n--- Story {i+1} (first 200 chars) ---\")\n",
    "    print(story[:200])"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Tokenizer\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_tokenizer.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_tokenizer"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Build a Word-Level Tokenizer\n",
    "\n",
    "We build a simple word-level tokenizer from the data. This keeps the vocabulary small and each token meaningful."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    \"\"\"Word-level tokenizer with a fixed vocabulary.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size=2000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        # Reserve special tokens\n",
    "        self.mask_token = \"[MASK]\"\n",
    "        self.pad_token = \"[PAD]\"\n",
    "        self.unk_token = \"[UNK]\"\n",
    "\n",
    "    def build_vocab(self, texts, max_texts=50000):\n",
    "        \"\"\"Build vocabulary from the most common words.\"\"\"\n",
    "        word_counts = Counter()\n",
    "        for i, text in enumerate(texts):\n",
    "            if i >= max_texts:\n",
    "                break\n",
    "            words = text.lower().split()\n",
    "            word_counts.update(words)\n",
    "\n",
    "        # Special tokens get indices 0, 1, 2\n",
    "        self.word2idx = {\n",
    "            self.mask_token: 0,\n",
    "            self.pad_token: 1,\n",
    "            self.unk_token: 2,\n",
    "        }\n",
    "\n",
    "        # Add most common words\n",
    "        for word, _ in word_counts.most_common(self.vocab_size - 3):\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "\n",
    "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
    "        print(f\"Vocabulary size: {len(self.word2idx)}\")\n",
    "        print(f\"Top 20 words: {[w for w, _ in word_counts.most_common(20)]}\")\n",
    "\n",
    "    def encode(self, text, max_len=64):\n",
    "        \"\"\"Convert text to token IDs, with padding/truncation.\"\"\"\n",
    "        words = text.lower().split()[:max_len]\n",
    "        ids = [self.word2idx.get(w, 2) for w in words]  # 2 = UNK\n",
    "        # Pad to max_len\n",
    "        ids = ids + [1] * (max_len - len(ids))  # 1 = PAD\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"Convert token IDs back to text.\"\"\"\n",
    "        words = []\n",
    "        for idx in ids:\n",
    "            if idx == 1:  # PAD\n",
    "                break\n",
    "            if idx == 0:  # MASK\n",
    "                words.append(\"[M]\")\n",
    "            else:\n",
    "                words.append(self.idx2word.get(idx, \"[?]\"))\n",
    "        return \" \".join(words)\n",
    "\n",
    "\n",
    "# Build tokenizer\n",
    "VOCAB_SIZE = 2000\n",
    "SEQ_LEN = 64\n",
    "tokenizer = SimpleTokenizer(vocab_size=VOCAB_SIZE)\n",
    "tokenizer.build_vocab([d['text'] for d in dataset])"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Test the tokenizer\n",
    "test_story = dataset[0]['text']\n",
    "encoded = tokenizer.encode(test_story, max_len=SEQ_LEN)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"Original: {test_story[:200]}\")\n",
    "print(f\"\\nEncoded (first 20): {encoded[:20]}\")\n",
    "print(f\"\\nDecoded: {decoded[:200]}\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Data Prep\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_data_prep.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_data_prep"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Prepare the Training Data"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset, tokenizer, max_len, n_samples=30000):\n",
    "    \"\"\"Convert stories to tensor of token IDs.\"\"\"\n",
    "    all_ids = []\n",
    "    for i in range(min(n_samples, len(dataset))):\n",
    "        ids = tokenizer.encode(dataset[i]['text'], max_len=max_len)\n",
    "        # Skip very short stories (mostly padding)\n",
    "        if sum(1 for x in ids if x > 1) >= 10:  # at least 10 real tokens\n",
    "            all_ids.append(ids)\n",
    "\n",
    "    data = torch.tensor(all_ids, dtype=torch.long)\n",
    "    print(f\"Prepared {len(data):,} stories, shape: {data.shape}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = prepare_dataset(dataset, tokenizer, SEQ_LEN, n_samples=30000)"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Dataset statistics\n",
    "real_token_counts = (train_data > 1).sum(dim=1).float()\n",
    "print(f\"Average tokens per story: {real_token_counts.mean():.1f}\")\n",
    "print(f\"Min / Max tokens: {real_token_counts.min():.0f} / {real_token_counts.max():.0f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.hist(real_token_counts.numpy(), bins=30, color='#1565c0', alpha=0.7, edgecolor='white')\n",
    "plt.xlabel('Number of Real Tokens per Story')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Story Length Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Model\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_model.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The Diffusion Transformer\n",
    "\n",
    "This is a proper implementation with layer normalization, positional encoding, and time conditioning â€” scaled up from our toy model."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(max_len).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class StoryDiffusionLM(nn.Module):\n",
    "    \"\"\"Bidirectional Transformer for story generation via masked diffusion.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model=256, n_heads=4, n_layers=6,\n",
    "                 max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, d_model),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_model, d_model),\n",
    "        )\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)\n",
    "        self.ln_final = nn.LayerNorm(d_model)\n",
    "        self.output_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, x_t, t):\n",
    "        \"\"\"\n",
    "        x_t: (B, L) masked token IDs\n",
    "        t: (B, 1) masking ratio\n",
    "        \"\"\"\n",
    "        h = self.token_embed(x_t) * math.sqrt(self.d_model)\n",
    "        h = self.pos_enc(h)\n",
    "        h = h + self.time_mlp(t).unsqueeze(1)  # broadcast time to all positions\n",
    "        h = self.transformer(h)  # bidirectional!\n",
    "        h = self.ln_final(h)\n",
    "        return self.output_head(h)  # (B, L, V)\n",
    "\n",
    "\n",
    "# Create model\n",
    "D_MODEL = 256\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 6\n",
    "model = StoryDiffusionLM(\n",
    "    vocab_size=VOCAB_SIZE, d_model=D_MODEL,\n",
    "    n_heads=N_HEADS, n_layers=N_LAYERS, max_len=SEQ_LEN\n",
    ").to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params:,}\")\n",
    "print(f\"Architecture: {N_LAYERS} layers, d_model={D_MODEL}, {N_HEADS} heads\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Key Design Choices\n",
    "\n",
    "Why these hyperparameters?\n",
    "- **d_model=256, 6 layers:** ~4M parameters â€” small enough to train in minutes on a T4, large enough to learn meaningful patterns in simple stories.\n",
    "- **vocab_size=2000:** Covers the most common words in TinyStories. Rare words become [UNK], but that is fine for learning structure.\n",
    "- **seq_len=64:** Long enough for a short story (2-4 sentences), short enough for fast training."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Masking\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_masking.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_masking"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Forward Masking Process"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_TOKEN_ID = 0\n",
    "PAD_TOKEN_ID = 1\n",
    "\n",
    "def mask_tokens(x_0, t):\n",
    "    \"\"\"Apply forward masking, but NEVER mask PAD tokens.\"\"\"\n",
    "    mask = torch.rand_like(x_0.float()) < t\n",
    "    # Don't mask padding tokens (they stay as PAD)\n",
    "    is_pad = (x_0 == PAD_TOKEN_ID)\n",
    "    mask = mask & ~is_pad\n",
    "\n",
    "    x_t = x_0.clone()\n",
    "    x_t[mask] = MASK_TOKEN_ID\n",
    "    return x_t, mask"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualize masking on a real story\n",
    "sample_idx = 0\n",
    "sample = train_data[sample_idx:sample_idx+1].to(device)\n",
    "print(f\"Original: {tokenizer.decode(sample[0].tolist())}\\n\")\n",
    "\n",
    "for t_val in [0.2, 0.5, 0.8, 1.0]:\n",
    "    t = torch.tensor([[t_val]], device=device)\n",
    "    masked, _ = mask_tokens(sample, t)\n",
    "    print(f\"t={t_val}: {tokenizer.decode(masked[0].tolist())}\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Training Loop"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_story_model(model, train_data, n_epochs=3, batch_size=64, lr=3e-4):\n",
    "    \"\"\"Train the story diffusion model.\"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    n_batches = len(train_data) // batch_size\n",
    "    total_steps = n_epochs * n_batches\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, total_steps)\n",
    "\n",
    "    losses = []\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Shuffle data\n",
    "        perm = torch.randperm(len(train_data))\n",
    "        train_data_shuffled = train_data[perm]\n",
    "\n",
    "        epoch_losses = []\n",
    "        for i in range(0, len(train_data_shuffled) - batch_size, batch_size):\n",
    "            x_0 = train_data_shuffled[i:i+batch_size].to(device)\n",
    "\n",
    "            # Sample masking ratio\n",
    "            t = torch.rand(batch_size, 1, device=device) * 0.98 + 0.02\n",
    "\n",
    "            # Forward masking\n",
    "            x_t, mask = mask_tokens(x_0, t)\n",
    "\n",
    "            # Predict original tokens\n",
    "            logits = model(x_t, t)\n",
    "\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            # Loss at masked positions only\n",
    "            loss = F.cross_entropy(logits[mask], x_0[mask])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            epoch_losses.append(loss.item())\n",
    "            step += 1\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                avg_loss = np.mean(epoch_losses[-100:])\n",
    "                print(f\"  Epoch {epoch+1}/{n_epochs} | Step {step}/{total_steps} | \"\n",
    "                      f\"Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1} complete | Avg loss: {np.mean(epoch_losses):.4f}\\n\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "print(\"Training the story diffusion model...\")\n",
    "print(\"(This will take ~5-10 minutes on a T4 GPU)\\n\")\n",
    "start_time = time.time()\n",
    "losses = train_story_model(model, train_data, n_epochs=3, batch_size=64)\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nTraining complete in {elapsed/60:.1f} minutes!\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Training curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "window = 50\n",
    "smoothed = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "plt.plot(smoothed, color='#1565c0', linewidth=1.5)\n",
    "plt.xlabel('Training Step', fontsize=12)\n",
    "plt.ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "plt.title('Story Diffusion LM Training', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_todo.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_todo"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ðŸ”§ Your Turn: Implement Story Generation\n",
    "\n",
    "### TODO: Complete the story generation function"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_story(model, tokenizer, seq_len=SEQ_LEN, n_steps=15,\n",
    "                   temperature=0.8):\n",
    "    \"\"\"Generate a story using iterative confidence-based unmasking.\n",
    "\n",
    "    Args:\n",
    "        model: Trained StoryDiffusionLM\n",
    "        tokenizer: SimpleTokenizer\n",
    "        seq_len: Length of sequence to generate\n",
    "        n_steps: Number of denoising steps\n",
    "        temperature: Sampling temperature (lower = more conservative)\n",
    "\n",
    "    Returns:\n",
    "        story_text: The generated story as a string\n",
    "        history: List of (text, step_label) tuples for visualization\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    x = torch.full((1, seq_len), MASK_TOKEN_ID, dtype=torch.long, device=device)\n",
    "    history = [(tokenizer.decode(x[0].tolist()), 'Start')]\n",
    "\n",
    "    for s in range(n_steps, 0, -1):\n",
    "        t = torch.tensor([[s / n_steps]], device=device, dtype=torch.float)\n",
    "        logits = model(x, t)\n",
    "\n",
    "        # ============ TODO ============\n",
    "        # Step 1: Apply temperature scaling to logits\n",
    "        #         Divide logits by the temperature parameter\n",
    "        scaled_logits = ???  # YOUR CODE\n",
    "\n",
    "        # Step 2: Compute probabilities\n",
    "        probs = ???  # YOUR CODE: softmax of scaled_logits\n",
    "\n",
    "        # Step 3: Sample tokens from the distributions\n",
    "        sampled = torch.multinomial(\n",
    "            probs.view(-1, VOCAB_SIZE), num_samples=1\n",
    "        ).view(1, -1)\n",
    "\n",
    "        # Step 4: Compute confidence for each sampled token\n",
    "        confidence = ???  # YOUR CODE: probability of the sampled token\n",
    "\n",
    "        # Step 5: Only consider masked (non-PAD, non-already-unmasked) positions\n",
    "        is_masked = (x == MASK_TOKEN_ID)\n",
    "\n",
    "        # Step 6: Compute how many tokens to unmask\n",
    "        n_unmask = max(1, int(is_masked.sum().item() / s))\n",
    "\n",
    "        # Step 7: Select the most confident masked positions\n",
    "        conf = confidence.clone()\n",
    "        conf[~is_masked] = -float('inf')\n",
    "        _, top_idx = conf.topk(min(n_unmask, max(1, is_masked.sum().item())), dim=-1)\n",
    "\n",
    "        # Step 8: Unmask those positions\n",
    "        x.scatter_(1, top_idx, sampled.gather(1, top_idx))\n",
    "        # ==============================\n",
    "\n",
    "        if s % max(1, n_steps // 5) == 0 or s == 1:\n",
    "            history.append((tokenizer.decode(x[0].tolist()), f'Step {n_steps - s + 1}'))\n",
    "\n",
    "    story = tokenizer.decode(x[0].tolist())\n",
    "    return story, history"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Verification\n",
    "try:\n",
    "    story, history = generate_story(model, tokenizer, n_steps=15)\n",
    "    assert isinstance(story, str), \"Should return a string\"\n",
    "    assert \"[M]\" not in story, \"All masks should be resolved\"\n",
    "    print(\"âœ… Story generation works!\")\n",
    "    print(f\"\\nGenerated story:\\n{story}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Post Todo\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_post_todo.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_11_post_todo"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### âœ‹ Stop and Think\n",
    "Before seeing the solution, consider:\n",
    "1. What effect does temperature have? (Low = safe/repetitive, High = creative/chaotic)\n",
    "2. Why do we divide logits by temperature before softmax?\n",
    "\n",
    "---"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_story(model, tokenizer, seq_len=SEQ_LEN, n_steps=15,\n",
    "                   temperature=0.8):\n",
    "    \"\"\"Generate a story using iterative confidence-based unmasking.\"\"\"\n",
    "    model.eval()\n",
    "    x = torch.full((1, seq_len), MASK_TOKEN_ID, dtype=torch.long, device=device)\n",
    "    history = [(tokenizer.decode(x[0].tolist()), 'Start')]\n",
    "\n",
    "    for s in range(n_steps, 0, -1):\n",
    "        t = torch.tensor([[s / n_steps]], device=device, dtype=torch.float)\n",
    "        logits = model(x, t)\n",
    "\n",
    "        # Temperature scaling\n",
    "        scaled_logits = logits / temperature\n",
    "        probs = F.softmax(scaled_logits, dim=-1)\n",
    "\n",
    "        # Sample tokens\n",
    "        sampled = torch.multinomial(\n",
    "            probs.view(-1, VOCAB_SIZE), num_samples=1\n",
    "        ).view(1, -1)\n",
    "\n",
    "        # Confidence = probability of sampled token\n",
    "        confidence = probs.gather(-1, sampled.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # Only unmask among currently masked positions\n",
    "        is_masked = (x == MASK_TOKEN_ID)\n",
    "        n_unmask = max(1, int(is_masked.sum().item() / s))\n",
    "\n",
    "        # Pick most confident masked positions\n",
    "        conf = confidence.clone()\n",
    "        conf[~is_masked] = -float('inf')\n",
    "        k = min(n_unmask, max(1, is_masked.sum().item()))\n",
    "        _, top_idx = conf.topk(k, dim=-1)\n",
    "        x.scatter_(1, top_idx, sampled.gather(1, top_idx))\n",
    "\n",
    "        if s % max(1, n_steps // 5) == 0 or s == 1:\n",
    "            history.append((tokenizer.decode(x[0].tolist()), f'Step {n_steps - s + 1}'))\n",
    "\n",
    "    story = tokenizer.decode(x[0].tolist())\n",
    "    return story, history"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Stories\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_stories.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_12_stories"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display multiple stories\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATED STORIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(5):\n",
    "    story, _ = generate_story(model, tokenizer, n_steps=20, temperature=0.8)\n",
    "    print(f\"\\nStory {i+1}:\")\n",
    "    print(f\"  {story}\")\n",
    "    print(f\"  {'â”€' * 50}\")"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Visualization\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_13_visualization"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ðŸ“Š Visualizing the Unmasking Process"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_story_generation(history):\n",
    "    \"\"\"Show how a story materializes from masks step by step.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16, len(history) * 0.6 + 1))\n",
    "\n",
    "    for row, (text, label) in enumerate(history):\n",
    "        words = text.split()\n",
    "        x_offset = 0\n",
    "        for word in words:\n",
    "            if word == \"[M]\":\n",
    "                color = '#e0e0e0'\n",
    "                text_color = '#999999'\n",
    "            else:\n",
    "                # Color real words by their first character\n",
    "                hue = (ord(word[0]) % 10) / 10\n",
    "                color = plt.cm.Pastel1(hue)\n",
    "                text_color = '#333333'\n",
    "\n",
    "            ax.text(x_offset, -row, word + \" \",\n",
    "                    fontsize=10, fontfamily='monospace',\n",
    "                    color=text_color,\n",
    "                    bbox=dict(boxstyle='round,pad=0.15', facecolor=color,\n",
    "                              edgecolor='none', alpha=0.7))\n",
    "            x_offset += len(word) + 1.5\n",
    "\n",
    "        ax.text(-3, -row, label, fontsize=10, fontweight='bold',\n",
    "                ha='right', va='center', color='#666666')\n",
    "\n",
    "    ax.set_xlim(-5, 80)\n",
    "    ax.set_ylim(-len(history) + 0.5, 1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Story Generation: Watch Words Materialize',\n",
    "                 fontsize=15, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate one story with full history\n",
    "story, history = generate_story(model, tokenizer, n_steps=15, temperature=0.8)\n",
    "visualize_story_generation(history)"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Word Order\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_word_order.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_14_word_order"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Words Appear First?"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def track_unmasking_order(model, tokenizer, n_steps=20):\n",
    "    \"\"\"Track the order in which tokens are unmasked during generation.\"\"\"\n",
    "    model.eval()\n",
    "    x = torch.full((1, SEQ_LEN), MASK_TOKEN_ID, dtype=torch.long, device=device)\n",
    "    unmask_order = [None] * SEQ_LEN  # When each position was unmasked\n",
    "\n",
    "    for step_num, s in enumerate(range(n_steps, 0, -1)):\n",
    "        t = torch.tensor([[s / n_steps]], device=device, dtype=torch.float)\n",
    "        logits = model(x, t)\n",
    "        probs = F.softmax(logits / 0.8, dim=-1)\n",
    "        sampled = torch.multinomial(probs.view(-1, VOCAB_SIZE), 1).view(1, -1)\n",
    "        confidence = probs.gather(-1, sampled.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        is_masked = (x == MASK_TOKEN_ID)\n",
    "        n_unmask = max(1, int(is_masked.sum().item() / s))\n",
    "\n",
    "        conf = confidence.clone()\n",
    "        conf[~is_masked] = -float('inf')\n",
    "        k = min(n_unmask, max(1, is_masked.sum().item()))\n",
    "        _, top_idx = conf.topk(k, dim=-1)\n",
    "\n",
    "        # Record which step each position was unmasked\n",
    "        for idx in top_idx[0].tolist():\n",
    "            if unmask_order[idx] is None:\n",
    "                unmask_order[idx] = step_num\n",
    "\n",
    "        x.scatter_(1, top_idx, sampled.gather(1, top_idx))\n",
    "\n",
    "    final_tokens = [tokenizer.idx2word.get(x[0, i].item(), '?') for i in range(SEQ_LEN)]\n",
    "    return final_tokens, unmask_order\n",
    "\n",
    "\n",
    "# Run multiple times and aggregate\n",
    "early_words = Counter()\n",
    "late_words = Counter()\n",
    "\n",
    "for _ in range(50):\n",
    "    tokens, order = track_unmasking_order(model, tokenizer)\n",
    "    n_steps_used = max(o for o in order if o is not None) + 1\n",
    "    mid = n_steps_used // 2\n",
    "\n",
    "    for tok, step in zip(tokens, order):\n",
    "        if tok in ('[PAD]', '[MASK]', '[UNK]'):\n",
    "            continue\n",
    "        if step is not None:\n",
    "            if step <= mid // 2:\n",
    "                early_words[tok] += 1\n",
    "            elif step >= n_steps_used - mid // 2:\n",
    "                late_words[tok] += 1\n",
    "\n",
    "# Show results\n",
    "print(\"Words that appear FIRST (most confident, easiest):\")\n",
    "for word, count in early_words.most_common(15):\n",
    "    print(f\"  '{word}': {count} times\")\n",
    "\n",
    "print(\"\\nWords that appear LAST (least confident, hardest):\")\n",
    "for word, count in late_words.most_common(15):\n",
    "    print(f\"  '{word}': {count} times\")"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualize early vs late words\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "top_early = early_words.most_common(12)\n",
    "top_late = late_words.most_common(12)\n",
    "\n",
    "ax1.barh([w for w, _ in top_early], [c for _, c in top_early],\n",
    "         color='#66bb6a', edgecolor='white')\n",
    "ax1.set_title('Appear FIRST (Structure Words)', fontsize=13, color='#2e7d32')\n",
    "ax1.set_xlabel('Frequency')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "ax2.barh([w for w, _ in top_late], [c for _, c in top_late],\n",
    "         color='#ef5350', edgecolor='white')\n",
    "ax2.set_title('Appear LAST (Content Words)', fontsize=13, color='#c62828')\n",
    "ax2.set_xlabel('Frequency')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.suptitle('Unmasking Order: Easy Words First, Hard Words Last', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Notice: function words (the, a, was, is) appear first.\")\n",
    "print(\"   Content words (names, adjectives, specific nouns) appear last.\")\n",
    "print(\"   The model plans structure before content â€” like a painter!\")"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Gallery\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/15_gallery.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_15_gallery"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ðŸŽ¯ Final Output: Story Generation Gallery"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def story_gallery(model, tokenizer, n_stories=8, n_steps=20):\n",
    "    \"\"\"Generate a gallery of stories with different temperatures.\"\"\"\n",
    "    temps = [0.5, 0.7, 0.9, 1.1]\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"STORY GALLERY â€” Diffusion LLM on TinyStories\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    for temp in temps:\n",
    "        print(f\"\\n{'â”€' * 70}\")\n",
    "        print(f\"Temperature = {temp}\")\n",
    "        print(f\"{'â”€' * 70}\")\n",
    "        for i in range(n_stories // len(temps)):\n",
    "            story, _ = generate_story(model, tokenizer,\n",
    "                                       n_steps=n_steps, temperature=temp)\n",
    "            print(f\"  {story}\")\n",
    "        print()\n",
    "\n",
    "story_gallery(model, tokenizer)"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Final Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/16_final_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_16_final_viz"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š One final beautiful visualization: generation grid\n",
    "fig, axes = plt.subplots(4, 1, figsize=(18, 8))\n",
    "step_snapshots = [1, 5, 10, 20]\n",
    "\n",
    "# Generate one story and capture at specific steps\n",
    "model.eval()\n",
    "x = torch.full((1, SEQ_LEN), MASK_TOKEN_ID, dtype=torch.long, device=device)\n",
    "snapshots = {}\n",
    "n_total_steps = 20\n",
    "\n",
    "for s in range(n_total_steps, 0, -1):\n",
    "    t = torch.tensor([[s / n_total_steps]], device=device, dtype=torch.float)\n",
    "    logits = model(x, t)\n",
    "    probs = F.softmax(logits / 0.8, dim=-1)\n",
    "    sampled = torch.multinomial(probs.view(-1, VOCAB_SIZE), 1).view(1, -1)\n",
    "    confidence = probs.gather(-1, sampled.unsqueeze(-1)).squeeze(-1)\n",
    "    is_masked = (x == MASK_TOKEN_ID)\n",
    "    n_unmask = max(1, int(is_masked.sum().item() / s))\n",
    "    conf = confidence.clone()\n",
    "    conf[~is_masked] = -float('inf')\n",
    "    k = min(n_unmask, max(1, is_masked.sum().item()))\n",
    "    _, top_idx = conf.topk(k, dim=-1)\n",
    "    x.scatter_(1, top_idx, sampled.gather(1, top_idx))\n",
    "\n",
    "    step_num = n_total_steps - s + 1\n",
    "    if step_num in step_snapshots:\n",
    "        snapshots[step_num] = x[0].cpu().clone()\n",
    "\n",
    "for ax_idx, step_num in enumerate(step_snapshots):\n",
    "    ax = axes[ax_idx]\n",
    "    seq = snapshots[step_num]\n",
    "    text = tokenizer.decode(seq.tolist())\n",
    "    words = text.split()\n",
    "\n",
    "    x_pos = 0\n",
    "    for word in words[:20]:  # Show first 20 words\n",
    "        if word == \"[M]\":\n",
    "            color = '#bdbdbd'\n",
    "            ax.text(x_pos, 0.5, word, fontsize=11, fontfamily='monospace',\n",
    "                    color='#888888', va='center',\n",
    "                    bbox=dict(boxstyle='round', facecolor=color, alpha=0.5,\n",
    "                              edgecolor='none'))\n",
    "        else:\n",
    "            hue = (hash(word) % 8) / 8\n",
    "            color = plt.cm.Set3(hue)\n",
    "            ax.text(x_pos, 0.5, word, fontsize=11, fontfamily='monospace',\n",
    "                    color='#222222', va='center', fontweight='bold',\n",
    "                    bbox=dict(boxstyle='round', facecolor=color, alpha=0.7,\n",
    "                              edgecolor='none'))\n",
    "        x_pos += len(word) + 1.2\n",
    "\n",
    "    ax.set_xlim(-1, 65)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_ylabel(f'Step {step_num}', fontsize=12, fontweight='bold',\n",
    "                  rotation=0, ha='right', va='center')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('A Story Materializing Through Diffusion\\n'\n",
    "             '(Gray = [MASK], Colored = revealed words)',\n",
    "             fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸŽ‰ Congratulations! You have built a diffusion language model from scratch\")\n",
    "print(\"   that generates coherent short stories through iterative unmasking!\")\n",
    "print(\"   The model learned English structure, grammar, and basic narrative flow\")\n",
    "print(\"   from the TinyStories dataset â€” all in under 10 minutes of training.\")"
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/17_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_17_closing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### ðŸ¤” Reflection Questions\n",
    "\n",
    "1. **Quality observations:** What kinds of grammatical patterns did the model learn well? Where does it struggle? (Compare to what a GPT-style model of the same size would produce.)\n",
    "\n",
    "2. **Temperature effects:** How did temperature affect the stories? At $T = 0.5$, stories are repetitive but grammatical. At $T = 1.1$, stories are more creative but sometimes nonsensical. Why?\n",
    "\n",
    "3. **Scaling intuition:** LLaDA used 8B parameters and 2.3T tokens. We used ~4M parameters and ~30K stories. What do you think would happen if we 10x our data? 10x our model?\n",
    "\n",
    "### ðŸ† Optional Challenges\n",
    "\n",
    "1. **Prompted generation:** Modify `generate_story` to accept a text prompt. Encode the prompt, set those positions as fixed (never mask them), and let the model fill in the rest.\n",
    "\n",
    "2. **Longer stories:** Increase `SEQ_LEN` to 128. You may need to reduce `batch_size` to fit in GPU memory. Do longer stories maintain coherence?\n",
    "\n",
    "3. **Character-level model:** Replace the word tokenizer with a character-level tokenizer. How does this change what the model learns? (Hint: it should learn to spell words correctly through diffusion.)\n",
    "\n",
    "4. **Infilling:** Given a story with a gap in the middle, let the model fill it in. This is something autoregressive models cannot do natively!"
   ],
   "id": "cell_40"
  }
 ]
}