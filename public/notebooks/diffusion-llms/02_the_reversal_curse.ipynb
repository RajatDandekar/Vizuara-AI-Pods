{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "The Reversal Curse ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1ventuhdj998YNr_9KusKPNX2VFJg7Av1\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Motivation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_motivation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_motivation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Reversal Curse ‚Äî Why Direction Matters in Language Models\n",
    "\n",
    "*Part 2 of the Vizuara series on Diffusion Language Models*\n",
    "*Estimated time: 40 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Here is a surprising fact about GPT-4, Claude, and every autoregressive language model you have used:\n",
    "\n",
    "If you train a model on *\"The capital of France is Paris\"*, it can complete *\"The capital of France is ___\"* perfectly. But ask it *\"Paris is the capital of ___\"* and it struggles ‚Äî even though the answer is trivially implied by the training data.\n",
    "\n",
    "This is called the **reversal curse**, and it is a fundamental limitation of left-to-right models. They can only model $P(\\text{later tokens} \\mid \\text{earlier tokens})$, never the reverse.\n",
    "\n",
    "Diffusion language models break free from this curse. Because they see all tokens simultaneously with bidirectional attention, they can fill in *any* position given *any* context ‚Äî forward, backward, or in the middle.\n",
    "\n",
    "In this notebook, we will **prove this experimentally** by training both models on the same dataset and comparing their forward vs. reverse completion accuracy.\n",
    "\n",
    "**Teaser ‚Äî what we will see:**\n",
    "\n",
    "| | Forward (A ‚Üí B) | Reverse (B ‚Üí A) |\n",
    "|---|---|---|\n",
    "| Autoregressive | ~95% | ~12% |\n",
    "| Diffusion | ~93% | ~88% |"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_intuition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### The One-Way Street Problem\n",
    "\n",
    "Think of an autoregressive model as someone who reads a book **only from left to right, one word at a time**. They have seen \"The capital of France is Paris\" many times. When you start them off with \"The capital of France is ___\", they smoothly continue with \"Paris.\"\n",
    "\n",
    "But when you say \"Paris is the capital of ___\", they are stuck. They have never practiced reading from right to left. Even though the knowledge is in their training data, the *direction* of their processing prevents them from accessing it.\n",
    "\n",
    "A diffusion model is different. During training, tokens are masked **uniformly at random** ‚Äî sometimes the left side is masked, sometimes the right, sometimes the middle, sometimes everything. The model learns to predict any token from any combination of context. There is no preferred direction.\n",
    "\n",
    "### ü§î Think About This\n",
    "\n",
    "Before we run the experiment, predict:\n",
    "- Will the autoregressive model have *any* ability to do reverse completion, or will it be at 0%?\n",
    "- Will the diffusion model perform *exactly* the same on forward and reverse, or will there be some asymmetry?\n",
    "\n",
    "*Think about this for a moment. The answers may surprise you.*"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_math"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Why Does the Reversal Curse Happen?\n",
    "\n",
    "An autoregressive model factorizes the joint probability as:\n",
    "\n",
    "$$P(x_1, x_2, \\ldots, x_L) = \\prod_{i=1}^{L} P(x_i \\mid x_1, \\ldots, x_{i-1})$$\n",
    "\n",
    "Each factor conditions *only on the left context*. The model never computes $P(x_i \\mid x_{i+1}, \\ldots, x_L)$ during training.\n",
    "\n",
    "**Numerical example:** For the pair \"cat ‚Üí dog\", the autoregressive model learns:\n",
    "- $P(\\text{dog} \\mid \\text{cat}) = 0.95$ (high ‚Äî seen in training)\n",
    "- $P(\\text{cat} \\mid \\text{dog}) = ?$ (never trained on this direction!)\n",
    "\n",
    "The diffusion model's training objective sees all masking patterns:\n",
    "- When $t = 0.5$: might mask \"cat\", must predict it from \"dog\" (reverse!)\n",
    "- When $t = 0.5$: might mask \"dog\", must predict it from \"cat\" (forward!)\n",
    "- Both directions are trained equally because masking is uniform random."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Data Models\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_data_models.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_data_models"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It ‚Äî Component by Component\n",
    "\n",
    "### 4.1 The Paired-Sequence Dataset\n",
    "\n",
    "We create a dataset of paired tokens: given token A, the \"answer\" is token B. Each training sequence is `[A, SEP, B]`. This simulates factual pairs like \"France ‚Üí Paris\" or \"cat ‚Üí dog\"."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "VOCAB_SIZE = 32\n",
    "SEQ_LEN = 6          # [A1, A2, SEP, B1, B2, PAD] ‚Äî simple pairs\n",
    "MASK_TOKEN = 0\n",
    "SEP_TOKEN = 1         # Separator between A and B\n",
    "PAD_TOKEN = 2\n",
    "D_MODEL = 64\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 3\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Create fixed A‚ÜíB pairs (like \"France ‚Üí Paris\")\n",
    "NUM_PAIRS = 50\n",
    "np.random.seed(42)\n",
    "A_tokens = np.random.randint(3, VOCAB_SIZE, size=(NUM_PAIRS, 2))\n",
    "B_tokens = np.random.randint(3, VOCAB_SIZE, size=(NUM_PAIRS, 2))\n",
    "\n",
    "# Ensure no overlap between A and B for clean evaluation\n",
    "for i in range(NUM_PAIRS):\n",
    "    while np.any(A_tokens[i] == B_tokens[i]):\n",
    "        B_tokens[i] = np.random.randint(3, VOCAB_SIZE, size=2)\n",
    "\n",
    "print(f\"Created {NUM_PAIRS} A‚ÜíB pairs\")\n",
    "print(f\"Example pairs:\")\n",
    "for i in range(5):\n",
    "    print(f\"  [{A_tokens[i][0]}, {A_tokens[i][1]}] ‚Üí [{B_tokens[i][0]}, {B_tokens[i][1]}]\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_forward_batch(batch_size):\n",
    "    \"\"\"Create training sequences in forward order: [A1, A2, SEP, B1, B2, PAD]\"\"\"\n",
    "    indices = np.random.randint(0, NUM_PAIRS, size=batch_size)\n",
    "    seqs = []\n",
    "    for idx in indices:\n",
    "        seq = [A_tokens[idx][0], A_tokens[idx][1], SEP_TOKEN,\n",
    "               B_tokens[idx][0], B_tokens[idx][1], PAD_TOKEN]\n",
    "        seqs.append(seq)\n",
    "    return torch.tensor(seqs, dtype=torch.long, device=device)\n",
    "\n",
    "# Peek at the data\n",
    "batch = make_forward_batch(3)\n",
    "for i in range(3):\n",
    "    print(f\"Training sequence: {batch[i].tolist()}\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Autoregressive Model\n",
    "\n",
    "A standard causal Transformer ‚Äî each position can only attend to itself and earlier positions."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveLM(nn.Module):\n",
    "    \"\"\"Causal (left-to-right) language model.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(SEQ_LEN, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1, batch_first=True, norm_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        # Causal mask: prevent attending to future positions\n",
    "        mask = torch.triu(torch.ones(SEQ_LEN, SEQ_LEN), diagonal=1).bool()\n",
    "        self.register_buffer('causal_mask', mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(x.size(1), device=x.device)\n",
    "        h = self.embed(x) + self.pos_embed(positions)\n",
    "        h = self.transformer(h, mask=self.causal_mask)\n",
    "        return self.head(h)\n",
    "\n",
    "\n",
    "ar_model = AutoregressiveLM(VOCAB_SIZE, D_MODEL, N_HEADS, N_LAYERS).to(device)\n",
    "print(f\"AR model parameters: {sum(p.numel() for p in ar_model.parameters()):,}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The Diffusion Model\n",
    "\n",
    "Same architecture but **bidirectional** ‚Äî no causal mask, plus time conditioning."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionLM(nn.Module):\n",
    "    \"\"\"Bidirectional Transformer for masked diffusion.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(SEQ_LEN, d_model)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, d_model), nn.SiLU(), nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1, batch_first=True, norm_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x_t, t):\n",
    "        positions = torch.arange(x_t.size(1), device=x_t.device)\n",
    "        h = self.embed(x_t) + self.pos_embed(positions)\n",
    "        h = h + self.time_mlp(t).unsqueeze(1)\n",
    "        h = self.transformer(h)  # NO causal mask ‚Äî bidirectional!\n",
    "        return self.head(h)\n",
    "\n",
    "\n",
    "diff_model = DiffusionLM(VOCAB_SIZE, D_MODEL, N_HEADS, N_LAYERS).to(device)\n",
    "print(f\"Diffusion model parameters: {sum(p.numel() for p in diff_model.parameters()):,}\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Training Both Models"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ar_model(model, n_steps=3000, lr=3e-4):\n",
    "    \"\"\"Train the autoregressive model with next-token prediction.\"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    losses = []\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        x = make_forward_batch(BATCH_SIZE)\n",
    "\n",
    "        # Predict next token: input is x[:, :-1], target is x[:, 1:]\n",
    "        logits = model(x[:, :-1])\n",
    "        targets = x[:, 1:]\n",
    "        loss = F.cross_entropy(logits.reshape(-1, VOCAB_SIZE), targets.reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if (step + 1) % 500 == 0:\n",
    "            print(f\"  AR Step {step+1}/{n_steps} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "def mask_tokens(x_0, t):\n",
    "    \"\"\"Forward masking process.\"\"\"\n",
    "    mask = torch.rand_like(x_0.float()) < t\n",
    "    x_t = x_0.clone()\n",
    "    x_t[mask] = MASK_TOKEN\n",
    "    return x_t, mask\n",
    "\n",
    "\n",
    "def train_diff_model(model, n_steps=3000, lr=3e-4):\n",
    "    \"\"\"Train the diffusion model with masked prediction.\"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    losses = []\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        x_0 = make_forward_batch(BATCH_SIZE)\n",
    "        t = torch.rand(BATCH_SIZE, 1, device=device) * 0.98 + 0.02\n",
    "        x_t, mask = mask_tokens(x_0, t)\n",
    "        logits = model(x_t, t)\n",
    "\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        loss = F.cross_entropy(logits[mask], x_0[mask])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if (step + 1) % 500 == 0:\n",
    "            print(f\"  Diff Step {step+1}/{n_steps} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "print(\"Training Autoregressive Model...\")\n",
    "ar_losses = train_ar_model(ar_model, n_steps=3000)\n",
    "print(\"\\nTraining Diffusion Model...\")\n",
    "diff_losses = train_diff_model(diff_model, n_steps=3000)\n",
    "print(\"\\nDone!\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Training curves side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "window = 50\n",
    "\n",
    "smoothed_ar = np.convolve(ar_losses, np.ones(window)/window, mode='valid')\n",
    "ax1.plot(smoothed_ar, color='#c62828', linewidth=2)\n",
    "ax1.set_title('Autoregressive Model', fontsize=13)\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "smoothed_diff = np.convolve(diff_losses, np.ones(window)/window, mode='valid')\n",
    "ax2.plot(smoothed_diff, color='#1565c0', linewidth=2)\n",
    "ax2.set_title('Diffusion Model', fontsize=13)\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Training Curves', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_todo.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_todo"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîß Your Turn: Implement Forward and Reverse Evaluation\n",
    "\n",
    "Now the key experiment. We test both models on:\n",
    "- **Forward task:** Given A tokens, predict B tokens (the direction seen during training)\n",
    "- **Reverse task:** Given B tokens, predict A tokens (the direction *never* seen by the AR model!)\n",
    "\n",
    "### TODO: Implement the diffusion model evaluation\n",
    "\n",
    "The autoregressive evaluation is provided. Complete the diffusion model evaluation."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_ar_forward(model, n_tests=NUM_PAIRS):\n",
    "    \"\"\"AR model: given [A1, A2, SEP], predict [B1, B2].\"\"\"\n",
    "    correct = 0\n",
    "    for i in range(n_tests):\n",
    "        prompt = torch.tensor(\n",
    "            [[A_tokens[i][0], A_tokens[i][1], SEP_TOKEN, PAD_TOKEN, PAD_TOKEN]],\n",
    "            dtype=torch.long, device=device\n",
    "        )\n",
    "        # Generate autoregressively\n",
    "        for pos in range(3, 5):  # positions 3 and 4\n",
    "            logits = model(prompt[:, :pos])\n",
    "            next_token = logits[0, -1].argmax()\n",
    "            prompt[0, pos] = next_token\n",
    "        pred = prompt[0, 3:5].cpu().numpy()\n",
    "        target = B_tokens[i]\n",
    "        if np.array_equal(pred, target):\n",
    "            correct += 1\n",
    "    return correct / n_tests * 100\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_ar_reverse(model, n_tests=NUM_PAIRS):\n",
    "    \"\"\"AR model: given [B1, B2, SEP], predict [A1, A2]. (Reverse!)\"\"\"\n",
    "    correct = 0\n",
    "    for i in range(n_tests):\n",
    "        # Present in REVERSE order: B first, then expect A\n",
    "        prompt = torch.tensor(\n",
    "            [[B_tokens[i][0], B_tokens[i][1], SEP_TOKEN, PAD_TOKEN, PAD_TOKEN]],\n",
    "            dtype=torch.long, device=device\n",
    "        )\n",
    "        for pos in range(3, 5):\n",
    "            logits = model(prompt[:, :pos])\n",
    "            next_token = logits[0, -1].argmax()\n",
    "            prompt[0, pos] = next_token\n",
    "        pred = prompt[0, 3:5].cpu().numpy()\n",
    "        target = A_tokens[i]\n",
    "        if np.array_equal(pred, target):\n",
    "            correct += 1\n",
    "    return correct / n_tests * 100\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_diff_completion(model, source, target, n_steps=10):\n",
    "    \"\"\"Diffusion model: given source tokens, predict target tokens.\n",
    "\n",
    "    Setup: [source1, source2, SEP, MASK, MASK, PAD]\n",
    "    The model must fill in the MASKed positions.\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Build the input sequence with source visible and target masked\n",
    "    x = torch.tensor(\n",
    "        [[source[0], source[1], SEP_TOKEN, MASK_TOKEN, MASK_TOKEN, PAD_TOKEN]],\n",
    "        dtype=torch.long, device=device\n",
    "    )\n",
    "\n",
    "    # Step 2: Run iterative unmasking for n_steps\n",
    "    # At each step:\n",
    "    #   a) Create the time tensor t = s/n_steps\n",
    "    #   b) Get logits from model(x, t)\n",
    "    #   c) For the masked positions (indices 3, 4), pick the argmax prediction\n",
    "    #   d) Unmask them\n",
    "    # Since we only have 2 positions to fill, a single step is usually enough,\n",
    "    # but iterating helps with harder cases.\n",
    "\n",
    "    for s in range(n_steps, 0, -1):\n",
    "        t = ???  # YOUR CODE: shape (1, 1), value s/n_steps\n",
    "        logits = ???  # YOUR CODE: get model predictions\n",
    "        # For masked positions, take the argmax\n",
    "        for pos in [3, 4]:\n",
    "            if x[0, pos] == MASK_TOKEN:\n",
    "                x[0, pos] = ???  # YOUR CODE: argmax of logits at this position\n",
    "    # ==============================\n",
    "\n",
    "    pred = x[0, 3:5].cpu().numpy()\n",
    "    return np.array_equal(pred, target)"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification\n",
    "try:\n",
    "    test_result = eval_diff_completion(diff_model, A_tokens[0], B_tokens[0])\n",
    "    print(f\"‚úÖ Diffusion evaluation works! Test result: {test_result}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Post Todo\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_post_todo.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_post_todo"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_diff_completion(model, source, target, n_steps=10):\n",
    "    \"\"\"Diffusion model: given source tokens, predict target tokens.\"\"\"\n",
    "    x = torch.tensor(\n",
    "        [[source[0], source[1], SEP_TOKEN, MASK_TOKEN, MASK_TOKEN, PAD_TOKEN]],\n",
    "        dtype=torch.long, device=device\n",
    "    )\n",
    "\n",
    "    for s in range(n_steps, 0, -1):\n",
    "        t = torch.tensor([[s / n_steps]], device=device, dtype=torch.float)\n",
    "        logits = model(x, t)\n",
    "        for pos in [3, 4]:\n",
    "            if x[0, pos] == MASK_TOKEN:\n",
    "                x[0, pos] = logits[0, pos].argmax()\n",
    "\n",
    "    pred = x[0, 3:5].cpu().numpy()\n",
    "    return np.array_equal(pred, target)\n",
    "\n",
    "\n",
    "def eval_diff_forward(model, n_tests=NUM_PAIRS):\n",
    "    \"\"\"Diffusion: given A, predict B.\"\"\"\n",
    "    correct = sum(\n",
    "        eval_diff_completion(model, A_tokens[i], B_tokens[i])\n",
    "        for i in range(n_tests)\n",
    "    )\n",
    "    return correct / n_tests * 100\n",
    "\n",
    "\n",
    "def eval_diff_reverse(model, n_tests=NUM_PAIRS):\n",
    "    \"\"\"Diffusion: given B, predict A. (Reverse!)\"\"\"\n",
    "    correct = sum(\n",
    "        eval_diff_completion(model, B_tokens[i], A_tokens[i])\n",
    "        for i in range(n_tests)\n",
    "    )\n",
    "    return correct / n_tests * 100"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Experiment\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_experiment.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_experiment"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together ‚Äî The Reversal Curse Experiment"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating both models on forward and reverse tasks...\\n\")\n",
    "\n",
    "ar_fwd = eval_ar_forward(ar_model)\n",
    "ar_rev = eval_ar_reverse(ar_model)\n",
    "diff_fwd = eval_diff_forward(diff_model)\n",
    "diff_rev = eval_diff_reverse(diff_model)\n",
    "\n",
    "print(f\"{'Model':<20} {'Forward (A‚ÜíB)':<18} {'Reverse (B‚ÜíA)':<18}\")\n",
    "print(f\"{'-'*56}\")\n",
    "print(f\"{'Autoregressive':<20} {ar_fwd:>10.1f}%       {ar_rev:>10.1f}%\")\n",
    "print(f\"{'Diffusion':<20} {diff_fwd:>10.1f}%       {diff_rev:>10.1f}%\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä The Reversal Curse ‚Äî Visualized\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x_pos = np.array([0, 1.5])\n",
    "width = 0.5\n",
    "\n",
    "bars_fwd = ax.bar(x_pos - width/2, [ar_fwd, diff_fwd], width,\n",
    "                   label='Forward (A ‚Üí B)', color=['#ef9a9a', '#90caf9'],\n",
    "                   edgecolor=['#c62828', '#1565c0'], linewidth=2)\n",
    "bars_rev = ax.bar(x_pos + width/2, [ar_rev, diff_rev], width,\n",
    "                   label='Reverse (B ‚Üí A)', color=['#c62828', '#1565c0'],\n",
    "                   edgecolor=['#c62828', '#1565c0'], linewidth=2)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars_fwd:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1.5,\n",
    "            f'{bar.get_height():.0f}%', ha='center', fontsize=13, fontweight='bold')\n",
    "for bar in bars_rev:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1.5,\n",
    "            f'{bar.get_height():.0f}%', ha='center', fontsize=13, fontweight='bold')\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(['Autoregressive\\n(Causal)', 'Diffusion\\n(Bidirectional)'], fontsize=13)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=13)\n",
    "ax.set_ylim(0, 110)\n",
    "ax.set_title('The Reversal Curse: Forward vs Reverse Completion', fontsize=15)\n",
    "ax.legend(fontsize=12, loc='upper right')\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_why.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_why"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üéØ Why Does This Happen?"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize WHAT the models see during training\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Autoregressive attention pattern\n",
    "attn_ar = np.tril(np.ones((SEQ_LEN, SEQ_LEN)))\n",
    "im1 = ax1.imshow(attn_ar, cmap='Reds', aspect='equal')\n",
    "ax1.set_title('Autoregressive: Causal Attention', fontsize=13)\n",
    "ax1.set_xlabel('Key Position')\n",
    "ax1.set_ylabel('Query Position')\n",
    "labels = ['A1', 'A2', 'SEP', 'B1', 'B2', 'PAD']\n",
    "ax1.set_xticks(range(SEQ_LEN))\n",
    "ax1.set_yticks(range(SEQ_LEN))\n",
    "ax1.set_xticklabels(labels)\n",
    "ax1.set_yticklabels(labels)\n",
    "ax1.text(0.5, -0.2, 'B can see A ‚úì,  A cannot see B ‚úó',\n",
    "         transform=ax1.transAxes, ha='center', fontsize=11, color='#c62828')\n",
    "\n",
    "# Diffusion attention pattern (bidirectional)\n",
    "attn_diff = np.ones((SEQ_LEN, SEQ_LEN))\n",
    "im2 = ax2.imshow(attn_diff, cmap='Blues', aspect='equal')\n",
    "ax2.set_title('Diffusion: Bidirectional Attention', fontsize=13)\n",
    "ax2.set_xlabel('Key Position')\n",
    "ax2.set_ylabel('Query Position')\n",
    "ax2.set_xticks(range(SEQ_LEN))\n",
    "ax2.set_yticks(range(SEQ_LEN))\n",
    "ax2.set_xticklabels(labels)\n",
    "ax2.set_yticklabels(labels)\n",
    "ax2.text(0.5, -0.2, 'Every position sees every other position ‚úì',\n",
    "         transform=ax2.transAxes, ha='center', fontsize=11, color='#1565c0')\n",
    "\n",
    "plt.suptitle('Why the Reversal Curse Exists', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "print(\"   The autoregressive model can only condition B on A (left context).\")\n",
    "print(\"   The diffusion model conditions any token on all other tokens.\")\n",
    "print(\"   This is why diffusion models handle reverse completion naturally.\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_closing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reflection and Next Steps\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "\n",
    "1. **Partial reversal:** The autoregressive model might still get a few reverse completions right. Why? (Hint: think about statistical correlations in the vocabulary.)\n",
    "\n",
    "2. **Training cost:** Both models were trained for the same number of steps. Do you think the diffusion model needs more or fewer steps to learn bidirectional associations? Why?\n",
    "\n",
    "3. **Real-world implications:** The LLaDA paper tested this on Chinese poetry couplets. LLaDA scored 42% on the reversal task vs. GPT-4o's 32%. Why might the gap be smaller in large models?\n",
    "\n",
    "### üèÜ Optional Challenges\n",
    "\n",
    "1. **Asymmetric pairs:** Create pairs where A‚ÜíB is easy but B‚ÜíA is ambiguous (e.g., multiple A values map to the same B). How does each model handle this?\n",
    "\n",
    "2. **Longer sequences:** Increase the sequence length. At what point does the diffusion model's advantage over the AR model grow or shrink?\n",
    "\n",
    "3. **Middle completion:** Instead of reverse, mask out the *middle* tokens and test both models. The AR model should also fail here ‚Äî can you show it?\n",
    "\n",
    "**Next notebook:** We will explore different **sampling strategies** ‚Äî varying the number of denoising steps, comparing remasking methods, and benchmarking speed."
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "chatbot"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üí¨ AI Teaching Assistant ‚Äî Click ‚ñ∂ to start\n",
    "#@markdown This AI chatbot reads your notebook and can answer questions about any concept, code, or exercise.\n",
    "\n",
    "import json as _json\n",
    "import requests as _requests\n",
    "from google.colab import output as _output\n",
    "from IPython.display import display, HTML as _HTML, Markdown as _Markdown\n",
    "\n",
    "# --- Read notebook content for context ---\n",
    "def _get_notebook_context():\n",
    "    try:\n",
    "        from google.colab import _message\n",
    "        nb = _message.blocking_request(\"get_ipynb\", request=\"\", timeout_sec=10)\n",
    "        cells = nb.get(\"ipynb\", {}).get(\"cells\", [])\n",
    "        parts = []\n",
    "        for cell in cells:\n",
    "            src = \"\".join(cell.get(\"source\", []))\n",
    "            tags = cell.get(\"metadata\", {}).get(\"tags\", [])\n",
    "            if \"chatbot\" in tags:\n",
    "                continue\n",
    "            if src.strip():\n",
    "                ct = cell.get(\"cell_type\", \"unknown\")\n",
    "                parts.append(f\"[{ct.upper()}]\\n{src}\")\n",
    "        return \"\\n\\n---\\n\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"Notebook content unavailable.\"\n",
    "\n",
    "_NOTEBOOK_CONTEXT = _get_notebook_context()\n",
    "_CHAT_HISTORY = []\n",
    "_API_URL = \"https://course-creator-brown.vercel.app/api/chat\"\n",
    "\n",
    "def _notebook_chat(question):\n",
    "    global _CHAT_HISTORY\n",
    "    try:\n",
    "        resp = _requests.post(_API_URL, json={\n",
    "            'question': question,\n",
    "            'context': _NOTEBOOK_CONTEXT[:100000],\n",
    "            'history': _CHAT_HISTORY[-10:],\n",
    "        }, timeout=60)\n",
    "        data = resp.json()\n",
    "        answer = data.get('answer', 'Sorry, I could not generate a response.')\n",
    "        _CHAT_HISTORY.append({'role': 'user', 'content': question})\n",
    "        _CHAT_HISTORY.append({'role': 'assistant', 'content': answer})\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f'Error connecting to teaching assistant: {str(e)}'\n",
    "\n",
    "_output.register_callback('notebook_chat', _notebook_chat)\n",
    "\n",
    "def ask(question):\n",
    "    \"\"\"Ask the AI teaching assistant a question about this notebook.\"\"\"\n",
    "    answer = _notebook_chat(question)\n",
    "    display(_Markdown(answer))\n",
    "\n",
    "print(\"\\u2705 AI Teaching Assistant is ready!\")\n",
    "print(\"\\U0001f4a1 Use the chat below, or call ask(\\'your question\\') in any cell.\")\n",
    "\n",
    "# --- Display chat widget ---\n",
    "display(_HTML('''<style>\n  .vc-wrap{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;max-width:100%;border-radius:16px;overflow:hidden;box-shadow:0 4px 24px rgba(0,0,0,.12);background:#fff;border:1px solid #e5e7eb}\n  .vc-hdr{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;padding:16px 20px;display:flex;align-items:center;gap:12px}\n  .vc-avatar{width:42px;height:42px;background:rgba(255,255,255,.2);border-radius:50%;display:flex;align-items:center;justify-content:center;font-size:22px}\n  .vc-hdr h3{font-size:16px;font-weight:600;margin:0}\n  .vc-hdr p{font-size:12px;opacity:.85;margin:2px 0 0}\n  .vc-msgs{height:420px;overflow-y:auto;padding:16px;background:#f8f9fb;display:flex;flex-direction:column;gap:10px}\n  .vc-msg{display:flex;flex-direction:column;animation:vc-fade .25s ease}\n  .vc-msg.user{align-items:flex-end}\n  .vc-msg.bot{align-items:flex-start}\n  .vc-bbl{max-width:85%;padding:10px 14px;border-radius:16px;font-size:14px;line-height:1.55;word-wrap:break-word}\n  .vc-msg.user .vc-bbl{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border-bottom-right-radius:4px}\n  .vc-msg.bot .vc-bbl{background:#fff;color:#1a1a2e;border:1px solid #e8e8e8;border-bottom-left-radius:4px}\n  .vc-bbl code{background:rgba(0,0,0,.07);padding:2px 6px;border-radius:4px;font-size:13px;font-family:'Fira Code',monospace}\n  .vc-bbl pre{background:#1e1e2e;color:#cdd6f4;padding:12px;border-radius:8px;overflow-x:auto;margin:8px 0;font-size:13px}\n  .vc-bbl pre code{background:none;padding:0;color:inherit}\n  .vc-bbl h3,.vc-bbl h4{margin:10px 0 4px;font-size:15px}\n  .vc-bbl ul,.vc-bbl ol{margin:4px 0;padding-left:20px}\n  .vc-bbl li{margin:2px 0}\n  .vc-chips{display:flex;flex-wrap:wrap;gap:8px;padding:0 16px 12px;background:#f8f9fb}\n  .vc-chip{background:#fff;border:1px solid #d1d5db;border-radius:20px;padding:6px 14px;font-size:12px;cursor:pointer;transition:all .15s;color:#4b5563}\n  .vc-chip:hover{border-color:#667eea;color:#667eea;background:#f0f0ff}\n  .vc-input{display:flex;padding:12px 16px;background:#fff;border-top:1px solid #eee;gap:8px}\n  .vc-input input{flex:1;padding:10px 16px;border:2px solid #e8e8e8;border-radius:24px;font-size:14px;outline:none;transition:border-color .2s}\n  .vc-input input:focus{border-color:#667eea}\n  .vc-input button{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border:none;border-radius:50%;width:42px;height:42px;cursor:pointer;display:flex;align-items:center;justify-content:center;font-size:18px;transition:transform .1s}\n  .vc-input button:hover{transform:scale(1.05)}\n  .vc-input button:disabled{opacity:.5;cursor:not-allowed;transform:none}\n  .vc-typing{display:flex;gap:5px;padding:4px 0}\n  .vc-typing span{width:8px;height:8px;background:#667eea;border-radius:50%;animation:vc-bounce 1.4s infinite ease-in-out}\n  .vc-typing span:nth-child(2){animation-delay:.2s}\n  .vc-typing span:nth-child(3){animation-delay:.4s}\n  @keyframes vc-bounce{0%,80%,100%{transform:scale(0)}40%{transform:scale(1)}}\n  @keyframes vc-fade{from{opacity:0;transform:translateY(8px)}to{opacity:1;transform:translateY(0)}}\n  .vc-note{text-align:center;font-size:11px;color:#9ca3af;padding:8px 16px 12px;background:#fff}\n</style>\n<div class=\"vc-wrap\">\n  <div class=\"vc-hdr\">\n    <div class=\"vc-avatar\">&#129302;</div>\n    <div>\n      <h3>Vizuara Teaching Assistant</h3>\n      <p>Ask me anything about this notebook</p>\n    </div>\n  </div>\n  <div class=\"vc-msgs\" id=\"vcMsgs\">\n    <div class=\"vc-msg bot\">\n      <div class=\"vc-bbl\">&#128075; Hi! I've read through this entire notebook. Ask me about any concept, code block, or exercise &mdash; I'm here to help you learn!</div>\n    </div>\n  </div>\n  <div class=\"vc-chips\" id=\"vcChips\">\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Explain the main concept</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Help with the TODO exercise</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Summarize what I learned</span>\n  </div>\n  <div class=\"vc-input\">\n    <input type=\"text\" id=\"vcIn\" placeholder=\"Ask about concepts, code, exercises...\" />\n    <button id=\"vcSend\" onclick=\"vcSendMsg()\">&#10148;</button>\n  </div>\n  <div class=\"vc-note\">AI-generated &middot; Verify important information &middot; <a href=\"#\" onclick=\"vcClear();return false\" style=\"color:#667eea\">Clear chat</a></div>\n</div>\n<script>\n(function(){\n  var msgs=document.getElementById('vcMsgs'),inp=document.getElementById('vcIn'),\n      btn=document.getElementById('vcSend'),chips=document.getElementById('vcChips');\n\n  function esc(s){var d=document.createElement('div');d.textContent=s;return d.innerHTML}\n\n  function md(t){\n    return t\n      .replace(/```(\\w*)\\n([\\s\\S]*?)```/g,function(_,l,c){return '<pre><code>'+esc(c)+'</code></pre>'})\n      .replace(/`([^`]+)`/g,'<code>$1</code>')\n      .replace(/\\*\\*([^*]+)\\*\\*/g,'<strong>$1</strong>')\n      .replace(/\\*([^*]+)\\*/g,'<em>$1</em>')\n      .replace(/^#### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^## (.+)$/gm,'<h3>$1</h3>')\n      .replace(/^\\d+\\. (.+)$/gm,'<li>$1</li>')\n      .replace(/^- (.+)$/gm,'<li>$1</li>')\n      .replace(/\\n\\n/g,'<br><br>')\n      .replace(/\\n/g,'<br>');\n  }\n\n  function addMsg(text,isUser){\n    var m=document.createElement('div');m.className='vc-msg '+(isUser?'user':'bot');\n    var b=document.createElement('div');b.className='vc-bbl';\n    b.innerHTML=isUser?esc(text):md(text);\n    m.appendChild(b);msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function showTyping(){\n    var m=document.createElement('div');m.className='vc-msg bot';m.id='vcTyping';\n    m.innerHTML='<div class=\"vc-bbl\"><div class=\"vc-typing\"><span></span><span></span><span></span></div></div>';\n    msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function hideTyping(){var e=document.getElementById('vcTyping');if(e)e.remove()}\n\n  window.vcSendMsg=function(){\n    var q=inp.value.trim();if(!q)return;\n    inp.value='';chips.style.display='none';\n    addMsg(q,true);showTyping();btn.disabled=true;\n    google.colab.kernel.invokeFunction('notebook_chat',[q],{})\n      .then(function(r){\n        hideTyping();\n        var a=r.data['application/json'];\n        addMsg(typeof a==='string'?a:JSON.stringify(a),false);\n      })\n      .catch(function(){\n        hideTyping();\n        addMsg('Sorry, I encountered an error. Please check your internet connection and try again.',false);\n      })\n      .finally(function(){btn.disabled=false;inp.focus()});\n  };\n\n  window.vcAsk=function(q){inp.value=q;vcSendMsg()};\n  window.vcClear=function(){\n    msgs.innerHTML='<div class=\"vc-msg bot\"><div class=\"vc-bbl\">&#128075; Chat cleared. Ask me anything!</div></div>';\n    chips.style.display='flex';\n  };\n\n  inp.addEventListener('keypress',function(e){if(e.key==='Enter')vcSendMsg()});\n  inp.focus();\n})();\n</script>'''))"
   ],
   "id": "vizuara_chatbot"
  }
 ]
}