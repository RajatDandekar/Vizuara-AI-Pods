{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "From Q-Tables to Neural Networks -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Q-Tables to Neural Networks: The Path to Deep Q-Learning\n",
    "\n",
    "*Part 4 of the Vizuara series on Value Functions and Q-Learning*\n",
    "*Estimated time: 50 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebooks, we built Q-Learning from scratch and trained an agent to solve FrozenLake -- a tiny 4x4 grid with just 16 states.\n",
    "\n",
    "But real-world problems are far larger. Chess has roughly $10^{46}$ states. Atari games have pixel-based observations with billions of possible frames. A self-driving car sees a continuous stream of camera images -- effectively an infinite state space.\n",
    "\n",
    "Can we store a Q-value for every single state-action pair? Absolutely not. We need to **generalize** -- learn a function that approximates Q-values for states we have never seen before.\n",
    "\n",
    "This is exactly what Deep Q-Networks (DQN) do: replace the Q-table with a neural network that takes a state as input and outputs Q-values for all actions. This is the breakthrough that allowed DeepMind to play Atari games at superhuman levels in 2013.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand why tables fail for large state spaces\n",
    "- Implement a neural network Q-function from scratch in PyTorch\n",
    "- Train a DQN agent to solve CartPole\n",
    "- See how experience replay and target networks stabilize training"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Think of it this way. A Q-table is like a phone book -- one entry for every person. If there are 16 people, the phone book is tiny. If there are 10 billion people, you need a very big book.\n",
    "\n",
    "A neural network is like a person who has seen enough phone numbers to guess the pattern: \"Oh, people in this area code tend to have numbers starting with 555.\" They do not memorize every number -- they learn the structure.\n",
    "\n",
    "Similarly, a DQN does not memorize Q-values for every state. It learns the structure: \"States that look like THIS tend to have high Q-values for action RIGHT.\" When it sees a new state it has never encountered, it can still estimate the Q-values by recognizing the pattern.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "If two game states look almost identical (say, the paddle in Pong is shifted by one pixel), should their Q-values be very different? If not, is it wasteful to store them separately? This is the core motivation for function approximation."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### The DQN Loss Function\n",
    "\n",
    "Instead of a table update, we train a neural network $Q_\\theta(s, a)$ to minimize:\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = \\mathbb{E}\\left[\\left(r + \\gamma \\max_{a'} Q_{\\theta^-}(s', a') - Q_\\theta(s, a)\\right)^2\\right]$$\n",
    "\n",
    "This is the mean squared TD error. The network tries to make its Q-value predictions match the TD targets.\n",
    "\n",
    "Key innovations of DQN:\n",
    "\n",
    "1. **Experience Replay**: Store transitions $(s, a, r, s', \\text{done})$ in a buffer. Sample random mini-batches for training. This breaks correlations between consecutive samples.\n",
    "\n",
    "2. **Target Network**: Use a separate, slowly-updated copy of the network ($Q_{\\theta^-}$) to compute TD targets. This prevents the moving target problem."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 First, Let's See the Table Limitation"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# The scaling problem\n",
    "state_spaces = {\n",
    "    \"FrozenLake (4x4)\": 16,\n",
    "    \"FrozenLake (8x8)\": 64,\n",
    "    \"Taxi-v3\": 500,\n",
    "    \"Blackjack\": 704,\n",
    "    \"Chess (approx)\": 1e46,\n",
    "    \"Go (approx)\": 1e170,\n",
    "    \"Atari (raw pixels)\": 256 ** (210 * 160 * 3),\n",
    "}\n",
    "\n",
    "print(\"State Space Sizes:\")\n",
    "print(\"-\" * 50)\n",
    "for name, size in state_spaces.items():\n",
    "    if size < 1e6:\n",
    "        print(f\"  {name:30s} {int(size):>15,} states\")\n",
    "    else:\n",
    "        print(f\"  {name:30s}  ~10^{np.log10(float(size)):.0f} states\")\n",
    "\n",
    "print()\n",
    "print(\"A Q-table with 10^46 entries would require more memory than\")\n",
    "print(\"all the atoms in the observable universe.\")\n",
    "print()\n",
    "print(\"Solution: Replace the table with a FUNCTION APPROXIMATOR.\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Q-Network"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"A simple feed-forward Q-Network.\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, n_actions, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Input: state tensor of shape (batch_size, state_dim)\n",
    "        Output: Q-values for each action, shape (batch_size, n_actions)\n",
    "        \"\"\"\n",
    "        return self.network(state)\n",
    "\n",
    "\n",
    "# Test the network\n",
    "state_dim = 4  # CartPole has 4 observations\n",
    "n_actions = 2  # CartPole has 2 actions (left, right)\n",
    "\n",
    "q_net = QNetwork(state_dim, n_actions)\n",
    "print(f\"Q-Network architecture:\")\n",
    "print(q_net)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in q_net.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_state = torch.randn(1, state_dim)\n",
    "q_values = q_net(dummy_state)\n",
    "print(f\"\\nInput state shape: {dummy_state.shape}\")\n",
    "print(f\"Output Q-values: {q_values.detach().numpy()}\")\n",
    "print(f\"Best action: {q_values.argmax().item()}\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Experience Replay Buffer"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store a transition.\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a random batch of transitions.\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)),\n",
    "            torch.LongTensor(actions),\n",
    "            torch.FloatTensor(rewards),\n",
    "            torch.FloatTensor(np.array(next_states)),\n",
    "            torch.FloatTensor(dones),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# Test\n",
    "buffer = ReplayBuffer(capacity=1000)\n",
    "\n",
    "# Add some dummy experiences\n",
    "for i in range(100):\n",
    "    s = np.random.randn(4)\n",
    "    a = np.random.randint(2)\n",
    "    r = np.random.randn()\n",
    "    s_next = np.random.randn(4)\n",
    "    done = np.random.random() < 0.1\n",
    "    buffer.push(s, a, r, s_next, done)\n",
    "\n",
    "print(f\"Buffer size: {len(buffer)}\")\n",
    "states, actions, rewards, next_states, dones = buffer.sample(32)\n",
    "print(f\"Batch shapes: states={states.shape}, actions={actions.shape}, rewards={rewards.shape}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The DQN Agent"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"Deep Q-Network agent with experience replay and target network.\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, n_actions, hidden_dim=128,\n",
    "                 lr=1e-3, gamma=0.99, epsilon=1.0, epsilon_decay=0.995,\n",
    "                 epsilon_min=0.01, buffer_size=10000, batch_size=64,\n",
    "                 target_update_freq=100):\n",
    "        self.state_dim = state_dim\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "\n",
    "        # Q-Network and Target Network\n",
    "        self.q_net = QNetwork(state_dim, n_actions, hidden_dim)\n",
    "        self.target_net = QNetwork(state_dim, n_actions, hidden_dim)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "        self.buffer = ReplayBuffer(buffer_size)\n",
    "        self.steps = 0\n",
    "        self.losses = []\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = self.q_net(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition in replay buffer.\"\"\"\n",
    "        self.buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def train_step(self):\n",
    "        \"\"\"Perform one training step (sample batch, compute loss, update).\"\"\"\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "\n",
    "        # Current Q-values: Q(s, a) for the actions we actually took\n",
    "        current_q = self.q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Target Q-values using the target network\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_net(next_states).max(1)[0]\n",
    "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
    "\n",
    "        # Loss: MSE between current and target Q-values\n",
    "        loss = nn.MSELoss()(current_q, target_q)\n",
    "\n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.losses.append(loss.item())\n",
    "        self.steps += 1\n",
    "\n",
    "        # Update target network periodically\n",
    "        if self.steps % self.target_update_freq == 0:\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "print(\"DQN Agent components:\")\n",
    "print(\"  1. Q-Network: predicts Q(s,a) for all actions\")\n",
    "print(\"  2. Target Network: stable target for TD computation\")\n",
    "print(\"  3. Replay Buffer: breaks correlation in training data\")\n",
    "print(\"  4. Epsilon-greedy: balances exploration and exploitation\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO: Implement the Training Loop for CartPole"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "def train_dqn(env_name=\"CartPole-v1\", n_episodes=500, render_every=None):\n",
    "    \"\"\"\n",
    "    Train a DQN agent on CartPole.\n",
    "\n",
    "    CartPole: Balance a pole on a cart by moving left or right.\n",
    "    State: [cart_position, cart_velocity, pole_angle, pole_angular_velocity]\n",
    "    Actions: 0 = push left, 1 = push right\n",
    "    Reward: +1 for each step the pole stays upright\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(\n",
    "        state_dim=state_dim,\n",
    "        n_actions=n_actions,\n",
    "        hidden_dim=128,\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.995,\n",
    "        epsilon_min=0.01,\n",
    "        buffer_size=10000,\n",
    "        batch_size=64,\n",
    "        target_update_freq=100,\n",
    "    )\n",
    "\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        # ============ TODO ============\n",
    "        # Implement the episode loop:\n",
    "        #\n",
    "        # While not done:\n",
    "        #   1. Choose action using agent.choose_action(state)\n",
    "        #   2. Take action: next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        #   3. Compute done = terminated or truncated\n",
    "        #   4. Store transition: agent.store(state, action, reward, next_state, float(terminated))\n",
    "        #   5. Perform training step: agent.train_step()\n",
    "        #   6. Update state and total_reward\n",
    "        #\n",
    "        # After episode:\n",
    "        #   7. Decay epsilon: agent.decay_epsilon()\n",
    "        #   8. Append total_reward to rewards_per_episode\n",
    "        # ==============================\n",
    "\n",
    "        pass  # YOUR CODE HERE\n",
    "\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            recent = np.mean(rewards_per_episode[-50:]) if rewards_per_episode else 0\n",
    "            print(f\"Episode {episode+1:4d} | Reward: {recent:6.1f} | Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "    env.close()\n",
    "    return agent, rewards_per_episode"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "agent_dqn, dqn_rewards = train_dqn(n_episodes=500)\n",
    "\n",
    "# CartPole is \"solved\" when average reward > 195 over 100 episodes\n",
    "final_avg = np.mean(dqn_rewards[-100:])\n",
    "print(f\"\\nFinal average reward (last 100): {final_avg:.1f}\")\n",
    "if final_avg > 195:\n",
    "    print(\"CartPole SOLVED!\")\n",
    "else:\n",
    "    print(f\"Not quite solved yet. Try more episodes or tuning hyperparameters.\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tabular Q-learning (impossible for CartPole) vs DQN\n",
    "\n",
    "print(\"Why DQN works where tables fail:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"CartPole state space: CONTINUOUS (4 real numbers)\")\n",
    "print(f\"CartPole state examples:\")\n",
    "env_demo = gym.make(\"CartPole-v1\")\n",
    "for i in range(5):\n",
    "    s, _ = env_demo.reset()\n",
    "    print(f\"  State {i+1}: cart_pos={s[0]:.3f}, cart_vel={s[1]:.3f}, \"\n",
    "          f\"pole_angle={s[2]:.3f}, pole_angvel={s[3]:.3f}\")\n",
    "env_demo.close()\n",
    "\n",
    "print()\n",
    "print(\"Each state is unique -- a table would need infinite entries!\")\n",
    "print(\"The neural network generalizes: similar states get similar Q-values.\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dqn_rewards:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # Reward curve\n",
    "    window = 20\n",
    "    moving_avg = [np.mean(dqn_rewards[max(0,i-window):i+1]) for i in range(len(dqn_rewards))]\n",
    "\n",
    "    axes[0].plot(dqn_rewards, alpha=0.3, color='#2171b5')\n",
    "    axes[0].plot(moving_avg, color='#2171b5', linewidth=2)\n",
    "    axes[0].axhline(y=195, color='green', linestyle='--', label='Solved threshold')\n",
    "    axes[0].set_xlabel('Episode', fontsize=11)\n",
    "    axes[0].set_ylabel('Reward', fontsize=11)\n",
    "    axes[0].set_title('DQN on CartPole', fontsize=13, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Loss curve\n",
    "    if agent_dqn.losses:\n",
    "        loss_window = 100\n",
    "        loss_avg = [np.mean(agent_dqn.losses[max(0,i-loss_window):i+1])\n",
    "                    for i in range(len(agent_dqn.losses))]\n",
    "        axes[1].plot(loss_avg[::10], color='#d94701', linewidth=1)\n",
    "        axes[1].set_xlabel('Training Step (x10)', fontsize=11)\n",
    "        axes[1].set_ylabel('Loss', fontsize=11)\n",
    "        axes[1].set_title('Training Loss', fontsize=13, fontweight='bold')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Q-value landscape: test different pole angles\n",
    "    angles = np.linspace(-0.2, 0.2, 100)\n",
    "    q_left = []\n",
    "    q_right = []\n",
    "    for angle in angles:\n",
    "        state = torch.FloatTensor([[0.0, 0.0, angle, 0.0]])\n",
    "        with torch.no_grad():\n",
    "            q = agent_dqn.q_net(state)\n",
    "        q_left.append(q[0, 0].item())\n",
    "        q_right.append(q[0, 1].item())\n",
    "\n",
    "    axes[2].plot(np.degrees(angles), q_left, label='Push Left', color='#d94701')\n",
    "    axes[2].plot(np.degrees(angles), q_right, label='Push Right', color='#2171b5')\n",
    "    axes[2].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[2].set_xlabel('Pole Angle (degrees)', fontsize=11)\n",
    "    axes[2].set_ylabel('Q-value', fontsize=11)\n",
    "    axes[2].set_title('Learned Q-values vs Pole Angle', fontsize=13, fontweight='bold')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle('Deep Q-Network: Complete Analysis', fontsize=15, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training data available. Make sure the training loop TODO is completed.\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained agent\n",
    "if dqn_rewards:\n",
    "    env_test = gym.make(\"CartPole-v1\")\n",
    "    test_rewards = []\n",
    "\n",
    "    for _ in range(100):\n",
    "        state, _ = env_test.reset()\n",
    "        total = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                q = agent_dqn.q_net(torch.FloatTensor(state).unsqueeze(0))\n",
    "                action = q.argmax().item()\n",
    "            state, reward, terminated, truncated, _ = env_test.step(action)\n",
    "            done = terminated or truncated\n",
    "            total += reward\n",
    "        test_rewards.append(total)\n",
    "    env_test.close()\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"  FINAL RESULTS: DQN on CartPole-v1\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Test episodes:     100\")\n",
    "    print(f\"  Mean reward:       {np.mean(test_rewards):.1f}\")\n",
    "    print(f\"  Std reward:        {np.std(test_rewards):.1f}\")\n",
    "    print(f\"  Min reward:        {np.min(test_rewards):.0f}\")\n",
    "    print(f\"  Max reward:        {np.max(test_rewards):.0f}\")\n",
    "    print(f\"  Solved (>195):     {'YES' if np.mean(test_rewards) > 195 else 'NO'}\")\n",
    "    print()\n",
    "    print(\"  The neural network learned to balance the pole!\")\n",
    "    print(\"  From tabular Q-learning on 16 states to DQN on continuous spaces.\")\n",
    "    print(\"  This is the same idea that DeepMind used to play Atari in 2013.\")\n",
    "\n",
    "    # Histogram of test rewards\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(test_rewards, bins=20, color='#2171b5', edgecolor='white', alpha=0.8)\n",
    "    plt.axvline(x=195, color='green', linestyle='--', linewidth=2, label='Solved threshold')\n",
    "    plt.xlabel('Episode Reward', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.title('DQN Test Performance Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print()\n",
    "    print(\"Congratulations! You have built Deep Q-Learning from scratch!\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "1. Why does DQN need two networks (Q-network and target network)? What goes wrong with just one?\n",
    "2. Experience replay breaks the correlation between consecutive samples. Why is this important for neural network training?\n",
    "3. The DQN loss function looks like supervised learning (MSE loss). But the \"labels\" (TD targets) change during training. How is this different from standard supervised learning?\n",
    "\n",
    "### Optional Challenges\n",
    "1. Implement Double DQN: use the Q-network to select the best action but the target network to evaluate it. This reduces overestimation.\n",
    "2. Implement Prioritized Experience Replay: sample transitions with high TD error more frequently.\n",
    "3. Try DQN on LunarLander-v3. This is a harder environment -- how many episodes does it take to solve?"
   ],
   "id": "cell_22"
  }
 ]
}