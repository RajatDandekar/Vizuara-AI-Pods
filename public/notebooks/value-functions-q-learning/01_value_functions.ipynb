{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Value Functions -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Functions: Teaching Machines to Evaluate Positions\n",
    "\n",
    "*Part 1 of the Vizuara series on Value Functions and Q-Learning*\n",
    "*Estimated time: 35 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Every decision-making agent -- whether it is a chess engine, a robot navigating a warehouse, or a recommendation system -- needs to answer one fundamental question: **\"How good is my current situation?\"**\n",
    "\n",
    "Value functions are the mathematical tool that answers this question. They assign a number to every possible state, telling the agent how much total reward it can expect from that point onward. This simple idea turns out to be one of the most powerful concepts in all of reinforcement learning.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Implement state value functions and action value functions from scratch\n",
    "- Visualize value landscapes on grid worlds\n",
    "- Understand discounting and why it matters\n",
    "- See how Q-values guide action selection\n",
    "\n",
    "Let us build the intuition, then the code."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Imagine you are playing a board game on a grid. Your goal is to reach the treasure at the top-right corner.\n",
    "\n",
    "At any moment, you are standing on a particular cell. A natural question arises: **\"How good is it to be in this cell?\"**\n",
    "\n",
    "If you are right next to the treasure, the answer is obvious -- it is very good. If you are far away with obstacles in the way, less so. But here is the key insight: even cells that are far from the treasure can be \"good\" if they lie on a clear path towards it.\n",
    "\n",
    "This is exactly what a value function captures -- a number for every position that says \"this is how much reward you can expect from here.\"\n",
    "\n",
    "### Think About This\n",
    "\n",
    "If you are at a crossroads and one path leads to a reward of +10 in 2 steps, while another path leads to a reward of +100 in 50 steps, which path is \"better\"? Does it depend on how patient you are? This question motivates the concept of discounting, which we will explore next."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### State Value Function\n",
    "\n",
    "The state value function $V^{\\pi}(s)$ tells us the expected total future reward starting from state $s$ and following policy $\\pi$:\n",
    "\n",
    "$$V^{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t \\mid S_t = s]$$\n",
    "\n",
    "Computationally, this says: \"Start at state $s$. Follow policy $\\pi$ to choose actions. Add up all the rewards you collect. Average over many runs. That average is the value.\"\n",
    "\n",
    "### Discounted Return\n",
    "\n",
    "Future rewards are worth less than immediate rewards. We capture this using a discount factor $\\gamma \\in [0, 1]$:\n",
    "\n",
    "$$G_t = r_t + \\gamma \\cdot r_{t+1} + \\gamma^2 \\cdot r_{t+2} + \\gamma^3 \\cdot r_{t+3} + \\cdots$$\n",
    "\n",
    "When $\\gamma = 0.9$, a reward of 10 received 5 steps later is worth only $10 \\times 0.9^5 = 5.9$ today. The higher the gamma, the more the agent cares about the future.\n",
    "\n",
    "### Action Value Function (Q-Function)\n",
    "\n",
    "The action value function $Q^{\\pi}(s, a)$ tells us the expected return if we start in state $s$, take action $a$, and then follow policy $\\pi$:\n",
    "\n",
    "$$Q^{\\pi}(s, a) = \\mathbb{E}_{\\pi}[G_t \\mid S_t = s, A_t = a]$$\n",
    "\n",
    "The relationship between V and Q is:\n",
    "\n",
    "$$V^{\\pi}(s) = \\sum_{a} \\pi(a \\mid s) \\cdot Q^{\\pi}(s, a)$$\n",
    "\n",
    "This says: the value of a state is the average Q-value over all actions, weighted by how likely each action is under our policy."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 Define a Simple Grid World\n",
    "\n",
    "We will start by building a simple grid world environment from scratch. No libraries -- just numpy."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "class GridWorld:\n",
    "    \"\"\"A simple grid world environment.\"\"\"\n",
    "\n",
    "    def __init__(self, rows=5, cols=5, goal=(4, 4), obstacles=None, step_reward=-0.1, goal_reward=10.0):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.goal = goal\n",
    "        self.obstacles = obstacles or [(2, 1), (2, 3)]\n",
    "        self.step_reward = step_reward\n",
    "        self.goal_reward = goal_reward\n",
    "\n",
    "        # Actions: 0=up, 1=right, 2=down, 3=left\n",
    "        self.actions = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n",
    "        self.action_names = ['Up', 'Right', 'Down', 'Left']\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.n_states = rows * cols\n",
    "\n",
    "    def is_valid(self, r, c):\n",
    "        return (0 <= r < self.rows and 0 <= c < self.cols\n",
    "                and (r, c) not in self.obstacles)\n",
    "\n",
    "    def step(self, state, action):\n",
    "        \"\"\"Take an action, return (next_state, reward, done).\"\"\"\n",
    "        r, c = state\n",
    "        dr, dc = self.actions[action]\n",
    "        nr, nc = r + dr, c + dc\n",
    "\n",
    "        if not self.is_valid(nr, nc):\n",
    "            nr, nc = r, c  # Stay in place if hitting wall or obstacle\n",
    "\n",
    "        reward = self.step_reward\n",
    "        done = False\n",
    "        if (nr, nc) == self.goal:\n",
    "            reward = self.goal_reward\n",
    "            done = True\n",
    "\n",
    "        return (nr, nc), reward, done\n",
    "\n",
    "    def get_all_states(self):\n",
    "        \"\"\"Return all non-obstacle states.\"\"\"\n",
    "        states = []\n",
    "        for r in range(self.rows):\n",
    "            for c in range(self.cols):\n",
    "                if (r, c) not in self.obstacles:\n",
    "                    states.append((r, c))\n",
    "        return states\n",
    "\n",
    "\n",
    "env = GridWorld()\n",
    "print(f\"Grid: {env.rows}x{env.cols}\")\n",
    "print(f\"Goal: {env.goal}\")\n",
    "print(f\"Obstacles: {env.obstacles}\")\n",
    "print(f\"Total valid states: {len(env.get_all_states())}\")\n",
    "print(f\"Actions: {env.action_names}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Compute Discounted Returns by Hand\n",
    "\n",
    "Before using any algorithm, let us compute discounted returns manually to build intuition."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_discounted_return(rewards, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Compute the discounted return from a sequence of rewards.\n",
    "\n",
    "    G_t = r_t + gamma * r_{t+1} + gamma^2 * r_{t+2} + ...\n",
    "    \"\"\"\n",
    "    G = 0.0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        G = rewards[t] + gamma * G\n",
    "    return G\n",
    "\n",
    "\n",
    "# Example 1: Simple chain A -> B -> C\n",
    "rewards_abc = [1.0, 2.0]  # r from A->B = 1, r from B->C = 2\n",
    "gamma = 0.9\n",
    "\n",
    "G_from_A = compute_discounted_return(rewards_abc, gamma)\n",
    "G_from_B = compute_discounted_return([2.0], gamma)\n",
    "\n",
    "print(\"=== Three-State Chain (A -> B -> C) ===\")\n",
    "print(f\"Rewards: A->{rewards_abc[0]} -> B->{rewards_abc[1]} -> C (terminal)\")\n",
    "print(f\"Gamma: {gamma}\")\n",
    "print(f\"V(A) = {rewards_abc[0]} + {gamma} x {rewards_abc[1]} = {G_from_A:.2f}\")\n",
    "print(f\"V(B) = {rewards_abc[1]:.2f}\")\n",
    "print(f\"V(C) = 0.00 (terminal)\")\n",
    "\n",
    "# Example 2: Longer trajectory\n",
    "rewards_long = [1, 2, 3, 4, 5]\n",
    "G_long = compute_discounted_return(rewards_long, gamma)\n",
    "print(f\"\\n=== Longer Trajectory ===\")\n",
    "print(f\"Rewards: {rewards_long}\")\n",
    "print(f\"Discounted return: {G_long:.2f}\")\n",
    "\n",
    "# Show how each reward contributes\n",
    "print(\"\\nBreakdown:\")\n",
    "for t, r in enumerate(rewards_long):\n",
    "    contribution = r * (gamma ** t)\n",
    "    print(f\"  t={t}: reward={r}, gamma^{t}={gamma**t:.4f}, contribution={contribution:.4f}\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visualize a Value Function"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_values(env, values, title=\"State Values\"):\n",
    "    \"\"\"Visualize value function as a heatmap on the grid.\"\"\"\n",
    "    grid = np.full((env.rows, env.cols), np.nan)\n",
    "\n",
    "    for (r, c), v in values.items():\n",
    "        grid[r, c] = v\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
    "\n",
    "    cmap = LinearSegmentedColormap.from_list('vizuara', ['#fee0d2', '#fc9272', '#de2d26'])\n",
    "    im = ax.imshow(grid, cmap=cmap, interpolation='nearest')\n",
    "\n",
    "    for r in range(env.rows):\n",
    "        for c in range(env.cols):\n",
    "            if (r, c) in env.obstacles:\n",
    "                ax.add_patch(plt.Rectangle((c-0.5, r-0.5), 1, 1, fill=True, color='gray', alpha=0.7))\n",
    "                ax.text(c, r, 'X', ha='center', va='center', fontsize=14, fontweight='bold', color='white')\n",
    "            elif (r, c) == env.goal:\n",
    "                ax.text(c, r, f'{grid[r,c]:.1f}\\nGOAL', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "            elif not np.isnan(grid[r, c]):\n",
    "                ax.text(c, r, f'{grid[r,c]:.1f}', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "    ax.set_xticks(range(env.cols))\n",
    "    ax.set_yticks(range(env.rows))\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, linewidth=2, color='white')\n",
    "    plt.colorbar(im, ax=ax, shrink=0.8, label='Value')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create a simple hand-crafted value function for visualization\n",
    "# (We will compute real ones shortly)\n",
    "manual_values = {}\n",
    "for r in range(5):\n",
    "    for c in range(5):\n",
    "        if (r, c) not in env.obstacles:\n",
    "            # Manhattan distance heuristic (rough approximation)\n",
    "            dist = abs(r - env.goal[0]) + abs(c - env.goal[1])\n",
    "            manual_values[(r, c)] = max(0, 10 - dist * 1.5)\n",
    "\n",
    "visualize_values(env, manual_values, \"Approximate Values (Distance Heuristic)\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Policy Evaluation: Computing V(s) for a Given Policy\n",
    "\n",
    "Now let us compute the true value function for a given policy using iterative policy evaluation."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, gamma=0.9, theta=1e-6, max_iters=1000):\n",
    "    \"\"\"\n",
    "    Compute V^pi(s) for all states using iterative policy evaluation.\n",
    "\n",
    "    The Bellman equation for V^pi is:\n",
    "    V^pi(s) = sum_a pi(a|s) * sum_{s'} p(s'|s,a) * [r + gamma * V^pi(s')]\n",
    "\n",
    "    Since our environment is deterministic, p(s'|s,a) = 1 for one s'.\n",
    "    \"\"\"\n",
    "    states = env.get_all_states()\n",
    "    V = {s: 0.0 for s in states}\n",
    "\n",
    "    for iteration in range(max_iters):\n",
    "        delta = 0.0\n",
    "\n",
    "        for s in states:\n",
    "            if s == env.goal:\n",
    "                continue  # Terminal state has value 0\n",
    "\n",
    "            old_v = V[s]\n",
    "            action = policy[s]\n",
    "            next_s, reward, done = env.step(s, action)\n",
    "\n",
    "            if done:\n",
    "                V[s] = reward\n",
    "            else:\n",
    "                V[s] = reward + gamma * V.get(next_s, 0.0)\n",
    "\n",
    "            delta = max(delta, abs(old_v - V[s]))\n",
    "\n",
    "        if delta < theta:\n",
    "            print(f\"Policy evaluation converged in {iteration + 1} iterations (delta={delta:.8f})\")\n",
    "            break\n",
    "\n",
    "    return V\n",
    "\n",
    "\n",
    "# Define a simple policy: always go RIGHT, then DOWN\n",
    "def simple_policy(env):\n",
    "    \"\"\"A simple policy: go right until wall, then go down.\"\"\"\n",
    "    policy = {}\n",
    "    for r in range(env.rows):\n",
    "        for c in range(env.cols):\n",
    "            if (r, c) not in env.obstacles:\n",
    "                if c < env.cols - 1 and (r, c + 1) not in env.obstacles:\n",
    "                    policy[(r, c)] = 1  # Right\n",
    "                else:\n",
    "                    policy[(r, c)] = 2  # Down\n",
    "    return policy\n",
    "\n",
    "\n",
    "policy = simple_policy(env)\n",
    "V = policy_evaluation(env, policy, gamma=0.9)\n",
    "\n",
    "print(\"\\nState Values under 'go right then down' policy:\")\n",
    "for r in range(env.rows):\n",
    "    row_str = \"\"\n",
    "    for c in range(env.cols):\n",
    "        if (r, c) in env.obstacles:\n",
    "            row_str += \"  XXX  \"\n",
    "        else:\n",
    "            row_str += f\" {V.get((r,c), 0.0):5.2f} \"\n",
    "    print(row_str)"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_values(env, V, \"V^pi: 'Go Right Then Down' Policy\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Computing Q-Values from V"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_values(env, V, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Compute Q(s, a) for all state-action pairs from V(s).\n",
    "\n",
    "    Q^pi(s, a) = r(s, a) + gamma * V^pi(s')\n",
    "    \"\"\"\n",
    "    Q = {}\n",
    "    states = env.get_all_states()\n",
    "\n",
    "    for s in states:\n",
    "        if s == env.goal:\n",
    "            Q[s] = {a: 0.0 for a in range(env.n_actions)}\n",
    "            continue\n",
    "\n",
    "        Q[s] = {}\n",
    "        for a in range(env.n_actions):\n",
    "            next_s, reward, done = env.step(s, a)\n",
    "            if done:\n",
    "                Q[s][a] = reward\n",
    "            else:\n",
    "                Q[s][a] = reward + gamma * V.get(next_s, 0.0)\n",
    "\n",
    "    return Q\n",
    "\n",
    "\n",
    "Q = compute_q_values(env, V, gamma=0.9)\n",
    "\n",
    "# Show Q-values for a specific state\n",
    "state = (0, 0)\n",
    "print(f\"Q-values at state {state}:\")\n",
    "for a in range(env.n_actions):\n",
    "    print(f\"  {env.action_names[a]:>5}: Q = {Q[state][a]:.3f}\")\n",
    "\n",
    "best_action = max(Q[state], key=Q[state].get)\n",
    "print(f\"\\nBest action at {state}: {env.action_names[best_action]} (Q = {Q[state][best_action]:.3f})\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO: Implement a Random Policy Evaluator\n",
    "\n",
    "Below, you need to implement policy evaluation for a *stochastic* (random) policy where the agent picks each action with equal probability."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_random_policy(env, gamma=0.9, theta=1e-6, max_iters=1000):\n",
    "    \"\"\"\n",
    "    Compute V(s) for a uniformly random policy.\n",
    "\n",
    "    Under a random policy, pi(a|s) = 1/n_actions for all a.\n",
    "    So: V(s) = (1/n_actions) * sum_a [r(s,a) + gamma * V(s')]\n",
    "\n",
    "    Returns:\n",
    "        dict mapping state -> value\n",
    "    \"\"\"\n",
    "    states = env.get_all_states()\n",
    "    V = {s: 0.0 for s in states}\n",
    "    n_actions = env.n_actions\n",
    "\n",
    "    for iteration in range(max_iters):\n",
    "        delta = 0.0\n",
    "\n",
    "        for s in states:\n",
    "            if s == env.goal:\n",
    "                continue\n",
    "\n",
    "            old_v = V[s]\n",
    "            # ============ TODO ============\n",
    "            # Compute the new value of state s under a random policy.\n",
    "            # For each action a, compute: r + gamma * V(s')\n",
    "            # Then average across all actions (since each has probability 1/n_actions)\n",
    "            # Hint: Use env.step(s, a) to get (next_state, reward, done)\n",
    "            # ==============================\n",
    "\n",
    "            new_v = 0.0  # YOUR CODE HERE\n",
    "\n",
    "            V[s] = new_v\n",
    "            delta = max(delta, abs(old_v - V[s]))\n",
    "\n",
    "        if delta < theta:\n",
    "            print(f\"Converged in {iteration + 1} iterations\")\n",
    "            break\n",
    "\n",
    "    return V"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "V_random = evaluate_random_policy(env, gamma=0.9)\n",
    "\n",
    "# The random policy should have lower values than the directed policy\n",
    "assert V_random[(0, 0)] < V[(0, 0)], \"Random policy should have lower value than directed policy!\"\n",
    "assert V_random[env.goal] == 0.0, \"Goal state should have value 0\"\n",
    "print(\"All checks passed!\")\n",
    "print(f\"V_random(0,0) = {V_random[(0,0)]:.3f} < V_directed(0,0) = {V[(0,0)]:.3f}\")\n",
    "\n",
    "visualize_values(env, V_random, \"V: Random Policy (Your Implementation)\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us now visualize the full picture: values, Q-values, and the implied greedy policy all on one grid."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy_and_values(env, V, Q, title=\"Policy and Values\"):\n",
    "    \"\"\"Visualize the greedy policy arrows overlaid on value heatmap.\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "    # Value heatmap\n",
    "    grid = np.full((env.rows, env.cols), np.nan)\n",
    "    for (r, c), v in V.items():\n",
    "        grid[r, c] = v\n",
    "\n",
    "    cmap = LinearSegmentedColormap.from_list('vizuara', ['#f7fbff', '#6baed6', '#08519c'])\n",
    "    im = ax.imshow(grid, cmap=cmap, interpolation='nearest')\n",
    "\n",
    "    # Arrow directions for each action\n",
    "    arrow_dx = [0, 0.3, 0, -0.3]  # Up, Right, Down, Left\n",
    "    arrow_dy = [-0.3, 0, 0.3, 0]\n",
    "\n",
    "    for (r, c), q_vals in Q.items():\n",
    "        if (r, c) == env.goal:\n",
    "            ax.text(c, r, 'GOAL', ha='center', va='center', fontsize=10,\n",
    "                    fontweight='bold', color='gold',\n",
    "                    bbox=dict(boxstyle='round', facecolor='green', alpha=0.8))\n",
    "            continue\n",
    "        if (r, c) in env.obstacles:\n",
    "            continue\n",
    "\n",
    "        # Draw greedy policy arrow\n",
    "        best_a = max(q_vals, key=q_vals.get)\n",
    "        ax.annotate('', xy=(c + arrow_dx[best_a], r + arrow_dy[best_a]),\n",
    "                     xytext=(c, r),\n",
    "                     arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "\n",
    "        ax.text(c, r + 0.35, f'{V[(r,c)]:.1f}', ha='center', va='center',\n",
    "                fontsize=9, color='white', fontweight='bold')\n",
    "\n",
    "    for r in range(env.rows):\n",
    "        for c in range(env.cols):\n",
    "            if (r, c) in env.obstacles:\n",
    "                ax.add_patch(plt.Rectangle((c-0.5, r-0.5), 1, 1, fill=True,\n",
    "                                           color='gray', alpha=0.8))\n",
    "                ax.text(c, r, 'X', ha='center', va='center', fontsize=16,\n",
    "                        fontweight='bold', color='white')\n",
    "\n",
    "    ax.set_xticks(range(env.cols))\n",
    "    ax.set_yticks(range(env.rows))\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, linewidth=2, color='white', alpha=0.5)\n",
    "    plt.colorbar(im, ax=ax, shrink=0.8, label='Value')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_policy_and_values(env, V, Q, \"Greedy Policy from V^pi\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "Let us compare value functions across different discount factors to see how gamma affects the agent's perspective."
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [0.5, 0.7, 0.9, 0.99]\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "for idx, gamma in enumerate(gammas):\n",
    "    V_g = policy_evaluation(env, policy, gamma=gamma)\n",
    "\n",
    "    grid = np.full((env.rows, env.cols), np.nan)\n",
    "    for (r, c), v in V_g.items():\n",
    "        grid[r, c] = v\n",
    "\n",
    "    cmap = LinearSegmentedColormap.from_list('vizuara', ['#fff5eb', '#fd8d3c', '#d94701'])\n",
    "    im = axes[idx].imshow(grid, cmap=cmap, interpolation='nearest')\n",
    "    axes[idx].set_title(f'gamma = {gamma}', fontsize=13, fontweight='bold')\n",
    "\n",
    "    for r in range(env.rows):\n",
    "        for c in range(env.cols):\n",
    "            if (r, c) in env.obstacles:\n",
    "                axes[idx].add_patch(plt.Rectangle((c-0.5, r-0.5), 1, 1, fill=True,\n",
    "                                                   color='gray', alpha=0.7))\n",
    "            elif not np.isnan(grid[r, c]):\n",
    "                axes[idx].text(c, r, f'{grid[r,c]:.1f}', ha='center', va='center',\n",
    "                               fontsize=9, fontweight='bold')\n",
    "\n",
    "    axes[idx].set_xticks(range(env.cols))\n",
    "    axes[idx].set_yticks(range(env.rows))\n",
    "    axes[idx].grid(True, linewidth=1, color='white')\n",
    "\n",
    "plt.suptitle('How Discount Factor Affects Value Functions', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how:\")\n",
    "print(\"- Low gamma (0.5): Only states very close to the goal have high values\")\n",
    "print(\"- High gamma (0.99): Distant states also have significant value\")\n",
    "print(\"- Gamma controls the 'horizon' -- how far ahead the agent looks\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the comprehensive summary figure\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Panel 1: State Values\n",
    "grid_v = np.full((env.rows, env.cols), np.nan)\n",
    "for (r, c), v in V.items():\n",
    "    grid_v[r, c] = v\n",
    "cmap1 = LinearSegmentedColormap.from_list('v', ['#f7fbff', '#2171b5'])\n",
    "axes[0].imshow(grid_v, cmap=cmap1)\n",
    "axes[0].set_title('State Values V(s)', fontsize=13, fontweight='bold')\n",
    "for r in range(env.rows):\n",
    "    for c in range(env.cols):\n",
    "        if (r, c) in env.obstacles:\n",
    "            axes[0].add_patch(plt.Rectangle((c-0.5, r-0.5), 1, 1, fill=True, color='gray'))\n",
    "        elif not np.isnan(grid_v[r, c]):\n",
    "            axes[0].text(c, r, f'{grid_v[r,c]:.1f}', ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "\n",
    "# Panel 2: Best Q-values\n",
    "grid_q = np.full((env.rows, env.cols), np.nan)\n",
    "for (r, c) in Q:\n",
    "    if (r, c) not in env.obstacles:\n",
    "        grid_q[r, c] = max(Q[(r, c)].values())\n",
    "cmap2 = LinearSegmentedColormap.from_list('q', ['#fff5f0', '#cb181d'])\n",
    "axes[1].imshow(grid_q, cmap=cmap2)\n",
    "axes[1].set_title('Best Q-values max_a Q(s,a)', fontsize=13, fontweight='bold')\n",
    "for r in range(env.rows):\n",
    "    for c in range(env.cols):\n",
    "        if (r, c) in env.obstacles:\n",
    "            axes[1].add_patch(plt.Rectangle((c-0.5, r-0.5), 1, 1, fill=True, color='gray'))\n",
    "        elif not np.isnan(grid_q[r, c]):\n",
    "            axes[1].text(c, r, f'{grid_q[r,c]:.1f}', ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "\n",
    "# Panel 3: Greedy Policy Arrows\n",
    "axes[2].set_xlim(-0.5, env.cols - 0.5)\n",
    "axes[2].set_ylim(env.rows - 0.5, -0.5)\n",
    "axes[2].set_aspect('equal')\n",
    "axes[2].set_title('Greedy Policy', fontsize=13, fontweight='bold')\n",
    "arrow_dx = [0, 0.35, 0, -0.35]\n",
    "arrow_dy = [-0.35, 0, 0.35, 0]\n",
    "\n",
    "for (r, c), q_vals in Q.items():\n",
    "    if (r, c) == env.goal:\n",
    "        axes[2].plot(c, r, 's', markersize=25, color='gold')\n",
    "        axes[2].text(c, r, 'G', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    elif (r, c) in env.obstacles:\n",
    "        axes[2].add_patch(plt.Rectangle((c-0.5, r-0.5), 1, 1, fill=True, color='gray'))\n",
    "    else:\n",
    "        best_a = max(q_vals, key=q_vals.get)\n",
    "        axes[2].annotate('', xy=(c + arrow_dx[best_a], r + arrow_dy[best_a]),\n",
    "                         xytext=(c, r),\n",
    "                         arrowprops=dict(arrowstyle='->', color='#2171b5', lw=2.5))\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks(range(env.cols))\n",
    "    ax.set_yticks(range(env.rows))\n",
    "    ax.grid(True, linewidth=1, color='lightgray')\n",
    "\n",
    "plt.suptitle('Value Functions and Q-Learning -- Notebook 1 Summary', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Congratulations! You have built value functions from scratch!\")\n",
    "print(\"You now understand V(s), Q(s,a), and how policies shape values.\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "1. Why does the random policy produce lower values than the directed policy? What does this tell us about the relationship between policy quality and value?\n",
    "2. If gamma = 0, what would the value function look like? What kind of agent would this produce?\n",
    "3. Can you think of a real-world scenario where you would want a low discount factor (gamma close to 0)?\n",
    "\n",
    "### Optional Challenges\n",
    "1. Implement a 10x10 grid world with multiple goals and obstacles. Visualize the value landscape.\n",
    "2. Try a stochastic environment where the agent slips (moves in a random direction 20% of the time). How does this change the value function?\n",
    "3. Implement value iteration (instead of policy evaluation) to find the optimal value function directly."
   ],
   "id": "cell_25"
  }
 ]
}