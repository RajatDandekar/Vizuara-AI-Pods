{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Q-Learning from Scratch -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning from Scratch: Learning Without a Map\n",
    "\n",
    "*Part 3 of the Vizuara series on Value Functions and Q-Learning*\n",
    "*Estimated time: 45 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebook, we solved the Bellman equations using value iteration. But there was a catch: we needed complete knowledge of the environment -- the transition dynamics, the reward function, everything.\n",
    "\n",
    "In the real world, you rarely have this luxury. A robot does not know the exact physics of every surface it might walk on. A game-playing agent does not know the rules until it tries things and sees what happens.\n",
    "\n",
    "**Q-Learning** solves this problem. It learns the optimal Q-values directly from experience -- by interacting with the environment, observing rewards, and updating its estimates one step at a time. No model required.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Implement Q-Learning from scratch\n",
    "- Train an agent to solve FrozenLake (a classic RL benchmark)\n",
    "- Understand exploration vs exploitation (epsilon-greedy)\n",
    "- Visualize how Q-values evolve during training\n",
    "- Watch your agent go from random stumbling to purposeful navigation"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Think of learning to navigate a new city without a map. You start by wandering randomly. Sometimes you find a great restaurant (positive reward). Sometimes you end up in a dead end (negative reward).\n",
    "\n",
    "Over time, you build a mental map: \"If I am at the train station and I go left, good things tend to happen.\" This mental map is your Q-table -- it stores, for every location and every direction, how good that combination has been in the past.\n",
    "\n",
    "The key insight of Q-Learning is that you update this mental map after every single step, not just at the end of the trip. And crucially, when you update, you always ask: \"What is the BEST thing I could do from where I ended up?\" -- even if you did not actually do the best thing (maybe you explored instead).\n",
    "\n",
    "### Think About This\n",
    "\n",
    "If you always go to your favorite restaurant (exploitation), you might miss discovering an even better one. But if you always try random places (exploration), you waste many meals on bad food. How do you balance these two? This is the exploration-exploitation dilemma, and Q-Learning has an elegant solution."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### The Q-Learning Update Rule\n",
    "\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\, \\max_{a'} Q(s', a') - Q(s, a) \\right]$$\n",
    "\n",
    "Let us break this down piece by piece:\n",
    "\n",
    "- $Q(s, a)$: our current estimate of the value of taking action $a$ in state $s$\n",
    "- $\\alpha$: the learning rate -- how much we trust the new information (typically 0.01 to 0.5)\n",
    "- $r + \\gamma \\max_{a'} Q(s', a')$: the **TD target** -- what we think the value should be based on what just happened\n",
    "- $r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)$: the **TD error** -- the gap between our target and our current estimate\n",
    "\n",
    "Computationally: \"I just took action $a$ in state $s$, got reward $r$, and landed in state $s'$. The best I can do from $s'$ is $\\max_{a'} Q(s', a')$. So a reasonable estimate of Q(s,a) is $r + \\gamma \\cdot \\max_{a'} Q(s', a')$. I nudge my old estimate toward this new one.\"\n",
    "\n",
    "### Epsilon-Greedy Policy\n",
    "\n",
    "$$a = \\begin{cases} \\text{random action} & \\text{with probability } \\epsilon \\\\ \\arg\\max_a Q(s, a) & \\text{with probability } 1 - \\epsilon \\end{cases}$$\n",
    "\n",
    "Start with high epsilon (lots of exploration), decay it over time (shift toward exploitation)."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 The Q-Learning Agent"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class QLearningAgent:\n",
    "    \"\"\"A tabular Q-Learning agent.\"\"\"\n",
    "\n",
    "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99,\n",
    "                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha        # Learning rate\n",
    "        self.gamma = gamma        # Discount factor\n",
    "        self.epsilon = epsilon    # Exploration rate\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "        # Initialize Q-table with zeros\n",
    "        self.Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "        # Tracking\n",
    "        self.td_errors = []\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.Q[state])            # Exploit\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Q-Learning update: Q(s,a) <- Q(s,a) + alpha * [TD_error]\n",
    "        where TD_error = r + gamma * max_a' Q(s',a') - Q(s,a)\n",
    "        \"\"\"\n",
    "        # TD target: what we think Q(s,a) should be\n",
    "        if done:\n",
    "            td_target = reward  # No future from terminal state\n",
    "        else:\n",
    "            td_target = reward + self.gamma * np.max(self.Q[next_state])\n",
    "\n",
    "        # TD error: how wrong our current estimate is\n",
    "        td_error = td_target - self.Q[state, action]\n",
    "\n",
    "        # Update Q-value\n",
    "        self.Q[state, action] += self.alpha * td_error\n",
    "\n",
    "        self.td_errors.append(abs(td_error))\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Reduce exploration rate.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "# Create agent\n",
    "agent = QLearningAgent(n_states=16, n_actions=4)\n",
    "print(f\"Q-table shape: {agent.Q.shape}\")\n",
    "print(f\"Initial epsilon: {agent.epsilon}\")\n",
    "print(f\"Learning rate: {agent.alpha}\")\n",
    "print(f\"Discount factor: {agent.gamma}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The FrozenLake Environment"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# FrozenLake: 4x4 grid, slippery ice\n",
    "# S = Start, F = Frozen (safe), H = Hole (fall, episode ends), G = Goal\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "print(\"FrozenLake-v1 (4x4, slippery)\")\n",
    "print(f\"States: {env.observation_space.n}\")\n",
    "print(f\"Actions: {env.action_space.n} (0=Left, 1=Down, 2=Right, 3=Up)\")\n",
    "print()\n",
    "print(\"Map:\")\n",
    "print(\"S F F F\")\n",
    "print(\"F H F H\")\n",
    "print(\"F F F H\")\n",
    "print(\"H F F G\")\n",
    "print()\n",
    "print(\"The ice is slippery! The agent may not move in the intended direction.\")\n",
    "print(\"Goal: reach G. Reward: +1 at goal, 0 everywhere else.\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training Loop"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(env, agent, n_episodes=10000, verbose_every=1000):\n",
    "    \"\"\"Train the Q-Learning agent.\"\"\"\n",
    "    rewards_per_episode = []\n",
    "    epsilon_history = []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            # Choose action\n",
    "            action = agent.choose_action(state)\n",
    "\n",
    "            # Take action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Q-Learning update\n",
    "            agent.update(state, action, reward, next_state, terminated)\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "\n",
    "        # Decay epsilon\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "\n",
    "        if (episode + 1) % verbose_every == 0:\n",
    "            recent_success = np.mean(rewards_per_episode[-100:])\n",
    "            print(f\"Episode {episode+1:5d} | \"\n",
    "                  f\"Success Rate (last 100): {recent_success:.2%} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "    return rewards_per_episode, epsilon_history\n",
    "\n",
    "\n",
    "# Create fresh agent and train\n",
    "agent = QLearningAgent(\n",
    "    n_states=env.observation_space.n,\n",
    "    n_actions=env.action_space.n,\n",
    "    alpha=0.1,\n",
    "    gamma=0.99,\n",
    "    epsilon=1.0,\n",
    "    epsilon_decay=0.995,\n",
    "    epsilon_min=0.01,\n",
    ")\n",
    "\n",
    "rewards, epsilons = train_q_learning(env, agent, n_episodes=10000)\n",
    "print(f\"\\nFinal success rate (last 100): {np.mean(rewards[-100:]):.2%}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Training progress\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Moving average of success rate\n",
    "window = 100\n",
    "moving_avg = [np.mean(rewards[max(0, i-window):i+1]) for i in range(len(rewards))]\n",
    "\n",
    "ax1.plot(moving_avg, color='#2171b5', linewidth=1.5)\n",
    "ax1.fill_between(range(len(moving_avg)), moving_avg, alpha=0.2, color='#2171b5')\n",
    "ax1.set_xlabel('Episode', fontsize=12)\n",
    "ax1.set_ylabel('Success Rate (100-ep avg)', fontsize=12)\n",
    "ax1.set_title('Q-Learning Training Progress on FrozenLake', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Epsilon decay\n",
    "ax2.plot(epsilons, color='#d94701', linewidth=1.5)\n",
    "ax2.set_xlabel('Episode', fontsize=12)\n",
    "ax2.set_ylabel('Epsilon', fontsize=12)\n",
    "ax2.set_title('Exploration Rate Decay', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Visualize the Learned Q-Table"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_q_table(Q, title=\"Learned Q-Table\"):\n",
    "    \"\"\"Visualize Q-values for FrozenLake.\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    ax.set_xlim(-0.5, 3.5)\n",
    "    ax.set_ylim(3.5, -0.5)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    action_names = ['Left', 'Down', 'Right', 'Up']\n",
    "    arrow_dx = [0, -0.3, 0, 0.3]  # Swapped for display: Left=-x, Right=+x\n",
    "    arrow_dy = [0.3, 0, -0.3, 0]  # Up=-y, Down=+y\n",
    "\n",
    "    # FrozenLake map\n",
    "    lake_map = [\n",
    "        ['S', 'F', 'F', 'F'],\n",
    "        ['F', 'H', 'F', 'H'],\n",
    "        ['F', 'F', 'F', 'H'],\n",
    "        ['H', 'F', 'F', 'G'],\n",
    "    ]\n",
    "\n",
    "    for r in range(4):\n",
    "        for c in range(4):\n",
    "            state = r * 4 + c\n",
    "            cell = lake_map[r][c]\n",
    "\n",
    "            # Background color\n",
    "            if cell == 'H':\n",
    "                ax.add_patch(plt.Rectangle((c-0.5, r-0.5), 1, 1, facecolor='#ffcccc', edgecolor='black'))\n",
    "                ax.text(c, r, 'HOLE', ha='center', va='center', fontsize=9, color='red', fontweight='bold')\n",
    "            elif cell == 'G':\n",
    "                ax.add_patch(plt.Rectangle((c-0.5, r-0.5), 1, 1, facecolor='#ccffcc', edgecolor='black'))\n",
    "                ax.text(c, r, 'GOAL', ha='center', va='center', fontsize=9, color='green', fontweight='bold')\n",
    "            elif cell == 'S':\n",
    "                ax.add_patch(plt.Rectangle((c-0.5, r-0.5), 1, 1, facecolor='#cce5ff', edgecolor='black'))\n",
    "            else:\n",
    "                ax.add_patch(plt.Rectangle((c-0.5, r-0.5), 1, 1, facecolor='white', edgecolor='black'))\n",
    "\n",
    "            if cell not in ['H', 'G']:\n",
    "                # Draw arrow for best action\n",
    "                best_a = np.argmax(Q[state])\n",
    "                q_max = Q[state, best_a]\n",
    "                if q_max > 0:\n",
    "                    # Arrow direction mapping for FrozenLake: 0=Left, 1=Down, 2=Right, 3=Up\n",
    "                    dx_map = [-0.3, 0, 0.3, 0]\n",
    "                    dy_map = [0, 0.3, 0, -0.3]\n",
    "                    ax.annotate('', xy=(c + dx_map[best_a], r + dy_map[best_a]),\n",
    "                               xytext=(c, r),\n",
    "                               arrowprops=dict(arrowstyle='->', color='#2171b5', lw=2.5))\n",
    "\n",
    "                # Show Q-values in corners\n",
    "                for a in range(4):\n",
    "                    q_val = Q[state, a]\n",
    "                    if q_val != 0:\n",
    "                        offset_x = [-0.35, 0, 0.35, 0][a]\n",
    "                        offset_y = [0, 0.35, 0, -0.35][a]\n",
    "                        ax.text(c + offset_x, r + offset_y, f'{q_val:.2f}',\n",
    "                               ha='center', va='center', fontsize=6, color='gray')\n",
    "\n",
    "    ax.set_xticks(range(4))\n",
    "    ax.set_yticks(range(4))\n",
    "    ax.set_xticklabels(['Col 0', 'Col 1', 'Col 2', 'Col 3'])\n",
    "    ax.set_yticklabels(['Row 0', 'Row 1', 'Row 2', 'Row 3'])\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, linewidth=1, color='lightgray')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_q_table(agent.Q, \"Learned Q-Table: FrozenLake (Slippery)\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO: Implement Q-Learning with Different Exploration Strategies\n",
    "\n",
    "Compare epsilon-greedy with a Boltzmann (softmax) exploration strategy."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoltzmannQLearningAgent:\n",
    "    \"\"\"Q-Learning agent with Boltzmann (softmax) exploration.\"\"\"\n",
    "\n",
    "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99,\n",
    "                 temperature=1.0, temp_decay=0.995, temp_min=0.01):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.temperature = temperature\n",
    "        self.temp_decay = temp_decay\n",
    "        self.temp_min = temp_min\n",
    "        self.Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Boltzmann exploration: probability of choosing action a is proportional\n",
    "        to exp(Q(s,a) / temperature).\n",
    "\n",
    "        Higher temperature -> more random (like high epsilon)\n",
    "        Lower temperature -> more greedy (like low epsilon)\n",
    "        \"\"\"\n",
    "        # ============ TODO ============\n",
    "        # Step 1: Compute logits = Q[state] / self.temperature\n",
    "        # Step 2: For numerical stability, subtract max(logits)\n",
    "        # Step 3: Compute exp_logits = np.exp(logits)\n",
    "        # Step 4: Compute probabilities = exp_logits / sum(exp_logits)\n",
    "        # Step 5: Return np.random.choice(self.n_actions, p=probabilities)\n",
    "        # ==============================\n",
    "\n",
    "        return 0  # YOUR CODE HERE\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Same Q-Learning update as before.\"\"\"\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            td_target = reward + self.gamma * np.max(self.Q[next_state])\n",
    "\n",
    "        td_error = td_target - self.Q[state, action]\n",
    "        self.Q[state, action] += self.alpha * td_error\n",
    "\n",
    "    def decay_temperature(self):\n",
    "        self.temperature = max(self.temp_min, self.temperature * self.temp_decay)"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: Train and compare\n",
    "boltzmann_agent = BoltzmannQLearningAgent(\n",
    "    n_states=env.observation_space.n,\n",
    "    n_actions=env.action_space.n,\n",
    ")\n",
    "\n",
    "boltz_rewards = []\n",
    "for ep in range(10000):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = boltzmann_agent.choose_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        boltzmann_agent.update(state, action, reward, next_state, terminated)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "    boltzmann_agent.decay_temperature()\n",
    "    boltz_rewards.append(total_reward)\n",
    "\n",
    "# Compare\n",
    "window = 100\n",
    "eps_avg = [np.mean(rewards[max(0,i-window):i+1]) for i in range(len(rewards))]\n",
    "boltz_avg = [np.mean(boltz_rewards[max(0,i-window):i+1]) for i in range(len(boltz_rewards))]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(eps_avg, label='Epsilon-Greedy', color='#2171b5', alpha=0.8)\n",
    "plt.plot(boltz_avg, label='Boltzmann', color='#d94701', alpha=0.8)\n",
    "plt.xlabel('Episode', fontsize=12)\n",
    "plt.ylabel('Success Rate (100-ep avg)', fontsize=12)\n",
    "plt.title('Epsilon-Greedy vs Boltzmann Exploration', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Epsilon-greedy final: {np.mean(rewards[-100:]):.2%}\")\n",
    "print(f\"Boltzmann final:      {np.mean(boltz_rewards[-100:]):.2%}\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us test the trained agent and watch it navigate FrozenLake."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, agent, n_episodes=100):\n",
    "    \"\"\"Test the trained agent without exploration.\"\"\"\n",
    "    successes = 0\n",
    "    trajectories = []\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        trajectory = [state]\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(agent.Q[state])  # Greedy\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            trajectory.append(state)\n",
    "\n",
    "            if reward > 0:\n",
    "                successes += 1\n",
    "\n",
    "        trajectories.append(trajectory)\n",
    "\n",
    "    print(f\"Test Results ({n_episodes} episodes):\")\n",
    "    print(f\"  Success rate: {successes / n_episodes:.2%}\")\n",
    "    return trajectories\n",
    "\n",
    "\n",
    "trajectories = test_agent(env, agent, n_episodes=1000)"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "Let us study the learning dynamics more carefully."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD error evolution\n",
    "window = 500\n",
    "td_moving_avg = [np.mean(agent.td_errors[max(0,i-window):i+1])\n",
    "                 for i in range(len(agent.td_errors))]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# TD errors\n",
    "axes[0].plot(td_moving_avg[::100], color='#756bb1', linewidth=1)\n",
    "axes[0].set_xlabel('Step (x100)', fontsize=11)\n",
    "axes[0].set_ylabel('Mean |TD Error|', fontsize=11)\n",
    "axes[0].set_title('TD Error Over Training', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-value distribution\n",
    "q_nonzero = agent.Q[agent.Q != 0]\n",
    "axes[1].hist(q_nonzero, bins=30, color='#2171b5', alpha=0.7, edgecolor='white')\n",
    "axes[1].set_xlabel('Q-value', fontsize=11)\n",
    "axes[1].set_ylabel('Count', fontsize=11)\n",
    "axes[1].set_title('Distribution of Learned Q-values', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# State visit frequency from test trajectories\n",
    "visit_counts = np.zeros(16)\n",
    "for traj in trajectories[:100]:\n",
    "    for s in traj:\n",
    "        visit_counts[s] += 1\n",
    "\n",
    "visit_grid = visit_counts.reshape(4, 4)\n",
    "im = axes[2].imshow(visit_grid, cmap='YlOrRd')\n",
    "axes[2].set_title('State Visit Frequency (Test)', fontsize=13, fontweight='bold')\n",
    "for r in range(4):\n",
    "    for c in range(4):\n",
    "        axes[2].text(c, r, f'{int(visit_grid[r,c])}', ha='center', va='center',\n",
    "                    fontsize=11, fontweight='bold')\n",
    "axes[2].set_xticks(range(4))\n",
    "axes[2].set_yticks(range(4))\n",
    "plt.colorbar(im, ax=axes[2], shrink=0.8)\n",
    "\n",
    "plt.suptitle('Q-Learning Analysis', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the learned policy as a visual map\n",
    "print(\"=\" * 50)\n",
    "print(\"  LEARNED POLICY FOR FROZENLAKE\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "\n",
    "action_arrows = ['<', 'v', '>', '^']\n",
    "lake_map = [\n",
    "    ['S', 'F', 'F', 'F'],\n",
    "    ['F', 'H', 'F', 'H'],\n",
    "    ['F', 'F', 'F', 'H'],\n",
    "    ['H', 'F', 'F', 'G'],\n",
    "]\n",
    "\n",
    "for r in range(4):\n",
    "    row = \"\"\n",
    "    for c in range(4):\n",
    "        state = r * 4 + c\n",
    "        cell = lake_map[r][c]\n",
    "        if cell == 'H':\n",
    "            row += \" [XX] \"\n",
    "        elif cell == 'G':\n",
    "            row += \" [GG] \"\n",
    "        else:\n",
    "            best = np.argmax(agent.Q[state])\n",
    "            row += f\" [{action_arrows[best]} ] \"\n",
    "    print(row)\n",
    "\n",
    "print()\n",
    "print(f\"Final success rate: {np.mean(rewards[-100:]):.1%}\")\n",
    "print(f\"The agent learned to navigate the slippery frozen lake!\")\n",
    "print()\n",
    "\n",
    "# Final comprehensive plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Q-table heatmap\n",
    "im1 = ax1.imshow(agent.Q, cmap='RdYlBu', aspect='auto')\n",
    "ax1.set_xlabel('Action (0=L, 1=D, 2=R, 3=U)', fontsize=11)\n",
    "ax1.set_ylabel('State', fontsize=11)\n",
    "ax1.set_title('Complete Q-Table', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# Right: Training curve\n",
    "moving_avg = [np.mean(rewards[max(0,i-100):i+1]) for i in range(len(rewards))]\n",
    "ax2.plot(moving_avg, color='#2171b5', linewidth=1.5)\n",
    "ax2.fill_between(range(len(moving_avg)), moving_avg, alpha=0.15, color='#2171b5')\n",
    "ax2.set_xlabel('Episode', fontsize=11)\n",
    "ax2.set_ylabel('Success Rate', fontsize=11)\n",
    "ax2.set_title('Learning Curve', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Q-Learning on FrozenLake -- Complete Results', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Congratulations! You have trained a Q-Learning agent from scratch!\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "1. Q-Learning is called \"off-policy.\" Why? What is the behavior policy, and what is the target policy?\n",
    "2. The slippery environment makes FrozenLake hard. How would the success rate change with is_slippery=False?\n",
    "3. What happens if alpha is too large (e.g., 0.9)? What if it is too small (e.g., 0.001)?\n",
    "\n",
    "### Optional Challenges\n",
    "1. Implement Double Q-Learning to reduce overestimation bias. Compare with vanilla Q-Learning.\n",
    "2. Try the 8x8 FrozenLake (FrozenLake-v1 with map_name=\"8x8\"). Does tabular Q-Learning still work? How many episodes does it need?\n",
    "3. Implement SARSA (on-policy TD learning) and compare its behavior with Q-Learning on slippery FrozenLake."
   ],
   "id": "cell_23"
  }
 ]
}