{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "The Bellman Equation -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bellman Equation: Breaking the Future into One Step at a Time\n",
    "\n",
    "*Part 2 of the Vizuara series on Value Functions and Q-Learning*\n",
    "*Estimated time: 40 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebook, we computed value functions by running policy evaluation until convergence. But we never asked the deeper question: **why does that iterative process converge at all?**\n",
    "\n",
    "The answer lies in the Bellman equation -- a recursive relationship discovered by Richard Bellman in the 1950s that says: \"You do not need to think about the entire future. Just think about the next step, and let the recursion handle the rest.\"\n",
    "\n",
    "This single insight is the mathematical foundation behind virtually every reinforcement learning algorithm ever invented -- from tabular Q-learning to the deep RL agents that play Atari and control robots.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Derive and implement the Bellman equation for V and Q\n",
    "- Solve the Bellman optimality equation to find the best policy\n",
    "- Implement value iteration from scratch\n",
    "- Watch optimal policies emerge from pure computation"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Imagine you are playing Pac-Man with a fixed strategy: if a ghost is nearby, move away; if there is a pellet, move towards it. This strategy is your policy.\n",
    "\n",
    "Now, someone asks: \"If you start at this specific maze location with this strategy, how many points will you score on average?\"\n",
    "\n",
    "You could simulate the entire game from that point thousands of times and average the scores. But the Bellman equation offers a shortcut: **just look at what happens in the next step, and then add the value of wherever that step takes you.**\n",
    "\n",
    "This is like asking for directions. Instead of getting a complete route from NYC to LA, you just ask: \"Which highway should I take next?\" and then ask again when you reach the next junction. One step at a time.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "If you know the value of every state your neighbor leads to, can you figure out the value of your own state? If so, and if everyone does this simultaneously, would the values eventually stabilize? This is exactly what the Bellman equation guarantees."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### The Bellman Equation for V\n",
    "\n",
    "$$V^{\\pi}(s) = \\sum_{a} \\pi(a \\mid s) \\sum_{s', r} p(s', r \\mid s, a) \\left[ r + \\gamma \\, V^{\\pi}(s') \\right]$$\n",
    "\n",
    "In words: the value of state $s$ equals the sum over all actions (weighted by the policy) of the sum over all possible next states (weighted by transition probabilities) of the immediate reward plus the discounted value of the next state.\n",
    "\n",
    "For a deterministic environment with a deterministic policy, this simplifies to:\n",
    "\n",
    "$$V^{\\pi}(s) = r(s, \\pi(s)) + \\gamma \\cdot V^{\\pi}(s')$$\n",
    "\n",
    "This is the recursive structure: the value of the current state depends on the value of the next state.\n",
    "\n",
    "### The Bellman Optimality Equation\n",
    "\n",
    "$$V^*(s) = \\max_{a} \\sum_{s', r} p(s', r \\mid s, a) \\left[ r + \\gamma \\, V^*(s') \\right]$$\n",
    "\n",
    "Instead of averaging over actions according to a policy, we take the **maximum** -- always pick the best action. This gives us the optimal value function.\n",
    "\n",
    "### The Bellman Optimality Equation for Q\n",
    "\n",
    "$$Q^*(s, a) = \\sum_{s', r} p(s', r \\mid s, a) \\left[ r + \\gamma \\, \\max_{a'} Q^*(s', a') \\right]$$\n",
    "\n",
    "This is the key equation for Q-learning: the optimal Q-value for taking action $a$ in state $s$ equals the reward plus the discounted maximum Q-value in the next state."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 Setup: Our Grid World"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "class GridWorld:\n",
    "    \"\"\"A simple deterministic grid world.\"\"\"\n",
    "\n",
    "    def __init__(self, rows=4, cols=4, goal=(3, 3), obstacles=None,\n",
    "                 step_reward=-1.0, goal_reward=10.0):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.goal = goal\n",
    "        self.obstacles = obstacles or [(1, 1)]\n",
    "        self.step_reward = step_reward\n",
    "        self.goal_reward = goal_reward\n",
    "        self.actions = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n",
    "        self.action_names = ['Up', 'Right', 'Down', 'Left']\n",
    "        self.action_symbols = ['^', '>', 'v', '<']\n",
    "        self.n_actions = 4\n",
    "\n",
    "    def is_valid(self, r, c):\n",
    "        return (0 <= r < self.rows and 0 <= c < self.cols\n",
    "                and (r, c) not in self.obstacles)\n",
    "\n",
    "    def step(self, state, action):\n",
    "        r, c = state\n",
    "        dr, dc = self.actions[action]\n",
    "        nr, nc = r + dr, c + dc\n",
    "        if not self.is_valid(nr, nc):\n",
    "            nr, nc = r, c\n",
    "        reward = self.goal_reward if (nr, nc) == self.goal else self.step_reward\n",
    "        done = (nr, nc) == self.goal\n",
    "        return (nr, nc), reward, done\n",
    "\n",
    "    def get_all_states(self):\n",
    "        return [(r, c) for r in range(self.rows)\n",
    "                for c in range(self.cols) if (r, c) not in self.obstacles]\n",
    "\n",
    "\n",
    "env = GridWorld(rows=4, cols=4, goal=(3, 3), obstacles=[(1, 1)])\n",
    "print(f\"Grid: {env.rows}x{env.cols}, Goal: {env.goal}, Obstacles: {env.obstacles}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Bellman Equation in Action: One-Step Backup\n",
    "\n",
    "Let us implement the core of the Bellman equation -- the one-step backup."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_backup_v(env, V, state, policy, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Compute one Bellman backup for V^pi(s).\n",
    "\n",
    "    V^pi(s) = sum_a pi(a|s) * [r(s,a) + gamma * V^pi(s')]\n",
    "\n",
    "    For a deterministic policy: V^pi(s) = r(s, pi(s)) + gamma * V^pi(s')\n",
    "    \"\"\"\n",
    "    if state == env.goal:\n",
    "        return 0.0\n",
    "\n",
    "    action = policy[state]\n",
    "    next_state, reward, done = env.step(state, action)\n",
    "\n",
    "    if done:\n",
    "        return reward\n",
    "    else:\n",
    "        return reward + gamma * V.get(next_state, 0.0)\n",
    "\n",
    "\n",
    "def bellman_backup_v_stochastic(env, V, state, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Compute one Bellman backup for V(s) under a random policy.\n",
    "\n",
    "    V(s) = (1/|A|) * sum_a [r(s,a) + gamma * V(s')]\n",
    "    \"\"\"\n",
    "    if state == env.goal:\n",
    "        return 0.0\n",
    "\n",
    "    total = 0.0\n",
    "    for a in range(env.n_actions):\n",
    "        next_state, reward, done = env.step(state, a)\n",
    "        if done:\n",
    "            total += reward\n",
    "        else:\n",
    "            total += reward + gamma * V.get(next_state, 0.0)\n",
    "\n",
    "    return total / env.n_actions\n",
    "\n",
    "\n",
    "# Demonstrate the backup step by step\n",
    "V = {s: 0.0 for s in env.get_all_states()}\n",
    "\n",
    "# Set some initial values to show the backup\n",
    "V[(2, 3)] = 5.0\n",
    "V[(3, 2)] = 3.0\n",
    "\n",
    "state = (2, 2)\n",
    "print(f\"State: {state}\")\n",
    "print(f\"Neighbors' values: right={(2,3)}:{V[(2,3)]:.1f}, down={(3,2)}:{V[(3,2)]:.1f}\")\n",
    "print()\n",
    "\n",
    "# Show the Bellman backup for a random policy at (2,2)\n",
    "new_v = bellman_backup_v_stochastic(env, V, state, gamma=0.9)\n",
    "print(f\"Bellman backup (random policy) at {state}:\")\n",
    "print(f\"  V({state}) = (1/4) * sum of [r + 0.9 * V(s')]\")\n",
    "for a in range(env.n_actions):\n",
    "    ns, r, d = env.step(state, a)\n",
    "    v_next = V.get(ns, 0.0)\n",
    "    print(f\"  Action {env.action_names[a]:>5}: next={ns}, r={r:.1f}, V(next)={v_next:.1f}, \"\n",
    "          f\"contribution={r + 0.9 * v_next:.2f}\")\n",
    "print(f\"  Average = {new_v:.2f}\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Value Iteration: Finding the Optimal V*\n",
    "\n",
    "Value iteration uses the Bellman optimality equation repeatedly until convergence."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.9, theta=1e-6, max_iters=1000):\n",
    "    \"\"\"\n",
    "    Find V*(s) using value iteration.\n",
    "\n",
    "    Update rule: V(s) <- max_a [r(s,a) + gamma * V(s')]\n",
    "\n",
    "    This applies the Bellman optimality equation as an update.\n",
    "    \"\"\"\n",
    "    states = env.get_all_states()\n",
    "    V = {s: 0.0 for s in states}\n",
    "    history = []\n",
    "\n",
    "    for iteration in range(max_iters):\n",
    "        delta = 0.0\n",
    "        V_snapshot = dict(V)\n",
    "\n",
    "        for s in states:\n",
    "            if s == env.goal:\n",
    "                continue\n",
    "\n",
    "            old_v = V[s]\n",
    "\n",
    "            # Bellman optimality backup: max over all actions\n",
    "            best_value = float('-inf')\n",
    "            for a in range(env.n_actions):\n",
    "                next_s, reward, done = env.step(s, a)\n",
    "                if done:\n",
    "                    value = reward\n",
    "                else:\n",
    "                    value = reward + gamma * V.get(next_s, 0.0)\n",
    "                best_value = max(best_value, value)\n",
    "\n",
    "            V[s] = best_value\n",
    "            delta = max(delta, abs(old_v - V[s]))\n",
    "\n",
    "        history.append(dict(V))\n",
    "\n",
    "        if delta < theta:\n",
    "            print(f\"Value iteration converged in {iteration + 1} iterations (delta={delta:.8f})\")\n",
    "            break\n",
    "\n",
    "    return V, history\n",
    "\n",
    "\n",
    "V_star, history = value_iteration(env, gamma=0.9)\n",
    "\n",
    "print(\"\\nOptimal Values V*(s):\")\n",
    "for r in range(env.rows):\n",
    "    row_str = \"\"\n",
    "    for c in range(env.cols):\n",
    "        if (r, c) in env.obstacles:\n",
    "            row_str += \"  XXX  \"\n",
    "        else:\n",
    "            row_str += f\" {V_star.get((r,c), 0.0):5.1f} \"\n",
    "    print(row_str)"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Watch value iteration converge\n",
    "snapshots_to_show = [0, 1, 2, 5, len(history)-1]\n",
    "snapshots_to_show = [i for i in snapshots_to_show if i < len(history)]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(snapshots_to_show), figsize=(4*len(snapshots_to_show), 4))\n",
    "if len(snapshots_to_show) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list('vi', ['#f7fbff', '#2171b5'])\n",
    "\n",
    "for idx, snap_idx in enumerate(snapshots_to_show):\n",
    "    V_snap = history[snap_idx]\n",
    "    grid = np.full((env.rows, env.cols), np.nan)\n",
    "    for (r, c), v in V_snap.items():\n",
    "        grid[r, c] = v\n",
    "\n",
    "    axes[idx].imshow(grid, cmap=cmap, vmin=-10, vmax=10)\n",
    "    axes[idx].set_title(f'Iteration {snap_idx + 1}', fontsize=12, fontweight='bold')\n",
    "\n",
    "    for r in range(env.rows):\n",
    "        for c in range(env.cols):\n",
    "            if (r, c) in env.obstacles:\n",
    "                axes[idx].add_patch(plt.Rectangle((c-0.5, r-0.5), 1, 1, fill=True, color='gray'))\n",
    "            elif not np.isnan(grid[r, c]):\n",
    "                axes[idx].text(c, r, f'{grid[r,c]:.1f}', ha='center', va='center',\n",
    "                               fontsize=10, fontweight='bold',\n",
    "                               color='white' if abs(grid[r,c]) > 3 else 'black')\n",
    "\n",
    "    axes[idx].set_xticks(range(env.cols))\n",
    "    axes[idx].set_yticks(range(env.rows))\n",
    "    axes[idx].grid(True, linewidth=1, color='white')\n",
    "\n",
    "plt.suptitle('Value Iteration: Watch V* Emerge', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Extract the Optimal Policy from V*"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy(env, V_star, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Extract the optimal policy from V*.\n",
    "\n",
    "    pi*(s) = argmax_a [r(s,a) + gamma * V*(s')]\n",
    "    \"\"\"\n",
    "    policy = {}\n",
    "    for s in env.get_all_states():\n",
    "        if s == env.goal:\n",
    "            policy[s] = 0  # Arbitrary for terminal\n",
    "            continue\n",
    "\n",
    "        best_action = 0\n",
    "        best_value = float('-inf')\n",
    "\n",
    "        for a in range(env.n_actions):\n",
    "            next_s, reward, done = env.step(s, a)\n",
    "            if done:\n",
    "                value = reward\n",
    "            else:\n",
    "                value = reward + gamma * V_star.get(next_s, 0.0)\n",
    "\n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_action = a\n",
    "\n",
    "        policy[s] = best_action\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "optimal_policy = extract_policy(env, V_star, gamma=0.9)\n",
    "\n",
    "print(\"Optimal Policy (arrows show best action):\")\n",
    "for r in range(env.rows):\n",
    "    row_str = \"\"\n",
    "    for c in range(env.cols):\n",
    "        if (r, c) in env.obstacles:\n",
    "            row_str += \"  X  \"\n",
    "        elif (r, c) == env.goal:\n",
    "            row_str += \"  G  \"\n",
    "        else:\n",
    "            row_str += f\"  {env.action_symbols[optimal_policy[(r,c)]]}  \"\n",
    "    print(row_str)"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO: Implement Q-Value Iteration\n",
    "\n",
    "Instead of finding V*, find Q* directly using the Bellman optimality equation for Q."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_value_iteration(env, gamma=0.9, theta=1e-6, max_iters=1000):\n",
    "    \"\"\"\n",
    "    Find Q*(s, a) for all state-action pairs using value iteration.\n",
    "\n",
    "    Update rule: Q(s, a) <- r(s,a) + gamma * max_{a'} Q(s', a')\n",
    "\n",
    "    This directly solves the Bellman optimality equation for Q.\n",
    "\n",
    "    Returns:\n",
    "        dict mapping state -> dict mapping action -> Q-value\n",
    "    \"\"\"\n",
    "    states = env.get_all_states()\n",
    "    Q = {s: {a: 0.0 for a in range(env.n_actions)} for s in states}\n",
    "\n",
    "    for iteration in range(max_iters):\n",
    "        delta = 0.0\n",
    "\n",
    "        for s in states:\n",
    "            if s == env.goal:\n",
    "                continue\n",
    "\n",
    "            for a in range(env.n_actions):\n",
    "                old_q = Q[s][a]\n",
    "\n",
    "                # ============ TODO ============\n",
    "                # Compute the new Q-value using the Bellman optimality equation:\n",
    "                # Q(s, a) = r(s,a) + gamma * max_{a'} Q(s', a')\n",
    "                #\n",
    "                # Step 1: Use env.step(s, a) to get (next_state, reward, done)\n",
    "                # Step 2: If done, Q(s,a) = reward\n",
    "                # Step 3: Otherwise, Q(s,a) = reward + gamma * max over a' of Q(s', a')\n",
    "                # ==============================\n",
    "\n",
    "                new_q = 0.0  # YOUR CODE HERE\n",
    "\n",
    "                Q[s][a] = new_q\n",
    "                delta = max(delta, abs(old_q - new_q))\n",
    "\n",
    "        if delta < theta:\n",
    "            print(f\"Q-value iteration converged in {iteration + 1} iterations\")\n",
    "            break\n",
    "\n",
    "    return Q"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "Q_star = q_value_iteration(env, gamma=0.9)\n",
    "\n",
    "# Check: V*(s) should equal max_a Q*(s, a)\n",
    "for s in env.get_all_states():\n",
    "    v_from_q = max(Q_star[s].values())\n",
    "    v_from_vi = V_star[s]\n",
    "    assert abs(v_from_q - v_from_vi) < 0.01, f\"Mismatch at {s}: Q gives {v_from_q:.3f}, V gives {v_from_vi:.3f}\"\n",
    "\n",
    "print(\"All checks passed! Q* is consistent with V*.\")\n",
    "print(f\"\\nQ* at state (0,0):\")\n",
    "for a in range(env.n_actions):\n",
    "    print(f\"  {env.action_names[a]:>5}: {Q_star[(0,0)][a]:.3f}\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison: suboptimal vs optimal policy\n",
    "\n",
    "# Suboptimal: always go right\n",
    "suboptimal_policy = {s: 1 for s in env.get_all_states()}  # Always right\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def policy_evaluation_full(env, policy, gamma=0.9, theta=1e-6, max_iters=1000):\n",
    "    states = env.get_all_states()\n",
    "    V = {s: 0.0 for s in states}\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            if s == env.goal: continue\n",
    "            old_v = V[s]\n",
    "            ns, r, d = env.step(s, policy[s])\n",
    "            V[s] = r + (0 if d else gamma * V.get(ns, 0))\n",
    "            delta = max(delta, abs(old_v - V[s]))\n",
    "        if delta < theta: break\n",
    "    return V\n",
    "\n",
    "V_subopt = policy_evaluation_full(env, suboptimal_policy)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for idx, (title, V_show, pol) in enumerate([\n",
    "    (\"Suboptimal: Always Right\", V_subopt, suboptimal_policy),\n",
    "    (\"Optimal: Value Iteration\", V_star, optimal_policy),\n",
    "]):\n",
    "    grid = np.full((env.rows, env.cols), np.nan)\n",
    "    for (r, c), v in V_show.items():\n",
    "        grid[r, c] = v\n",
    "\n",
    "    color = 'Reds' if idx == 0 else 'Blues'\n",
    "    axes[idx].imshow(grid, cmap=color, interpolation='nearest')\n",
    "    axes[idx].set_title(title, fontsize=14, fontweight='bold')\n",
    "\n",
    "    arrow_dx = [0, 0.35, 0, -0.35]\n",
    "    arrow_dy = [-0.35, 0, 0.35, 0]\n",
    "\n",
    "    for (r, c) in env.get_all_states():\n",
    "        if (r, c) == env.goal:\n",
    "            axes[idx].text(c, r, 'G', ha='center', va='center', fontsize=14,\n",
    "                          fontweight='bold', color='gold')\n",
    "            continue\n",
    "\n",
    "        a = pol[(r, c)]\n",
    "        axes[idx].annotate('', xy=(c + arrow_dx[a], r + arrow_dy[a]),\n",
    "                           xytext=(c, r),\n",
    "                           arrowprops=dict(arrowstyle='->', lw=2,\n",
    "                                           color='darkred' if idx == 0 else 'darkblue'))\n",
    "        axes[idx].text(c, r + 0.35, f'{V_show[(r,c)]:.1f}', ha='center', va='center',\n",
    "                      fontsize=9, fontweight='bold', color='white')\n",
    "\n",
    "    for r in range(env.rows):\n",
    "        for c in range(env.cols):\n",
    "            if (r, c) in env.obstacles:\n",
    "                axes[idx].add_patch(plt.Rectangle((c-0.5, r-0.5), 1, 1, color='gray'))\n",
    "\n",
    "    axes[idx].set_xticks(range(env.cols))\n",
    "    axes[idx].set_yticks(range(env.rows))\n",
    "    axes[idx].grid(True, linewidth=1, color='white')\n",
    "\n",
    "plt.suptitle('Suboptimal vs Optimal Policy', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "Let us track how value iteration converges over iterations."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergence analysis\n",
    "max_value_per_iter = []\n",
    "for V_snap in history:\n",
    "    vals = [v for s, v in V_snap.items() if s != env.goal]\n",
    "    max_value_per_iter.append(max(vals) if vals else 0)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(range(1, len(max_value_per_iter)+1), max_value_per_iter, 'b-o', markersize=4)\n",
    "ax1.set_xlabel('Iteration', fontsize=12)\n",
    "ax1.set_ylabel('Max V(s)', fontsize=12)\n",
    "ax1.set_title('Value Iteration Convergence', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Show delta (change) per iteration\n",
    "deltas = []\n",
    "for i in range(1, len(history)):\n",
    "    d = max(abs(history[i][s] - history[i-1][s]) for s in env.get_all_states())\n",
    "    deltas.append(d)\n",
    "\n",
    "ax2.semilogy(range(2, len(history)+1), deltas, 'r-o', markersize=4)\n",
    "ax2.set_xlabel('Iteration', fontsize=12)\n",
    "ax2.set_ylabel('Max Delta (log scale)', fontsize=12)\n",
    "ax2.set_title('Convergence Speed', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Converged in {len(history)} iterations\")\n",
    "print(f\"Final max delta: {deltas[-1] if deltas else 0:.2e}\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive summary visualization\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Left: Optimal values with policy\n",
    "ax1 = fig.add_subplot(121)\n",
    "grid = np.full((env.rows, env.cols), np.nan)\n",
    "for (r, c), v in V_star.items():\n",
    "    grid[r, c] = v\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list('opt', ['#f7fbff', '#08519c'])\n",
    "ax1.imshow(grid, cmap=cmap)\n",
    "ax1.set_title('Optimal Policy (V* + arrows)', fontsize=14, fontweight='bold')\n",
    "\n",
    "arrow_dx = [0, 0.35, 0, -0.35]\n",
    "arrow_dy = [-0.35, 0, 0.35, 0]\n",
    "\n",
    "for (r, c) in env.get_all_states():\n",
    "    if (r, c) == env.goal:\n",
    "        ax1.text(c, r, 'GOAL', ha='center', va='center', fontsize=10,\n",
    "                fontweight='bold', color='gold',\n",
    "                bbox=dict(boxstyle='round', facecolor='green', alpha=0.8))\n",
    "    elif (r, c) in env.obstacles:\n",
    "        ax1.add_patch(plt.Rectangle((c-0.5, r-0.5), 1, 1, color='gray'))\n",
    "    else:\n",
    "        a = optimal_policy[(r, c)]\n",
    "        ax1.annotate('', xy=(c + arrow_dx[a], r + arrow_dy[a]),\n",
    "                     xytext=(c, r),\n",
    "                     arrowprops=dict(arrowstyle='->', color='white', lw=2.5))\n",
    "        ax1.text(c, r + 0.35, f'{V_star[(r,c)]:.1f}', ha='center', va='center',\n",
    "                fontsize=10, fontweight='bold', color='white')\n",
    "\n",
    "ax1.set_xticks(range(env.cols))\n",
    "ax1.set_yticks(range(env.rows))\n",
    "ax1.grid(True, linewidth=1, color='white', alpha=0.3)\n",
    "\n",
    "# Right: Q* heatmap for all actions\n",
    "ax2 = fig.add_subplot(122)\n",
    "q_grid = np.zeros((env.rows * 2, env.cols * 2))\n",
    "for (r, c), q_vals in Q_star.items():\n",
    "    for a, q in q_vals.items():\n",
    "        # Map actions to sub-cells\n",
    "        sub_r = r * 2 + (1 if a == 2 else 0 if a == 0 else 0 if a in [1,3] else 0)\n",
    "        sub_c = c * 2 + (1 if a == 1 else 0 if a == 3 else 0 if a in [0,2] else 0)\n",
    "        if a == 0: sub_r, sub_c = r*2, c*2  # Up -> top-left\n",
    "        elif a == 1: sub_r, sub_c = r*2, c*2+1  # Right -> top-right\n",
    "        elif a == 2: sub_r, sub_c = r*2+1, c*2  # Down -> bottom-left\n",
    "        elif a == 3: sub_r, sub_c = r*2+1, c*2+1  # Left -> bottom-right\n",
    "        q_grid[sub_r, sub_c] = q\n",
    "\n",
    "im = ax2.imshow(q_grid, cmap='RdYlBu', interpolation='nearest')\n",
    "ax2.set_title('Q* values (4 sub-cells per state)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax2, shrink=0.8)\n",
    "\n",
    "plt.suptitle('Bellman Optimality -- Complete Solution', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Congratulations! You have solved the Bellman equations from scratch!\")\n",
    "print(\"Next up: learning these values from experience with Q-Learning.\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "1. Value iteration converges because the Bellman operator is a contraction. What does \"contraction\" mean intuitively?\n",
    "2. Why does the Bellman optimality equation use max instead of sum (compared to the standard Bellman equation)?\n",
    "3. In our implementation, we needed the transition dynamics (env.step). What if we did not have access to them?\n",
    "\n",
    "### Optional Challenges\n",
    "1. Implement policy iteration (alternating policy evaluation and policy improvement) and compare convergence speed to value iteration.\n",
    "2. Add stochastic transitions (the agent slips 20% of the time) and re-run value iteration. How does the optimal policy change?\n",
    "3. Implement the Bellman equation for a continuous state space using function approximation (a simple neural network)."
   ],
   "id": "cell_23"
  }
 ]
}