{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Noise Conditioned Score Networks -- Architecture and Training -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Conditioned Score Networks -- Architecture and Training -- Vizuara\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebook, we saw that score estimation fails in low-density regions. The score function becomes unreliable precisely where we need it most -- in the empty spaces of high-dimensional data.\n",
    "\n",
    "Noise Conditioned Score Networks (NCSN) solve this with a brilliantly simple idea: **perturb the data with noise at multiple scales**, then train a single neural network to predict the score at every noise level.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand why multi-scale noise perturbation fills empty space\n",
    "- Implement the geometric noise schedule\n",
    "- Build a noise-conditioned score network from scratch\n",
    "- Train it on 2D data and visualize the learned score fields at each noise level\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "Think of it like fogging up a window. Behind the glass is a picture -- your data.\n",
    "\n",
    "- **Thin fog** (small noise): You can still see the shapes clearly. The data structure is preserved, but you can see the glass itself -- the score is defined on the glass surface too.\n",
    "- **Medium fog**: The shapes are blurred, but you can tell something is there. The score now covers more area.\n",
    "- **Thick fog** (large noise): Everything is covered. You cannot see the picture at all, but you know something is behind the glass. The score is now defined everywhere.\n",
    "\n",
    "Each fog thickness gives you different information. The thick fog tells you \"there is data somewhere over there\" (coarse, global structure). The thin fog tells you \"the data has this precise shape right here\" (fine, local structure).\n",
    "\n",
    "NCSN learns all these levels simultaneously in a single neural network.\n",
    "\n",
    "## 3. The Mathematics\n",
    "\n",
    "### 3.1 Noise Perturbation\n",
    "\n",
    "Given data distribution $p(x)$ and noise level $\\sigma$, the perturbed distribution is:\n",
    "\n",
    "$$q_\\sigma(\\tilde{x}) = \\int p(x) \\cdot \\mathcal{N}(\\tilde{x} \\mid x, \\sigma^2 I) \\, dx$$\n",
    "\n",
    "The score of this perturbed distribution is:\n",
    "\n",
    "$$\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}) = -\\frac{\\tilde{x} - x}{\\sigma^2} = -\\frac{\\epsilon}{\\sigma}$$\n",
    "\n",
    "where $\\epsilon \\sim \\mathcal{N}(0, I)$ is the noise that was added.\n",
    "\n",
    "**Worked example:** Data point $x = 3$, noise $\\epsilon = 0.4$, $\\sigma = 0.5$.\n",
    "- Noisy sample: $\\tilde{x} = 3 + 0.5 \\times 0.4 = 3.2$\n",
    "- Score target: $-\\epsilon / \\sigma = -0.4 / 0.5 = -0.8$\n",
    "- This says \"move left by 0.8\" -- pointing back toward the original data. This is exactly what we want.\n",
    "\n",
    "### 3.2 The Geometric Noise Schedule\n",
    "\n",
    "We use $L$ noise levels arranged in a geometric sequence:\n",
    "\n",
    "$$\\sigma_i = \\sigma_1 \\cdot \\left(\\frac{\\sigma_L}{\\sigma_1}\\right)^{\\frac{i-1}{L-1}}$$\n",
    "\n",
    "**Worked example:** $\\sigma_1 = 10$, $\\sigma_L = 0.01$, $L = 5$.\n",
    "\n",
    "$$\\sigma_1 = 10, \\quad \\sigma_2 = 10 \\cdot (0.001)^{0.25} = 10 \\cdot 0.1778 = 1.778$$\n",
    "$$\\sigma_3 = 10 \\cdot (0.001)^{0.5} = 10 \\cdot 0.03162 = 0.3162$$\n",
    "$$\\sigma_4 = 10 \\cdot (0.001)^{0.75} = 10 \\cdot 0.005623 = 0.05623, \\quad \\sigma_5 = 0.01$$\n",
    "\n",
    "### 3.3 The NCSN Training Objective\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = \\frac{1}{L} \\sum_{i=1}^{L} \\sigma_i^2 \\, \\mathbb{E}_{p(x)} \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)} \\left[ \\left\\| s_\\theta(\\tilde{x}, \\sigma_i) + \\frac{\\epsilon}{\\sigma_i} \\right\\|^2 \\right]$$\n",
    "\n",
    "The $\\sigma_i^2$ weighting ensures that losses at different scales are comparable.\n",
    "\n",
    "**Worked example:** Suppose $s_\\theta$ predicts $[-0.7]$ but the target is $[-0.8]$ at $\\sigma = 0.5$.\n",
    "- Unweighted loss: $(\u22120.7 \u2212 (\u22120.8))^2 = 0.01$\n",
    "- Weighted loss: $0.5^2 \\times 0.01 = 0.0025$\n",
    "\n",
    "At $\\sigma = 5.0$ with the same error:\n",
    "- Weighted loss: $5.0^2 \\times 0.01 = 0.25$\n",
    "\n",
    "The weighting makes errors at large $\\sigma$ contribute more, which helps learn the global structure first.\n",
    "\n",
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 Generate 2D Training Data"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_data(n=5000):\n",
    "    \"\"\"Generate a mixture of 2 Gaussians in 2D.\"\"\"\n",
    "    mix = torch.rand(n, 1)\n",
    "    centers = torch.tensor([[-3.0, 0.0], [3.0, 0.0]])\n",
    "    idx = (mix > 0.5).long().squeeze()\n",
    "    data = centers[idx] + 0.5 * torch.randn(n, 2)\n",
    "    return data\n",
    "\n",
    "data = generate_data(5000)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(data[:, 0].numpy(), data[:, 1].numpy(), s=2, alpha=0.3, c='blue')\n",
    "plt.title('Training Data: Mixture of 2 Gaussians')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "print(f\"Data shape: {data.shape}\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Define the Geometric Noise Schedule"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geometric_noise_schedule(sigma_1, sigma_L, L):\n",
    "    \"\"\"Create L noise levels in a geometric sequence.\"\"\"\n",
    "    sigmas = torch.tensor([\n",
    "        sigma_1 * (sigma_L / sigma_1) ** (i / (L - 1))\n",
    "        for i in range(L)\n",
    "    ])\n",
    "    return sigmas\n",
    "\n",
    "# Create our noise schedule\n",
    "L = 10\n",
    "sigma_1, sigma_L = 10.0, 0.01\n",
    "sigmas = geometric_noise_schedule(sigma_1, sigma_L, L)\n",
    "\n",
    "print(\"Noise levels:\")\n",
    "for i, s in enumerate(sigmas):\n",
    "    print(f\"  sigma_{i+1:2d} = {s:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(range(1, L+1), sigmas.numpy(), color='steelblue')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Noise Level Index')\n",
    "plt.ylabel('sigma (log scale)')\n",
    "plt.title('Geometric Noise Schedule')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visualize the Effect of Different Noise Levels"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "noise_levels_to_show = [0, 3, 6, 9]  # indices into sigmas\n",
    "\n",
    "for ax, idx in zip(axes, noise_levels_to_show):\n",
    "    sigma = sigmas[idx]\n",
    "    noisy_data = data + sigma * torch.randn_like(data)\n",
    "    ax.scatter(noisy_data[:, 0].numpy(), noisy_data[:, 1].numpy(),\n",
    "               s=1, alpha=0.2, c='blue')\n",
    "    ax.set_title(f'sigma = {sigma:.4f}')\n",
    "    ax.set_xlim(-15, 15)\n",
    "    ax.set_ylim(-10, 10)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Data Perturbed at Different Noise Levels', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: large sigma spreads data everywhere, small sigma preserves structure\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Build the Noise Conditioned Score Network"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreNet(nn.Module):\n",
    "    \"\"\"Noise Conditioned Score Network for 2D data.\"\"\"\n",
    "    def __init__(self, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        # Input: 2D data point + 1D noise level = 3\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(3, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, 2),  # Output: 2D score\n",
    "        )\n",
    "\n",
    "    def forward(self, x, sigma):\n",
    "        # x: (batch, 2), sigma: (batch,) or (batch, 1)\n",
    "        if sigma.dim() == 1:\n",
    "            sigma = sigma.unsqueeze(1)\n",
    "        net_input = torch.cat([x, sigma], dim=1)\n",
    "        return self.net(net_input)\n",
    "\n",
    "model = ScoreNet(hidden_dim=128)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model created with {total_params:,} parameters\")\n",
    "\n",
    "# Quick test\n",
    "test_x = torch.randn(4, 2)\n",
    "test_sigma = torch.tensor([1.0, 2.0, 0.5, 0.1])\n",
    "output = model(test_x, test_sigma)\n",
    "print(f\"Input shape: {test_x.shape}, Sigma shape: {test_sigma.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** The model should accept a batch of 2D points and their noise levels, and output 2D score vectors.\n",
    "\n",
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Implement the NCSN Training Loss"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ncsn_loss(model, data, sigmas):\n",
    "    \"\"\"\n",
    "    Compute the NCSN training loss.\n",
    "\n",
    "    Args:\n",
    "        model: ScoreNet that takes (x, sigma) and returns score\n",
    "        data: clean data tensor of shape (batch, 2)\n",
    "        sigmas: tensor of all noise levels, shape (L,)\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss value\n",
    "\n",
    "    Steps:\n",
    "        1. Sample random noise level indices for each data point\n",
    "        2. Get the corresponding sigma values\n",
    "        3. Sample noise epsilon ~ N(0, I)\n",
    "        4. Create noisy data: x_tilde = data + sigma * epsilon\n",
    "        5. Compute target: -epsilon / sigma\n",
    "        6. Get model prediction: model(x_tilde, sigma)\n",
    "        7. Compute weighted MSE loss: mean of sigma^2 * ||pred - target||^2\n",
    "    \"\"\"\n",
    "    batch_size = data.shape[0]\n",
    "    L = len(sigmas)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # idx = ...\n",
    "    # sigma = ...\n",
    "    # epsilon = ...\n",
    "    # noisy_data = ...\n",
    "    # target = ...\n",
    "    # pred = ...\n",
    "    # loss = ...\n",
    "    pass\n",
    "\n",
    "# Verify: loss should be a positive scalar\n",
    "# test_loss = ncsn_loss(model, data[:32], sigmas)\n",
    "# print(f\"Test loss: {test_loss.item():.4f}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement the Training Loop"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ncsn(model, data, sigmas, n_epochs=3000, lr=1e-3, batch_size=256):\n",
    "    \"\"\"\n",
    "    Train the NCSN model.\n",
    "\n",
    "    Args:\n",
    "        model: ScoreNet\n",
    "        data: training data\n",
    "        sigmas: noise levels\n",
    "        n_epochs: number of epochs\n",
    "        lr: learning rate\n",
    "        batch_size: mini-batch size\n",
    "\n",
    "    Returns:\n",
    "        List of loss values per epoch\n",
    "\n",
    "    Hints:\n",
    "        - Use Adam optimizer\n",
    "        - Sample random batches each epoch\n",
    "        - Call ncsn_loss for each batch\n",
    "        - Print loss every 500 epochs\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# losses = train_ncsn(model, data, sigmas)"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training (reference implementation)\n",
    "model = ScoreNet(hidden_dim=128)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "data = generate_data(5000)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(3000):\n",
    "    # Sample random noise level for each data point\n",
    "    idx = torch.randint(0, L, (data.shape[0],))\n",
    "    sigma = sigmas[idx].unsqueeze(1)  # (N, 1)\n",
    "\n",
    "    # Add noise\n",
    "    noise = torch.randn_like(data)\n",
    "    noisy_data = data + sigma * noise\n",
    "\n",
    "    # Target: -noise / sigma\n",
    "    target = -noise / sigma\n",
    "\n",
    "    # Predict score\n",
    "    pred = model(noisy_data, sigma.squeeze(1))\n",
    "\n",
    "    # Weighted loss: sigma^2 * ||pred - target||^2\n",
    "    loss = (sigma ** 2 * (pred - target) ** 2).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"Epoch {epoch+1:4d}, Loss: {loss.item():.4f}\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.semilogy(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (log scale)')\n",
    "plt.title('NCSN Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned score fields at different noise levels\n",
    "def plot_score_field(model, sigma_val, ax, grid_range=(-8, 8), n_grid=20):\n",
    "    \"\"\"Plot the learned score field at a specific noise level.\"\"\"\n",
    "    x = np.linspace(grid_range[0], grid_range[1], n_grid)\n",
    "    y = np.linspace(-5, 5, n_grid)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    points = torch.tensor(np.stack([X.ravel(), Y.ravel()], axis=1),\n",
    "                          dtype=torch.float32)\n",
    "    sigma_tensor = torch.full((points.shape[0],), sigma_val)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = model(points, sigma_tensor).numpy()\n",
    "\n",
    "    # Normalize arrow lengths for visibility\n",
    "    norms = np.sqrt(scores[:, 0]**2 + scores[:, 1]**2).reshape(n_grid, n_grid)\n",
    "    max_norm = np.percentile(norms, 95) + 1e-6\n",
    "\n",
    "    ax.quiver(X, Y,\n",
    "              scores[:, 0].reshape(n_grid, n_grid) / max_norm,\n",
    "              scores[:, 1].reshape(n_grid, n_grid) / max_norm,\n",
    "              color='steelblue', alpha=0.7)\n",
    "    ax.set_title(f'sigma = {sigma_val:.4f}', fontsize=12)\n",
    "    ax.set_xlim(grid_range)\n",
    "    ax.set_ylim(-5, 5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.2)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "sigma_indices = [0, 3, 6, 9]\n",
    "for ax, idx in zip(axes.ravel(), sigma_indices):\n",
    "    plot_score_field(model, sigmas[idx].item(), ax)\n",
    "    # Overlay true data\n",
    "    ax.scatter(data[:500, 0].numpy(), data[:500, 1].numpy(),\n",
    "               s=1, alpha=0.2, c='red')\n",
    "\n",
    "plt.suptitle('Learned Score Fields at Different Noise Levels', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Large sigma: single broad basin pulling everything inward\")\n",
    "print(\"Small sigma: two distinct basins at the cluster centers\")\n",
    "print(\"This is exactly what we want!\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: compare score reliability across noise levels\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Panel 1: Score field at large sigma -- covers everywhere\n",
    "plot_score_field(model, sigmas[0].item(), axes[0], n_grid=25)\n",
    "axes[0].set_title(f'Large Noise (sigma={sigmas[0]:.1f})\\nScore defined EVERYWHERE',\n",
    "                  fontsize=11, fontweight='bold')\n",
    "\n",
    "# Panel 2: Score field at medium sigma -- two basins emerging\n",
    "plot_score_field(model, sigmas[5].item(), axes[1], n_grid=25)\n",
    "axes[1].set_title(f'Medium Noise (sigma={sigmas[5]:.3f})\\nTwo basins emerging',\n",
    "                  fontsize=11, fontweight='bold')\n",
    "\n",
    "# Panel 3: Score field at small sigma -- precise clusters\n",
    "plot_score_field(model, sigmas[9].item(), axes[2], n_grid=25)\n",
    "axes[2].set_title(f'Small Noise (sigma={sigmas[9]:.4f})\\nPrecise cluster structure',\n",
    "                  fontsize=11, fontweight='bold')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.scatter(data[:300, 0].numpy(), data[:300, 1].numpy(), s=2, alpha=0.3, c='red')\n",
    "\n",
    "plt.suptitle('NCSN: One Network Learns Score at All Noise Levels', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe NCSN has learned to provide meaningful score estimates at EVERY noise level.\")\n",
    "print(\"At large sigma: global structure (where are the clusters roughly?)\")\n",
    "print(\"At small sigma: precise structure (exact cluster boundaries)\")\n",
    "print(\"The low-density problem from Notebook 1 is solved!\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**What we learned:**\n",
    "1. Adding noise at multiple scales fills the empty space, making score estimation reliable everywhere\n",
    "2. A geometric noise schedule spans from coarse to fine resolution\n",
    "3. The NCSN takes both data and noise level as input, outputting a noise-level-specific score\n",
    "4. The $\\sigma^2$ weighting ensures balanced learning across all scales\n",
    "\n",
    "**Reflection questions:**\n",
    "- Why is a geometric sequence better than a linear sequence for the noise levels?\n",
    "- What would happen if we used too few noise levels (e.g., L = 2)?\n",
    "- How does the choice of $\\sigma_1$ (largest noise) affect the generated samples?\n",
    "\n",
    "**Next notebook:** We will use the trained NCSN to actually generate new samples using Annealed Langevin Dynamics -- the \"coarse to fine\" sampling procedure."
   ],
   "id": "cell_20"
  }
 ]
}