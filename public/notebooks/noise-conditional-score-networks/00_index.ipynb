{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "NCSN Learning Path -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Conditioned Score Networks (NCSN) -- Learning Path\n",
    "\n",
    "## Welcome to the NCSN Notebook Series -- Vizuara\n",
    "\n",
    "This series of 4 interactive notebooks takes you from understanding why score estimation fails to building and sampling from Noise Conditioned Score Networks -- the breakthrough that bridged score-based models and modern diffusion models.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of probability distributions and gradients\n",
    "- Familiarity with PyTorch (nn.Module, tensors, optimizers)\n",
    "- The Vizuara articles on Score Matching and Denoising Score Matching are helpful but not required\n",
    "\n",
    "## Learning Path\n",
    "\n",
    "### Notebook 1: Score Estimation and the Low-Density Problem\n",
    "**File:** `01_score_estimation_low_density.ipynb`\n",
    "\n",
    "- What is the score function and why does it act like a compass?\n",
    "- The manifold hypothesis: why most of high-dimensional space is empty\n",
    "- Hands-on: implement score functions and see where estimation breaks down\n",
    "- Key takeaway: neural networks cannot learn scores where there is no data\n",
    "\n",
    "### Notebook 2: NCSN Architecture and Training\n",
    "**File:** `02_ncsn_architecture.ipynb`\n",
    "\n",
    "- The elegant solution: perturb data with noise at multiple scales\n",
    "- Geometric noise schedule: from coarse to fine resolution\n",
    "- Build a noise-conditioned score network in PyTorch\n",
    "- Train it on 2D data and visualize score fields at each noise level\n",
    "- Key takeaway: one network learns reliable scores at every noise level\n",
    "\n",
    "### Notebook 3: Annealed Langevin Dynamics\n",
    "**File:** `03_annealed_langevin_dynamics.ipynb`\n",
    "\n",
    "- Standard Langevin dynamics and why it gets stuck\n",
    "- Annealed Langevin Dynamics: sampling from coarse to fine\n",
    "- Adaptive step sizes for each noise level\n",
    "- Generate hundreds of samples and evaluate quality\n",
    "- Key takeaway: the sculpting analogy -- chisel first, sandpaper last\n",
    "\n",
    "### Notebook 4: From NCSN to Diffusion Models\n",
    "**File:** `04_ncsn_to_diffusion_models.ipynb`\n",
    "\n",
    "- NCSN vs DDPM: score prediction vs noise prediction\n",
    "- Mathematical proof of equivalence\n",
    "- Train both models and verify they learn the same function\n",
    "- The Score SDE unification (Song et al., 2021)\n",
    "- Key takeaway: all modern diffusion models are score-based at their core\n",
    "\n",
    "## How to Use These Notebooks\n",
    "\n",
    "1. Open each notebook in Google Colab (GPU runtime recommended)\n",
    "2. Run the setup cell first\n",
    "3. Read the markdown explanations before running code cells\n",
    "4. Complete the TODO sections -- they are essential for deep understanding\n",
    "5. Check your work against the visualization checkpoints\n",
    "\n",
    "## References\n",
    "\n",
    "- Song & Ermon, \"Generative Modeling by Estimating Gradients of the Data Distribution\" (NeurIPS 2019)\n",
    "- Ho et al., \"Denoising Diffusion Probabilistic Models\" (NeurIPS 2020)\n",
    "- Song et al., \"Score-Based Generative Modeling through SDEs\" (ICLR 2021)"
   ],
   "id": "cell_1"
  }
 ]
}