{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Multimodal Projection: Bridging Vision and Language -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Projection: Bridging Vision and Language from First Principles\n",
    "\n",
    "*Part 1 of the Vizuara series on Multimodal Instruction Tuning*\n",
    "*Estimated time: 45 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Large language models are extraordinarily capable at understanding text -- but they are completely blind. They cannot process a single pixel. Yet the world is inherently visual: medical scans, satellite imagery, architectural blueprints, everyday photographs.\n",
    "\n",
    "What if we could teach a language model to \"see\" by simply projecting visual features into its token space? That is exactly what we will build in this notebook.\n",
    "\n",
    "**By the end of this notebook, you will have:**\n",
    "- Built a multimodal projection layer from scratch\n",
    "- Understood how vision encoder features map into LLM embedding space\n",
    "- Visualized how images become \"visual tokens\" that a language model can process\n",
    "- Implemented a complete forward pass combining image and text"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and installations\n",
    "!pip install -q torch torchvision transformers matplotlib numpy Pillow\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Let us start with a concrete analogy.\n",
    "\n",
    "Imagine two brilliant diplomats at the United Nations -- one speaks only Mandarin and the other speaks only French. Both are experts in their respective domains. Both understand complex negotiations. But they cannot communicate with each other directly.\n",
    "\n",
    "What do you need? An **interpreter** -- someone who takes the meaning expressed in one language and converts it into the other language.\n",
    "\n",
    "In our case:\n",
    "- The **Mandarin-speaking diplomat** is the vision encoder (CLIP ViT) -- it \"speaks\" in 1024-dimensional visual feature vectors\n",
    "- The **French-speaking diplomat** is the language model (LLaMA/GPT-2) -- it \"speaks\" in 768 or 4096-dimensional token embeddings\n",
    "- The **interpreter** is a simple learned projection layer -- an MLP that maps from one space to the other\n",
    "\n",
    "The key insight is stunning in its simplicity: once you project visual features into the LLM's embedding space, the language model cannot even tell the difference between a visual token and a text token. It just sees a sequence of embeddings and processes them normally.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "If you had a 1024-dimensional vector representing the concept of \"a red car on a highway,\" and you needed to express that same concept in a 768-dimensional space, what mathematical operation would you use? What information might be lost, and what might be preserved?"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "The projection from vision space to language space is a learned linear transformation (or a small MLP).\n",
    "\n",
    "### The Simple Linear Projection\n",
    "\n",
    "Given visual features $Z_v \\in \\mathbb{R}^{N \\times d_v}$ from a vision encoder (where $N$ is the number of image patches and $d_v$ is the vision feature dimension), we project them into the LLM's embedding space of dimension $d_l$:\n",
    "\n",
    "$$H_v = W \\cdot Z_v + b$$\n",
    "\n",
    "where $W \\in \\mathbb{R}^{d_l \\times d_v}$ and $b \\in \\mathbb{R}^{d_l}$.\n",
    "\n",
    "**Computationally, this says:** take each visual patch vector (a 1024-dimensional vector capturing what that patch \"sees\"), multiply it by a learned weight matrix, and add a bias. The output is a vector in the LLM's embedding space.\n",
    "\n",
    "Let us plug in concrete numbers. Suppose $d_v = 3$ (vision) and $d_l = 2$ (language). One patch feature is $z = [0.5, 0.8, 0.3]$.\n",
    "\n",
    "With $W = \\begin{bmatrix} 0.2 & 0.4 & 0.1 \\\\ 0.3 & 0.1 & 0.5 \\end{bmatrix}$ and $b = [0.1, 0.1]$:\n",
    "\n",
    "$$h = W \\cdot z + b = \\begin{bmatrix} 0.2 \\times 0.5 + 0.4 \\times 0.8 + 0.1 \\times 0.3 + 0.1 \\\\ 0.3 \\times 0.5 + 0.1 \\times 0.8 + 0.5 \\times 0.3 + 0.1 \\end{bmatrix} = \\begin{bmatrix} 0.55 \\\\ 0.48 \\end{bmatrix}$$\n",
    "\n",
    "This tells us that our 3D visual feature has been mapped to a 2D LLM-compatible token. This is exactly what we want.\n",
    "\n",
    "### The Two-Layer MLP Projection (LLaVA-1.5)\n",
    "\n",
    "LLaVA-1.5 uses a more expressive projection -- a two-layer MLP with GELU activation:\n",
    "\n",
    "$$H_v = W_2 \\cdot \\text{GELU}(W_1 \\cdot Z_v + b_1) + b_2$$\n",
    "\n",
    "The GELU non-linearity allows the projection to learn more complex mappings than a simple linear transformation. This is important because the relationship between vision features and language tokens is not purely linear."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us verify the linear projection numerically\n",
    "z = torch.tensor([0.5, 0.8, 0.3])\n",
    "W = torch.tensor([[0.2, 0.4, 0.1],\n",
    "                   [0.3, 0.1, 0.5]])\n",
    "b = torch.tensor([0.1, 0.1])\n",
    "\n",
    "h = W @ z + b\n",
    "print(f\"Visual feature z = {z.tolist()}\")\n",
    "print(f\"Projected token h = {h.tolist()}\")\n",
    "print(f\"\\nDimension reduction: {len(z)}D -> {len(h)}D\")\n",
    "print(f\"To the LLM, h = {h.tolist()} looks just like any other token embedding!\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us visualize what projection does to a batch of visual features\n",
    "# Simulating 16 patch features in 3D, projected to 2D\n",
    "\n",
    "torch.manual_seed(42)\n",
    "num_patches = 16\n",
    "d_v, d_l = 3, 2\n",
    "\n",
    "# Simulated visual features (clustered around different concepts)\n",
    "z_sky = torch.randn(4, d_v) * 0.3 + torch.tensor([1.0, 0.5, -0.5])    # \"sky\" patches\n",
    "z_car = torch.randn(4, d_v) * 0.3 + torch.tensor([-0.5, 1.0, 0.8])    # \"car\" patches\n",
    "z_road = torch.randn(4, d_v) * 0.3 + torch.tensor([0.0, -0.5, 1.0])   # \"road\" patches\n",
    "z_tree = torch.randn(4, d_v) * 0.3 + torch.tensor([0.8, 0.8, 0.8])    # \"tree\" patches\n",
    "Z = torch.cat([z_sky, z_car, z_road, z_tree], dim=0)\n",
    "\n",
    "# Learnable projection\n",
    "projector = nn.Linear(d_v, d_l)\n",
    "with torch.no_grad():\n",
    "    H = projector(Z)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Original 3D features (shown as 2D slice)\n",
    "colors = ['skyblue']*4 + ['red']*4 + ['gray']*4 + ['green']*4\n",
    "labels = ['sky']*4 + ['car']*4 + ['road']*4 + ['tree']*4\n",
    "\n",
    "ax = axes[0]\n",
    "for i, (z, c, l) in enumerate(zip(Z, colors, labels)):\n",
    "    ax.scatter(z[0].item(), z[1].item(), c=c, s=100, edgecolors='black', linewidths=0.5)\n",
    "    if i % 4 == 0:\n",
    "        ax.scatter([], [], c=c, s=100, label=l, edgecolors='black', linewidths=0.5)\n",
    "ax.set_title(\"Vision Encoder Space (d_v = 3, shown as 2D slice)\", fontsize=12)\n",
    "ax.set_xlabel(\"Feature dim 1\")\n",
    "ax.set_ylabel(\"Feature dim 2\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Projected features in LLM space\n",
    "ax = axes[1]\n",
    "for i, (h, c, l) in enumerate(zip(H, colors, labels)):\n",
    "    ax.scatter(h[0].item(), h[1].item(), c=c, s=100, edgecolors='black', linewidths=0.5)\n",
    "    if i % 4 == 0:\n",
    "        ax.scatter([], [], c=c, s=100, label=l, edgecolors='black', linewidths=0.5)\n",
    "ax.set_title(\"LLM Embedding Space (d_l = 2, after projection)\", fontsize=12)\n",
    "ax.set_xlabel(\"LLM dim 1\")\n",
    "ax.set_ylabel(\"LLM dim 2\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Projection: Vision Features -> LLM Token Space\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 The Simple Linear Projector\n",
    "\n",
    "Let us start with the simplest possible projection -- a single linear layer."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProjector(nn.Module):\n",
    "    \"\"\"Simple linear projection from vision to language space.\"\"\"\n",
    "\n",
    "    def __init__(self, vision_dim: int, llm_dim: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(vision_dim, llm_dim)\n",
    "\n",
    "    def forward(self, visual_features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            visual_features: (batch, num_patches, vision_dim)\n",
    "        Returns:\n",
    "            visual_tokens: (batch, num_patches, llm_dim)\n",
    "        \"\"\"\n",
    "        return self.proj(visual_features)\n",
    "\n",
    "\n",
    "# Test it\n",
    "projector_linear = LinearProjector(vision_dim=1024, llm_dim=768)\n",
    "dummy_features = torch.randn(2, 196, 1024)  # 2 images, 196 patches each\n",
    "visual_tokens = projector_linear(dummy_features)\n",
    "print(f\"Input:  {dummy_features.shape}  (batch=2, patches=196, vision_dim=1024)\")\n",
    "print(f\"Output: {visual_tokens.shape}  (batch=2, patches=196, llm_dim=768)\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in projector_linear.parameters()):,}\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Two-Layer MLP Projector (LLaVA-1.5 Style)\n",
    "\n",
    "Now let us build the more powerful two-layer MLP projection that LLaVA-1.5 uses."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPProjector(nn.Module):\n",
    "    \"\"\"Two-layer MLP projection (LLaVA-1.5 style) with GELU activation.\"\"\"\n",
    "\n",
    "    def __init__(self, vision_dim: int, llm_dim: int, hidden_dim: int = None):\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = llm_dim  # Default: same as output dim\n",
    "\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(vision_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, llm_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, visual_features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.projector(visual_features)\n",
    "\n",
    "\n",
    "# Test it\n",
    "projector_mlp = MLPProjector(vision_dim=1024, llm_dim=768, hidden_dim=768)\n",
    "visual_tokens_mlp = projector_mlp(dummy_features)\n",
    "print(f\"Input:  {dummy_features.shape}\")\n",
    "print(f\"Output: {visual_tokens_mlp.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in projector_mlp.parameters()):,}\")\n",
    "print(f\"\\nCompare: Linear has {sum(p.numel() for p in projector_linear.parameters()):,} params\")\n",
    "print(f\"         MLP has    {sum(p.numel() for p in projector_mlp.parameters()):,} params\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Compare linear vs MLP projection\n",
    "# The MLP can learn non-linear mappings\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Create visual features with a non-linear pattern\n",
    "t = torch.linspace(0, 2 * np.pi, 100)\n",
    "Z_circle = torch.stack([torch.cos(t), torch.sin(t), 0.5 * torch.cos(2*t)], dim=1)\n",
    "\n",
    "# Apply both projections\n",
    "with torch.no_grad():\n",
    "    linear_out = LinearProjector(3, 2)(Z_circle)\n",
    "    mlp_out = MLPProjector(3, 2, hidden_dim=16)(Z_circle)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, 100))\n",
    "\n",
    "axes[0].scatter(Z_circle[:, 0], Z_circle[:, 1], c=colors, s=10)\n",
    "axes[0].set_title(\"Original Features (3D, shown as 2D)\", fontsize=11)\n",
    "axes[0].set_xlabel(\"Dim 1\"); axes[0].set_ylabel(\"Dim 2\")\n",
    "\n",
    "axes[1].scatter(linear_out[:, 0], linear_out[:, 1], c=colors, s=10)\n",
    "axes[1].set_title(\"After Linear Projection\", fontsize=11)\n",
    "axes[1].set_xlabel(\"LLM Dim 1\"); axes[1].set_ylabel(\"LLM Dim 2\")\n",
    "\n",
    "axes[2].scatter(mlp_out[:, 0], mlp_out[:, 1], c=colors, s=10)\n",
    "axes[2].set_title(\"After MLP Projection (random init)\", fontsize=11)\n",
    "axes[2].set_xlabel(\"LLM Dim 1\"); axes[2].set_ylabel(\"LLM Dim 2\")\n",
    "\n",
    "plt.suptitle(\"Linear vs MLP Projection: MLP can capture non-linear relationships\",\n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Simulating a Vision Encoder\n",
    "\n",
    "In a full LLaVA system, the vision encoder is a pretrained CLIP ViT. Let us simulate one to understand the patch extraction process."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVisionEncoder(nn.Module):\n",
    "    \"\"\"Simulated vision encoder that extracts patch features from images.\n",
    "\n",
    "    This mimics what CLIP ViT does:\n",
    "    1. Split image into non-overlapping patches\n",
    "    2. Project each patch to a feature vector\n",
    "    3. Process with transformer layers (simplified here)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size: int = 224, patch_size: int = 16, embed_dim: int = 1024):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2  # 196 for 224/16\n",
    "\n",
    "        # Patch embedding: flatten each patch and project\n",
    "        self.patch_embed = nn.Conv2d(\n",
    "            3, embed_dim,\n",
    "            kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "        # Simple transformer layer (simplified)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: (batch, 3, H, W) pixel values\n",
    "        Returns:\n",
    "            patch_features: (batch, num_patches, embed_dim)\n",
    "        \"\"\"\n",
    "        # Extract patches\n",
    "        x = self.patch_embed(images)        # (B, embed_dim, H/P, W/P)\n",
    "        x = x.flatten(2).transpose(1, 2)    # (B, num_patches, embed_dim)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test it\n",
    "encoder = SimpleVisionEncoder(image_size=224, patch_size=16, embed_dim=1024)\n",
    "dummy_image = torch.randn(1, 3, 224, 224)\n",
    "patch_features = encoder(dummy_image)\n",
    "print(f\"Image shape:    {dummy_image.shape}\")\n",
    "print(f\"Patch features: {patch_features.shape}\")\n",
    "print(f\"Each image is split into {encoder.num_patches} patches of {encoder.patch_size}x{encoder.patch_size} pixels\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how an image gets split into patches\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Create a simple gradient image for visualization\n",
    "img = np.zeros((224, 224, 3))\n",
    "for i in range(224):\n",
    "    for j in range(224):\n",
    "        img[i, j, 0] = i / 224   # Red gradient top-to-bottom\n",
    "        img[i, j, 2] = j / 224   # Blue gradient left-to-right\n",
    "\n",
    "# Show original image\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title(\"Original Image (224x224)\", fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Show with patch grid overlay\n",
    "axes[1].imshow(img)\n",
    "patch_size = 16\n",
    "for i in range(0, 224, patch_size):\n",
    "    axes[1].axhline(y=i, color='white', linewidth=0.5, alpha=0.8)\n",
    "    axes[1].axvline(x=i, color='white', linewidth=0.5, alpha=0.8)\n",
    "axes[1].set_title(f\"Split into {(224//patch_size)**2} patches ({patch_size}x{patch_size})\", fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Add patch numbers for a few patches\n",
    "for row in range(3):\n",
    "    for col in range(3):\n",
    "        patch_idx = row * (224 // patch_size) + col\n",
    "        axes[1].text(col * patch_size + patch_size//2, row * patch_size + patch_size//2,\n",
    "                    str(patch_idx), color='white', fontsize=7, ha='center', va='center',\n",
    "                    fontweight='bold', bbox=dict(boxstyle='round,pad=0.1', facecolor='black', alpha=0.5))\n",
    "\n",
    "plt.suptitle(\"Image -> Patches -> Features\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO: Implement the Sequence Concatenation\n",
    "\n",
    "The key step in LLaVA is concatenating visual tokens with text tokens into a single sequence. Implement this function."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_multimodal_sequence(\n",
    "    visual_tokens: torch.Tensor,\n",
    "    text_embeddings: torch.Tensor,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Concatenate visual and text tokens into a single sequence for the LLM.\n",
    "\n",
    "    The resulting sequence should be: [visual_token_1, ..., visual_token_N, text_token_1, ..., text_token_M]\n",
    "\n",
    "    Args:\n",
    "        visual_tokens:   (batch, N, dim) projected visual tokens\n",
    "        text_embeddings: (batch, M, dim) text token embeddings\n",
    "\n",
    "    Returns:\n",
    "        combined: (batch, N+M, dim) the concatenated sequence\n",
    "        attention_mask: (batch, N+M) all ones (both modalities should be attended to)\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Concatenate visual_tokens and text_embeddings along the sequence dimension (dim=1)\n",
    "    # Step 2: Create an attention mask of all ones with shape (batch, N+M)\n",
    "    # ==============================\n",
    "\n",
    "    combined = ???  # YOUR CODE HERE\n",
    "    attention_mask = ???  # YOUR CODE HERE\n",
    "\n",
    "    return combined, attention_mask"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "batch_size, N, M, dim = 2, 196, 10, 768\n",
    "vis = torch.randn(batch_size, N, dim)\n",
    "txt = torch.randn(batch_size, M, dim)\n",
    "\n",
    "combined, mask = concatenate_multimodal_sequence(vis, txt)\n",
    "assert combined.shape == (batch_size, N + M, dim), f\"Expected shape {(batch_size, N+M, dim)}, got {combined.shape}\"\n",
    "assert mask.shape == (batch_size, N + M), f\"Expected mask shape {(batch_size, N+M)}, got {mask.shape}\"\n",
    "assert torch.all(mask == 1), \"Attention mask should be all ones\"\n",
    "assert torch.allclose(combined[:, :N, :], vis), \"First N tokens should be visual\"\n",
    "assert torch.allclose(combined[:, N:, :], txt), \"Last M tokens should be text\"\n",
    "print(\"All assertions passed!\")\n",
    "print(f\"Combined sequence: {N} visual + {M} text = {N+M} total tokens\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implement the Full Multimodal Forward Pass\n",
    "\n",
    "Now implement the complete forward pass that takes an image and text and produces the combined representation."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_forward(\n",
    "    image: torch.Tensor,\n",
    "    text_token_ids: torch.Tensor,\n",
    "    vision_encoder: nn.Module,\n",
    "    projector: nn.Module,\n",
    "    text_embedding_layer: nn.Embedding,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Complete multimodal forward pass.\n",
    "\n",
    "    Pipeline:\n",
    "    1. image -> vision_encoder -> patch_features\n",
    "    2. patch_features -> projector -> visual_tokens\n",
    "    3. text_token_ids -> text_embedding_layer -> text_embeddings\n",
    "    4. Concatenate [visual_tokens, text_embeddings]\n",
    "\n",
    "    Args:\n",
    "        image: (batch, 3, H, W) input image\n",
    "        text_token_ids: (batch, M) text token indices\n",
    "        vision_encoder: produces (batch, N, d_v) features\n",
    "        projector: maps (batch, N, d_v) -> (batch, N, d_l)\n",
    "        text_embedding_layer: maps token ids to embeddings\n",
    "\n",
    "    Returns:\n",
    "        combined_sequence: (batch, N+M, d_l) ready for the LLM\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Extract visual features using vision_encoder\n",
    "    # Step 2: Project visual features using projector\n",
    "    # Step 3: Get text embeddings using text_embedding_layer\n",
    "    # Step 4: Concatenate visual tokens and text embeddings\n",
    "    # ==============================\n",
    "\n",
    "    combined_sequence = ???  # YOUR CODE HERE\n",
    "\n",
    "    return combined_sequence"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "vision_enc = SimpleVisionEncoder(image_size=224, patch_size=16, embed_dim=1024)\n",
    "proj = MLPProjector(vision_dim=1024, llm_dim=768)\n",
    "text_emb = nn.Embedding(1000, 768)\n",
    "\n",
    "test_img = torch.randn(1, 3, 224, 224)\n",
    "test_ids = torch.randint(0, 1000, (1, 5))\n",
    "\n",
    "result = multimodal_forward(test_img, test_ids, vision_enc, proj, text_emb)\n",
    "expected_seq_len = 196 + 5  # 196 patches + 5 text tokens\n",
    "assert result.shape == (1, expected_seq_len, 768), f\"Expected (1, {expected_seq_len}, 768), got {result.shape}\"\n",
    "print(f\"Multimodal sequence shape: {result.shape}\")\n",
    "print(f\"  - {196} visual tokens from 224x224 image with 16x16 patches\")\n",
    "print(f\"  - {5} text tokens\")\n",
    "print(f\"  - Total: {expected_seq_len} tokens in the LLM's 768-dim space\")\n",
    "print(\"All assertions passed!\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us combine all components into a complete multimodal model class."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniMultimodalModel(nn.Module):\n",
    "    \"\"\"A complete (simplified) LLaVA-style multimodal model.\n",
    "\n",
    "    Components:\n",
    "    1. Vision encoder (simulated CLIP ViT)\n",
    "    2. MLP projection layer\n",
    "    3. Text embedding layer (simulated LLM embedding)\n",
    "    4. Simple transformer decoder (simulated LLM)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        vision_dim: int = 512,\n",
    "        llm_dim: int = 256,\n",
    "        vocab_size: int = 1000,\n",
    "        num_heads: int = 4,\n",
    "        num_layers: int = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = SimpleVisionEncoder(image_size, patch_size, vision_dim)\n",
    "        self.projector = MLPProjector(vision_dim, llm_dim, hidden_dim=llm_dim)\n",
    "        self.text_embedding = nn.Embedding(vocab_size, llm_dim)\n",
    "        self.position_embedding = nn.Embedding(1024, llm_dim)\n",
    "\n",
    "        # Simple transformer decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=llm_dim, nhead=num_heads, dim_feedforward=llm_dim * 4,\n",
    "            batch_first=True, dropout=0.1\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.output_head = nn.Linear(llm_dim, vocab_size)\n",
    "\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "    def forward(self, images: torch.Tensor, text_ids: torch.Tensor) -> torch.Tensor:\n",
    "        # 1. Get visual tokens\n",
    "        visual_features = self.vision_encoder(images)          # (B, N, vision_dim)\n",
    "        visual_tokens = self.projector(visual_features)         # (B, N, llm_dim)\n",
    "\n",
    "        # 2. Get text embeddings\n",
    "        text_tokens = self.text_embedding(text_ids)             # (B, M, llm_dim)\n",
    "\n",
    "        # 3. Concatenate\n",
    "        combined = torch.cat([visual_tokens, text_tokens], dim=1)  # (B, N+M, llm_dim)\n",
    "\n",
    "        # 4. Add positional embeddings\n",
    "        seq_len = combined.shape[1]\n",
    "        positions = torch.arange(seq_len, device=combined.device).unsqueeze(0)\n",
    "        combined = combined + self.position_embedding(positions)\n",
    "\n",
    "        # 5. Create causal mask for autoregressive generation\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(combined.device)\n",
    "\n",
    "        # 6. Decoder forward pass (self-attention only, no cross-attention)\n",
    "        memory = torch.zeros_like(combined[:, :1, :])  # dummy memory\n",
    "        output = self.decoder(combined, memory, tgt_mask=causal_mask)\n",
    "\n",
    "        # 7. Project to vocabulary\n",
    "        logits = self.output_head(output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Create and inspect the model\n",
    "model = MiniMultimodalModel()\n",
    "print(\"Model architecture:\")\n",
    "print(f\"  Vision encoder patches: {model.num_patches}\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "proj_params = sum(p.numel() for p in model.projector.parameters())\n",
    "print(f\"  Total parameters:      {total_params:,}\")\n",
    "print(f\"  Projector parameters:  {proj_params:,} ({proj_params/total_params*100:.1f}%)\")\n",
    "print(f\"\\n  The projector is only {proj_params/total_params*100:.1f}% of the model -- yet it is the entire bridge!\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a forward pass\n",
    "images = torch.randn(2, 3, 224, 224)\n",
    "text_ids = torch.randint(0, 1000, (2, 10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(images, text_ids)\n",
    "\n",
    "print(f\"\\nForward pass:\")\n",
    "print(f\"  Images:  {images.shape}\")\n",
    "print(f\"  Text:    {text_ids.shape}\")\n",
    "print(f\"  Output:  {logits.shape}\")\n",
    "print(f\"  -> {model.num_patches} visual + {text_ids.shape[1]} text = {model.num_patches + text_ids.shape[1]} sequence positions\")\n",
    "print(f\"  -> Each position has {logits.shape[-1]} vocabulary logits\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "Let us train our mini-multimodal model on a simple image-captioning task to verify the projection works."
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic dataset: images of colored squares with captions\n",
    "# \"red\", \"blue\", \"green\" encoded as token IDs 1, 2, 3\n",
    "# Target: given a colored image, predict the color word\n",
    "\n",
    "def create_colored_image(color_idx, size=224):\n",
    "    \"\"\"Create a simple solid-color image.\"\"\"\n",
    "    img = torch.zeros(3, size, size)\n",
    "    if color_idx == 0:    # Red\n",
    "        img[0] = 1.0\n",
    "    elif color_idx == 1:  # Blue\n",
    "        img[2] = 1.0\n",
    "    elif color_idx == 2:  # Green\n",
    "        img[1] = 1.0\n",
    "    return img + torch.randn(3, size, size) * 0.1  # Add noise\n",
    "\n",
    "def create_dataset(n_samples=300):\n",
    "    images, labels = [], []\n",
    "    for _ in range(n_samples):\n",
    "        color = np.random.randint(0, 3)\n",
    "        images.append(create_colored_image(color))\n",
    "        labels.append(color + 1)  # Token IDs: 1=red, 2=blue, 3=green\n",
    "    return torch.stack(images), torch.tensor(labels)\n",
    "\n",
    "train_images, train_labels = create_dataset(300)\n",
    "test_images, test_labels = create_dataset(60)\n",
    "\n",
    "print(f\"Training set: {train_images.shape[0]} images\")\n",
    "print(f\"Test set:     {test_images.shape[0]} images\")\n",
    "print(f\"Task: Given a colored image, predict the color (red=1, blue=2, green=3)\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "model = MiniMultimodalModel(\n",
    "    vision_dim=128, llm_dim=64, vocab_size=10,\n",
    "    num_heads=2, num_layers=1\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training: feed image + start token (0), predict color token\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    perm = torch.randperm(len(train_images))\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(0, len(train_images), 32):\n",
    "        batch_idx = perm[i:i+32]\n",
    "        imgs = train_images[batch_idx].to(device)\n",
    "        labels = train_labels[batch_idx].to(device)\n",
    "\n",
    "        # Input: image + start token (ID=0)\n",
    "        text_input = torch.zeros(len(batch_idx), 1, dtype=torch.long, device=device)\n",
    "\n",
    "        logits = model(imgs, text_input)\n",
    "\n",
    "        # We want the model to predict the color at the last position\n",
    "        last_logits = logits[:, -1, :]  # (batch, vocab_size)\n",
    "        loss = criterion(last_logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * len(batch_idx)\n",
    "        preds = last_logits.argmax(dim=-1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += len(batch_idx)\n",
    "\n",
    "    avg_loss = epoch_loss / total\n",
    "    accuracy = correct / total\n",
    "    losses.append(avg_loss)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | Loss: {avg_loss:.4f} | Accuracy: {accuracy:.1%}\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(losses, color='steelblue', linewidth=2)\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Training Loss\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(accuracies, color='seagreen', linewidth=2)\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].set_title(\"Training Accuracy\")\n",
    "axes[1].axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].set_ylim(0, 1.05)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Multimodal Model Training: Learning to Map Colors to Words\",\n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on held-out images\n",
    "model.eval()\n",
    "color_names = {1: \"Red\", 2: \"Blue\", 3: \"Green\"}\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        img = test_images[i].to(device)\n",
    "        label = test_labels[i].item()\n",
    "        text_input = torch.zeros(1, 1, dtype=torch.long, device=device)\n",
    "\n",
    "        logits = model(img.unsqueeze(0), text_input)\n",
    "        pred = logits[0, -1, :].argmax().item()\n",
    "\n",
    "        # Display\n",
    "        display_img = test_images[i].permute(1, 2, 0).clamp(0, 1).numpy()\n",
    "        axes[i].imshow(display_img)\n",
    "        correct = pred == label\n",
    "        axes[i].set_title(\n",
    "            f\"Pred: {color_names.get(pred, '?')} | True: {color_names[label]}\",\n",
    "            color='green' if correct else 'red',\n",
    "            fontsize=9, fontweight='bold'\n",
    "        )\n",
    "        axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Multimodal Model Predictions: Image -> Color Word\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Overall test accuracy\n",
    "with torch.no_grad():\n",
    "    text_input = torch.zeros(len(test_images), 1, dtype=torch.long, device=device)\n",
    "    logits = model(test_images.to(device), text_input)\n",
    "    preds = logits[:, -1, :].argmax(dim=-1)\n",
    "    test_acc = (preds == test_labels.to(device)).float().mean()\n",
    "    print(f\"\\nTest accuracy: {test_acc:.1%}\")\n",
    "\n",
    "print(\"\\nCongratulations! You have built a multimodal model from scratch!\")\n",
    "print(\"The projection layer successfully bridges vision features into the language model's space.\")"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "\n",
    "1. Why does a simple two-layer MLP work as the projection layer, rather than requiring a more complex module like a cross-attention network?\n",
    "\n",
    "2. If you doubled the number of image patches (by using smaller patch size), how would this affect: (a) the sequence length, (b) the computational cost of self-attention, and (c) the detail captured from the image?\n",
    "\n",
    "3. In our training experiment, the vision encoder and projection layer were both trained from scratch. In real LLaVA, the vision encoder is frozen. Why is freezing the vision encoder a good idea?\n",
    "\n",
    "### Optional Challenges\n",
    "\n",
    "1. **Variable resolution:** Modify the `SimpleVisionEncoder` to accept images of different sizes and produce different numbers of patches. How would you handle the position embeddings?\n",
    "\n",
    "2. **Cross-attention projector:** Replace the MLP projector with a cross-attention module (like InstructBLIP's Q-Former). Use a fixed set of 32 learnable query tokens that cross-attend to the visual features. Compare the number of visual tokens sent to the LLM.\n",
    "\n",
    "3. **Multiple images:** Extend the model to accept two images and answer questions comparing them. How do you organize the visual tokens?"
   ],
   "id": "cell_31"
  }
 ]
}