{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Cross-Modal Attention -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Modal Attention: How Language Models Learn to See\n",
    "\n",
    "*Part 3 of the Vizuara series on Multimodal Instruction Tuning*\n",
    "*Estimated time: 40 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "We have built the projection layer and the training pipeline. But here is a question that might be nagging you: how does the language model actually \"look at\" the image when generating text?\n",
    "\n",
    "The answer is beautiful in its simplicity: **self-attention does everything**. When the LLM processes the concatenated sequence [visual_tokens, text_tokens], the standard self-attention mechanism naturally allows text tokens to attend to visual tokens. No special cross-attention module is needed.\n",
    "\n",
    "**By the end of this notebook, you will have:**\n",
    "- Visualized attention patterns in a multimodal model\n",
    "- Understood the block structure of cross-modal attention\n",
    "- Built and interpreted attention heatmaps\n",
    "- Seen how different text tokens attend to different image patches"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "!pip install -q torch torchvision matplotlib numpy seaborn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Imagine you are reading a detective novel with illustrations. As you read the sentence \"The suspect wore a **red hat**,\" your eyes automatically glance at the illustration to find the red hat. When you read \"standing near the **doorway**,\" your eyes shift to the doorway in the image.\n",
    "\n",
    "This is exactly what cross-modal attention does. When the language model generates the word \"red,\" the text token can attend to (look at) the visual tokens corresponding to the red region of the image. When it generates \"car,\" it attends to the car patches.\n",
    "\n",
    "The remarkable thing is that this behavior **emerges naturally** from standard self-attention. We do not need to design a special mechanism for it.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "In a sequence of 196 visual tokens followed by 10 text tokens, the attention matrix is 206 x 206. How many of those attention weights represent \"text attending to image\" (cross-modal attention)? What fraction of the total attention matrix is this?"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Self-Attention on the Combined Sequence\n",
    "\n",
    "Given the combined sequence $X = [h_1, ..., h_N, t_1, ..., t_M]$ where $h_i$ are visual tokens and $t_j$ are text tokens, self-attention computes:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "**Computationally, this says:** compute a similarity score between every pair of tokens. Normalize these scores with softmax. Use the normalized scores to create a weighted sum of value vectors. This happens for every token in the sequence.\n",
    "\n",
    "### The Block Structure\n",
    "\n",
    "The attention matrix naturally decomposes into four blocks:\n",
    "\n",
    "| | Visual Keys | Text Keys |\n",
    "|---|---|---|\n",
    "| **Visual Queries** | V-V: Spatial relationships | V-T: Text grounding |\n",
    "| **Text Queries** | T-V: Visual reasoning | T-T: Language modeling |\n",
    "\n",
    "The **T-V block** is where \"seeing\" happens: text tokens attend to visual tokens.\n",
    "\n",
    "Let us compute a numerical example. Suppose we have 2 visual tokens and 2 text tokens, all in dimension $d_k = 4$.\n",
    "\n",
    "$$Q = \\begin{bmatrix} q_{v1} \\\\ q_{v2} \\\\ q_{t1} \\\\ q_{t2} \\end{bmatrix}, \\quad K = \\begin{bmatrix} k_{v1} \\\\ k_{v2} \\\\ k_{t1} \\\\ k_{t2} \\end{bmatrix}$$\n",
    "\n",
    "The raw attention scores are $\\frac{QK^\\top}{\\sqrt{d_k}}$. Say we get:\n",
    "\n",
    "$$\\text{scores} = \\begin{bmatrix} 1.2 & 0.8 & 0.3 & 0.1 \\\\ 0.7 & 1.5 & 0.2 & 0.4 \\\\ 0.9 & 1.8 & 2.1 & 0.5 \\\\ 0.3 & 0.4 & 0.8 & 1.9 \\end{bmatrix}$$\n",
    "\n",
    "After softmax on each row, the third row (text token 1) becomes approximately:\n",
    "\n",
    "$$[0.13, 0.33, 0.44, 0.09]$$\n",
    "\n",
    "This tells us that text token 1 gives 13% attention to visual patch 1, 33% attention to visual patch 2, 44% to itself, and 9% to text token 2. The cross-modal attention (0.13 + 0.33 = 0.46) shows the text token is \"looking at\" the image for almost half its attention. This is exactly what we want."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical verification of cross-modal attention\n",
    "d_k = 4\n",
    "scores = torch.tensor([\n",
    "    [1.2, 0.8, 0.3, 0.1],   # visual token 1\n",
    "    [0.7, 1.5, 0.2, 0.4],   # visual token 2\n",
    "    [0.9, 1.8, 2.1, 0.5],   # text token 1\n",
    "    [0.3, 0.4, 0.8, 1.9],   # text token 2\n",
    "])\n",
    "\n",
    "# Scale by sqrt(d_k)\n",
    "scaled_scores = scores / np.sqrt(d_k)\n",
    "\n",
    "# Softmax\n",
    "attn_weights = F.softmax(scaled_scores, dim=-1)\n",
    "\n",
    "print(\"Attention weights (each row sums to 1):\")\n",
    "labels = [\"vis_1\", \"vis_2\", \"txt_1\", \"txt_2\"]\n",
    "for i, label in enumerate(labels):\n",
    "    weights = [f\"{w:.3f}\" for w in attn_weights[i].tolist()]\n",
    "    print(f\"  {label} attends to: {dict(zip(labels, weights))}\")\n",
    "\n",
    "# Cross-modal attention for text token 1\n",
    "cross_modal = attn_weights[2, :2].sum().item()\n",
    "print(f\"\\nText token 1 cross-modal attention: {cross_modal:.1%} (looking at the image)\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 Multi-Head Self-Attention with Visualization"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualizableAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention that stores attention weights for visualization.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.attention_weights = None  # Stored for visualization\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None) -> torch.Tensor:\n",
    "        B, S, D = x.shape\n",
    "\n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_q(x).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        self.attention_weights = attn_weights.detach()  # Store for visualization\n",
    "\n",
    "        # Weighted sum\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        output = output.transpose(1, 2).contiguous().view(B, S, D)\n",
    "        return self.W_o(output)\n",
    "\n",
    "\n",
    "# Test\n",
    "attn = VisualizableAttention(d_model=64, num_heads=4)\n",
    "x = torch.randn(1, 10, 64)  # 10 tokens\n",
    "output = attn(x)\n",
    "print(f\"Input:  {x.shape}\")\n",
    "print(f\"Output: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn.attention_weights.shape}\")\n",
    "print(f\"  -> {attn.num_heads} heads, each with {x.shape[1]}x{x.shape[1]} attention matrix\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 A Simple Multimodal Model for Attention Analysis"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionAnalysisModel(nn.Module):\n",
    "    \"\"\"Simplified model designed for attention visualization.\"\"\"\n",
    "\n",
    "    def __init__(self, image_size=64, patch_size=8, vision_dim=64,\n",
    "                 llm_dim=64, vocab_size=20, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.num_patches = (image_size // patch_size) ** 2  # 64\n",
    "\n",
    "        # Vision\n",
    "        self.patch_embed = nn.Conv2d(3, vision_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(vision_dim, llm_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(llm_dim, llm_dim),\n",
    "        )\n",
    "\n",
    "        # Text\n",
    "        self.text_embed = nn.Embedding(vocab_size, llm_dim)\n",
    "        self.pos_embed = nn.Embedding(256, llm_dim)\n",
    "\n",
    "        # Self-attention (with visualization support)\n",
    "        self.attention = VisualizableAttention(llm_dim, num_heads)\n",
    "        self.norm = nn.LayerNorm(llm_dim)\n",
    "        self.output_head = nn.Linear(llm_dim, vocab_size)\n",
    "\n",
    "    def forward(self, images, token_ids):\n",
    "        # Vision path\n",
    "        vis = self.patch_embed(images).flatten(2).transpose(1, 2)\n",
    "        vis_tokens = self.projector(vis)\n",
    "\n",
    "        # Text path\n",
    "        text_tokens = self.text_embed(token_ids)\n",
    "\n",
    "        # Combine\n",
    "        combined = torch.cat([vis_tokens, text_tokens], dim=1)\n",
    "        positions = torch.arange(combined.shape[1], device=combined.device).unsqueeze(0)\n",
    "        combined = combined + self.pos_embed(positions)\n",
    "\n",
    "        # Self-attention\n",
    "        attended = self.attention(combined)\n",
    "        output = self.norm(attended + combined)\n",
    "\n",
    "        return self.output_head(output)\n",
    "\n",
    "\n",
    "model_attn = AttentionAnalysisModel().to(device)\n",
    "print(f\"Model created with {model_attn.num_patches} visual patches\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test image with distinct regions\n",
    "def create_test_image(size=64):\n",
    "    \"\"\"Create an image with a red circle in the top-left and blue square in the bottom-right.\"\"\"\n",
    "    img = torch.ones(3, size, size) * 0.9  # Light gray background\n",
    "\n",
    "    # Red circle in top-left\n",
    "    y, x = torch.meshgrid(torch.arange(size), torch.arange(size), indexing='ij')\n",
    "    circle_mask = ((x - size//4)**2 + (y - size//4)**2) < (size//8)**2\n",
    "    img[0][circle_mask] = 1.0\n",
    "    img[1][circle_mask] = 0.1\n",
    "    img[2][circle_mask] = 0.1\n",
    "\n",
    "    # Blue square in bottom-right\n",
    "    sq_mask = (abs(x - 3*size//4) < size//8) & (abs(y - 3*size//4) < size//8)\n",
    "    img[0][sq_mask] = 0.1\n",
    "    img[1][sq_mask] = 0.1\n",
    "    img[2][sq_mask] = 1.0\n",
    "\n",
    "    return img\n",
    "\n",
    "test_img = create_test_image().unsqueeze(0).to(device)\n",
    "test_tokens = torch.tensor([[1, 3, 4, 9]], device=device)  # <start> what color ?\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model_attn(test_img, test_tokens)\n",
    "\n",
    "print(f\"Forward pass complete. Attention weights stored.\")\n",
    "print(f\"Attention shape: {model_attn.attention.attention_weights.shape}\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the full attention matrix with block structure\n",
    "attn_weights = model_attn.attention.attention_weights[0, 0].cpu().numpy()  # Head 0\n",
    "\n",
    "num_vis = model_attn.num_patches  # 64\n",
    "num_txt = test_tokens.shape[1]     # 4\n",
    "total = num_vis + num_txt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Full attention matrix\n",
    "im = axes[0].imshow(attn_weights, cmap='Blues', aspect='auto')\n",
    "axes[0].set_title(\"Full Attention Matrix (Head 0)\", fontsize=12)\n",
    "axes[0].set_xlabel(\"Key position\")\n",
    "axes[0].set_ylabel(\"Query position\")\n",
    "\n",
    "# Draw block boundaries\n",
    "axes[0].axhline(y=num_vis-0.5, color='red', linewidth=2, linestyle='--')\n",
    "axes[0].axvline(x=num_vis-0.5, color='red', linewidth=2, linestyle='--')\n",
    "\n",
    "# Label blocks\n",
    "axes[0].text(num_vis//2, num_vis//2, \"V-V\\nSpatial\", ha='center', va='center',\n",
    "            fontsize=10, color='white', fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='steelblue', alpha=0.8))\n",
    "axes[0].text(num_vis + num_txt//2, num_vis//2, \"V-T\\nGrounding\", ha='center', va='center',\n",
    "            fontsize=10, color='white', fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='green', alpha=0.8))\n",
    "axes[0].text(num_vis//2, num_vis + num_txt//2, \"T-V\\nSeeing\", ha='center', va='center',\n",
    "            fontsize=10, color='white', fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='orange', alpha=0.8))\n",
    "axes[0].text(num_vis + num_txt//2, num_vis + num_txt//2, \"T-T\\nLanguage\", ha='center', va='center',\n",
    "            fontsize=10, color='white', fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='purple', alpha=0.8))\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Zoom into T-V block (text attending to visual)\n",
    "tv_block = attn_weights[num_vis:, :num_vis]\n",
    "token_labels = [\"<start>\", \"what\", \"color\", \"?\"]\n",
    "im2 = axes[1].imshow(tv_block, cmap='Oranges', aspect='auto')\n",
    "axes[1].set_title(\"T-V Block: Text Tokens Looking at Image\", fontsize=12)\n",
    "axes[1].set_xlabel(\"Visual patch index\")\n",
    "axes[1].set_ylabel(\"Text token\")\n",
    "axes[1].set_yticks(range(num_txt))\n",
    "axes[1].set_yticklabels(token_labels)\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.suptitle(\"Cross-Modal Attention: How Text Tokens 'See' the Image\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Spatial Attention Heatmap"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the T-V attention to the spatial grid of the image\n",
    "def visualize_spatial_attention(attention_weights, num_patches_side, image, token_labels,\n",
    "                               text_start_idx, num_text_tokens):\n",
    "    \"\"\"Overlay attention weights on the image to show which patches each text token attends to.\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_text_tokens + 1, figsize=(4 * (num_text_tokens + 1), 4))\n",
    "\n",
    "    # Show original image\n",
    "    display_img = image[0].permute(1, 2, 0).cpu().numpy()\n",
    "    axes[0].imshow(display_img)\n",
    "    axes[0].set_title(\"Original Image\", fontsize=11)\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # For each text token, show its attention over image patches\n",
    "    for t in range(num_text_tokens):\n",
    "        # Get attention weights from this text token to all visual patches\n",
    "        text_pos = text_start_idx + t\n",
    "        vis_attention = attention_weights[0, 0, text_pos, :text_start_idx].cpu().numpy()\n",
    "\n",
    "        # Reshape to spatial grid\n",
    "        attn_map = vis_attention.reshape(num_patches_side, num_patches_side)\n",
    "        # Upsample to image size\n",
    "        attn_map_up = np.kron(attn_map, np.ones((image.shape[2]//num_patches_side,\n",
    "                                                    image.shape[3]//num_patches_side)))\n",
    "\n",
    "        axes[t+1].imshow(display_img)\n",
    "        axes[t+1].imshow(attn_map_up, cmap='hot', alpha=0.6)\n",
    "        axes[t+1].set_title(f'\"{token_labels[t]}\"', fontsize=11, fontweight='bold')\n",
    "        axes[t+1].axis('off')\n",
    "\n",
    "    plt.suptitle(\"Where Each Text Token Looks in the Image\",\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize\n",
    "num_patches_side = int(np.sqrt(model_attn.num_patches))\n",
    "visualize_spatial_attention(\n",
    "    model_attn.attention.attention_weights,\n",
    "    num_patches_side,\n",
    "    test_img,\n",
    "    [\"<start>\", \"what\", \"color\", \"?\"],\n",
    "    text_start_idx=model_attn.num_patches,\n",
    "    num_text_tokens=4\n",
    ")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO: Compute Cross-Modal Attention Statistics\n",
    "\n",
    "Implement a function that analyzes the attention patterns and computes key statistics about cross-modal attention."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cross_modal_attention(\n",
    "    attention_weights: torch.Tensor,\n",
    "    num_visual_tokens: int,\n",
    "    num_text_tokens: int,\n",
    ") -> dict:\n",
    "    \"\"\"Analyze cross-modal attention patterns.\n",
    "\n",
    "    Args:\n",
    "        attention_weights: (batch, num_heads, seq_len, seq_len) attention matrix\n",
    "        num_visual_tokens: number of visual tokens (N)\n",
    "        num_text_tokens: number of text tokens (M)\n",
    "\n",
    "    Returns:\n",
    "        dict with keys:\n",
    "        - \"tv_mean\": float, mean attention from text to visual tokens\n",
    "        - \"vt_mean\": float, mean attention from visual to text tokens\n",
    "        - \"vv_mean\": float, mean attention from visual to visual tokens\n",
    "        - \"tt_mean\": float, mean attention from text to text tokens\n",
    "        - \"most_attended_patch\": int, the visual patch index most attended by text tokens\n",
    "        - \"cross_modal_ratio\": float, fraction of text attention going to visual tokens\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Average across batch and heads: (seq_len, seq_len)\n",
    "    # Step 2: Extract the 4 blocks: V-V, V-T, T-V, T-T\n",
    "    # Step 3: Compute mean attention for each block\n",
    "    # Step 4: Find the most attended visual patch (from text perspective)\n",
    "    # Step 5: Compute cross-modal ratio for text tokens\n",
    "    # ==============================\n",
    "\n",
    "    result = {}  # YOUR CODE HERE\n",
    "\n",
    "    return result"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "with torch.no_grad():\n",
    "    _ = model_attn(test_img, test_tokens)\n",
    "\n",
    "stats = analyze_cross_modal_attention(\n",
    "    model_attn.attention.attention_weights,\n",
    "    num_visual_tokens=model_attn.num_patches,\n",
    "    num_text_tokens=test_tokens.shape[1]\n",
    ")\n",
    "\n",
    "assert \"tv_mean\" in stats, \"Missing 'tv_mean' key\"\n",
    "assert \"cross_modal_ratio\" in stats, \"Missing 'cross_modal_ratio' key\"\n",
    "assert 0 <= stats[\"cross_modal_ratio\"] <= 1, f\"Cross-modal ratio should be in [0,1], got {stats['cross_modal_ratio']}\"\n",
    "assert 0 <= stats[\"most_attended_patch\"] < model_attn.num_patches, \"Invalid patch index\"\n",
    "\n",
    "print(\"Cross-Modal Attention Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "print(\"\\nAll assertions passed!\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Multi-Head Attention Comparison\n",
    "\n",
    "Implement a visualization that shows how different attention heads specialize in different aspects."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_attention_heads(\n",
    "    attention_weights: torch.Tensor,\n",
    "    num_visual_tokens: int,\n",
    "    num_patches_side: int,\n",
    "    image: torch.Tensor,\n",
    "    text_token_idx: int,\n",
    "    token_name: str,\n",
    "):\n",
    "    \"\"\"Visualize how different attention heads attend to image patches for a given text token.\n",
    "\n",
    "    Args:\n",
    "        attention_weights: (1, num_heads, seq_len, seq_len)\n",
    "        num_visual_tokens: number of visual patches\n",
    "        num_patches_side: grid size (e.g., 8 for 64 patches)\n",
    "        image: (1, 3, H, W) the input image\n",
    "        text_token_idx: which text token to visualize (0-indexed from text start)\n",
    "        token_name: name of the token for the title\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Get number of heads from attention_weights shape\n",
    "    # Step 2: For each head, extract the attention from the text token to visual patches\n",
    "    #         Position in the full sequence = num_visual_tokens + text_token_idx\n",
    "    # Step 3: Reshape each head's attention to (num_patches_side, num_patches_side)\n",
    "    # Step 4: Create a subplot grid with the image + one heatmap per head\n",
    "    # Step 5: Overlay each head's attention on the image using imshow with alpha\n",
    "    # ==============================\n",
    "\n",
    "    pass  # YOUR CODE HERE"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification (visual check)\n",
    "compare_attention_heads(\n",
    "    model_attn.attention.attention_weights,\n",
    "    num_visual_tokens=model_attn.num_patches,\n",
    "    num_patches_side=int(np.sqrt(model_attn.num_patches)),\n",
    "    image=test_img,\n",
    "    text_token_idx=2,  # \"color\" token\n",
    "    token_name=\"color\"\n",
    ")\n",
    "print(\"Check the visualization above -- each head should show different attention patterns!\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare attention patterns across all heads for all text tokens\n",
    "num_heads = model_attn.attention.num_heads\n",
    "num_txt = test_tokens.shape[1]\n",
    "token_labels = [\"<start>\", \"what\", \"color\", \"?\"]\n",
    "\n",
    "fig, axes = plt.subplots(num_txt, num_heads + 1, figsize=(3 * (num_heads + 1), 3 * num_txt))\n",
    "\n",
    "display_img = test_img[0].permute(1, 2, 0).cpu().numpy()\n",
    "num_ps = int(np.sqrt(model_attn.num_patches))\n",
    "\n",
    "for t in range(num_txt):\n",
    "    # Show image in first column\n",
    "    axes[t, 0].imshow(display_img)\n",
    "    axes[t, 0].set_title(f'\"{token_labels[t]}\"', fontsize=10, fontweight='bold')\n",
    "    axes[t, 0].axis('off')\n",
    "\n",
    "    for h in range(num_heads):\n",
    "        text_pos = model_attn.num_patches + t\n",
    "        vis_attn = model_attn.attention.attention_weights[0, h, text_pos, :model_attn.num_patches].cpu().numpy()\n",
    "        attn_map = vis_attn.reshape(num_ps, num_ps)\n",
    "        attn_up = np.kron(attn_map, np.ones((test_img.shape[2]//num_ps, test_img.shape[3]//num_ps)))\n",
    "\n",
    "        axes[t, h+1].imshow(display_img)\n",
    "        axes[t, h+1].imshow(attn_up, cmap='hot', alpha=0.6)\n",
    "        axes[t, h+1].set_title(f'Head {h}', fontsize=9)\n",
    "        axes[t, h+1].axis('off')\n",
    "\n",
    "plt.suptitle(\"All Text Tokens x All Attention Heads: Spatial Attention Heatmaps\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each row shows how one text token looks at the image across different heads.\")\n",
    "print(\"Different heads learn to attend to different regions -- this is the power of multi-head attention!\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the attention model on our VQA task and observe how attention patterns change\n",
    "\n",
    "# Quick training on colored shapes\n",
    "def create_shape_data(n=200, size=64):\n",
    "    data = []\n",
    "    for _ in range(n):\n",
    "        color = np.random.randint(0, 3)\n",
    "        img = torch.zeros(3, size, size)\n",
    "        y, x = torch.meshgrid(torch.arange(size), torch.arange(size), indexing='ij')\n",
    "        mask = ((x - size//2)**2 + (y - size//2)**2) < (size//4)**2\n",
    "        img[color][mask] = 1.0\n",
    "        img += torch.randn_like(img) * 0.05\n",
    "        data.append((img.clamp(0,1), color + 10))  # 10=red, 11=blue, 12=green\n",
    "    return data\n",
    "\n",
    "train_data = create_shape_data(300)\n",
    "model_attn2 = AttentionAnalysisModel().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_attn2.parameters(), lr=1e-3)\n",
    "losses = []\n",
    "\n",
    "for epoch in range(40):\n",
    "    random.shuffle(train_data)\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, len(train_data), 32):\n",
    "        batch = train_data[i:i+32]\n",
    "        imgs = torch.stack([b[0] for b in batch]).to(device)\n",
    "        labels = torch.tensor([b[1] for b in batch], device=device)\n",
    "\n",
    "        input_ids = torch.ones(len(batch), 1, dtype=torch.long, device=device)  # <start>\n",
    "        logits = model_attn2(imgs, input_ids)\n",
    "        loss = F.cross_entropy(logits[:, -1, :], labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    losses.append(epoch_loss / (len(train_data) // 32))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses, color='steelblue', linewidth=2)\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss: Attention Model Learning Color Classification\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compare attention before and after training\n",
    "# Create a red circle image\n",
    "red_img = torch.zeros(1, 3, 64, 64)\n",
    "y, x = torch.meshgrid(torch.arange(64), torch.arange(64), indexing='ij')\n",
    "circle_mask = ((x - 32)**2 + (y - 32)**2) < 12**2\n",
    "red_img[0, 0][circle_mask] = 1.0\n",
    "red_img = red_img.to(device)\n",
    "\n",
    "# Trained model attention\n",
    "with torch.no_grad():\n",
    "    _ = model_attn2(red_img, torch.ones(1, 1, dtype=torch.long, device=device))\n",
    "\n",
    "trained_attn = model_attn2.attention.attention_weights.cpu()\n",
    "\n",
    "# Untrained model attention\n",
    "untrained_model = AttentionAnalysisModel().to(device)\n",
    "with torch.no_grad():\n",
    "    _ = untrained_model(red_img, torch.ones(1, 1, dtype=torch.long, device=device))\n",
    "\n",
    "untrained_attn = untrained_model.attention.attention_weights.cpu()\n",
    "\n",
    "# Compare\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "display_img = red_img[0].permute(1, 2, 0).cpu().numpy()\n",
    "num_ps = int(np.sqrt(model_attn2.num_patches))\n",
    "\n",
    "axes[0].imshow(display_img)\n",
    "axes[0].set_title(\"Input: Red Circle\", fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Untrained\n",
    "vis_attn = untrained_attn[0, 0, -1, :model_attn2.num_patches].numpy()\n",
    "attn_map = vis_attn.reshape(num_ps, num_ps)\n",
    "attn_up = np.kron(attn_map, np.ones((64//num_ps, 64//num_ps)))\n",
    "axes[1].imshow(display_img)\n",
    "axes[1].imshow(attn_up, cmap='hot', alpha=0.6)\n",
    "axes[1].set_title(\"Before Training: Uniform Attention\", fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Trained\n",
    "vis_attn = trained_attn[0, 0, -1, :model_attn2.num_patches].numpy()\n",
    "attn_map = vis_attn.reshape(num_ps, num_ps)\n",
    "attn_up = np.kron(attn_map, np.ones((64//num_ps, 64//num_ps)))\n",
    "axes[2].imshow(display_img)\n",
    "axes[2].imshow(attn_up, cmap='hot', alpha=0.6)\n",
    "axes[2].set_title(\"After Training: Focused Attention\", fontsize=12)\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle(\"Training Teaches the Model WHERE to Look in the Image\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization: Attention flow in a multimodal model\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Create a rich test scene\n",
    "scene_img = torch.ones(1, 3, 64, 64) * 0.85\n",
    "# Red object top-left\n",
    "mask1 = ((x - 16)**2 + (y - 16)**2) < 8**2\n",
    "scene_img[0, 0][mask1] = 1.0; scene_img[0, 1][mask1] = 0.1; scene_img[0, 2][mask1] = 0.1\n",
    "# Blue object bottom-right\n",
    "mask2 = (abs(x - 48) < 8) & (abs(y - 48) < 8)\n",
    "scene_img[0, 0][mask2] = 0.1; scene_img[0, 1][mask2] = 0.1; scene_img[0, 2][mask2] = 1.0\n",
    "# Green object center\n",
    "mask3 = ((x - 32)**2 + (y - 32)**2) < 6**2\n",
    "scene_img[0, 0][mask3] = 0.1; scene_img[0, 1][mask3] = 0.8; scene_img[0, 2][mask3] = 0.1\n",
    "scene_img = scene_img.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model_attn2(scene_img, torch.ones(1, 1, dtype=torch.long, device=device))\n",
    "\n",
    "final_attn = model_attn2.attention.attention_weights.cpu()\n",
    "display_scene = scene_img[0].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "# Show all 4 heads for this scene\n",
    "gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax0.imshow(display_scene)\n",
    "ax0.set_title(\"Input Scene\", fontsize=12, fontweight='bold')\n",
    "ax0.axis('off')\n",
    "\n",
    "for h in range(4):\n",
    "    row, col = divmod(h, 2)\n",
    "    if h < 2:\n",
    "        ax = fig.add_subplot(gs[0, h+1])\n",
    "    else:\n",
    "        ax = fig.add_subplot(gs[1, h-2])\n",
    "\n",
    "    vis_attn = final_attn[0, h, -1, :model_attn2.num_patches].numpy()\n",
    "    attn_map = vis_attn.reshape(num_ps, num_ps)\n",
    "    attn_up = np.kron(attn_map, np.ones((64//num_ps, 64//num_ps)))\n",
    "\n",
    "    ax.imshow(display_scene)\n",
    "    ax.imshow(attn_up, cmap='hot', alpha=0.6)\n",
    "    ax.set_title(f\"Attention Head {h}\", fontsize=11)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Add summary text\n",
    "ax_text = fig.add_subplot(gs[1, 2])\n",
    "ax_text.axis('off')\n",
    "summary = (\n",
    "    \"Key Takeaways:\\n\\n\"\n",
    "    \"1. Different heads attend to\\n   different image regions\\n\\n\"\n",
    "    \"2. Cross-modal attention\\n   emerges from standard\\n   self-attention\\n\\n\"\n",
    "    \"3. No special architecture\\n   needed -- just projection\\n   into shared space\"\n",
    ")\n",
    "ax_text.text(0.1, 0.9, summary, fontsize=10, verticalalignment='top',\n",
    "            fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "\n",
    "plt.suptitle(\"Cross-Modal Attention: How a Language Model Learns to See\",\n",
    "             fontsize=15, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"Congratulations! You have visualized and understood cross-modal attention!\")\n",
    "print(\"The key insight: self-attention on the combined [visual, text] sequence\")\n",
    "print(\"naturally enables the LLM to 'see' relevant parts of the image.\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "\n",
    "1. In the attention heatmaps, different heads often attend to different spatial regions. Why is multi-head attention particularly important for multimodal models compared to single-head attention?\n",
    "\n",
    "2. The LLaVA architecture uses 576 visual tokens for a 336x336 image. Each visual token can attend to every other token. What is the computational cost (in terms of Big-O) of self-attention on a sequence with 576 visual + 100 text tokens? How does this compare to a text-only model with 100 tokens?\n",
    "\n",
    "3. Some models (like InstructBLIP) compress 576 visual tokens down to 32 \"query tokens\" before feeding them to the LLM. What are the trade-offs of this compression in terms of (a) computational cost, (b) detail preservation, and (c) attention pattern flexibility?\n",
    "\n",
    "### Optional Challenges\n",
    "\n",
    "1. **Attention rollout:** Instead of looking at a single attention layer, implement attention rollout -- multiply attention matrices across layers to see how information flows from image patches to the final output token.\n",
    "\n",
    "2. **Head pruning:** After training, set the attention weights of one head to uniform and measure the accuracy drop. Which heads are most important for the task?\n",
    "\n",
    "3. **Positional attention bias:** Analyze whether the model develops positional biases -- do patches near the center of the image get more attention than edge patches? Plot average attention vs. patch position."
   ],
   "id": "cell_28"
  }
 ]
}