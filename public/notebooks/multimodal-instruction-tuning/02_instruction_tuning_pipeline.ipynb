{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Instruction Tuning Pipeline -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Tuning Pipeline: Teaching a Multimodal Model to Follow Instructions\n",
    "\n",
    "*Part 2 of the Vizuara series on Multimodal Instruction Tuning*\n",
    "*Estimated time: 50 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "A model that can project images into language space is interesting, but not yet useful. It can \"see\" the image but it cannot answer questions about it, describe it in detail, or reason about its contents.\n",
    "\n",
    "The magic of instruction tuning is that it teaches the model to **follow instructions about images** -- \"What color is the car?\", \"Describe this scene in detail\", \"What is unusual about this image?\". This transforms a simple image-to-text mapper into an interactive visual assistant.\n",
    "\n",
    "**By the end of this notebook, you will have:**\n",
    "- Implemented the two-stage training procedure (alignment + instruction tuning)\n",
    "- Built and trained a model that can answer simple visual questions\n",
    "- Visualized how the loss changes across training stages\n",
    "- Understood why data quality matters more than architectural complexity"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "!pip install -q torch torchvision matplotlib numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Think about how a human interpreter is trained:\n",
    "\n",
    "**Stage 1 -- Basic Vocabulary:**\n",
    "The interpreter first learns to translate simple phrases. \"Apple\" in Mandarin maps to \"apple\" in French. \"Red car\" maps to \"voiture rouge.\" This is pure vocabulary alignment -- mapping concepts from one representation to another.\n",
    "\n",
    "**Stage 2 -- Complex Conversations:**\n",
    "Once the basic vocabulary is aligned, the interpreter practices translating complex negotiations, nuanced arguments, and multi-step reasoning. This requires not just word-for-word translation but understanding context, intent, and inference.\n",
    "\n",
    "LLaVA's two-stage training follows exactly this pattern:\n",
    "\n",
    "- **Stage 1 (Feature Alignment):** Only the projection layer trains. The model learns that \"these visual features\" correspond to \"these caption words.\" Simple mapping.\n",
    "- **Stage 2 (Instruction Tuning):** The projection layer AND the LLM both train. The model learns to follow complex instructions about images -- answering questions, providing descriptions, reasoning about visual content.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "Why do we freeze the LLM during Stage 1 but unfreeze it during Stage 2? What would happen if we unfroze everything from the start?"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### The Training Objective\n",
    "\n",
    "Both stages use the same fundamental objective: **autoregressive next-token prediction**. Given visual tokens $H_v$ and a sequence of text tokens $x_1, x_2, ..., x_T$, we minimize the negative log-likelihood:\n",
    "\n",
    "$$\\mathcal{L} = -\\sum_{t=1}^{T} \\log p_\\theta(x_t \\mid x_{<t}, H_v)$$\n",
    "\n",
    "**Computationally, this says:** for each text token in the sequence, compute the probability of that token given all previous tokens and the visual context. Take the negative log of that probability. Sum over all positions. We want this to be small, meaning the model assigns high probability to the correct next token.\n",
    "\n",
    "Let us walk through a numerical example. Suppose our caption is \"a red car\" (3 tokens), and the model predicts:\n",
    "\n",
    "- $p(\\text{\"a\"} \\mid H_v) = 0.7$ --> $-\\log(0.7) = 0.357$\n",
    "- $p(\\text{\"red\"} \\mid H_v, \\text{\"a\"}) = 0.4$ --> $-\\log(0.4) = 0.916$\n",
    "- $p(\\text{\"car\"} \\mid H_v, \\text{\"a\"}, \\text{\"red\"}) = 0.6$ --> $-\\log(0.6) = 0.511$\n",
    "\n",
    "Total loss: $\\mathcal{L} = 0.357 + 0.916 + 0.511 = 1.784$\n",
    "\n",
    "This tells us the model is fairly confident about \"a\" and \"car\" but less sure about \"red.\" Training will push the model to assign higher probability to \"red\" by adjusting the projection and LLM weights.\n",
    "\n",
    "### What Changes Between Stages\n",
    "\n",
    "In Stage 1, only projector parameters $W_1, b_1, W_2, b_2$ receive gradients. The loss still flows through the LLM, but only the projector weights update.\n",
    "\n",
    "In Stage 2, both the projector and LLM parameters update. This means the LLM itself adapts to the visual inputs, learning deeper multimodal reasoning patterns."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical demonstration of the loss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simulated model outputs (logits) for a 3-token caption\n",
    "# Vocabulary: {0: \"PAD\", 1: \"a\", 2: \"red\", 3: \"car\", 4: \"blue\", 5: \"the\"}\n",
    "logits = torch.tensor([\n",
    "    [2.0, 3.5, 0.5, 0.3, 0.2, 1.0],   # Position 1: should predict \"a\" (idx 1)\n",
    "    [0.5, 0.3, 2.8, 0.8, 0.5, 0.1],   # Position 2: should predict \"red\" (idx 2)\n",
    "    [0.1, 0.2, 0.5, 3.2, 0.3, 0.4],   # Position 3: should predict \"car\" (idx 3)\n",
    "])\n",
    "targets = torch.tensor([1, 2, 3])  # \"a\", \"red\", \"car\"\n",
    "\n",
    "# Compute probabilities\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "print(\"Token probabilities:\")\n",
    "for i, (t, p) in enumerate(zip([\"a\", \"red\", \"car\"], probs)):\n",
    "    token_prob = p[targets[i]].item()\n",
    "    print(f\"  p('{t}') = {token_prob:.3f}  -->  -log(p) = {-np.log(token_prob):.3f}\")\n",
    "\n",
    "# Compute loss\n",
    "loss = F.cross_entropy(logits, targets)\n",
    "print(f\"\\nTotal loss (cross-entropy): {loss.item():.4f}\")\n",
    "manual_loss = -torch.log(probs[range(3), targets]).mean()\n",
    "print(f\"Manual calculation:         {manual_loss.item():.4f}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 Synthetic Visual Question Answering Dataset\n",
    "\n",
    "For training, we need image-text pairs. Let us create a simple but illustrative dataset: images of colored shapes with questions and answers."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualQADataset:\n",
    "    \"\"\"Synthetic VQA dataset with colored shapes.\n",
    "\n",
    "    Image types:\n",
    "    - Colored circles/squares in different positions\n",
    "    Questions:\n",
    "    - \"What color?\" -> \"red\" / \"blue\" / \"green\"\n",
    "    - \"What shape?\" -> \"circle\" / \"square\"\n",
    "    - \"Where is it?\" -> \"top\" / \"bottom\" / \"left\" / \"right\" / \"center\"\n",
    "    \"\"\"\n",
    "\n",
    "    COLORS = {0: \"red\", 1: \"blue\", 2: \"green\"}\n",
    "    SHAPES = {0: \"circle\", 1: \"square\"}\n",
    "    POSITIONS = {0: \"top-left\", 1: \"top-right\", 2: \"bottom-left\", 3: \"bottom-right\", 4: \"center\"}\n",
    "\n",
    "    # Token vocabulary\n",
    "    VOCAB = {\n",
    "        \"<pad>\": 0, \"<start>\": 1, \"<end>\": 2,\n",
    "        \"what\": 3, \"color\": 4, \"shape\": 5, \"where\": 6, \"is\": 7, \"it\": 8, \"?\": 9,\n",
    "        \"red\": 10, \"blue\": 11, \"green\": 12,\n",
    "        \"circle\": 13, \"square\": 14,\n",
    "        \"top-left\": 15, \"top-right\": 16, \"bottom-left\": 17, \"bottom-right\": 18, \"center\": 19,\n",
    "    }\n",
    "    INV_VOCAB = {v: k for k, v in VOCAB.items()}\n",
    "\n",
    "    def __init__(self, n_samples: int = 500, image_size: int = 64):\n",
    "        self.image_size = image_size\n",
    "        self.data = []\n",
    "\n",
    "        for _ in range(n_samples):\n",
    "            color_idx = random.randint(0, 2)\n",
    "            shape_idx = random.randint(0, 1)\n",
    "            pos_idx = random.randint(0, 4)\n",
    "            q_type = random.choice([\"color\", \"shape\", \"position\"])\n",
    "\n",
    "            img = self._create_image(color_idx, shape_idx, pos_idx)\n",
    "\n",
    "            if q_type == \"color\":\n",
    "                q_tokens = [self.VOCAB[\"what\"], self.VOCAB[\"color\"], self.VOCAB[\"?\"]]\n",
    "                a_tokens = [self.VOCAB[self.COLORS[color_idx]]]\n",
    "            elif q_type == \"shape\":\n",
    "                q_tokens = [self.VOCAB[\"what\"], self.VOCAB[\"shape\"], self.VOCAB[\"?\"]]\n",
    "                a_tokens = [self.VOCAB[self.SHAPES[shape_idx]]]\n",
    "            else:\n",
    "                q_tokens = [self.VOCAB[\"where\"], self.VOCAB[\"is\"], self.VOCAB[\"it\"], self.VOCAB[\"?\"]]\n",
    "                a_tokens = [self.VOCAB[self.POSITIONS[pos_idx]]]\n",
    "\n",
    "            # Also create a simple caption for Stage 1\n",
    "            caption_tokens = [\n",
    "                self.VOCAB[self.COLORS[color_idx]],\n",
    "                self.VOCAB[self.SHAPES[shape_idx]],\n",
    "            ]\n",
    "\n",
    "            self.data.append({\n",
    "                \"image\": img,\n",
    "                \"question_tokens\": q_tokens,\n",
    "                \"answer_tokens\": a_tokens,\n",
    "                \"caption_tokens\": caption_tokens,\n",
    "                \"color\": self.COLORS[color_idx],\n",
    "                \"shape\": self.SHAPES[shape_idx],\n",
    "                \"position\": self.POSITIONS[pos_idx],\n",
    "            })\n",
    "\n",
    "    def _create_image(self, color_idx, shape_idx, pos_idx):\n",
    "        img = torch.zeros(3, self.image_size, self.image_size)\n",
    "\n",
    "        # Position offsets\n",
    "        positions = {\n",
    "            0: (self.image_size//4, self.image_size//4),\n",
    "            1: (self.image_size//4, 3*self.image_size//4),\n",
    "            2: (3*self.image_size//4, self.image_size//4),\n",
    "            3: (3*self.image_size//4, 3*self.image_size//4),\n",
    "            4: (self.image_size//2, self.image_size//2),\n",
    "        }\n",
    "        cy, cx = positions[pos_idx]\n",
    "        r = self.image_size // 6\n",
    "\n",
    "        # Draw shape\n",
    "        y, x = torch.meshgrid(torch.arange(self.image_size), torch.arange(self.image_size), indexing='ij')\n",
    "\n",
    "        if shape_idx == 0:  # Circle\n",
    "            mask = ((x - cx)**2 + (y - cy)**2) < r**2\n",
    "        else:  # Square\n",
    "            mask = (abs(x - cx) < r) & (abs(y - cy) < r)\n",
    "\n",
    "        # Set color\n",
    "        color_channels = {0: [1.0, 0.1, 0.1], 1: [0.1, 0.1, 1.0], 2: [0.1, 0.8, 0.1]}\n",
    "        for c, val in enumerate(color_channels[color_idx]):\n",
    "            img[c][mask] = val\n",
    "\n",
    "        # Add noise\n",
    "        img += torch.randn_like(img) * 0.05\n",
    "        return img.clamp(0, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = VisualQADataset(n_samples=600)\n",
    "test_dataset = VisualQADataset(n_samples=120)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples:     {len(test_dataset)}\")\n",
    "print(f\"Vocabulary size:  {len(VisualQADataset.VOCAB)}\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    sample = train_dataset[i]\n",
    "    img = sample[\"image\"].permute(1, 2, 0).numpy()\n",
    "    axes[i].imshow(img)\n",
    "\n",
    "    q = \" \".join([VisualQADataset.INV_VOCAB[t] for t in sample[\"question_tokens\"]])\n",
    "    a = \" \".join([VisualQADataset.INV_VOCAB[t] for t in sample[\"answer_tokens\"]])\n",
    "    axes[i].set_title(f\"Q: {q}\\nA: {a}\", fontsize=8)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Sample Training Data: Colored Shapes with Questions\", fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Multimodal Model with Frozen/Unfrozen Components"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionTunableModel(nn.Module):\n",
    "    \"\"\"Multimodal model with configurable frozen/trainable components.\"\"\"\n",
    "\n",
    "    def __init__(self, image_size=64, patch_size=8, vision_dim=128,\n",
    "                 llm_dim=64, vocab_size=20, num_heads=2, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        # Vision encoder (always frozen in LLaVA)\n",
    "        self.vision_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, vision_dim, kernel_size=patch_size, stride=patch_size),\n",
    "            nn.Flatten(2),\n",
    "        )\n",
    "\n",
    "        # Projector (trainable in both stages)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(vision_dim, llm_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(llm_dim, llm_dim),\n",
    "        )\n",
    "\n",
    "        # LLM components (frozen in Stage 1, trainable in Stage 2)\n",
    "        self.text_embedding = nn.Embedding(vocab_size, llm_dim)\n",
    "        self.position_embedding = nn.Embedding(512, llm_dim)\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=llm_dim, nhead=num_heads, dim_feedforward=llm_dim * 4,\n",
    "            batch_first=True, dropout=0.1\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.output_head = nn.Linear(llm_dim, vocab_size)\n",
    "\n",
    "    def set_stage(self, stage: int):\n",
    "        \"\"\"Configure which components are trainable.\n",
    "\n",
    "        Stage 1: Only projector is trainable\n",
    "        Stage 2: Projector + LLM are trainable\n",
    "        \"\"\"\n",
    "        # Vision encoder is always frozen\n",
    "        for p in self.vision_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Projector is always trainable\n",
    "        for p in self.projector.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        # LLM components\n",
    "        llm_trainable = (stage == 2)\n",
    "        for module in [self.text_embedding, self.position_embedding,\n",
    "                       self.decoder, self.output_head]:\n",
    "            for p in module.parameters():\n",
    "                p.requires_grad = llm_trainable\n",
    "\n",
    "        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        total = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"Stage {stage}: {trainable:,} / {total:,} parameters trainable ({trainable/total:.1%})\")\n",
    "\n",
    "    def forward(self, images, token_ids):\n",
    "        B = images.shape[0]\n",
    "\n",
    "        # Vision path\n",
    "        with torch.no_grad():\n",
    "            vis = self.vision_encoder(images)       # (B, vision_dim, N)\n",
    "            vis = vis.transpose(1, 2)                # (B, N, vision_dim)\n",
    "        vis_tokens = self.projector(vis)              # (B, N, llm_dim)\n",
    "\n",
    "        # Text path\n",
    "        text_tokens = self.text_embedding(token_ids)  # (B, M, llm_dim)\n",
    "\n",
    "        # Combine\n",
    "        combined = torch.cat([vis_tokens, text_tokens], dim=1)\n",
    "        seq_len = combined.shape[1]\n",
    "\n",
    "        positions = torch.arange(seq_len, device=combined.device).unsqueeze(0)\n",
    "        combined = combined + self.position_embedding(positions)\n",
    "\n",
    "        # Causal attention\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(combined.device)\n",
    "        memory = torch.zeros(B, 1, combined.shape[-1], device=combined.device)\n",
    "        output = self.decoder(combined, memory, tgt_mask=mask)\n",
    "\n",
    "        logits = self.output_head(output)\n",
    "        return logits"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO: Implement Stage 1 Training (Feature Alignment)\n",
    "\n",
    "Implement the training loop for Stage 1, where only the projector is trainable and the data is image-caption pairs."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage1(model, dataset, num_epochs=30, batch_size=32, lr=1e-3):\n",
    "    \"\"\"Stage 1: Feature alignment with captioning data.\n",
    "\n",
    "    Only the projector should be trainable. The training data is\n",
    "    image-caption pairs (not instruction-following data).\n",
    "\n",
    "    Args:\n",
    "        model: InstructionTunableModel\n",
    "        dataset: VisualQADataset (we use the caption_tokens)\n",
    "        num_epochs: number of training epochs\n",
    "        batch_size: batch size\n",
    "        lr: learning rate\n",
    "\n",
    "    Returns:\n",
    "        list of per-epoch losses\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Call model.set_stage(1) to freeze LLM\n",
    "    # Step 2: Create optimizer with only trainable parameters\n",
    "    #         Hint: filter(lambda p: p.requires_grad, model.parameters())\n",
    "    # Step 3: Training loop:\n",
    "    #   a) Sample a batch of images and captions\n",
    "    #   b) Input tokens = [<start>] + caption_tokens (teacher forcing)\n",
    "    #   c) Target tokens = caption_tokens + [<end>]\n",
    "    #   d) Forward pass through model\n",
    "    #   e) Compute cross-entropy loss on caption positions only\n",
    "    #      (ignore visual token positions in the loss)\n",
    "    #   f) Backprop and optimize\n",
    "    # ==============================\n",
    "\n",
    "    losses = []  # YOUR CODE HERE\n",
    "\n",
    "    return losses"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification - check your implementation produces decreasing loss\n",
    "model = InstructionTunableModel().to(device)\n",
    "stage1_losses = train_stage1(model, train_dataset, num_epochs=30)\n",
    "\n",
    "assert len(stage1_losses) == 30, f\"Expected 30 loss values, got {len(stage1_losses)}\"\n",
    "assert stage1_losses[-1] < stage1_losses[0], \"Loss should decrease during training\"\n",
    "print(f\"\\nStage 1 complete!\")\n",
    "print(f\"  Initial loss: {stage1_losses[0]:.4f}\")\n",
    "print(f\"  Final loss:   {stage1_losses[-1]:.4f}\")\n",
    "print(\"All checks passed!\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implement Stage 2 Training (Instruction Tuning)\n",
    "\n",
    "Now implement Stage 2, where both the projector and LLM are trainable and the data is question-answer pairs."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage2(model, dataset, num_epochs=50, batch_size=32, lr=5e-4):\n",
    "    \"\"\"Stage 2: Instruction tuning with QA data.\n",
    "\n",
    "    Both projector and LLM should be trainable. The training data is\n",
    "    image + question -> answer pairs.\n",
    "\n",
    "    Args:\n",
    "        model: InstructionTunableModel (already Stage 1 trained)\n",
    "        dataset: VisualQADataset (we use question_tokens and answer_tokens)\n",
    "        num_epochs: number of training epochs\n",
    "        batch_size: batch size\n",
    "        lr: learning rate\n",
    "\n",
    "    Returns:\n",
    "        list of per-epoch losses\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Call model.set_stage(2) to unfreeze LLM\n",
    "    # Step 2: Create optimizer (note: lower learning rate!)\n",
    "    # Step 3: Training loop:\n",
    "    #   a) Sample a batch of images, questions, and answers\n",
    "    #   b) Input tokens = [<start>] + question_tokens + answer_tokens\n",
    "    #   c) Target = answer_tokens + [<end>] (only compute loss on answer positions)\n",
    "    #   d) Forward pass, compute loss on answer positions only\n",
    "    #   e) Backprop and optimize\n",
    "    # ==============================\n",
    "\n",
    "    losses = []  # YOUR CODE HERE\n",
    "\n",
    "    return losses"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "stage2_losses = train_stage2(model, train_dataset, num_epochs=50)\n",
    "\n",
    "assert len(stage2_losses) == 50, f\"Expected 50 loss values, got {len(stage2_losses)}\"\n",
    "assert stage2_losses[-1] < stage2_losses[0], \"Loss should decrease during training\"\n",
    "print(f\"\\nStage 2 complete!\")\n",
    "print(f\"  Initial loss: {stage2_losses[0]:.4f}\")\n",
    "print(f\"  Final loss:   {stage2_losses[-1]:.4f}\")\n",
    "print(\"All checks passed!\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference implementation of both stages for those who want to see the full code\n",
    "\n",
    "def train_both_stages(model, dataset, stage1_epochs=30, stage2_epochs=50):\n",
    "    \"\"\"Complete two-stage training pipeline.\"\"\"\n",
    "\n",
    "    all_losses = {\"stage1\": [], \"stage2\": []}\n",
    "\n",
    "    # ---- Stage 1: Feature Alignment ----\n",
    "    model.set_stage(1)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3\n",
    "    )\n",
    "\n",
    "    for epoch in range(stage1_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "\n",
    "        indices = list(range(len(dataset)))\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        for i in range(0, len(indices), 32):\n",
    "            batch_idx = indices[i:i+32]\n",
    "            batch = [dataset[j] for j in batch_idx]\n",
    "\n",
    "            images = torch.stack([b[\"image\"] for b in batch]).to(device)\n",
    "            captions = [b[\"caption_tokens\"] for b in batch]\n",
    "\n",
    "            # Build input/target sequences\n",
    "            max_len = max(len(c) for c in captions) + 1  # +1 for start token\n",
    "            input_ids = torch.zeros(len(batch), max_len, dtype=torch.long, device=device)\n",
    "            target_ids = torch.full((len(batch), max_len), -100, dtype=torch.long, device=device)\n",
    "\n",
    "            for j, cap in enumerate(captions):\n",
    "                input_ids[j, 0] = 1  # <start>\n",
    "                for k, t in enumerate(cap):\n",
    "                    input_ids[j, k+1] = t\n",
    "                    target_ids[j, k] = t\n",
    "                target_ids[j, len(cap)] = 2  # <end>\n",
    "\n",
    "            logits = model(images, input_ids)\n",
    "            # Only compute loss on text positions (after visual tokens)\n",
    "            num_patches = model.num_patches\n",
    "            text_logits = logits[:, num_patches:, :]\n",
    "            loss = F.cross_entropy(text_logits.reshape(-1, text_logits.shape[-1]),\n",
    "                                   target_ids.reshape(-1), ignore_index=-100)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        all_losses[\"stage1\"].append(epoch_loss / n_batches)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"  Stage 1 Epoch {epoch+1}: loss = {all_losses['stage1'][-1]:.4f}\")\n",
    "\n",
    "    # ---- Stage 2: Instruction Tuning ----\n",
    "    model.set_stage(2)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=5e-4\n",
    "    )\n",
    "\n",
    "    for epoch in range(stage2_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "\n",
    "        indices = list(range(len(dataset)))\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        for i in range(0, len(indices), 32):\n",
    "            batch_idx = indices[i:i+32]\n",
    "            batch = [dataset[j] for j in batch_idx]\n",
    "\n",
    "            images = torch.stack([b[\"image\"] for b in batch]).to(device)\n",
    "\n",
    "            # Build input: [<start>] + question + answer\n",
    "            all_tokens = []\n",
    "            for b in batch:\n",
    "                all_tokens.append([1] + b[\"question_tokens\"] + b[\"answer_tokens\"])\n",
    "\n",
    "            max_len = max(len(t) for t in all_tokens)\n",
    "            input_ids = torch.zeros(len(batch), max_len, dtype=torch.long, device=device)\n",
    "            target_ids = torch.full((len(batch), max_len), -100, dtype=torch.long, device=device)\n",
    "\n",
    "            for j, tokens in enumerate(all_tokens):\n",
    "                for k, t in enumerate(tokens):\n",
    "                    input_ids[j, k] = t\n",
    "                # Only compute loss on answer tokens\n",
    "                q_len = len(batch[j][\"question_tokens\"]) + 1  # +1 for start\n",
    "                a_tokens = batch[j][\"answer_tokens\"]\n",
    "                for k, t in enumerate(a_tokens):\n",
    "                    target_ids[j, q_len - 1 + k] = t\n",
    "                target_ids[j, q_len - 1 + len(a_tokens)] = 2  # <end>\n",
    "\n",
    "            logits = model(images, input_ids)\n",
    "            num_patches = model.num_patches\n",
    "            text_logits = logits[:, num_patches:, :]\n",
    "            loss = F.cross_entropy(text_logits.reshape(-1, text_logits.shape[-1]),\n",
    "                                   target_ids.reshape(-1), ignore_index=-100)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        all_losses[\"stage2\"].append(epoch_loss / n_batches)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"  Stage 2 Epoch {epoch+1}: loss = {all_losses['stage2'][-1]:.4f}\")\n",
    "\n",
    "    return all_losses\n",
    "\n",
    "# Train the model\n",
    "model = InstructionTunableModel().to(device)\n",
    "losses = train_both_stages(model, train_dataset)"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Two-stage training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Stage 1\n",
    "axes[0].plot(losses[\"stage1\"], color='steelblue', linewidth=2)\n",
    "axes[0].set_title(\"Stage 1: Feature Alignment\\n(Only projector trains)\", fontsize=12)\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=losses[\"stage1\"][-1], color='steelblue', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Stage 2\n",
    "axes[1].plot(losses[\"stage2\"], color='coral', linewidth=2)\n",
    "axes[1].set_title(\"Stage 2: Instruction Tuning\\n(Projector + LLM train)\", fontsize=12)\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Loss\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=losses[\"stage2\"][-1], color='coral', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.suptitle(\"Two-Stage Training: This is exactly how LLaVA is trained!\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Stage 1 final loss: {losses['stage1'][-1]:.4f}\")\n",
    "print(f\"Stage 2 final loss: {losses['stage2'][-1]:.4f}\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "results_by_type = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sample in test_dataset:\n",
    "        img = sample[\"image\"].unsqueeze(0).to(device)\n",
    "        q_tokens = sample[\"question_tokens\"]\n",
    "        a_tokens = sample[\"answer_tokens\"]\n",
    "\n",
    "        # Input: [<start>] + question\n",
    "        input_ids = torch.tensor([[1] + q_tokens], device=device)\n",
    "        logits = model(img, input_ids)\n",
    "\n",
    "        # Predict answer at the position after the question\n",
    "        num_patches = model.num_patches\n",
    "        answer_pos = num_patches + len(q_tokens)\n",
    "        pred_id = logits[0, answer_pos, :].argmax().item()\n",
    "        true_id = a_tokens[0]\n",
    "\n",
    "        is_correct = (pred_id == true_id)\n",
    "        correct += int(is_correct)\n",
    "        total += 1\n",
    "\n",
    "        # Track by question type\n",
    "        if 4 in q_tokens:  # \"color\" question\n",
    "            q_type = \"color\"\n",
    "        elif 5 in q_tokens:  # \"shape\" question\n",
    "            q_type = \"shape\"\n",
    "        else:\n",
    "            q_type = \"position\"\n",
    "        results_by_type[q_type][\"correct\"] += int(is_correct)\n",
    "        results_by_type[q_type][\"total\"] += 1\n",
    "\n",
    "print(f\"Overall test accuracy: {correct/total:.1%}\")\n",
    "print(f\"\\nBreakdown by question type:\")\n",
    "for q_type, stats in sorted(results_by_type.items()):\n",
    "    acc = stats[\"correct\"] / stats[\"total\"]\n",
    "    print(f\"  {q_type:10s}: {acc:.1%} ({stats['correct']}/{stats['total']})\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive demo: show images and model's answers\n",
    "fig, axes = plt.subplots(3, 5, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "model.eval()\n",
    "inv_vocab = VisualQADataset.INV_VOCAB\n",
    "\n",
    "demo_samples = random.sample(range(len(test_dataset)), 15)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, idx in enumerate(demo_samples):\n",
    "        sample = test_dataset[idx]\n",
    "        img = sample[\"image\"].unsqueeze(0).to(device)\n",
    "        q_tokens = sample[\"question_tokens\"]\n",
    "        a_tokens = sample[\"answer_tokens\"]\n",
    "\n",
    "        input_ids = torch.tensor([[1] + q_tokens], device=device)\n",
    "        logits = model(img, input_ids)\n",
    "\n",
    "        num_patches = model.num_patches\n",
    "        answer_pos = num_patches + len(q_tokens)\n",
    "        pred_id = logits[0, answer_pos, :].argmax().item()\n",
    "        true_id = a_tokens[0]\n",
    "\n",
    "        question = \" \".join([inv_vocab[t] for t in q_tokens])\n",
    "        pred_answer = inv_vocab.get(pred_id, \"?\")\n",
    "        true_answer = inv_vocab[true_id]\n",
    "        is_correct = pred_id == true_id\n",
    "\n",
    "        display_img = sample[\"image\"].permute(1, 2, 0).numpy()\n",
    "        axes[i].imshow(display_img)\n",
    "        axes[i].set_title(\n",
    "            f\"Q: {question}\\nPred: {pred_answer} | True: {true_answer}\",\n",
    "            fontsize=8,\n",
    "            color='green' if is_correct else 'red',\n",
    "            fontweight='bold'\n",
    "        )\n",
    "        axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Visual QA: Model Predictions After Two-Stage Training\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Congratulations! You have trained a multimodal model with the exact two-stage\")\n",
    "print(\"pipeline used by LLaVA -- feature alignment followed by instruction tuning!\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "\n",
    "1. In our experiment, Stage 1 used caption data and Stage 2 used QA data. What would happen if you skipped Stage 1 entirely and went straight to Stage 2? Try it and compare the results.\n",
    "\n",
    "2. LLaVA uses 595K caption pairs for Stage 1 but only 158K instruction-following examples for Stage 2. Why might the ratio be weighted toward simpler data? What happens if you use more Stage 2 data?\n",
    "\n",
    "3. During Stage 2, the LLM is unfrozen. This means it might \"forget\" some of its pretraining knowledge (catastrophic forgetting). How does the lower learning rate in Stage 2 help mitigate this?\n",
    "\n",
    "### Optional Challenges\n",
    "\n",
    "1. **Skip Stage 1:** Modify the code to train directly with Stage 2. Compare final accuracy with the two-stage approach. Which works better?\n",
    "\n",
    "2. **Data augmentation:** Add color jitter and random rotations to the training images. Does the model become more robust?\n",
    "\n",
    "3. **Multi-turn conversations:** Extend the dataset to include follow-up questions (e.g., \"What color?\" -> \"red\" -> \"And what shape?\" -> \"circle\"). Modify the training to handle multi-turn dialogues."
   ],
   "id": "cell_25"
  }
 ]
}