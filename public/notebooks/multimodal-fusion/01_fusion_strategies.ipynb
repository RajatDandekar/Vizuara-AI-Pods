{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Multimodal Fusion Strategies from First Principles \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Fusion Strategies from First Principles\n",
    "\n",
    "*Part 1 of the Vizuara series on Multimodal Fusion Architectures*\n",
    "*Estimated time: 45 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Modern AI systems need to understand the world through multiple senses -- just like humans do. A doctor reads X-rays (vision), medical records (text), and listens to patient descriptions (audio). A self-driving car combines camera feeds with LiDAR point clouds. A virtual assistant processes both voice and text.\n",
    "\n",
    "The fundamental question is: **how do we combine information from different modalities inside a neural network?**\n",
    "\n",
    "By the end of this notebook, you will have built three different fusion architectures from scratch, trained them on a real multimodal classification task, and compared their performance head-to-head. You will see exactly when and why each strategy works best."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Install dependencies and set random seeds\n",
    "!pip install torch torchvision matplotlib numpy -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Let us start with a concrete analogy. Imagine you are trying to identify a bird species. You have:\n",
    "- A **photograph** of the bird (vision)\n",
    "- A **text description** like \"small, bright red, with a short beak\" (language)\n",
    "\n",
    "There are three fundamentally different ways to combine these:\n",
    "\n",
    "**Early Fusion (The Blender):** Throw the photo pixels and the text characters into one big blender from the start. One shared network processes everything together.\n",
    "\n",
    "**Late Fusion (The Committee):** Give the photo to a bird photography expert and the text to a bird book expert. Each works independently. At the very end, they meet and vote on the species.\n",
    "\n",
    "**Cross-Attention Fusion (The Collaborators):** Two experts work on the same problem, but they constantly check each other's notes. The text expert asks \"is there anything red in the photo?\" and the vision expert points to the relevant region.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "Which approach would you choose if the question was: \"Is the bird in the photo sitting on a branch or flying?\" Does the text description help? Does the photo help more? How much does the answer depend on relating specific words to specific image regions?"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### 3.1 Modality Representations\n",
    "\n",
    "Each modality produces a feature vector. After encoding:\n",
    "- Vision: $h_v \\in \\mathbb{R}^{d_v}$ (e.g., from a CNN or ViT)\n",
    "- Text: $h_t \\in \\mathbb{R}^{d_t}$ (e.g., from an embedding layer + pooling)\n",
    "\n",
    "**Computational meaning:** $h_v$ is a vector of numbers summarizing what the image shows. $h_t$ is a vector summarizing what the text says. Our job is to combine these two vectors meaningfully.\n",
    "\n",
    "### 3.2 Early Fusion\n",
    "\n",
    "$$z_{\\text{early}} = f_{\\text{shared}}\\left([h_v; h_t]\\right)$$\n",
    "\n",
    "Here, $[h_v; h_t]$ means we concatenate the two vectors. If $h_v$ has 128 dimensions and $h_t$ has 64 dimensions, the concatenated vector has 192 dimensions. Then $f_{\\text{shared}}$ is a neural network (like an MLP) that processes the full 192-dimensional input.\n",
    "\n",
    "Let us plug in numbers. Suppose $h_v = [0.3, 0.7]$ and $h_t = [0.5, 0.2, 0.1]$:\n",
    "\n",
    "$$[h_v; h_t] = [0.3, 0.7, 0.5, 0.2, 0.1]$$\n",
    "\n",
    "This 5-dimensional vector goes into the shared network. The network can now learn interactions between ALL features, regardless of which modality they came from. This is exactly what we want when cross-modal interactions matter from the start.\n",
    "\n",
    "### 3.3 Late Fusion\n",
    "\n",
    "$$z_{\\text{late}} = g\\left([f_v(h_v); f_t(h_t)]\\right)$$\n",
    "\n",
    "Here, $f_v$ and $f_t$ are separate networks for each modality. They process independently, then their outputs are concatenated and passed through a small fusion head $g$.\n",
    "\n",
    "### 3.4 Cross-Attention\n",
    "\n",
    "$$\\text{CrossAttn}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where $Q$ comes from one modality (queries) and $K, V$ come from the other (keys/values). This lets one modality \"ask questions\" of the other.\n",
    "\n",
    "Let us trace through a tiny example. $Q = [1, 0]$ (one text query), $K = [[1, 1], [0, 1]]$ (two visual keys), $V = [[0.5, 0.3], [0.1, 0.8]]$ (two visual values), $d_k = 2$:\n",
    "\n",
    "Step 1: $QK^\\top = [1 \\times 1 + 0 \\times 1, \\; 1 \\times 0 + 0 \\times 1] = [1, 0]$\n",
    "\n",
    "Step 2: Divide by $\\sqrt{2} \\approx 1.414$: $[0.707, 0]$\n",
    "\n",
    "Step 3: Softmax: $[\\frac{e^{0.707}}{e^{0.707} + e^{0}}, \\frac{e^{0}}{e^{0.707} + e^{0}}] = [0.67, 0.33]$\n",
    "\n",
    "Step 4: Weighted sum of values: $0.67 \\times [0.5, 0.3] + 0.33 \\times [0.1, 0.8] = [0.37, 0.47]$\n",
    "\n",
    "This tells us the text query attended 67% to the first visual patch and 33% to the second, producing a blended visual representation $[0.37, 0.47]$."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 Creating a Multimodal Dataset\n",
    "\n",
    "First, let us create a synthetic multimodal dataset. We will generate \"image features\" and \"text features\" that are correlated in specific ways, making it easy to see which fusion strategies work best."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Synthetic multimodal dataset for comparing fusion strategies.\n",
    "\n",
    "    Creates paired (image_features, text_features, label) samples where:\n",
    "    - Class 0: image features are centered around [1, 0, ...], text around [0, 1, ...]\n",
    "    - Class 1: image features are centered around [0, 1, ...], text around [1, 0, ...]\n",
    "    - Class 2: label depends on INTERACTION between image and text features\n",
    "\n",
    "    Class 2 is the key: it requires cross-modal reasoning to classify correctly.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_samples=3000, img_dim=32, txt_dim=16, noise=0.3):\n",
    "        self.n_samples = n_samples\n",
    "        self.img_dim = img_dim\n",
    "        self.txt_dim = txt_dim\n",
    "\n",
    "        self.img_features = torch.zeros(n_samples, img_dim)\n",
    "        self.txt_features = torch.zeros(n_samples, txt_dim)\n",
    "        self.labels = torch.zeros(n_samples, dtype=torch.long)\n",
    "\n",
    "        n_per_class = n_samples // 3\n",
    "\n",
    "        # Class 0: image pattern A + text pattern A\n",
    "        for i in range(n_per_class):\n",
    "            self.img_features[i] = torch.randn(img_dim) * noise + torch.tensor([1.0] + [0.0]*(img_dim-1))\n",
    "            self.txt_features[i] = torch.randn(txt_dim) * noise + torch.tensor([0.0, 1.0] + [0.0]*(txt_dim-2))\n",
    "            self.labels[i] = 0\n",
    "\n",
    "        # Class 1: image pattern B + text pattern B\n",
    "        for i in range(n_per_class, 2*n_per_class):\n",
    "            self.img_features[i] = torch.randn(img_dim) * noise + torch.tensor([0.0, 1.0] + [0.0]*(img_dim-2))\n",
    "            self.txt_features[i] = torch.randn(txt_dim) * noise + torch.tensor([1.0] + [0.0]*(txt_dim-1))\n",
    "            self.labels[i] = 1\n",
    "\n",
    "        # Class 2: requires cross-modal interaction (image[0] * text[0] > 0.5)\n",
    "        for i in range(2*n_per_class, n_samples):\n",
    "            img = torch.randn(img_dim) * noise\n",
    "            txt = torch.randn(txt_dim) * noise\n",
    "            # Make the interaction signal clear\n",
    "            img[0] = torch.randn(1).abs() + 0.5\n",
    "            txt[0] = torch.randn(1).abs() + 0.5\n",
    "            self.img_features[i] = img\n",
    "            self.txt_features[i] = txt\n",
    "            self.labels[i] = 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.img_features[idx], self.txt_features[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MultimodalDataset(n_samples=3000)\n",
    "test_dataset = MultimodalDataset(n_samples=600)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Image feature dim: {train_dataset.img_dim}\")\n",
    "print(f\"Text feature dim: {train_dataset.txt_dim}\")\n",
    "print(f\"Classes: {torch.unique(train_dataset.labels)}\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization checkpoint: look at the data distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot first 2 dims of image features\n",
    "colors = ['#2196F3', '#FF5722', '#4CAF50']\n",
    "for c in range(3):\n",
    "    mask = train_dataset.labels == c\n",
    "    axes[0].scatter(train_dataset.img_features[mask, 0].numpy(),\n",
    "                   train_dataset.img_features[mask, 1].numpy(),\n",
    "                   c=colors[c], alpha=0.4, label=f'Class {c}', s=20)\n",
    "axes[0].set_title('Image Features (first 2 dims)')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('Dimension 0')\n",
    "axes[0].set_ylabel('Dimension 1')\n",
    "\n",
    "# Plot first 2 dims of text features\n",
    "for c in range(3):\n",
    "    mask = train_dataset.labels == c\n",
    "    axes[1].scatter(train_dataset.txt_features[mask, 0].numpy(),\n",
    "                   train_dataset.txt_features[mask, 1].numpy(),\n",
    "                   c=colors[c], alpha=0.4, label=f'Class {c}', s=20)\n",
    "axes[1].set_title('Text Features (first 2 dims)')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('Dimension 0')\n",
    "axes[1].set_ylabel('Dimension 1')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Notice how Class 2 (green) overlaps in both modalities -- it requires cross-modal interaction to classify!\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Building the Early Fusion Model"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyFusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Early Fusion: Concatenate features first, then process jointly.\n",
    "\n",
    "    Architecture:\n",
    "        [img_features ; txt_features] -> MLP -> output\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dim=32, txt_dim=16, hidden_dim=64, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(img_dim + txt_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_feat, txt_feat):\n",
    "        combined = torch.cat([img_feat, txt_feat], dim=-1)\n",
    "        return self.fusion(combined)\n",
    "\n",
    "model_early = EarlyFusionModel().to(device)\n",
    "print(f\"Early Fusion parameters: {sum(p.numel() for p in model_early.parameters()):,}\")\n",
    "print(model_early)"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Building the Late Fusion Model"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LateFusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Late Fusion: Process each modality independently, combine at the end.\n",
    "\n",
    "    Architecture:\n",
    "        img_features -> img_encoder -> img_embed\n",
    "        txt_features -> txt_encoder -> txt_embed\n",
    "        [img_embed ; txt_embed] -> fusion_head -> output\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dim=32, txt_dim=16, embed_dim=32, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.img_encoder = nn.Sequential(\n",
    "            nn.Linear(img_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "        self.txt_encoder = nn.Sequential(\n",
    "            nn.Linear(txt_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "        self.fusion_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 2, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_feat, txt_feat):\n",
    "        img_embed = self.img_encoder(img_feat)\n",
    "        txt_embed = self.txt_encoder(txt_feat)\n",
    "        combined = torch.cat([img_embed, txt_embed], dim=-1)\n",
    "        return self.fusion_head(combined)\n",
    "\n",
    "model_late = LateFusionModel().to(device)\n",
    "print(f\"Late Fusion parameters: {sum(p.numel() for p in model_late.parameters()):,}\")\n",
    "print(model_late)"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Building the Cross-Attention Fusion Model\n",
    "\n",
    "This is the most interesting one. We will implement cross-attention from scratch."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-attention: text queries attend to visual keys/values.\n",
    "\n",
    "    Q = W_q * text_features\n",
    "    K = W_k * img_features\n",
    "    V = W_v * img_features\n",
    "    output = softmax(Q K^T / sqrt(d_k)) V\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, query, kv):\n",
    "        \"\"\"\n",
    "        query: (B, N_q, dim) - from text modality\n",
    "        kv: (B, N_kv, dim) - from visual modality\n",
    "        \"\"\"\n",
    "        B, N_q, D = query.shape\n",
    "        _, N_kv, _ = kv.shape\n",
    "\n",
    "        # Project queries, keys, values\n",
    "        q = self.q_proj(query).reshape(B, N_q, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(kv).reshape(B, N_kv, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(kv).reshape(B, N_kv, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Attention scores\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        out = attn @ v\n",
    "        out = out.transpose(1, 2).reshape(B, N_q, D)\n",
    "\n",
    "        # Store attention weights for visualization\n",
    "        self.last_attn_weights = attn.detach()\n",
    "\n",
    "        return self.out_proj(out)\n",
    "\n",
    "class CrossAttentionFusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-Attention Fusion: text features query visual features via attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dim=32, txt_dim=16, embed_dim=32, num_heads=4, num_classes=3):\n",
    "        super().__init__()\n",
    "        # Project both modalities to same dimension\n",
    "        self.img_proj = nn.Linear(img_dim, embed_dim)\n",
    "        self.txt_proj = nn.Linear(txt_dim, embed_dim)\n",
    "\n",
    "        # Cross-attention: text queries, visual keys/values\n",
    "        self.cross_attn = CrossAttentionLayer(embed_dim, num_heads)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_feat, txt_feat):\n",
    "        # Project to shared dimension and add sequence dimension\n",
    "        img_embed = self.img_proj(img_feat).unsqueeze(1)  # (B, 1, embed_dim)\n",
    "        txt_embed = self.txt_proj(txt_feat).unsqueeze(1)  # (B, 1, embed_dim)\n",
    "\n",
    "        # Cross-attention: text queries attend to visual features\n",
    "        attn_out = self.cross_attn(txt_embed, img_embed)\n",
    "        fused = self.norm(txt_embed + attn_out)  # Residual connection\n",
    "\n",
    "        # Pool and classify\n",
    "        pooled = fused.squeeze(1)  # (B, embed_dim)\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "model_cross = CrossAttentionFusionModel().to(device)\n",
    "print(f\"Cross-Attention Fusion parameters: {sum(p.numel() for p in model_cross.parameters()):,}\")\n",
    "print(model_cross)"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization checkpoint: verify model architectures\n",
    "print(\"=\" * 60)\n",
    "print(\"Model Parameter Comparison\")\n",
    "print(\"=\" * 60)\n",
    "for name, model in [(\"Early Fusion\", model_early),\n",
    "                     (\"Late Fusion\", model_late),\n",
    "                     (\"Cross-Attention\", model_cross)]:\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{name:20s}: {total:,} parameters\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO: Implement Bidirectional Cross-Attention\n",
    "\n",
    "The cross-attention model above only goes in one direction: text queries attend to visual features. But what if we also want visual features to attend to text? This is called **bidirectional cross-attention**."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalCrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional cross-attention: both modalities attend to each other.\n",
    "\n",
    "    Step 1: text queries attend to visual keys/values -> text_updated\n",
    "    Step 2: visual queries attend to text keys/values -> img_updated\n",
    "    Step 3: Combine both updated representations\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        # ============ TODO ============\n",
    "        # Create two CrossAttentionLayer instances:\n",
    "        # 1. self.txt_to_img: text queries, visual keys/values\n",
    "        # 2. self.img_to_txt: visual queries, text keys/values\n",
    "        # Also create two LayerNorm layers for residual connections\n",
    "        # ==============================\n",
    "\n",
    "        self.txt_to_img = None  # YOUR CODE HERE\n",
    "        self.img_to_txt = None  # YOUR CODE HERE\n",
    "        self.norm_txt = None    # YOUR CODE HERE\n",
    "        self.norm_img = None    # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, img_embed, txt_embed):\n",
    "        \"\"\"\n",
    "        img_embed: (B, 1, dim)\n",
    "        txt_embed: (B, 1, dim)\n",
    "        Returns: (B, dim) fused representation\n",
    "        \"\"\"\n",
    "        # ============ TODO ============\n",
    "        # Step 1: text attends to image (text queries, image keys/values)\n",
    "        # Step 2: image attends to text (image queries, text keys/values)\n",
    "        # Step 3: Add residual connections with layer norm\n",
    "        # Step 4: Concatenate and return mean-pooled result\n",
    "        # ==============================\n",
    "\n",
    "        result = None  # YOUR CODE HERE\n",
    "\n",
    "        return result"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification cell\n",
    "# Once you implement the above, run this to verify:\n",
    "if BidirectionalCrossAttention(32).txt_to_img is not None:\n",
    "    bidir = BidirectionalCrossAttention(32)\n",
    "    test_img = torch.randn(2, 1, 32)\n",
    "    test_txt = torch.randn(2, 1, 32)\n",
    "    out = bidir(test_img, test_txt)\n",
    "    assert out.shape == (2, 32), f\"Expected (2, 32), got {out.shape}\"\n",
    "    print(\"Correct! Bidirectional cross-attention works.\")\n",
    "else:\n",
    "    print(\"TODO: Implement BidirectionalCrossAttention above\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Add a Fourth Fusion Strategy -- Gated Fusion\n",
    "\n",
    "In gated fusion, a learnable gate $\\alpha$ controls how much each modality contributes:\n",
    "\n",
    "$$z = \\sigma(\\alpha) \\cdot h_v + (1 - \\sigma(\\alpha)) \\cdot h_t$$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedFusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Gated Fusion: A learnable gate controls the mix of modalities.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dim=32, txt_dim=16, embed_dim=32, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.img_proj = nn.Linear(img_dim, embed_dim)\n",
    "        self.txt_proj = nn.Linear(txt_dim, embed_dim)\n",
    "\n",
    "        # ============ TODO ============\n",
    "        # Create a learnable gate parameter (nn.Parameter)\n",
    "        # initialized to 0.0 (sigmoid(0) = 0.5, equal mix)\n",
    "        # Also create a classifier head\n",
    "        # ==============================\n",
    "\n",
    "        self.gate = None  # YOUR CODE HERE (hint: nn.Parameter(torch.zeros(1)))\n",
    "        self.classifier = None  # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, img_feat, txt_feat):\n",
    "        # ============ TODO ============\n",
    "        # Step 1: Project both modalities to embed_dim\n",
    "        # Step 2: Compute gate value: alpha = sigmoid(self.gate)\n",
    "        # Step 3: Fuse: z = alpha * img_embed + (1 - alpha) * txt_embed\n",
    "        # Step 4: Classify\n",
    "        # ==============================\n",
    "        return None  # YOUR CODE HERE"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "if GatedFusionModel().gate is not None:\n",
    "    gated = GatedFusionModel()\n",
    "    out = gated(torch.randn(2, 32), torch.randn(2, 16))\n",
    "    assert out.shape == (2, 3), f\"Expected (2, 3), got {out.shape}\"\n",
    "    alpha = torch.sigmoid(gated.gate).item()\n",
    "    print(f\"Initial gate value: {alpha:.3f} (should be ~0.5)\")\n",
    "    print(\"Correct! Gated fusion works.\")\n",
    "else:\n",
    "    print(\"TODO: Implement GatedFusionModel above\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us now train all three models and compare them head-to-head."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, epochs=30, lr=1e-3):\n",
    "    \"\"\"Train a multimodal model and track metrics.\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    test_accs = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for img, txt, labels in train_loader:\n",
    "            img, txt, labels = img.to(device), txt.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(img, txt)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for img, txt, labels in test_loader:\n",
    "                img, txt, labels = img.to(device), txt.to(device), labels.to(device)\n",
    "                output = model(img, txt)\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        acc = correct / total\n",
    "        test_accs.append(acc)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"  Epoch {epoch+1:3d}: Loss={train_losses[-1]:.4f}, Acc={acc:.4f}\")\n",
    "\n",
    "    return train_losses, test_accs"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all three models\n",
    "print(\"Training Early Fusion...\")\n",
    "model_early = EarlyFusionModel().to(device)\n",
    "early_losses, early_accs = train_model(model_early, train_loader, test_loader)\n",
    "\n",
    "print(\"\\nTraining Late Fusion...\")\n",
    "model_late = LateFusionModel().to(device)\n",
    "late_losses, late_accs = train_model(model_late, train_loader, test_loader)\n",
    "\n",
    "print(\"\\nTraining Cross-Attention Fusion...\")\n",
    "model_cross = CrossAttentionFusionModel().to(device)\n",
    "cross_losses, cross_accs = train_model(model_cross, train_loader, test_loader)"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization checkpoint: training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(early_losses, label='Early Fusion', color='#2196F3', linewidth=2)\n",
    "axes[0].plot(late_losses, label='Late Fusion', color='#FF5722', linewidth=2)\n",
    "axes[0].plot(cross_losses, label='Cross-Attention', color='#4CAF50', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('Training Loss Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(early_accs, label='Early Fusion', color='#2196F3', linewidth=2)\n",
    "axes[1].plot(late_accs, label='Late Fusion', color='#FF5722', linewidth=2)\n",
    "axes[1].plot(cross_accs, label='Cross-Attention', color='#4CAF50', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Test Accuracy')\n",
    "axes[1].set_title('Test Accuracy Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Test Accuracies:\")\n",
    "print(f\"  Early Fusion:      {early_accs[-1]:.4f}\")\n",
    "print(f\"  Late Fusion:       {late_accs[-1]:.4f}\")\n",
    "print(f\"  Cross-Attention:   {cross_accs[-1]:.4f}\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class accuracy breakdown\n",
    "def per_class_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    class_correct = {0: 0, 1: 0, 2: 0}\n",
    "    class_total = {0: 0, 1: 0, 2: 0}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, txt, labels in loader:\n",
    "            img, txt, labels = img.to(device), txt.to(device), labels.to(device)\n",
    "            output = model(img, txt)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            for c in range(3):\n",
    "                mask = labels == c\n",
    "                class_total[c] += mask.sum().item()\n",
    "                class_correct[c] += ((predicted == labels) & mask).sum().item()\n",
    "\n",
    "    return {c: class_correct[c] / max(class_total[c], 1) for c in range(3)}\n",
    "\n",
    "print(\"\\nPer-Class Accuracy Breakdown:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Model':20s} {'Class 0':>10s} {'Class 1':>10s} {'Class 2':>10s}\")\n",
    "print(\"-\" * 50)\n",
    "for name, model in [(\"Early Fusion\", model_early),\n",
    "                     (\"Late Fusion\", model_late),\n",
    "                     (\"Cross-Attention\", model_cross)]:\n",
    "    accs = per_class_accuracy(model, test_loader)\n",
    "    print(f\"{name:20s} {accs[0]:>10.4f} {accs[1]:>10.4f} {accs[2]:>10.4f}\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\\nClass 2 requires cross-modal interaction -- watch which model does best here!\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a comprehensive comparison visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "models = [(\"Early Fusion\", model_early, '#2196F3'),\n",
    "          (\"Late Fusion\", model_late, '#FF5722'),\n",
    "          (\"Cross-Attention\", model_cross, '#4CAF50')]\n",
    "\n",
    "for idx, (name, model, color) in enumerate(models):\n",
    "    accs = per_class_accuracy(model, test_loader)\n",
    "    bars = axes[idx].bar(['Class 0\\n(Simple)', 'Class 1\\n(Simple)', 'Class 2\\n(Cross-Modal)'],\n",
    "                         [accs[0], accs[1], accs[2]],\n",
    "                         color=[color, color, color],\n",
    "                         alpha=[0.5, 0.5, 1.0],\n",
    "                         edgecolor=color, linewidth=2)\n",
    "    axes[idx].set_ylim(0, 1.1)\n",
    "    axes[idx].set_title(f'{name}', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Accuracy')\n",
    "    for bar, acc in zip(bars, accs.values()):\n",
    "        axes[idx].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,\n",
    "                      f'{acc:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    axes[idx].axhline(y=0.33, color='gray', linestyle='--', alpha=0.5, label='Random')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.suptitle('Multimodal Fusion Strategy Comparison', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCongratulations! You have built and compared three multimodal fusion strategies from scratch!\")\n",
    "print(\"Key takeaway: Cross-attention excels when the task requires fine-grained cross-modal interaction.\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "1. Why does late fusion struggle with Class 2 (cross-modal interaction)? Think about what information the fusion head actually receives.\n",
    "2. If you increased the depth of the late fusion head (more layers), would it eventually match cross-attention performance? Why or why not?\n",
    "3. In what real-world scenarios would you choose late fusion over cross-attention, despite its limitations?\n",
    "\n",
    "### Optional Challenges\n",
    "1. Implement the `BidirectionalCrossAttention` TODO above and compare its performance to unidirectional cross-attention.\n",
    "2. Add a fourth class that requires reasoning about the ABSENCE of a feature in one modality. Which fusion strategy handles this best?\n",
    "3. Scale up the cross-attention model to use multiple heads and multiple layers. Plot how accuracy changes with model complexity."
   ],
   "id": "cell_30"
  }
 ]
}