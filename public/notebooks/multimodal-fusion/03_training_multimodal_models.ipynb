{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Training Multimodal Models: Contrastive Learning and Instruction Tuning \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Multimodal Models: Contrastive Learning and Instruction Tuning\n",
    "\n",
    "*Part 3 of the Vizuara series on Multimodal Fusion Architectures*\n",
    "*Estimated time: 50 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebooks, we built multimodal architectures. But building the architecture is only half the story. The other half is **training** -- and modern multimodal models use a clever two-stage training recipe:\n",
    "\n",
    "1. **Stage 1 (Alignment):** Teach the model that an image of a dog and the text \"a photo of a dog\" should have similar representations. This is done with **contrastive learning** (CLIP-style).\n",
    "2. **Stage 2 (Instruction Tuning):** Teach the model to follow instructions like \"Describe this image in detail\" or \"What color is the car?\" This is done with **next-token prediction**.\n",
    "\n",
    "By the end of this notebook, you will have:\n",
    "- Implemented contrastive loss from scratch and trained a mini-CLIP model\n",
    "- Built an instruction-tuned model and visualized the alignment space\n",
    "- Understood why freezing pretrained components matters"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "!pip install torch torchvision matplotlib numpy -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Think of contrastive learning as a **matching game**. You are given a set of images and a set of text descriptions. Your job is to match each image with its correct description.\n",
    "\n",
    "Before training, the image of a dog might be close to the text \"airplane\" in the embedding space. After training, it should be close to \"a photo of a dog\" and far from everything else.\n",
    "\n",
    "The trick is in the \"far from everything else\" part. You do not just push matching pairs together -- you simultaneously push non-matching pairs apart. This is what makes the embeddings useful: they create a structured space where similar concepts cluster together.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "Why is cosine similarity used instead of Euclidean distance for comparing image and text embeddings? What happens if one encoder produces much larger vectors than the other?"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### 3.1 Cosine Similarity\n",
    "\n",
    "$$\\text{sim}(a, b) = \\frac{a \\cdot b}{\\|a\\| \\|b\\|}$$\n",
    "\n",
    "This measures the angle between two vectors, ignoring their magnitudes. A value of 1 means perfectly aligned, 0 means perpendicular, -1 means opposite.\n",
    "\n",
    "Let us compute: $a = [3, 4]$, $b = [4, 3]$:\n",
    "\n",
    "$$\\text{sim}(a, b) = \\frac{3 \\times 4 + 4 \\times 3}{\\sqrt{9+16} \\times \\sqrt{16+9}} = \\frac{24}{5 \\times 5} = \\frac{24}{25} = 0.96$$\n",
    "\n",
    "These vectors are very similar (96% aligned). This is exactly what we want for a matching image-text pair.\n",
    "\n",
    "### 3.2 Contrastive Loss (InfoNCE)\n",
    "\n",
    "Given a batch of $N$ image-text pairs, the contrastive loss for image $i$ is:\n",
    "\n",
    "$$\\mathcal{L}_i = -\\log \\frac{\\exp(\\text{sim}(v_i, t_i) / \\tau)}{\\sum_{j=1}^{N} \\exp(\\text{sim}(v_i, t_j) / \\tau)}$$\n",
    "\n",
    "The numerator contains the similarity of the **matching pair** $(v_i, t_i)$. The denominator sums over **all pairs**, including non-matching ones. The temperature $\\tau$ controls how sharp the distribution is.\n",
    "\n",
    "Let us compute with $N = 3$, $\\tau = 0.1$:\n",
    "\n",
    "Suppose similarities are: $\\text{sim}(v_1, t_1) = 0.9$ (match), $\\text{sim}(v_1, t_2) = 0.2$, $\\text{sim}(v_1, t_3) = 0.1$:\n",
    "\n",
    "$$\\mathcal{L}_1 = -\\log \\frac{\\exp(0.9 / 0.1)}{\\exp(9) + \\exp(2) + \\exp(1)} = -\\log \\frac{8103.1}{8103.1 + 7.39 + 2.72} = -\\log(0.9988) = 0.0012$$\n",
    "\n",
    "The loss is very small because the matching pair has much higher similarity. This tells us the model is doing a good job distinguishing the correct match.\n",
    "\n",
    "Now consider a bad model where $\\text{sim}(v_1, t_1) = 0.3$, $\\text{sim}(v_1, t_2) = 0.4$, $\\text{sim}(v_1, t_3) = 0.3$:\n",
    "\n",
    "$$\\mathcal{L}_1 = -\\log \\frac{\\exp(3)}{\\exp(3) + \\exp(4) + \\exp(3)} = -\\log \\frac{20.1}{20.1 + 54.6 + 20.1} = -\\log(0.212) = 1.55$$\n",
    "\n",
    "Much higher loss! The model cannot distinguish the matching pair from non-matching ones.\n",
    "\n",
    "### 3.3 Temperature Parameter\n",
    "\n",
    "$\\tau$ controls how \"picky\" the model is. Small $\\tau$ (like 0.07) makes the softmax very sharp -- the model needs high similarity for matching pairs and low for non-matching. Large $\\tau$ (like 1.0) makes it more lenient.\n",
    "\n",
    "With $\\tau = 0.07$ and similarities $[0.8, 0.7, 0.1]$:\n",
    "$$\\text{softmax}([0.8/0.07, 0.7/0.07, 0.1/0.07]) = \\text{softmax}([11.4, 10.0, 1.43]) \\approx [0.80, 0.20, 0.00]$$\n",
    "\n",
    "With $\\tau = 1.0$:\n",
    "$$\\text{softmax}([0.8, 0.7, 0.1]) \\approx [0.38, 0.34, 0.19]$$\n",
    "\n",
    "The small $\\tau$ creates much sharper distinctions. This is exactly what we want for good alignment."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 Creating an Image-Text Paired Dataset"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10PairedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Pairs CIFAR-10 images with text descriptions of their class.\n",
    "    This simulates the image-text pairs used in CLIP training.\n",
    "    \"\"\"\n",
    "    def __init__(self, cifar_dataset):\n",
    "        self.cifar = cifar_dataset\n",
    "        self.class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                           'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "        self.descriptions = [\n",
    "            'a photo of an airplane',\n",
    "            'a photo of an automobile',\n",
    "            'a photo of a bird',\n",
    "            'a photo of a cat',\n",
    "            'a photo of a deer',\n",
    "            'a photo of a dog',\n",
    "            'a photo of a frog',\n",
    "            'a photo of a horse',\n",
    "            'a photo of a ship',\n",
    "            'a photo of a truck',\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cifar)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.cifar[idx]\n",
    "        # Return image, label index (used as text ID), and label\n",
    "        return img, label, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "cifar_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "cifar_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                           download=True, transform=transform)\n",
    "\n",
    "# Use subsets for speed\n",
    "train_paired = CIFAR10PairedDataset(torch.utils.data.Subset(cifar_train, range(5000)))\n",
    "test_paired = CIFAR10PairedDataset(torch.utils.data.Subset(cifar_test, range(1000)))\n",
    "\n",
    "train_loader = DataLoader(train_paired, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_paired, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"Training pairs: {len(train_paired)}\")\n",
    "print(f\"Test pairs: {len(test_paired)}\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Building the Mini-CLIP Model"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniCLIP(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified CLIP model with separate image and text encoders.\n",
    "\n",
    "    The image encoder is a small CNN.\n",
    "    The text encoder is an embedding lookup (since we have 10 classes).\n",
    "    Both produce vectors in the same shared embedding space.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=128, num_classes=10):\n",
    "        super().__init__()\n",
    "        # Image encoder: small CNN\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, embed_dim)\n",
    "        )\n",
    "\n",
    "        # Text encoder: learned embeddings for each class description\n",
    "        self.text_encoder = nn.Embedding(num_classes, embed_dim)\n",
    "\n",
    "        # Learnable temperature (like in CLIP)\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * np.log(1/0.07))\n",
    "\n",
    "    def encode_image(self, images):\n",
    "        \"\"\"Encode images to normalized embeddings.\"\"\"\n",
    "        features = self.image_encoder(images)\n",
    "        return F.normalize(features, dim=-1)\n",
    "\n",
    "    def encode_text(self, text_ids):\n",
    "        \"\"\"Encode text IDs to normalized embeddings.\"\"\"\n",
    "        features = self.text_encoder(text_ids)\n",
    "        return F.normalize(features, dim=-1)\n",
    "\n",
    "    def forward(self, images, text_ids):\n",
    "        \"\"\"\n",
    "        Returns: image_features, text_features, temperature\n",
    "        \"\"\"\n",
    "        img_features = self.encode_image(images)\n",
    "        txt_features = self.encode_text(text_ids)\n",
    "        return img_features, txt_features, self.temperature.exp()\n",
    "\n",
    "model = MiniCLIP().to(device)\n",
    "print(f\"MiniCLIP parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Implementing Contrastive Loss from Scratch"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(img_features, txt_features, temperature):\n",
    "    \"\"\"\n",
    "    Symmetric contrastive loss (InfoNCE).\n",
    "\n",
    "    img_features: (B, D) -- normalized image embeddings\n",
    "    txt_features: (B, D) -- normalized text embeddings\n",
    "    temperature: scalar -- controls sharpness\n",
    "\n",
    "    Loss = (image-to-text loss + text-to-image loss) / 2\n",
    "    \"\"\"\n",
    "    # Compute similarity matrix: (B, B)\n",
    "    # sim[i, j] = cosine similarity between image i and text j\n",
    "    logits = (img_features @ txt_features.T) * temperature\n",
    "\n",
    "    # Labels: the diagonal (image i matches text i)\n",
    "    labels = torch.arange(len(img_features), device=img_features.device)\n",
    "\n",
    "    # Image-to-text loss: for each image, find its matching text\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "\n",
    "    # Text-to-image loss: for each text, find its matching image\n",
    "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "\n",
    "    return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "# Test with dummy data\n",
    "dummy_img = F.normalize(torch.randn(4, 128), dim=-1)\n",
    "dummy_txt = F.normalize(torch.randn(4, 128), dim=-1)\n",
    "loss = contrastive_loss(dummy_img, dummy_txt, temperature=torch.tensor(14.29))\n",
    "print(f\"Dummy contrastive loss: {loss.item():.4f}\")\n",
    "print(f\"Expected: ~log(4) = {np.log(4):.4f} (random embeddings)\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization checkpoint: similarity matrix before training\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_imgs = torch.stack([cifar_train[i][0] for i in range(10)]).to(device)\n",
    "    sample_labels = torch.tensor([cifar_train[i][1] for i in range(10)]).to(device)\n",
    "\n",
    "    img_feat, txt_feat, temp = model(sample_imgs, torch.arange(10).to(device))\n",
    "    sim_matrix = (img_feat @ txt_feat.T).cpu().numpy()\n",
    "\n",
    "class_names = ['airplane', 'auto', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "im = ax.imshow(sim_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "ax.set_xticks(range(10))\n",
    "ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "ax.set_yticks(range(10))\n",
    "ax.set_yticklabels([f'Image {i}' for i in range(10)])\n",
    "ax.set_title('Similarity Matrix BEFORE Training\\n(should be random)', fontsize=14)\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Before training, similarities are random -- no alignment yet!\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO: Implement Hard Negative Mining\n",
    "\n",
    "In the basic contrastive loss, all non-matching pairs are treated equally. But some non-matching pairs are harder than others -- e.g., a cat image vs \"a photo of a dog\" is harder to distinguish than a cat image vs \"a photo of an airplane\"."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss_hard_negatives(img_features, txt_features, temperature, k=3):\n",
    "    \"\"\"\n",
    "    Contrastive loss with hard negative mining.\n",
    "\n",
    "    Instead of using ALL non-matching pairs in the denominator,\n",
    "    only use the top-k hardest negatives (highest similarity non-matches).\n",
    "\n",
    "    Args:\n",
    "        img_features: (B, D)\n",
    "        txt_features: (B, D)\n",
    "        temperature: scalar\n",
    "        k: number of hard negatives to keep\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Compute similarity matrix (B, B)\n",
    "    # Step 2: Create a mask for the diagonal (matching pairs)\n",
    "    # Step 3: For each row, find the top-k non-diagonal similarities\n",
    "    # Step 4: Compute loss using only matching pair + top-k hard negatives\n",
    "    # Hint: Use torch.topk on the non-matching similarities\n",
    "    # ==============================\n",
    "\n",
    "    loss = None  # YOUR CODE HERE\n",
    "    return loss"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "# If implemented, compare losses:\n",
    "if contrastive_loss_hard_negatives(dummy_img, dummy_txt, torch.tensor(14.29)) is not None:\n",
    "    loss_all = contrastive_loss(dummy_img, dummy_txt, torch.tensor(14.29))\n",
    "    loss_hard = contrastive_loss_hard_negatives(dummy_img, dummy_txt, torch.tensor(14.29), k=2)\n",
    "    print(f\"Loss (all negatives): {loss_all.item():.4f}\")\n",
    "    print(f\"Loss (hard negatives only): {loss_hard.item():.4f}\")\n",
    "    print(\"Hard negative loss should be >= all-negatives loss (fewer easy negatives in denominator)\")\n",
    "else:\n",
    "    print(\"TODO: Implement contrastive_loss_hard_negatives above\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implement Temperature Scheduling\n",
    "\n",
    "Instead of a learnable temperature, implement a schedule that starts high (lenient) and decreases during training (more strict)."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemperatureScheduler:\n",
    "    \"\"\"\n",
    "    Linear temperature schedule: starts at tau_start, ends at tau_end.\n",
    "    \"\"\"\n",
    "    def __init__(self, tau_start=1.0, tau_end=0.07, total_steps=1000):\n",
    "        # ============ TODO ============\n",
    "        # Store the start, end, and total steps\n",
    "        # ==============================\n",
    "        pass\n",
    "\n",
    "    def get_temperature(self, step):\n",
    "        \"\"\"\n",
    "        Returns the temperature at a given training step.\n",
    "        Linear interpolation between tau_start and tau_end.\n",
    "        \"\"\"\n",
    "        # ============ TODO ============\n",
    "        # Compute: tau = tau_start + (tau_end - tau_start) * (step / total_steps)\n",
    "        # Clamp between tau_end and tau_start\n",
    "        # ==============================\n",
    "        return None  # YOUR CODE HERE"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "scheduler = TemperatureScheduler(1.0, 0.07, 100)\n",
    "if scheduler.get_temperature(0) is not None:\n",
    "    temps = [scheduler.get_temperature(s) for s in range(101)]\n",
    "    plt.plot(temps)\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Temperature')\n",
    "    plt.title('Temperature Schedule')\n",
    "    plt.show()\n",
    "    print(f\"Start: {temps[0]:.3f}, End: {temps[-1]:.3f}\")\n",
    "else:\n",
    "    print(\"TODO: Implement TemperatureScheduler above\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training pipeline\n",
    "model = MiniCLIP().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "train_losses = []\n",
    "retrieval_accs = []"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the contrastive model\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for imgs, text_ids, labels in train_loader:\n",
    "        imgs, text_ids = imgs.to(device), text_ids.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        img_feat, txt_feat, temp = model(imgs, text_ids)\n",
    "        loss = contrastive_loss(img_feat, txt_feat, temp)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    # Evaluate: zero-shot classification accuracy\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        # Encode all 10 class descriptions\n",
    "        all_text_feat = model.encode_text(torch.arange(10).to(device))  # (10, D)\n",
    "\n",
    "        for imgs, text_ids, labels in test_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            img_feat = model.encode_image(imgs)  # (B, D)\n",
    "\n",
    "            # Find nearest text for each image\n",
    "            sims = img_feat @ all_text_feat.T  # (B, 10)\n",
    "            preds = sims.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    retrieval_accs.append(acc)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        temp_val = model.temperature.exp().item()\n",
    "        print(f\"Epoch {epoch+1:3d}: Loss={avg_loss:.4f}, Acc={acc:.4f}, Temp={temp_val:.2f}\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization checkpoint: training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(train_losses, color='#2196F3', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Contrastive Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(retrieval_accs, color='#4CAF50', linewidth=2)\n",
    "axes[1].axhline(y=0.1, color='gray', linestyle='--', alpha=0.5, label='Random (10%)')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Zero-Shot Classification Accuracy')\n",
    "axes[1].set_title('Zero-Shot Accuracy (no task-specific training!)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"\\nFinal zero-shot accuracy: {retrieval_accs[-1]:.4f}\")\n",
    "print(\"This model classifies images it has never been explicitly trained to classify!\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization checkpoint: similarity matrix AFTER training\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_imgs = torch.stack([cifar_test[i][0] for i in range(10)]).to(device)\n",
    "    sample_labels = torch.tensor([cifar_test[i][1] for i in range(10)])\n",
    "\n",
    "    img_feat = model.encode_image(sample_imgs)\n",
    "    txt_feat = model.encode_text(torch.arange(10).to(device))\n",
    "    sim_matrix = (img_feat @ txt_feat.T).cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "im = ax.imshow(sim_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "ax.set_xticks(range(10))\n",
    "ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "ax.set_yticks(range(10))\n",
    "actual_labels = [class_names[sample_labels[i].item()] for i in range(10)]\n",
    "ax.set_yticklabels([f'{actual_labels[i]}' for i in range(10)])\n",
    "ax.set_title('Similarity Matrix AFTER Training\\n(diagonal should be bright)', fontsize=14)\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"After training, matching image-text pairs have high similarity (bright diagonal)!\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned embedding space with t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "model.eval()\n",
    "all_img_features = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, text_ids, labels in test_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        feat = model.encode_image(imgs)\n",
    "        all_img_features.append(feat.cpu())\n",
    "        all_labels.append(labels)\n",
    "\n",
    "all_img_features = torch.cat(all_img_features).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "# t-SNE reduction\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "features_2d = tsne.fit_transform(all_img_features)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "for c in range(10):\n",
    "    mask = all_labels == c\n",
    "    ax.scatter(features_2d[mask, 0], features_2d[mask, 1],\n",
    "              c=[colors[c]], label=class_names[c], alpha=0.5, s=10)\n",
    "ax.legend(fontsize=9, ncol=2)\n",
    "ax.set_title('Learned Image Embedding Space (t-SNE)\\nImages cluster by semantic category!', fontsize=14)\n",
    "ax.set_xlabel('t-SNE dim 1')\n",
    "ax.set_ylabel('t-SNE dim 2')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot image retrieval demonstration\n",
    "model.eval()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ZERO-SHOT IMAGE RETRIEVAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "\n",
    "for query_idx in range(10):\n",
    "    query_text = f\"a photo of a {class_names[query_idx]}\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        txt_feat = model.encode_text(torch.tensor([query_idx]).to(device))\n",
    "\n",
    "        # Search through test set for best match\n",
    "        best_sim = -1\n",
    "        best_img = None\n",
    "        best_label = None\n",
    "\n",
    "        for imgs, text_ids, labels in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            img_feat = model.encode_image(imgs)\n",
    "            sims = (img_feat @ txt_feat.T).squeeze()\n",
    "            max_sim, max_idx = sims.max(0)\n",
    "\n",
    "            if max_sim > best_sim:\n",
    "                best_sim = max_sim.item()\n",
    "                best_img = imgs[max_idx].cpu()\n",
    "                best_label = labels[max_idx].item()\n",
    "\n",
    "    row, col = query_idx // 5, query_idx % 5\n",
    "    axes[row, col].imshow(best_img.permute(1, 2, 0) * 0.5 + 0.5)\n",
    "    match_str = \"MATCH\" if best_label == query_idx else \"MISS\"\n",
    "    color = 'green' if best_label == query_idx else 'red'\n",
    "    axes[row, col].set_title(\n",
    "        f'Query: \"{query_text}\"\\n'\n",
    "        f'Retrieved: {class_names[best_label]} ({match_str})\\n'\n",
    "        f'Similarity: {best_sim:.3f}',\n",
    "        fontsize=9, color=color\n",
    "    )\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle('Zero-Shot Text-to-Image Retrieval with Mini-CLIP', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCongratulations! You have trained a contrastive model from scratch!\")\n",
    "print(\"This model can retrieve images from text queries without ANY task-specific training.\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "1. The contrastive loss treats all non-matching pairs equally. But \"cat\" vs \"dog\" is harder to distinguish than \"cat\" vs \"airplane.\" How would you modify the loss to account for semantic similarity between classes?\n",
    "2. We used a batch size of 128. CLIP uses batch sizes of 32,768. Why does larger batch size help contrastive learning? (Hint: think about the number of negative examples.)\n",
    "3. After contrastive pretraining, we get zero-shot classification \"for free.\" What are the limitations of this zero-shot approach compared to supervised fine-tuning?\n",
    "\n",
    "### Optional Challenges\n",
    "1. Implement the hard negative mining TODO above and compare retrieval accuracy with and without hard negatives.\n",
    "2. Add data augmentation (random crops, color jitter) to the image encoder and measure the effect on alignment quality.\n",
    "3. Replace the text embedding lookup with a character-level model that can handle novel text queries (not just the 10 class descriptions)."
   ],
   "id": "cell_28"
  }
 ]
}