{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Building LLaVA and Flamingo from Scratch \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LLaVA and Flamingo from Scratch\n",
    "\n",
    "*Part 2 of the Vizuara series on Multimodal Fusion Architectures*\n",
    "*Estimated time: 60 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebook, we compared three fusion strategies on a synthetic task. Now we are going to build the two most influential vision-language architectures from scratch: **LLaVA** and **Flamingo**.\n",
    "\n",
    "LLaVA answers the question: what is the simplest way to make an LLM see? Flamingo answers a different question: how do you inject vision into a frozen LLM without breaking it?\n",
    "\n",
    "By the end of this notebook, you will have:\n",
    "- Built a simplified LLaVA model that processes images and generates text\n",
    "- Built a Flamingo-style gated cross-attention module\n",
    "- Compared both architectures on a visual question answering task with CIFAR-10 images"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "!pip install torch torchvision matplotlib numpy -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Let us think about LLaVA and Flamingo through a classroom analogy.\n",
    "\n",
    "**LLaVA is like a student who takes notes on a lecture slide before class discussion.** The student (LLM) first looks at the slide (image), converts it into personal notes (visual tokens via projection), and then discusses the topic with those notes integrated right into the conversation transcript.\n",
    "\n",
    "**Flamingo is like a student who can glance at a reference book during an exam.** The student (frozen LLM) takes the exam (processes text) normally, but at certain checkpoints, they are allowed to glance at a reference book (visual tokens via gated cross-attention). The glancing is controlled -- at first they barely look, and gradually they learn which parts of the book are helpful.\n",
    "\n",
    "The key difference: LLaVA changes the input (longer sequence), while Flamingo changes the architecture (adds cross-attention layers).\n",
    "\n",
    "### Think About This\n",
    "\n",
    "If you wanted to add vision to an existing LLM without retraining the entire model, which approach would be cheaper? Which would preserve more of the LLM's original language ability?"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### 3.1 LLaVA Forward Pass\n",
    "\n",
    "$$y = \\text{LLM}\\left([W \\cdot \\text{ViT}(I) \\;;\\; \\text{Embed}(T)]\\right)$$\n",
    "\n",
    "Let us trace the dimensions. If $\\text{ViT}(I) \\in \\mathbb{R}^{P \\times d_v}$ (P patches, each $d_v$-dimensional), and $W \\in \\mathbb{R}^{d_{LLM} \\times d_v}$, then:\n",
    "\n",
    "$$W \\cdot \\text{ViT}(I) \\in \\mathbb{R}^{P \\times d_{LLM}}$$\n",
    "\n",
    "After concatenation with text embeddings $\\text{Embed}(T) \\in \\mathbb{R}^{N \\times d_{LLM}}$:\n",
    "\n",
    "$$[W \\cdot \\text{ViT}(I) ; \\text{Embed}(T)] \\in \\mathbb{R}^{(P+N) \\times d_{LLM}}$$\n",
    "\n",
    "**Computational meaning:** We project visual patches into the same dimensional space as text tokens, then treat them as extra tokens. The LLM sees a longer sequence but does not know (or care) which tokens are visual and which are textual.\n",
    "\n",
    "Let us plug in concrete numbers. Suppose $P = 4$ patches, $d_v = 8$, $d_{LLM} = 16$, $N = 3$ text tokens:\n",
    "- ViT output: $4 \\times 8$ matrix\n",
    "- After projection: $4 \\times 16$ matrix\n",
    "- Text embeddings: $3 \\times 16$ matrix\n",
    "- Concatenated: $7 \\times 16$ matrix (7 tokens total, 16 dims each)\n",
    "\n",
    "This is exactly what we want -- the LLM now processes a unified 7-token sequence.\n",
    "\n",
    "### 3.2 Flamingo Gated Cross-Attention\n",
    "\n",
    "$$h' = h + \\tanh(\\alpha) \\cdot \\text{CrossAttn}(h, v)$$\n",
    "\n",
    "Here $\\alpha$ starts at 0, so $\\tanh(0) = 0$, meaning no visual information flows at first. As training progresses, $\\alpha$ moves away from zero.\n",
    "\n",
    "Let us compute with $\\alpha = 0.5$, $h = [1.0, 2.0]$, and $\\text{CrossAttn}(h, v) = [0.3, -0.1]$:\n",
    "\n",
    "$$h' = [1.0, 2.0] + \\tanh(0.5) \\cdot [0.3, -0.1]$$\n",
    "$$= [1.0, 2.0] + 0.462 \\cdot [0.3, -0.1]$$\n",
    "$$= [1.0, 2.0] + [0.139, -0.046]$$\n",
    "$$= [1.139, 1.954]$$\n",
    "\n",
    "This tells us that at $\\alpha = 0.5$, about 46% of the cross-attention signal passes through. The visual information gently modifies the LLM hidden states. This is exactly what we want -- a smooth, controlled injection."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 A Simple Vision Encoder\n",
    "\n",
    "First, we need something to convert images into visual tokens. We will build a simplified patch-based vision encoder (inspired by ViT but much smaller)."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePatchEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified vision encoder that splits an image into patches\n",
    "    and processes them into visual tokens.\n",
    "\n",
    "    For a 32x32 CIFAR image with patch_size=8:\n",
    "    - We get 4x4 = 16 patches\n",
    "    - Each patch is 8x8x3 = 192 pixels\n",
    "    - Projected to embed_dim dimensions\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=8, in_channels=3, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Linear projection of flattened patches\n",
    "        patch_dim = patch_size * patch_size * in_channels\n",
    "        self.patch_proj = nn.Linear(patch_dim, embed_dim)\n",
    "\n",
    "        # Learnable position embeddings\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches, embed_dim) * 0.02)\n",
    "\n",
    "        # A small transformer layer for inter-patch reasoning\n",
    "        self.transformer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=4, dim_feedforward=embed_dim*2,\n",
    "            batch_first=True, dropout=0.1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x: (B, C, H, W) -> (B, num_patches, embed_dim)\"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        p = self.patch_size\n",
    "\n",
    "        # Extract patches: (B, C, H, W) -> (B, num_patches, patch_dim)\n",
    "        patches = x.unfold(2, p, p).unfold(3, p, p)  # (B, C, H/p, W/p, p, p)\n",
    "        patches = patches.contiguous().view(B, C, -1, p, p)  # (B, C, num_patches, p, p)\n",
    "        patches = patches.permute(0, 2, 1, 3, 4)  # (B, num_patches, C, p, p)\n",
    "        patches = patches.reshape(B, self.num_patches, -1)  # (B, num_patches, patch_dim)\n",
    "\n",
    "        # Project patches to embedding dim and add position embeddings\n",
    "        tokens = self.patch_proj(patches) + self.pos_embed\n",
    "\n",
    "        # Process with transformer\n",
    "        tokens = self.transformer(tokens)\n",
    "\n",
    "        return tokens  # (B, num_patches, embed_dim)\n",
    "\n",
    "# Test the encoder\n",
    "encoder = SimplePatchEncoder()\n",
    "dummy_img = torch.randn(2, 3, 32, 32)\n",
    "visual_tokens = encoder(dummy_img)\n",
    "print(f\"Input image: {dummy_img.shape}\")\n",
    "print(f\"Visual tokens: {visual_tokens.shape}\")\n",
    "print(f\"Number of patches: {encoder.num_patches}\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization checkpoint: what do patches look like?\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "cifar = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "img, label = cifar[0]\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "\n",
    "# Show original image\n",
    "axes[0].imshow(img.permute(1, 2, 0))\n",
    "axes[0].set_title(f'Original: {class_names[label]}')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Show 4 patches\n",
    "p = 8\n",
    "patches = img.unfold(1, p, p).unfold(2, p, p)\n",
    "patches = patches.permute(1, 2, 0, 3, 4).reshape(-1, 3, p, p)\n",
    "for i in range(4):\n",
    "    axes[i+1].imshow(patches[i].permute(1, 2, 0))\n",
    "    axes[i+1].set_title(f'Patch {i}')\n",
    "    axes[i+1].axis('off')\n",
    "\n",
    "plt.suptitle('Image split into patches (the visual tokens)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 A Simple Text Encoder and Decoder\n",
    "\n",
    "For our task, we will use a simple vocabulary for visual question answering on CIFAR-10."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTextProcessor(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple text processor with a small vocabulary for VQA on CIFAR-10.\n",
    "\n",
    "    Vocabulary: class names + special tokens + question words\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=64, max_seq_len=20):\n",
    "        super().__init__()\n",
    "        # Build vocabulary\n",
    "        self.vocab = ['<pad>', '<bos>', '<eos>', '<unk>',\n",
    "                      'what', 'is', 'this', 'in', 'the', 'image', '?',\n",
    "                      'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                      'dog', 'frog', 'horse', 'ship', 'truck',\n",
    "                      'a', 'an', 'it', 'of', 'photo']\n",
    "        self.word2idx = {w: i for i, w in enumerate(self.vocab)}\n",
    "        self.idx2word = {i: w for w, i in self.word2idx.items()}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_seq_len, embed_dim) * 0.02)\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert text string to token IDs.\"\"\"\n",
    "        words = text.lower().split()\n",
    "        ids = [self.word2idx.get(w, self.word2idx['<unk>']) for w in words]\n",
    "        return torch.tensor(ids)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"token_ids: (B, N) -> (B, N, embed_dim)\"\"\"\n",
    "        B, N = token_ids.shape\n",
    "        embeds = self.embedding(token_ids) + self.pos_embed[:, :N, :]\n",
    "        return embeds\n",
    "\n",
    "text_proc = SimpleTextProcessor()\n",
    "print(f\"Vocabulary size: {text_proc.vocab_size}\")\n",
    "print(f\"Example encoding: 'what is this' -> {text_proc.encode('what is this')}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Building LLaVA (Simplified)\n",
    "\n",
    "Now let us build the complete LLaVA architecture. The key is the **projection layer** that maps visual tokens to the LLM's dimension."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLLaVA(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified LLaVA architecture:\n",
    "    1. Image -> Patch Encoder -> Visual tokens (P x d_v)\n",
    "    2. Visual tokens -> Projection -> Visual tokens (P x d_llm)\n",
    "    3. [Visual tokens ; Text tokens] -> Transformer -> Output\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=8, embed_dim=64, num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Vision encoder\n",
    "        self.vision_encoder = SimplePatchEncoder(img_size, patch_size, embed_dim=embed_dim)\n",
    "\n",
    "        # Visual projection (in real LLaVA, this maps ViT dim -> LLM dim)\n",
    "        self.visual_proj = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        # Text processor\n",
    "        self.text_proc = SimpleTextProcessor(embed_dim=embed_dim)\n",
    "\n",
    "        # Transformer (acts as the \"LLM\")\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads,\n",
    "            dim_feedforward=embed_dim*4, batch_first=True, dropout=0.1\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output head: classify into CIFAR-10 classes\n",
    "        self.classifier = nn.Linear(embed_dim, 10)\n",
    "\n",
    "    def forward(self, images, text_ids):\n",
    "        \"\"\"\n",
    "        images: (B, 3, 32, 32)\n",
    "        text_ids: (B, N)\n",
    "        \"\"\"\n",
    "        # Step 1-2: Encode and project visual tokens\n",
    "        vis_tokens = self.vision_encoder(images)        # (B, P, embed_dim)\n",
    "        vis_tokens = self.visual_proj(vis_tokens)        # (B, P, embed_dim)\n",
    "\n",
    "        # Step 3: Encode text tokens\n",
    "        txt_tokens = self.text_proc(text_ids)            # (B, N, embed_dim)\n",
    "\n",
    "        # Step 4: Concatenate [visual ; text]\n",
    "        combined = torch.cat([vis_tokens, txt_tokens], dim=1)  # (B, P+N, embed_dim)\n",
    "\n",
    "        # Step 5: Process through transformer\n",
    "        hidden = self.transformer(combined)              # (B, P+N, embed_dim)\n",
    "\n",
    "        # Pool over all tokens and classify\n",
    "        pooled = hidden.mean(dim=1)                      # (B, embed_dim)\n",
    "        logits = self.classifier(pooled)                 # (B, 10)\n",
    "\n",
    "        return logits\n",
    "\n",
    "llava_model = SimpleLLaVA().to(device)\n",
    "print(f\"LLaVA parameters: {sum(p.numel() for p in llava_model.parameters()):,}\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Building Flamingo (Simplified)\n",
    "\n",
    "Now let us build Flamingo. The key innovations are the **Perceiver Resampler** and **Gated Cross-Attention**."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverResampler(nn.Module):\n",
    "    \"\"\"\n",
    "    Perceiver Resampler: compresses variable-length visual tokens\n",
    "    into a fixed number of \"summary\" tokens.\n",
    "\n",
    "    Uses a set of learned queries that attend to the visual tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=64, num_queries=4, num_heads=4):\n",
    "        super().__init__()\n",
    "        # Learnable query tokens\n",
    "        self.queries = nn.Parameter(torch.randn(1, num_queries, embed_dim) * 0.02)\n",
    "\n",
    "        # Cross-attention: queries attend to visual tokens\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim, num_heads, batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 2, embed_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, visual_tokens):\n",
    "        \"\"\"\n",
    "        visual_tokens: (B, P, embed_dim) -- variable length P\n",
    "        Returns: (B, num_queries, embed_dim) -- fixed length\n",
    "        \"\"\"\n",
    "        B = visual_tokens.shape[0]\n",
    "        queries = self.queries.expand(B, -1, -1)  # (B, num_queries, embed_dim)\n",
    "\n",
    "        # Cross-attention: queries attend to visual tokens\n",
    "        attn_out, _ = self.cross_attn(queries, visual_tokens, visual_tokens)\n",
    "        x = self.norm(queries + attn_out)\n",
    "        x = self.norm2(x + self.ffn(x))\n",
    "\n",
    "        return x  # (B, num_queries, embed_dim)\n",
    "\n",
    "# Test\n",
    "resampler = PerceiverResampler()\n",
    "vis = torch.randn(2, 16, 64)  # 16 visual tokens\n",
    "summary = resampler(vis)\n",
    "print(f\"Visual tokens: {vis.shape} -> Summary tokens: {summary.shape}\")\n",
    "print(f\"Compressed {vis.shape[1]} tokens into {summary.shape[1]} summary tokens!\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedCrossAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Gated Cross-Attention: the core Flamingo innovation.\n",
    "\n",
    "    h' = h + tanh(alpha) * CrossAttn(h, v)\n",
    "\n",
    "    alpha starts at 0, so tanh(0) = 0: no visual info at first.\n",
    "    As training progresses, alpha learns to open the gate.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=64, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim, num_heads, batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        # THE KEY: gate initialized to zero\n",
    "        self.gate = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, text_hidden, visual_summary):\n",
    "        \"\"\"\n",
    "        text_hidden: (B, N, embed_dim) -- LLM hidden states\n",
    "        visual_summary: (B, K, embed_dim) -- from Perceiver Resampler\n",
    "        \"\"\"\n",
    "        # Cross-attention: text queries, visual keys/values\n",
    "        attn_out, attn_weights = self.cross_attn(\n",
    "            self.norm(text_hidden), visual_summary, visual_summary\n",
    "        )\n",
    "\n",
    "        # Gated residual connection\n",
    "        gate_value = torch.tanh(self.gate)\n",
    "        output = text_hidden + gate_value * attn_out\n",
    "\n",
    "        return output, gate_value.item()\n",
    "\n",
    "\n",
    "class SimpleFlamingo(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Flamingo architecture:\n",
    "    1. Image -> Patch Encoder -> Visual tokens\n",
    "    2. Visual tokens -> Perceiver Resampler -> Visual summary (fixed K tokens)\n",
    "    3. Text -> Embedding -> Text tokens\n",
    "    4. Text tokens processed by Transformer layers with Gated Cross-Attention\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=8, embed_dim=64,\n",
    "                 num_heads=4, num_layers=2, num_visual_queries=4):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Vision encoder (frozen in real Flamingo)\n",
    "        self.vision_encoder = SimplePatchEncoder(img_size, patch_size, embed_dim=embed_dim)\n",
    "\n",
    "        # Perceiver Resampler\n",
    "        self.resampler = PerceiverResampler(embed_dim, num_visual_queries, num_heads)\n",
    "\n",
    "        # Text processor\n",
    "        self.text_proc = SimpleTextProcessor(embed_dim=embed_dim)\n",
    "\n",
    "        # Interleaved: Transformer layer -> Gated Cross-Attention -> Transformer layer -> ...\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.cross_attn_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(nn.TransformerEncoderLayer(\n",
    "                d_model=embed_dim, nhead=num_heads,\n",
    "                dim_feedforward=embed_dim*4, batch_first=True, dropout=0.1\n",
    "            ))\n",
    "            self.cross_attn_layers.append(GatedCrossAttentionLayer(embed_dim, num_heads))\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(embed_dim, 10)\n",
    "\n",
    "    def forward(self, images, text_ids):\n",
    "        # Step 1: Encode image -> visual tokens -> summary\n",
    "        vis_tokens = self.vision_encoder(images)\n",
    "        vis_summary = self.resampler(vis_tokens)  # (B, K, embed_dim)\n",
    "\n",
    "        # Step 2: Encode text\n",
    "        txt_hidden = self.text_proc(text_ids)     # (B, N, embed_dim)\n",
    "\n",
    "        # Step 3: Interleaved processing\n",
    "        gate_values = []\n",
    "        for transformer_layer, cross_attn_layer in zip(self.layers, self.cross_attn_layers):\n",
    "            txt_hidden = transformer_layer(txt_hidden)\n",
    "            txt_hidden, gate_val = cross_attn_layer(txt_hidden, vis_summary)\n",
    "            gate_values.append(gate_val)\n",
    "\n",
    "        # Pool and classify\n",
    "        pooled = txt_hidden.mean(dim=1)\n",
    "        logits = self.classifier(pooled)\n",
    "\n",
    "        self.last_gate_values = gate_values\n",
    "        return logits\n",
    "\n",
    "flamingo_model = SimpleFlamingo().to(device)\n",
    "print(f\"Flamingo parameters: {sum(p.numel() for p in flamingo_model.parameters()):,}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization checkpoint: gate values at initialization\n",
    "dummy_img = torch.randn(1, 3, 32, 32).to(device)\n",
    "dummy_text = torch.tensor([[4, 5, 6]]).to(device)  # \"what is this\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = flamingo_model(dummy_img, dummy_text)\n",
    "\n",
    "print(\"Gate values at initialization:\")\n",
    "for i, gv in enumerate(flamingo_model.last_gate_values):\n",
    "    print(f\"  Layer {i}: tanh(alpha) = {gv:.6f}\")\n",
    "print(\"\\nAll gates are near zero -- the model starts as a pure text model!\")\n",
    "print(\"Visual information will gradually flow in during training.\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO: Implement Multi-Image Flamingo\n",
    "\n",
    "Real Flamingo can process multiple images. Modify the Flamingo architecture to accept a list of images and produce a visual summary that combines all of them."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiImageFlamingo(nn.Module):\n",
    "    \"\"\"\n",
    "    Extended Flamingo that processes multiple images per input.\n",
    "\n",
    "    Instead of a single image, accepts a list of images.\n",
    "    Each image gets its own visual tokens, then all are\n",
    "    concatenated before the Perceiver Resampler.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=8, embed_dim=64,\n",
    "                 num_heads=4, num_visual_queries=8):\n",
    "        super().__init__()\n",
    "        # ============ TODO ============\n",
    "        # Step 1: Create a SimplePatchEncoder\n",
    "        # Step 2: Create a PerceiverResampler with more queries\n",
    "        #         (since we have more visual tokens from multiple images)\n",
    "        # Step 3: Create a GatedCrossAttentionLayer\n",
    "        # ==============================\n",
    "\n",
    "        self.vision_encoder = None   # YOUR CODE HERE\n",
    "        self.resampler = None        # YOUR CODE HERE\n",
    "        self.cross_attn = None       # YOUR CODE HERE\n",
    "\n",
    "    def encode_multiple_images(self, images_list):\n",
    "        \"\"\"\n",
    "        images_list: list of (B, C, H, W) tensors\n",
    "\n",
    "        Returns: (B, total_patches, embed_dim) -- all visual tokens concatenated\n",
    "        \"\"\"\n",
    "        # ============ TODO ============\n",
    "        # For each image in images_list:\n",
    "        #   1. Encode with vision_encoder\n",
    "        #   2. Collect all visual tokens\n",
    "        # Concatenate all visual tokens along sequence dim\n",
    "        # ==============================\n",
    "\n",
    "        return None  # YOUR CODE HERE"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "if MultiImageFlamingo().vision_encoder is not None:\n",
    "    multi = MultiImageFlamingo()\n",
    "    imgs = [torch.randn(2, 3, 32, 32), torch.randn(2, 3, 32, 32)]\n",
    "    all_tokens = multi.encode_multiple_images(imgs)\n",
    "    expected_patches = multi.vision_encoder.num_patches * 2\n",
    "    assert all_tokens.shape == (2, expected_patches, 64), f\"Expected (2, {expected_patches}, 64)\"\n",
    "    print(f\"Correct! {len(imgs)} images -> {all_tokens.shape[1]} visual tokens\")\n",
    "else:\n",
    "    print(\"TODO: Implement MultiImageFlamingo above\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implement a Visual Projection with MLP (LLaVA-1.5)\n",
    "\n",
    "LLaVA-1.5 upgraded the single linear projection to a 2-layer MLP with GELU activation. Implement this and compare."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPProjection(nn.Module):\n",
    "    \"\"\"\n",
    "    LLaVA-1.5 style MLP projection:\n",
    "    visual_tokens -> Linear -> GELU -> Linear -> projected_tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        # ============ TODO ============\n",
    "        # Create a 2-layer MLP with GELU activation between layers\n",
    "        # Hint: nn.Sequential(Linear, GELU, Linear)\n",
    "        # ==============================\n",
    "        self.mlp = None  # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ============ TODO ============\n",
    "        return None  # YOUR CODE HERE"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "if MLPProjection(64, 64).mlp is not None:\n",
    "    proj = MLPProjection(64, 128)\n",
    "    x = torch.randn(2, 16, 64)\n",
    "    out = proj(x)\n",
    "    assert out.shape == (2, 16, 128), f\"Expected (2, 16, 128), got {out.shape}\"\n",
    "    print(\"Correct! MLP projection works.\")\n",
    "else:\n",
    "    print(\"TODO: Implement MLPProjection above\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us prepare the CIFAR-10 dataset for our VQA task."
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare CIFAR-10 data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                         download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "# Use subset for speed\n",
    "train_subset = torch.utils.data.Subset(trainset, range(5000))\n",
    "test_subset = torch.utils.data.Subset(testset, range(1000))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "# The \"question\" is always \"what is this\" encoded as token IDs\n",
    "question = torch.tensor([4, 5, 6]).unsqueeze(0)  # \"what is this\"\n",
    "print(f\"Question tokens: {question}\")\n",
    "print(f\"Training on {len(train_subset)} images, testing on {len(test_subset)}\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vqa(model, train_loader, test_loader, question, epochs=15, lr=1e-3):\n",
    "    \"\"\"Train a VQA model (either LLaVA or Flamingo).\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses, test_accs = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            q = question.expand(imgs.size(0), -1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(imgs, q)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in test_loader:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                q = question.expand(imgs.size(0), -1).to(device)\n",
    "                logits = model(imgs, q)\n",
    "                _, pred = logits.max(1)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        test_accs.append(correct / total)\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"  Epoch {epoch+1:3d}: Loss={train_losses[-1]:.4f}, Acc={test_accs[-1]:.4f}\")\n",
    "\n",
    "    return train_losses, test_accs\n",
    "\n",
    "print(\"Training LLaVA...\")\n",
    "llava_model = SimpleLLaVA().to(device)\n",
    "llava_losses, llava_accs = train_vqa(llava_model, train_loader, test_loader, question)\n",
    "\n",
    "print(\"\\nTraining Flamingo...\")\n",
    "flamingo_model = SimpleFlamingo().to(device)\n",
    "flamingo_losses, flamingo_accs = train_vqa(flamingo_model, train_loader, test_loader, question)"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization checkpoint: training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(llava_losses, label='LLaVA', color='#2196F3', linewidth=2)\n",
    "axes[0].plot(flamingo_losses, label='Flamingo', color='#FF5722', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('Training Loss: LLaVA vs Flamingo')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(llava_accs, label='LLaVA', color='#2196F3', linewidth=2)\n",
    "axes[1].plot(flamingo_accs, label='Flamingo', color='#FF5722', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Test Accuracy')\n",
    "axes[1].set_title('Test Accuracy: LLaVA vs Flamingo')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Accuracies:\")\n",
    "print(f\"  LLaVA:    {llava_accs[-1]:.4f}\")\n",
    "print(f\"  Flamingo: {flamingo_accs[-1]:.4f}\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track Flamingo gate values during training\n",
    "print(\"\\nFlamingo Gate Values (after training):\")\n",
    "flamingo_model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_img = torch.randn(1, 3, 32, 32).to(device)\n",
    "    sample_q = question.to(device)\n",
    "    _ = flamingo_model(sample_img, sample_q)\n",
    "\n",
    "for i, gv in enumerate(flamingo_model.last_gate_values):\n",
    "    bar = '#' * int(abs(gv) * 50)\n",
    "    print(f\"  Layer {i}: tanh(alpha) = {gv:+.4f} |{bar}|\")\n",
    "print(\"\\nThe gates have opened! Visual info is now flowing through the model.\")"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test images\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "\n",
    "llava_model.eval()\n",
    "flamingo_model.eval()\n",
    "\n",
    "for idx in range(10):\n",
    "    img, label = test_subset[idx]\n",
    "    img_input = img.unsqueeze(0).to(device)\n",
    "    q = question.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        llava_pred = llava_model(img_input, q).argmax(1).item()\n",
    "        flamingo_pred = flamingo_model(img_input, q).argmax(1).item()\n",
    "\n",
    "    row, col = idx // 5, idx % 5\n",
    "    axes[row, col].imshow(img.permute(1, 2, 0) * 0.5 + 0.5)  # Unnormalize\n",
    "    axes[row, col].set_title(\n",
    "        f'True: {class_names[label]}\\n'\n",
    "        f'LLaVA: {class_names[llava_pred]}\\n'\n",
    "        f'Flamingo: {class_names[flamingo_pred]}',\n",
    "        fontsize=9\n",
    "    )\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle('LLaVA vs Flamingo Predictions on CIFAR-10', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Congratulations! You have built both LLaVA and Flamingo from scratch!\")"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "1. LLaVA's sequence length grows with the number of visual patches. What happens if we use a higher-resolution image (more patches)? How does this affect memory and compute?\n",
    "2. Flamingo's gate starts at zero. Why is this important? What would happen if we initialized it to a large value?\n",
    "3. The Perceiver Resampler compresses 16 visual tokens to 4 summary tokens. What information might be lost? For what tasks would this matter most?\n",
    "\n",
    "### Optional Challenges\n",
    "1. Replace the `SimplePatchEncoder` with a pretrained ResNet backbone. Compare accuracy.\n",
    "2. Implement the Perceiver Resampler with multiple cross-attention layers (as in the real Flamingo paper).\n",
    "3. Add a text generation head (instead of classification) and generate captions for CIFAR images."
   ],
   "id": "cell_31"
  }
 ]
}