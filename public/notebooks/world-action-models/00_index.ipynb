{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "World Action Models \u2014 Notebook Series Index \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\ude80 World Action Models \u2014 Notebook Series\n",
    "\n",
    "## How AI Learns to Imagine Before It Acts\n",
    "\n",
    "*A Vizuara learning series based on the article: \"World Action Models: How AI Learns to Imagine Before It Acts\"*\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to this series of six hands-on Google Colab notebooks! Together, they take you on a journey from the fundamentals of world models to the cutting edge of embodied AI \u2014 Vision-Language-Action models that can see, understand, and act in the physical world.\n",
    "\n",
    "Every notebook is designed to be run independently in Google Colab with a T4 GPU. Each one builds on the concepts from the previous notebooks, but includes enough context to stand alone."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcda The Learning Path\n",
    "\n",
    "### Notebook 1: World Models from First Principles\n",
    "**Estimated time:** ~25 minutes | **Prerequisites:** Basic PyTorch\n",
    "\n",
    "- What is a world model? The formal definition: $s_{t+1} = f(s_t, a_t)$\n",
    "- Model-free vs model-based reinforcement learning\n",
    "- Build a neural network world model for CartPole\n",
    "- Implement a random shooting planner that uses imagination to act\n",
    "- **Final output:** Planning agent vs random agent comparison\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook 2: Ha & Schmidhuber World Models \u2014 Teaching Agents to Dream\n",
    "**Estimated time:** ~40 minutes | **Prerequisites:** Notebook 1, basic VAE knowledge\n",
    "\n",
    "- The V-M-C architecture: Vision (VAE) + Memory (MDN-RNN) + Controller\n",
    "- The reparameterization trick and probabilistic state prediction\n",
    "- Dream training \u2014 training a controller entirely inside the world model's imagination\n",
    "- **Final output:** Real environment vs agent's dream trajectory comparison\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook 3: DreamerV3 \u2014 Imagination-Based RL with the RSSM\n",
    "**Estimated time:** ~45 minutes | **Prerequisites:** Notebooks 1-2\n",
    "\n",
    "- The Recurrent State-Space Model: deterministic + stochastic paths\n",
    "- Why the world is both predictable AND unpredictable\n",
    "- Imagination-based actor-critic training\n",
    "- **Final output:** RSSM imagining CartPole trajectories, real vs imagined comparison\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook 4: JEPA \u2014 Predicting in Abstract Space\n",
    "**Estimated time:** ~40 minutes | **Prerequisites:** Basic knowledge of transformers\n",
    "\n",
    "- Yann LeCun's insight: predict what matters, ignore irrelevant details\n",
    "- I-JEPA architecture: context encoder, target encoder, predictor\n",
    "- EMA updates to prevent representation collapse\n",
    "- **Final output:** t-SNE of learned representations, linear probe accuracy on CIFAR-10\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook 5: Genie \u2014 From a Single Image to an Interactive World\n",
    "**Estimated time:** ~40 minutes | **Prerequisites:** Notebook 4 recommended\n",
    "\n",
    "- World models as generators of entire interactive environments\n",
    "- Learning actions from unlabeled video \u2014 no action labels needed!\n",
    "- VQ-VAE tokenizer, Latent Action Model, Dynamics Model\n",
    "- **Final output:** Interactive world generation \u2014 same start frame, different action sequences\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook 6: Vision-Language-Action Models \u2014 See, Understand, Act\n",
    "**Estimated time:** ~45 minutes | **Prerequisites:** Notebooks 1-5 recommended\n",
    "\n",
    "- The convergence: vision + language + action in one model\n",
    "- Flow matching for smooth action trajectory generation\n",
    "- Physical Intelligence's \u03c00 and Meta's V-JEPA 2\n",
    "- **Final output:** VLA generating robot trajectories from language commands"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\uddfa\ufe0f How These Notebooks Connect\n",
    "\n",
    "```\n",
    "Notebook 1: World Models          \u2500\u2500\u25ba \"What if agents could imagine?\"\n",
    "    \u2502\n",
    "    \u25bc\n",
    "Notebook 2: V-M-C (Dream)        \u2500\u2500\u25ba \"Train inside your own dream\"\n",
    "    \u2502\n",
    "    \u25bc\n",
    "Notebook 3: DreamerV3 (RSSM)     \u2500\u2500\u25ba \"Scale imagination to 150+ tasks\"\n",
    "    \u2502\n",
    "    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    \u25bc                     \u25bc\n",
    "Notebook 4: JEPA          Notebook 5: Genie\n",
    "\"Predict abstractly\"      \"Generate entire worlds\"\n",
    "    \u2502                     \u2502\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "              \u25bc\n",
    "Notebook 6: VLA Models    \u2500\u2500\u25ba \"See, understand, and act\"\n",
    "```"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 Setup Instructions\n",
    "\n",
    "All notebooks are designed to run in **Google Colab** with a **T4 GPU**. To get started:\n",
    "\n",
    "1. Open any notebook in Google Colab\n",
    "2. Go to **Runtime \u2192 Change runtime type \u2192 GPU (T4)**\n",
    "3. Run the first setup cell to verify GPU access\n",
    "4. Follow along from top to bottom!\n",
    "\n",
    "Each notebook installs its own dependencies. No local setup is needed."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcd6 About This Series\n",
    "\n",
    "This notebook series accompanies the Vizuara Substack article: **\"World Action Models: How AI Learns to Imagine Before It Acts.\"**\n",
    "\n",
    "The article covers the conceptual landscape; these notebooks give you hands-on experience building each system from scratch. By the end, you will have implemented:\n",
    "\n",
    "- A world model that predicts physics\n",
    "- A VAE + RNN that enables dreaming\n",
    "- An RSSM with imagination-based RL\n",
    "- An I-JEPA that learns without pixel reconstruction\n",
    "- A Genie-style interactive world generator\n",
    "- A VLA model with flow matching\n",
    "\n",
    "Happy learning! \ud83c\udf93\n",
    "\n",
    "*\u2014 Vizuara AI*"
   ],
   "id": "cell_5"
  }
 ]
}