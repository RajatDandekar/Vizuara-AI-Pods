{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "DreamerV3: Imagination-Based RL with the RSSM â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ DreamerV3: Imagination-Based Reinforcement Learning with the RSSM\n",
    "\n",
    "**Notebook 3 of 6 â€” World Action Models Series | Vizuara AI**\n",
    "\n",
    "*Estimated time: ~45 minutes*\n",
    "\n",
    "In this notebook, we will build a Recurrent State-Space Model (RSSM) from scratch, train it as a world model on CartPole, and then train an actor-critic agent entirely inside the world model's imagination â€” no real environment needed during policy learning."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup and Dependencies ---\n",
    "!pip install gymnasium -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In 2023, a single reinforcement learning algorithm mastered over **150 diverse tasks** â€” from Atari games to continuous control to 3D navigation â€” with one set of hyperparameters.\n",
    "\n",
    "That algorithm was **DreamerV3**.\n",
    "\n",
    "Even more remarkably, DreamerV3 was the first algorithm to **collect diamonds in Minecraft from scratch** â€” a task requiring a long chain of subgoals (chop trees, craft planks, build a crafting table, make a wooden pickaxe, mine stone, and so on) with only a sparse reward at the very end.\n",
    "\n",
    "The secret? DreamerV3 does not learn by endlessly interacting with the real environment. Instead, it builds an internal model of the world and then **trains its policy entirely inside its own imagination.**\n",
    "\n",
    "By the end of this notebook, you will build an RSSM world model that can imagine future trajectories, and then train an actor-critic agent inside those imaginations â€” all from scratch in PyTorch."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### From Simple RNNs to the RSSM\n",
    "\n",
    "In the previous notebook, we saw the original World Model by Ha and Schmidhuber (2018). That model used a simple RNN to predict future latent states. The idea was brilliant â€” but there was a fundamental limitation.\n",
    "\n",
    "The real world has **both predictable parts and random parts.**\n",
    "\n",
    "Let us think about this with an analogy.\n",
    "\n",
    "**Weather forecasting.** Some aspects of weather are highly predictable: the sun will rise tomorrow, summer will be warmer than winter, and high pressure systems move in predictable patterns. A deterministic model captures these regularities beautifully.\n",
    "\n",
    "But other aspects are genuinely random: will it rain at 3:47 PM on Tuesday? Will a sudden gust of wind knock over your umbrella? These stochastic elements cannot be captured by a purely deterministic model â€” no matter how large we make it.\n",
    "\n",
    "The RSSM captures both.\n",
    "\n",
    "It maintains two parallel streams of information:\n",
    "1. A **deterministic path** (like a GRU) that captures the predictable, consistent dynamics\n",
    "2. A **stochastic path** that captures the randomness and uncertainty\n",
    "\n",
    "Together, they form a more complete picture of the world."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤” Think About This\n",
    "\n",
    "Before we proceed, consider this question:\n",
    "\n",
    "> **Why would a purely deterministic world model fail in a stochastic environment?**\n",
    "\n",
    "Think about a game where an enemy can appear from the left OR the right with equal probability. A deterministic model must commit to one prediction â€” say, always predicting the enemy appears from the left. It has no way to express \"I am uncertain â€” the enemy could be on either side.\"\n",
    "\n",
    "A stochastic model, on the other hand, can output a probability distribution: 50% left, 50% right. This uncertainty is not a weakness â€” it is an accurate representation of the world."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "Now let us formalize these ideas. Brace yourselves â€” there are three key equations, but we will walk through each one carefully.\n",
    "\n",
    "### 3.1 The Deterministic Path\n",
    "\n",
    "$$h_t = f_\\theta(h_{t-1}, z_{t-1}, a_{t-1})$$\n",
    "\n",
    "This equation says: take the previous deterministic hidden state $h_{t-1}$, the previous stochastic latent state $z_{t-1}$, and the previous action $a_{t-1}$, pass them through a recurrent function $f_\\theta$ (implemented as a GRU), and produce the new deterministic state $h_t$.\n",
    "\n",
    "Computationally, this means: concatenate $z_{t-1}$ and $a_{t-1}$ into a single vector, and feed it as input to a GRU cell whose hidden state is $h_{t-1}$. The output is $h_t$ â€” a compressed summary of everything predictable about the trajectory so far.\n",
    "\n",
    "**Numerical example.** Suppose $h_{t-1}$ is a vector of dimension 3: $[0.5, -0.2, 0.8]$, $z_{t-1} = [0.1, 0.3]$, and $a_{t-1} = [1.0]$ (a single action). We concatenate $z_{t-1}$ and $a_{t-1}$ to get $[0.1, 0.3, 1.0]$ and feed this as input to the GRU with hidden state $[0.5, -0.2, 0.8]$. The GRU applies its learned gates and produces a new hidden state, say $h_t = [0.7, 0.1, -0.3]$. This new state captures the predictable evolution of the world."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The Encoder (Posterior)\n",
    "\n",
    "$$z_t \\sim q_\\theta(z_t \\mid h_t, x_t)$$\n",
    "\n",
    "This equation says: when we have access to a real observation $x_t$, combine it with the deterministic state $h_t$ and compute a probability distribution over the stochastic latent state $z_t$. Then sample $z_t$ from that distribution.\n",
    "\n",
    "Computationally, this means: concatenate $h_t$ and $x_t$, pass them through an MLP that outputs a mean $\\mu$ and standard deviation $\\sigma$, and then sample $z_t = \\mu + \\sigma \\cdot \\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, I)$ (the reparameterization trick from VAEs).\n",
    "\n",
    "This is the **posterior** â€” it uses real data to compute what the stochastic state should be. We use this during training, when we have access to real observations.\n",
    "\n",
    "**Numerical example.** Suppose $h_t = [0.7, 0.1, -0.3]$ and $x_t = [1.0, 0.0, -0.5, 0.2]$ (a CartPole observation). We concatenate them to get $[0.7, 0.1, -0.3, 1.0, 0.0, -0.5, 0.2]$ and pass this through an MLP. The MLP outputs $\\mu = [0.4, -0.1]$ and $\\log\\sigma = [-0.5, -0.3]$, giving $\\sigma = [0.61, 0.74]$. We sample $\\epsilon = [0.2, -0.5]$ from a standard normal, so $z_t = [0.4 + 0.61 \\times 0.2,\\; -0.1 + 0.74 \\times (-0.5)] = [0.52, -0.47]$."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 The Prior (Imagination)\n",
    "\n",
    "$$\\hat{z}_t \\sim p_\\theta(\\hat{z}_t \\mid h_t)$$\n",
    "\n",
    "This equation says: when we do **not** have access to a real observation (we are imagining), predict the stochastic state from the deterministic state alone.\n",
    "\n",
    "Computationally, this means: pass $h_t$ through a different MLP that outputs a mean and standard deviation, and sample $\\hat{z}_t$ from that distribution. This is the model's best guess at what the stochastic state should be, without seeing what actually happened.\n",
    "\n",
    "This is what makes \"dreaming\" possible. During imagination, we only use the deterministic path and the prior â€” no real observations are needed."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 The KL Divergence Loss\n",
    "\n",
    "How do we train the prior to be a good predictor? We minimize the KL divergence between the posterior and the prior:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{KL}} = D_{\\text{KL}}\\big[q_\\theta(z_t \\mid h_t, x_t) \\;\\|\\; p_\\theta(\\hat{z}_t \\mid h_t)\\big]$$\n",
    "\n",
    "This loss pushes the prior (which only sees $h_t$) to match the posterior (which sees both $h_t$ and the real observation $x_t$). As training progresses, the prior gets better at predicting $z_t$ without seeing $x_t$ â€” which means the model's imagination gets more accurate.\n",
    "\n",
    "For two Gaussian distributions, the KL divergence has a closed-form solution:\n",
    "\n",
    "$$D_{\\text{KL}} = \\log \\frac{\\sigma_{\\text{prior}}}{\\sigma_{\\text{post}}} + \\frac{\\sigma_{\\text{post}}^2 + (\\mu_{\\text{post}} - \\mu_{\\text{prior}})^2}{2\\sigma_{\\text{prior}}^2} - \\frac{1}{2}$$\n",
    "\n",
    "**Numerical example.** Suppose the posterior has $\\mu_{\\text{post}} = 0.5$, $\\sigma_{\\text{post}} = 0.6$ and the prior has $\\mu_{\\text{prior}} = 0.3$, $\\sigma_{\\text{prior}} = 0.8$. Then:\n",
    "\n",
    "$$D_{\\text{KL}} = \\log\\frac{0.8}{0.6} + \\frac{0.36 + 0.04}{1.28} - 0.5 = 0.288 + 0.3125 - 0.5 = 0.10$$\n",
    "\n",
    "A small KL means the prior is already close to the posterior â€” the model's imagination is accurate. This is exactly what we want."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Full Loss Function\n",
    "\n",
    "The total world model loss combines three objectives:\n",
    "\n",
    "$$\\mathcal{L} = \\mathcal{L}_{\\text{obs}} + \\mathcal{L}_{\\text{reward}} + \\beta \\cdot \\mathcal{L}_{\\text{KL}}$$\n",
    "\n",
    "where $\\mathcal{L}_{\\text{obs}}$ is the observation reconstruction loss (can the model predict what it sees?), $\\mathcal{L}_{\\text{reward}}$ is the reward prediction loss, and $\\beta$ scales the KL term. We also add a continuation predictor that estimates whether the episode continues."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let Us Build It\n",
    "\n",
    "Now let us implement the RSSM step by step.\n",
    "\n",
    "### 4.1 Collect CartPole Data\n",
    "\n",
    "First, we need training data. We will collect trajectories from CartPole using a random policy. This is the same pattern from Notebook 2, but here we are collecting data specifically to train the RSSM."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters ---\n",
    "OBS_DIM = 4       # CartPole observation: [cart_pos, cart_vel, pole_angle, pole_vel]\n",
    "ACTION_DIM = 2    # CartPole actions: left (0) or right (1)\n",
    "H_DIM = 64        # Deterministic hidden state dimension\n",
    "Z_DIM = 16        # Stochastic latent state dimension\n",
    "BATCH_SIZE = 32\n",
    "SEQ_LEN = 20      # Sequence length for training\n",
    "LEARNING_RATE = 3e-4\n",
    "BETA_KL = 0.1     # KL loss weight\n",
    "NUM_EPISODES = 200\n",
    "GAMMA = 0.99      # Discount factor for actor-critic\n",
    "IMAGINE_HORIZON = 15  # Steps to imagine ahead"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_cartpole_data(num_episodes=200, seed=42):\n",
    "    \"\"\"Collect trajectories from CartPole with a random policy.\"\"\"\n",
    "    env = gym.make('CartPole-v1')\n",
    "    all_episodes = []\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        obs, _ = env.reset(seed=seed + ep)\n",
    "        episode = {'obs': [], 'actions': [], 'rewards': [], 'dones': []}\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            episode['obs'].append(obs)\n",
    "            episode['actions'].append(action)\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode['rewards'].append(reward)\n",
    "            episode['dones'].append(float(done))\n",
    "            obs = next_obs\n",
    "\n",
    "        all_episodes.append(episode)\n",
    "\n",
    "    env.close()\n",
    "    return all_episodes\n",
    "\n",
    "episodes = collect_cartpole_data(NUM_EPISODES)\n",
    "print(f\"Collected {len(episodes)} episodes\")\n",
    "print(f\"Average episode length: {np.mean([len(e['rewards']) for e in episodes]):.1f}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us prepare the data into fixed-length sequences for training."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(episodes, seq_len=20):\n",
    "    \"\"\"Convert episodes into fixed-length training sequences.\"\"\"\n",
    "    obs_seqs, act_seqs, rew_seqs, done_seqs = [], [], [], []\n",
    "\n",
    "    for ep in episodes:\n",
    "        T = len(ep['rewards'])\n",
    "        if T < seq_len:\n",
    "            continue\n",
    "        # Sample a random starting point\n",
    "        start = np.random.randint(0, T - seq_len + 1)\n",
    "        obs_seqs.append(ep['obs'][start:start + seq_len])\n",
    "        act_seqs.append(ep['actions'][start:start + seq_len])\n",
    "        rew_seqs.append(ep['rewards'][start:start + seq_len])\n",
    "        done_seqs.append(ep['dones'][start:start + seq_len])\n",
    "\n",
    "    obs_t = torch.tensor(np.array(obs_seqs), dtype=torch.float32)\n",
    "    act_t = F.one_hot(torch.tensor(np.array(act_seqs)), ACTION_DIM).float()\n",
    "    rew_t = torch.tensor(np.array(rew_seqs), dtype=torch.float32)\n",
    "    done_t = torch.tensor(np.array(done_seqs), dtype=torch.float32)\n",
    "\n",
    "    return obs_t, act_t, rew_t, done_t\n",
    "\n",
    "obs_data, act_data, rew_data, done_data = prepare_sequences(episodes, SEQ_LEN)\n",
    "print(f\"Training sequences: {obs_data.shape[0]}\")\n",
    "print(f\"Sequence shape: obs={obs_data.shape}, act={act_data.shape}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Build the RSSM\n",
    "\n",
    "This is the core of the notebook. The RSSM has three components:\n",
    "\n",
    "1. **Sequence model** (GRUCell): the deterministic path $h_t = f(h_{t-1}, z_{t-1}, a_{t-1})$\n",
    "2. **Encoder**: the posterior $q(z_t \\mid h_t, x_t)$ â€” used when we have real observations\n",
    "3. **Prior**: $p(\\hat{z}_t \\mid h_t)$ â€” used during imagination\n",
    "\n",
    "We use continuous Gaussian latents (rather than the discrete categoricals in the full DreamerV3). This is a design decision: Gaussian latents are simpler to implement and sufficient for CartPole. The full DreamerV3 uses 32 categorical variables with 32 classes each â€” we will note this difference but keep things approachable."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSSM(nn.Module):\n",
    "    \"\"\"\n",
    "    Recurrent State-Space Model.\n",
    "\n",
    "    Combines a deterministic recurrent path (GRU) with\n",
    "    stochastic latent variables (Gaussian) to model both\n",
    "    predictable and uncertain dynamics.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_dim, action_dim, h_dim, z_dim):\n",
    "        super().__init__()\n",
    "        self.h_dim = h_dim\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        # Deterministic path: GRU takes (z_{t-1}, a_{t-1}) as input\n",
    "        self.sequence_model = nn.GRUCell(z_dim + action_dim, h_dim)\n",
    "\n",
    "        # Encoder (posterior): q(z_t | h_t, x_t)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(h_dim + obs_dim, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, z_dim * 2),  # outputs mean and log_std\n",
    "        )\n",
    "\n",
    "        # Prior (imagination): p(z_hat_t | h_t)\n",
    "        self.prior = nn.Sequential(\n",
    "            nn.Linear(h_dim, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, z_dim * 2),  # outputs mean and log_std\n",
    "        )\n",
    "\n",
    "    def initial_state(self, batch_size):\n",
    "        \"\"\"Return zero-initialized hidden state and latent.\"\"\"\n",
    "        h = torch.zeros(batch_size, self.h_dim, device=device)\n",
    "        z = torch.zeros(batch_size, self.z_dim, device=device)\n",
    "        return h, z"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us understand this architecture before proceeding.\n",
    "\n",
    "The `sequence_model` is a GRUCell. At each timestep, it takes the concatenation of $z_{t-1}$ and $a_{t-1}$ as input and updates its hidden state from $h_{t-1}$ to $h_t$. This is the deterministic backbone.\n",
    "\n",
    "The `encoder` takes the concatenation of $h_t$ and the real observation $x_t$, and outputs the parameters of a Gaussian distribution (mean and log standard deviation). We split the output in half: the first `z_dim` values are the mean, the remaining `z_dim` values are the log standard deviation.\n",
    "\n",
    "The `prior` does the same thing, but only using $h_t$ â€” no real observation. This is what the model uses when it is imagining."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us add the methods that sample from these distributions."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_gaussian(self, stats):\n",
    "    \"\"\"Reparameterized sampling from Gaussian parameters.\"\"\"\n",
    "    mean, log_std = torch.chunk(stats, 2, dim=-1)\n",
    "    std = F.softplus(log_std) + 1e-4  # Ensure positive std\n",
    "    eps = torch.randn_like(mean)\n",
    "    z = mean + std * eps  # Reparameterization trick\n",
    "    return z, mean, std\n",
    "\n",
    "RSSM._sample_gaussian = _sample_gaussian"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Visualization: RSSM Architecture\n",
    "\n",
    "Let us visualize the architecture to solidify our understanding."
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "\n",
    "# --- Draw three timesteps ---\n",
    "for i, t_label in enumerate(['t-1', 't', 't+1']):\n",
    "    x_center = 2.5 + i * 4.0\n",
    "\n",
    "    # Deterministic state (blue box)\n",
    "    rect_h = plt.Rectangle((x_center - 0.8, 3.8), 1.6, 1.0,\n",
    "                            facecolor='#3b82f6', edgecolor='black', lw=1.5, alpha=0.85)\n",
    "    ax.add_patch(rect_h)\n",
    "    ax.text(x_center, 4.3, f'$h_{{{t_label}}}$', ha='center', va='center',\n",
    "            fontsize=14, fontweight='bold', color='white')\n",
    "\n",
    "    # Stochastic state (orange box)\n",
    "    rect_z = plt.Rectangle((x_center - 0.8, 1.8), 1.6, 1.0,\n",
    "                            facecolor='#f97316', edgecolor='black', lw=1.5, alpha=0.85)\n",
    "    ax.add_patch(rect_z)\n",
    "    ax.text(x_center, 2.3, f'$z_{{{t_label}}}$', ha='center', va='center',\n",
    "            fontsize=14, fontweight='bold', color='white')\n",
    "\n",
    "    # Arrow: h -> z (encoder or prior)\n",
    "    ax.annotate('', xy=(x_center, 2.8), xytext=(x_center, 3.8),\n",
    "                arrowprops=dict(arrowstyle='->', lw=1.5, color='#6b7280'))\n",
    "\n",
    "# Arrows between timesteps (deterministic path)\n",
    "for i in range(2):\n",
    "    x_start = 2.5 + i * 4.0 + 0.8\n",
    "    x_end = 2.5 + (i + 1) * 4.0 - 0.8\n",
    "    ax.annotate('', xy=(x_end, 4.3), xytext=(x_start, 4.3),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2.5, color='#3b82f6'))\n",
    "    # Label action + z feeding into next h\n",
    "    ax.text((x_start + x_end) / 2, 4.75, f'$a, z$', ha='center',\n",
    "            fontsize=10, color='#374151')\n",
    "\n",
    "# Observation arrows going into z (encoder path)\n",
    "for i in range(3):\n",
    "    x_center = 2.5 + i * 4.0\n",
    "    ax.annotate('', xy=(x_center - 0.3, 1.8), xytext=(x_center - 0.3, 0.8),\n",
    "                arrowprops=dict(arrowstyle='->', lw=1.5, color='#10b981',\n",
    "                                linestyle='dashed'))\n",
    "    ax.text(x_center - 0.3, 0.5, f'$x_{{{[\"t-1\",\"t\",\"t+1\"][i]}}}$',\n",
    "            ha='center', fontsize=11, color='#10b981', fontweight='bold')\n",
    "\n",
    "# Legend\n",
    "ax.text(0.3, 5.6, 'Deterministic path', fontsize=11, color='#3b82f6', fontweight='bold')\n",
    "ax.text(0.3, 5.2, 'Stochastic path', fontsize=11, color='#f97316', fontweight='bold')\n",
    "ax.text(0.3, 4.8, 'Observation (encoder)', fontsize=11, color='#10b981', fontweight='bold')\n",
    "\n",
    "ax.set_title('RSSM Architecture: Deterministic + Stochastic Paths', fontsize=14, pad=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Implement `observe()` â€” Processing Real Data\n",
    "\n",
    "The `observe()` method processes a full sequence of real observations through the RSSM. At each timestep, it:\n",
    "\n",
    "1. Advances the deterministic state using the GRU\n",
    "2. Computes the posterior (encoder) distribution using the real observation\n",
    "3. Computes the prior distribution (for the KL loss)\n",
    "4. Samples the stochastic state from the posterior\n",
    "\n",
    "This is how the RSSM processes real experience during training."
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe(self, obs_seq, act_seq):\n",
    "    \"\"\"\n",
    "    Process a sequence of real observations through the RSSM.\n",
    "\n",
    "    Args:\n",
    "        obs_seq: (batch, seq_len, obs_dim) â€” real observations\n",
    "        act_seq: (batch, seq_len, action_dim) â€” one-hot actions\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with all states and distribution parameters\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, _ = obs_seq.shape\n",
    "    h, z = self.initial_state(batch_size)\n",
    "\n",
    "    # Storage for outputs\n",
    "    h_list, z_list = [], []\n",
    "    post_mean_list, post_std_list = [], []\n",
    "    prior_mean_list, prior_std_list = [], []\n",
    "\n",
    "    for t in range(seq_len):\n",
    "        # Step 1: Deterministic path â€” advance the GRU\n",
    "        gz_input = torch.cat([z, act_seq[:, t]], dim=-1)\n",
    "        h = self.sequence_model(gz_input, h)\n",
    "\n",
    "        # Step 2: Posterior â€” use real observation\n",
    "        post_stats = self.encoder(torch.cat([h, obs_seq[:, t]], dim=-1))\n",
    "        z, post_mean, post_std = self._sample_gaussian(post_stats)\n",
    "\n",
    "        # Step 3: Prior â€” predict without observation\n",
    "        prior_stats = self.prior(h)\n",
    "        _, prior_mean, prior_std = self._sample_gaussian(prior_stats)\n",
    "\n",
    "        # Store everything\n",
    "        h_list.append(h)\n",
    "        z_list.append(z)\n",
    "        post_mean_list.append(post_mean)\n",
    "        post_std_list.append(post_std)\n",
    "        prior_mean_list.append(prior_mean)\n",
    "        prior_std_list.append(prior_std)\n",
    "\n",
    "    return {\n",
    "        'h': torch.stack(h_list, dim=1),          # (B, T, h_dim)\n",
    "        'z': torch.stack(z_list, dim=1),          # (B, T, z_dim)\n",
    "        'post_mean': torch.stack(post_mean_list, dim=1),\n",
    "        'post_std': torch.stack(post_std_list, dim=1),\n",
    "        'prior_mean': torch.stack(prior_mean_list, dim=1),\n",
    "        'prior_std': torch.stack(prior_std_list, dim=1),\n",
    "    }\n",
    "\n",
    "RSSM.observe = observe"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Implement `imagine()` â€” Rolling Forward Without Observations\n",
    "\n",
    "This is where the magic happens. The `imagine()` method rolls the RSSM forward using only the deterministic path and the prior â€” **no real observations.** The model is dreaming."
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imagine(self, initial_h, initial_z, policy_fn, horizon):\n",
    "    \"\"\"\n",
    "    Imagine future trajectories using only the prior (no real observations).\n",
    "\n",
    "    Args:\n",
    "        initial_h: (batch, h_dim) â€” starting deterministic state\n",
    "        initial_z: (batch, z_dim) â€” starting stochastic state\n",
    "        policy_fn: function(h, z) -> action (one-hot)\n",
    "        horizon: number of steps to imagine ahead\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with imagined states\n",
    "    \"\"\"\n",
    "    h, z = initial_h, initial_z\n",
    "    h_list, z_list = [h], [z]\n",
    "\n",
    "    for t in range(horizon):\n",
    "        # Get action from policy\n",
    "        action = policy_fn(h, z)\n",
    "\n",
    "        # Step 1: Deterministic path\n",
    "        gz_input = torch.cat([z, action], dim=-1)\n",
    "        h = self.sequence_model(gz_input, h)\n",
    "\n",
    "        # Step 2: Prior only â€” no real observation available\n",
    "        prior_stats = self.prior(h)\n",
    "        z, _, _ = self._sample_gaussian(prior_stats)\n",
    "\n",
    "        h_list.append(h)\n",
    "        z_list.append(z)\n",
    "\n",
    "    return {\n",
    "        'h': torch.stack(h_list, dim=1),  # (B, H+1, h_dim)\n",
    "        'z': torch.stack(z_list, dim=1),  # (B, H+1, z_dim)\n",
    "    }\n",
    "\n",
    "RSSM.imagine = imagine"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the critical difference between `observe()` and `imagine()`:\n",
    "\n",
    "| | `observe()` (Training) | `imagine()` (Dreaming) |\n",
    "|---|---|---|\n",
    "| Deterministic path | Yes (GRU) | Yes (GRU) |\n",
    "| Stochastic source | Posterior $q(z \\mid h, x)$ | Prior $p(\\hat{z} \\mid h)$ |\n",
    "| Real observations | Required | **Not needed** |\n",
    "\n",
    "This is exactly the design that allows DreamerV3 to be so sample-efficient. Once the world model is trained, we can generate unlimited imagined experience at nearly zero cost."
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Visualization: Observed vs Imagined Trajectories\n",
    "\n",
    "Let us create a quick test to see the RSSM in action. We will run `observe()` on a real sequence and `imagine()` from the same starting point, then compare."
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the RSSM\n",
    "rssm = RSSM(OBS_DIM, ACTION_DIM, H_DIM, Z_DIM).to(device)\n",
    "\n",
    "# Take a small batch for testing\n",
    "test_obs = obs_data[:4].to(device)\n",
    "test_act = act_data[:4].to(device)\n",
    "\n",
    "# Run observe\n",
    "with torch.no_grad():\n",
    "    result = rssm.observe(test_obs, test_act)\n",
    "    print(f\"Observed h shape: {result['h'].shape}\")\n",
    "    print(f\"Observed z shape: {result['z'].shape}\")\n",
    "\n",
    "# Run imagine from timestep 5\n",
    "def random_policy(h, z):\n",
    "    batch = h.shape[0]\n",
    "    actions = torch.randint(0, ACTION_DIM, (batch,))\n",
    "    return F.one_hot(actions, ACTION_DIM).float().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    init_h = result['h'][:, 5, :]\n",
    "    init_z = result['z'][:, 5, :]\n",
    "    imagined = rssm.imagine(init_h, init_z, random_policy, horizon=14)\n",
    "    print(f\"Imagined h shape: {imagined['h'].shape}\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize deterministic states: observed vs imagined\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Pick first sequence in batch, first 8 dimensions of h\n",
    "obs_h = result['h'][0, :, :8].cpu().numpy()\n",
    "img_h = imagined['h'][0, :, :8].cpu().numpy()\n",
    "\n",
    "axes[0].imshow(obs_h.T, aspect='auto', cmap='RdBu_r', interpolation='nearest')\n",
    "axes[0].set_title('Observed Deterministic States $h_t$', fontsize=12)\n",
    "axes[0].set_xlabel('Timestep')\n",
    "axes[0].set_ylabel('Hidden Dim (first 8)')\n",
    "\n",
    "axes[1].imshow(img_h.T, aspect='auto', cmap='RdBu_r', interpolation='nearest')\n",
    "axes[1].set_title('Imagined Deterministic States $h_t$', fontsize=12)\n",
    "axes[1].set_xlabel('Timestep')\n",
    "axes[1].set_ylabel('Hidden Dim (first 8)')\n",
    "\n",
    "plt.suptitle('RSSM States (before training â€” random)', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Note: The model is untrained, so these patterns are random.\")\n",
    "print(\"After training, imagined states should closely match observed states.\")"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Reward and Continuation Predictors\n",
    "\n",
    "DreamerV3 does not just predict states â€” it also predicts **rewards** and **whether the episode continues.** These predictions are essential for training the actor-critic inside imagination."
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardPredictor(nn.Module):\n",
    "    \"\"\"Predicts reward from the RSSM state (h, z).\"\"\"\n",
    "    def __init__(self, h_dim, z_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(h_dim + z_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, h, z):\n",
    "        return self.net(torch.cat([h, z], dim=-1)).squeeze(-1)\n",
    "\n",
    "\n",
    "class ContinuationPredictor(nn.Module):\n",
    "    \"\"\"Predicts whether the episode continues (1 - done probability).\"\"\"\n",
    "    def __init__(self, h_dim, z_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(h_dim + z_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, h, z):\n",
    "        return torch.sigmoid(self.net(torch.cat([h, z], dim=-1))).squeeze(-1)"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Actor and Critic for Imagination-Based Training\n",
    "\n",
    "Now for the most exciting part. The actor and critic are trained **entirely inside the world model's imagination.** They never interact with the real environment.\n",
    "\n",
    "The **actor** selects actions given the current RSSM state $(h, z)$.\n",
    "\n",
    "The **critic** estimates the expected return from a given RSSM state."
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Policy network: maps RSSM state to action probabilities.\"\"\"\n",
    "    def __init__(self, h_dim, z_dim, action_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(h_dim + z_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, h, z):\n",
    "        logits = self.net(torch.cat([h, z], dim=-1))\n",
    "        return logits  # raw logits\n",
    "\n",
    "    def get_action(self, h, z):\n",
    "        \"\"\"Sample action and return one-hot + log_prob.\"\"\"\n",
    "        logits = self.forward(h, z)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action_idx = dist.sample()\n",
    "        log_prob = dist.log_prob(action_idx)\n",
    "        action_onehot = F.one_hot(action_idx, logits.shape[-1]).float()\n",
    "        return action_onehot, log_prob\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Value network: estimates expected return from RSSM state.\"\"\"\n",
    "    def __init__(self, h_dim, z_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(h_dim + z_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, h, z):\n",
    "        return self.net(torch.cat([h, z], dim=-1)).squeeze(-1)"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a moment to appreciate the full picture. We have built:\n",
    "\n",
    "| Component | Purpose | Input | Output |\n",
    "|---|---|---|---|\n",
    "| RSSM (GRU) | Deterministic dynamics | $h_{t-1}, z_{t-1}, a_{t-1}$ | $h_t$ |\n",
    "| Encoder | Posterior (training) | $h_t, x_t$ | $z_t$ |\n",
    "| Prior | Imagination | $h_t$ | $\\hat{z}_t$ |\n",
    "| Reward Predictor | Reward estimation | $h_t, z_t$ | $\\hat{r}_t$ |\n",
    "| Continuation | Episode termination | $h_t, z_t$ | $\\hat{c}_t$ |\n",
    "| Actor | Action selection | $h_t, z_t$ | $a_t$ |\n",
    "| Critic | Value estimation | $h_t, z_t$ | $V(h_t, z_t)$ |"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ðŸ”§ Your Turn\n",
    "\n",
    "Now it is your turn. We have built all the pieces â€” can you put them together?\n",
    "\n",
    "### TODO 1: Implement `imagine_ahead()`\n",
    "\n",
    "This function takes an initial RSSM state, rolls the world model forward for $H$ steps using the actor for actions and the prior for stochastic states, and collects the imagined trajectory with predicted rewards."
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imagine_ahead(rssm, actor, reward_pred, cont_pred,\n",
    "                  initial_h, initial_z, horizon):\n",
    "    \"\"\"\n",
    "    Imagine a trajectory of length `horizon` from the given initial state.\n",
    "\n",
    "    Uses the actor to select actions and the RSSM prior to predict\n",
    "    future stochastic states. Collects predicted rewards and continuation\n",
    "    probabilities along the way.\n",
    "\n",
    "    Args:\n",
    "        rssm: trained RSSM model\n",
    "        actor: trained Actor model\n",
    "        reward_pred: RewardPredictor model\n",
    "        cont_pred: ContinuationPredictor model\n",
    "        initial_h: (batch, h_dim) starting deterministic state\n",
    "        initial_z: (batch, z_dim) starting stochastic state\n",
    "        horizon: number of steps to imagine\n",
    "\n",
    "    Returns:\n",
    "        dict with keys:\n",
    "            'h': (batch, horizon+1, h_dim) â€” deterministic states\n",
    "            'z': (batch, horizon+1, z_dim) â€” stochastic states\n",
    "            'rewards': (batch, horizon) â€” predicted rewards\n",
    "            'continues': (batch, horizon) â€” predicted continuation probs\n",
    "            'log_probs': (batch, horizon) â€” action log probabilities\n",
    "\n",
    "    Hints:\n",
    "        Step 1: Initialize h, z from the inputs. Create empty lists.\n",
    "        Step 2: Loop for `horizon` steps:\n",
    "            a) Get action from actor.get_action(h, z)\n",
    "            b) Advance deterministic state: concat z and action, feed to GRU\n",
    "            c) Sample stochastic state from the prior\n",
    "            d) Predict reward and continuation from (h, z)\n",
    "            e) Append everything to lists\n",
    "        Step 3: Stack lists into tensors and return\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Implement the imagination loop described above.\n",
    "    # Use rssm.sequence_model, rssm.prior, rssm._sample_gaussian,\n",
    "    # actor.get_action, reward_pred, and cont_pred.\n",
    "    # ==============================\n",
    "\n",
    "    h, z = initial_h, initial_z\n",
    "    h_list, z_list = [h], [z]\n",
    "    reward_list, cont_list, logp_list = [], [], []\n",
    "\n",
    "    for t in range(horizon):\n",
    "        action_onehot, log_prob = ???  # YOUR CODE HERE\n",
    "        gz_input = ???                 # YOUR CODE HERE\n",
    "        h = ???                        # YOUR CODE HERE\n",
    "        prior_stats = ???              # YOUR CODE HERE\n",
    "        z, _, _ = ???                  # YOUR CODE HERE\n",
    "\n",
    "        rew = ???                      # YOUR CODE HERE\n",
    "        cont = ???                     # YOUR CODE HERE\n",
    "\n",
    "        h_list.append(h)\n",
    "        z_list.append(z)\n",
    "        reward_list.append(rew)\n",
    "        cont_list.append(cont)\n",
    "        logp_list.append(log_prob)\n",
    "\n",
    "    return {\n",
    "        'h': torch.stack(h_list, dim=1),\n",
    "        'z': torch.stack(z_list, dim=1),\n",
    "        'rewards': torch.stack(reward_list, dim=1),\n",
    "        'continues': torch.stack(cont_list, dim=1),\n",
    "        'log_probs': torch.stack(logp_list, dim=1),\n",
    "    }"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Verification: Run this cell to check your imagine_ahead implementation\n",
    "\n",
    "# Create temporary models for testing\n",
    "_rssm = RSSM(OBS_DIM, ACTION_DIM, H_DIM, Z_DIM).to(device)\n",
    "_actor = Actor(H_DIM, Z_DIM, ACTION_DIM).to(device)\n",
    "_rew_pred = RewardPredictor(H_DIM, Z_DIM).to(device)\n",
    "_cont_pred = ContinuationPredictor(H_DIM, Z_DIM).to(device)\n",
    "\n",
    "_h0 = torch.randn(8, H_DIM, device=device)\n",
    "_z0 = torch.randn(8, Z_DIM, device=device)\n",
    "\n",
    "try:\n",
    "    _result = imagine_ahead(_rssm, _actor, _rew_pred, _cont_pred, _h0, _z0, 10)\n",
    "    assert _result['h'].shape == (8, 11, H_DIM), f\"h shape wrong: {_result['h'].shape}\"\n",
    "    assert _result['z'].shape == (8, 11, Z_DIM), f\"z shape wrong: {_result['z'].shape}\"\n",
    "    assert _result['rewards'].shape == (8, 10), f\"rewards shape wrong: {_result['rewards'].shape}\"\n",
    "    assert _result['continues'].shape == (8, 10), f\"continues shape wrong: {_result['continues'].shape}\"\n",
    "    assert _result['log_probs'].shape == (8, 10), f\"log_probs shape wrong: {_result['log_probs'].shape}\"\n",
    "    print(\"All shape checks passed!\")\n",
    "    print(\"rewards sample:\", _result['rewards'][0, :3].detach().cpu().numpy())\n",
    "    print(\"continues sample:\", _result['continues'][0, :3].detach().cpu().numpy())\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Check your implementation and try again.\")\n",
    "\n",
    "del _rssm, _actor, _rew_pred, _cont_pred"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement the Actor Loss\n",
    "\n",
    "The actor is trained using imagined value estimates. The idea is simple: imagine trajectories, compute returns using the critic, and update the actor to select actions that lead to higher imagined returns.\n",
    "\n",
    "We use a simplified **REINFORCE-style policy gradient** on imagined trajectories:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{actor}} = -\\frac{1}{H} \\sum_{t=0}^{H-1} \\log \\pi(a_t \\mid h_t, z_t) \\cdot \\text{sg}(R_t - V(h_t, z_t))$$\n",
    "\n",
    "where $R_t$ is the lambda-return computed from imagined rewards and $\\text{sg}$ means stop-gradient (we do not backpropagate through the returns)."
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_actor_loss(log_probs, rewards, continues, values, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Compute the actor (policy gradient) loss from imagined trajectories.\n",
    "\n",
    "    Args:\n",
    "        log_probs: (batch, horizon) â€” log probabilities of chosen actions\n",
    "        rewards: (batch, horizon) â€” predicted rewards\n",
    "        continues: (batch, horizon) â€” predicted continuation probabilities\n",
    "        values: (batch, horizon+1) â€” critic values for all states\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss (to be minimized)\n",
    "\n",
    "    Hints:\n",
    "        Step 1: Compute discounted returns working BACKWARDS from the last step.\n",
    "                R_t = r_t + gamma * c_t * R_{t+1}\n",
    "                Initialize R_H = values[:, -1] (bootstrap from critic)\n",
    "        Step 2: Compute advantages: A_t = R_t - values[:, t] (detached)\n",
    "        Step 3: Policy gradient loss: -mean(log_prob * advantage)\n",
    "    \"\"\"\n",
    "    batch_size, horizon = rewards.shape\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Compute returns backwards\n",
    "    returns = torch.zeros(batch_size, horizon, device=rewards.device)\n",
    "    R = ???  # Bootstrap: use the critic's value at the last imagined state\n",
    "\n",
    "    for t in reversed(range(horizon)):\n",
    "        R = ???  # Bellman backup with continuation probability\n",
    "        returns[:, t] = R\n",
    "\n",
    "    # Step 2: Compute advantages (stop gradient on returns and values)\n",
    "    advantages = ???  # YOUR CODE HERE\n",
    "\n",
    "    # Step 3: Policy gradient loss\n",
    "    actor_loss = ???  # YOUR CODE HERE\n",
    "    # ==============================\n",
    "\n",
    "    return actor_loss"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Verification: Run this cell to check your actor loss\n",
    "\n",
    "_log_probs = torch.randn(4, 10)\n",
    "_rewards = torch.ones(4, 10)       # constant reward of 1\n",
    "_continues = torch.ones(4, 10)     # never terminates\n",
    "_values = torch.zeros(4, 11)       # critic predicts 0\n",
    "\n",
    "try:\n",
    "    _loss = compute_actor_loss(_log_probs, _rewards, _continues, _values, gamma=0.99)\n",
    "    assert _loss.dim() == 0, f\"Loss should be scalar, got shape {_loss.shape}\"\n",
    "    # With constant rewards=1, continues=1, values=0, gamma=0.99:\n",
    "    # Returns should be large positive numbers â†’ advantages are positive\n",
    "    # Actor loss should be finite\n",
    "    assert torch.isfinite(_loss), f\"Loss is not finite: {_loss}\"\n",
    "    print(f\"Actor loss: {_loss.item():.4f}\")\n",
    "    print(\"Loss is finite and scalar â€” implementation looks correct!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Check your implementation and try again.\")"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Now let us build the complete training pipeline. We train in two phases:\n",
    "\n",
    "**Phase 1: Train the world model** on real data (observation reconstruction + KL divergence).\n",
    "\n",
    "**Phase 2: Train the actor-critic** inside the world model's imagination."
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_gaussian(post_mean, post_std, prior_mean, prior_std):\n",
    "    \"\"\"\n",
    "    Compute KL divergence between two diagonal Gaussians.\n",
    "    KL(q || p) for each dimension, then sum over the latent dims.\n",
    "    \"\"\"\n",
    "    var_ratio = (post_std / prior_std).pow(2)\n",
    "    mean_diff = (post_mean - prior_mean).pow(2) / prior_std.pow(2)\n",
    "    kl = 0.5 * (var_ratio + mean_diff - 1.0 - var_ratio.log())\n",
    "    return kl.sum(dim=-1)  # Sum over latent dimensions"
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservationDecoder(nn.Module):\n",
    "    \"\"\"Reconstructs observations from RSSM state (h, z).\"\"\"\n",
    "    def __init__(self, h_dim, z_dim, obs_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(h_dim + z_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, obs_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, h, z):\n",
    "        return self.net(torch.cat([h, z], dim=-1))"
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instantiate all models ---\n",
    "rssm = RSSM(OBS_DIM, ACTION_DIM, H_DIM, Z_DIM).to(device)\n",
    "obs_decoder = ObservationDecoder(H_DIM, Z_DIM, OBS_DIM).to(device)\n",
    "reward_pred = RewardPredictor(H_DIM, Z_DIM).to(device)\n",
    "cont_pred = ContinuationPredictor(H_DIM, Z_DIM).to(device)\n",
    "actor = Actor(H_DIM, Z_DIM, ACTION_DIM).to(device)\n",
    "critic = Critic(H_DIM, Z_DIM).to(device)\n",
    "\n",
    "# --- Optimizers ---\n",
    "world_model_params = (\n",
    "    list(rssm.parameters()) +\n",
    "    list(obs_decoder.parameters()) +\n",
    "    list(reward_pred.parameters()) +\n",
    "    list(cont_pred.parameters())\n",
    ")\n",
    "world_opt = optim.Adam(world_model_params, lr=LEARNING_RATE)\n",
    "actor_opt = optim.Adam(actor.parameters(), lr=LEARNING_RATE)\n",
    "critic_opt = optim.Adam(critic.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"World model parameters: {sum(p.numel() for p in world_model_params):,}\")\n",
    "print(f\"Actor parameters: {sum(p.numel() for p in actor.parameters()):,}\")\n",
    "print(f\"Critic parameters: {sum(p.numel() for p in critic.parameters()):,}\")"
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "### Phase 1: World Model Training\n",
    "\n",
    "We train the RSSM to reconstruct observations, predict rewards, predict episode continuation, and minimize the KL divergence between the posterior and prior."
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_world_model_step(rssm, obs_decoder, reward_pred, cont_pred,\n",
    "                           obs_batch, act_batch, rew_batch, done_batch,\n",
    "                           optimizer, beta_kl=0.1):\n",
    "    \"\"\"One training step for the world model.\"\"\"\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass through the RSSM\n",
    "    result = rssm.observe(obs_batch, act_batch)\n",
    "    h, z = result['h'], result['z']\n",
    "\n",
    "    # Observation reconstruction loss\n",
    "    obs_pred = obs_decoder(h, z)\n",
    "    obs_loss = F.mse_loss(obs_pred, obs_batch)\n",
    "\n",
    "    # Reward prediction loss\n",
    "    rew_pred = reward_pred(h, z)\n",
    "    rew_loss = F.mse_loss(rew_pred, rew_batch)\n",
    "\n",
    "    # Continuation prediction loss (binary cross-entropy)\n",
    "    cont_target = 1.0 - done_batch  # continue = 1 - done\n",
    "    cont_logit = cont_pred(h, z)\n",
    "    cont_loss = F.binary_cross_entropy(cont_logit, cont_target)\n",
    "\n",
    "    # KL divergence loss\n",
    "    kl_loss = kl_divergence_gaussian(\n",
    "        result['post_mean'], result['post_std'],\n",
    "        result['prior_mean'], result['prior_std']\n",
    "    ).mean()\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = obs_loss + rew_loss + cont_loss + beta_kl * kl_loss\n",
    "    total_loss.backward()\n",
    "    nn.utils.clip_grad_norm_(world_model_params, max_norm=100.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    return {\n",
    "        'total': total_loss.item(),\n",
    "        'obs': obs_loss.item(),\n",
    "        'reward': rew_loss.item(),\n",
    "        'kl': kl_loss.item(),\n",
    "        'cont': cont_loss.item(),\n",
    "    }"
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Phase 1: Train the World Model ---\n",
    "NUM_WORLD_EPOCHS = 80\n",
    "world_losses = {'total': [], 'obs': [], 'reward': [], 'kl': [], 'cont': []}\n",
    "n_sequences = obs_data.shape[0]\n",
    "\n",
    "print(\"Phase 1: Training World Model...\")\n",
    "for epoch in range(NUM_WORLD_EPOCHS):\n",
    "    # Re-sample sequences each epoch for diversity\n",
    "    obs_data, act_data, rew_data, done_data = prepare_sequences(episodes, SEQ_LEN)\n",
    "\n",
    "    # Shuffle and batch\n",
    "    perm = torch.randperm(obs_data.shape[0])\n",
    "    epoch_losses = {k: [] for k in world_losses}\n",
    "\n",
    "    for i in range(0, len(perm) - BATCH_SIZE + 1, BATCH_SIZE):\n",
    "        idx = perm[i:i + BATCH_SIZE]\n",
    "        obs_b = obs_data[idx].to(device)\n",
    "        act_b = act_data[idx].to(device)\n",
    "        rew_b = rew_data[idx].to(device)\n",
    "        done_b = done_data[idx].to(device)\n",
    "\n",
    "        losses = train_world_model_step(\n",
    "            rssm, obs_decoder, reward_pred, cont_pred,\n",
    "            obs_b, act_b, rew_b, done_b, world_opt, BETA_KL\n",
    "        )\n",
    "        for k, v in losses.items():\n",
    "            epoch_losses[k].append(v)\n",
    "\n",
    "    for k in world_losses:\n",
    "        world_losses[k].append(np.mean(epoch_losses[k]))\n",
    "\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"  Epoch {epoch+1:3d}/{NUM_WORLD_EPOCHS} | \"\n",
    "              f\"Loss: {world_losses['total'][-1]:.4f} | \"\n",
    "              f\"Obs: {world_losses['obs'][-1]:.4f} | \"\n",
    "              f\"KL: {world_losses['kl'][-1]:.4f}\")\n",
    "\n",
    "print(\"World model training complete!\")"
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š World Model Training Curves"
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "axes[0, 0].plot(world_losses['total'], color='#2563eb', lw=2)\n",
    "axes[0, 0].set_title('Total Loss', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(world_losses['obs'], color='#16a34a', lw=2)\n",
    "axes[0, 1].set_title('Observation Reconstruction Loss', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(world_losses['kl'], color='#ea580c', lw=2)\n",
    "axes[1, 0].set_title('KL Divergence', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(world_losses['reward'], color='#9333ea', lw=2)\n",
    "axes[1, 1].set_title('Reward Prediction Loss', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('World Model Training', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Actor-Critic Training in Imagination\n",
    "\n",
    "Now the exciting part. We freeze the world model and train the actor-critic **entirely inside the RSSM's imagination.** No real environment interaction.\n",
    "\n",
    "The training loop works as follows:\n",
    "1. Sample a batch of real sequences and run them through the RSSM to get starting states\n",
    "2. From those starting states, imagine forward using the actor and prior\n",
    "3. Predict rewards and continuation probabilities along the imagined trajectory\n",
    "4. Train the critic to estimate returns\n",
    "5. Train the actor to maximize those returns"
   ],
   "id": "cell_51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor_critic_step(rssm, actor, critic, reward_pred, cont_pred,\n",
    "                            start_h, start_z, actor_opt, critic_opt,\n",
    "                            horizon=15, gamma=0.99):\n",
    "    \"\"\"One training step for actor-critic in imagination.\"\"\"\n",
    "\n",
    "    # --- Imagine trajectories ---\n",
    "    h, z = start_h.detach(), start_z.detach()\n",
    "    h_traj, z_traj = [h], [z]\n",
    "    log_probs_list = []\n",
    "\n",
    "    for t in range(horizon):\n",
    "        action_oh, log_prob = actor.get_action(h, z)\n",
    "        gz_input = torch.cat([z, action_oh], dim=-1)\n",
    "        h = rssm.sequence_model(gz_input, h)\n",
    "        prior_stats = rssm.prior(h)\n",
    "        z, _, _ = rssm._sample_gaussian(prior_stats)\n",
    "        h_traj.append(h)\n",
    "        z_traj.append(z)\n",
    "        log_probs_list.append(log_prob)\n",
    "\n",
    "    h_traj = torch.stack(h_traj, dim=1)   # (B, H+1, h_dim)\n",
    "    z_traj = torch.stack(z_traj, dim=1)   # (B, H+1, z_dim)\n",
    "    log_probs = torch.stack(log_probs_list, dim=1)  # (B, H)\n",
    "\n",
    "    # --- Predict rewards and continuation ---\n",
    "    with torch.no_grad():\n",
    "        rewards = reward_pred(h_traj[:, 1:], z_traj[:, 1:])   # (B, H)\n",
    "        continues = cont_pred(h_traj[:, 1:], z_traj[:, 1:])   # (B, H)\n",
    "\n",
    "    # --- Train Critic ---\n",
    "    values = critic(h_traj.detach(), z_traj.detach())  # (B, H+1)\n",
    "\n",
    "    # Compute targets (discounted returns)\n",
    "    with torch.no_grad():\n",
    "        targets = torch.zeros_like(rewards)\n",
    "        R = values[:, -1]\n",
    "        for t in reversed(range(horizon)):\n",
    "            R = rewards[:, t] + gamma * continues[:, t] * R\n",
    "            targets[:, t] = R\n",
    "\n",
    "    critic_loss = F.mse_loss(values[:, :-1], targets)\n",
    "    critic_opt.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    nn.utils.clip_grad_norm_(critic.parameters(), max_norm=100.0)\n",
    "    critic_opt.step()\n",
    "\n",
    "    # --- Train Actor ---\n",
    "    values_detached = critic(h_traj, z_traj).detach()  # fresh forward pass\n",
    "    advantages = targets - values_detached[:, :-1]\n",
    "    actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "\n",
    "    actor_opt.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    nn.utils.clip_grad_norm_(actor.parameters(), max_norm=100.0)\n",
    "    actor_opt.step()\n",
    "\n",
    "    return actor_loss.item(), critic_loss.item()"
   ],
   "id": "cell_52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Phase 2: Train Actor-Critic in Imagination ---\n",
    "NUM_AC_EPOCHS = 120\n",
    "actor_losses, critic_losses = [], []\n",
    "\n",
    "print(\"Phase 2: Training Actor-Critic in Imagination...\")\n",
    "for epoch in range(NUM_AC_EPOCHS):\n",
    "    # Get starting states from real data\n",
    "    obs_data_fresh, act_data_fresh, _, _ = prepare_sequences(episodes, SEQ_LEN)\n",
    "    perm = torch.randperm(obs_data_fresh.shape[0])\n",
    "    epoch_actor, epoch_critic = [], []\n",
    "\n",
    "    for i in range(0, len(perm) - BATCH_SIZE + 1, BATCH_SIZE):\n",
    "        idx = perm[i:i + BATCH_SIZE]\n",
    "        obs_b = obs_data_fresh[idx].to(device)\n",
    "        act_b = act_data_fresh[idx].to(device)\n",
    "\n",
    "        # Get RSSM states from real data (frozen world model)\n",
    "        with torch.no_grad():\n",
    "            result = rssm.observe(obs_b, act_b)\n",
    "\n",
    "        # Pick a random starting timestep\n",
    "        t0 = np.random.randint(0, SEQ_LEN - 1)\n",
    "        start_h = result['h'][:, t0, :]\n",
    "        start_z = result['z'][:, t0, :]\n",
    "\n",
    "        a_loss, c_loss = train_actor_critic_step(\n",
    "            rssm, actor, critic, reward_pred, cont_pred,\n",
    "            start_h, start_z, actor_opt, critic_opt,\n",
    "            IMAGINE_HORIZON, GAMMA\n",
    "        )\n",
    "        epoch_actor.append(a_loss)\n",
    "        epoch_critic.append(c_loss)\n",
    "\n",
    "    actor_losses.append(np.mean(epoch_actor))\n",
    "    critic_losses.append(np.mean(epoch_critic))\n",
    "\n",
    "    if (epoch + 1) % 30 == 0:\n",
    "        print(f\"  Epoch {epoch+1:3d}/{NUM_AC_EPOCHS} | \"\n",
    "              f\"Actor: {actor_losses[-1]:.4f} | \"\n",
    "              f\"Critic: {critic_losses[-1]:.4f}\")\n",
    "\n",
    "print(\"Actor-critic training complete!\")"
   ],
   "id": "cell_53"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Actor-Critic Training Curves"
   ],
   "id": "cell_54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(actor_losses, color='#dc2626', lw=2, label='Actor Loss')\n",
    "axes[0].set_title('Actor Loss (Policy Gradient)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(critic_losses, color='#2563eb', lw=2, label='Critic Loss')\n",
    "axes[1].set_title('Critic Loss (Value Estimation)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Actor-Critic Training (Inside Imagination)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_55"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ðŸŽ¯ Final Output: Real vs Imagined Trajectories\n",
    "\n",
    "Now let us see the trained RSSM in action. We will:\n",
    "\n",
    "1. Collect a real trajectory from CartPole using the learned actor\n",
    "2. From the same initial state, imagine a trajectory using only the RSSM\n",
    "3. Compare the two side by side\n",
    "\n",
    "This is the ultimate test: does the world model's imagination match reality?"
   ],
   "id": "cell_56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_real_episode(actor, rssm, max_steps=200):\n",
    "    \"\"\"Run a real CartPole episode using the learned actor.\"\"\"\n",
    "    env = gym.make('CartPole-v1')\n",
    "    obs, _ = env.reset(seed=123)\n",
    "\n",
    "    h = torch.zeros(1, H_DIM, device=device)\n",
    "    z = torch.zeros(1, Z_DIM, device=device)\n",
    "\n",
    "    real_obs_list = [obs.copy()]\n",
    "    actions_taken = []\n",
    "    rewards = []\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        obs_t = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Update RSSM state with real observation\n",
    "        if step > 0:\n",
    "            act_t = F.one_hot(\n",
    "                torch.tensor([actions_taken[-1]], device=device), ACTION_DIM\n",
    "            ).float()\n",
    "            gz_input = torch.cat([z, act_t], dim=-1)\n",
    "            h = rssm.sequence_model(gz_input, h)\n",
    "\n",
    "        # Encode with real observation\n",
    "        post_stats = rssm.encoder(torch.cat([h, obs_t], dim=-1))\n",
    "        z, _, _ = rssm._sample_gaussian(post_stats)\n",
    "\n",
    "        # Select action\n",
    "        with torch.no_grad():\n",
    "            logits = actor(h, z)\n",
    "            action = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        real_obs_list.append(obs.copy())\n",
    "        actions_taken.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return np.array(real_obs_list), actions_taken, rewards"
   ],
   "id": "cell_57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_imagined_episode(rssm, actor, reward_pred, initial_obs,\n",
    "                         actions, max_steps=200):\n",
    "    \"\"\"Imagine a trajectory from an initial observation, replaying the same actions.\"\"\"\n",
    "    h = torch.zeros(1, H_DIM, device=device)\n",
    "    z = torch.zeros(1, Z_DIM, device=device)\n",
    "\n",
    "    # Encode the initial observation\n",
    "    obs_t = torch.tensor(initial_obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    post_stats = rssm.encoder(torch.cat([h, obs_t], dim=-1))\n",
    "    z, _, _ = rssm._sample_gaussian(post_stats)\n",
    "\n",
    "    imagined_rewards = []\n",
    "    imagined_h_states = [h.squeeze(0).detach().cpu().numpy()]\n",
    "\n",
    "    steps = min(len(actions), max_steps)\n",
    "    for step in range(steps):\n",
    "        act_t = F.one_hot(\n",
    "            torch.tensor([actions[step]], device=device), ACTION_DIM\n",
    "        ).float()\n",
    "\n",
    "        # Deterministic step\n",
    "        gz_input = torch.cat([z, act_t], dim=-1)\n",
    "        h = rssm.sequence_model(gz_input, h)\n",
    "\n",
    "        # Prior only â€” no real observation\n",
    "        prior_stats = rssm.prior(h)\n",
    "        z, _, _ = rssm._sample_gaussian(prior_stats)\n",
    "\n",
    "        # Predict reward\n",
    "        with torch.no_grad():\n",
    "            rew = reward_pred(h, z).item()\n",
    "\n",
    "        imagined_rewards.append(rew)\n",
    "        imagined_h_states.append(h.squeeze(0).detach().cpu().numpy())\n",
    "\n",
    "    return np.array(imagined_h_states), imagined_rewards"
   ],
   "id": "cell_58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run real episode\n",
    "real_obs, real_actions, real_rewards = run_real_episode(actor, rssm)\n",
    "print(f\"Real episode length: {len(real_rewards)} steps\")\n",
    "print(f\"Real total reward: {sum(real_rewards):.0f}\")\n",
    "\n",
    "# Run imagined episode from the same starting point\n",
    "img_h_states, img_rewards = run_imagined_episode(\n",
    "    rssm, actor, reward_pred, real_obs[0], real_actions\n",
    ")\n",
    "print(f\"Imagined episode length: {len(img_rewards)} steps\")\n",
    "print(f\"Imagined total reward: {sum(img_rewards):.2f}\")"
   ],
   "id": "cell_59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare real vs imagined rewards ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 9))\n",
    "\n",
    "# Plot 1: Reward comparison\n",
    "steps = min(len(real_rewards), len(img_rewards))\n",
    "axes[0, 0].plot(range(steps), real_rewards[:steps],\n",
    "                'b-', lw=2, label='Real Reward', alpha=0.8)\n",
    "axes[0, 0].plot(range(steps), img_rewards[:steps],\n",
    "                'r--', lw=2, label='Imagined Reward', alpha=0.8)\n",
    "axes[0, 0].set_title('Real vs Imagined Rewards', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Timestep')\n",
    "axes[0, 0].set_ylabel('Reward')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Cumulative reward comparison\n",
    "real_cumsum = np.cumsum(real_rewards[:steps])\n",
    "img_cumsum = np.cumsum(img_rewards[:steps])\n",
    "axes[0, 1].plot(range(steps), real_cumsum, 'b-', lw=2, label='Real Cumulative')\n",
    "axes[0, 1].plot(range(steps), img_cumsum, 'r--', lw=2, label='Imagined Cumulative')\n",
    "axes[0, 1].set_title('Cumulative Reward', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Timestep')\n",
    "axes[0, 1].set_ylabel('Cumulative Reward')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Real observation trajectories (CartPole state variables)\n",
    "obs_labels = ['Cart Position', 'Cart Velocity', 'Pole Angle', 'Pole Velocity']\n",
    "colors = ['#2563eb', '#16a34a', '#ea580c', '#9333ea']\n",
    "for j in range(4):\n",
    "    axes[1, 0].plot(real_obs[:steps, j], color=colors[j],\n",
    "                     lw=1.5, label=obs_labels[j], alpha=0.8)\n",
    "axes[1, 0].set_title('Real CartPole Observations', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Timestep')\n",
    "axes[1, 0].legend(fontsize=8, loc='upper left')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Imagined hidden states (first 4 dims)\n",
    "img_h_arr = np.array(img_h_states[:steps])\n",
    "for j in range(min(4, img_h_arr.shape[1])):\n",
    "    axes[1, 1].plot(img_h_arr[:, j], color=colors[j],\n",
    "                     lw=1.5, label=f'h dim {j}', alpha=0.8)\n",
    "axes[1, 1].set_title('Imagined Deterministic States $h_t$', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Timestep')\n",
    "axes[1, 1].legend(fontsize=8, loc='upper left')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Real Environment vs RSSM Imagination', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_60"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Imagination Improves with Training\n",
    "\n",
    "Let us also visualize how the quality of the imagined rewards changed as the world model trained. We will measure the reward prediction error at different training checkpoints."
   ],
   "id": "cell_61"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate imagination quality: predicted vs actual reward on held-out data\n",
    "with torch.no_grad():\n",
    "    # Take a fresh batch of real sequences\n",
    "    eval_obs, eval_act, eval_rew, _ = prepare_sequences(episodes, SEQ_LEN)\n",
    "    eval_obs = eval_obs[:16].to(device)\n",
    "    eval_act = eval_act[:16].to(device)\n",
    "    eval_rew = eval_rew[:16].to(device)\n",
    "\n",
    "    result = rssm.observe(eval_obs, eval_act)\n",
    "    pred_rew = reward_pred(result['h'], result['z'])\n",
    "\n",
    "    # Per-timestep comparison\n",
    "    mean_real = eval_rew.mean(dim=0).cpu().numpy()\n",
    "    mean_pred = pred_rew.mean(dim=0).cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "t_range = range(SEQ_LEN)\n",
    "ax.plot(t_range, mean_real, 'bo-', lw=2, markersize=5, label='Real Reward')\n",
    "ax.plot(t_range, mean_pred, 'r^--', lw=2, markersize=5, label='Predicted Reward')\n",
    "ax.fill_between(t_range, mean_real, mean_pred, alpha=0.15, color='purple')\n",
    "ax.set_title('Reward Prediction: Real vs World Model', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Timestep in Sequence', fontsize=11)\n",
    "ax.set_ylabel('Reward', fontsize=11)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "mae = np.mean(np.abs(mean_real - mean_pred))\n",
    "print(f\"Mean Absolute Reward Prediction Error: {mae:.4f}\")"
   ],
   "id": "cell_62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final evaluation: run multiple episodes with the learned actor ---\n",
    "print(\"=\" * 60)\n",
    "print(\"  FINAL EVALUATION: DreamerV3-Style Actor on CartPole\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "eval_rewards = []\n",
    "for ep in range(20):\n",
    "    obs_list, _, rewards = run_real_episode(actor, rssm)\n",
    "    eval_rewards.append(sum(rewards))\n",
    "\n",
    "mean_reward = np.mean(eval_rewards)\n",
    "std_reward = np.std(eval_rewards)\n",
    "max_reward = np.max(eval_rewards)\n",
    "\n",
    "print(f\"\\n  Episodes evaluated: 20\")\n",
    "print(f\"  Mean reward: {mean_reward:.1f} +/- {std_reward:.1f}\")\n",
    "print(f\"  Max reward:  {max_reward:.0f}\")\n",
    "print(f\"  Min reward:  {np.min(eval_rewards):.0f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  Congratulations!\")\n",
    "print(\"  You built an RSSM world model from scratch,\")\n",
    "print(\"  trained it on CartPole data, and then trained\")\n",
    "print(\"  an actor-critic entirely inside the model's\")\n",
    "print(\"  imagination. This is the core idea behind DreamerV3.\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "cell_63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of evaluation episodes\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "bars = ax.bar(range(20), eval_rewards, color=['#3b82f6' if r >= mean_reward\n",
    "              else '#f97316' for r in eval_rewards], edgecolor='black', lw=0.5)\n",
    "ax.axhline(y=mean_reward, color='red', linestyle='--', lw=2,\n",
    "           label=f'Mean: {mean_reward:.1f}')\n",
    "ax.set_title('Evaluation: Actor Trained in Imagination', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Episode', fontsize=11)\n",
    "ax.set_ylabel('Total Reward', fontsize=11)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_64"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection\n",
    "\n",
    "Take a moment to reflect on what we have built. The key insight is profound: once you have a good world model, you can train a policy **without any real environment interaction.** This is what makes DreamerV3 so sample-efficient.\n",
    "\n",
    "### ðŸ¤” Reflection Questions\n",
    "\n",
    "1. **Why does the RSSM use both a deterministic and stochastic path?** Could we achieve the same result with just one of them? What would we lose?\n",
    "\n",
    "2. **The KL divergence loss pushes the prior toward the posterior.** What would happen if we removed this loss entirely? How would imagination quality be affected?\n",
    "\n",
    "3. **DreamerV3 uses discrete categorical latents (32 variables x 32 classes) instead of continuous Gaussians.** Why might discrete latents be better for modeling complex environments? (Hint: think about multimodal distributions â€” the ball could go left OR right.)\n",
    "\n",
    "### Optional Challenges\n",
    "\n",
    "**Challenge 1: Discrete Latents.** Replace the Gaussian stochastic path with categorical distributions. Use `torch.distributions.OneHotCategorical` with straight-through gradients. Compare the KL divergence behavior.\n",
    "\n",
    "**Challenge 2: Symlog Predictions.** DreamerV3 uses a \"symlog\" transformation for predictions: $\\text{symlog}(x) = \\text{sign}(x) \\ln(|x| + 1)$. Implement this for the reward predictor and see if it stabilizes training on environments with varying reward scales."
   ],
   "id": "cell_65"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Comes Next\n",
    "\n",
    "In this notebook, we built the RSSM â€” the world model that powers DreamerV3's imagination. But DreamerV3 is just one approach to learning about the world.\n",
    "\n",
    "In the next notebook, we will explore a fundamentally different philosophy: **what if we do not need to predict pixels at all?** Yann LeCun's Joint Embedding Predictive Architecture (JEPA) argues that predicting in abstract representation space is both more efficient and more aligned with how biological brains work.\n",
    "\n",
    "The question is: can we build a world model that understands without reconstructing?\n",
    "\n",
    "We will find out in **Notebook 4: JEPA â€” Predicting in Abstract Space.**\n",
    "\n",
    "See you there!"
   ],
   "id": "cell_66"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "chatbot"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸ’¬ AI Teaching Assistant â€” Click â–¶ to start\n",
    "#@markdown This AI chatbot reads your notebook and can answer questions about any concept, code, or exercise.\n",
    "\n",
    "import json as _json\n",
    "import requests as _requests\n",
    "from google.colab import output as _output\n",
    "from IPython.display import display, HTML as _HTML, Markdown as _Markdown\n",
    "\n",
    "# --- Read notebook content for context ---\n",
    "def _get_notebook_context():\n",
    "    try:\n",
    "        from google.colab import _message\n",
    "        nb = _message.blocking_request(\"get_ipynb\", request=\"\", timeout_sec=10)\n",
    "        cells = nb.get(\"ipynb\", {}).get(\"cells\", [])\n",
    "        parts = []\n",
    "        for cell in cells:\n",
    "            src = \"\".join(cell.get(\"source\", []))\n",
    "            tags = cell.get(\"metadata\", {}).get(\"tags\", [])\n",
    "            if \"chatbot\" in tags:\n",
    "                continue\n",
    "            if src.strip():\n",
    "                ct = cell.get(\"cell_type\", \"unknown\")\n",
    "                parts.append(f\"[{ct.upper()}]\\n{src}\")\n",
    "        return \"\\n\\n---\\n\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"Notebook content unavailable.\"\n",
    "\n",
    "_NOTEBOOK_CONTEXT = _get_notebook_context()\n",
    "_CHAT_HISTORY = []\n",
    "_API_URL = \"https://course-creator-brown.vercel.app/api/chat\"\n",
    "\n",
    "def _notebook_chat(question):\n",
    "    global _CHAT_HISTORY\n",
    "    try:\n",
    "        resp = _requests.post(_API_URL, json={\n",
    "            'question': question,\n",
    "            'context': _NOTEBOOK_CONTEXT[:100000],\n",
    "            'history': _CHAT_HISTORY[-10:],\n",
    "        }, timeout=60)\n",
    "        data = resp.json()\n",
    "        answer = data.get('answer', 'Sorry, I could not generate a response.')\n",
    "        _CHAT_HISTORY.append({'role': 'user', 'content': question})\n",
    "        _CHAT_HISTORY.append({'role': 'assistant', 'content': answer})\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f'Error connecting to teaching assistant: {str(e)}'\n",
    "\n",
    "_output.register_callback('notebook_chat', _notebook_chat)\n",
    "\n",
    "def ask(question):\n",
    "    \"\"\"Ask the AI teaching assistant a question about this notebook.\"\"\"\n",
    "    answer = _notebook_chat(question)\n",
    "    display(_Markdown(answer))\n",
    "\n",
    "print(\"\\u2705 AI Teaching Assistant is ready!\")\n",
    "print(\"\\U0001f4a1 Use the chat below, or call ask(\\'your question\\') in any cell.\")\n",
    "\n",
    "# --- Display chat widget ---\n",
    "display(_HTML('''<style>\n  .vc-wrap{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;max-width:100%;border-radius:16px;overflow:hidden;box-shadow:0 4px 24px rgba(0,0,0,.12);background:#fff;border:1px solid #e5e7eb}\n  .vc-hdr{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;padding:16px 20px;display:flex;align-items:center;gap:12px}\n  .vc-avatar{width:42px;height:42px;background:rgba(255,255,255,.2);border-radius:50%;display:flex;align-items:center;justify-content:center;font-size:22px}\n  .vc-hdr h3{font-size:16px;font-weight:600;margin:0}\n  .vc-hdr p{font-size:12px;opacity:.85;margin:2px 0 0}\n  .vc-msgs{height:420px;overflow-y:auto;padding:16px;background:#f8f9fb;display:flex;flex-direction:column;gap:10px}\n  .vc-msg{display:flex;flex-direction:column;animation:vc-fade .25s ease}\n  .vc-msg.user{align-items:flex-end}\n  .vc-msg.bot{align-items:flex-start}\n  .vc-bbl{max-width:85%;padding:10px 14px;border-radius:16px;font-size:14px;line-height:1.55;word-wrap:break-word}\n  .vc-msg.user .vc-bbl{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border-bottom-right-radius:4px}\n  .vc-msg.bot .vc-bbl{background:#fff;color:#1a1a2e;border:1px solid #e8e8e8;border-bottom-left-radius:4px}\n  .vc-bbl code{background:rgba(0,0,0,.07);padding:2px 6px;border-radius:4px;font-size:13px;font-family:'Fira Code',monospace}\n  .vc-bbl pre{background:#1e1e2e;color:#cdd6f4;padding:12px;border-radius:8px;overflow-x:auto;margin:8px 0;font-size:13px}\n  .vc-bbl pre code{background:none;padding:0;color:inherit}\n  .vc-bbl h3,.vc-bbl h4{margin:10px 0 4px;font-size:15px}\n  .vc-bbl ul,.vc-bbl ol{margin:4px 0;padding-left:20px}\n  .vc-bbl li{margin:2px 0}\n  .vc-chips{display:flex;flex-wrap:wrap;gap:8px;padding:0 16px 12px;background:#f8f9fb}\n  .vc-chip{background:#fff;border:1px solid #d1d5db;border-radius:20px;padding:6px 14px;font-size:12px;cursor:pointer;transition:all .15s;color:#4b5563}\n  .vc-chip:hover{border-color:#667eea;color:#667eea;background:#f0f0ff}\n  .vc-input{display:flex;padding:12px 16px;background:#fff;border-top:1px solid #eee;gap:8px}\n  .vc-input input{flex:1;padding:10px 16px;border:2px solid #e8e8e8;border-radius:24px;font-size:14px;outline:none;transition:border-color .2s}\n  .vc-input input:focus{border-color:#667eea}\n  .vc-input button{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border:none;border-radius:50%;width:42px;height:42px;cursor:pointer;display:flex;align-items:center;justify-content:center;font-size:18px;transition:transform .1s}\n  .vc-input button:hover{transform:scale(1.05)}\n  .vc-input button:disabled{opacity:.5;cursor:not-allowed;transform:none}\n  .vc-typing{display:flex;gap:5px;padding:4px 0}\n  .vc-typing span{width:8px;height:8px;background:#667eea;border-radius:50%;animation:vc-bounce 1.4s infinite ease-in-out}\n  .vc-typing span:nth-child(2){animation-delay:.2s}\n  .vc-typing span:nth-child(3){animation-delay:.4s}\n  @keyframes vc-bounce{0%,80%,100%{transform:scale(0)}40%{transform:scale(1)}}\n  @keyframes vc-fade{from{opacity:0;transform:translateY(8px)}to{opacity:1;transform:translateY(0)}}\n  .vc-note{text-align:center;font-size:11px;color:#9ca3af;padding:8px 16px 12px;background:#fff}\n</style>\n<div class=\"vc-wrap\">\n  <div class=\"vc-hdr\">\n    <div class=\"vc-avatar\">&#129302;</div>\n    <div>\n      <h3>Vizuara Teaching Assistant</h3>\n      <p>Ask me anything about this notebook</p>\n    </div>\n  </div>\n  <div class=\"vc-msgs\" id=\"vcMsgs\">\n    <div class=\"vc-msg bot\">\n      <div class=\"vc-bbl\">&#128075; Hi! I've read through this entire notebook. Ask me about any concept, code block, or exercise &mdash; I'm here to help you learn!</div>\n    </div>\n  </div>\n  <div class=\"vc-chips\" id=\"vcChips\">\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Explain the main concept</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Help with the TODO exercise</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Summarize what I learned</span>\n  </div>\n  <div class=\"vc-input\">\n    <input type=\"text\" id=\"vcIn\" placeholder=\"Ask about concepts, code, exercises...\" />\n    <button id=\"vcSend\" onclick=\"vcSendMsg()\">&#10148;</button>\n  </div>\n  <div class=\"vc-note\">AI-generated &middot; Verify important information &middot; <a href=\"#\" onclick=\"vcClear();return false\" style=\"color:#667eea\">Clear chat</a></div>\n</div>\n<script>\n(function(){\n  var msgs=document.getElementById('vcMsgs'),inp=document.getElementById('vcIn'),\n      btn=document.getElementById('vcSend'),chips=document.getElementById('vcChips');\n\n  function esc(s){var d=document.createElement('div');d.textContent=s;return d.innerHTML}\n\n  function md(t){\n    return t\n      .replace(/```(\\w*)\\n([\\s\\S]*?)```/g,function(_,l,c){return '<pre><code>'+esc(c)+'</code></pre>'})\n      .replace(/`([^`]+)`/g,'<code>$1</code>')\n      .replace(/\\*\\*([^*]+)\\*\\*/g,'<strong>$1</strong>')\n      .replace(/\\*([^*]+)\\*/g,'<em>$1</em>')\n      .replace(/^#### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^## (.+)$/gm,'<h3>$1</h3>')\n      .replace(/^\\d+\\. (.+)$/gm,'<li>$1</li>')\n      .replace(/^- (.+)$/gm,'<li>$1</li>')\n      .replace(/\\n\\n/g,'<br><br>')\n      .replace(/\\n/g,'<br>');\n  }\n\n  function addMsg(text,isUser){\n    var m=document.createElement('div');m.className='vc-msg '+(isUser?'user':'bot');\n    var b=document.createElement('div');b.className='vc-bbl';\n    b.innerHTML=isUser?esc(text):md(text);\n    m.appendChild(b);msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function showTyping(){\n    var m=document.createElement('div');m.className='vc-msg bot';m.id='vcTyping';\n    m.innerHTML='<div class=\"vc-bbl\"><div class=\"vc-typing\"><span></span><span></span><span></span></div></div>';\n    msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function hideTyping(){var e=document.getElementById('vcTyping');if(e)e.remove()}\n\n  window.vcSendMsg=function(){\n    var q=inp.value.trim();if(!q)return;\n    inp.value='';chips.style.display='none';\n    addMsg(q,true);showTyping();btn.disabled=true;\n    google.colab.kernel.invokeFunction('notebook_chat',[q],{})\n      .then(function(r){\n        hideTyping();\n        var a=r.data['application/json'];\n        addMsg(typeof a==='string'?a:JSON.stringify(a),false);\n      })\n      .catch(function(){\n        hideTyping();\n        addMsg('Sorry, I encountered an error. Please check your internet connection and try again.',false);\n      })\n      .finally(function(){btn.disabled=false;inp.focus()});\n  };\n\n  window.vcAsk=function(q){inp.value=q;vcSendMsg()};\n  window.vcClear=function(){\n    msgs.innerHTML='<div class=\"vc-msg bot\"><div class=\"vc-bbl\">&#128075; Chat cleared. Ask me anything!</div></div>';\n    chips.style.display='flex';\n  };\n\n  inp.addEventListener('keypress',function(e){if(e.key==='Enter')vcSendMsg()});\n  inp.focus();\n})();\n</script>'''))"
   ],
   "id": "vizuara_chatbot"
  }
 ]
}