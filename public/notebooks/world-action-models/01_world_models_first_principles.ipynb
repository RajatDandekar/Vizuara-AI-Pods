{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "World Models from First Principles â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"142TELN2j0jMWSJ94WD61z2eEAaja4ur-\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Setup & Libraries\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ World Models from First Principles\n",
    "## Notebook 1 of 6 â€” *World Action Models: How AI Learns to Imagine Before It Acts*\n",
    "\n",
    "**Vizuara AI â€” Google Colab Notebook**\n",
    "\n",
    "In this notebook, we will build a world model from scratch. Not a toy example â€” a real neural network that learns the physics of CartPole and uses that learned physics to make intelligent decisions.\n",
    "\n",
    "**Estimated time: ~25 minutes**\n",
    "\n",
    "**What you will learn:**\n",
    "- Why imagining the future is the key to intelligent behavior\n",
    "- The formal mathematics behind world models\n",
    "- How to build, train, and use a neural network world model in PyTorch"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup â€” run this cell first\n",
    "!pip install gymnasium -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"All libraries loaded successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Gymnasium version: {gym.__version__}\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why World Models Matter\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_why_it_matters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Let us start with something you do every single day without thinking about it.\n",
    "\n",
    "Right now, imagine reaching out and picking up a glass of water from your desk. Before your hand even moves, your brain has already **simulated** the entire action:\n",
    "\n",
    "- How far is the glass? Your brain estimates the distance.\n",
    "- How heavy will it be? Your brain predicts the weight based on how full it looks.\n",
    "- What angle should your fingers approach at? Your brain plans the grasp.\n",
    "- What happens if you grip too loosely? Your brain imagines the glass slipping.\n",
    "\n",
    "All of this happens in a fraction of a second, **before you move a single muscle.** You are running a simulation â€” a *world model* â€” inside your head.\n",
    "\n",
    "Now here is the remarkable part. A toddler learning to pick up a glass does not need to drop it 10,000 times to learn. They watch, they think, they imagine, and they try â€” maybe fumbling a few times. Compare this to how most AI agents learn today: through millions of blind trial-and-error interactions.\n",
    "\n",
    "> The core idea behind world models is simple: **what if we gave AI the ability to imagine before it acts?**\n",
    "\n",
    "By the end of this notebook, you will build a world model that can predict CartPole physics without ever seeing the environment â€” and use that model to make intelligent decisions through pure imagination."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Building Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_building_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_building_intuition"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Think About It\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_think_about_it.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_think_about_it"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Before we write any code or equations, let us build a mental model for what a world model actually does. We will compare two fundamentally different approaches to learning.\n",
    "\n",
    "### Model-Free Learning: Learning by Crashing\n",
    "\n",
    "Imagine you are learning to drive a car, but the *only* way you are allowed to learn is by getting into a real car on a real road:\n",
    "\n",
    "- You press the gas. The car lurches forward. You crash into a wall. **Negative reward.**\n",
    "- You try again. This time you turn the wheel slightly. You swerve off the road. **Negative reward.**\n",
    "- After 50,000 crashes, you start to develop reflexes. Gas pedal here, brake there, turn the wheel this much at that speed.\n",
    "\n",
    "You have learned a **policy** â€” a mapping from situations to actions â€” but you have learned it entirely through pain. You never understood *why* pressing the gas makes the car accelerate. You just learned the pattern: \"when I see this, do that.\"\n",
    "\n",
    "This is **model-free reinforcement learning.** It works, but it is brutally inefficient.\n",
    "\n",
    "### Model-Based Learning: The Driving Simulator\n",
    "\n",
    "Now imagine a different approach. Before you ever touch a real car, someone gives you a **driving simulator:**\n",
    "\n",
    "- You can try any action and instantly see what *would* happen.\n",
    "- You can rewind and try a different action.\n",
    "- You can simulate 1,000 scenarios in the time it takes to drive one mile.\n",
    "\n",
    "With the simulator, you learn the **physics** of driving â€” how steering angle affects trajectory, how speed affects braking distance. You build an internal model of how the world works, and then you use that model to plan your actions.\n",
    "\n",
    "This is **model-based reinforcement learning.** The simulator is your world model.\n",
    "\n",
    "### ðŸ¤” Think About This\n",
    "\n",
    "Before we continue, consider this thought experiment:\n",
    "\n",
    "- A model-free agent needs to interact with the real environment for every single learning step. Each interaction costs time, energy, and potentially real-world consequences.\n",
    "- A model-based agent can \"imagine\" thousands of interactions inside its head. The only real-world interactions it needs are to improve its internal model.\n",
    "\n",
    "Which approach would you want for a robot surgeon? A self-driving car? A Mars rover that cannot afford a single mistake?\n",
    "\n",
    "This is exactly why world models matter. They enable **sample-efficient learning** â€” achieving good performance with far fewer real-world interactions."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: The Core Equation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_math_core_equation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_math_core_equation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "Now let us formalize these intuitions. We will build up the mathematics step by step, and after every equation, we will explain exactly what it means computationally.\n",
    "\n",
    "### 3.1 State Transitions: The Core Equation\n",
    "\n",
    "At the heart of every world model is a single idea: **given where I am now and what I do, where will I be next?**\n",
    "\n",
    "Mathematically, we write this as:\n",
    "\n",
    "$$s_{t+1} = f(s_t, a_t)$$\n",
    "\n",
    "Let us break this down:\n",
    "- $s_t$ is the **state** at time $t$ â€” everything the agent knows about the world right now. In CartPole, this is a vector of 4 numbers: cart position, cart velocity, pole angle, and pole angular velocity.\n",
    "- $a_t$ is the **action** the agent takes â€” in CartPole, this is either 0 (push left) or 1 (push right).\n",
    "- $f$ is the **transition function** â€” the \"physics engine\" that determines what happens next.\n",
    "- $s_{t+1}$ is the **next state** â€” where the world ends up after the action.\n",
    "\n",
    "Computationally, this means: we take a vector (the current state) and an integer (the action), concatenate them, feed them through a neural network, and get back a new vector (the predicted next state). That is it. The neural network *is* our world model.\n",
    "\n",
    "### A Numerical Example\n",
    "\n",
    "Let us plug in some concrete numbers to see how this works. Suppose at time $t = 0$:\n",
    "\n",
    "- $s_0 = [0.02, 0.15, -0.03, 0.08]$ â€” the cart is slightly right of center, moving right, with the pole tilted slightly left\n",
    "- $a_0 = 1$ â€” we push right\n",
    "\n",
    "Our world model $f$ takes this input and predicts:\n",
    "\n",
    "- $s_1 = [0.05, 0.34, -0.02, -0.15]$ â€” the cart moved further right, sped up, and the pole angle changed\n",
    "\n",
    "The question is: how close is this prediction to what the *real* physics would produce? That is exactly what we will train our model to get right."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Reward Prediction\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_math_reward.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_math_reward"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Reward Prediction\n",
    "\n",
    "A world model that only predicts states is useful, but a *complete* world model also predicts **rewards.** The reward tells the agent whether what happened was good or bad.\n",
    "\n",
    "$$\\hat{r}_t = g(s_t, a_t)$$\n",
    "\n",
    "Here:\n",
    "- $\\hat{r}_t$ is the **predicted reward** at time $t$ (the hat symbol $\\hat{}$ means \"predicted\" or \"estimated\")\n",
    "- $g$ is a **reward prediction function** â€” another neural network\n",
    "- $s_t$ and $a_t$ are the current state and action, just as before\n",
    "\n",
    "Computationally, this means: we take the same state-action input, feed it through a *second* neural network, and get back a single number â€” the predicted reward. In CartPole, the real reward is +1 for every step the pole stays upright, and 0 when the episode ends.\n",
    "\n",
    "### A Numerical Example\n",
    "\n",
    "Using our earlier numbers, if $s_0 = [0.02, 0.15, -0.03, 0.08]$ and $a_0 = 1$:\n",
    "\n",
    "- The real reward is $r_0 = 1.0$ (the pole has not fallen)\n",
    "- Our reward predictor outputs $\\hat{r}_0 = 0.97$\n",
    "\n",
    "The error is $|1.0 - 0.97| = 0.03$, which is quite small. This tells us our reward model has learned that as long as the pole angle is small and the cart is near center, the reward is close to 1."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: The Planning Loop\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_math_planning.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_math_planning"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 The Model-Based Planning Loop\n",
    "\n",
    "Now the question is: once we have a world model, **how do we use it to make decisions?**\n",
    "\n",
    "The answer is planning. We simulate multiple possible futures inside our model, and then pick the action sequence that leads to the best outcome.\n",
    "\n",
    "Here is the planning loop, step by step:\n",
    "\n",
    "**Step 1: Sample action sequences.** Generate $N$ random action sequences, each of length $H$ (the planning horizon):\n",
    "\n",
    "$$A^{(i)} = [a_0^{(i)}, a_1^{(i)}, \\ldots, a_{H-1}^{(i)}] \\quad \\text{for } i = 1, \\ldots, N$$\n",
    "\n",
    "**Step 2: Roll forward through the world model.** For each action sequence, use the world model to predict the trajectory:\n",
    "\n",
    "$$s_{t+1}^{(i)} = f(s_t^{(i)}, a_t^{(i)})$$\n",
    "$$\\hat{r}_t^{(i)} = g(s_t^{(i)}, a_t^{(i)})$$\n",
    "\n",
    "Computationally, this means: start from the current real state, feed it into our neural network along with the first action, get the predicted next state, then feed *that* predicted state along with the second action, and so on. We are \"imagining\" an entire trajectory.\n",
    "\n",
    "**Step 3: Evaluate each sequence.** Sum up the predicted rewards for each trajectory:\n",
    "\n",
    "$$R^{(i)} = \\sum_{t=0}^{H-1} \\hat{r}_t^{(i)}$$\n",
    "\n",
    "**Step 4: Pick the best one.** Select the action sequence with the highest total predicted reward:\n",
    "\n",
    "$$A^* = A^{(\\arg\\max_i R^{(i)})}$$\n",
    "\n",
    "**Step 5: Execute the first action.** Take only the first action $a_0^*$ from the best sequence. Then re-plan from the new real state.\n",
    "\n",
    "This is called **random shooting** â€” we randomly \"shoot\" many trajectories into the future and keep the best one. It is simple, but remarkably effective. Let us now build this entire pipeline."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Collecting Training Data\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_data_collection.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_data_collection"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let Us Build It\n",
    "\n",
    "Enough theory â€” let us get our hands dirty. We will build a complete world model for CartPole in four steps:\n",
    "\n",
    "1. Collect training data from the real environment\n",
    "2. Build the world model architecture\n",
    "3. Train it to predict next states\n",
    "4. Test its predictions against reality\n",
    "\n",
    "### 4.1 Collecting Training Data\n",
    "\n",
    "First, we need examples of how CartPole actually behaves. We will run a random policy (taking random actions) and record every transition: $(s_t, a_t) \\rightarrow s_{t+1}$."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(env_name=\"CartPole-v1\", num_episodes=200):\n",
    "    \"\"\"\n",
    "    Collect state transitions by running a random policy.\n",
    "\n",
    "    Returns:\n",
    "        states: array of current states\n",
    "        actions: array of actions taken\n",
    "        next_states: array of resulting next states\n",
    "        rewards: array of rewards received\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    states, actions, next_states, rewards = [], [], [], []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset(seed=episode)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = env.action_space.sample()  # Random action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            next_states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    env.close()\n",
    "    return (np.array(states), np.array(actions),\n",
    "            np.array(next_states), np.array(rewards))\n",
    "\n",
    "# Collect data\n",
    "states, actions, next_states, rewards = collect_trajectories()\n",
    "print(f\"Collected {len(states)} transitions from 200 episodes\")\n",
    "print(f\"State shape: {states.shape}\")\n",
    "print(f\"Example state: {states[0]}\")\n",
    "print(f\"Example action: {actions[0]}\")\n",
    "print(f\"Example next state: {next_states[0]}\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a moment to understand what we just collected. Each transition is a single step of CartPole physics: we were in state $s_t$, we took action $a_t$, and the environment moved to state $s_{t+1}$.\n",
    "\n",
    "The state has 4 dimensions:\n",
    "\n",
    "| Index | Meaning | Typical Range |\n",
    "|-------|---------|---------------|\n",
    "| 0 | Cart position | -2.4 to 2.4 |\n",
    "| 1 | Cart velocity | -3.0 to 3.0 |\n",
    "| 2 | Pole angle (radians) | -0.21 to 0.21 |\n",
    "| 3 | Pole angular velocity | -3.0 to 3.0 |\n",
    "\n",
    "Our world model needs to learn the relationship between these 4 numbers + the action and predict what the next 4 numbers will be. This is a regression problem."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Let us visualize the data we collected\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "labels = ['Cart Position', 'Cart Velocity', 'Pole Angle', 'Pole Angular Velocity']\n",
    "\n",
    "for i, (ax, label) in enumerate(zip(axes.flat, labels)):\n",
    "    ax.hist(states[:, i], bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    ax.set_title(label, fontsize=13)\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "plt.suptitle('Distribution of Collected States', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Action distribution: {np.bincount(actions)} (roughly 50/50 â€” as expected for random)\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: World Model Architecture\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_world_model_architecture.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_world_model_architecture"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Building the World Model\n",
    "\n",
    "Now let us build the core of our system: a neural network that takes $(s_t, a_t)$ and predicts $s_{t+1}$.\n",
    "\n",
    "We will use a simple feedforward network with two hidden layers. The key design choice is that the input is the state (4 values) concatenated with the action (1 value), giving us 5 input features. The output is the predicted next state (4 values)."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network that learns environment dynamics.\n",
    "\n",
    "    Given the current state and action, predicts the next state.\n",
    "    Architecture: Linear(5, 128) -> ReLU -> Linear(128, 128) -> ReLU -> Linear(128, 4)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_dim=4, action_dim=1, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, state_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"\n",
    "        Predict the next state.\n",
    "\n",
    "        Args:\n",
    "            state: (batch, 4) current state\n",
    "            action: (batch, 1) action taken\n",
    "        Returns:\n",
    "            next_state: (batch, 4) predicted next state\n",
    "        \"\"\"\n",
    "        # Concatenate state and action into a single input vector\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        return self.network(x)\n",
    "\n",
    "# Create the model\n",
    "world_model = WorldModel()\n",
    "print(world_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in world_model.parameters()):,}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Training the World Model\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how simple this is. The entire world model is just three linear layers with ReLU activations. There is no magic here â€” the network learns to approximate the function $f(s_t, a_t) = s_{t+1}$ from data. The 128 hidden units give it enough capacity to capture the nonlinear physics of CartPole without overfitting.\n",
    "\n",
    "### 4.3 Training the World Model\n",
    "\n",
    "Now we train the model. The loss function is straightforward â€” mean squared error (MSE) between the predicted next state and the actual next state:\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} \\|f(s_t^{(i)}, a_t^{(i)}) - s_{t+1}^{(i)}\\|^2$$\n",
    "\n",
    "Computationally, this means: for each training example, we feed the state and action through the network, get a predicted next state, compute the squared difference with the real next state across all 4 dimensions, and average over the batch."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(states, actions, next_states):\n",
    "    \"\"\"Convert numpy arrays to PyTorch tensors and create a simple dataset.\"\"\"\n",
    "    X_state = torch.FloatTensor(states)\n",
    "    X_action = torch.FloatTensor(actions).unsqueeze(-1)  # Shape: (N, 1)\n",
    "    Y = torch.FloatTensor(next_states)\n",
    "\n",
    "    # Train/validation split (90/10)\n",
    "    n = len(X_state)\n",
    "    split = int(0.9 * n)\n",
    "    idx = torch.randperm(n)\n",
    "\n",
    "    train_idx, val_idx = idx[:split], idx[split:]\n",
    "    return (X_state[train_idx], X_action[train_idx], Y[train_idx],\n",
    "            X_state[val_idx], X_action[val_idx], Y[val_idx])\n",
    "\n",
    "# Prepare data\n",
    "(train_states, train_actions, train_targets,\n",
    " val_states, val_actions, val_targets) = prepare_data(states, actions, next_states)\n",
    "\n",
    "print(f\"Training samples: {len(train_states)}\")\n",
    "print(f\"Validation samples: {len(val_states)}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_world_model(model, train_states, train_actions, train_targets,\n",
    "                      val_states, val_actions, val_targets,\n",
    "                      epochs=100, batch_size=256, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train the world model using MSE loss.\n",
    "\n",
    "    Returns lists of training and validation losses for plotting.\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    n_train = len(train_states)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle training data each epoch\n",
    "        perm = torch.randperm(n_train)\n",
    "        epoch_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        model.train()\n",
    "        for i in range(0, n_train, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            pred = model(train_states[idx], train_actions[idx])\n",
    "            loss = loss_fn(pred, train_targets[idx])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(val_states, val_actions)\n",
    "            val_loss = loss_fn(val_pred, val_targets).item()\n",
    "\n",
    "        train_losses.append(epoch_loss / n_batches)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1:3d} | Train Loss: {train_losses[-1]:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Train the model\n",
    "print(\"Training world model...\")\n",
    "train_losses, val_losses = train_world_model(\n",
    "    world_model, train_states, train_actions, train_targets,\n",
    "    val_states, val_actions, val_targets\n",
    ")\n",
    "print(\"\\nTraining complete!\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualization: Training and Validation Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss', color='steelblue', linewidth=2)\n",
    "plt.plot(val_losses, label='Validation Loss', color='coral', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=13)\n",
    "plt.ylabel('MSE Loss', fontsize=13)\n",
    "plt.title('World Model Training Progress', fontsize=15, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"Final validation loss: {val_losses[-1]:.6f}\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Testing Predictions\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_testing_predictions.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_11_testing_predictions"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the training went well, you should see both curves dropping rapidly in the first 20 epochs and then leveling off. The fact that the validation loss closely tracks the training loss tells us the model is not overfitting â€” it has genuinely learned the physics, not just memorized the data.\n",
    "\n",
    "This is exactly what we want.\n",
    "\n",
    "### 4.4 Testing Predictions Against Reality\n",
    "\n",
    "The true test of our world model is this: can it accurately predict what CartPole will do over multiple steps? Let us take a real trajectory from the environment and compare it with what our model *thinks* will happen."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_predictions(model, env_name=\"CartPole-v1\", max_steps=50):\n",
    "    \"\"\"\n",
    "    Run a trajectory in the real environment and compare\n",
    "    with multi-step predictions from the world model.\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    state, _ = env.reset(seed=99)\n",
    "\n",
    "    real_states = [state.copy()]\n",
    "    model_states = [state.copy()]\n",
    "    action_log = []\n",
    "\n",
    "    # Use the same fixed action sequence for both\n",
    "    current_model_state = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "    model.eval()\n",
    "    for step in range(max_steps):\n",
    "        action = env.action_space.sample()\n",
    "        action_log.append(action)\n",
    "\n",
    "        # Real environment step\n",
    "        next_state, _, terminated, truncated, _ = env.step(action)\n",
    "        real_states.append(next_state.copy())\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "        # World model prediction (autoregressive â€” feeds its own predictions back)\n",
    "        action_tensor = torch.FloatTensor([[action]])\n",
    "        with torch.no_grad():\n",
    "            predicted_next = model(current_model_state, action_tensor)\n",
    "        current_model_state = predicted_next\n",
    "        model_states.append(predicted_next.squeeze().numpy())\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    env.close()\n",
    "    return np.array(real_states), np.array(model_states), action_log\n",
    "\n",
    "real_states, model_states, action_log = compare_predictions(world_model)\n",
    "print(f\"Trajectory length: {len(real_states)} steps\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualization: Predicted vs Actual States\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "labels = ['Cart Position', 'Cart Velocity', 'Pole Angle', 'Pole Angular Velocity']\n",
    "n_steps = min(len(real_states), len(model_states))\n",
    "\n",
    "for i, (ax, label) in enumerate(zip(axes.flat, labels)):\n",
    "    ax.plot(real_states[:n_steps, i], label='Real Environment',\n",
    "            color='steelblue', linewidth=2)\n",
    "    ax.plot(model_states[:n_steps, i], label='World Model Prediction',\n",
    "            color='coral', linewidth=2, linestyle='--')\n",
    "    ax.set_xlabel('Time Step', fontsize=11)\n",
    "    ax.set_ylabel(label, fontsize=11)\n",
    "    ax.set_title(label, fontsize=13, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Real Environment vs World Model Predictions (Multi-Step)',\n",
    "             fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to study these plots. In the first few steps, the predicted and actual trajectories are nearly identical â€” the world model has learned the local physics accurately. As we go further into the future, small prediction errors accumulate and the trajectories may diverge. This is called **compounding error** and it is the fundamental challenge of learned world models.\n",
    "\n",
    "But here is the key insight: for planning, we do not need perfect long-horizon predictions. We only need the predictions to be accurate enough over a short planning horizon (say 10-20 steps) to distinguish good action sequences from bad ones."
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Your Turn â€” Reward Predictor\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_todo1_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_12_todo1_intro"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ðŸ”§ Your Turn\n",
    "\n",
    "Now it is your turn. You have seen how to build a world model that predicts next states. In this section, you will extend it with two critical components:\n",
    "\n",
    "1. A **reward predictor** â€” so the world model knows what is good and bad\n",
    "2. A **random shooting planner** â€” so the agent can use imagination to make decisions\n",
    "\n",
    "### TODO 1: Build a Reward Predictor\n",
    "\n",
    "Your task is to build a neural network that predicts the reward given a state and action. In CartPole, the reward is 1.0 for every step the pole stays upright and the episode has not ended. The reward predictor should learn to approximate this."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Predicts the reward given current state and action.\n",
    "\n",
    "    Architecture: Linear(5, 64) -> ReLU -> Linear(64, 64) -> ReLU -> Linear(64, 1)\n",
    "\n",
    "    Args:\n",
    "        state_dim: dimension of the state vector (4 for CartPole)\n",
    "        action_dim: dimension of the action (1 for CartPole)\n",
    "        hidden_dim: number of hidden units\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_dim=4, action_dim=1, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        # ============ TODO ============\n",
    "        # Build a neural network similar to WorldModel but with output dim = 1\n",
    "        # Step 1: Create a nn.Sequential with:\n",
    "        #   - Linear layer from (state_dim + action_dim) to hidden_dim\n",
    "        #   - ReLU activation\n",
    "        #   - Linear layer from hidden_dim to hidden_dim\n",
    "        #   - ReLU activation\n",
    "        #   - Linear layer from hidden_dim to 1\n",
    "        #   - Sigmoid activation (rewards in CartPole are 0 or 1)\n",
    "        # ==============================\n",
    "\n",
    "        self.network = ???  # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"\n",
    "        Predict reward for a state-action pair.\n",
    "\n",
    "        Args:\n",
    "            state: (batch, 4) current state\n",
    "            action: (batch, 1) action taken\n",
    "        Returns:\n",
    "            reward: (batch, 1) predicted reward\n",
    "        \"\"\"\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        return self.network(x)"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Verification: Run this cell to check your RewardPredictor implementation\n",
    "reward_model = RewardPredictor()\n",
    "\n",
    "# Test with a dummy input\n",
    "test_state = torch.randn(8, 4)\n",
    "test_action = torch.randint(0, 2, (8, 1)).float()\n",
    "test_output = reward_model(test_state, test_action)\n",
    "\n",
    "assert test_output.shape == (8, 1), \\\n",
    "    f\"Expected output shape (8, 1), got {test_output.shape}\"\n",
    "assert (test_output >= 0).all() and (test_output <= 1).all(), \\\n",
    "    f\"Expected outputs in [0, 1] (sigmoid), got min={test_output.min():.3f}, max={test_output.max():.3f}\"\n",
    "\n",
    "param_count = sum(p.numel() for p in reward_model.parameters())\n",
    "assert param_count > 0, \"Model has no parameters â€” did you define the network?\"\n",
    "\n",
    "print(\"âœ… Correct! Your RewardPredictor architecture is valid.\")\n",
    "print(f\"   Output shape: {test_output.shape}\")\n",
    "print(f\"   Output range: [{test_output.min():.3f}, {test_output.max():.3f}]\")\n",
    "print(f\"   Parameters: {param_count:,}\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: After TODO 1\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_after_todo1.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_13_after_todo1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us train your reward predictor on the data we already collected."
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the reward predictor (uses the same data we collected earlier)\n",
    "def train_reward_model(model, states, actions, rewards, epochs=50, lr=1e-3):\n",
    "    \"\"\"Train the reward predictor using binary cross-entropy loss.\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    X_state = torch.FloatTensor(states)\n",
    "    X_action = torch.FloatTensor(actions).unsqueeze(-1)\n",
    "    Y = torch.FloatTensor(rewards).unsqueeze(-1)\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        pred = model(X_state, X_action)\n",
    "        loss = loss_fn(pred, Y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            accuracy = ((pred > 0.5).float() == Y).float().mean()\n",
    "            print(f\"Epoch {epoch+1:3d} | Loss: {loss.item():.4f} | Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "reward_losses = train_reward_model(reward_model, states, actions, rewards)\n",
    "print(\"\\nReward predictor training complete!\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Your Turn â€” Random Shooting Planner\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_todo2_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_14_todo2_intro"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Build a Random Shooting Planner\n",
    "\n",
    "Now the exciting part â€” let us use our trained world model and reward predictor to **plan.** The idea is simple:\n",
    "\n",
    "1. From the current real state, sample $N$ random action sequences of length $H$\n",
    "2. For each sequence, roll it forward through the world model to predict the trajectory\n",
    "3. Use the reward predictor to estimate the total reward for each trajectory\n",
    "4. Pick the action sequence with the highest predicted reward\n",
    "5. Execute only the first action, then re-plan\n",
    "\n",
    "This is the random shooting algorithm we described in Section 3.3."
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_shooting_planner(world_model, reward_model, current_state,\n",
    "                            num_sequences=500, horizon=15, action_dim=2):\n",
    "    \"\"\"\n",
    "    Plan using random shooting with the learned world model.\n",
    "\n",
    "    Args:\n",
    "        world_model: trained WorldModel that predicts next states\n",
    "        reward_model: trained RewardPredictor that predicts rewards\n",
    "        current_state: numpy array of shape (4,) â€” current real state\n",
    "        num_sequences: N â€” how many random action sequences to evaluate\n",
    "        horizon: H â€” how many steps to look ahead\n",
    "        action_dim: number of possible actions (2 for CartPole: left/right)\n",
    "\n",
    "    Returns:\n",
    "        best_action: int â€” the first action of the best sequence\n",
    "    \"\"\"\n",
    "    world_model.eval()\n",
    "    reward_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # ============ TODO ============\n",
    "        # Step 1: Sample N random action sequences of length H\n",
    "        #   - Each action is 0 or 1 (for CartPole)\n",
    "        #   - Shape should be (num_sequences, horizon)\n",
    "        #   Hint: use torch.randint\n",
    "\n",
    "        action_sequences = ???  # YOUR CODE HERE\n",
    "\n",
    "        # Step 2: Initialize states â€” repeat current_state N times\n",
    "        #   - Convert current_state to a tensor of shape (num_sequences, 4)\n",
    "        #   Hint: use torch.FloatTensor and .unsqueeze(0).repeat(num_sequences, 1)\n",
    "\n",
    "        sim_states = ???  # YOUR CODE HERE\n",
    "\n",
    "        # Step 3: Roll forward and accumulate rewards\n",
    "        total_rewards = torch.zeros(num_sequences)\n",
    "\n",
    "        for t in range(horizon):\n",
    "            # Get the action for this timestep: shape (num_sequences, 1)\n",
    "            actions_t = action_sequences[:, t].unsqueeze(-1).float()\n",
    "\n",
    "            # Predict next state using world model\n",
    "            # Hint: sim_states = world_model(sim_states, actions_t)\n",
    "\n",
    "            ???  # YOUR CODE HERE\n",
    "\n",
    "            # Predict reward using reward model\n",
    "            # Hint: predicted_reward = reward_model(sim_states, actions_t)\n",
    "\n",
    "            ???  # YOUR CODE HERE\n",
    "\n",
    "            # Accumulate reward\n",
    "            # Hint: total_rewards += predicted_reward.squeeze()\n",
    "\n",
    "            ???  # YOUR CODE HERE\n",
    "\n",
    "        # Step 4: Find the best action sequence\n",
    "        # Hint: use torch.argmax on total_rewards\n",
    "\n",
    "        best_idx = ???  # YOUR CODE HERE\n",
    "        best_action = action_sequences[best_idx, 0].item()\n",
    "        # ==============================\n",
    "\n",
    "    return best_action"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Verification: Run this cell to test your planner\n",
    "\n",
    "# Test the planner on a simple state (pole nearly upright)\n",
    "test_state = np.array([0.0, 0.0, 0.01, 0.0])  # Nearly balanced\n",
    "best_action = random_shooting_planner(world_model, reward_model, test_state)\n",
    "\n",
    "assert best_action in [0, 1], f\"Expected action 0 or 1, got {best_action}\"\n",
    "print(f\"âœ… Planner returned action: {best_action}\")\n",
    "\n",
    "# Test that it runs without errors on multiple states\n",
    "for s in [np.array([0.0, 0.0, 0.05, 0.3]),\n",
    "          np.array([0.0, 0.0, -0.05, -0.3]),\n",
    "          np.array([0.5, 0.1, 0.0, 0.0])]:\n",
    "    a = random_shooting_planner(world_model, reward_model, s)\n",
    "    assert a in [0, 1]\n",
    "\n",
    "print(\"âœ… Planner works correctly on all test states!\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Putting It All Together\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/15_putting_together.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_15_putting_together"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Now we have all the pieces of the puzzle ready. Let us assemble the full pipeline:\n",
    "\n",
    "1. **Collect data** from the real environment (done)\n",
    "2. **Train a world model** to predict next states (done)\n",
    "3. **Train a reward predictor** to predict rewards (done)\n",
    "4. **Use random shooting** to plan actions (done)\n",
    "\n",
    "The following function runs our model-based agent in the real CartPole environment. At every step, instead of taking a random action, the agent uses the planner to imagine 500 possible futures and picks the best one."
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_based_agent(world_model, reward_model, num_episodes=10,\n",
    "                          max_steps=500, num_sequences=500, horizon=15):\n",
    "    \"\"\"\n",
    "    Run the model-based planning agent in the real environment.\n",
    "\n",
    "    Returns:\n",
    "        episode_rewards: list of total rewards per episode\n",
    "    \"\"\"\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    episode_rewards = []\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        state, _ = env.reset(seed=ep + 1000)\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Plan using imagination\n",
    "            action = random_shooting_planner(\n",
    "                world_model, reward_model, state,\n",
    "                num_sequences=num_sequences, horizon=horizon\n",
    "            )\n",
    "\n",
    "            # Execute in real environment\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        print(f\"Episode {ep+1:2d} | Reward: {total_reward:6.1f} | Steps: {step+1}\")\n",
    "\n",
    "    env.close()\n",
    "    return episode_rewards"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a baseline to compare against. Let us run a purely random agent â€” one that takes actions without any planning."
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_agent(num_episodes=10, max_steps=500):\n",
    "    \"\"\"Run a random agent for comparison.\"\"\"\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    episode_rewards = []\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        state, _ = env.reset(seed=ep + 1000)\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "    env.close()\n",
    "    return episode_rewards\n",
    "\n",
    "# Run the random baseline\n",
    "print(\"Running random agent...\")\n",
    "random_rewards = run_random_agent()\n",
    "print(f\"\\nRandom agent â€” Mean reward: {np.mean(random_rewards):.1f} +/- {np.std(random_rewards):.1f}\")"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Results\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/16_results.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_16_results"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "Now let us run our model-based agent and see how imagination compares to random action."
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running model-based planning agent...\")\n",
    "print(\"(This uses the world model to imagine futures at every step)\\n\")\n",
    "planned_rewards = run_model_based_agent(world_model, reward_model)\n",
    "print(f\"\\nModel-based agent â€” Mean reward: {np.mean(planned_rewards):.1f} +/- {np.std(planned_rewards):.1f}\")"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualization: Random vs Model-Based Agent Performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart comparison\n",
    "episodes = range(1, len(random_rewards) + 1)\n",
    "width = 0.35\n",
    "axes[0].bar([e - width/2 for e in episodes], random_rewards, width,\n",
    "            label='Random Agent', color='lightcoral', edgecolor='black')\n",
    "axes[0].bar([e + width/2 for e in episodes], planned_rewards, width,\n",
    "            label='Planning Agent', color='steelblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Episode', fontsize=12)\n",
    "axes[0].set_ylabel('Total Reward', fontsize=12)\n",
    "axes[0].set_title('Episode Rewards: Random vs Planning', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Summary statistics\n",
    "categories = ['Random\\nAgent', 'Planning\\nAgent']\n",
    "means = [np.mean(random_rewards), np.mean(planned_rewards)]\n",
    "stds = [np.std(random_rewards), np.std(planned_rewards)]\n",
    "colors = ['lightcoral', 'steelblue']\n",
    "\n",
    "bars = axes[1].bar(categories, means, yerr=stds, capsize=8,\n",
    "                   color=colors, edgecolor='black', linewidth=1.2)\n",
    "axes[1].set_ylabel('Mean Reward', fontsize=12)\n",
    "axes[1].set_title('Average Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, mean, std in zip(bars, means, stds):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 2,\n",
    "                 f'{mean:.1f}', ha='center', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print improvement\n",
    "improvement = np.mean(planned_rewards) / max(np.mean(random_rewards), 1)\n",
    "print(f\"\\nImprovement factor: {improvement:.1f}x\")"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to appreciate what just happened. The planning agent has **never been trained with reinforcement learning.** It has never received a gradient signal telling it which actions are good. Instead, it learned a model of the world and uses that model to *think ahead* â€” to simulate possible futures and pick the best one.\n",
    "\n",
    "The random agent stumbles around blindly, averaging roughly 20-30 steps before the pole falls. The planning agent, by imagining 500 possible futures at every step and picking the most promising one, keeps the pole balanced for significantly longer.\n",
    "\n",
    "This is the power of world models: **learning the rules of the game lets you plan, rather than just react.**"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Final Output\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/17_final_output.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_17_final_output"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ðŸŽ¯ Final Output\n",
    "\n",
    "Let us run one final side-by-side comparison to clearly see the difference between an agent that imagines and one that does not."
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_episode(world_model, reward_model, seed=42):\n",
    "    \"\"\"\n",
    "    Run and visualize one episode each for random and planning agents.\n",
    "    Shows state trajectories side-by-side.\n",
    "    \"\"\"\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "    # Random agent trajectory\n",
    "    state, _ = env.reset(seed=seed)\n",
    "    random_traj = [state.copy()]\n",
    "    for _ in range(500):\n",
    "        action = env.action_space.sample()\n",
    "        state, _, terminated, truncated, _ = env.step(action)\n",
    "        random_traj.append(state.copy())\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    # Planning agent trajectory\n",
    "    state, _ = env.reset(seed=seed)\n",
    "    planned_traj = [state.copy()]\n",
    "    for _ in range(500):\n",
    "        action = random_shooting_planner(world_model, reward_model, state)\n",
    "        state, _, terminated, truncated, _ = env.step(action)\n",
    "        planned_traj.append(state.copy())\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    random_traj = np.array(random_traj)\n",
    "    planned_traj = np.array(planned_traj)\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 9))\n",
    "    labels = ['Cart Position', 'Cart Velocity', 'Pole Angle', 'Pole Angular Velocity']\n",
    "\n",
    "    for i, (ax, label) in enumerate(zip(axes.flat, labels)):\n",
    "        ax.plot(random_traj[:, i], label=f'Random ({len(random_traj)} steps)',\n",
    "                color='lightcoral', linewidth=2, alpha=0.8)\n",
    "        ax.plot(planned_traj[:, i], label=f'Planning ({len(planned_traj)} steps)',\n",
    "                color='steelblue', linewidth=2)\n",
    "        ax.set_xlabel('Time Step', fontsize=11)\n",
    "        ax.set_ylabel(label, fontsize=11)\n",
    "        ax.set_title(label, fontsize=13, fontweight='bold')\n",
    "        ax.legend(fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle('Single Episode: Random Agent vs World Model Planning Agent',\n",
    "                 fontsize=15, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return len(random_traj), len(planned_traj)\n",
    "\n",
    "random_len, planned_len = visualize_episode(world_model, reward_model)"
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"   WORLD MODELS FROM FIRST PRINCIPLES â€” COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"What you built today:\")\n",
    "print(f\"  1. A neural network world model ({sum(p.numel() for p in world_model.parameters()):,} parameters)\")\n",
    "print(f\"  2. A reward predictor ({sum(p.numel() for p in reward_model.parameters()):,} parameters)\")\n",
    "print(f\"  3. A random shooting planner (500 imagined futures per step)\")\n",
    "print()\n",
    "print(\"Results:\")\n",
    "print(f\"  Random agent:   {np.mean(random_rewards):5.1f} avg reward\")\n",
    "print(f\"  Planning agent: {np.mean(planned_rewards):5.1f} avg reward\")\n",
    "print(f\"  Improvement:    {np.mean(planned_rewards)/max(np.mean(random_rewards),1):.1f}x\")\n",
    "print()\n",
    "print(\"The key idea: the planning agent NEVER learned a policy directly.\")\n",
    "print(\"It learned the RULES of the world, then used imagination to act.\")\n",
    "print()\n",
    "print(\"Congratulations â€” you have built a world model from scratch!\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Closing & Next Steps\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/18_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_18_closing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### ðŸ¤” Reflection Questions\n",
    "\n",
    "Take a few minutes to think about these questions. They will deepen your understanding and prepare you for the next notebook.\n",
    "\n",
    "1. **Compounding error:** We saw that multi-step predictions diverge from reality over time. What would happen if we increased the planning horizon from 15 to 100 steps? Would the agent perform better or worse? Why?\n",
    "\n",
    "2. **Data distribution:** We collected training data using a *random* policy. What would happen if we trained the world model on data from a *good* policy instead? Would the model be better or worse at predicting what happens when the agent makes mistakes?\n",
    "\n",
    "3. **Model capacity:** Our world model has just 2 hidden layers with 128 units. CartPole has simple physics. What would need to change if we wanted to model a more complex environment like a robotic arm with 20 joints?\n",
    "\n",
    "### Optional Challenges\n",
    "\n",
    "**Challenge 1: Improve the planner.** The random shooting planner is the simplest possible planning algorithm. Try implementing the **Cross-Entropy Method (CEM)**: instead of sampling completely random sequences, sample from a distribution, evaluate them, keep the top 10%, and refit the distribution. Repeat for 3-5 iterations. Does this improve performance?\n",
    "\n",
    "**Challenge 2: Learn from imagination.** Right now, we plan at every step, which is slow. Can you use the world model to generate *synthetic training data* â€” imagined trajectories â€” and then train a simple policy network on that data? This is the core idea behind the Dyna architecture (Sutton, 1991).\n",
    "\n",
    "---\n",
    "\n",
    "**Coming up in Notebook 2:** We will move beyond simple feedforward models and explore how Ha and Schmidhuber's *\"World Models\"* paper (2018) used a **VAE + RNN** architecture to learn compressed, temporal world models â€” enabling an agent to learn entirely inside its own dream.\n",
    "\n",
    "*This notebook is part of the Vizuara series: World Action Models â€” How AI Learns to Imagine Before It Acts.*"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "chatbot"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸ’¬ AI Teaching Assistant â€” Click â–¶ to start\n",
    "#@markdown This AI chatbot reads your notebook and can answer questions about any concept, code, or exercise.\n",
    "\n",
    "import json as _json\n",
    "import requests as _requests\n",
    "from google.colab import output as _output\n",
    "from IPython.display import display, HTML as _HTML, Markdown as _Markdown\n",
    "\n",
    "# --- Read notebook content for context ---\n",
    "def _get_notebook_context():\n",
    "    try:\n",
    "        from google.colab import _message\n",
    "        nb = _message.blocking_request(\"get_ipynb\", request=\"\", timeout_sec=10)\n",
    "        cells = nb.get(\"ipynb\", {}).get(\"cells\", [])\n",
    "        parts = []\n",
    "        for cell in cells:\n",
    "            src = \"\".join(cell.get(\"source\", []))\n",
    "            tags = cell.get(\"metadata\", {}).get(\"tags\", [])\n",
    "            if \"chatbot\" in tags:\n",
    "                continue\n",
    "            if src.strip():\n",
    "                ct = cell.get(\"cell_type\", \"unknown\")\n",
    "                parts.append(f\"[{ct.upper()}]\\n{src}\")\n",
    "        return \"\\n\\n---\\n\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"Notebook content unavailable.\"\n",
    "\n",
    "_NOTEBOOK_CONTEXT = _get_notebook_context()\n",
    "_CHAT_HISTORY = []\n",
    "_API_URL = \"https://course-creator-brown.vercel.app/api/chat\"\n",
    "\n",
    "def _notebook_chat(question):\n",
    "    global _CHAT_HISTORY\n",
    "    try:\n",
    "        resp = _requests.post(_API_URL, json={\n",
    "            'question': question,\n",
    "            'context': _NOTEBOOK_CONTEXT[:100000],\n",
    "            'history': _CHAT_HISTORY[-10:],\n",
    "        }, timeout=60)\n",
    "        data = resp.json()\n",
    "        answer = data.get('answer', 'Sorry, I could not generate a response.')\n",
    "        _CHAT_HISTORY.append({'role': 'user', 'content': question})\n",
    "        _CHAT_HISTORY.append({'role': 'assistant', 'content': answer})\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f'Error connecting to teaching assistant: {str(e)}'\n",
    "\n",
    "_output.register_callback('notebook_chat', _notebook_chat)\n",
    "\n",
    "def ask(question):\n",
    "    \"\"\"Ask the AI teaching assistant a question about this notebook.\"\"\"\n",
    "    answer = _notebook_chat(question)\n",
    "    display(_Markdown(answer))\n",
    "\n",
    "print(\"\\u2705 AI Teaching Assistant is ready!\")\n",
    "print(\"\\U0001f4a1 Use the chat below, or call ask(\\'your question\\') in any cell.\")\n",
    "\n",
    "# --- Display chat widget ---\n",
    "display(_HTML('''<style>\n  .vc-wrap{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;max-width:100%;border-radius:16px;overflow:hidden;box-shadow:0 4px 24px rgba(0,0,0,.12);background:#fff;border:1px solid #e5e7eb}\n  .vc-hdr{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;padding:16px 20px;display:flex;align-items:center;gap:12px}\n  .vc-avatar{width:42px;height:42px;background:rgba(255,255,255,.2);border-radius:50%;display:flex;align-items:center;justify-content:center;font-size:22px}\n  .vc-hdr h3{font-size:16px;font-weight:600;margin:0}\n  .vc-hdr p{font-size:12px;opacity:.85;margin:2px 0 0}\n  .vc-msgs{height:420px;overflow-y:auto;padding:16px;background:#f8f9fb;display:flex;flex-direction:column;gap:10px}\n  .vc-msg{display:flex;flex-direction:column;animation:vc-fade .25s ease}\n  .vc-msg.user{align-items:flex-end}\n  .vc-msg.bot{align-items:flex-start}\n  .vc-bbl{max-width:85%;padding:10px 14px;border-radius:16px;font-size:14px;line-height:1.55;word-wrap:break-word}\n  .vc-msg.user .vc-bbl{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border-bottom-right-radius:4px}\n  .vc-msg.bot .vc-bbl{background:#fff;color:#1a1a2e;border:1px solid #e8e8e8;border-bottom-left-radius:4px}\n  .vc-bbl code{background:rgba(0,0,0,.07);padding:2px 6px;border-radius:4px;font-size:13px;font-family:'Fira Code',monospace}\n  .vc-bbl pre{background:#1e1e2e;color:#cdd6f4;padding:12px;border-radius:8px;overflow-x:auto;margin:8px 0;font-size:13px}\n  .vc-bbl pre code{background:none;padding:0;color:inherit}\n  .vc-bbl h3,.vc-bbl h4{margin:10px 0 4px;font-size:15px}\n  .vc-bbl ul,.vc-bbl ol{margin:4px 0;padding-left:20px}\n  .vc-bbl li{margin:2px 0}\n  .vc-chips{display:flex;flex-wrap:wrap;gap:8px;padding:0 16px 12px;background:#f8f9fb}\n  .vc-chip{background:#fff;border:1px solid #d1d5db;border-radius:20px;padding:6px 14px;font-size:12px;cursor:pointer;transition:all .15s;color:#4b5563}\n  .vc-chip:hover{border-color:#667eea;color:#667eea;background:#f0f0ff}\n  .vc-input{display:flex;padding:12px 16px;background:#fff;border-top:1px solid #eee;gap:8px}\n  .vc-input input{flex:1;padding:10px 16px;border:2px solid #e8e8e8;border-radius:24px;font-size:14px;outline:none;transition:border-color .2s}\n  .vc-input input:focus{border-color:#667eea}\n  .vc-input button{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border:none;border-radius:50%;width:42px;height:42px;cursor:pointer;display:flex;align-items:center;justify-content:center;font-size:18px;transition:transform .1s}\n  .vc-input button:hover{transform:scale(1.05)}\n  .vc-input button:disabled{opacity:.5;cursor:not-allowed;transform:none}\n  .vc-typing{display:flex;gap:5px;padding:4px 0}\n  .vc-typing span{width:8px;height:8px;background:#667eea;border-radius:50%;animation:vc-bounce 1.4s infinite ease-in-out}\n  .vc-typing span:nth-child(2){animation-delay:.2s}\n  .vc-typing span:nth-child(3){animation-delay:.4s}\n  @keyframes vc-bounce{0%,80%,100%{transform:scale(0)}40%{transform:scale(1)}}\n  @keyframes vc-fade{from{opacity:0;transform:translateY(8px)}to{opacity:1;transform:translateY(0)}}\n  .vc-note{text-align:center;font-size:11px;color:#9ca3af;padding:8px 16px 12px;background:#fff}\n</style>\n<div class=\"vc-wrap\">\n  <div class=\"vc-hdr\">\n    <div class=\"vc-avatar\">&#129302;</div>\n    <div>\n      <h3>Vizuara Teaching Assistant</h3>\n      <p>Ask me anything about this notebook</p>\n    </div>\n  </div>\n  <div class=\"vc-msgs\" id=\"vcMsgs\">\n    <div class=\"vc-msg bot\">\n      <div class=\"vc-bbl\">&#128075; Hi! I've read through this entire notebook. Ask me about any concept, code block, or exercise &mdash; I'm here to help you learn!</div>\n    </div>\n  </div>\n  <div class=\"vc-chips\" id=\"vcChips\">\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Explain the main concept</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Help with the TODO exercise</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Summarize what I learned</span>\n  </div>\n  <div class=\"vc-input\">\n    <input type=\"text\" id=\"vcIn\" placeholder=\"Ask about concepts, code, exercises...\" />\n    <button id=\"vcSend\" onclick=\"vcSendMsg()\">&#10148;</button>\n  </div>\n  <div class=\"vc-note\">AI-generated &middot; Verify important information &middot; <a href=\"#\" onclick=\"vcClear();return false\" style=\"color:#667eea\">Clear chat</a></div>\n</div>\n<script>\n(function(){\n  var msgs=document.getElementById('vcMsgs'),inp=document.getElementById('vcIn'),\n      btn=document.getElementById('vcSend'),chips=document.getElementById('vcChips');\n\n  function esc(s){var d=document.createElement('div');d.textContent=s;return d.innerHTML}\n\n  function md(t){\n    return t\n      .replace(/```(\\w*)\\n([\\s\\S]*?)```/g,function(_,l,c){return '<pre><code>'+esc(c)+'</code></pre>'})\n      .replace(/`([^`]+)`/g,'<code>$1</code>')\n      .replace(/\\*\\*([^*]+)\\*\\*/g,'<strong>$1</strong>')\n      .replace(/\\*([^*]+)\\*/g,'<em>$1</em>')\n      .replace(/^#### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^## (.+)$/gm,'<h3>$1</h3>')\n      .replace(/^\\d+\\. (.+)$/gm,'<li>$1</li>')\n      .replace(/^- (.+)$/gm,'<li>$1</li>')\n      .replace(/\\n\\n/g,'<br><br>')\n      .replace(/\\n/g,'<br>');\n  }\n\n  function addMsg(text,isUser){\n    var m=document.createElement('div');m.className='vc-msg '+(isUser?'user':'bot');\n    var b=document.createElement('div');b.className='vc-bbl';\n    b.innerHTML=isUser?esc(text):md(text);\n    m.appendChild(b);msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function showTyping(){\n    var m=document.createElement('div');m.className='vc-msg bot';m.id='vcTyping';\n    m.innerHTML='<div class=\"vc-bbl\"><div class=\"vc-typing\"><span></span><span></span><span></span></div></div>';\n    msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function hideTyping(){var e=document.getElementById('vcTyping');if(e)e.remove()}\n\n  window.vcSendMsg=function(){\n    var q=inp.value.trim();if(!q)return;\n    inp.value='';chips.style.display='none';\n    addMsg(q,true);showTyping();btn.disabled=true;\n    google.colab.kernel.invokeFunction('notebook_chat',[q],{})\n      .then(function(r){\n        hideTyping();\n        var a=r.data['application/json'];\n        addMsg(typeof a==='string'?a:JSON.stringify(a),false);\n      })\n      .catch(function(){\n        hideTyping();\n        addMsg('Sorry, I encountered an error. Please check your internet connection and try again.',false);\n      })\n      .finally(function(){btn.disabled=false;inp.focus()});\n  };\n\n  window.vcAsk=function(q){inp.value=q;vcSendMsg()};\n  window.vcClear=function(){\n    msgs.innerHTML='<div class=\"vc-msg bot\"><div class=\"vc-bbl\">&#128075; Chat cleared. Ask me anything!</div></div>';\n    chips.style.display='flex';\n  };\n\n  inp.addEventListener('keypress',function(e){if(e.key==='Enter')vcSendMsg()});\n  inp.focus();\n})();\n</script>'''))"
   ],
   "id": "vizuara_chatbot"
  }
 ]
}