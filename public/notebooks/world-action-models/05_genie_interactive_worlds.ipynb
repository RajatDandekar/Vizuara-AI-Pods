{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Genie: From a Single Image to an Interactive World â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Genie: From a Single Image to an Interactive World\n",
    "\n",
    "**Notebook 5 of 6 â€” World Action Models Series | Vizuara**\n",
    "\n",
    "What if you could sketch a game level on a napkin and then *play* it? What if a single photograph of a mountain trail could become a world you walk through? This is exactly what Genie does â€” it generates fully interactive environments from a single image, and it learns to do this *without ever being told what the actions are*.\n",
    "\n",
    "By the end of this notebook, you will build a simplified Genie-style model that **learns actions from unlabeled video** and **generates interactive frame sequences** â€” all from scratch in PyTorch.\n",
    "\n",
    "**Estimated time: ~40 minutes**"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://course-creator-brown.vercel.app/courses/world-action-models/practice/5/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# --- Reproducibility ---\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "%matplotlib inline\n",
    "print(\"Setup complete!\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Let us start with a thought experiment. Imagine you hand a child a crayon drawing of a simple platformer game â€” a stick figure, some platforms, maybe a flag at the top. Now imagine the child could *step inside* that drawing and actually play it. Jump on the platforms. Run left and right. Reach the flag.\n",
    "\n",
    "This is not science fiction. This is **Genie**.\n",
    "\n",
    "Genie, developed by Google DeepMind, takes a single image and generates a fully interactive world from it. The user can take actions â€” move left, move right, jump â€” and the world responds, frame by frame, as if it were a real game.\n",
    "\n",
    "But here is the truly remarkable part: **Genie learns what \"actions\" are without ever being told.** No one labels the training data with \"this is a jump\" or \"this is a move right.\" The model watches thousands of unlabeled videos and *discovers* that there are consistent controls that transform one frame into the next.\n",
    "\n",
    "This brings us to an important question: **How does a model learn what actions exist just by watching?**"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### The Problem with Previous World Models\n",
    "\n",
    "In our earlier notebooks, world models like Ha and Schmidhuber's \"World Models\" required **explicit action labels** during training. Someone had to tell the model: \"Between frame $t$ and frame $t+1$, the agent pressed the 'right' button.\"\n",
    "\n",
    "This is expensive. It means you need either:\n",
    "1. A human to label every frame transition with the action taken, or\n",
    "2. A simulator that provides action labels automatically.\n",
    "\n",
    "What if we want to learn from the billions of hours of unlabeled video on the internet? YouTube gameplay videos, nature documentaries, dashcam footage â€” none of these come with action labels. **Genie solves this.**"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Child Watching a Video Game\n",
    "\n",
    "Let us build intuition with an analogy that captures the core idea.\n",
    "\n",
    "Imagine a child sitting behind someone playing a video game. The child can see the screen perfectly, but **cannot see the controller at all**. The child watches for hours.\n",
    "\n",
    "After enough watching, something remarkable happens. The child starts to understand:\n",
    "- \"Something causes the character to move right\"\n",
    "- \"Something causes the character to jump\"\n",
    "- \"Something causes the character to stop\"\n",
    "\n",
    "The child has **inferred the existence of actions** without ever seeing the controller. They do not know that these actions correspond to specific buttons â€” they just know that *some latent cause* consistently produces certain visual effects.\n",
    "\n",
    "This is precisely what Genie's **Latent Action Model** does. It watches pairs of consecutive frames and learns: \"Something happened between these two frames, and I will represent that something as a discrete code.\""
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three Components: Eyes, Instinct, and Imagination\n",
    "\n",
    "Genie has three core components. Let us map them to something intuitive:\n",
    "\n",
    "| Component | Analogy | What It Does |\n",
    "|-----------|---------|-------------|\n",
    "| **Video Tokenizer** | Eyes | Compresses each frame into a compact grid of discrete tokens |\n",
    "| **Latent Action Model** | Instinct | Looks at two consecutive frames and infers what action was taken |\n",
    "| **Dynamics Model** | Imagination | Given a frame and an action, predicts what happens next |\n",
    "\n",
    "The pipeline works like this:\n",
    "\n",
    "1. **Eyes (Tokenizer):** Take a raw image frame and convert it into a compact, discrete representation â€” a grid of tokens from a learned vocabulary.\n",
    "2. **Instinct (Latent Action Model):** Given two consecutive tokenized frames, infer the latent action that caused the transition. No supervision needed!\n",
    "3. **Imagination (Dynamics Model):** Given the current tokenized frame and a chosen action, predict the next tokenized frame.\n",
    "\n",
    "At inference time, a user provides a single starting image and a sequence of actions. The dynamics model unrolls the world forward, frame by frame, generating an interactive experience."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤” Think About This\n",
    "\n",
    "Before we move to the mathematics, pause and consider:\n",
    "\n",
    "> **How would YOU figure out what \"actions\" exist just by watching game footage?**\n",
    ">\n",
    "> You see thousands of frame pairs. Some pairs show the character moving right. Some show a jump. Some show no movement at all.\n",
    ">\n",
    "> What patterns would you look for? How would you group these transitions into a small number of categories?\n",
    ">\n",
    "> *Take a moment to think about this. The answer is at the heart of Genie.*"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "Now let us formalize the three components. We will go through each one, show the equation, and then explain exactly what it means computationally.\n",
    "\n",
    "### 3.1 Video Tokenizer\n",
    "\n",
    "The tokenizer converts a continuous image into a discrete representation using **Vector Quantized Variational Autoencoder (VQ-VAE)**.\n",
    "\n",
    "$$t_i = \\text{VQ}(\\text{Encoder}(x_i))$$\n",
    "\n",
    "where $x_i \\in \\mathbb{R}^{H \\times W \\times 3}$ is the raw image frame and $t_i$ is a grid of discrete token indices.\n",
    "\n",
    "**What this means computationally:**\n",
    "\n",
    "1. The encoder (a CNN) takes the raw frame $x_i$ and produces a continuous feature map $z_i \\in \\mathbb{R}^{h \\times w \\times D}$, where $h, w$ are the spatial dimensions after downsampling and $D$ is the embedding dimension.\n",
    "2. We maintain a **codebook** $\\mathcal{C} = \\{e_1, e_2, \\ldots, e_K\\}$ of $K$ learnable vectors, each of dimension $D$.\n",
    "3. For each spatial position in $z_i$, we find the **nearest codebook vector**: $t_{i,j} = \\arg\\min_k \\|z_{i,j} - e_k\\|_2$.\n",
    "4. The quantized output replaces each continuous vector with its nearest codebook entry.\n",
    "\n",
    "**Numerical example:** Suppose $D = 4$ and $K = 3$. Our codebook has three vectors:\n",
    "- $e_1 = [1.0, 0.0, 0.0, 0.0]$\n",
    "- $e_2 = [0.0, 1.0, 0.0, 0.0]$\n",
    "- $e_3 = [0.0, 0.0, 1.0, 0.0]$\n",
    "\n",
    "If the encoder outputs $z = [0.9, 0.1, 0.05, 0.0]$ at some spatial position, we compute:\n",
    "- $\\|z - e_1\\|_2 = \\sqrt{0.01 + 0.01 + 0.0025 + 0} = 0.15$\n",
    "- $\\|z - e_2\\|_2 = \\sqrt{0.81 + 0.81 + 0.0025 + 0} = 1.27$\n",
    "- $\\|z - e_3\\|_2 = \\sqrt{0.81 + 0.01 + 0.9025 + 0} = 1.31$\n",
    "\n",
    "The nearest codebook vector is $e_1$, so the token index at this position is $t = 1$. This is exactly what we want â€” the continuous encoder output has been \"snapped\" to the closest entry in our discrete vocabulary."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Latent Action Model (LAM)\n",
    "\n",
    "The Latent Action Model infers **what action caused the transition** between two consecutive frames:\n",
    "\n",
    "$$a_t = \\text{LAM}(t_t, t_{t+1})$$\n",
    "\n",
    "where $t_t$ and $t_{t+1}$ are the tokenized representations of consecutive frames, and $a_t \\in \\{1, 2, \\ldots, N_a\\}$ is a discrete latent action.\n",
    "\n",
    "**What this means computationally:**\n",
    "\n",
    "1. Encode both tokenized frames (or their continuous embeddings) through a small network.\n",
    "2. Compute a difference or concatenation that captures \"what changed.\"\n",
    "3. Output a categorical distribution over $N_a$ possible latent actions.\n",
    "4. Sample (or take argmax of) this distribution to get the discrete action $a_t$.\n",
    "\n",
    "The key insight: **no one tells the model what these actions mean.** The model discovers that the space of frame transitions can be clustered into $N_a$ distinct categories. If the training data contains platformer games, the model might discover actions that correspond to \"left,\" \"right,\" \"jump,\" and \"idle\" â€” but these labels are never provided.\n",
    "\n",
    "**Numerical example:** Suppose $N_a = 4$ and the LAM sees two frames where a character moved to the right. The LAM outputs logits $[0.1, 2.5, -0.3, 0.2]$. Applying softmax:\n",
    "\n",
    "$$p = \\text{softmax}([0.1, 2.5, -0.3, 0.2]) = [0.07, 0.78, 0.05, 0.08]$$\n",
    "\n",
    "The model assigns 78% probability to action index 1. Over time, all \"move right\" transitions get mapped to this same index. This is exactly what we want â€” the model has discovered an action category from unlabeled data."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Dynamics Model\n",
    "\n",
    "The dynamics model predicts the next tokenized frame given the current frame and an action:\n",
    "\n",
    "$$\\hat{t}_{t+1} = \\text{Dynamics}(t_t, a_t)$$\n",
    "\n",
    "**What this means computationally:**\n",
    "\n",
    "1. Take the current tokenized frame $t_t$ and embed each token using the codebook.\n",
    "2. Embed the latent action $a_t$ into a vector.\n",
    "3. Combine the frame embedding and action embedding (e.g., via concatenation or addition).\n",
    "4. Pass through a neural network (CNN or Transformer) that predicts the token indices for the next frame.\n",
    "\n",
    "This is the component that enables **interactivity**. At inference time, you provide:\n",
    "- A starting frame $t_0$\n",
    "- A sequence of actions $a_0, a_1, a_2, \\ldots$\n",
    "\n",
    "And the dynamics model generates: $\\hat{t}_1 = \\text{Dynamics}(t_0, a_0)$, then $\\hat{t}_2 = \\text{Dynamics}(\\hat{t}_1, a_1)$, and so on â€” creating a controllable, interactive world."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Training Objective\n",
    "\n",
    "The three components are trained jointly. The total loss has three terms:\n",
    "\n",
    "$$\\mathcal{L} = \\underbrace{\\mathcal{L}_{\\text{recon}}}_{\\text{Tokenizer}} + \\beta \\underbrace{\\mathcal{L}_{\\text{commit}}}_{\\text{VQ Regularizer}} + \\underbrace{\\mathcal{L}_{\\text{dynamics}}}_{\\text{Next-frame prediction}}$$\n",
    "\n",
    "- **Reconstruction loss** $\\mathcal{L}_{\\text{recon}} = \\|x - \\hat{x}\\|_2^2$: The tokenizer should faithfully reconstruct frames.\n",
    "- **Commitment loss** $\\mathcal{L}_{\\text{commit}} = \\|z - \\text{sg}(e)\\|_2^2$: The encoder outputs should stay close to codebook entries ($\\text{sg}$ means stop-gradient).\n",
    "- **Dynamics loss** $\\mathcal{L}_{\\text{dynamics}} = \\text{CrossEntropy}(\\hat{t}_{t+1}, t_{t+1})$: The predicted next tokens should match the actual next tokens.\n",
    "\n",
    "Let us now build all of this from scratch."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let Us Build It\n",
    "\n",
    "We will build a simplified Genie using a procedurally generated sprite dataset. Our sprite (a colored square) moves on a grid with 4 possible actions: up, down, left, right. Crucially, we will **only store the frames, not the actions** â€” the model must discover the actions on its own."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create the Sprite Dataset"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Constants for our sprite world ---\n",
    "GRID_SIZE = 16         # 16x16 pixel frames\n",
    "SPRITE_SIZE = 4        # 4x4 pixel sprite\n",
    "NUM_ACTIONS = 4        # up, down, left, right\n",
    "SEQ_LENGTH = 8         # frames per sequence\n",
    "NUM_SEQUENCES = 3000   # total training sequences\n",
    "\n",
    "# Action definitions (hidden from the model!)\n",
    "# 0=up, 1=down, 2=left, 3=right\n",
    "ACTION_DELTAS = {\n",
    "    0: (-1, 0),   # up\n",
    "    1: (1, 0),    # down\n",
    "    2: (0, -1),   # left\n",
    "    3: (0, 1),    # right\n",
    "}\n",
    "\n",
    "def generate_sprite_sequence(seq_length, grid_size, sprite_size):\n",
    "    \"\"\"Generate a sequence of frames with a moving sprite.\n",
    "\n",
    "    Returns frames only â€” actions are NOT returned.\n",
    "    Also returns actions separately for evaluation only.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    actions_taken = []\n",
    "\n",
    "    # Random starting position\n",
    "    max_pos = grid_size - sprite_size\n",
    "    row = np.random.randint(0, max_pos + 1)\n",
    "    col = np.random.randint(0, max_pos + 1)\n",
    "\n",
    "    # Random sprite color (RGB, normalized)\n",
    "    color = np.random.rand(3) * 0.5 + 0.5  # bright colors\n",
    "\n",
    "    for step in range(seq_length):\n",
    "        # Create frame\n",
    "        frame = np.zeros((grid_size, grid_size, 3), dtype=np.float32)\n",
    "        frame[row:row+sprite_size, col:col+sprite_size] = color\n",
    "        frames.append(frame)\n",
    "\n",
    "        if step < seq_length - 1:\n",
    "            # Choose random action\n",
    "            action = np.random.randint(0, NUM_ACTIONS)\n",
    "            dr, dc = ACTION_DELTAS[action]\n",
    "            row = np.clip(row + dr, 0, max_pos)\n",
    "            col = np.clip(col + dc, 0, max_pos)\n",
    "            actions_taken.append(action)\n",
    "\n",
    "    return np.array(frames), np.array(actions_taken)"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us generate the dataset and visualize some sequences."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "all_frames = []\n",
    "all_actions = []  # kept only for evaluation, NOT given to the model\n",
    "\n",
    "for _ in range(NUM_SEQUENCES):\n",
    "    frames, actions = generate_sprite_sequence(\n",
    "        SEQ_LENGTH, GRID_SIZE, SPRITE_SIZE\n",
    "    )\n",
    "    all_frames.append(frames)\n",
    "    all_actions.append(actions)\n",
    "\n",
    "all_frames = np.array(all_frames)   # (N, T, H, W, 3)\n",
    "all_actions = np.array(all_actions) # (N, T-1) â€” for eval only\n",
    "\n",
    "print(f\"Dataset shape: {all_frames.shape}\")\n",
    "print(f\"Actions shape: {all_actions.shape} (hidden from model)\")\n",
    "print(f\"Frame value range: [{all_frames.min():.2f}, {all_frames.max():.2f}]\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Visualization: Example Sprite Sequences"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, SEQ_LENGTH, figsize=(16, 6))\n",
    "fig.suptitle(\"Example Sprite Sequences (Model only sees frames, NOT actions)\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "action_names = {0: 'Up', 1: 'Down', 2: 'Left', 3: 'Right'}\n",
    "\n",
    "for row in range(3):\n",
    "    idx = np.random.randint(len(all_frames))\n",
    "    for t in range(SEQ_LENGTH):\n",
    "        axes[row, t].imshow(all_frames[idx, t])\n",
    "        axes[row, t].set_xticks([])\n",
    "        axes[row, t].set_yticks([])\n",
    "        if t < SEQ_LENGTH - 1:\n",
    "            act = action_names[all_actions[idx, t]]\n",
    "            axes[row, t].set_title(f\"t={t}\\n({act})\", fontsize=8)\n",
    "        else:\n",
    "            axes[row, t].set_title(f\"t={t}\", fontsize=8)\n",
    "    axes[row, 0].set_ylabel(f\"Seq {row+1}\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Note: Action labels shown here are the GROUND TRUTH.\")\n",
    "print(\"The model will NEVER see these labels during training!\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Build the Video Tokenizer (VQ-VAE)\n",
    "\n",
    "Let us build the first component: the Video Tokenizer. This is a VQ-VAE that compresses each frame into a grid of discrete tokens.\n",
    "\n",
    "**VQ-VAE in brief:** A standard autoencoder maps inputs to a continuous latent space. A VQ-VAE adds a discrete bottleneck â€” the continuous encoder output is \"snapped\" to the nearest vector in a learnable codebook. This gives us a finite vocabulary of visual tokens.\n",
    "\n",
    "The tricky part is the **gradient problem**: the argmin operation (finding the nearest codebook vector) is not differentiable. The solution is the **straight-through estimator** â€” during the backward pass, we simply copy the gradient from the decoder input to the encoder output, bypassing the quantization step."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    \"\"\"Vector Quantization layer with straight-through estimator.\"\"\"\n",
    "\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Initialize codebook\n",
    "        self.codebook = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.codebook.weight.data.uniform_(\n",
    "            -1.0 / num_embeddings, 1.0 / num_embeddings\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"Quantize continuous input to nearest codebook vectors.\n",
    "\n",
    "        Args:\n",
    "            z: (batch, height, width, embedding_dim)\n",
    "        Returns:\n",
    "            z_q: quantized output (same shape as z)\n",
    "            indices: token indices (batch, height, width)\n",
    "            vq_loss: commitment + codebook loss\n",
    "        \"\"\"\n",
    "        # Flatten spatial dims for distance computation\n",
    "        flat_z = z.reshape(-1, self.embedding_dim)  # (B*H*W, D)\n",
    "\n",
    "        # Compute distances to all codebook entries\n",
    "        distances = (\n",
    "            torch.sum(flat_z ** 2, dim=1, keepdim=True)\n",
    "            - 2 * flat_z @ self.codebook.weight.t()\n",
    "            + torch.sum(self.codebook.weight ** 2, dim=1)\n",
    "        )\n",
    "\n",
    "        # Find nearest codebook entry\n",
    "        indices = torch.argmin(distances, dim=1)  # (B*H*W,)\n",
    "        z_q = self.codebook(indices).reshape(z.shape)\n",
    "\n",
    "        # Losses\n",
    "        commitment_loss = F.mse_loss(z, z_q.detach())\n",
    "        codebook_loss = F.mse_loss(z.detach(), z_q)\n",
    "        vq_loss = codebook_loss + 0.25 * commitment_loss\n",
    "\n",
    "        # Straight-through estimator\n",
    "        z_q = z + (z_q - z).detach()\n",
    "\n",
    "        indices = indices.reshape(z.shape[:-1])\n",
    "        return z_q, indices, vq_loss"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us build the encoder and decoder that wrap around the VQ layer."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"CNN encoder: (B, 3, H, W) -> (B, h, w, D).\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 16->8\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1), # 8->4\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, embedding_dim, 3, padding=1), # 4->4\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 3, H, W) -> (B, D, h, w)\n",
    "        z = self.net(x)\n",
    "        # Rearrange to (B, h, w, D) for VQ\n",
    "        return z.permute(0, 2, 3, 1)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"CNN decoder: (B, D, h, w) -> (B, 3, H, W).\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(embedding_dim, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 3, 3, padding=1),\n",
    "            nn.Sigmoid(),  # output in [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z_q):\n",
    "        # z_q: (B, h, w, D) -> (B, D, h, w)\n",
    "        z_q = z_q.permute(0, 3, 1, 2)\n",
    "        return self.net(z_q)\n",
    "\n",
    "\n",
    "class VideoTokenizer(nn.Module):\n",
    "    \"\"\"VQ-VAE video tokenizer.\"\"\"\n",
    "\n",
    "    def __init__(self, num_codes=64, embedding_dim=32):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(embedding_dim)\n",
    "        self.vq = VectorQuantizer(num_codes, embedding_dim)\n",
    "        self.decoder = Decoder(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z_q, indices, vq_loss = self.vq(z)\n",
    "        x_recon = self.decoder(z_q)\n",
    "        return x_recon, indices, vq_loss, z\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode frame to token indices.\"\"\"\n",
    "        z = self.encoder(x)\n",
    "        _, indices, _, _ = self.vq(z)\n",
    "        return indices\n",
    "\n",
    "    def decode_indices(self, indices):\n",
    "        \"\"\"Decode token indices back to image.\"\"\"\n",
    "        z_q = self.vq.codebook(indices)\n",
    "        return self.decoder(z_q)\n",
    "\n",
    "print(\"VideoTokenizer created.\")\n",
    "print(f\"Each {GRID_SIZE}x{GRID_SIZE} frame -> 4x4 grid of tokens\")\n",
    "print(f\"from a codebook of 64 entries.\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Build the Latent Action Model (LAM)\n",
    "\n",
    "This is the most novel component. The LAM takes two consecutive frames (their encoded representations) and infers a discrete latent action â€” **without any supervision**.\n",
    "\n",
    "The model outputs a categorical distribution over $N_a$ possible actions. During training, it uses the Gumbel-Softmax trick to allow gradients to flow through the discrete sampling."
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentActionModel(nn.Module):\n",
    "    \"\"\"Infers discrete latent actions from consecutive frames.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim=32, spatial_size=4, num_actions=8):\n",
    "        super().__init__()\n",
    "        self.num_actions = num_actions\n",
    "        input_dim = embedding_dim * spatial_size * spatial_size * 2\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_actions),\n",
    "        )\n",
    "        self.temperature = 1.0  # Gumbel-Softmax temperature\n",
    "\n",
    "    def forward(self, z_t, z_t1):\n",
    "        \"\"\"Infer latent action from two consecutive encoded frames.\n",
    "\n",
    "        Args:\n",
    "            z_t:  (B, h, w, D) â€” current frame encoding\n",
    "            z_t1: (B, h, w, D) â€” next frame encoding\n",
    "        Returns:\n",
    "            action_onehot: (B, num_actions) â€” soft one-hot\n",
    "            action_logits: (B, num_actions) â€” raw logits\n",
    "        \"\"\"\n",
    "        batch_size = z_t.shape[0]\n",
    "        z_cat = torch.cat([\n",
    "            z_t.reshape(batch_size, -1),\n",
    "            z_t1.reshape(batch_size, -1)\n",
    "        ], dim=-1)\n",
    "\n",
    "        logits = self.net(z_cat)\n",
    "\n",
    "        if self.training:\n",
    "            # Gumbel-Softmax for differentiable discrete sampling\n",
    "            action_onehot = F.gumbel_softmax(\n",
    "                logits, tau=self.temperature, hard=True\n",
    "            )\n",
    "        else:\n",
    "            # Hard argmax at inference\n",
    "            idx = logits.argmax(dim=-1)\n",
    "            action_onehot = F.one_hot(\n",
    "                idx, self.num_actions\n",
    "            ).float()\n",
    "\n",
    "        return action_onehot, logits\n",
    "\n",
    "print(\"LatentActionModel created.\")\n",
    "print(f\"Discovers {8} latent actions from unlabeled frame pairs.\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Build the Dynamics Model\n",
    "\n",
    "The dynamics model takes the current tokenized frame and a latent action, then predicts the next tokenized frame. We use a small CNN that takes the frame embedding concatenated with a spatially-broadcast action embedding."
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicsModel(nn.Module):\n",
    "    \"\"\"Predicts next tokenized frame given current frame and action.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim=32, num_actions=8,\n",
    "                 num_codes=64, spatial_size=4):\n",
    "        super().__init__()\n",
    "        self.spatial_size = spatial_size\n",
    "        self.num_codes = num_codes\n",
    "\n",
    "        # Action embedding projected to spatial map\n",
    "        self.action_proj = nn.Linear(num_actions, embedding_dim)\n",
    "\n",
    "        # Prediction network\n",
    "        self.net = nn.Sequential(\n",
    "            # Input: embedding_dim * 2 channels (frame + action)\n",
    "            nn.Conv2d(embedding_dim * 2, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, num_codes, 1),  # predict token logits\n",
    "        )\n",
    "\n",
    "    def forward(self, z_q, action_onehot):\n",
    "        \"\"\"Predict next frame tokens.\n",
    "\n",
    "        Args:\n",
    "            z_q: (B, h, w, D) â€” current quantized frame\n",
    "            action_onehot: (B, num_actions) â€” action one-hot\n",
    "        Returns:\n",
    "            token_logits: (B, num_codes, h, w)\n",
    "        \"\"\"\n",
    "        batch_size = z_q.shape[0]\n",
    "        h, w = self.spatial_size, self.spatial_size\n",
    "\n",
    "        # Project action and broadcast to spatial grid\n",
    "        action_emb = self.action_proj(action_onehot)  # (B, D)\n",
    "        action_map = action_emb.unsqueeze(-1).unsqueeze(-1)\n",
    "        action_map = action_map.expand(-1, -1, h, w)  # (B, D, h, w)\n",
    "\n",
    "        # Frame features: (B, h, w, D) -> (B, D, h, w)\n",
    "        frame_feat = z_q.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Concatenate and predict\n",
    "        combined = torch.cat([frame_feat, action_map], dim=1)\n",
    "        token_logits = self.net(combined)  # (B, num_codes, h, w)\n",
    "\n",
    "        return token_logits\n",
    "\n",
    "print(\"DynamicsModel created.\")\n",
    "print(\"Takes (frame, action) -> predicts next frame tokens.\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Visualization: Model Architecture Overview"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(14, 4))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 4)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "\n",
    "# Draw components\n",
    "components = [\n",
    "    (1, 2, 2.0, 1.5, 'Frame $x_t$\\n(16x16x3)', '#E8F5E9'),\n",
    "    (3.5, 2, 2.0, 1.5, 'Encoder\\n(CNN)', '#BBDEFB'),\n",
    "    (6.0, 2, 2.0, 1.5, 'VQ\\nCodebook', '#FFE0B2'),\n",
    "    (6.0, 0.2, 2.0, 1.2, 'LAM\\n(infer action)', '#F8BBD0'),\n",
    "    (9.0, 2, 2.0, 1.5, 'Dynamics\\nModel', '#CE93D8'),\n",
    "    (11.5, 2, 2.0, 1.5, 'Next Frame\\nTokens $\\\\hat{t}_{t+1}$', '#C8E6C9'),\n",
    "]\n",
    "\n",
    "for x, y, w, h, text, color in components:\n",
    "    rect = plt.Rectangle((x, y), w, h, fill=True,\n",
    "                          facecolor=color, edgecolor='black', lw=1.5)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x + w/2, y + h/2, text, ha='center', va='center', fontsize=9)\n",
    "\n",
    "# Draw arrows\n",
    "arrows = [(3.0, 2.75, 0.4, 0), (5.5, 2.75, 0.4, 0),\n",
    "          (8.0, 2.75, 0.9, 0), (10.9, 2.75, 0.5, 0)]\n",
    "for x, y, dx, dy in arrows:\n",
    "    ax.annotate('', xy=(x+dx, y+dy), xytext=(x, y),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2))\n",
    "\n",
    "# Arrow from VQ to LAM\n",
    "ax.annotate('', xy=(7.0, 1.4), xytext=(7.0, 2.0),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='red'))\n",
    "\n",
    "# Arrow from LAM to Dynamics\n",
    "ax.annotate('', xy=(9.5, 1.5), xytext=(8.0, 0.8),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='red'))\n",
    "ax.text(8.5, 1.0, '$a_t$', fontsize=12, color='red', fontweight='bold')\n",
    "\n",
    "ax.set_title(\"Genie Architecture: Three Components Working Together\",\n",
    "             fontsize=13, fontweight='bold', pad=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Prepare the Data Pipeline"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpriteDataset(Dataset):\n",
    "    \"\"\"Dataset of sprite movement sequences (frames only, no actions).\"\"\"\n",
    "\n",
    "    def __init__(self, frames_array):\n",
    "        # frames_array: (N, T, H, W, 3)\n",
    "        # Convert to (N, T, 3, H, W) for PyTorch\n",
    "        self.frames = torch.FloatTensor(frames_array).permute(0, 1, 4, 2, 3)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.frames[idx]  # (T, 3, H, W)\n",
    "\n",
    "\n",
    "# Split into train/validation\n",
    "n_train = int(0.9 * NUM_SEQUENCES)\n",
    "train_dataset = SpriteDataset(all_frames[:n_train])\n",
    "val_dataset = SpriteDataset(all_frames[n_train:])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True, drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=64, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Training sequences: {len(train_dataset)}\")\n",
    "print(f\"Validation sequences: {len(val_dataset)}\")\n",
    "print(f\"Each sequence: {SEQ_LENGTH} frames of {GRID_SIZE}x{GRID_SIZE}\")"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Train All Three Components End-to-End\n",
    "\n",
    "Now we train the full Genie pipeline. The training loop processes sequences of frames:\n",
    "\n",
    "1. **Encode** all frames in the sequence with the tokenizer.\n",
    "2. For each consecutive pair, **infer the latent action** with the LAM.\n",
    "3. Use the dynamics model to **predict the next frame tokens**.\n",
    "4. Compute the combined loss and backpropagate."
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters ---\n",
    "EMBEDDING_DIM = 32\n",
    "NUM_CODES = 64        # codebook size\n",
    "NUM_LATENT_ACTIONS = 8  # more than true 4, model must discover\n",
    "LEARNING_RATE = 3e-4\n",
    "NUM_EPOCHS = 30\n",
    "BETA_VQ = 1.0         # weight for VQ loss\n",
    "\n",
    "# --- Initialize models ---\n",
    "tokenizer = VideoTokenizer(NUM_CODES, EMBEDDING_DIM).to(device)\n",
    "lam = LatentActionModel(\n",
    "    EMBEDDING_DIM, spatial_size=4, num_actions=NUM_LATENT_ACTIONS\n",
    ").to(device)\n",
    "dynamics = DynamicsModel(\n",
    "    EMBEDDING_DIM, NUM_LATENT_ACTIONS, NUM_CODES, spatial_size=4\n",
    ").to(device)\n",
    "\n",
    "# --- Single optimizer for all components ---\n",
    "all_params = (\n",
    "    list(tokenizer.parameters())\n",
    "    + list(lam.parameters())\n",
    "    + list(dynamics.parameters())\n",
    ")\n",
    "optimizer = torch.optim.Adam(all_params, lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"Tokenizer params:  {sum(p.numel() for p in tokenizer.parameters()):,}\")\n",
    "print(f\"LAM params:        {sum(p.numel() for p in lam.parameters()):,}\")\n",
    "print(f\"Dynamics params:   {sum(p.numel() for p in dynamics.parameters()):,}\")\n",
    "print(f\"Total params:      {sum(p.numel() for p in all_params):,}\")"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop ---\n",
    "history = {\n",
    "    'recon_loss': [], 'vq_loss': [],\n",
    "    'dynamics_loss': [], 'total_loss': []\n",
    "}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    tokenizer.train()\n",
    "    lam.train()\n",
    "    dynamics.train()\n",
    "\n",
    "    epoch_losses = {k: 0.0 for k in history}\n",
    "    n_batches = 0\n",
    "\n",
    "    for batch_sequences in train_loader:\n",
    "        batch_sequences = batch_sequences.to(device)  # (B, T, 3, H, W)\n",
    "        B, T = batch_sequences.shape[:2]\n",
    "\n",
    "        total_recon = 0.0\n",
    "        total_vq = 0.0\n",
    "        total_dyn = 0.0\n",
    "\n",
    "        # Encode all frames\n",
    "        all_z = []\n",
    "        all_z_q = []\n",
    "        all_indices = []\n",
    "\n",
    "        for t in range(T):\n",
    "            frame = batch_sequences[:, t]          # (B, 3, H, W)\n",
    "            recon, indices, vq_loss, z = tokenizer(frame)\n",
    "            z_q = tokenizer.vq.codebook(indices)   # (B, h, w, D)\n",
    "\n",
    "            total_recon += F.mse_loss(recon, frame)\n",
    "            total_vq += vq_loss\n",
    "            all_z.append(z)\n",
    "            all_z_q.append(z_q)\n",
    "            all_indices.append(indices)\n",
    "\n",
    "        # For consecutive pairs, infer actions and predict next frame\n",
    "        for t in range(T - 1):\n",
    "            # Infer latent action\n",
    "            action_onehot, _ = lam(all_z[t], all_z[t+1])\n",
    "\n",
    "            # Predict next frame tokens\n",
    "            token_logits = dynamics(all_z_q[t], action_onehot)\n",
    "            target_indices = all_indices[t+1]  # (B, h, w)\n",
    "\n",
    "            # Cross-entropy loss over token predictions\n",
    "            dyn_loss = F.cross_entropy(\n",
    "                token_logits, target_indices\n",
    "            )\n",
    "            total_dyn += dyn_loss\n",
    "\n",
    "        # Average losses over time steps\n",
    "        loss_recon = total_recon / T\n",
    "        loss_vq = total_vq / T\n",
    "        loss_dyn = total_dyn / (T - 1)\n",
    "\n",
    "        loss = loss_recon + BETA_VQ * loss_vq + loss_dyn\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(all_params, 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses['recon_loss'] += loss_recon.item()\n",
    "        epoch_losses['vq_loss'] += loss_vq.item()\n",
    "        epoch_losses['dynamics_loss'] += loss_dyn.item()\n",
    "        epoch_losses['total_loss'] += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    # Record averages\n",
    "    for k in history:\n",
    "        avg = epoch_losses[k] / n_batches\n",
    "        history[k].append(avg)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{NUM_EPOCHS} | \"\n",
    "              f\"Recon: {history['recon_loss'][-1]:.4f} | \"\n",
    "              f\"VQ: {history['vq_loss'][-1]:.4f} | \"\n",
    "              f\"Dyn: {history['dynamics_loss'][-1]:.4f} | \"\n",
    "              f\"Total: {history['total_loss'][-1]:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Visualization: Tokenizer Reconstruction Quality\n",
    "\n",
    "Let us see how well our tokenizer reconstructs frames. This tells us whether the discrete bottleneck preserves enough information."
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eval()\n",
    "with torch.no_grad():\n",
    "    sample_batch = next(iter(val_loader)).to(device)\n",
    "    sample_frames = sample_batch[:8, 0]  # 8 frames from first timestep\n",
    "    recon, _, _, _ = tokenizer(sample_frames)\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "fig.suptitle(\"Video Tokenizer: Original (top) vs Reconstructed (bottom)\",\n",
    "             fontsize=13, fontweight='bold')\n",
    "\n",
    "for i in range(8):\n",
    "    orig = sample_frames[i].cpu().permute(1, 2, 0).numpy()\n",
    "    rec = recon[i].cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "    axes[0, i].imshow(orig)\n",
    "    axes[0, i].set_title(f\"Original\", fontsize=8)\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    axes[1, i].imshow(rec)\n",
    "    axes[1, i].set_title(f\"Reconstructed\", fontsize=8)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Visualization: Discovered Latent Actions\n",
    "\n",
    "Now the exciting part â€” let us see whether the Latent Action Model has discovered meaningful actions. We will color-code frame transitions by their inferred latent action and see if they cluster by true action."
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eval()\n",
    "lam.eval()\n",
    "\n",
    "# Collect inferred vs true actions\n",
    "inferred_actions = []\n",
    "true_actions_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seq_idx in range(min(500, n_train)):\n",
    "        frames = train_dataset[seq_idx].unsqueeze(0).to(device)  # (1,T,3,H,W)\n",
    "\n",
    "        for t in range(SEQ_LENGTH - 1):\n",
    "            frame_t = frames[:, t]\n",
    "            frame_t1 = frames[:, t+1]\n",
    "\n",
    "            z_t = tokenizer.encoder(frame_t)\n",
    "            z_t1 = tokenizer.encoder(frame_t1)\n",
    "\n",
    "            action_onehot, logits = lam(z_t, z_t1)\n",
    "            inferred = logits.argmax(dim=-1).item()\n",
    "\n",
    "            inferred_actions.append(inferred)\n",
    "            true_actions_list.append(all_actions[seq_idx, t])\n",
    "\n",
    "inferred_actions = np.array(inferred_actions)\n",
    "true_actions_list = np.array(true_actions_list)\n",
    "\n",
    "# Build confusion-style matrix\n",
    "action_names_true = ['Up', 'Down', 'Left', 'Right']\n",
    "confusion = np.zeros((NUM_LATENT_ACTIONS, NUM_ACTIONS))\n",
    "for inf_a, true_a in zip(inferred_actions, true_actions_list):\n",
    "    confusion[inf_a, true_a] += 1\n",
    "\n",
    "# Normalize rows\n",
    "row_sums = confusion.sum(axis=1, keepdims=True)\n",
    "row_sums[row_sums == 0] = 1\n",
    "confusion_norm = confusion / row_sums\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix\n",
    "im = axes[0].imshow(confusion_norm, cmap='Blues', aspect='auto')\n",
    "axes[0].set_xlabel(\"True Action\")\n",
    "axes[0].set_ylabel(\"Inferred Latent Action\")\n",
    "axes[0].set_xticks(range(NUM_ACTIONS))\n",
    "axes[0].set_xticklabels(action_names_true)\n",
    "axes[0].set_yticks(range(NUM_LATENT_ACTIONS))\n",
    "axes[0].set_title(\"Latent Action vs True Action\\n(each row should map to one true action)\")\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Action distribution\n",
    "action_counts = np.bincount(inferred_actions, minlength=NUM_LATENT_ACTIONS)\n",
    "axes[1].bar(range(NUM_LATENT_ACTIONS), action_counts, color='steelblue')\n",
    "axes[1].set_xlabel(\"Latent Action Index\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].set_title(\"Distribution of Discovered Actions\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute cluster purity\n",
    "purity = 0\n",
    "for i in range(NUM_LATENT_ACTIONS):\n",
    "    if confusion[i].sum() > 0:\n",
    "        purity += confusion[i].max()\n",
    "purity /= confusion.sum()\n",
    "print(f\"Cluster purity: {purity:.2%}\")\n",
    "print(\"(Higher = latent actions better align with true actions)\")"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Visualization: Dynamics Model â€” Predicted vs Actual Next Frame"
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eval()\n",
    "lam.eval()\n",
    "dynamics.eval()\n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(9, 12))\n",
    "col_titles = ['Current Frame', 'Actual Next', 'Predicted Next']\n",
    "\n",
    "with torch.no_grad():\n",
    "    for row in range(4):\n",
    "        idx = np.random.randint(len(val_dataset))\n",
    "        t = np.random.randint(SEQ_LENGTH - 1)\n",
    "\n",
    "        frames = val_dataset[idx].unsqueeze(0).to(device)\n",
    "        frame_t = frames[:, t]\n",
    "        frame_t1 = frames[:, t + 1]\n",
    "\n",
    "        # Encode and infer action\n",
    "        z_t = tokenizer.encoder(frame_t)\n",
    "        z_t1 = tokenizer.encoder(frame_t1)\n",
    "        z_q_t, indices_t, _, _ = tokenizer.vq(z_t)\n",
    "\n",
    "        action_onehot, _ = lam(z_t, z_t1)\n",
    "\n",
    "        # Predict next frame tokens\n",
    "        token_logits = dynamics(z_q_t, action_onehot)\n",
    "        pred_indices = token_logits.argmax(dim=1)  # (B, h, w)\n",
    "\n",
    "        # Decode both actual and predicted\n",
    "        actual_recon = tokenizer.decode_indices(\n",
    "            tokenizer.encode(frame_t1)\n",
    "        )\n",
    "        pred_recon = tokenizer.decode_indices(pred_indices)\n",
    "\n",
    "        # Display\n",
    "        for col, img_tensor in enumerate([frame_t, frame_t1, pred_recon]):\n",
    "            img = img_tensor[0].cpu().permute(1, 2, 0).numpy()\n",
    "            img = np.clip(img, 0, 1)\n",
    "            axes[row, col].imshow(img)\n",
    "            axes[row, col].axis('off')\n",
    "            if row == 0:\n",
    "                axes[row, col].set_title(col_titles[col], fontsize=11)\n",
    "\n",
    "fig.suptitle(\"Dynamics Model: Current â†’ Predicted vs Actual Next Frame\",\n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ðŸ”§ Your Turn (TODO)\n",
    "\n",
    "Now it is your turn! We have two exercises that test your understanding of the core mechanisms."
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1: Implement Vector Quantization\n",
    "\n",
    "The vector quantization step is the heart of the tokenizer. Given a continuous encoder output, you must:\n",
    "1. Compute distances to all codebook entries\n",
    "2. Find the nearest codebook vector\n",
    "3. Apply the straight-through estimator so gradients flow through"
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_quantize(z, codebook_weight):\n",
    "    \"\"\"Implement the vector quantization step.\n",
    "\n",
    "    Args:\n",
    "        z: Continuous encoder output (batch, height, width, embedding_dim)\n",
    "        codebook_weight: Codebook vectors (num_codes, embedding_dim)\n",
    "\n",
    "    Returns:\n",
    "        z_q: Quantized output (same shape as z)\n",
    "        indices: Token indices (batch, height, width)\n",
    "\n",
    "    Hints:\n",
    "        - Flatten z to (B*H*W, D) for distance computation\n",
    "        - Use the squared distance formula: ||a-b||^2 = ||a||^2 - 2*a.b + ||b||^2\n",
    "        - For straight-through: z_q_st = z + (z_q - z).detach()\n",
    "    \"\"\"\n",
    "    B, H, W, D = z.shape\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Flatten z to (B*H*W, D)\n",
    "    # Step 2: Compute squared distances to each codebook entry\n",
    "    # Step 3: Find nearest codebook indices (argmin)\n",
    "    # Step 4: Look up codebook vectors for those indices\n",
    "    # Step 5: Reshape z_q back to (B, H, W, D)\n",
    "    # Step 6: Apply straight-through estimator\n",
    "    # ==============================\n",
    "\n",
    "    flat_z = ???         # YOUR CODE: flatten\n",
    "    distances = ???      # YOUR CODE: squared distances\n",
    "    indices = ???        # YOUR CODE: argmin\n",
    "    z_q = ???            # YOUR CODE: look up codebook vectors\n",
    "    z_q = z_q.reshape(B, H, W, D)\n",
    "    z_q_st = ???         # YOUR CODE: straight-through estimator\n",
    "\n",
    "    indices = indices.reshape(B, H, W)\n",
    "    return z_q_st, indices"
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification Cell for TODO 1"
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Verification: Run this cell to check your implementation\n",
    "torch.manual_seed(42)\n",
    "test_z = torch.randn(2, 4, 4, 32)\n",
    "test_codebook = torch.randn(64, 32)\n",
    "\n",
    "try:\n",
    "    test_zq, test_indices = vector_quantize(test_z, test_codebook)\n",
    "\n",
    "    assert test_zq.shape == test_z.shape, \\\n",
    "        f\"Wrong output shape: expected {test_z.shape}, got {test_zq.shape}\"\n",
    "    assert test_indices.shape == (2, 4, 4), \\\n",
    "        f\"Wrong indices shape: expected (2, 4, 4), got {test_indices.shape}\"\n",
    "    assert test_indices.dtype == torch.int64, \\\n",
    "        f\"Indices should be int64, got {test_indices.dtype}\"\n",
    "    assert torch.all(test_indices >= 0) and torch.all(test_indices < 64), \\\n",
    "        \"Indices should be in range [0, 64)\"\n",
    "\n",
    "    # Check straight-through: z_q should require grad if z does\n",
    "    test_z_grad = test_z.clone().requires_grad_(True)\n",
    "    test_zq_grad, _ = vector_quantize(test_z_grad, test_codebook)\n",
    "    loss = test_zq_grad.sum()\n",
    "    loss.backward()\n",
    "    assert test_z_grad.grad is not None, \\\n",
    "        \"Gradients should flow through straight-through estimator\"\n",
    "\n",
    "    print(\"âœ… Correct! Your vector quantization implementation works perfectly.\")\n",
    "    print(f\"   Output shape: {test_zq.shape}\")\n",
    "    print(f\"   Indices range: [{test_indices.min().item()}, {test_indices.max().item()}]\")\n",
    "    print(f\"   Gradients flow: True\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(\"   Check your implementation and try again.\")"
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement the Interactive Generation Loop\n",
    "\n",
    "This is where everything comes together for interactivity. Given a starting frame and a sequence of user-chosen actions, generate a trajectory of frames â€” just like playing a game!"
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def interactive_generate(start_frame, action_sequence,\n",
    "                         tokenizer, lam, dynamics):\n",
    "    \"\"\"Generate a sequence of frames interactively.\n",
    "\n",
    "    Args:\n",
    "        start_frame: Initial frame tensor (1, 3, H, W)\n",
    "        action_sequence: List of latent action indices [int]\n",
    "        tokenizer: Trained VideoTokenizer\n",
    "        lam: Trained LatentActionModel\n",
    "        dynamics: Trained DynamicsModel\n",
    "\n",
    "    Returns:\n",
    "        generated_frames: List of numpy images (H, W, 3)\n",
    "\n",
    "    Hints:\n",
    "        - Encode the starting frame with the tokenizer\n",
    "        - For each action in the sequence:\n",
    "            1. Convert action index to one-hot\n",
    "            2. Feed (current_z_q, action_onehot) to dynamics model\n",
    "            3. Get predicted token indices from logits (argmax)\n",
    "            4. Decode predicted indices to an image\n",
    "            5. Re-encode the decoded image for the next step\n",
    "        - Collect each generated frame as a numpy image\n",
    "    \"\"\"\n",
    "    tokenizer.eval()\n",
    "    dynamics.eval()\n",
    "\n",
    "    generated_frames = []\n",
    "\n",
    "    # Store the starting frame\n",
    "    start_img = start_frame[0].cpu().permute(1, 2, 0).numpy()\n",
    "    generated_frames.append(np.clip(start_img, 0, 1))\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Encode starting frame -> z, z_q, indices\n",
    "    # Step 2: Loop through action_sequence:\n",
    "    #     a. Create one-hot action vector\n",
    "    #     b. Predict next token logits with dynamics model\n",
    "    #     c. Get predicted indices (argmax over dim=1)\n",
    "    #     d. Decode indices to image with tokenizer.decode_indices\n",
    "    #     e. Append decoded image (as numpy) to generated_frames\n",
    "    #     f. Re-encode decoded image for next iteration\n",
    "    # ==============================\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return generated_frames"
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification Cell for TODO 2"
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Verification: Run this cell to check your implementation\n",
    "tokenizer.eval()\n",
    "dynamics.eval()\n",
    "\n",
    "# Get a starting frame\n",
    "test_seq = val_dataset[0].unsqueeze(0).to(device)\n",
    "test_start = test_seq[:, 0]  # (1, 3, H, W)\n",
    "test_actions = [0, 1, 2, 3, 0, 1]  # 6-step trajectory\n",
    "\n",
    "try:\n",
    "    generated = interactive_generate(\n",
    "        test_start, test_actions, tokenizer, lam, dynamics\n",
    "    )\n",
    "\n",
    "    assert len(generated) == len(test_actions) + 1, \\\n",
    "        f\"Expected {len(test_actions)+1} frames, got {len(generated)}\"\n",
    "    assert generated[0].shape == (GRID_SIZE, GRID_SIZE, 3), \\\n",
    "        f\"Wrong frame shape: expected ({GRID_SIZE},{GRID_SIZE},3), got {generated[0].shape}\"\n",
    "\n",
    "    # Visualize the generated trajectory\n",
    "    fig, axes = plt.subplots(1, len(generated), figsize=(16, 3))\n",
    "    fig.suptitle(\"âœ… Interactive Generation â€” Your Implementation Works!\",\n",
    "                 fontsize=13, fontweight='bold')\n",
    "    for i, frame in enumerate(generated):\n",
    "        axes[i].imshow(np.clip(frame, 0, 1))\n",
    "        axes[i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[i].set_title(\"Start\", fontsize=9)\n",
    "        else:\n",
    "            axes[i].set_title(f\"a={test_actions[i-1]}\", fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"âœ… Correct! Your interactive generation loop works perfectly.\")\n",
    "    print(f\"   Generated {len(generated)} frames from {len(test_actions)} actions.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(\"   Check your implementation and try again.\")"
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together â€” The Full Genie Pipeline\n",
    "\n",
    "Now let us step back and see the complete pipeline working end-to-end. This function runs the full Genie process: tokenize, infer actions, predict dynamics."
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def full_genie_pipeline(sequence, tokenizer, lam, dynamics):\n",
    "    \"\"\"Run the full Genie pipeline on a sequence of frames.\n",
    "\n",
    "    Args:\n",
    "        sequence: (T, 3, H, W) tensor of frames\n",
    "\n",
    "    Returns:\n",
    "        dict with all intermediate outputs\n",
    "    \"\"\"\n",
    "    tokenizer.eval()\n",
    "    lam.eval()\n",
    "    dynamics.eval()\n",
    "\n",
    "    T = sequence.shape[0]\n",
    "    sequence = sequence.unsqueeze(0).to(device)  # (1, T, 3, H, W)\n",
    "\n",
    "    results = {\n",
    "        'originals': [], 'reconstructed': [],\n",
    "        'token_indices': [], 'inferred_actions': [],\n",
    "        'predicted_next': []\n",
    "    }\n",
    "\n",
    "    # Stage 1: Tokenize all frames\n",
    "    encoded_z = []\n",
    "    for t in range(T):\n",
    "        frame = sequence[:, t]\n",
    "        recon, indices, _, z = tokenizer(frame)\n",
    "        z_q = tokenizer.vq.codebook(indices)\n",
    "\n",
    "        results['originals'].append(\n",
    "            frame[0].cpu().permute(1, 2, 0).numpy()\n",
    "        )\n",
    "        results['reconstructed'].append(\n",
    "            recon[0].cpu().permute(1, 2, 0).numpy()\n",
    "        )\n",
    "        results['token_indices'].append(indices[0].cpu().numpy())\n",
    "        encoded_z.append((z, z_q, indices))\n",
    "\n",
    "    # Stage 2: Infer actions between consecutive frames\n",
    "    for t in range(T - 1):\n",
    "        z_t = encoded_z[t][0]\n",
    "        z_t1 = encoded_z[t + 1][0]\n",
    "        action_onehot, logits = lam(z_t, z_t1)\n",
    "        action_idx = logits.argmax(dim=-1).item()\n",
    "        results['inferred_actions'].append(action_idx)\n",
    "\n",
    "        # Stage 3: Predict next frame\n",
    "        z_q_t = encoded_z[t][1]\n",
    "        token_logits = dynamics(z_q_t, action_onehot)\n",
    "        pred_indices = token_logits.argmax(dim=1)\n",
    "        pred_frame = tokenizer.decode_indices(pred_indices)\n",
    "        results['predicted_next'].append(\n",
    "            pred_frame[0].cpu().permute(1, 2, 0).numpy()\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run on a validation sequence\n",
    "test_sequence = val_dataset[42]\n",
    "results = full_genie_pipeline(test_sequence, tokenizer, lam, dynamics)\n",
    "print(\"Full Genie pipeline complete!\")\n",
    "print(f\"Processed {len(results['originals'])} frames\")\n",
    "print(f\"Inferred {len(results['inferred_actions'])} actions\")"
   ],
   "id": "cell_51"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "### ðŸ“Š Training Loss Curves"
   ],
   "id": "cell_52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "loss_names = ['recon_loss', 'vq_loss', 'dynamics_loss', 'total_loss']\n",
    "titles = [\n",
    "    'Reconstruction Loss\\n(Tokenizer quality)',\n",
    "    'VQ Commitment Loss\\n(Codebook alignment)',\n",
    "    'Dynamics Loss\\n(Next-frame prediction)',\n",
    "    'Total Loss'\n",
    "]\n",
    "colors = ['#2196F3', '#FF9800', '#4CAF50', '#9C27B0']\n",
    "\n",
    "for ax, name, title, color in zip(axes, loss_names, titles, colors):\n",
    "    ax.plot(history[name], color=color, linewidth=2)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title(title, fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(0, NUM_EPOCHS - 1)\n",
    "\n",
    "fig.suptitle(\"Training Progress â€” All Three Components\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_53"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Discovery Quality\n",
    "\n",
    "How well did the model discover the true 4 actions from unlabeled data? Let us compute a more detailed analysis."
   ],
   "id": "cell_54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-compute with more data\n",
    "tokenizer.eval()\n",
    "lam.eval()\n",
    "\n",
    "all_inf = []\n",
    "all_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seq_idx in range(min(1000, n_train)):\n",
    "        frames = train_dataset[seq_idx].unsqueeze(0).to(device)\n",
    "        for t in range(SEQ_LENGTH - 1):\n",
    "            z_t = tokenizer.encoder(frames[:, t])\n",
    "            z_t1 = tokenizer.encoder(frames[:, t+1])\n",
    "            _, logits = lam(z_t, z_t1)\n",
    "            all_inf.append(logits.argmax(dim=-1).item())\n",
    "            all_true.append(all_actions[seq_idx, t])\n",
    "\n",
    "all_inf = np.array(all_inf)\n",
    "all_true = np.array(all_true)\n",
    "\n",
    "# For each latent action, find its dominant true action\n",
    "latent_to_true = {}\n",
    "for la in range(NUM_LATENT_ACTIONS):\n",
    "    mask = all_inf == la\n",
    "    if mask.sum() > 0:\n",
    "        counts = np.bincount(all_true[mask], minlength=NUM_ACTIONS)\n",
    "        dominant = counts.argmax()\n",
    "        latent_to_true[la] = {\n",
    "            'dominant': action_names_true[dominant],\n",
    "            'purity': counts.max() / counts.sum(),\n",
    "            'count': mask.sum()\n",
    "        }\n",
    "\n",
    "print(\"Latent Action Discovery Results:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Latent':>8} {'Maps to':>10} {'Purity':>10} {'Count':>8}\")\n",
    "print(\"-\" * 50)\n",
    "for la in sorted(latent_to_true.keys()):\n",
    "    info = latent_to_true[la]\n",
    "    print(f\"{la:>8} {info['dominant']:>10} {info['purity']:>9.1%} {info['count']:>8}\")\n",
    "\n",
    "# Overall accuracy (with best mapping)\n",
    "mapped_correct = sum(\n",
    "    1 for i, t in zip(all_inf, all_true)\n",
    "    if i in latent_to_true and\n",
    "    latent_to_true[i]['dominant'] == action_names_true[t]\n",
    ")\n",
    "overall_acc = mapped_correct / len(all_true)\n",
    "print(f\"\\nOverall action discovery accuracy: {overall_acc:.1%}\")"
   ],
   "id": "cell_55"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ðŸŽ¯ Final Output â€” Interactive World Generation\n",
    "\n",
    "This is the moment everything comes together. We will generate interactive worlds by starting from a single frame and applying different action sequences. **Same starting frame, different action sequences lead to different worlds.**"
   ],
   "id": "cell_56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_world(start_frame, action_sequence,\n",
    "                   tokenizer, dynamics, device):\n",
    "    \"\"\"Generate an interactive world from a starting frame.\n",
    "\n",
    "    This is the core Genie inference loop.\n",
    "    \"\"\"\n",
    "    tokenizer.eval()\n",
    "    dynamics.eval()\n",
    "\n",
    "    frames = [start_frame[0].cpu().permute(1, 2, 0).numpy()]\n",
    "\n",
    "    # Encode starting frame\n",
    "    z = tokenizer.encoder(start_frame)\n",
    "    z_q, indices, _, _ = tokenizer.vq(z)\n",
    "\n",
    "    for action_idx in action_sequence:\n",
    "        # Create action one-hot\n",
    "        action_oh = F.one_hot(\n",
    "            torch.tensor([action_idx]), NUM_LATENT_ACTIONS\n",
    "        ).float().to(device)\n",
    "\n",
    "        # Predict next frame tokens\n",
    "        token_logits = dynamics(z_q, action_oh)\n",
    "        pred_indices = token_logits.argmax(dim=1)\n",
    "\n",
    "        # Decode to image\n",
    "        pred_frame = tokenizer.decode_indices(pred_indices)\n",
    "        frames.append(\n",
    "            pred_frame[0].cpu().permute(1, 2, 0).numpy()\n",
    "        )\n",
    "\n",
    "        # Re-encode for next step\n",
    "        z = tokenizer.encoder(pred_frame)\n",
    "        z_q, _, _, _ = tokenizer.vq(z)\n",
    "\n",
    "    return [np.clip(f, 0, 1) for f in frames]"
   ],
   "id": "cell_57"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us generate worlds with different action sequences from the same starting frame."
   ],
   "id": "cell_58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most-used latent action for each true direction\n",
    "best_actions = {}\n",
    "for la, info in latent_to_true.items():\n",
    "    direction = info['dominant']\n",
    "    if direction not in best_actions or info['count'] > best_actions[direction][1]:\n",
    "        best_actions[direction] = (la, info['count'])\n",
    "\n",
    "print(\"Best latent action mapping:\")\n",
    "for direction, (la, count) in best_actions.items():\n",
    "    print(f\"  {direction}: latent action {la} ({count} examples)\")"
   ],
   "id": "cell_59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a starting frame\n",
    "np.random.seed(99)\n",
    "start_frames_np, _ = generate_sprite_sequence(\n",
    "    1, GRID_SIZE, SPRITE_SIZE\n",
    ")\n",
    "start_tensor = torch.FloatTensor(\n",
    "    start_frames_np[0]\n",
    ").permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "# Define 4 different \"journeys\" from the same starting point\n",
    "# Use the discovered latent actions\n",
    "journey_length = 10\n",
    "journeys = {}\n",
    "\n",
    "# Try to use discovered mappings, fallback to simple indices\n",
    "action_map = {d: la for d, (la, _) in best_actions.items()}\n",
    "up_a = action_map.get('Up', 0)\n",
    "down_a = action_map.get('Down', 1)\n",
    "left_a = action_map.get('Left', 2)\n",
    "right_a = action_map.get('Right', 3)\n",
    "\n",
    "journeys['Go Right'] = [right_a] * journey_length\n",
    "journeys['Go Down'] = [down_a] * journey_length\n",
    "journeys['Zigzag (R/D)'] = [right_a, down_a] * (journey_length // 2)\n",
    "journeys['Spiral'] = (\n",
    "    [right_a]*3 + [down_a]*2 + [left_a]*3 + [up_a]*2\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    len(journeys), journey_length + 1, figsize=(20, 8)\n",
    ")\n",
    "fig.suptitle(\n",
    "    \"ðŸŽ¯ Interactive World Generation\\n\"\n",
    "    \"Same Starting Frame â†’ Different Action Sequences â†’ Different Worlds\",\n",
    "    fontsize=14, fontweight='bold'\n",
    ")\n",
    "\n",
    "for row, (journey_name, action_seq) in enumerate(journeys.items()):\n",
    "    frames = generate_world(\n",
    "        start_tensor, action_seq, tokenizer, dynamics, device\n",
    "    )\n",
    "\n",
    "    for col in range(min(len(frames), journey_length + 1)):\n",
    "        axes[row, col].imshow(frames[col])\n",
    "        axes[row, col].axis('off')\n",
    "        if col == 0:\n",
    "            axes[row, col].set_title(\"Start\", fontsize=8)\n",
    "        elif col < len(action_seq) + 1:\n",
    "            axes[row, col].set_title(f\"t={col}\", fontsize=8)\n",
    "\n",
    "    axes[row, 0].set_ylabel(journey_name, fontsize=11, rotation=0,\n",
    "                            labelpad=70, va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_60"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Grand Finale: Random Exploration\n",
    "\n",
    "Let us generate a long random trajectory â€” the model exploring its own generated world."
   ],
   "id": "cell_61"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long random exploration\n",
    "np.random.seed(7)\n",
    "random_actions = np.random.choice(\n",
    "    list(range(NUM_LATENT_ACTIONS)), size=20\n",
    ").tolist()\n",
    "\n",
    "exploration_frames = generate_world(\n",
    "    start_tensor, random_actions, tokenizer, dynamics, device\n",
    ")\n",
    "\n",
    "# Display as a 3-row grid\n",
    "n_cols = 7\n",
    "n_rows = 3\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 7))\n",
    "fig.suptitle(\n",
    "    \"Random Exploration: 20 Steps of Autonomous World Generation\",\n",
    "    fontsize=14, fontweight='bold'\n",
    ")\n",
    "\n",
    "for i in range(min(n_rows * n_cols, len(exploration_frames))):\n",
    "    r, c = i // n_cols, i % n_cols\n",
    "    axes[r, c].imshow(exploration_frames[i])\n",
    "    axes[r, c].axis('off')\n",
    "    axes[r, c].set_title(f\"t={i}\", fontsize=9)\n",
    "\n",
    "# Turn off unused axes\n",
    "for i in range(len(exploration_frames), n_rows * n_cols):\n",
    "    r, c = i // n_cols, i % n_cols\n",
    "    axes[r, c].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸŽ‰ Congratulations!\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"You have built a simplified Genie-style model that:\")\n",
    "print(\"  1. Compresses frames into discrete tokens (VQ-VAE)\")\n",
    "print(\"  2. Discovers actions from unlabeled video (LAM)\")\n",
    "print(\"  3. Predicts interactive next frames (Dynamics Model)\")\n",
    "print()\n",
    "print(\"From a SINGLE starting frame and a sequence of actions,\")\n",
    "print(\"you can now generate entire interactive trajectories â€”\")\n",
    "print(\"just like playing a video game that was never programmed!\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "cell_62"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### ðŸ¤” Reflection Questions\n",
    "\n",
    "1. **Why discrete actions?** Our Latent Action Model outputs a *discrete* categorical distribution over actions, not a continuous vector. Why is discreteness important for interactive control? What would happen if we used a continuous latent action space instead?\n",
    "\n",
    "2. **Codebook size matters.** We used a codebook of 64 entries for the video tokenizer. What would happen if we used only 4 entries? What about 1024? How does codebook size relate to the reconstruction quality vs. compactness tradeoff?\n",
    "\n",
    "3. **Scaling to real images.** Our sprites are 16x16 pixels. Genie 1 works on 256x256 platformer games. Genie 2 generates minute-long videos. What architectural changes would be needed to scale this approach to realistic images? (Hint: think about the tokenizer, the dynamics model architecture, and training data.)\n",
    "\n",
    "### ðŸ”§ Optional Challenges\n",
    "\n",
    "**Challenge 1: Better Dynamics Model.** Replace the CNN dynamics model with a small Transformer. Instead of operating on the 2D spatial grid with convolutions, flatten the token grid into a sequence, prepend the action token, and use self-attention to predict next-frame tokens. Does this improve prediction quality?\n",
    "\n",
    "**Challenge 2: Conditional Starting Frames.** Instead of using a frame from the dataset, try creating a hand-drawn starting frame (e.g., draw a white square at a specific position). Feed it to the model and generate an interactive trajectory. Does the model generalize to starting frames it has never seen during training? This is exactly what Genie does â€” it takes a *sketch* and makes it playable."
   ],
   "id": "cell_63"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genie at Scale: From Genie 1 to Genie 3\n",
    "\n",
    "Our simplified version captures the core idea, but the real Genie models are far more powerful:\n",
    "\n",
    "| Feature | Our Notebook | Genie 1 (2024) | Genie 2 (2024) | Genie 3 (2025) |\n",
    "|---------|-------------|-----------------|-----------------|-----------------|\n",
    "| **Resolution** | 16x16 | 256x256 | 720p | 1080p |\n",
    "| **Duration** | 8 frames | 16 frames | ~60 seconds | Real-time |\n",
    "| **Training data** | 3K sprite seqs | 200K hrs video | Internet-scale | Internet-scale |\n",
    "| **Tokenizer** | Small CNN VQ-VAE | ST-ViViT | Large VQ-VAE | Advanced VQ |\n",
    "| **Dynamics** | Small CNN | 11B param Transformer | Large Transformer | Optimized Transformer |\n",
    "| **Actions** | 4 discovered | 8 discovered | Rich action space | Continuous control |\n",
    "| **3D** | No | No | Limited | Full 3D worlds |\n",
    "\n",
    "The progression from Genie 1 to Genie 3 shows a clear trajectory: **from simple 2D platformers to real-time, interactive 3D worlds** â€” all learned from unlabeled video.\n",
    "\n",
    "This brings us to a profound question that we will explore in the next notebook: If a model can generate interactive worlds from a single image, what happens when we give it the ability to *act* in those worlds? This is where world models meet embodied agents â€” and where the future of AI truly begins."
   ],
   "id": "cell_64"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- **Genie 1:** Bruce et al., \"Genie: Generative Interactive Environments,\" *ICML 2024*. [arXiv:2402.15391](https://arxiv.org/abs/2402.15391)\n",
    "- **Genie 2:** DeepMind, \"Genie 2: A Large-Scale Foundation World Model,\" *2024*. [Blog post](https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/)\n",
    "- **VQ-VAE:** van den Oord et al., \"Neural Discrete Representation Learning,\" *NeurIPS 2017*. [arXiv:1711.00937](https://arxiv.org/abs/1711.00937)\n",
    "\n",
    "See you in the next notebook!"
   ],
   "id": "cell_65"
  }
 ]
}