{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "JEPA: Predicting in Abstract Space â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ JEPA: Predicting in Abstract Space â€” Why Pixels Don't Matter\n",
    "**Notebook 4 of 6 â€” World Action Models Series | Vizuara**\n",
    "\n",
    "*Estimated time: ~40 minutes*\n",
    "\n",
    "In this notebook, we will build **I-JEPA (Image-based Joint Embedding Predictive Architecture)** from scratch and discover why predicting in abstract representation space is far more powerful than predicting pixels."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup and GPU Check ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn -q"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Let us start with a simple thought experiment.\n",
    "\n",
    "When you imagine catching a ball thrown towards you, what goes through your mind? Do you simulate every blade of grass on the field, every cloud in the sky, the exact pattern of light reflecting off the ball's surface?\n",
    "\n",
    "Of course not. You predict **trajectory, speed, and landing position** â€” the things that actually matter. You filter out all the irrelevant visual details and reason about the **abstract essence** of what is happening.\n",
    "\n",
    "This is precisely the insight behind **JEPA â€” Joint Embedding Predictive Architecture**, proposed by Yann LeCun. Instead of predicting raw pixels (like a VAE or diffusion model would), JEPA predicts in **abstract representation space**. It learns to capture what matters and ignore what does not.\n",
    "\n",
    "> **By the end of this notebook, you will build I-JEPA from scratch and watch it learn powerful visual representations without ever generating a single pixel.**\n",
    "\n",
    "We will:\n",
    "1. Understand why pixel-level prediction wastes capacity\n",
    "2. Walk through the JEPA mathematics step by step\n",
    "3. Implement patch embeddings, a simplified Vision Transformer encoder, a predictor, and the EMA target encoder\n",
    "4. Train I-JEPA on CIFAR-10 and visualize the learned representations with t-SNE\n",
    "\n",
    "Let us begin."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition â€” Why Predict in Abstract Space?\n",
    "\n",
    "### The Problem with Pixel Prediction\n",
    "\n",
    "Generative models like VAEs and diffusion models learn by reconstructing pixels. They must predict every single pixel value â€” the exact shade of grass, the precise pattern of shadows, the specific texture of a wall.\n",
    "\n",
    "Think about what that means: a model that is trying to understand \"there is a dog in this image\" must also spend capacity learning \"the third blade of grass is 2 pixels wide and has RGB value (34, 120, 45).\"\n",
    "\n",
    "This is enormously wasteful. The model wastes representational power on **surface-level details** that carry little semantic meaning.\n",
    "\n",
    "### JEPA's Core Insight\n",
    "\n",
    "JEPA says: **predict in abstract space, not pixel space.**\n",
    "\n",
    "Instead of forcing the model to reconstruct the raw image, we:\n",
    "1. Encode the image into a compact representation (an embedding)\n",
    "2. Predict **embeddings** of missing parts from the embeddings of visible parts\n",
    "\n",
    "The key difference: embeddings naturally discard irrelevant details and preserve semantic meaning.\n",
    "\n",
    "### An Analogy\n",
    "\n",
    "Imagine you are a student and your professor asks you to summarize a chapter from a textbook.\n",
    "\n",
    "- **Pixel prediction** is like being asked to copy the chapter word-for-word â€” you would reproduce everything, including typos, formatting quirks, and irrelevant footnotes.\n",
    "- **JEPA (abstract prediction)** is like being asked to write a summary â€” you capture the **meaning** and ignore the surface noise.\n",
    "\n",
    "Which approach demonstrates deeper understanding? The summary, of course. And that is what JEPA optimizes for."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤” Think About This\n",
    "\n",
    "Before we dive into the mathematics, consider this question:\n",
    "\n",
    "> *If a robot needs to pick up a mug from a table, does it need to predict the exact reflection pattern on the mug's surface? What information actually matters for the task â€” and what is just noise?*\n",
    "\n",
    "The information that matters: the mug's **position**, **orientation**, **shape**, and **size**. The reflection pattern? Completely irrelevant. Yet a pixel-level generative model would spend significant capacity modeling those reflections.\n",
    "\n",
    "JEPA learns to focus on what matters by predicting in a space where irrelevant details have already been discarded."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How I-JEPA Works at a High Level\n",
    "\n",
    "Here is the recipe for I-JEPA:\n",
    "\n",
    "1. Take an image and split it into **patches** (like a grid of small squares)\n",
    "2. **Mask out** some patches (the \"target\" patches)\n",
    "3. **Encode** the visible patches using a context encoder\n",
    "4. **Encode** the masked patches using a separate target encoder (which the model does not train directly via gradients)\n",
    "5. Use a **predictor** to predict the target embeddings from the context embeddings\n",
    "6. The loss is simply: how close are the predicted embeddings to the actual target embeddings?\n",
    "\n",
    "The target encoder is updated slowly using an **Exponential Moving Average (EMA)** of the context encoder's weights. This is the critical trick that prevents the model from finding a trivial shortcut (like mapping everything to zero).\n",
    "\n",
    "Now let us formalize this."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics of JEPA\n",
    "\n",
    "Let us walk through the mathematics step by step. We will define each component and then immediately explain what it means computationally.\n",
    "\n",
    "### 3.1 Context Encoder\n",
    "\n",
    "$$s_x = E_\\theta(x)$$\n",
    "\n",
    "Here, $x$ represents the **visible patches** of the image, $E_\\theta$ is our context encoder (a Vision Transformer with learnable parameters $\\theta$), and $s_x$ is the resulting embedding â€” a vector that captures the semantic content of the visible patches.\n",
    "\n",
    "**Computationally:** we take the visible patches, pass each one through a shared encoder network, and get out embedding vectors. If we have 10 visible patches and our embedding dimension is 128, then $s_x$ is a matrix of shape $(10, 128)$."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Target Encoder\n",
    "\n",
    "$$s_y = E_{\\bar{\\theta}}(y)$$\n",
    "\n",
    "Here, $y$ represents the **masked (target) patches**, and $E_{\\bar{\\theta}}$ is the target encoder. The bar over $\\theta$ means this encoder uses a **different set of weights** â€” specifically, an exponentially moving average of the context encoder's weights. Critically, we apply **stop-gradient** to the target encoder: it does not receive gradients during backpropagation.\n",
    "\n",
    "**Computationally:** we pass the masked patches through this second encoder to get their \"ground truth\" embeddings. If we have 6 masked patches, $s_y$ has shape $(6, 128)$.\n",
    "\n",
    "### 3.3 Predictor\n",
    "\n",
    "$$\\hat{s}_y = f_\\theta(s_x, c)$$\n",
    "\n",
    "This is the heart of JEPA. The predictor $f_\\theta$ takes the context embeddings $s_x$ and a conditioning signal $c$ (which tells the predictor **where** the masked patches are), and predicts what the target embeddings should be.\n",
    "\n",
    "**Computationally:** the predictor is a small Transformer that receives the context embeddings plus learnable \"mask tokens\" positioned at the masked locations, and outputs predicted embeddings for the masked positions only."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 The Loss Function\n",
    "\n",
    "$$\\mathcal{L} = \\| \\hat{s}_y - \\text{sg}(s_y) \\|^2$$\n",
    "\n",
    "This is a simple **Mean Squared Error (MSE)** between the predicted embeddings $\\hat{s}_y$ and the actual target embeddings $s_y$. The $\\text{sg}(\\cdot)$ denotes **stop-gradient** â€” we do not backpropagate through the target encoder.\n",
    "\n",
    "**Computationally:** for each masked patch, we compute the squared difference between the predicted embedding vector and the actual embedding vector, then average across all masked patches.\n",
    "\n",
    "Let us plug in some simple numbers to see how this works.\n",
    "\n",
    "Suppose we have 2 masked patches with embedding dimension 3:\n",
    "\n",
    "- Predicted embeddings: $\\hat{s}_y = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]$\n",
    "- Target embeddings: $s_y = [[1.1, 1.9, 3.2], [3.8, 5.1, 5.9]]$\n",
    "\n",
    "The loss for each patch:\n",
    "- Patch 1: $(1.0 - 1.1)^2 + (2.0 - 1.9)^2 + (3.0 - 3.2)^2 = 0.01 + 0.01 + 0.04 = 0.06$\n",
    "- Patch 2: $(4.0 - 3.8)^2 + (5.0 - 5.1)^2 + (6.0 - 5.9)^2 = 0.04 + 0.01 + 0.01 = 0.06$\n",
    "\n",
    "Average loss: $(0.06 + 0.06) / 2 = 0.06$\n",
    "\n",
    "This tells us our predictor is fairly close to the targets â€” a loss of 0.06 means the predicted embeddings are quite accurate. This is exactly what we want: as training progresses, this loss should decrease, meaning our predictions in abstract space get better and better."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 EMA Update â€” Preventing Representation Collapse\n",
    "\n",
    "$$\\bar{\\theta} \\leftarrow \\tau \\bar{\\theta} + (1 - \\tau)\\theta$$\n",
    "\n",
    "Here, $\\tau$ is the EMA decay rate (typically 0.996 to 0.999), $\\bar{\\theta}$ are the target encoder weights, and $\\theta$ are the context encoder weights.\n",
    "\n",
    "**Why do we need this?** Without the EMA + stop-gradient mechanism, the model could find a trivial solution: map every input to the same constant embedding (say, all zeros). Then the predictor's job is trivially easy â€” just predict zeros. The loss would be zero, but the representations would be completely useless. This is called **representation collapse**.\n",
    "\n",
    "The EMA update prevents this by making the target encoder a **slowly evolving** version of the context encoder. Since the target does not receive gradients, it cannot collapse to a trivial solution. And since it tracks the context encoder slowly, it provides a stable, meaningful target.\n",
    "\n",
    "Let us plug in some numbers. Suppose $\\tau = 0.99$:\n",
    "\n",
    "- Current target encoder weight: $\\bar{\\theta} = 5.0$\n",
    "- Current context encoder weight: $\\theta = 5.5$\n",
    "- After EMA update: $\\bar{\\theta} \\leftarrow 0.99 \\times 5.0 + 0.01 \\times 5.5 = 4.95 + 0.055 = 5.005$\n",
    "\n",
    "Notice how the target encoder weight barely changed â€” it moved from 5.0 to 5.005. This is the \"slow\" tracking that keeps the target stable. Over hundreds of training steps, the target gradually catches up to the context encoder, providing a smooth and stable learning signal."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let Us Build It â€” Component by Component\n",
    "\n",
    "Now let us implement I-JEPA from scratch. We will build each component independently, test it, and then combine everything.\n",
    "\n",
    "### 4.1 Load and Explore CIFAR-10"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load CIFAR-10 ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2470, 0.2399, 0.2010))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True,\n",
    "    download=True, transform=transform\n",
    ")\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False,\n",
    "    download=True, transform=transform\n",
    ")\n",
    "\n",
    "CLASSES = trainset.classes\n",
    "print(f\"Training samples: {len(trainset)}\")\n",
    "print(f\"Test samples: {len(testset)}\")\n",
    "print(f\"Classes: {CLASSES}\")\n",
    "print(f\"Image shape: {trainset[0][0].shape}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualize some CIFAR-10 images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "# Unnormalize for display\n",
    "mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "std = torch.tensor([0.2470, 0.2399, 0.2010]).view(3, 1, 1)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img, label = trainset[i]\n",
    "    img_display = (img * std + mean).clamp(0, 1)\n",
    "    ax.imshow(img_display.permute(1, 2, 0).numpy())\n",
    "    ax.set_title(CLASSES[label], fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"CIFAR-10 Sample Images\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Patch Embedding â€” Splitting Images into Patches\n",
    "\n",
    "The first step in I-JEPA is to split each image into a grid of patches and project each patch into an embedding space.\n",
    "\n",
    "For CIFAR-10 (32x32 images), we will use **4x4 patches**, giving us a grid of **8x8 = 64 patches**, where each patch is 4x4 pixels with 3 color channels.\n",
    "\n",
    "Think of it like cutting a photograph into 64 small squares. Each square gets converted into a single embedding vector."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Split image into patches and project to embedding dim.\"\"\"\n",
    "\n",
    "    def __init__(self, img_size=32, patch_size=4,\n",
    "                 in_channels=3, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2  # 64\n",
    "        self.grid_size = img_size // patch_size            # 8\n",
    "\n",
    "        # Conv2d with kernel=patch_size and stride=patch_size\n",
    "        # acts as a learnable linear projection of each patch\n",
    "        self.projection = nn.Conv2d(\n",
    "            in_channels, embed_dim,\n",
    "            kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        # Learnable position embeddings\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.randn(1, self.num_patches, embed_dim) * 0.02\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x: (B, C, H, W) -> (B, num_patches, embed_dim)\"\"\"\n",
    "        B = x.shape[0]\n",
    "        # Project patches: (B, embed_dim, grid, grid)\n",
    "        x = self.projection(x)\n",
    "        # Flatten spatial dims: (B, embed_dim, num_patches)\n",
    "        x = x.flatten(2)\n",
    "        # Transpose: (B, num_patches, embed_dim)\n",
    "        x = x.transpose(1, 2)\n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_embed\n",
    "        return x"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualize patch splitting\n",
    "sample_img, sample_label = trainset[0]\n",
    "img_display = (sample_img * std + mean).clamp(0, 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(img_display.permute(1, 2, 0).numpy())\n",
    "axes[0].set_title(f\"Original: {CLASSES[sample_label]}\", fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Image with patch grid overlay\n",
    "axes[1].imshow(img_display.permute(1, 2, 0).numpy())\n",
    "patch_size = 4\n",
    "for i in range(0, 33, patch_size):\n",
    "    axes[1].axhline(y=i-0.5, color='red', linewidth=0.8, alpha=0.7)\n",
    "    axes[1].axvline(x=i-0.5, color='red', linewidth=0.8, alpha=0.7)\n",
    "axes[1].set_title(f\"8x8 Grid = 64 Patches (each {patch_size}x{patch_size})\",\n",
    "                  fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.suptitle(\"Step 1: Split Image into Patches\", fontsize=14,\n",
    "             fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Each 32x32 image -> {(32//patch_size)**2} patches \"\n",
    "      f\"of size {patch_size}x{patch_size}\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Building the Transformer Encoder\n",
    "\n",
    "Now we need the encoder â€” a simplified **Vision Transformer (ViT)**. This is the backbone that transforms patch embeddings into rich semantic representations.\n",
    "\n",
    "Each Transformer block consists of:\n",
    "1. **Multi-Head Self-Attention** â€” patches attend to each other\n",
    "2. **Feed-Forward Network** â€” per-patch nonlinear transformation\n",
    "3. **Layer Normalization** and **residual connections** for stable training"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention for patch embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim=128, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x: (B, N, D) -> (B, N, D)\"\"\"\n",
    "        B, N, D = x.shape\n",
    "        # Compute Q, K, V in one projection\n",
    "        qkv = self.qkv(x).reshape(\n",
    "            B, N, 3, self.num_heads, self.head_dim\n",
    "        ).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        # Combine heads\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, D)\n",
    "        return self.proj(x)"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Single Transformer encoder block.\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim=128, num_heads=4,\n",
    "                 mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x: (B, N, D) -> (B, N, D)\"\"\"\n",
    "        x = x + self.attn(self.norm1(x))   # Residual + attention\n",
    "        x = x + self.mlp(self.norm2(x))    # Residual + FFN\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViTEncoder(nn.Module):\n",
    "    \"\"\"Simplified Vision Transformer encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim=128, depth=4,\n",
    "                 num_heads=4, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x: (B, N, D) -> (B, N, D)\"\"\"\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.norm(x)"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify that our encoder works correctly. We will pass a batch of patch embeddings through it and check the output shape."
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of patch embedding + encoder\n",
    "test_batch = torch.randn(4, 3, 32, 32)  # 4 images\n",
    "patch_embed = PatchEmbedding(embed_dim=128)\n",
    "encoder = ViTEncoder(embed_dim=128, depth=4)\n",
    "\n",
    "patches = patch_embed(test_batch)\n",
    "print(f\"Patch embeddings shape: {patches.shape}\")\n",
    "# Expected: (4, 64, 128) = (batch, num_patches, embed_dim)\n",
    "\n",
    "encoded = encoder(patches)\n",
    "print(f\"Encoder output shape: {encoded.shape}\")\n",
    "# Expected: (4, 64, 128)\n",
    "\n",
    "print(\"âœ… Patch embedding and encoder work correctly!\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The Target Encoder â€” Same Architecture, EMA Weights\n",
    "\n",
    "The target encoder has the **exact same architecture** as the context encoder, but its weights are not updated by gradients. Instead, they are updated using EMA (we will implement this in Section 4.7).\n",
    "\n",
    "At initialization, we simply copy the context encoder's weights."
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_encoder(context_encoder):\n",
    "    \"\"\"Create target encoder by copying context encoder weights.\n",
    "\n",
    "    The target encoder has identical architecture but its\n",
    "    parameters will be updated via EMA, not gradients.\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    target_encoder = copy.deepcopy(context_encoder)\n",
    "    # Freeze: target encoder does not receive gradients\n",
    "    for param in target_encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    return target_encoder\n",
    "\n",
    "# Test it\n",
    "target_enc = create_target_encoder(encoder)\n",
    "print(f\"Target encoder parameters frozen: \"\n",
    "      f\"{all(not p.requires_grad for p in target_enc.parameters())}\")\n",
    "print(\"âœ… Target encoder created successfully!\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 The Predictor â€” Predicting Target Embeddings\n",
    "\n",
    "Now let us build the predictor. This is a **smaller Transformer** that takes the context embeddings (visible patch embeddings) plus learnable **mask tokens** (placed at the masked positions), and predicts the target embeddings.\n",
    "\n",
    "The mask tokens act as \"queries\" â€” they say \"I am at position (3, 5) in the grid, and I need you to fill in my embedding based on what you can see from the visible patches.\""
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    \"\"\"Predict target embeddings from context embeddings.\n",
    "\n",
    "    Takes context (visible) embeddings and mask token positions,\n",
    "    outputs predictions for the masked positions only.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim=128, predictor_dim=64,\n",
    "                 depth=2, num_heads=4):\n",
    "        super().__init__()\n",
    "        # Project from encoder dim to predictor dim\n",
    "        self.input_proj = nn.Linear(embed_dim, predictor_dim)\n",
    "        # Learnable mask token\n",
    "        self.mask_token = nn.Parameter(\n",
    "            torch.randn(1, 1, predictor_dim) * 0.02\n",
    "        )\n",
    "        # Positional embedding for predictor\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.randn(1, 64, predictor_dim) * 0.02  # 64 patches\n",
    "        )\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(predictor_dim, num_heads, mlp_ratio=4.0)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(predictor_dim)\n",
    "        # Project back to encoder dim for loss computation\n",
    "        self.output_proj = nn.Linear(predictor_dim, embed_dim)\n",
    "\n",
    "    def forward(self, context_emb, context_idx, target_idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            context_emb: (B, N_ctx, embed_dim) visible embeddings\n",
    "            context_idx: (B, N_ctx) indices of visible patches\n",
    "            target_idx:  (B, N_tgt) indices of masked patches\n",
    "        Returns:\n",
    "            predicted: (B, N_tgt, embed_dim) predicted embeddings\n",
    "        \"\"\"\n",
    "        B = context_emb.shape[0]\n",
    "        N_tgt = target_idx.shape[1]\n",
    "\n",
    "        # Project context to predictor dimension\n",
    "        x_ctx = self.input_proj(context_emb)\n",
    "\n",
    "        # Create mask tokens for target positions\n",
    "        mask_tokens = self.mask_token.expand(B, N_tgt, -1)\n",
    "\n",
    "        # Add positional embeddings\n",
    "        ctx_pos = self._gather_pos(context_idx)\n",
    "        tgt_pos = self._gather_pos(target_idx)\n",
    "        x_ctx = x_ctx + ctx_pos\n",
    "        mask_tokens = mask_tokens + tgt_pos\n",
    "\n",
    "        # Concatenate context and mask tokens\n",
    "        x = torch.cat([x_ctx, mask_tokens], dim=1)\n",
    "\n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Extract only the mask token outputs\n",
    "        predicted = x[:, -N_tgt:]\n",
    "\n",
    "        # Project back to encoder embedding dim\n",
    "        return self.output_proj(predicted)\n",
    "\n",
    "    def _gather_pos(self, indices):\n",
    "        \"\"\"Gather positional embeddings for given indices.\"\"\"\n",
    "        B = indices.shape[0]\n",
    "        pos = self.pos_embed.expand(B, -1, -1)\n",
    "        indices_expanded = indices.unsqueeze(-1).expand(\n",
    "            -1, -1, pos.shape[-1]\n",
    "        )\n",
    "        return torch.gather(pos, 1, indices_expanded)"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the predictor\n",
    "predictor = Predictor(embed_dim=128, predictor_dim=64, depth=2)\n",
    "\n",
    "# Simulate: 40 visible patches, 24 masked patches\n",
    "B = 4\n",
    "context_emb = torch.randn(B, 40, 128)\n",
    "context_idx = torch.arange(40).unsqueeze(0).expand(B, -1)\n",
    "target_idx = torch.arange(40, 64).unsqueeze(0).expand(B, -1)\n",
    "\n",
    "predicted = predictor(context_emb, context_idx, target_idx)\n",
    "print(f\"Predicted embeddings shape: {predicted.shape}\")\n",
    "# Expected: (4, 24, 128)\n",
    "print(\"âœ… Predictor works correctly!\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Multi-Block Masking Strategy\n",
    "\n",
    "I-JEPA does not mask random individual patches. Instead, it masks **contiguous rectangular blocks** â€” this forces the model to predict larger semantic regions rather than trivially interpolating from neighbors.\n",
    "\n",
    "The masking strategy generates rectangular blocks covering roughly **40-60%** of the patches."
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_block_mask(grid_size=8, num_blocks=4,\n",
    "                            min_block_size=2, max_block_size=4):\n",
    "    \"\"\"Create a mask by selecting contiguous rectangular blocks.\n",
    "\n",
    "    Returns:\n",
    "        mask: (grid_size * grid_size,) boolean tensor\n",
    "              True = masked (target), False = visible (context)\n",
    "    \"\"\"\n",
    "    mask = torch.zeros(grid_size, grid_size, dtype=torch.bool)\n",
    "\n",
    "    for _ in range(num_blocks):\n",
    "        # Random block size\n",
    "        bh = torch.randint(min_block_size, max_block_size + 1, (1,)).item()\n",
    "        bw = torch.randint(min_block_size, max_block_size + 1, (1,)).item()\n",
    "        # Random top-left corner\n",
    "        top = torch.randint(0, grid_size - bh + 1, (1,)).item()\n",
    "        left = torch.randint(0, grid_size - bw + 1, (1,)).item()\n",
    "        # Apply block mask\n",
    "        mask[top:top+bh, left:left+bw] = True\n",
    "\n",
    "    return mask.flatten()  # (64,)\n",
    "\n",
    "\n",
    "def get_context_target_indices(mask):\n",
    "    \"\"\"Split patch indices into context (visible) and target (masked).\"\"\"\n",
    "    context_idx = (~mask).nonzero(as_tuple=True)[0]\n",
    "    target_idx = mask.nonzero(as_tuple=True)[0]\n",
    "    return context_idx, target_idx"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualize the masking strategy\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "\n",
    "for i in range(8):\n",
    "    row, col = i // 4, i % 4\n",
    "    mask = create_multi_block_mask(grid_size=8)\n",
    "    mask_2d = mask.reshape(8, 8).numpy()\n",
    "\n",
    "    ctx_idx, tgt_idx = get_context_target_indices(mask)\n",
    "    mask_ratio = len(tgt_idx) / 64 * 100\n",
    "\n",
    "    axes[row, col].imshow(mask_2d, cmap='RdYlGn_r',\n",
    "                          vmin=0, vmax=1)\n",
    "    axes[row, col].set_title(f\"Mask ratio: {mask_ratio:.0f}%\",\n",
    "                             fontsize=10)\n",
    "    # Add grid lines\n",
    "    for g in range(9):\n",
    "        axes[row, col].axhline(y=g-0.5, color='gray',\n",
    "                               linewidth=0.5, alpha=0.5)\n",
    "        axes[row, col].axvline(x=g-0.5, color='gray',\n",
    "                               linewidth=0.5, alpha=0.5)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle(\"Multi-Block Masking Examples\\n\"\n",
    "             \"(Red = Masked/Target, Green = Visible/Context)\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see what a masked image looks like visually. This helps us understand what the model \"sees\" versus what it needs to predict."
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Show masked vs visible patches on a real image\n",
    "sample_img, sample_label = trainset[7]\n",
    "img_display = (sample_img * std + mean).clamp(0, 1).permute(1, 2, 0).numpy()\n",
    "\n",
    "mask = create_multi_block_mask(grid_size=8)\n",
    "ctx_idx, tgt_idx = get_context_target_indices(mask)\n",
    "\n",
    "# Create masked version of the image\n",
    "masked_img = img_display.copy()\n",
    "mask_2d = mask.reshape(8, 8).numpy()\n",
    "for r in range(8):\n",
    "    for c in range(8):\n",
    "        if mask_2d[r, c]:\n",
    "            masked_img[r*4:(r+1)*4, c*4:(c+1)*4] = 0.5  # Gray out\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow(img_display)\n",
    "axes[0].set_title(f\"Original: {CLASSES[sample_label]}\", fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(masked_img)\n",
    "axes[1].set_title(f\"Visible to Context Encoder\\n\"\n",
    "                  f\"({len(ctx_idx)} patches)\", fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Show only masked region\n",
    "target_only = np.ones_like(img_display) * 0.8\n",
    "for r in range(8):\n",
    "    for c in range(8):\n",
    "        if mask_2d[r, c]:\n",
    "            target_only[r*4:(r+1)*4, c*4:(c+1)*4] = \\\n",
    "                img_display[r*4:(r+1)*4, c*4:(c+1)*4]\n",
    "axes[2].imshow(target_only)\n",
    "axes[2].set_title(f\"Target to Predict\\n\"\n",
    "                  f\"({len(tgt_idx)} patches)\", fontsize=12)\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle(\"JEPA Masking: Context vs Target\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 EMA Update for Target Encoder\n",
    "\n",
    "The final component is the EMA update rule. After every training step, we slowly update the target encoder's weights to track the context encoder."
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def ema_update(context_encoder, target_encoder, tau=0.996):\n",
    "    \"\"\"Update target encoder weights via exponential moving average.\n",
    "\n",
    "    target_params <- tau * target_params + (1 - tau) * context_params\n",
    "\n",
    "    Args:\n",
    "        context_encoder: encoder being trained with gradients\n",
    "        target_encoder: encoder updated via EMA (no gradients)\n",
    "        tau: decay rate (higher = slower update, more stable)\n",
    "    \"\"\"\n",
    "    for ctx_param, tgt_param in zip(\n",
    "        context_encoder.parameters(),\n",
    "        target_encoder.parameters()\n",
    "    ):\n",
    "        tgt_param.data.mul_(tau).add_(\n",
    "            ctx_param.data, alpha=1.0 - tau\n",
    "        )"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify EMA update works\n",
    "import copy\n",
    "enc_a = nn.Linear(10, 10)\n",
    "enc_b = copy.deepcopy(enc_a)\n",
    "\n",
    "# Modify enc_a (simulate a gradient step)\n",
    "with torch.no_grad():\n",
    "    enc_a.weight.add_(torch.randn_like(enc_a.weight) * 0.1)\n",
    "\n",
    "# Check they diverged\n",
    "diff_before = (enc_a.weight - enc_b.weight).abs().mean().item()\n",
    "\n",
    "# Apply EMA\n",
    "ema_update(enc_a, enc_b, tau=0.99)\n",
    "\n",
    "# Check they got closer\n",
    "diff_after = (enc_a.weight - enc_b.weight).abs().mean().item()\n",
    "print(f\"Weight difference before EMA: {diff_before:.4f}\")\n",
    "print(f\"Weight difference after EMA:  {diff_after:.4f}\")\n",
    "print(f\"Target moved {((diff_before - diff_after)/diff_before)*100:.1f}%\"\n",
    "      f\" closer to context\")\n",
    "print(\"âœ… EMA update works correctly!\")"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ðŸ”§ Your Turn â€” Implement Key Components\n",
    "\n",
    "Now it is your turn. We have built all the pieces; here are two critical functions for you to implement.\n",
    "\n",
    "### TODO 1: Implement Multi-Block Masking\n",
    "\n",
    "Complete the function below that creates a multi-block mask ensuring the mask ratio stays within 40-60% of total patches."
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_mask(grid_size=8, target_ratio=0.5,\n",
    "                         tolerance=0.1):\n",
    "    \"\"\"Create a multi-block mask with a controlled mask ratio.\n",
    "\n",
    "    Generate rectangular blocks until the mask ratio falls\n",
    "    within [target_ratio - tolerance, target_ratio + tolerance].\n",
    "\n",
    "    Args:\n",
    "        grid_size: number of patches per side (8 for our setup)\n",
    "        target_ratio: desired fraction of patches to mask\n",
    "        tolerance: acceptable deviation from target_ratio\n",
    "\n",
    "    Returns:\n",
    "        mask: (grid_size * grid_size,) boolean tensor\n",
    "\n",
    "    Hints:\n",
    "        - Start with an empty (all False) mask\n",
    "        - Repeatedly add random rectangular blocks\n",
    "        - Check the ratio after each block\n",
    "        - Stop when within the target range\n",
    "        - If you overshoot, start over\n",
    "    \"\"\"\n",
    "    total_patches = grid_size * grid_size\n",
    "    min_ratio = target_ratio - tolerance\n",
    "    max_ratio = target_ratio + tolerance\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Initialize an empty mask (grid_size x grid_size, all False)\n",
    "    # Step 2: Loop â€” add random rectangular blocks (size 2-4)\n",
    "    # Step 3: After each block, compute current mask ratio\n",
    "    # Step 4: If ratio is in [min_ratio, max_ratio], return mask\n",
    "    # Step 5: If ratio exceeds max_ratio, reset and try again\n",
    "    # ==============================\n",
    "\n",
    "    mask = None  # YOUR CODE HERE\n",
    "\n",
    "    return mask.flatten()"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Verification: Test your create_balanced_mask implementation\n",
    "print(\"Testing create_balanced_mask()...\")\n",
    "passed = True\n",
    "for trial in range(20):\n",
    "    mask = create_balanced_mask(grid_size=8)\n",
    "    ratio = mask.float().mean().item()\n",
    "    if not (0.4 <= ratio <= 0.6):\n",
    "        print(f\"âŒ Trial {trial}: mask ratio {ratio:.2f} \"\n",
    "              f\"is outside [0.4, 0.6]\")\n",
    "        passed = False\n",
    "\n",
    "    # Check that mask is a boolean tensor of correct size\n",
    "    if mask.shape != (64,):\n",
    "        print(f\"âŒ Trial {trial}: expected shape (64,), \"\n",
    "              f\"got {mask.shape}\")\n",
    "        passed = False\n",
    "\n",
    "if passed:\n",
    "    print(\"âœ… All 20 trials passed! Mask ratios are within \"\n",
    "          \"[40%, 60%] and shapes are correct.\")"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement the JEPA Loss Function\n",
    "\n",
    "Complete the loss function that computes MSE between predicted and target embeddings, with stop-gradient on the target."
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jepa_loss(predicted_emb, target_emb):\n",
    "    \"\"\"Compute JEPA loss: MSE with stop-gradient on target.\n",
    "\n",
    "    Loss = mean(|| predicted - sg(target) ||^2)\n",
    "\n",
    "    Args:\n",
    "        predicted_emb: (B, N_tgt, D) predicted target embeddings\n",
    "        target_emb: (B, N_tgt, D) actual target embeddings\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar tensor\n",
    "\n",
    "    Hints:\n",
    "        - Use target_emb.detach() to apply stop-gradient\n",
    "        - Compute squared L2 distance per embedding vector\n",
    "        - Average over all patches and batch elements\n",
    "        - F.mse_loss does exactly this\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Detach target embeddings (stop-gradient)\n",
    "    # Step 2: Compute MSE between predicted and detached target\n",
    "    # ==============================\n",
    "\n",
    "    loss = None  # YOUR CODE HERE\n",
    "\n",
    "    return loss"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Verification: Test your jepa_loss implementation\n",
    "pred = torch.tensor([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]])\n",
    "targ = torch.tensor([[[1.1, 1.9, 3.2], [3.8, 5.1, 5.9]]])\n",
    "\n",
    "loss = jepa_loss(pred, targ)\n",
    "expected = 0.06  # We computed this by hand in Section 3.4!\n",
    "\n",
    "print(f\"Your loss: {loss.item():.4f}\")\n",
    "print(f\"Expected:  {expected:.4f}\")\n",
    "\n",
    "if abs(loss.item() - expected) < 1e-3:\n",
    "    print(\"âœ… Correct! Your loss matches our hand calculation.\")\n",
    "else:\n",
    "    print(\"âŒ Loss does not match. Check your implementation.\")\n",
    "\n",
    "# Verify stop-gradient: target should not require grad\n",
    "targ_grad = torch.randn(2, 3, 4, requires_grad=True)\n",
    "pred_grad = torch.randn(2, 3, 4, requires_grad=True)\n",
    "l = jepa_loss(pred_grad, targ_grad)\n",
    "l.backward()\n",
    "assert pred_grad.grad is not None, \"âŒ Predicted should have gradient\"\n",
    "assert targ_grad.grad is None, \"âŒ Target should NOT have gradient\"\n",
    "print(\"âœ… Stop-gradient verified: gradients flow to predictor \"\n",
    "      \"but not to target encoder.\")"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together â€” The Full I-JEPA Model\n",
    "\n",
    "Now let us assemble all our components into a single, clean I-JEPA model."
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IJEPA(nn.Module):\n",
    "    \"\"\"Image-based Joint Embedding Predictive Architecture.\n",
    "\n",
    "    Components:\n",
    "        - Patch embedding: splits image into patches\n",
    "        - Context encoder: ViT that encodes visible patches\n",
    "        - Target encoder: EMA copy that encodes masked patches\n",
    "        - Predictor: small transformer that predicts target embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=32, patch_size=4,\n",
    "                 embed_dim=128, encoder_depth=4,\n",
    "                 predictor_dim=64, predictor_depth=2,\n",
    "                 num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            img_size, patch_size, 3, embed_dim\n",
    "        )\n",
    "        self.context_encoder = ViTEncoder(\n",
    "            embed_dim, encoder_depth, num_heads\n",
    "        )\n",
    "        self.target_encoder = create_target_encoder(\n",
    "            self.context_encoder\n",
    "        )\n",
    "        self.predictor = Predictor(\n",
    "            embed_dim, predictor_dim,\n",
    "            predictor_depth, num_heads\n",
    "        )\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_patches = self.patch_embed.num_patches\n",
    "\n",
    "    def forward(self, images, context_idx, target_idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: (B, 3, H, W) input images\n",
    "            context_idx: (B, N_ctx) visible patch indices\n",
    "            target_idx: (B, N_tgt) masked patch indices\n",
    "        Returns:\n",
    "            predicted: (B, N_tgt, embed_dim)\n",
    "            target: (B, N_tgt, embed_dim)\n",
    "        \"\"\"\n",
    "        # Get all patch embeddings\n",
    "        all_patches = self.patch_embed(images)\n",
    "\n",
    "        # Gather context patches and encode\n",
    "        ctx_patches = self._gather(all_patches, context_idx)\n",
    "        ctx_encoded = self.context_encoder(ctx_patches)\n",
    "\n",
    "        # Gather target patches and encode (no gradient!)\n",
    "        with torch.no_grad():\n",
    "            tgt_patches = self._gather(all_patches, target_idx)\n",
    "            tgt_encoded = self.target_encoder(tgt_patches)\n",
    "\n",
    "        # Predict target embeddings from context\n",
    "        predicted = self.predictor(\n",
    "            ctx_encoded, context_idx, target_idx\n",
    "        )\n",
    "\n",
    "        return predicted, tgt_encoded\n",
    "\n",
    "    def _gather(self, patches, indices):\n",
    "        \"\"\"Gather specific patches by index.\"\"\"\n",
    "        B, N, D = patches.shape\n",
    "        indices_expanded = indices.unsqueeze(-1).expand(-1, -1, D)\n",
    "        return torch.gather(patches, 1, indices_expanded)\n",
    "\n",
    "    def update_target_encoder(self, tau=0.996):\n",
    "        \"\"\"Update target encoder via EMA.\"\"\"\n",
    "        ema_update(\n",
    "            self.context_encoder,\n",
    "            self.target_encoder, tau\n",
    "        )"
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the full model\n",
    "model = IJEPA(\n",
    "    img_size=32, patch_size=4, embed_dim=128,\n",
    "    encoder_depth=4, predictor_dim=64,\n",
    "    predictor_depth=2, num_heads=4\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable = sum(p.numel() for p in model.parameters()\n",
    "                if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable:,}\")\n",
    "print(f\"Target encoder (frozen): {total_params - trainable:,}\")"
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "### 7.1 Training Loop\n",
    "\n",
    "Now let us train our I-JEPA on CIFAR-10. We will train for 15 epochs â€” this should complete in about 3-4 minutes on a T4 GPU."
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Configuration ---\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 1.5e-3\n",
    "NUM_EPOCHS = 15\n",
    "EMA_TAU = 0.996\n",
    "GRID_SIZE = 8\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=BATCH_SIZE,\n",
    "    shuffle=True, num_workers=2,\n",
    "    pin_memory=True, drop_last=True\n",
    ")\n",
    "\n",
    "model = IJEPA(\n",
    "    img_size=32, patch_size=4, embed_dim=128,\n",
    "    encoder_depth=4, predictor_dim=64,\n",
    "    predictor_depth=2, num_heads=4\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr=LEARNING_RATE, weight_decay=0.05\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=NUM_EPOCHS\n",
    ")"
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop ---\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "\n",
    "    for batch_idx, (images, _) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        B = images.shape[0]\n",
    "\n",
    "        # Generate masks for this batch\n",
    "        masks = torch.stack(\n",
    "            [create_multi_block_mask(GRID_SIZE) for _ in range(B)]\n",
    "        )\n",
    "        ctx_indices = []\n",
    "        tgt_indices = []\n",
    "        for m in masks:\n",
    "            ci, ti = get_context_target_indices(m)\n",
    "            ctx_indices.append(ci)\n",
    "            tgt_indices.append(ti)\n",
    "\n",
    "        # Pad to same length within batch\n",
    "        max_ctx = max(c.shape[0] for c in ctx_indices)\n",
    "        max_tgt = max(t.shape[0] for t in tgt_indices)\n",
    "\n",
    "        ctx_batch = torch.zeros(B, max_ctx, dtype=torch.long)\n",
    "        tgt_batch = torch.zeros(B, max_tgt, dtype=torch.long)\n",
    "\n",
    "        for i in range(B):\n",
    "            ctx_batch[i, :ctx_indices[i].shape[0]] = ctx_indices[i]\n",
    "            tgt_batch[i, :tgt_indices[i].shape[0]] = tgt_indices[i]\n",
    "\n",
    "        ctx_batch = ctx_batch.to(device)\n",
    "        tgt_batch = tgt_batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predicted, target = model(images, ctx_batch, tgt_batch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(predicted, target.detach())\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update target encoder\n",
    "        model.update_target_encoder(tau=EMA_TAU)\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    loss_history.append(avg_loss)\n",
    "\n",
    "    if (epoch + 1) % 3 == 0 or epoch == 0:\n",
    "        print(f\"Epoch [{epoch+1:2d}/{NUM_EPOCHS}] | \"\n",
    "              f\"Loss: {avg_loss:.4f} | \"\n",
    "              f\"LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "print(f\"\\nTraining complete! Final loss: {loss_history[-1]:.4f}\")"
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Training Loss Curve\n",
    "\n",
    "Let us visualize how the training loss evolved. A steadily decreasing loss means our predictor is learning to predict target embeddings more and more accurately."
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), loss_history,\n",
    "         'b-o', linewidth=2, markersize=6)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('MSE Loss', fontsize=12)\n",
    "plt.title('I-JEPA Training Loss on CIFAR-10', fontsize=14,\n",
    "          fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(range(1, NUM_EPOCHS + 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "reduction = (loss_history[0] - loss_history[-1]) / loss_history[0]\n",
    "print(f\"Loss reduced by {reduction*100:.1f}% over {NUM_EPOCHS} \"\n",
    "      f\"epochs â€” the model is learning to predict in abstract \"\n",
    "      f\"space!\")"
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 ðŸ“Š How Predictions Improve Over Training\n",
    "\n",
    "Let us visualize how the predicted embeddings get closer to the target embeddings as training progresses. We will use cosine similarity as our metric."
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Measure prediction quality\n",
    "model.eval()\n",
    "sample_images, _ = next(iter(train_loader))\n",
    "sample_images = sample_images[:16].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    B = sample_images.shape[0]\n",
    "    masks = torch.stack(\n",
    "        [create_multi_block_mask(GRID_SIZE) for _ in range(B)]\n",
    "    )\n",
    "    ctx_list, tgt_list = [], []\n",
    "    for m in masks:\n",
    "        ci, ti = get_context_target_indices(m)\n",
    "        ctx_list.append(ci)\n",
    "        tgt_list.append(ti)\n",
    "\n",
    "    max_ctx = max(c.shape[0] for c in ctx_list)\n",
    "    max_tgt = max(t.shape[0] for t in tgt_list)\n",
    "\n",
    "    ctx_b = torch.zeros(B, max_ctx, dtype=torch.long)\n",
    "    tgt_b = torch.zeros(B, max_tgt, dtype=torch.long)\n",
    "    for i in range(B):\n",
    "        ctx_b[i, :ctx_list[i].shape[0]] = ctx_list[i]\n",
    "        tgt_b[i, :tgt_list[i].shape[0]] = tgt_list[i]\n",
    "\n",
    "    ctx_b, tgt_b = ctx_b.to(device), tgt_b.to(device)\n",
    "    pred, targ = model(sample_images, ctx_b, tgt_b)\n",
    "\n",
    "    # Cosine similarity per patch\n",
    "    cos_sim = F.cosine_similarity(pred, targ, dim=-1)\n",
    "    avg_sim = cos_sim.mean().item()\n",
    "\n",
    "print(f\"Average cosine similarity between predicted \"\n",
    "      f\"and target embeddings: {avg_sim:.4f}\")\n",
    "print(f\"(1.0 = perfect prediction, 0.0 = orthogonal)\")"
   ],
   "id": "cell_51"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Linear Probing â€” Evaluating Representation Quality\n",
    "\n",
    "The real test of self-supervised learning: are the learned representations **useful**? We freeze the encoder and train a simple linear classifier on top. If the representations are good, even a linear classifier should achieve decent accuracy.\n",
    "\n",
    "This is called **linear probing** â€” it measures how much useful information the encoder has captured about the data."
   ],
   "id": "cell_52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(model, dataloader, device, max_samples=10000):\n",
    "    \"\"\"Extract features from the context encoder.\"\"\"\n",
    "    model.eval()\n",
    "    features, labels = [], []\n",
    "    count = 0\n",
    "\n",
    "    for images, targets in dataloader:\n",
    "        if count >= max_samples:\n",
    "            break\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Encode all patches (no masking for evaluation)\n",
    "        patches = model.patch_embed(images)\n",
    "        encoded = model.context_encoder(patches)\n",
    "\n",
    "        # Global average pooling over patches\n",
    "        pooled = encoded.mean(dim=1)  # (B, embed_dim)\n",
    "\n",
    "        features.append(pooled.cpu().numpy())\n",
    "        labels.append(targets.numpy())\n",
    "        count += images.shape[0]\n",
    "\n",
    "    features = np.concatenate(features)[:max_samples]\n",
    "    labels = np.concatenate(labels)[:max_samples]\n",
    "    return features, labels"
   ],
   "id": "cell_53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from trained I-JEPA encoder\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=256, shuffle=False, num_workers=2\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=256, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "print(\"Extracting JEPA features from training set...\")\n",
    "train_feats, train_labels = extract_features(\n",
    "    model, eval_loader, device, max_samples=10000\n",
    ")\n",
    "print(\"Extracting JEPA features from test set...\")\n",
    "test_feats, test_labels = extract_features(\n",
    "    model, test_loader, device, max_samples=5000\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "train_feats_scaled = scaler.fit_transform(train_feats)\n",
    "test_feats_scaled = scaler.transform(test_feats)\n",
    "\n",
    "print(f\"Train features shape: {train_feats.shape}\")\n",
    "print(f\"Test features shape: {test_feats.shape}\")"
   ],
   "id": "cell_54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear classifier on JEPA features\n",
    "print(\"Training linear classifier on JEPA features...\")\n",
    "clf_jepa = SGDClassifier(\n",
    "    loss='hinge', max_iter=1000,\n",
    "    random_state=42, tol=1e-3\n",
    ")\n",
    "clf_jepa.fit(train_feats_scaled, train_labels)\n",
    "jepa_acc = clf_jepa.score(test_feats_scaled, test_labels)\n",
    "print(f\"JEPA Linear Probe Accuracy: {jepa_acc*100:.1f}%\")\n",
    "\n",
    "# Compare with random encoder (untrained)\n",
    "import copy\n",
    "random_model = IJEPA(\n",
    "    img_size=32, patch_size=4, embed_dim=128,\n",
    "    encoder_depth=4, predictor_dim=64,\n",
    "    predictor_depth=2, num_heads=4\n",
    ").to(device)\n",
    "\n",
    "print(\"\\nExtracting random encoder features...\")\n",
    "rand_train, _ = extract_features(\n",
    "    random_model, eval_loader, device, max_samples=10000\n",
    ")\n",
    "rand_test, _ = extract_features(\n",
    "    random_model, test_loader, device, max_samples=5000\n",
    ")\n",
    "\n",
    "scaler_rand = StandardScaler()\n",
    "rand_train_scaled = scaler_rand.fit_transform(rand_train)\n",
    "rand_test_scaled = scaler_rand.transform(rand_test)\n",
    "\n",
    "clf_rand = SGDClassifier(\n",
    "    loss='hinge', max_iter=1000,\n",
    "    random_state=42, tol=1e-3\n",
    ")\n",
    "clf_rand.fit(rand_train_scaled, train_labels)\n",
    "rand_acc = clf_rand.score(rand_test_scaled, test_labels)\n",
    "print(f\"Random Encoder Linear Probe Accuracy: {rand_acc*100:.1f}%\")"
   ],
   "id": "cell_55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Compare accuracies\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "methods = ['Random Encoder', 'I-JEPA (Ours)']\n",
    "accs = [rand_acc * 100, jepa_acc * 100]\n",
    "colors = ['#d9534f', '#5cb85c']\n",
    "bars = ax.bar(methods, accs, color=colors, width=0.5,\n",
    "              edgecolor='black', linewidth=1.2)\n",
    "\n",
    "for bar, acc in zip(bars, accs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "            f'{acc:.1f}%', ha='center', fontsize=14,\n",
    "            fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Linear Probe Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Representation Quality: Random vs I-JEPA',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "improvement = jepa_acc - rand_acc\n",
    "print(f\"\\nI-JEPA representations are {improvement*100:.1f} \"\n",
    "      f\"percentage points better than random!\")\n",
    "print(\"This confirms that JEPA learns meaningful, \"\n",
    "      \"structured representations.\")"
   ],
   "id": "cell_56"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ðŸŽ¯ Final Output â€” Visualizing What JEPA Learned\n",
    "\n",
    "This is the most satisfying part. Let us see **what** our model learned by visualizing the embedding space.\n",
    "\n",
    "### 8.1 t-SNE: JEPA vs Random\n",
    "\n",
    "We will project the 128-dimensional embeddings down to 2D using t-SNE and color them by class. If JEPA learned good representations, images of the same class should cluster together."
   ],
   "id": "cell_57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# t-SNE on JEPA features\n",
    "print(\"Computing t-SNE for JEPA features (this takes ~30 sec)...\")\n",
    "tsne = TSNE(n_components=2, random_state=42,\n",
    "            perplexity=30, n_iter=1000)\n",
    "jepa_2d = tsne.fit_transform(test_feats_scaled[:2000])\n",
    "\n",
    "# t-SNE on random features\n",
    "print(\"Computing t-SNE for random features...\")\n",
    "tsne_rand = TSNE(n_components=2, random_state=42,\n",
    "                 perplexity=30, n_iter=1000)\n",
    "rand_2d = tsne_rand.fit_transform(rand_test_scaled[:2000])\n",
    "labels_2d = test_labels[:2000]\n",
    "\n",
    "print(\"Done!\")"
   ],
   "id": "cell_58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š t-SNE visualization â€” the big reveal!\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "cmap = plt.cm.get_cmap('tab10')\n",
    "\n",
    "for i, cls_name in enumerate(CLASSES):\n",
    "    mask_cls = labels_2d == i\n",
    "\n",
    "    axes[0].scatter(\n",
    "        rand_2d[mask_cls, 0], rand_2d[mask_cls, 1],\n",
    "        c=[cmap(i)], label=cls_name, alpha=0.5, s=8\n",
    "    )\n",
    "    axes[1].scatter(\n",
    "        jepa_2d[mask_cls, 0], jepa_2d[mask_cls, 1],\n",
    "        c=[cmap(i)], label=cls_name, alpha=0.5, s=8\n",
    "    )\n",
    "\n",
    "axes[0].set_title('Random Encoder\\n(No Structure)',\n",
    "                   fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=8, markerscale=3, loc='upper right')\n",
    "axes[0].set_xticks([])\n",
    "axes[0].set_yticks([])\n",
    "\n",
    "axes[1].set_title('I-JEPA Encoder\\n(Clear Clustering!)',\n",
    "                   fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=8, markerscale=3, loc='upper right')\n",
    "axes[1].set_xticks([])\n",
    "axes[1].set_yticks([])\n",
    "\n",
    "plt.suptitle(\n",
    "    't-SNE of CIFAR-10 Representations: '\n",
    "    'Random vs I-JEPA',\n",
    "    fontsize=16, fontweight='bold'\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The I-JEPA plot should show clear clusters â€” images of \"\n",
    "      \"the same class have similar embeddings.\")\n",
    "print(\"The random encoder plot should look like a scattered \"\n",
    "      \"mess with no meaningful structure.\")\n",
    "print(\"\\nThis is exactly what we want. JEPA learned \"\n",
    "      \"powerful representations without generating \"\n",
    "      \"a single pixel!\")"
   ],
   "id": "cell_59"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Nearest-Neighbor Retrieval\n",
    "\n",
    "Another way to verify the quality of learned representations: given a query image, find the images with the **most similar embeddings**. If the representations are good, the nearest neighbors should be visually and semantically similar."
   ],
   "id": "cell_60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest-neighbor retrieval using JEPA embeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Use test features\n",
    "sim_matrix = cosine_similarity(test_feats_scaled[:1000])\n",
    "\n",
    "# Display query images and their nearest neighbors\n",
    "fig, axes = plt.subplots(4, 6, figsize=(14, 10))\n",
    "query_indices = [0, 42, 100, 250]\n",
    "\n",
    "# Unnormalize helper\n",
    "raw_transform = transforms.ToTensor()\n",
    "raw_testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=False,\n",
    "    transform=raw_transform\n",
    ")\n",
    "\n",
    "for row, qi in enumerate(query_indices):\n",
    "    # Query image\n",
    "    img_q, lbl_q = raw_testset[qi]\n",
    "    axes[row, 0].imshow(img_q.permute(1, 2, 0).numpy())\n",
    "    axes[row, 0].set_title(f\"Query:\\n{CLASSES[lbl_q]}\",\n",
    "                           fontsize=9, fontweight='bold',\n",
    "                           color='blue')\n",
    "    axes[row, 0].axis('off')\n",
    "    # Add blue border\n",
    "    for spine in axes[row, 0].spines.values():\n",
    "        spine.set_edgecolor('blue')\n",
    "        spine.set_linewidth(3)\n",
    "        spine.set_visible(True)\n",
    "\n",
    "    # Top-5 nearest neighbors (skip self)\n",
    "    sims = sim_matrix[qi]\n",
    "    nn_indices = np.argsort(sims)[::-1][1:6]\n",
    "\n",
    "    for col, ni in enumerate(nn_indices):\n",
    "        img_n, lbl_n = raw_testset[ni]\n",
    "        axes[row, col+1].imshow(img_n.permute(1, 2, 0).numpy())\n",
    "        match = \"âœ“\" if lbl_n == lbl_q else \"âœ—\"\n",
    "        color = 'green' if lbl_n == lbl_q else 'red'\n",
    "        axes[row, col+1].set_title(\n",
    "            f\"{match} {CLASSES[lbl_n]}\\n\"\n",
    "            f\"sim={sims[ni]:.2f}\",\n",
    "            fontsize=8, color=color\n",
    "        )\n",
    "        axes[row, col+1].axis('off')\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Nearest-Neighbor Retrieval Using JEPA Embeddings\\n\"\n",
    "    \"(Blue = Query, Green âœ“ = Same Class, Red âœ— = Different)\",\n",
    "    fontsize=13, fontweight='bold'\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_61"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Congratulations!\n",
    "\n",
    "You have successfully built **I-JEPA from scratch** and demonstrated that it learns meaningful representations of images **without ever generating a single pixel**.\n",
    "\n",
    "Let us recap what we accomplished:\n",
    "\n",
    "| Component | What It Does |\n",
    "|---|---|\n",
    "| **Patch Embedding** | Splits 32x32 images into 64 patches, projects each to 128-d |\n",
    "| **Context Encoder (ViT)** | Encodes visible patches into rich embeddings |\n",
    "| **Target Encoder (EMA)** | Provides stable target embeddings via exponential moving average |\n",
    "| **Predictor** | Predicts target embeddings from context embeddings |\n",
    "| **Multi-Block Masking** | Masks contiguous regions to force semantic prediction |\n",
    "\n",
    "And the results speak for themselves:\n",
    "- The training loss decreased steadily, confirming the model learned to predict in abstract space\n",
    "- Linear probing showed a large accuracy gap between JEPA and random features\n",
    "- t-SNE revealed clear class-based clustering in the JEPA embedding space\n",
    "- Nearest-neighbor retrieval returned semantically similar images\n",
    "\n",
    "**This is exactly what we want.** JEPA focuses on the meaning, not the surface."
   ],
   "id": "cell_62"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### ðŸ¤” Reflection Questions\n",
    "\n",
    "Take a moment to think about these questions. They will deepen your understanding of JEPA and self-supervised learning.\n",
    "\n",
    "1. **Why contiguous block masking?** We mask contiguous rectangular regions instead of random patches. What would happen if we masked random individual patches instead? Why might that be \"too easy\" for the predictor? *(Hint: think about what information neighboring patches provide.)*\n",
    "\n",
    "2. **The role of the EMA target.** We said the EMA prevents representation collapse. But what would happen if we set $\\tau = 0$ (target encoder instantly copies context encoder)? What about $\\tau = 1$ (target encoder never updates)? Why is a value like 0.996 the sweet spot?\n",
    "\n",
    "3. **JEPA vs Contrastive Learning.** Methods like SimCLR and DINO also learn representations without labels, but they use contrastive objectives (push different images apart, pull augmentations together). How is JEPA's predictive objective fundamentally different? What are the advantages?"
   ],
   "id": "cell_63"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Challenges\n",
    "\n",
    "If you want to go further, try these:\n",
    "\n",
    "**Challenge 1: V-JEPA Extension**\n",
    "The temporal extension of JEPA (V-JEPA) predicts video frame embeddings from past frame embeddings. How would you modify our architecture to handle video? Sketch out the changes needed:\n",
    "- How would patch embedding change for 3D data (spatial + temporal)?\n",
    "- What would the masking strategy look like across frames?\n",
    "- How would the predictor condition on both spatial and temporal context?\n",
    "\n",
    "**Challenge 2: Improve Linear Probe Accuracy**\n",
    "Try to push the linear probe accuracy higher by experimenting with:\n",
    "- Increasing `embed_dim` from 128 to 256\n",
    "- Training for more epochs (30-50)\n",
    "- Using a different masking ratio (try 0.6 or 0.75)\n",
    "- Adding data augmentation (random crops, color jitter)\n",
    "- Using a deeper encoder (6-8 layers instead of 4)\n",
    "\n",
    "Track how each change affects the final accuracy and training time."
   ],
   "id": "cell_64"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Comes Next\n",
    "\n",
    "In the next notebook, we will explore how these learned representations can be connected to **actions** â€” bridging the gap from perception to decision-making. We will see how models like **VLAs (Vision-Language-Action models)** combine visual understanding with language instructions to produce robot actions.\n",
    "\n",
    "The key insight: JEPA gives us a powerful way to **understand** the world. But understanding is only half the story â€” we also need to **act** in the world. This brings us to the exciting intersection of representation learning and embodied AI.\n",
    "\n",
    "See you in Notebook 5!"
   ],
   "id": "cell_65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "chatbot"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸ’¬ AI Teaching Assistant â€” Click â–¶ to start\n",
    "#@markdown This AI chatbot reads your notebook and can answer questions about any concept, code, or exercise.\n",
    "\n",
    "import json as _json\n",
    "import requests as _requests\n",
    "from google.colab import output as _output\n",
    "from IPython.display import display, HTML as _HTML, Markdown as _Markdown\n",
    "\n",
    "# --- Read notebook content for context ---\n",
    "def _get_notebook_context():\n",
    "    try:\n",
    "        from google.colab import _message\n",
    "        nb = _message.blocking_request(\"get_ipynb\", request=\"\", timeout_sec=10)\n",
    "        cells = nb.get(\"ipynb\", {}).get(\"cells\", [])\n",
    "        parts = []\n",
    "        for cell in cells:\n",
    "            src = \"\".join(cell.get(\"source\", []))\n",
    "            tags = cell.get(\"metadata\", {}).get(\"tags\", [])\n",
    "            if \"chatbot\" in tags:\n",
    "                continue\n",
    "            if src.strip():\n",
    "                ct = cell.get(\"cell_type\", \"unknown\")\n",
    "                parts.append(f\"[{ct.upper()}]\\n{src}\")\n",
    "        return \"\\n\\n---\\n\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"Notebook content unavailable.\"\n",
    "\n",
    "_NOTEBOOK_CONTEXT = _get_notebook_context()\n",
    "_CHAT_HISTORY = []\n",
    "_API_URL = \"https://course-creator-brown.vercel.app/api/chat\"\n",
    "\n",
    "def _notebook_chat(question):\n",
    "    global _CHAT_HISTORY\n",
    "    try:\n",
    "        resp = _requests.post(_API_URL, json={\n",
    "            'question': question,\n",
    "            'context': _NOTEBOOK_CONTEXT[:100000],\n",
    "            'history': _CHAT_HISTORY[-10:],\n",
    "        }, timeout=60)\n",
    "        data = resp.json()\n",
    "        answer = data.get('answer', 'Sorry, I could not generate a response.')\n",
    "        _CHAT_HISTORY.append({'role': 'user', 'content': question})\n",
    "        _CHAT_HISTORY.append({'role': 'assistant', 'content': answer})\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f'Error connecting to teaching assistant: {str(e)}'\n",
    "\n",
    "_output.register_callback('notebook_chat', _notebook_chat)\n",
    "\n",
    "def ask(question):\n",
    "    \"\"\"Ask the AI teaching assistant a question about this notebook.\"\"\"\n",
    "    answer = _notebook_chat(question)\n",
    "    display(_Markdown(answer))\n",
    "\n",
    "print(\"\\u2705 AI Teaching Assistant is ready!\")\n",
    "print(\"\\U0001f4a1 Use the chat below, or call ask(\\'your question\\') in any cell.\")\n",
    "\n",
    "# --- Display chat widget ---\n",
    "display(_HTML('''<style>\n  .vc-wrap{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;max-width:100%;border-radius:16px;overflow:hidden;box-shadow:0 4px 24px rgba(0,0,0,.12);background:#fff;border:1px solid #e5e7eb}\n  .vc-hdr{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;padding:16px 20px;display:flex;align-items:center;gap:12px}\n  .vc-avatar{width:42px;height:42px;background:rgba(255,255,255,.2);border-radius:50%;display:flex;align-items:center;justify-content:center;font-size:22px}\n  .vc-hdr h3{font-size:16px;font-weight:600;margin:0}\n  .vc-hdr p{font-size:12px;opacity:.85;margin:2px 0 0}\n  .vc-msgs{height:420px;overflow-y:auto;padding:16px;background:#f8f9fb;display:flex;flex-direction:column;gap:10px}\n  .vc-msg{display:flex;flex-direction:column;animation:vc-fade .25s ease}\n  .vc-msg.user{align-items:flex-end}\n  .vc-msg.bot{align-items:flex-start}\n  .vc-bbl{max-width:85%;padding:10px 14px;border-radius:16px;font-size:14px;line-height:1.55;word-wrap:break-word}\n  .vc-msg.user .vc-bbl{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border-bottom-right-radius:4px}\n  .vc-msg.bot .vc-bbl{background:#fff;color:#1a1a2e;border:1px solid #e8e8e8;border-bottom-left-radius:4px}\n  .vc-bbl code{background:rgba(0,0,0,.07);padding:2px 6px;border-radius:4px;font-size:13px;font-family:'Fira Code',monospace}\n  .vc-bbl pre{background:#1e1e2e;color:#cdd6f4;padding:12px;border-radius:8px;overflow-x:auto;margin:8px 0;font-size:13px}\n  .vc-bbl pre code{background:none;padding:0;color:inherit}\n  .vc-bbl h3,.vc-bbl h4{margin:10px 0 4px;font-size:15px}\n  .vc-bbl ul,.vc-bbl ol{margin:4px 0;padding-left:20px}\n  .vc-bbl li{margin:2px 0}\n  .vc-chips{display:flex;flex-wrap:wrap;gap:8px;padding:0 16px 12px;background:#f8f9fb}\n  .vc-chip{background:#fff;border:1px solid #d1d5db;border-radius:20px;padding:6px 14px;font-size:12px;cursor:pointer;transition:all .15s;color:#4b5563}\n  .vc-chip:hover{border-color:#667eea;color:#667eea;background:#f0f0ff}\n  .vc-input{display:flex;padding:12px 16px;background:#fff;border-top:1px solid #eee;gap:8px}\n  .vc-input input{flex:1;padding:10px 16px;border:2px solid #e8e8e8;border-radius:24px;font-size:14px;outline:none;transition:border-color .2s}\n  .vc-input input:focus{border-color:#667eea}\n  .vc-input button{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border:none;border-radius:50%;width:42px;height:42px;cursor:pointer;display:flex;align-items:center;justify-content:center;font-size:18px;transition:transform .1s}\n  .vc-input button:hover{transform:scale(1.05)}\n  .vc-input button:disabled{opacity:.5;cursor:not-allowed;transform:none}\n  .vc-typing{display:flex;gap:5px;padding:4px 0}\n  .vc-typing span{width:8px;height:8px;background:#667eea;border-radius:50%;animation:vc-bounce 1.4s infinite ease-in-out}\n  .vc-typing span:nth-child(2){animation-delay:.2s}\n  .vc-typing span:nth-child(3){animation-delay:.4s}\n  @keyframes vc-bounce{0%,80%,100%{transform:scale(0)}40%{transform:scale(1)}}\n  @keyframes vc-fade{from{opacity:0;transform:translateY(8px)}to{opacity:1;transform:translateY(0)}}\n  .vc-note{text-align:center;font-size:11px;color:#9ca3af;padding:8px 16px 12px;background:#fff}\n</style>\n<div class=\"vc-wrap\">\n  <div class=\"vc-hdr\">\n    <div class=\"vc-avatar\">&#129302;</div>\n    <div>\n      <h3>Vizuara Teaching Assistant</h3>\n      <p>Ask me anything about this notebook</p>\n    </div>\n  </div>\n  <div class=\"vc-msgs\" id=\"vcMsgs\">\n    <div class=\"vc-msg bot\">\n      <div class=\"vc-bbl\">&#128075; Hi! I've read through this entire notebook. Ask me about any concept, code block, or exercise &mdash; I'm here to help you learn!</div>\n    </div>\n  </div>\n  <div class=\"vc-chips\" id=\"vcChips\">\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Explain the main concept</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Help with the TODO exercise</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Summarize what I learned</span>\n  </div>\n  <div class=\"vc-input\">\n    <input type=\"text\" id=\"vcIn\" placeholder=\"Ask about concepts, code, exercises...\" />\n    <button id=\"vcSend\" onclick=\"vcSendMsg()\">&#10148;</button>\n  </div>\n  <div class=\"vc-note\">AI-generated &middot; Verify important information &middot; <a href=\"#\" onclick=\"vcClear();return false\" style=\"color:#667eea\">Clear chat</a></div>\n</div>\n<script>\n(function(){\n  var msgs=document.getElementById('vcMsgs'),inp=document.getElementById('vcIn'),\n      btn=document.getElementById('vcSend'),chips=document.getElementById('vcChips');\n\n  function esc(s){var d=document.createElement('div');d.textContent=s;return d.innerHTML}\n\n  function md(t){\n    return t\n      .replace(/```(\\w*)\\n([\\s\\S]*?)```/g,function(_,l,c){return '<pre><code>'+esc(c)+'</code></pre>'})\n      .replace(/`([^`]+)`/g,'<code>$1</code>')\n      .replace(/\\*\\*([^*]+)\\*\\*/g,'<strong>$1</strong>')\n      .replace(/\\*([^*]+)\\*/g,'<em>$1</em>')\n      .replace(/^#### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^## (.+)$/gm,'<h3>$1</h3>')\n      .replace(/^\\d+\\. (.+)$/gm,'<li>$1</li>')\n      .replace(/^- (.+)$/gm,'<li>$1</li>')\n      .replace(/\\n\\n/g,'<br><br>')\n      .replace(/\\n/g,'<br>');\n  }\n\n  function addMsg(text,isUser){\n    var m=document.createElement('div');m.className='vc-msg '+(isUser?'user':'bot');\n    var b=document.createElement('div');b.className='vc-bbl';\n    b.innerHTML=isUser?esc(text):md(text);\n    m.appendChild(b);msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function showTyping(){\n    var m=document.createElement('div');m.className='vc-msg bot';m.id='vcTyping';\n    m.innerHTML='<div class=\"vc-bbl\"><div class=\"vc-typing\"><span></span><span></span><span></span></div></div>';\n    msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function hideTyping(){var e=document.getElementById('vcTyping');if(e)e.remove()}\n\n  window.vcSendMsg=function(){\n    var q=inp.value.trim();if(!q)return;\n    inp.value='';chips.style.display='none';\n    addMsg(q,true);showTyping();btn.disabled=true;\n    google.colab.kernel.invokeFunction('notebook_chat',[q],{})\n      .then(function(r){\n        hideTyping();\n        var a=r.data['application/json'];\n        addMsg(typeof a==='string'?a:JSON.stringify(a),false);\n      })\n      .catch(function(){\n        hideTyping();\n        addMsg('Sorry, I encountered an error. Please check your internet connection and try again.',false);\n      })\n      .finally(function(){btn.disabled=false;inp.focus()});\n  };\n\n  window.vcAsk=function(q){inp.value=q;vcSendMsg()};\n  window.vcClear=function(){\n    msgs.innerHTML='<div class=\"vc-msg bot\"><div class=\"vc-bbl\">&#128075; Chat cleared. Ask me anything!</div></div>';\n    chips.style.display='flex';\n  };\n\n  inp.addEventListener('keypress',function(e){if(e.key==='Enter')vcSendMsg()});\n  inp.focus();\n})();\n</script>'''))"
   ],
   "id": "vizuara_chatbot"
  }
 ]
}