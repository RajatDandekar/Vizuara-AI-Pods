{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Vision-Language-Action Models: See, Understand, Act â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Vision-Language-Action Models: Teaching Robots to See, Understand, and Act\n",
    "\n",
    "**Notebook 6 of 6 â€” World Action Models Series | Vizuara**\n",
    "\n",
    "**Estimated time: ~45 minutes**\n",
    "\n",
    "In the previous notebooks, we built world models that imagine the future, learned latent dynamics, and explored how agents can plan inside a learned simulator. But there has always been a missing piece: how do we close the loop from *seeing the world* and *understanding language* all the way to *physically acting*?\n",
    "\n",
    "Today, we bring everything together. Vision-Language-Action (VLA) models are the grand convergence â€” world models (imagination) + large language models (language understanding) + vision encoders (perception), all fused into a single system that outputs real robot actions.\n",
    "\n",
    "By the end of this notebook, you will build a simplified VLA that takes an image and a language command (\"reach the red circle\") and outputs a smooth robot action trajectory using **flow matching** â€” the same technique behind Physical Intelligence's pi-zero robot controller."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://course-creator-brown.vercel.app/courses/world-action-models/practice/6/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 â€” Why Does This Matter?\n",
    "\n",
    "Let us start with a simple observation. Over the past few years, three revolutions happened almost simultaneously:\n",
    "\n",
    "1. **Vision models** (like ViT, DINO) learned to see the world with remarkable precision.\n",
    "2. **Language models** (like GPT, LLaMA) learned to understand and reason about instructions.\n",
    "3. **World models** (like JEPA, Dreamer) learned to imagine future states from experience.\n",
    "\n",
    "For the longest time, these lived in separate worlds. Computer vision researchers built perception systems. NLP researchers built language systems. Robotics researchers built controllers. Each module was excellent on its own â€” but getting them to work together was a nightmare of glue code, interface mismatches, and cascading errors.\n",
    "\n",
    "Then, in late 2024 and 2025, a breakthrough happened: researchers asked, **\"What if one model could do all three?\"**\n",
    "\n",
    "**Physical Intelligence's pi-zero ($\\pi_0$)** demonstrated exactly this. A single neural network takes in camera images and a language instruction like \"fold the laundry\" and outputs continuous robot joint actions at 50Hz â€” fast enough to control a real robot arm in real time. The secret ingredient? **Flow matching** for action generation.\n",
    "\n",
    "Meanwhile, **Meta's V-JEPA 2** showed something equally stunning: a model pre-trained on internet video (no labels, no language, just watching the world) developed such deep physical understanding that it could control a robot with minimal task-specific data. It watched millions of videos of the world and, from that alone, learned enough physics to manipulate objects.\n",
    "\n",
    "This is the frontier of AI: **one model that sees, understands language, and acts.**"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 0: Setup and imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.collections import LineCollection\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 â€” Building Intuition (No Code)\n",
    "\n",
    "### The Old Way: A Relay Race\n",
    "\n",
    "Traditionally, a robot control pipeline looked like a relay race with three runners:\n",
    "\n",
    "1. **Perception module** â€” takes the camera image and extracts features (\"I see a red cup at position (0.3, 0.5)\")\n",
    "2. **Planning module** â€” takes those features + the language command and plans a path (\"Move to (0.3, 0.5) while avoiding obstacles\")\n",
    "3. **Control module** â€” takes the plan and converts it to motor commands (\"Set joint 1 to 45 degrees, joint 2 to 30 degrees...\")\n",
    "\n",
    "Each runner passes a baton to the next. The problem? **Errors accumulate at every handoff.** If the perception module misidentifies the cup's position by 2 centimeters, the planner plans to the wrong spot, and the controller faithfully executes the wrong plan. The modules cannot correct each other because information only flows in one direction.\n",
    "\n",
    "### The VLA Way: A Single Athlete\n",
    "\n",
    "A Vision-Language-Action model replaces the relay race with **a single athlete who sees, thinks, and moves in one fluid motion.** There is no baton-passing, no interface mismatch, no error accumulation between stages. The entire system is trained end-to-end:\n",
    "\n",
    "$$a_{1:T} = \\pi_\\theta(\\text{img}, \\text{text})$$\n",
    "\n",
    "One model. Image in, language in, action trajectory out. This is exactly what we want.\n",
    "\n",
    "### Flow Matching Intuition\n",
    "\n",
    "Now the question is: how does the model actually *generate* the action trajectory? This is where **flow matching** comes in.\n",
    "\n",
    "Imagine you have a cloud of random dust particles floating in the air (this is noise â€” random vectors sampled from a Gaussian distribution). Now imagine there is an invisible force field that gently pushes each particle along a smooth path until it lands exactly where it needs to be â€” forming a beautiful, smooth action trajectory.\n",
    "\n",
    "**Flow matching learns this force field.** Formally, it learns a velocity field $v_\\theta(x_t, t)$ that tells you: \"If you are at position $x_t$ at time $t$, move in *this* direction.\" Start from random noise at $t=0$, follow the velocity field step by step, and at $t=1$ you arrive at a valid action trajectory.\n",
    "\n",
    "Think of it like water carving a river path through random terrain. The water does not teleport to its destination â€” it flows smoothly, and the path it carves is the trajectory.\n",
    "\n",
    "> **Why is this better than just directly predicting the actions?** Because robot actions need to be *smooth*. A robot arm cannot teleport â€” it must follow a continuous path. Flow matching naturally produces smooth trajectories because the velocity field itself is smooth. Direct regression, on the other hand, tends to produce jerky, averaged-out predictions â€” especially when there are multiple valid ways to reach a target.\n",
    "\n",
    "### ðŸ¤” Think About This\n",
    "\n",
    "\"Why is it important that the action output is a smooth trajectory rather than discrete commands?\"\n",
    "\n",
    "Consider a robot pouring water from a kettle. If the actions are discrete (\"move left\", \"tilt\", \"stop\"), the motion is jerky and the water splashes everywhere. But if the actions are a smooth, continuous trajectory of positions and angles, the pour is clean and controlled. Real-world physics demands smoothness â€” and flow matching delivers it."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 â€” The Mathematics\n",
    "\n",
    "Let us now formalize the intuitions we built above. We will break this down step by step.\n",
    "\n",
    "### Step 1: The VLA Formulation\n",
    "\n",
    "A Vision-Language-Action model is a policy $\\pi_\\theta$ that maps an observation (image) and an instruction (text) to a sequence of actions:\n",
    "\n",
    "$$a_{1:T} = \\pi_\\theta(\\text{img}, \\text{text})$$\n",
    "\n",
    "Here, $a_{1:T} = (a_1, a_2, \\ldots, a_T)$ is a trajectory of $T$ action vectors. Each $a_t \\in \\mathbb{R}^d$ where $d$ is the action dimension (e.g., $d=2$ for 2D position changes).\n",
    "\n",
    "Computationally, the image goes through a vision encoder, the text goes through a language encoder, their features are fused, and this fused representation conditions an action decoder that generates the full trajectory.\n",
    "\n",
    "### Step 2: Flow Matching â€” The Velocity Field\n",
    "\n",
    "The action decoder uses **flow matching**. The core idea is to learn a velocity field $v_\\theta(x_t, t)$ that transports samples from a simple noise distribution to the data distribution:\n",
    "\n",
    "$$v_\\theta : \\mathbb{R}^{T \\times d} \\times [0, 1] \\rightarrow \\mathbb{R}^{T \\times d}$$\n",
    "\n",
    "At time $t=0$, we have noise: $x_0 \\sim \\mathcal{N}(0, I)$. At time $t=1$, we want valid action trajectories: $x_1 \\sim p_{\\text{data}}$.\n",
    "\n",
    "The velocity field tells us the direction and speed to move at each point in this \"flow time.\"\n",
    "\n",
    "Let us plug in some simple numbers to build intuition. Suppose our action is 1-dimensional and we have noise $x_0 = -1.5$ and target action $x_1 = 3.0$. The \"ideal\" velocity that would take us from noise to data in a straight line is simply $(x_1 - x_0) = 3.0 - (-1.5) = 4.5$. The model learns to predict this velocity at each interpolation point along the way.\n",
    "\n",
    "### Step 3: The Interpolation Path\n",
    "\n",
    "During training, we define a linear interpolation between noise and data:\n",
    "\n",
    "$$x_t = (1 - t) \\cdot x_0 + t \\cdot x_1$$\n",
    "\n",
    "At $t=0$, we are at the noise $x_0$. At $t=1$, we are at the target action $x_1$. At $t=0.5$, we are exactly halfway.\n",
    "\n",
    "Using our numbers: at $t = 0.3$, we get $x_{0.3} = (1 - 0.3) \\times (-1.5) + 0.3 \\times 3.0 = 0.7 \\times (-1.5) + 0.3 \\times 3.0 = -1.05 + 0.9 = -0.15$. This makes sense â€” at 30% of the way through the flow, we are close to zero, partway between noise and the target.\n",
    "\n",
    "### Step 4: The Training Loss\n",
    "\n",
    "The model learns by predicting the velocity at each interpolation point. The target velocity for the linear interpolation is simply:\n",
    "\n",
    "$$u_t = x_1 - x_0$$\n",
    "\n",
    "And the loss is:\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}_{t \\sim U(0,1),\\; x_0 \\sim \\mathcal{N}(0,I),\\; x_1 \\sim p_{\\text{data}}} \\left[ \\| v_\\theta(x_t, t) - (x_1 - x_0) \\|^2 \\right]$$\n",
    "\n",
    "In plain English: sample a random time $t$, sample noise $x_0$, sample a real action trajectory $x_1$, compute the interpolation point $x_t$, ask the model to predict the velocity at that point, and penalize it if the prediction differs from the true velocity $(x_1 - x_0)$.\n",
    "\n",
    "Let us verify with our numbers. If $x_0 = -1.5$ and $x_1 = 3.0$, the target velocity is $3.0 - (-1.5) = 4.5$. If the model predicts $v_\\theta = 4.2$, the loss is $(4.5 - 4.2)^2 = 0.09$. If the model predicts $4.5$ exactly, the loss is $0$. This is exactly what we want.\n",
    "\n",
    "### Step 5: Inference via ODE Integration\n",
    "\n",
    "At inference time, we start from noise $x_0 \\sim \\mathcal{N}(0, I)$ and integrate the learned velocity field using Euler steps:\n",
    "\n",
    "$$x_{t + \\Delta t} = x_t + \\Delta t \\cdot v_\\theta(x_t, t)$$\n",
    "\n",
    "We take $K$ steps from $t=0$ to $t=1$, with step size $\\Delta t = 1/K$. The final $x_1$ is our generated action trajectory.\n",
    "\n",
    "Using our 1D example with $K=4$ steps ($\\Delta t = 0.25$): starting from $x_0 = -1.5$, if the model has learned perfectly ($v_\\theta \\approx 4.5$ everywhere), then after step 1: $x_{0.25} = -1.5 + 0.25 \\times 4.5 = -0.375$. After step 2: $x_{0.5} = -0.375 + 0.25 \\times 4.5 = 0.75$. After step 3: $x_{0.75} = 0.75 + 0.25 \\times 4.5 = 1.875$. After step 4: $x_{1.0} = 1.875 + 0.25 \\times 4.5 = 3.0$. We arrive exactly at the target. This is the power of flow matching."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick demonstration: flow matching in 1D\n",
    "# Let us verify our numerical example from above\n",
    "\n",
    "x_0 = -1.5  # noise sample\n",
    "x_1 = 3.0   # target action\n",
    "K = 4        # number of Euler steps\n",
    "dt = 1.0 / K\n",
    "\n",
    "# True velocity (what a perfectly trained model predicts)\n",
    "true_velocity = x_1 - x_0\n",
    "print(f\"True velocity: {true_velocity}\")\n",
    "\n",
    "# Euler integration\n",
    "x_t = x_0\n",
    "trajectory = [x_t]\n",
    "for step in range(K):\n",
    "    t = step * dt\n",
    "    x_t = x_t + dt * true_velocity\n",
    "    trajectory.append(x_t)\n",
    "    print(f\"Step {step+1}: t={t+dt:.2f}, x_t={x_t:.3f}\")\n",
    "\n",
    "print(f\"\\nFinal x_1 = {trajectory[-1]:.3f} (target = {x_1})\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualize this flow in 1D to build even deeper intuition."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the 1D flow matching process\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: The interpolation path\n",
    "t_vals = np.linspace(0, 1, 50)\n",
    "x_t_vals = (1 - t_vals) * x_0 + t_vals * x_1\n",
    "axes[0].plot(t_vals, x_t_vals, 'b-', linewidth=2)\n",
    "axes[0].scatter([0, 1], [x_0, x_1], c=['red', 'green'],\n",
    "                s=100, zorder=5)\n",
    "axes[0].set_xlabel('Flow time t')\n",
    "axes[0].set_ylabel('x_t')\n",
    "axes[0].set_title('Interpolation: Noise â†’ Action')\n",
    "axes[0].legend(['Path', 'Noise (t=0)', 'Target (t=1)'])\n",
    "\n",
    "# Plot 2: The velocity field (constant for linear flow)\n",
    "axes[1].barh([0], [true_velocity], color='purple', height=0.3)\n",
    "axes[1].set_xlabel('Velocity')\n",
    "axes[1].set_title(f'Target Velocity = {true_velocity}')\n",
    "axes[1].set_yticks([])\n",
    "\n",
    "# Plot 3: Euler integration steps\n",
    "steps_t = np.linspace(0, 1, K + 1)\n",
    "axes[2].plot(steps_t, trajectory, 'ro-', linewidth=2, markersize=8)\n",
    "axes[2].plot(t_vals, x_t_vals, 'b--', alpha=0.3)\n",
    "axes[2].set_xlabel('Flow time t')\n",
    "axes[2].set_ylabel('x_t')\n",
    "axes[2].set_title(f'Euler Integration ({K} steps)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 â€” Let Us Build It\n",
    "\n",
    "Now we have all the mathematical machinery. Let us build a complete (simplified) Vision-Language-Action model for a 2D reaching task.\n",
    "\n",
    "Here is the setup:\n",
    "- A \"robot arm\" (2-link arm in 2D) must reach a colored target\n",
    "- **Observation** = an image of the scene (rendered with matplotlib)\n",
    "- **Action** = a sequence of $(dx, dy)$ movements that move the end-effector\n",
    "- **Language instruction** = \"reach the [color] circle\"\n",
    "\n",
    "We will build every component from scratch: vision encoder, language encoder, fusion module, and flow matching action decoder."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 â€” The 2D Environment\n",
    "\n",
    "Let us create our simple environment. We will render a scene with a 2-link robot arm and a colored target circle. The robot must learn to move its end-effector to the target."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment constants\n",
    "IMG_SIZE = 64\n",
    "ARM_LINK_1 = 0.3   # length of first link\n",
    "ARM_LINK_2 = 0.25  # length of second link\n",
    "ARM_BASE = np.array([0.5, 0.1])  # base position (normalized)\n",
    "TRAJ_LEN = 16      # action trajectory length\n",
    "ACTION_DIM = 2     # (dx, dy) per step\n",
    "\n",
    "COLORS = {\n",
    "    'red': [1.0, 0.2, 0.2],\n",
    "    'blue': [0.2, 0.4, 1.0],\n",
    "    'green': [0.2, 0.8, 0.3],\n",
    "}\n",
    "COLOR_NAMES = list(COLORS.keys())\n",
    "\n",
    "def forward_kinematics(theta1, theta2):\n",
    "    \"\"\"Compute end-effector position from joint angles.\"\"\"\n",
    "    joint = ARM_BASE + ARM_LINK_1 * np.array(\n",
    "        [np.cos(theta1), np.sin(theta1)])\n",
    "    end = joint + ARM_LINK_2 * np.array(\n",
    "        [np.cos(theta1 + theta2), np.sin(theta1 + theta2)])\n",
    "    return joint, end"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_scene(target_pos, target_color_name,\n",
    "                 theta1=np.pi/3, theta2=np.pi/4,\n",
    "                 trajectory=None):\n",
    "    \"\"\"Render the 2D scene as a numpy image array.\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(2, 2), dpi=32)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_facecolor('#1a1a2e')\n",
    "\n",
    "    # Draw target circle\n",
    "    color_rgb = COLORS[target_color_name]\n",
    "    circle = plt.Circle(target_pos, 0.06, color=color_rgb,\n",
    "                        alpha=0.9)\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "    # Draw robot arm\n",
    "    joint, end = forward_kinematics(theta1, theta2)\n",
    "    ax.plot([ARM_BASE[0], joint[0]], [ARM_BASE[1], joint[1]],\n",
    "            'w-', linewidth=3)\n",
    "    ax.plot([joint[0], end[0]], [joint[1], end[1]],\n",
    "            'w-', linewidth=3)\n",
    "    ax.plot(*ARM_BASE, 'wo', markersize=6)\n",
    "    ax.plot(*joint, 'wo', markersize=4)\n",
    "    ax.plot(*end, 'yo', markersize=5)\n",
    "\n",
    "    # Draw trajectory if provided\n",
    "    if trajectory is not None:\n",
    "        traj = np.cumsum(trajectory, axis=0) + end\n",
    "        ax.plot(traj[:, 0], traj[:, 1], 'y--', linewidth=1,\n",
    "                alpha=0.7)\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    buf = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)\n",
    "    w, h = fig.canvas.get_width_height()\n",
    "    img = buf.reshape(h, w, 4)[:, :, :3]  # drop alpha\n",
    "    plt.close(fig)\n",
    "    return img"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize our environment with different targets."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "for idx, color_name in enumerate(COLOR_NAMES):\n",
    "    target = np.array([0.3 + 0.2 * idx, 0.6 + 0.1 * idx])\n",
    "    img = render_scene(target, color_name)\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(f'\"reach the {color_name} circle\"',\n",
    "                        fontsize=11)\n",
    "    axes[idx].axis('off')\n",
    "plt.suptitle('Our 2D Robot Environment', fontsize=14,\n",
    "             fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 â€” Generating Training Data\n",
    "\n",
    "Before we build the model, let us generate our training dataset. For each sample, we randomly place a colored target, compute the ideal trajectory from the arm's end-effector to the target, and render the scene image."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(start_pos, target_pos, n_steps=TRAJ_LEN):\n",
    "    \"\"\"Generate a smooth trajectory from start to target.\n",
    "\n",
    "    Returns (dx, dy) deltas at each step, with slight\n",
    "    curvature for realism.\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, 1, n_steps + 1)\n",
    "    # Add slight sinusoidal curvature\n",
    "    curve = 0.03 * np.sin(np.pi * t[:-1])\n",
    "    direction = target_pos - start_pos\n",
    "    perp = np.array([-direction[1], direction[0]])\n",
    "    perp = perp / (np.linalg.norm(perp) + 1e-8)\n",
    "\n",
    "    positions = np.outer(t[1:], direction) + start_pos\n",
    "    positions += np.outer(curve, perp)\n",
    "\n",
    "    # Convert to deltas\n",
    "    all_pos = np.vstack([start_pos[None, :], positions])\n",
    "    deltas = np.diff(all_pos, axis=0)\n",
    "    return deltas.astype(np.float32)"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(n_samples=2000):\n",
    "    \"\"\"Generate the full training dataset.\"\"\"\n",
    "    images = []\n",
    "    instructions = []\n",
    "    trajectories = []\n",
    "\n",
    "    theta1, theta2 = np.pi / 3, np.pi / 4\n",
    "    _, end_effector = forward_kinematics(theta1, theta2)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # Random target position (reachable area)\n",
    "        tx = np.random.uniform(0.15, 0.85)\n",
    "        ty = np.random.uniform(0.35, 0.90)\n",
    "        target_pos = np.array([tx, ty])\n",
    "        color_name = COLOR_NAMES[i % len(COLOR_NAMES)]\n",
    "\n",
    "        # Render scene\n",
    "        img = render_scene(target_pos, color_name,\n",
    "                           theta1, theta2)\n",
    "        # Generate trajectory\n",
    "        traj = generate_trajectory(end_effector, target_pos)\n",
    "\n",
    "        images.append(img)\n",
    "        instructions.append(color_name)  # simplified encoding\n",
    "        trajectories.append(traj)\n",
    "\n",
    "    images = np.stack(images)              # (N, H, W, 3)\n",
    "    trajectories = np.stack(trajectories)  # (N, T, 2)\n",
    "    return images, instructions, trajectories\n",
    "\n",
    "print(\"Generating training data...\")\n",
    "images, instructions, trajectories = generate_dataset(2000)\n",
    "print(f\"Images shape: {images.shape}\")\n",
    "print(f\"Trajectories shape: {trajectories.shape}\")\n",
    "print(f\"Sample instructions: {instructions[:6]}\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize a few training samples with their ground truth trajectories."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for idx in range(4):\n",
    "    axes[idx].imshow(images[idx])\n",
    "    # Overlay trajectory\n",
    "    _, end = forward_kinematics(np.pi/3, np.pi/4)\n",
    "    traj_pos = np.cumsum(trajectories[idx], axis=0) + end\n",
    "    # Scale to pixel coordinates\n",
    "    h, w = images[idx].shape[:2]\n",
    "    px = traj_pos[:, 0] * w\n",
    "    py = (1 - traj_pos[:, 1]) * h  # flip y\n",
    "    axes[idx].plot(px, py, 'y.-', linewidth=1.5, markersize=3)\n",
    "    axes[idx].set_title(f'\"{instructions[idx]}\"')\n",
    "    axes[idx].axis('off')\n",
    "plt.suptitle('Training Samples with Ground Truth Trajectories',\n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 â€” The Vision Encoder\n",
    "\n",
    "Our vision encoder is a simple CNN that takes the scene image and encodes it into a compact feature vector. In the real $\\pi_0$ system, this would be a large pre-trained vision transformer. For our 2D task, a small CNN is sufficient."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEncoder(nn.Module):\n",
    "    \"\"\"Simple CNN that encodes a scene image into features.\"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.fc = nn.Linear(64, feature_dim)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # img: (B, 3, H, W) normalized to [0, 1]\n",
    "        x = self.conv(img)         # (B, 64, 1, 1)\n",
    "        x = x.view(x.size(0), -1)  # (B, 64)\n",
    "        return self.fc(x)           # (B, feature_dim)\n",
    "\n",
    "vision_enc = VisionEncoder(feature_dim=64).to(DEVICE)\n",
    "print(f\"Vision encoder parameters: \"\n",
    "      f\"{sum(p.numel() for p in vision_enc.parameters()):,}\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 â€” The Language Encoder\n",
    "\n",
    "For language, we use a simple embedding layer that maps each color name to a learned feature vector. In the real $\\pi_0$, this would be a full language model (like PaLM or Gemma). For our task, since the vocabulary is just three color names, an embedding suffices."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageEncoder(nn.Module):\n",
    "    \"\"\"Embedding-based encoder for color instructions.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size=3, feature_dim=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, feature_dim)\n",
    "\n",
    "    def forward(self, instruction_ids):\n",
    "        # instruction_ids: (B,) integer tensor\n",
    "        return self.embedding(instruction_ids)  # (B, feature_dim)\n",
    "\n",
    "# Vocabulary mapping\n",
    "VOCAB = {name: i for i, name in enumerate(COLOR_NAMES)}\n",
    "print(f\"Vocabulary: {VOCAB}\")\n",
    "\n",
    "lang_enc = LanguageEncoder(vocab_size=len(VOCAB),\n",
    "                           feature_dim=64).to(DEVICE)\n",
    "print(f\"Language encoder parameters: \"\n",
    "      f\"{sum(p.numel() for p in lang_enc.parameters()):,}\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 â€” The Fusion Module\n",
    "\n",
    "Now we combine vision and language features. We concatenate them and pass through a small MLP to produce a single fused representation. This fused vector will condition our flow matching decoder."
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionModule(nn.Module):\n",
    "    \"\"\"Fuse vision and language features via concatenation + MLP.\"\"\"\n",
    "\n",
    "    def __init__(self, vision_dim=64, lang_dim=64, fused_dim=128):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(vision_dim + lang_dim, fused_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fused_dim, fused_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, vision_feat, lang_feat):\n",
    "        combined = torch.cat([vision_feat, lang_feat], dim=-1)\n",
    "        return self.mlp(combined)  # (B, fused_dim)\n",
    "\n",
    "fusion = FusionModule(fused_dim=128).to(DEVICE)\n",
    "print(f\"Fusion module parameters: \"\n",
    "      f\"{sum(p.numel() for p in fusion.parameters()):,}\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the fused features for different instructions to verify the fusion module differentiates between color commands (even before training, the features should differ due to random initialization)."
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode a small batch to visualize\n",
    "sample_imgs = torch.tensor(\n",
    "    images[:6], dtype=torch.float32\n",
    ").permute(0, 3, 1, 2).to(DEVICE) / 255.0\n",
    "sample_ids = torch.tensor(\n",
    "    [VOCAB[c] for c in instructions[:6]], dtype=torch.long\n",
    ").to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    v_feat = vision_enc(sample_imgs)\n",
    "    l_feat = lang_enc(sample_ids)\n",
    "    fused = fusion(v_feat, l_feat)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 5))\n",
    "for idx in range(6):\n",
    "    row, col = idx // 3, idx % 3\n",
    "    axes[row, col].bar(range(fused.shape[1]),\n",
    "                       fused[idx].cpu().numpy(), width=1.0,\n",
    "                       alpha=0.7)\n",
    "    axes[row, col].set_title(f'\"{instructions[idx]}\" features')\n",
    "    axes[row, col].set_xlim(0, fused.shape[1])\n",
    "plt.suptitle('Fused Vision + Language Features (Before Training)',\n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 â€” The Flow Matching Action Decoder\n",
    "\n",
    "This is the heart of our VLA. The decoder takes:\n",
    "1. **Fused features** (from vision + language)\n",
    "2. **A noisy action trajectory** $x_t$ (the current state in the flow)\n",
    "3. **A timestep** $t$ (where we are in the flow from noise to data)\n",
    "\n",
    "And outputs the predicted velocity field $v_\\theta(x_t, t)$ â€” the direction to move from noise toward a valid action trajectory."
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowMatchingDecoder(nn.Module):\n",
    "    \"\"\"Predicts velocity field for flow matching.\n",
    "\n",
    "    Takes fused conditioning features, noisy trajectory,\n",
    "    and flow timestep. Outputs predicted velocity.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fused_dim=128, traj_len=TRAJ_LEN,\n",
    "                 action_dim=ACTION_DIM, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.traj_len = traj_len\n",
    "        self.action_dim = action_dim\n",
    "        traj_flat = traj_len * action_dim  # flattened trajectory\n",
    "\n",
    "        # Timestep embedding\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(64, 64),\n",
    "        )\n",
    "\n",
    "        # Main network\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(fused_dim + traj_flat + 64, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, traj_flat),\n",
    "        )\n",
    "\n",
    "    def forward(self, fused_feat, x_t, t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            fused_feat: (B, fused_dim) conditioning\n",
    "            x_t: (B, traj_len, action_dim) noisy trajectory\n",
    "            t: (B, 1) flow timestep in [0, 1]\n",
    "        Returns:\n",
    "            v_theta: (B, traj_len, action_dim) predicted velocity\n",
    "        \"\"\"\n",
    "        B = fused_feat.shape[0]\n",
    "        x_flat = x_t.view(B, -1)\n",
    "        t_emb = self.time_embed(t)\n",
    "        inp = torch.cat([fused_feat, x_flat, t_emb], dim=-1)\n",
    "        out = self.net(inp)\n",
    "        return out.view(B, self.traj_len, self.action_dim)"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = FlowMatchingDecoder(fused_dim=128).to(DEVICE)\n",
    "print(f\"Flow matching decoder parameters: \"\n",
    "      f\"{sum(p.numel() for p in decoder.parameters()):,}\")\n",
    "\n",
    "# Total model parameters\n",
    "total_params = sum(\n",
    "    sum(p.numel() for p in m.parameters())\n",
    "    for m in [vision_enc, lang_enc, fusion, decoder]\n",
    ")\n",
    "print(f\"\\nTotal VLA parameters: {total_params:,}\")\n",
    "print(\"(A real VLA like pi-zero has billions â€” ours is tiny \"\n",
    "      \"but captures the same architecture!)\")"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 â€” The Flow Matching Training Loop\n",
    "\n",
    "Now let us implement the training procedure. Recall the key steps:\n",
    "\n",
    "1. Sample a batch of (image, instruction, target trajectory)\n",
    "2. Sample random noise $x_0 \\sim \\mathcal{N}(0, I)$\n",
    "3. Sample random flow time $t \\sim U(0, 1)$\n",
    "4. Compute interpolation $x_t = (1-t) \\cdot x_0 + t \\cdot x_1$\n",
    "5. Predict velocity $v_\\theta(x_t, t)$\n",
    "6. Loss = $\\|v_\\theta - (x_1 - x_0)\\|^2$"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_matching_loss(decoder, fused_feat, target_traj):\n",
    "    \"\"\"Compute the flow matching training loss.\n",
    "\n",
    "    Args:\n",
    "        decoder: FlowMatchingDecoder\n",
    "        fused_feat: (B, fused_dim) conditioning features\n",
    "        target_traj: (B, T, 2) ground truth trajectory (x_1)\n",
    "    Returns:\n",
    "        loss: scalar tensor\n",
    "    \"\"\"\n",
    "    B = target_traj.shape[0]\n",
    "\n",
    "    # Step 1: Sample noise x_0\n",
    "    x_0 = torch.randn_like(target_traj)\n",
    "\n",
    "    # Step 2: Sample random timestep t ~ U(0, 1)\n",
    "    t = torch.rand(B, 1, device=target_traj.device)\n",
    "\n",
    "    # Step 3: Interpolate x_t = (1-t)*x_0 + t*x_1\n",
    "    t_expand = t.unsqueeze(-1)  # (B, 1, 1)\n",
    "    x_t = (1 - t_expand) * x_0 + t_expand * target_traj\n",
    "\n",
    "    # Step 4: True velocity is (x_1 - x_0)\n",
    "    true_velocity = target_traj - x_0\n",
    "\n",
    "    # Step 5: Predict velocity\n",
    "    pred_velocity = decoder(fused_feat, x_t, t)\n",
    "\n",
    "    # Step 6: MSE loss\n",
    "    loss = F.mse_loss(pred_velocity, true_velocity)\n",
    "    return loss"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 â€” Flow Matching Inference\n",
    "\n",
    "At inference time, we start from pure noise and iteratively follow the learned velocity field to generate the action trajectory. This is an ODE (Ordinary Differential Equation) integration using Euler steps."
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def flow_matching_inference(decoder, fused_feat,\n",
    "                            n_steps=20):\n",
    "    \"\"\"Generate action trajectory via flow matching inference.\n",
    "\n",
    "    Start from noise, integrate the velocity field using\n",
    "    Euler steps from t=0 to t=1.\n",
    "\n",
    "    Args:\n",
    "        decoder: trained FlowMatchingDecoder\n",
    "        fused_feat: (B, fused_dim) conditioning\n",
    "        n_steps: number of Euler integration steps\n",
    "    Returns:\n",
    "        x_t: (B, TRAJ_LEN, ACTION_DIM) generated trajectory\n",
    "    \"\"\"\n",
    "    B = fused_feat.shape[0]\n",
    "    dt = 1.0 / n_steps\n",
    "\n",
    "    # Start from noise\n",
    "    x_t = torch.randn(B, TRAJ_LEN, ACTION_DIM,\n",
    "                       device=fused_feat.device)\n",
    "\n",
    "    # Euler integration\n",
    "    for step in range(n_steps):\n",
    "        t = torch.full((B, 1), step * dt,\n",
    "                       device=fused_feat.device)\n",
    "        velocity = decoder(fused_feat, x_t, t)\n",
    "        x_t = x_t + dt * velocity\n",
    "\n",
    "    return x_t"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the flow field before training. Since the model is randomly initialized, the velocity vectors will point in random directions â€” but after training, they will consistently point from noise toward valid trajectories."
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize flow field (pre-training): 2D slice\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "# Create a grid of points in 2D (first action step)\n",
    "grid_pts = 10\n",
    "xx, yy = np.meshgrid(np.linspace(-2, 2, grid_pts),\n",
    "                      np.linspace(-2, 2, grid_pts))\n",
    "grid = np.stack([xx.ravel(), yy.ravel()], axis=-1)\n",
    "\n",
    "# Create dummy fused features (single instruction)\n",
    "dummy_fused = torch.zeros(grid_pts**2, 128, device=DEVICE)\n",
    "# Create dummy trajectories (only first step varies)\n",
    "dummy_traj = torch.zeros(grid_pts**2, TRAJ_LEN, ACTION_DIM,\n",
    "                         device=DEVICE)\n",
    "dummy_traj[:, 0, :] = torch.tensor(grid, dtype=torch.float32,\n",
    "                                    device=DEVICE)\n",
    "dummy_t = torch.full((grid_pts**2, 1), 0.5, device=DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    vel = decoder(dummy_fused, dummy_traj, dummy_t)\n",
    "vel_2d = vel[:, 0, :].cpu().numpy()\n",
    "\n",
    "ax.quiver(grid[:, 0], grid[:, 1],\n",
    "          vel_2d[:, 0], vel_2d[:, 1],\n",
    "          color='cyan', alpha=0.7)\n",
    "ax.set_title('Flow Field (Before Training) â€” Random Directions',\n",
    "             fontsize=12)\n",
    "ax.set_xlabel('dx')\n",
    "ax.set_ylabel('dy')\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 â€” ðŸ”§ Your Turn (TODO Exercises)\n",
    "\n",
    "Now it is your turn to implement two critical components. These exercises will solidify your understanding of flow matching.\n",
    "\n",
    "### TODO 1: Implement the Flow Matching Interpolation and Loss\n",
    "\n",
    "Fill in the missing parts of the function below. You need to:\n",
    "1. Compute the interpolation $x_t = (1 - t) \\cdot x_0 + t \\cdot x_1$\n",
    "2. Compute the target velocity $u_t = x_1 - x_0$\n",
    "3. Compute the MSE loss between predicted and target velocity"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_matching_loss_todo(decoder, fused_feat, target_traj):\n",
    "    \"\"\"TODO: Implement flow matching loss.\n",
    "\n",
    "    Args:\n",
    "        decoder: FlowMatchingDecoder\n",
    "        fused_feat: (B, fused_dim) conditioning features\n",
    "        target_traj: (B, T, 2) ground truth trajectory (x_1)\n",
    "    Returns:\n",
    "        loss: scalar tensor\n",
    "    \"\"\"\n",
    "    B = target_traj.shape[0]\n",
    "    x_0 = torch.randn_like(target_traj)\n",
    "    t = torch.rand(B, 1, device=target_traj.device)\n",
    "    t_expand = t.unsqueeze(-1)  # (B, 1, 1) for broadcasting\n",
    "\n",
    "    # ---- YOUR CODE HERE ----\n",
    "    # Step 1: Compute x_t (interpolation between x_0 and target)\n",
    "    x_t = None  # TODO\n",
    "\n",
    "    # Step 2: Compute the target velocity\n",
    "    target_velocity = None  # TODO\n",
    "\n",
    "    # Step 3: Get model's predicted velocity\n",
    "    pred_velocity = decoder(fused_feat, x_t, t)\n",
    "\n",
    "    # Step 4: Compute MSE loss\n",
    "    loss = None  # TODO\n",
    "    # ---- END YOUR CODE ----\n",
    "\n",
    "    return loss"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the verification cell below to check your implementation."
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification for TODO 1\n",
    "print(\"Verifying TODO 1: Flow Matching Loss\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create small test tensors\n",
    "_B, _T, _D = 4, TRAJ_LEN, ACTION_DIM\n",
    "_fused = torch.randn(_B, 128, device=DEVICE)\n",
    "_traj = torch.randn(_B, _T, _D, device=DEVICE)\n",
    "\n",
    "try:\n",
    "    _loss = flow_matching_loss_todo(decoder, _fused, _traj)\n",
    "    assert _loss is not None, \"Loss is None â€” did you forget to compute it?\"\n",
    "    assert _loss.shape == torch.Size([]), \\\n",
    "        f\"Loss should be scalar, got shape {_loss.shape}\"\n",
    "    assert _loss.item() > 0, \"Loss should be positive\"\n",
    "    assert _loss.requires_grad, \"Loss should require gradients\"\n",
    "    print(f\"Loss value: {_loss.item():.4f}\")\n",
    "    print(\"All checks passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Hint: x_t = (1-t)*x_0 + t*x_1, velocity = x_1 - x_0\")"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement the Inference Loop\n",
    "\n",
    "Now implement the inference procedure. Starting from random noise, take $K$ Euler steps following the learned velocity field to generate the final action trajectory."
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def flow_matching_inference_todo(decoder, fused_feat,\n",
    "                                 n_steps=20):\n",
    "    \"\"\"TODO: Generate trajectory via flow matching inference.\n",
    "\n",
    "    Start from noise x_0 ~ N(0, I), then for each step k:\n",
    "      t = k / n_steps\n",
    "      velocity = decoder(fused_feat, x_t, t)\n",
    "      x_t = x_t + dt * velocity\n",
    "\n",
    "    Args:\n",
    "        decoder: trained FlowMatchingDecoder\n",
    "        fused_feat: (B, fused_dim) conditioning\n",
    "        n_steps: number of Euler integration steps\n",
    "    Returns:\n",
    "        x_t: (B, TRAJ_LEN, ACTION_DIM) generated trajectory\n",
    "    \"\"\"\n",
    "    B = fused_feat.shape[0]\n",
    "    dt = 1.0 / n_steps\n",
    "\n",
    "    # ---- YOUR CODE HERE ----\n",
    "    # Step 1: Start from random noise\n",
    "    x_t = None  # TODO\n",
    "\n",
    "    # Step 2: Euler integration loop (n_steps iterations)\n",
    "    # For each step k from 0 to n_steps-1:\n",
    "    #   - compute t = k * dt\n",
    "    #   - predict velocity from decoder\n",
    "    #   - update x_t = x_t + dt * velocity\n",
    "    pass  # TODO: replace with your loop\n",
    "    # ---- END YOUR CODE ----\n",
    "\n",
    "    return x_t"
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the verification cell below. Since the model is not trained yet, the generated trajectory will be random â€” but we can verify the shapes and that the function runs."
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification for TODO 2\n",
    "print(\"Verifying TODO 2: Flow Matching Inference\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "_fused = torch.randn(4, 128, device=DEVICE)\n",
    "try:\n",
    "    _gen_traj = flow_matching_inference_todo(decoder, _fused,\n",
    "                                             n_steps=10)\n",
    "    assert _gen_traj is not None, \\\n",
    "        \"Output is None â€” did you forget to return x_t?\"\n",
    "    assert _gen_traj.shape == (4, TRAJ_LEN, ACTION_DIM), \\\n",
    "        f\"Expected shape (4, {TRAJ_LEN}, {ACTION_DIM}), \" \\\n",
    "        f\"got {_gen_traj.shape}\"\n",
    "    assert not torch.isnan(_gen_traj).any(), \\\n",
    "        \"Output contains NaN values\"\n",
    "    print(f\"Output shape: {_gen_traj.shape}\")\n",
    "    print(f\"Output range: [{_gen_traj.min():.3f}, \"\n",
    "          f\"{_gen_traj.max():.3f}]\")\n",
    "    print(\"All checks passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Hint: initialize x_t with torch.randn, then loop \"\n",
    "          \"with Euler steps\")"
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6 â€” Putting It All Together\n",
    "\n",
    "Let us now assemble the full VLA pipeline into a single class that combines all four components: vision encoder, language encoder, fusion module, and flow matching decoder."
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionLanguageActionModel(nn.Module):\n",
    "    \"\"\"Complete VLA: image + text â†’ action trajectory.\n",
    "\n",
    "    Architecture:\n",
    "        Image â†’ VisionEncoder â†’ vision_feat\n",
    "        Text  â†’ LanguageEncoder â†’ lang_feat\n",
    "        (vision_feat, lang_feat) â†’ FusionModule â†’ fused\n",
    "        (fused, noise, t) â†’ FlowMatchingDecoder â†’ velocity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vision_enc = VisionEncoder(feature_dim=64)\n",
    "        self.lang_enc = LanguageEncoder(\n",
    "            vocab_size=len(VOCAB), feature_dim=64)\n",
    "        self.fusion = FusionModule(fused_dim=128)\n",
    "        self.decoder = FlowMatchingDecoder(fused_dim=128)\n",
    "\n",
    "    def get_fused_features(self, images, instruction_ids):\n",
    "        \"\"\"Encode image + text and fuse.\"\"\"\n",
    "        v_feat = self.vision_enc(images)\n",
    "        l_feat = self.lang_enc(instruction_ids)\n",
    "        return self.fusion(v_feat, l_feat)\n",
    "\n",
    "    def compute_loss(self, images, instruction_ids,\n",
    "                     target_traj):\n",
    "        \"\"\"Full forward pass + flow matching loss.\"\"\"\n",
    "        fused = self.get_fused_features(images, instruction_ids)\n",
    "        return flow_matching_loss(self.decoder, fused,\n",
    "                                  target_traj)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, images, instruction_ids, n_steps=20):\n",
    "        \"\"\"Generate action trajectory from image + text.\"\"\"\n",
    "        fused = self.get_fused_features(images, instruction_ids)\n",
    "        return flow_matching_inference(self.decoder, fused,\n",
    "                                       n_steps)\n",
    "\n",
    "# Instantiate the full model\n",
    "vla_model = VisionLanguageActionModel().to(DEVICE)\n",
    "total = sum(p.numel() for p in vla_model.parameters())\n",
    "print(f\"Complete VLA model: {total:,} parameters\")"
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7 â€” Training and Results\n",
    "\n",
    "Now let us train our VLA on the 2D reaching task. We will train for enough epochs to see the flow matching loss converge, then test generalization to unseen target positions."
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data tensors\n",
    "train_images = torch.tensor(\n",
    "    images, dtype=torch.float32\n",
    ").permute(0, 3, 1, 2) / 255.0  # (N, 3, H, W)\n",
    "train_ids = torch.tensor(\n",
    "    [VOCAB[c] for c in instructions], dtype=torch.long)\n",
    "train_trajs = torch.tensor(trajectories, dtype=torch.float32)\n",
    "\n",
    "print(f\"Training images: {train_images.shape}\")\n",
    "print(f\"Training instructions: {train_ids.shape}\")\n",
    "print(f\"Training trajectories: {train_trajs.shape}\")"
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 3e-4\n",
    "N_EPOCHS = 40\n",
    "\n",
    "optimizer = torch.optim.Adam(vla_model.parameters(),\n",
    "                             lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=N_EPOCHS)\n",
    "n_batches = len(train_images) // BATCH_SIZE\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "vla_model.train()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    perm = torch.randperm(len(train_images))\n",
    "\n",
    "    for batch_idx in range(n_batches):\n",
    "        idx = perm[batch_idx * BATCH_SIZE:\n",
    "                   (batch_idx + 1) * BATCH_SIZE]\n",
    "        imgs = train_images[idx].to(DEVICE)\n",
    "        ids = train_ids[idx].to(DEVICE)\n",
    "        trajs = train_trajs[idx].to(DEVICE)\n",
    "\n",
    "        loss = vla_model.compute_loss(imgs, ids, trajs)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{N_EPOCHS} | \"\n",
    "              f\"Loss: {avg_loss:.6f} | \"\n",
    "              f\"LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the training loss curve. We expect the flow matching loss to decrease smoothly as the model learns to predict the correct velocity field."
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "ax.plot(losses, 'b-', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Flow Matching Loss', fontsize=12)\n",
    "ax.set_title('Training Loss: VLA with Flow Matching',\n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Final loss: {losses[-1]:.6f}\")"
   ],
   "id": "cell_51"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on Novel Positions\n",
    "\n",
    "The real test of our VLA: can it generalize to target positions it has never seen during training? Let us create new scenes with unseen target locations and color combinations."
   ],
   "id": "cell_52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vla_model.eval()\n",
    "\n",
    "# Generate test scenes with novel positions\n",
    "test_targets = [\n",
    "    (np.array([0.2, 0.85]), 'red'),\n",
    "    (np.array([0.8, 0.4]),  'blue'),\n",
    "    (np.array([0.5, 0.7]),  'green'),\n",
    "    (np.array([0.7, 0.85]), 'red'),\n",
    "    (np.array([0.3, 0.5]),  'blue'),\n",
    "    (np.array([0.6, 0.55]), 'green'),\n",
    "]\n",
    "\n",
    "theta1, theta2 = np.pi / 3, np.pi / 4\n",
    "_, end_eff = forward_kinematics(theta1, theta2)\n",
    "\n",
    "test_imgs = []\n",
    "test_ids = []\n",
    "for pos, color in test_targets:\n",
    "    img = render_scene(pos, color, theta1, theta2)\n",
    "    test_imgs.append(img)\n",
    "    test_ids.append(VOCAB[color])\n",
    "\n",
    "test_imgs_t = torch.tensor(\n",
    "    np.stack(test_imgs), dtype=torch.float32\n",
    ").permute(0, 3, 1, 2).to(DEVICE) / 255.0\n",
    "test_ids_t = torch.tensor(test_ids, dtype=torch.long).to(DEVICE)"
   ],
   "id": "cell_53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate trajectories\n",
    "gen_trajs = vla_model.generate(test_imgs_t, test_ids_t,\n",
    "                                n_steps=30)\n",
    "gen_trajs_np = gen_trajs.cpu().numpy()\n",
    "\n",
    "# Also compute ground truth for comparison\n",
    "gt_trajs = []\n",
    "for pos, _ in test_targets:\n",
    "    gt = generate_trajectory(end_eff, pos)\n",
    "    gt_trajs.append(gt)\n",
    "gt_trajs_np = np.stack(gt_trajs)\n",
    "\n",
    "print(\"Generated trajectory shapes:\", gen_trajs_np.shape)\n",
    "print(\"Ground truth trajectory shapes:\", gt_trajs_np.shape)"
   ],
   "id": "cell_54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: generated vs ground truth trajectories\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for idx in range(6):\n",
    "    row, col = idx // 3, idx % 3\n",
    "    ax = axes[row, col]\n",
    "    ax.imshow(test_imgs[idx])\n",
    "\n",
    "    h, w = test_imgs[idx].shape[:2]\n",
    "\n",
    "    # Ground truth trajectory (green)\n",
    "    gt_pos = np.cumsum(gt_trajs_np[idx], axis=0) + end_eff\n",
    "    gt_px = gt_pos[:, 0] * w\n",
    "    gt_py = (1 - gt_pos[:, 1]) * h\n",
    "    ax.plot(gt_px, gt_py, 'g-', linewidth=2, alpha=0.8,\n",
    "            label='Ground Truth')\n",
    "\n",
    "    # Generated trajectory (yellow)\n",
    "    gen_pos = np.cumsum(gen_trajs_np[idx], axis=0) + end_eff\n",
    "    gen_px = gen_pos[:, 0] * w\n",
    "    gen_py = (1 - gen_pos[:, 1]) * h\n",
    "    ax.plot(gen_px, gen_py, 'y--', linewidth=2, alpha=0.9,\n",
    "            label='Generated')\n",
    "\n",
    "    color_name = test_targets[idx][1]\n",
    "    ax.set_title(f'\"reach the {color_name} circle\"',\n",
    "                 fontsize=11)\n",
    "    ax.axis('off')\n",
    "    if idx == 0:\n",
    "        ax.legend(loc='lower right', fontsize=8)\n",
    "\n",
    "plt.suptitle('Generated vs Ground Truth Trajectories '\n",
    "             '(Novel Positions)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_55"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow Matching vs Direct Regression\n",
    "\n",
    "Let us now compare flow matching with a simpler baseline: **direct regression**, where the model directly predicts the trajectory without the flow process. This will demonstrate why flow matching produces smoother, higher-quality trajectories."
   ],
   "id": "cell_56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectRegressionDecoder(nn.Module):\n",
    "    \"\"\"Baseline: directly predicts trajectory (no flow).\"\"\"\n",
    "\n",
    "    def __init__(self, fused_dim=128, traj_len=TRAJ_LEN,\n",
    "                 action_dim=ACTION_DIM, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        traj_flat = traj_len * action_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(fused_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, traj_flat),\n",
    "        )\n",
    "        self.traj_len = traj_len\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def forward(self, fused_feat):\n",
    "        out = self.net(fused_feat)\n",
    "        return out.view(-1, self.traj_len, self.action_dim)"
   ],
   "id": "cell_57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the direct regression baseline\n",
    "baseline_decoder = DirectRegressionDecoder(fused_dim=128\n",
    "                                           ).to(DEVICE)\n",
    "\n",
    "# Reuse vision/language/fusion from vla_model (frozen)\n",
    "baseline_opt = torch.optim.Adam(baseline_decoder.parameters(),\n",
    "                                lr=3e-4)\n",
    "baseline_losses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    perm = torch.randperm(len(train_images))\n",
    "\n",
    "    for batch_idx in range(n_batches):\n",
    "        idx = perm[batch_idx * BATCH_SIZE:\n",
    "                   (batch_idx + 1) * BATCH_SIZE]\n",
    "        imgs = train_images[idx].to(DEVICE)\n",
    "        ids = train_ids[idx].to(DEVICE)\n",
    "        trajs = train_trajs[idx].to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fused = vla_model.get_fused_features(imgs, ids)\n",
    "        pred = baseline_decoder(fused)\n",
    "        loss = F.mse_loss(pred, trajs)\n",
    "\n",
    "        baseline_opt.zero_grad()\n",
    "        loss.backward()\n",
    "        baseline_opt.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg = epoch_loss / n_batches\n",
    "    baseline_losses.append(avg)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Baseline Epoch {epoch+1:3d}/{N_EPOCHS} | \"\n",
    "              f\"Loss: {avg:.6f}\")\n",
    "\n",
    "print(\"Baseline training complete!\")"
   ],
   "id": "cell_58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare trajectories: flow matching vs direct regression\n",
    "baseline_decoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    fused_test = vla_model.get_fused_features(test_imgs_t,\n",
    "                                               test_ids_t)\n",
    "    baseline_trajs = baseline_decoder(fused_test).cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for idx in range(6):\n",
    "    row, col = idx // 3, idx % 3\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    # Ground truth\n",
    "    gt_pos = np.cumsum(gt_trajs_np[idx], axis=0) + end_eff\n",
    "    ax.plot(gt_pos[:, 0], gt_pos[:, 1], 'g-o', linewidth=2,\n",
    "            markersize=3, label='Ground Truth', alpha=0.7)\n",
    "\n",
    "    # Flow matching (ours)\n",
    "    fm_pos = np.cumsum(gen_trajs_np[idx], axis=0) + end_eff\n",
    "    ax.plot(fm_pos[:, 0], fm_pos[:, 1], 'b-s', linewidth=2,\n",
    "            markersize=3, label='Flow Matching', alpha=0.7)\n",
    "\n",
    "    # Direct regression (baseline)\n",
    "    dr_pos = np.cumsum(baseline_trajs[idx], axis=0) + end_eff\n",
    "    ax.plot(dr_pos[:, 0], dr_pos[:, 1], 'r-^', linewidth=2,\n",
    "            markersize=3, label='Direct Regression', alpha=0.7)\n",
    "\n",
    "    # Target\n",
    "    target_pos = test_targets[idx][0]\n",
    "    ax.plot(*target_pos, '*', color='gold', markersize=15,\n",
    "            zorder=5, label='Target')\n",
    "\n",
    "    ax.set_title(f'\"reach the {test_targets[idx][1]} circle\"')\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    if idx == 0:\n",
    "        ax.legend(fontsize=7, loc='lower right')\n",
    "\n",
    "plt.suptitle('Flow Matching vs Direct Regression',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_59"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also compare the smoothness of the trajectories quantitatively. We measure smoothness as the variance of the velocity (lower = smoother)."
   ],
   "id": "cell_60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajectory_smoothness(traj_deltas):\n",
    "    \"\"\"Measure smoothness as variance of step-to-step changes.\n",
    "    Lower = smoother.\n",
    "    \"\"\"\n",
    "    # Second differences (jerk)\n",
    "    jerk = np.diff(traj_deltas, axis=0)\n",
    "    return np.mean(jerk ** 2)\n",
    "\n",
    "print(\"Trajectory Smoothness (lower = smoother)\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "fm_smooth = np.mean([trajectory_smoothness(gen_trajs_np[i])\n",
    "                     for i in range(6)])\n",
    "dr_smooth = np.mean([trajectory_smoothness(baseline_trajs[i])\n",
    "                     for i in range(6)])\n",
    "gt_smooth = np.mean([trajectory_smoothness(gt_trajs_np[i])\n",
    "                     for i in range(6)])\n",
    "\n",
    "print(f\"Ground Truth:       {gt_smooth:.6f}\")\n",
    "print(f\"Flow Matching:      {fm_smooth:.6f}\")\n",
    "print(f\"Direct Regression:  {dr_smooth:.6f}\")\n",
    "print(f\"\\nFlow matching produces \"\n",
    "      f\"{dr_smooth/fm_smooth:.1f}x smoother trajectories!\")"
   ],
   "id": "cell_61"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Learned Flow Field\n",
    "\n",
    "Now that the model is trained, let us revisit the flow field visualization. This time, the velocity arrows should consistently point from noise toward valid action trajectories. Compare this with the random arrows we saw before training."
   ],
   "id": "cell_62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned flow field at different timesteps\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "t_values = [0.0, 0.5, 0.9]\n",
    "grid_pts = 12\n",
    "xx, yy = np.meshgrid(np.linspace(-2, 2, grid_pts),\n",
    "                      np.linspace(-2, 2, grid_pts))\n",
    "grid = np.stack([xx.ravel(), yy.ravel()], axis=-1)\n",
    "n_pts = grid_pts ** 2\n",
    "\n",
    "# Use a real fused feature (red target top-right)\n",
    "with torch.no_grad():\n",
    "    sample_fused = vla_model.get_fused_features(\n",
    "        test_imgs_t[:1].repeat(n_pts, 1, 1, 1),\n",
    "        test_ids_t[:1].repeat(n_pts))\n",
    "\n",
    "for ax_idx, t_val in enumerate(t_values):\n",
    "    dummy_traj = torch.zeros(n_pts, TRAJ_LEN, ACTION_DIM,\n",
    "                             device=DEVICE)\n",
    "    dummy_traj[:, 0, :] = torch.tensor(\n",
    "        grid, dtype=torch.float32, device=DEVICE)\n",
    "    t_tensor = torch.full((n_pts, 1), t_val, device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vel = vla_model.decoder(sample_fused, dummy_traj,\n",
    "                                t_tensor)\n",
    "    vel_2d = vel[:, 0, :].cpu().numpy()\n",
    "\n",
    "    axes[ax_idx].quiver(grid[:, 0], grid[:, 1],\n",
    "                        vel_2d[:, 0], vel_2d[:, 1],\n",
    "                        color='cyan', alpha=0.8)\n",
    "    axes[ax_idx].set_title(f'Learned Flow Field (t={t_val})',\n",
    "                           fontsize=12)\n",
    "    axes[ax_idx].set_xlabel('dx')\n",
    "    axes[ax_idx].set_ylabel('dy')\n",
    "    axes[ax_idx].set_aspect('equal')\n",
    "    axes[ax_idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Velocity Field at Different Flow Timesteps '\n",
    "             '(After Training)',\n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_63"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8 â€” ðŸŽ¯ Final Output\n",
    "\n",
    "Let us bring everything together in a stunning final visualization. We will show a 2x3 grid of different language commands, each producing a different generated trajectory overlaid on the scene. Then we will animate one trajectory showing the \"robot\" moving step by step."
   ],
   "id": "cell_64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final showcase: 2x3 grid of different commands\n",
    "showcase = [\n",
    "    (np.array([0.25, 0.80]), 'red',   'reach the red circle'),\n",
    "    (np.array([0.75, 0.45]), 'blue',  'reach the blue circle'),\n",
    "    (np.array([0.50, 0.90]), 'green', 'reach the green circle'),\n",
    "    (np.array([0.15, 0.55]), 'red',   'reach the red circle'),\n",
    "    (np.array([0.85, 0.75]), 'blue',  'reach the blue circle'),\n",
    "    (np.array([0.40, 0.65]), 'green', 'reach the green circle'),\n",
    "]\n",
    "\n",
    "show_imgs = []\n",
    "show_ids = []\n",
    "for pos, color, _ in showcase:\n",
    "    img = render_scene(pos, color, theta1, theta2)\n",
    "    show_imgs.append(img)\n",
    "    show_ids.append(VOCAB[color])\n",
    "\n",
    "show_imgs_t = torch.tensor(\n",
    "    np.stack(show_imgs), dtype=torch.float32\n",
    ").permute(0, 3, 1, 2).to(DEVICE) / 255.0\n",
    "show_ids_t = torch.tensor(show_ids, dtype=torch.long\n",
    "                          ).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    show_trajs = vla_model.generate(\n",
    "        show_imgs_t, show_ids_t, n_steps=30\n",
    "    ).cpu().numpy()"
   ],
   "id": "cell_65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(16, 11))\n",
    "\n",
    "for idx in range(6):\n",
    "    row, col = idx // 3, idx % 3\n",
    "    ax = axes[row, col]\n",
    "    ax.imshow(show_imgs[idx])\n",
    "\n",
    "    h, w = show_imgs[idx].shape[:2]\n",
    "    gen_pos = np.cumsum(show_trajs[idx], axis=0) + end_eff\n",
    "    px = gen_pos[:, 0] * w\n",
    "    py = (1 - gen_pos[:, 1]) * h\n",
    "\n",
    "    # Color gradient along trajectory\n",
    "    points = np.array([px, py]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    norm = plt.Normalize(0, TRAJ_LEN)\n",
    "    lc = LineCollection(segments, cmap='plasma', norm=norm,\n",
    "                        linewidth=3, alpha=0.9)\n",
    "    lc.set_array(np.arange(TRAJ_LEN))\n",
    "    ax.add_collection(lc)\n",
    "\n",
    "    # Start and end markers\n",
    "    ax.plot(px[0], py[0], 'wo', markersize=8, zorder=5)\n",
    "    ax.plot(px[-1], py[-1], 'w*', markersize=12, zorder=5)\n",
    "\n",
    "    ax.set_title(f'\"{showcase[idx][2]}\"', fontsize=11,\n",
    "                 fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('VLA in Action: One Model â€” See, Understand, Act',\n",
    "             fontsize=15, fontweight='bold',\n",
    "             color='#2d3436')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_66"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us animate one trajectory to see the \"robot\" moving step by step toward its target. This is the closest we get to watching our VLA control a real robot."
   ],
   "id": "cell_67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animate one trajectory\n",
    "anim_idx = 0\n",
    "anim_img = show_imgs[anim_idx]\n",
    "anim_traj = show_trajs[anim_idx]\n",
    "anim_pos = np.cumsum(anim_traj, axis=0) + end_eff\n",
    "\n",
    "h, w = anim_img.shape[:2]\n",
    "anim_px = anim_pos[:, 0] * w\n",
    "anim_py = (1 - anim_pos[:, 1]) * h\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.imshow(anim_img)\n",
    "ax.axis('off')\n",
    "ax.set_title(f'\"{showcase[anim_idx][2]}\"', fontsize=12)\n",
    "\n",
    "line, = ax.plot([], [], 'y-', linewidth=2, alpha=0.8)\n",
    "dot, = ax.plot([], [], 'yo', markersize=10, zorder=5)\n",
    "\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    dot.set_data([], [])\n",
    "    return line, dot\n",
    "\n",
    "def animate(frame):\n",
    "    f = min(frame, len(anim_px) - 1)\n",
    "    line.set_data(anim_px[:f+1], anim_py[:f+1])\n",
    "    dot.set_data([anim_px[f]], [anim_py[f]])\n",
    "    return line, dot\n",
    "\n",
    "anim = animation.FuncAnimation(\n",
    "    fig, animate, init_func=init,\n",
    "    frames=TRAJ_LEN + 5, interval=150, blit=True)\n",
    "plt.close(fig)\n",
    "\n",
    "# Display in notebook\n",
    "HTML(anim.to_jshtml())"
   ],
   "id": "cell_68"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congratulations message\n",
    "print(\"=\" * 55)\n",
    "print(\"  CONGRATULATIONS!\")\n",
    "print(\"=\" * 55)\n",
    "print()\n",
    "print(\"  You have built a Vision-Language-Action model\")\n",
    "print(\"  that takes an image + language command and\")\n",
    "print(\"  generates smooth robot action trajectories\")\n",
    "print(\"  using flow matching!\")\n",
    "print()\n",
    "print(\"  What you implemented:\")\n",
    "print(\"  - Vision Encoder (CNN)\")\n",
    "print(\"  - Language Encoder (Embeddings)\")\n",
    "print(\"  - Feature Fusion Module\")\n",
    "print(\"  - Flow Matching Action Decoder\")\n",
    "print(\"  - Training with velocity field prediction\")\n",
    "print(\"  - Inference via ODE integration\")\n",
    "print()\n",
    "print(\"  This is the same architecture (simplified)\")\n",
    "print(\"  behind Physical Intelligence's pi-zero, which\")\n",
    "print(\"  controls real robots from language instructions.\")\n",
    "print()\n",
    "print(\"  You have completed all 6 notebooks in the\")\n",
    "print(\"  World Action Models series!\")\n",
    "print(\"=\" * 55)"
   ],
   "id": "cell_69"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9 â€” Reflection\n",
    "\n",
    "### Three Questions to Think About\n",
    "\n",
    "1. **Scalability:** Our VLA uses a tiny CNN and a simple embedding for language. Real systems like $\\pi_0$ use pre-trained vision transformers (SigLIP) and language models (Gemma). How would scaling up these components change the model's capabilities? What new behaviors might emerge?\n",
    "\n",
    "2. **Multimodality of actions:** In our 2D task, there is essentially one good trajectory to each target. But in the real world, there are often multiple valid ways to perform a task (e.g., reaching around an obstacle from the left or right). How does flow matching handle this multimodality compared to direct regression? (Hint: think about what happens when you average two different valid trajectories.)\n",
    "\n",
    "3. **From simulation to reality:** Our model was trained entirely in a simple 2D simulation. V-JEPA 2 showed that pre-training on internet video can dramatically reduce the amount of real-robot data needed. Why might watching videos of the world (even without any robot in them) help a robot learn to manipulate objects?\n",
    "\n",
    "### Optional Challenges\n",
    "\n",
    "**Challenge 1: Add Obstacle Avoidance**\n",
    "\n",
    "Add a static obstacle (a gray circle) to the scene. Modify the training data generation to produce trajectories that curve around the obstacle. Then test whether the VLA learns to avoid obstacles when generating trajectories. You will need to modify `render_scene` and `generate_trajectory`.\n",
    "\n",
    "**Challenge 2: Three-Link Arm**\n",
    "\n",
    "Extend the robot arm from 2 links to 3 links. This increases the complexity of forward kinematics and makes the reaching task more interesting â€” there are now multiple joint configurations that reach the same target (redundancy). Implement the new kinematics and observe how the model handles this added complexity."
   ],
   "id": "cell_70"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series Recap â€” World Action Models\n",
    "\n",
    "Over the course of six notebooks, we have journeyed from basic world models to the cutting edge of embodied AI:\n",
    "\n",
    "1. **World Models** â€” learned to imagine the future in latent space\n",
    "2. **Latent Dynamics** â€” built models that predict state transitions without pixel reconstruction\n",
    "3. **Planning in Imagination** â€” used world models to plan actions by simulating outcomes\n",
    "4. **Video Prediction** â€” extended world models to predict future video frames\n",
    "5. **Self-Supervised Physical Understanding** â€” learned physics from observation alone (JEPA-style)\n",
    "6. **Vision-Language-Action** â€” the grand convergence: see, understand, act\n",
    "\n",
    "The trajectory of this field is clear: we are moving toward **general-purpose embodied agents** that can understand any instruction, perceive any environment, and act smoothly in the physical world. The models are getting larger, the training data is scaling from simulation to internet-scale video, and the gap between digital intelligence and physical capability is closing rapidly.\n",
    "\n",
    "This is the frontier. And now you have the foundations to understand â€” and contribute to â€” what comes next.\n",
    "\n",
    "See you next time!"
   ],
   "id": "cell_71"
  }
 ]
}