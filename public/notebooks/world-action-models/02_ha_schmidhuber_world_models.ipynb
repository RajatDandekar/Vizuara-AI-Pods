{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Ha & Schmidhuber World Models: Teaching Agents to Dream â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Ha & Schmidhuber World Models: Teaching Agents to Dream\n",
    "\n",
    "**Notebook 2 of 6 â€” World Action Models Series | Vizuara**\n",
    "\n",
    "**Estimated time: ~40 minutes**\n",
    "\n",
    "In this notebook, we will build the landmark **World Models** architecture from Ha & Schmidhuber (2018) â€” a system where an agent learns a compressed model of its environment and then trains entirely inside its own imagination. By the end, you will have a working V-M-C pipeline where an agent learns to act by dreaming."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://course-creator-brown.vercel.app/courses/world-action-models/practice/2/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Setup â€” Install dependencies and configure environment\n",
    "# ============================================================\n",
    "!pip install gymnasium matplotlib numpy -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "What if an agent could **dream**?\n",
    "\n",
    "Think about how you learned to ride a bicycle. Yes, you needed real practice â€” but much of your learning happened *mentally*. Before your next attempt, your brain replayed what went wrong, imagined corrections, and rehearsed new strategies. You were training inside a model of the world that your brain had built.\n",
    "\n",
    "Ha and Schmidhuber asked: **can we give this same ability to artificial agents?**\n",
    "\n",
    "Their 2018 paper introduced a beautifully simple three-component architecture:\n",
    "\n",
    "| Component | Role | Analogy |\n",
    "|-----------|------|---------|\n",
    "| **V** (Vision) | Compress raw observations into compact codes | Your retina compressing millions of photons into a scene |\n",
    "| **M** (Memory) | Predict what happens next | Your brain imagining \"if I lean left, I will fall\" |\n",
    "| **C** (Controller) | Choose actions | Your reflexes â€” fast, simple, reactive |\n",
    "\n",
    "The breakthrough insight: once V and M are trained on real experience, the Controller can be trained **entirely inside M's imagination** â€” no real environment needed. The agent literally learns by dreaming.\n",
    "\n",
    "> **By the end of this notebook, you will build a complete V-M-C pipeline, train each component, and watch an agent learn to balance a pole â€” first in reality, then inside its own dream.**"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition (No Code Yet)\n",
    "\n",
    "Let us build intuition for each component with three analogies before we touch any mathematics.\n",
    "\n",
    "### 2.1 The Vision Model (V) â€” Like Compressing a Photo\n",
    "\n",
    "Imagine you take a high-resolution photograph (10 megapixels) and save it as a JPEG. The JPEG is much smaller, but it still captures the essential content â€” the people, the objects, the scene. You threw away pixel-level noise but kept the *meaning*.\n",
    "\n",
    "The Vision model does exactly this. It takes a raw observation (the full state of the environment) and compresses it into a tiny **latent vector** $z$. This vector is like a JPEG of the observation â€” much smaller, but it captures everything the agent needs to know.\n",
    "\n",
    "### 2.2 The Memory Model (M) â€” Like Predicting the Next Scene in a Movie\n",
    "\n",
    "Now imagine you are watching a movie, and someone pauses it. Can you predict what happens in the next frame? If a ball is flying through the air, you predict it will continue along its arc. If a character is about to open a door, you predict the door will swing open.\n",
    "\n",
    "The Memory model does precisely this. Given the current compressed observation $z_t$ and the action the agent takes $a_t$, it predicts what the *next* compressed observation $z_{t+1}$ will look like. It has learned the dynamics of the world.\n",
    "\n",
    "### 2.3 The Controller (C) â€” Simple Reflexes\n",
    "\n",
    "Here is the surprising part: the Controller is deliberately **tiny**. It is just a single linear layer â€” a matrix multiply and a bias term. Why so simple?\n",
    "\n",
    "Because the hard work has already been done by V and M. The Vision model has compressed the raw observation into a meaningful representation. The Memory model has built up a rich internal state that summarizes the history. The Controller just needs to react to these â€” like how your reflexes are fast and simple, but they draw on all the context your brain has built up.\n",
    "\n",
    "### The Full Loop: See, Remember, Act\n",
    "\n",
    "```\n",
    "Observation â†’ [V] â†’ latent z â†’ [M] â†’ next hidden state h â†’ [C] â†’ action a\n",
    "                                  â†‘                                    |\n",
    "                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "The agent sees the world through V, remembers through M, and acts through C. And when we want to train in dreams? We simply disconnect from the real environment and let M generate imaginary observations."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "Now let us formalize each component. After every equation, we will explain what it means computationally so that the math never feels abstract.\n",
    "\n",
    "### 3.1 The VAE (Vision Model)\n",
    "\n",
    "The Vision model is a **Variational Autoencoder (VAE)**. It has two parts:\n",
    "\n",
    "**Encoder** â€” compresses observation $x_t$ into a latent distribution:\n",
    "\n",
    "$$\\mu_t, \\log \\sigma_t^2 = \\text{Encoder}(x_t)$$\n",
    "\n",
    "Computationally, this says: pass the observation through a neural network that outputs two vectors â€” a mean $\\mu$ and a log-variance $\\log \\sigma^2$. These define a Gaussian distribution in latent space.\n",
    "\n",
    "**Reparameterization Trick** â€” sample from this distribution in a differentiable way:\n",
    "\n",
    "$$z_t = \\mu_t + \\sigma_t \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\n",
    "\n",
    "Computationally, this says: take the mean vector, then add noise scaled by the standard deviation. We sample the noise $\\epsilon$ from a standard normal distribution. This is clever because the randomness is in $\\epsilon$ (which does not depend on any parameters), so gradients can flow through $\\mu$ and $\\sigma$.\n",
    "\n",
    "Let us plug in some numbers. Suppose our latent dimension is 2, and the encoder outputs $\\mu = [0.5, -0.3]$ and $\\sigma = [0.1, 0.2]$. We sample $\\epsilon = [1.0, -0.5]$. Then:\n",
    "\n",
    "$$z = [0.5 + 0.1 \\times 1.0, \\; -0.3 + 0.2 \\times (-0.5)] = [0.6, \\; -0.4]$$\n",
    "\n",
    "This is exactly what we want â€” a sample that is close to the mean but with controlled randomness.\n",
    "\n",
    "**Decoder** â€” reconstructs the observation from $z$:\n",
    "\n",
    "$$\\hat{x}_t = \\text{Decoder}(z_t)$$\n",
    "\n",
    "**VAE Loss** â€” balances reconstruction quality and regularization:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{VAE}} = \\underbrace{\\| x_t - \\hat{x}_t \\|^2}_{\\text{Reconstruction}} + \\underbrace{\\beta \\cdot D_{\\text{KL}}(\\mathcal{N}(\\mu, \\sigma^2) \\| \\mathcal{N}(0, I))}_{\\text{Regularization}}$$\n",
    "\n",
    "The first term says: the reconstruction should look like the original. The second term says: the latent distribution should stay close to a standard normal â€” this prevents the encoder from \"cheating\" by using wildly different regions of latent space for different observations. The $\\beta$ parameter controls the balance."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The MDN-RNN (Memory Model)\n",
    "\n",
    "The Memory model is an **LSTM** combined with a **Mixture Density Network (MDN)**. It predicts the next latent state as a probability distribution.\n",
    "\n",
    "At each time step, it receives the current latent vector $z_t$ and action $a_t$, concatenates them, and feeds them to an LSTM:\n",
    "\n",
    "$$h_{t+1} = \\text{LSTM}([z_t, a_t], h_t)$$\n",
    "\n",
    "Computationally, this says: take the compressed observation and the action, glue them together into one vector, and feed them into an LSTM cell along with the previous hidden state. The LSTM updates its internal memory.\n",
    "\n",
    "The LSTM hidden state $h_{t+1}$ is then used to predict the distribution over the next latent state:\n",
    "\n",
    "$$P(z_{t+1} \\mid a_t, z_t, h_t) = \\sum_{k=1}^{K} \\pi_k \\, \\mathcal{N}(z_{t+1} \\mid \\mu_k, \\sigma_k)$$\n",
    "\n",
    "This is a **mixture of Gaussians** â€” instead of predicting a single point, the model predicts $K$ possible next states with different weights $\\pi_k$. This is important because the future is often uncertain. If the agent is at the top of a hill, it could roll left or right â€” a single Gaussian cannot capture this bimodality, but a mixture can.\n",
    "\n",
    "Let us plug in numbers with $K=2$ Gaussians in a 1D latent space. Suppose the model outputs:\n",
    "- Component 1: $\\pi_1 = 0.7$, $\\mu_1 = 0.3$, $\\sigma_1 = 0.1$\n",
    "- Component 2: $\\pi_2 = 0.3$, $\\mu_2 = -0.5$, $\\sigma_2 = 0.2$\n",
    "\n",
    "The model is saying: \"There is a 70% chance the next latent state will be near 0.3, and a 30% chance it will be near -0.5.\" This captures genuine uncertainty about the future."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 The Controller (C)\n",
    "\n",
    "The Controller is remarkably simple â€” just a linear mapping:\n",
    "\n",
    "$$a_t = W_c \\cdot [z_t, h_t] + b_c$$\n",
    "\n",
    "Computationally, this says: concatenate the current compressed observation $z_t$ and the LSTM hidden state $h_t$, multiply by a weight matrix $W_c$, and add a bias $b_c$.\n",
    "\n",
    "Why so simple? The intelligence lives in V (which creates meaningful representations) and M (which builds up a rich hidden state summarizing history). The Controller just needs to map from this already-rich representation to an action. Ha and Schmidhuber showed that even a linear controller suffices when it sits on top of powerful V and M components.\n",
    "\n",
    "For CartPole, our Controller takes a concatenated vector of size $(\\text{latent\\_dim} + \\text{hidden\\_dim})$ and outputs a single logit for the binary action (left or right)."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Dream Training\n",
    "\n",
    "Here is where the magic happens. Once V and M are trained on real experience, we can train C entirely inside M's imagination:\n",
    "\n",
    "1. Start with a real initial observation $x_0$, encode it to get $z_0 = V(x_0)$\n",
    "2. Initialize the LSTM hidden state $h_0$\n",
    "3. For each dream step $t$:\n",
    "   - Controller picks action: $a_t = C(z_t, h_t)$\n",
    "   - Memory predicts next state: $z_{t+1} \\sim M(z_t, a_t, h_t)$\n",
    "   - Memory updates hidden state: $h_{t+1} = \\text{LSTM}([z_t, a_t], h_t)$\n",
    "   - Estimate reward from the predicted state (we will define a simple reward function)\n",
    "4. Accumulate total dream reward\n",
    "5. Update Controller parameters to maximize dream reward\n",
    "\n",
    "The agent is literally learning by dreaming â€” no interaction with the real environment during Controller training. This is exactly what we want."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let Us Build It\n",
    "\n",
    "Now let us implement each component step by step. We will use CartPole-v1, which provides a 4-dimensional state vector (cart position, cart velocity, pole angle, pole angular velocity). This keeps things tractable for a Colab notebook while preserving all the conceptual richness."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Data Collection â€” Gathering Real Experience\n",
    "\n",
    "First, we need to collect experience from the real environment using a random policy. The agent will act randomly and record what it sees."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(env_name=\"CartPole-v1\", num_episodes=200, seed=SEED):\n",
    "    \"\"\"Collect (observation, action, next_observation) tuples\n",
    "    using a random policy.\"\"\"\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    observations, actions, next_observations, dones = [], [], [], []\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        obs, _ = env.reset(seed=seed + ep)\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.action_space.sample()  # Random action\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            observations.append(obs)\n",
    "            actions.append(action)\n",
    "            next_observations.append(next_obs)\n",
    "            dones.append(done)\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "    env.close()\n",
    "    print(f\"Collected {len(observations)} transitions \"\n",
    "          f\"from {num_episodes} episodes\")\n",
    "    return (np.array(observations), np.array(actions),\n",
    "            np.array(next_observations), np.array(dones))\n",
    "\n",
    "obs_data, act_data, next_obs_data, done_data = collect_data()"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize some of the collected observations to understand what our agent is seeing."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualize collected observations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 6))\n",
    "labels = [\"Cart Position\", \"Cart Velocity\",\n",
    "          \"Pole Angle\", \"Pole Angular Velocity\"]\n",
    "\n",
    "for i, (ax, label) in enumerate(zip(axes.flat, labels)):\n",
    "    ax.plot(obs_data[:200, i], alpha=0.7, linewidth=0.8)\n",
    "    ax.set_title(label, fontsize=12)\n",
    "    ax.set_xlabel(\"Time step\")\n",
    "    ax.set_ylabel(\"Value\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"CartPole Observations (First 200 Steps)\",\n",
    "             fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Observation shape: {obs_data.shape}\")\n",
    "print(f\"Observation range: [{obs_data.min():.2f}, {obs_data.max():.2f}]\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Building the VAE (Vision Model)\n",
    "\n",
    "Now let us build the Vision model. Since CartPole gives us a 4D state vector (not images), we will use simple linear layers. The architecture is:\n",
    "\n",
    "- **Encoder**: 4D observation â†’ hidden layer â†’ ($\\mu$, $\\log \\sigma^2$) of dimension `latent_dim`\n",
    "- **Decoder**: latent $z$ â†’ hidden layer â†’ 4D reconstruction"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"Variational Autoencoder for compressing observations\n",
    "    into a latent space.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim=4, hidden_dim=64, latent_dim=2):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder: observation -> (mu, logvar)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # Decoder: z -> reconstructed observation\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, obs_dim),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode observation to latent distribution parameters.\"\"\"\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Sample z using the reparameterization trick.\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent vector back to observation space.\"\"\"\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar, z\n",
    "\n",
    "vae = VAE(obs_dim=4, hidden_dim=64, latent_dim=2).to(device)\n",
    "print(f\"VAE parameters: {sum(p.numel() for p in vae.parameters()):,}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us define the VAE loss function. Recall from Section 3: it is the sum of reconstruction loss and KL divergence."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_fn(x_recon, x, mu, logvar, beta=1.0):\n",
    "    \"\"\"Compute VAE loss = reconstruction + beta * KL divergence.\n",
    "\n",
    "    The KL divergence for a Gaussian q(z|x) against N(0,I) has\n",
    "    a closed-form solution, which we use here.\n",
    "    \"\"\"\n",
    "    # Reconstruction loss (MSE)\n",
    "    recon_loss = F.mse_loss(x_recon, x, reduction=\"sum\")\n",
    "\n",
    "    # KL divergence: -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return (recon_loss + beta * kl_loss) / x.size(0), recon_loss / x.size(0), kl_loss / x.size(0)"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train the VAE on our collected observations."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(vae, obs_data, epochs=30, batch_size=128,\n",
    "              lr=1e-3, beta=0.5):\n",
    "    \"\"\"Train the VAE on collected observations.\"\"\"\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=lr)\n",
    "    dataset = torch.FloatTensor(obs_data).to(device)\n",
    "\n",
    "    losses = {\"total\": [], \"recon\": [], \"kl\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle data each epoch\n",
    "        perm = torch.randperm(len(dataset))\n",
    "        epoch_loss, epoch_recon, epoch_kl = 0.0, 0.0, 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch = dataset[perm[i:i + batch_size]]\n",
    "            x_recon, mu, logvar, z = vae(batch)\n",
    "            loss, recon, kl = vae_loss_fn(x_recon, batch, mu, logvar, beta)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_recon += recon.item()\n",
    "            epoch_kl += kl.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        losses[\"total\"].append(epoch_loss / n_batches)\n",
    "        losses[\"recon\"].append(epoch_recon / n_batches)\n",
    "        losses[\"kl\"].append(epoch_kl / n_batches)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1:3d} | Loss: {losses['total'][-1]:.4f} \"\n",
    "                  f\"| Recon: {losses['recon'][-1]:.4f} \"\n",
    "                  f\"| KL: {losses['kl'][-1]:.4f}\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "vae_losses = train_vae(vae, obs_data)"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the training curves and the quality of reconstruction."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š VAE Training Curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "axes[0].plot(vae_losses[\"total\"], color=\"navy\")\n",
    "axes[0].set_title(\"Total VAE Loss\", fontsize=12)\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(vae_losses[\"recon\"], color=\"crimson\")\n",
    "axes[1].set_title(\"Reconstruction Loss\", fontsize=12)\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(vae_losses[\"kl\"], color=\"forestgreen\")\n",
    "axes[2].set_title(\"KL Divergence\", fontsize=12)\n",
    "axes[2].set_xlabel(\"Epoch\")\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"VAE Training Progress\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see how well the VAE reconstructs observations. Good reconstruction means the latent space captures the essential information."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Original vs Reconstructed Observations\n",
    "vae.eval()\n",
    "test_obs = torch.FloatTensor(obs_data[:50]).to(device)\n",
    "with torch.no_grad():\n",
    "    recon, mu, logvar, z = vae(test_obs)\n",
    "\n",
    "test_np = test_obs.cpu().numpy()\n",
    "recon_np = recon.cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "labels = [\"Cart Position\", \"Cart Velocity\",\n",
    "          \"Pole Angle\", \"Pole Angular Velocity\"]\n",
    "\n",
    "for i, (ax, label) in enumerate(zip(axes.flat, labels)):\n",
    "    ax.plot(test_np[:, i], \"b-\", label=\"Original\", alpha=0.8)\n",
    "    ax.plot(recon_np[:, i], \"r--\", label=\"Reconstructed\", alpha=0.8)\n",
    "    ax.set_title(label, fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"VAE Reconstruction Quality\",\n",
    "             fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Quantify reconstruction error\n",
    "mse = np.mean((test_np - recon_np) ** 2)\n",
    "print(f\"Mean Squared Reconstruction Error: {mse:.6f}\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, right? The VAE has learned to compress 4D observations into 2D latent vectors while preserving the essential information. Let us also visualize the latent space itself."
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualize the 2D Latent Space\n",
    "vae.eval()\n",
    "all_obs = torch.FloatTensor(obs_data).to(device)\n",
    "with torch.no_grad():\n",
    "    _, mu_all, _, _ = vae(all_obs)\n",
    "\n",
    "mu_np = mu_all.cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(mu_np[:, 0], mu_np[:, 1],\n",
    "                      c=obs_data[:, 2],  # Color by pole angle\n",
    "                      cmap=\"coolwarm\", alpha=0.3, s=5)\n",
    "plt.colorbar(scatter, label=\"Pole Angle\")\n",
    "plt.xlabel(\"Latent Dimension 1\", fontsize=12)\n",
    "plt.ylabel(\"Latent Dimension 2\", fontsize=12)\n",
    "plt.title(\"VAE Latent Space (colored by pole angle)\",\n",
    "          fontsize=14, fontweight=\"bold\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly what we want. The latent space has organized itself so that similar physical states (similar pole angles) cluster together. The VAE has learned a meaningful compression of the environment.\n",
    "\n",
    "### 4.3 Building the MDN-RNN (Memory Model)\n",
    "\n",
    "Now let us build the Memory model. It uses an LSTM cell to maintain a running summary of the agent's history, and a Mixture Density Network head to predict the distribution over the next latent state."
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDNRNN(nn.Module):\n",
    "    \"\"\"MDN-RNN World Model: LSTM + Mixture Density Network.\n",
    "\n",
    "    Takes (z_t, a_t) as input, maintains hidden state h_t,\n",
    "    and predicts P(z_{t+1}) as a mixture of Gaussians.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=2, action_dim=1,\n",
    "                 hidden_dim=64, num_gaussians=3):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_gaussians = num_gaussians\n",
    "\n",
    "        # LSTM cell: processes (z_t, a_t) sequentially\n",
    "        input_dim = latent_dim + action_dim\n",
    "        self.lstm = nn.LSTMCell(input_dim, hidden_dim)\n",
    "\n",
    "        # MDN head: predicts mixture parameters from h\n",
    "        # For each Gaussian: pi (weight), mu (mean), sigma (std)\n",
    "        n_params = num_gaussians * (1 + latent_dim + latent_dim)\n",
    "        self.mdn_head = nn.Linear(hidden_dim, n_params)\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        \"\"\"Initialize LSTM hidden and cell states to zeros.\"\"\"\n",
    "        h = torch.zeros(batch_size, self.hidden_dim).to(device)\n",
    "        c = torch.zeros(batch_size, self.hidden_dim).to(device)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, z, action, hidden):\n",
    "        \"\"\"One step of the world model.\n",
    "\n",
    "        Args:\n",
    "            z: latent vector [batch, latent_dim]\n",
    "            action: action taken [batch, 1]\n",
    "            hidden: tuple of (h, c) for LSTM\n",
    "        Returns:\n",
    "            pi, mu, sigma: MDN parameters\n",
    "            hidden: updated (h, c)\n",
    "        \"\"\"\n",
    "        # Concatenate z and action as input to LSTM\n",
    "        inp = torch.cat([z, action.float().unsqueeze(-1)\n",
    "                         if action.dim() == 1 else action.float()],\n",
    "                        dim=-1)\n",
    "        h, c = self.lstm(inp, hidden)\n",
    "\n",
    "        # Get MDN parameters from hidden state\n",
    "        mdn_params = self.mdn_head(h)\n",
    "        pi, mu, sigma = self._parse_mdn_params(mdn_params)\n",
    "\n",
    "        return pi, mu, sigma, (h, c)\n",
    "\n",
    "    def _parse_mdn_params(self, params):\n",
    "        \"\"\"Parse raw network output into mixture parameters.\"\"\"\n",
    "        K = self.num_gaussians\n",
    "        D = self.latent_dim\n",
    "\n",
    "        # Split into pi, mu, sigma\n",
    "        pi_raw = params[:, :K]\n",
    "        mu = params[:, K:K + K * D].view(-1, K, D)\n",
    "        sigma_raw = params[:, K + K * D:].view(-1, K, D)\n",
    "\n",
    "        # Apply activations\n",
    "        pi = F.softmax(pi_raw, dim=-1)        # Weights sum to 1\n",
    "        sigma = torch.exp(sigma_raw) + 1e-6   # Positive std dev\n",
    "\n",
    "        return pi, mu, sigma\n",
    "\n",
    "mdnrnn = MDNRNN(latent_dim=2, action_dim=1,\n",
    "                 hidden_dim=64, num_gaussians=3).to(device)\n",
    "print(f\"MDN-RNN parameters: {sum(p.numel() for p in mdnrnn.parameters()):,}\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a loss function for the MDN-RNN. The loss is the negative log-likelihood of the actual next latent state under the predicted mixture of Gaussians."
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_loss_fn(pi, mu, sigma, target):\n",
    "    \"\"\"Negative log-likelihood of target under the mixture.\n",
    "\n",
    "    Args:\n",
    "        pi: mixture weights [batch, K]\n",
    "        mu: means [batch, K, D]\n",
    "        sigma: std devs [batch, K, D]\n",
    "        target: actual next z [batch, D]\n",
    "    \"\"\"\n",
    "    target = target.unsqueeze(1)  # [batch, 1, D]\n",
    "\n",
    "    # Log probability of target under each Gaussian component\n",
    "    # log N(x | mu, sigma) = -0.5 * ((x-mu)/sigma)^2 - log(sigma) - 0.5*log(2*pi)\n",
    "    var = sigma ** 2\n",
    "    log_prob = (-0.5 * ((target - mu) ** 2 / var)\n",
    "                - torch.log(sigma)\n",
    "                - 0.5 * np.log(2 * np.pi))\n",
    "    log_prob = log_prob.sum(dim=-1)  # Sum over latent dims [batch, K]\n",
    "\n",
    "    # Weighted by mixture coefficients (log-sum-exp for stability)\n",
    "    log_pi = torch.log(pi + 1e-8)\n",
    "    log_mixture = torch.logsumexp(log_pi + log_prob, dim=-1)  # [batch]\n",
    "\n",
    "    return -log_mixture.mean()"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us prepare the sequential training data and train the MDN-RNN."
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(obs_data, act_data, next_obs_data,\n",
    "                      done_data, vae, seq_len=16):\n",
    "    \"\"\"Encode observations to latent space and create sequences\n",
    "    for MDN-RNN training.\"\"\"\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        obs_t = torch.FloatTensor(obs_data).to(device)\n",
    "        next_obs_t = torch.FloatTensor(next_obs_data).to(device)\n",
    "        mu_z, _ = vae.encode(obs_t)\n",
    "        mu_z_next, _ = vae.encode(next_obs_t)\n",
    "\n",
    "    z_data = mu_z.cpu().numpy()\n",
    "    z_next_data = mu_z_next.cpu().numpy()\n",
    "\n",
    "    # Build sequences, respecting episode boundaries\n",
    "    sequences = []\n",
    "    start = 0\n",
    "    for i in range(len(done_data)):\n",
    "        if done_data[i] or i == len(done_data) - 1:\n",
    "            ep_len = i - start + 1\n",
    "            if ep_len >= seq_len:\n",
    "                for j in range(start, i + 1 - seq_len):\n",
    "                    seq_z = z_data[j:j + seq_len]\n",
    "                    seq_a = act_data[j:j + seq_len]\n",
    "                    seq_z_next = z_next_data[j:j + seq_len]\n",
    "                    sequences.append((seq_z, seq_a, seq_z_next))\n",
    "            start = i + 1\n",
    "\n",
    "    print(f\"Created {len(sequences)} training sequences \"\n",
    "          f\"of length {seq_len}\")\n",
    "    return sequences\n",
    "\n",
    "sequences = prepare_sequences(\n",
    "    obs_data, act_data, next_obs_data, done_data, vae\n",
    ")"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mdnrnn(mdnrnn, sequences, epochs=20,\n",
    "                 batch_size=64, lr=1e-3):\n",
    "    \"\"\"Train MDN-RNN on sequential latent data.\"\"\"\n",
    "    optimizer = optim.Adam(mdnrnn.parameters(), lr=lr)\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        np.random.shuffle(sequences)\n",
    "        epoch_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for i in range(0, len(sequences), batch_size):\n",
    "            batch_seqs = sequences[i:i + batch_size]\n",
    "            bs = len(batch_seqs)\n",
    "\n",
    "            # Stack batch\n",
    "            z_batch = torch.FloatTensor(\n",
    "                np.array([s[0] for s in batch_seqs])).to(device)\n",
    "            a_batch = torch.FloatTensor(\n",
    "                np.array([s[1] for s in batch_seqs])).to(device)\n",
    "            z_next_batch = torch.FloatTensor(\n",
    "                np.array([s[2] for s in batch_seqs])).to(device)\n",
    "\n",
    "            # Process sequence step by step\n",
    "            hidden = mdnrnn.init_hidden(bs)\n",
    "            seq_loss = 0.0\n",
    "            seq_len = z_batch.shape[1]\n",
    "\n",
    "            for t in range(seq_len):\n",
    "                pi, mu, sigma, hidden = mdnrnn(\n",
    "                    z_batch[:, t], a_batch[:, t], hidden)\n",
    "                step_loss = mdn_loss_fn(\n",
    "                    pi, mu, sigma, z_next_batch[:, t])\n",
    "                seq_loss += step_loss\n",
    "\n",
    "            loss = seq_loss / seq_len\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(mdnrnn.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = epoch_loss / max(n_batches, 1)\n",
    "        losses.append(avg_loss)\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1:3d} | MDN-RNN Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "mdnrnn_losses = train_mdnrnn(mdnrnn, sequences)"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š MDN-RNN Training Curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(mdnrnn_losses, color=\"darkorange\", linewidth=2)\n",
    "plt.title(\"MDN-RNN Training Loss\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Negative Log-Likelihood\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify that the Memory model can actually predict future latent states. We will take a real trajectory, feed it through the model step by step, and compare predicted vs actual latent states."
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Predicted vs Actual Latent Trajectories\n",
    "mdnrnn.eval()\n",
    "vae.eval()\n",
    "\n",
    "# Pick a sequence from training data\n",
    "test_seq = sequences[0]\n",
    "z_seq = torch.FloatTensor(test_seq[0]).unsqueeze(0).to(device)\n",
    "a_seq = torch.FloatTensor(test_seq[1]).unsqueeze(0).to(device)\n",
    "z_next_seq = torch.FloatTensor(test_seq[2]).unsqueeze(0).to(device)\n",
    "\n",
    "predicted_z = []\n",
    "hidden = mdnrnn.init_hidden(1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for t in range(z_seq.shape[1]):\n",
    "        pi, mu, sigma, hidden = mdnrnn(\n",
    "            z_seq[:, t], a_seq[:, t], hidden)\n",
    "        # Use the highest-weight Gaussian's mean as prediction\n",
    "        best_k = pi.argmax(dim=-1)\n",
    "        pred = mu[0, best_k[0]].cpu().numpy()\n",
    "        predicted_z.append(pred)\n",
    "\n",
    "predicted_z = np.array(predicted_z)\n",
    "actual_z = z_next_seq[0].cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "for d in range(2):\n",
    "    axes[d].plot(actual_z[:, d], \"b-o\", label=\"Actual\", markersize=4)\n",
    "    axes[d].plot(predicted_z[:, d], \"r--x\",\n",
    "                 label=\"Predicted\", markersize=4)\n",
    "    axes[d].set_title(f\"Latent Dimension {d+1}\", fontsize=12)\n",
    "    axes[d].legend()\n",
    "    axes[d].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"MDN-RNN: Predicted vs Actual Latent Trajectories\",\n",
    "             fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Memory model has learned the dynamics of the latent space. It can predict where the agent's state will go next, given its current state and action. This is exactly what we need for dream training.\n",
    "\n",
    "### 4.4 Building the Controller\n",
    "\n",
    "The Controller is the simplest component â€” a single linear layer that maps from the combined representation $[z_t, h_t]$ to an action."
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "    \"\"\"Simple linear controller: maps [z, h] to action logits.\n",
    "\n",
    "    This is intentionally minimal â€” the intelligence lives\n",
    "    in the Vision and Memory models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=2, hidden_dim=64, action_dim=2):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(latent_dim + hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, z, h):\n",
    "        \"\"\"Select action from latent state and memory.\n",
    "\n",
    "        Args:\n",
    "            z: current latent observation [batch, latent_dim]\n",
    "            h: LSTM hidden state [batch, hidden_dim]\n",
    "        Returns:\n",
    "            action_logits: [batch, action_dim]\n",
    "        \"\"\"\n",
    "        combined = torch.cat([z, h], dim=-1)\n",
    "        return self.fc(combined)\n",
    "\n",
    "controller = Controller(latent_dim=2, hidden_dim=64,\n",
    "                        action_dim=2).to(device)\n",
    "print(f\"Controller parameters: \"\n",
    "      f\"{sum(p.numel() for p in controller.parameters()):,}\")\n",
    "print(f\"\\nTotal system parameters: \"\n",
    "      f\"{sum(p.numel() for p in vae.parameters()) + sum(p.numel() for p in mdnrnn.parameters()) + sum(p.numel() for p in controller.parameters()):,}\")"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how few parameters the Controller has compared to V and M. This is by design â€” the Controller is a thin decision layer on top of rich representations.\n",
    "\n",
    "### 4.5 Dream Training Loop\n",
    "\n",
    "Now we arrive at the most exciting part: training the Controller inside the Memory model's imagination. The agent will never touch the real environment during this phase."
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dream_reward(z, latent_dim=2):\n",
    "    \"\"\"Estimate reward from latent state.\n",
    "\n",
    "    Since we cannot access the real reward function inside\n",
    "    a dream, we learn a simple proxy. For CartPole, we\n",
    "    decode z back to observation space and check if the\n",
    "    pole angle is small (the pole is balanced).\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        obs_recon = vae.decode(z)\n",
    "    # CartPole reward: +1 if pole angle (dim 2) is small\n",
    "    # and cart position (dim 0) is near center\n",
    "    pole_angle = obs_recon[:, 2].abs()\n",
    "    cart_pos = obs_recon[:, 0].abs()\n",
    "    reward = (1.0 - pole_angle) * (1.0 - 0.1 * cart_pos)\n",
    "    return reward.clamp(min=0.0)"
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dream_rollout(controller, mdnrnn, vae, initial_obs,\n",
    "                  dream_steps=50, temperature=1.0):\n",
    "    \"\"\"Roll out a trajectory inside the world model's dream.\n",
    "\n",
    "    Args:\n",
    "        initial_obs: real observation to start the dream [1, obs_dim]\n",
    "        dream_steps: how many steps to dream forward\n",
    "        temperature: controls stochasticity of MDN sampling\n",
    "\n",
    "    Returns:\n",
    "        total_reward: accumulated reward in the dream\n",
    "        log_probs: log probabilities of chosen actions (for REINFORCE)\n",
    "    \"\"\"\n",
    "    # Encode initial observation\n",
    "    with torch.no_grad():\n",
    "        mu, logvar = vae.encode(initial_obs)\n",
    "        z = mu  # Use mean for initial state (no sampling noise)\n",
    "\n",
    "    hidden = mdnrnn.init_hidden(1)\n",
    "    total_reward = 0.0\n",
    "    log_probs = []\n",
    "\n",
    "    for t in range(dream_steps):\n",
    "        # Controller selects action\n",
    "        action_logits = controller(z, hidden[0])\n",
    "        action_dist = torch.distributions.Categorical(\n",
    "            logits=action_logits)\n",
    "        action = action_dist.sample()\n",
    "        log_probs.append(action_dist.log_prob(action))\n",
    "\n",
    "        # Memory predicts next state\n",
    "        with torch.no_grad():\n",
    "            pi, mu_mdn, sigma_mdn, hidden = mdnrnn(\n",
    "                z, action.float().unsqueeze(-1), hidden)\n",
    "\n",
    "        # Sample from the predicted distribution\n",
    "        # Pick a Gaussian component\n",
    "        k = torch.multinomial(pi, 1).squeeze(-1)\n",
    "        chosen_mu = mu_mdn[0, k[0]]\n",
    "        chosen_sigma = sigma_mdn[0, k[0]] * temperature\n",
    "        z = (chosen_mu\n",
    "             + chosen_sigma * torch.randn_like(chosen_mu))\n",
    "        z = z.unsqueeze(0)  # [1, latent_dim]\n",
    "\n",
    "        # Estimate reward\n",
    "        r = dream_reward(z)\n",
    "        total_reward += r.item()\n",
    "\n",
    "    return total_reward, log_probs"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us train the Controller using the REINFORCE algorithm, but entirely inside dreams."
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_controller_in_dreams(controller, mdnrnn, vae,\n",
    "                               real_obs_data, epochs=100,\n",
    "                               dreams_per_epoch=8,\n",
    "                               dream_steps=50, lr=1e-3):\n",
    "    \"\"\"Train the Controller using REINFORCE, entirely in dreams.\n",
    "\n",
    "    Each epoch: start multiple dreams from different real\n",
    "    observations, accumulate policy gradient, update Controller.\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(controller.parameters(), lr=lr)\n",
    "    reward_history = []\n",
    "\n",
    "    mdnrnn.eval()\n",
    "    vae.eval()\n",
    "    controller.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_rewards = []\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for _ in range(dreams_per_epoch):\n",
    "            # Pick a random real observation to start the dream\n",
    "            idx = np.random.randint(len(real_obs_data))\n",
    "            init_obs = torch.FloatTensor(\n",
    "                real_obs_data[idx:idx+1]).to(device)\n",
    "\n",
    "            total_reward, log_probs = dream_rollout(\n",
    "                controller, mdnrnn, vae, init_obs, dream_steps)\n",
    "            epoch_rewards.append(total_reward)\n",
    "\n",
    "            # REINFORCE loss: -log_prob * reward\n",
    "            dream_loss = 0\n",
    "            for lp in log_probs:\n",
    "                dream_loss -= lp * total_reward\n",
    "            dream_loss = dream_loss / dreams_per_epoch\n",
    "            dream_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_reward = np.mean(epoch_rewards)\n",
    "        reward_history.append(avg_reward)\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1:3d} | \"\n",
    "                  f\"Avg Dream Reward: {avg_reward:.2f}\")\n",
    "\n",
    "    return reward_history\n",
    "\n",
    "print(\"Training Controller in dreams...\")\n",
    "print(\"(No real environment interaction!)\\n\")\n",
    "dream_rewards = train_controller_in_dreams(\n",
    "    controller, mdnrnn, vae, obs_data,\n",
    "    epochs=100, dreams_per_epoch=8, dream_steps=50\n",
    ")"
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Dream Training Reward Curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(dream_rewards, alpha=0.3, color=\"mediumpurple\", linewidth=0.8)\n",
    "\n",
    "# Smoothed curve\n",
    "window = 10\n",
    "smoothed = np.convolve(dream_rewards,\n",
    "                       np.ones(window)/window, mode=\"valid\")\n",
    "plt.plot(range(window-1, len(dream_rewards)), smoothed,\n",
    "         color=\"indigo\", linewidth=2, label=\"Smoothed (10-epoch)\")\n",
    "\n",
    "plt.title(\"Controller Training â€” Learning in Dreams\",\n",
    "          fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Dream Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ðŸ”§ Your Turn (TODO)\n",
    "\n",
    "Now it is your turn to implement two key functions. These are the core mechanisms that make World Models work.\n",
    "\n",
    "### TODO 1: Implement the Reparameterization Trick\n",
    "\n",
    "The reparameterization trick is what makes the VAE trainable. Given the mean $\\mu$ and log-variance $\\log \\sigma^2$, you need to sample a latent vector $z$ such that gradients can flow through $\\mu$ and $\\log \\sigma^2$.\n",
    "\n",
    "Recall the formula: $z = \\mu + \\sigma \\cdot \\epsilon$, where $\\sigma = e^{0.5 \\cdot \\log \\sigma^2}$ and $\\epsilon \\sim \\mathcal{N}(0, I)$."
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize_todo(mu, logvar):\n",
    "    \"\"\"Sample z using the reparameterization trick.\n",
    "\n",
    "    Args:\n",
    "        mu: mean of the latent distribution [batch, latent_dim]\n",
    "        logvar: log variance [batch, latent_dim]\n",
    "\n",
    "    Returns:\n",
    "        z: sampled latent vector [batch, latent_dim]\n",
    "\n",
    "    Steps:\n",
    "        1. Compute std from logvar: std = exp(0.5 * logvar)\n",
    "        2. Sample epsilon from N(0, I) with same shape as std\n",
    "        3. Return mu + std * epsilon\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Implement the three steps described above.\n",
    "    # Hint: use torch.exp() and torch.randn_like()\n",
    "    # ==============================\n",
    "\n",
    "    z = None  # YOUR CODE HERE\n",
    "\n",
    "    return z"
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Verification: Run this cell to check your implementation\n",
    "torch.manual_seed(42)\n",
    "test_mu = torch.tensor([[0.5, -0.3]])\n",
    "test_logvar = torch.tensor([[-2.0, -1.0]])\n",
    "\n",
    "torch.manual_seed(42)  # Reset seed for reproducible epsilon\n",
    "result = reparameterize_todo(test_mu, test_logvar)\n",
    "\n",
    "# Expected: mu + exp(0.5*logvar) * epsilon\n",
    "# std = exp([-1.0, -0.5]) = [0.3679, 0.6065]\n",
    "# epsilon (seed=42) = [0.3367, 0.1288]\n",
    "# z = [0.5 + 0.3679*0.3367, -0.3 + 0.6065*0.1288]\n",
    "#   = [0.6239, -0.2219]\n",
    "expected = torch.tensor([[0.6239, -0.2219]])\n",
    "\n",
    "if result is None:\n",
    "    print(\"âŒ You haven't implemented the function yet. \"\n",
    "          \"Replace 'z = None' with your code.\")\n",
    "elif torch.allclose(result, expected, atol=1e-3):\n",
    "    print(\"âœ… Correct! Your reparameterization trick works perfectly.\")\n",
    "    print(f\"   Result: {result.squeeze().tolist()}\")\n",
    "    print(f\"   Expected: {expected.squeeze().tolist()}\")\n",
    "else:\n",
    "    print(f\"âŒ Not quite. Got {result.squeeze().tolist()}, \"\n",
    "          f\"expected {expected.squeeze().tolist()}\")\n",
    "    print(\"   Hint: std = exp(0.5 * logvar), not exp(logvar)\")"
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement the Dream Rollout\n",
    "\n",
    "This is the heart of World Models â€” rolling forward through the Memory model to generate an imaginary trajectory and accumulate reward."
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dream_rollout_todo(controller, mdnrnn, vae,\n",
    "                       initial_z, initial_hidden,\n",
    "                       num_steps=20):\n",
    "    \"\"\"Roll forward through the world model's imagination.\n",
    "\n",
    "    Args:\n",
    "        controller: the Controller network\n",
    "        mdnrnn: the Memory (MDN-RNN) network\n",
    "        vae: the Vision (VAE) network (for reward estimation)\n",
    "        initial_z: starting latent state [1, latent_dim]\n",
    "        initial_hidden: tuple of (h, c) for LSTM\n",
    "        num_steps: how many dream steps to take\n",
    "\n",
    "    Returns:\n",
    "        total_reward: float, accumulated reward over the dream\n",
    "        z_trajectory: list of latent states visited\n",
    "\n",
    "    For each step:\n",
    "        1. Use controller to get action logits from (z, h)\n",
    "        2. Sample action from Categorical distribution\n",
    "        3. Run mdnrnn forward with (z, action, hidden)\n",
    "           to get (pi, mu, sigma, new_hidden)\n",
    "        4. Sample next z from the highest-weight Gaussian\n",
    "           (use the mean of the component with largest pi)\n",
    "        5. Compute reward using dream_reward(z)\n",
    "        6. Accumulate reward and store z in trajectory\n",
    "    \"\"\"\n",
    "    z = initial_z\n",
    "    hidden = initial_hidden\n",
    "    total_reward = 0.0\n",
    "    z_trajectory = [z.detach().cpu().numpy().squeeze()]\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Implement the dream rollout loop described above.\n",
    "    # Remember:\n",
    "    #   - action_logits = controller(z, hidden[0])\n",
    "    #   - Use torch.distributions.Categorical for sampling\n",
    "    #   - For MDN sampling, use pi.argmax to pick best component\n",
    "    #   - Use torch.no_grad() for mdnrnn forward pass\n",
    "    #   - Accumulate reward as a float with .item()\n",
    "    # ==============================\n",
    "\n",
    "    pass  # YOUR CODE HERE\n",
    "\n",
    "    return total_reward, z_trajectory"
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Verification: Run this cell to check your implementation\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a test initial state\n",
    "test_init_obs = torch.FloatTensor(obs_data[0:1]).to(device)\n",
    "with torch.no_grad():\n",
    "    test_mu, _ = vae.encode(test_init_obs)\n",
    "test_hidden = mdnrnn.init_hidden(1)\n",
    "\n",
    "controller.eval()\n",
    "mdnrnn.eval()\n",
    "\n",
    "result = dream_rollout_todo(\n",
    "    controller, mdnrnn, vae,\n",
    "    test_mu, test_hidden, num_steps=20\n",
    ")\n",
    "\n",
    "if result is None or result[0] == 0.0 and len(result[1]) == 1:\n",
    "    print(\"âŒ You haven't implemented the dream rollout yet. \"\n",
    "          \"Replace 'pass' with your loop code.\")\n",
    "else:\n",
    "    total_r, z_traj = result\n",
    "    print(f\"âœ… Dream rollout completed!\")\n",
    "    print(f\"   Total reward over 20 steps: {total_r:.2f}\")\n",
    "    print(f\"   Trajectory length: {len(z_traj)} states\")\n",
    "    if len(z_traj) == 21:  # initial + 20 steps\n",
    "        print(\"   âœ… Correct trajectory length!\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ Expected 21 states (initial + 20 steps), \"\n",
    "              f\"got {len(z_traj)}\")\n",
    "\n",
    "    # Plot the dream trajectory\n",
    "    z_traj_np = np.array(z_traj)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(z_traj_np[:, 0], z_traj_np[:, 1], \"o-\",\n",
    "             color=\"mediumpurple\", markersize=5, alpha=0.8)\n",
    "    plt.plot(z_traj_np[0, 0], z_traj_np[0, 1], \"g*\",\n",
    "             markersize=15, label=\"Start\")\n",
    "    plt.plot(z_traj_np[-1, 0], z_traj_np[-1, 1], \"r*\",\n",
    "             markersize=15, label=\"End\")\n",
    "    plt.title(\"Dream Trajectory in Latent Space\",\n",
    "              fontsize=14, fontweight=\"bold\")\n",
    "    plt.xlabel(\"Latent Dim 1\")\n",
    "    plt.ylabel(\"Latent Dim 2\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together â€” The Full V-M-C Pipeline\n",
    "\n",
    "Now let us connect all three components into a single coherent system that can both interact with the real environment and dream."
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldModelAgent:\n",
    "    \"\"\"Complete V-M-C World Model agent.\n",
    "\n",
    "    Combines Vision (VAE), Memory (MDN-RNN), and Controller\n",
    "    into a unified system that can:\n",
    "    1. Act in the real environment\n",
    "    2. Dream (roll forward in imagination)\n",
    "    3. Train the Controller via dream experience\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vae, mdnrnn, controller):\n",
    "        self.vae = vae\n",
    "        self.mdnrnn = mdnrnn\n",
    "        self.controller = controller\n",
    "        self.hidden = None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the memory state for a new episode.\"\"\"\n",
    "        self.hidden = self.mdnrnn.init_hidden(1)\n",
    "\n",
    "    def act(self, observation):\n",
    "        \"\"\"Choose an action given a real observation.\n",
    "\n",
    "        Full pipeline: observation â†’ V â†’ z â†’ [z,h] â†’ C â†’ action\n",
    "        \"\"\"\n",
    "        self.vae.eval()\n",
    "        self.controller.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            obs_t = torch.FloatTensor(observation).unsqueeze(0).to(device)\n",
    "            mu, _ = self.vae.encode(obs_t)\n",
    "            z = mu  # Use mean (no sampling noise at test time)\n",
    "\n",
    "            action_logits = self.controller(z, self.hidden[0])\n",
    "            action = action_logits.argmax(dim=-1).item()\n",
    "\n",
    "            # Update memory with this step\n",
    "            action_t = torch.tensor([[action]]).float().to(device)\n",
    "            _, _, _, self.hidden = self.mdnrnn(\n",
    "                z, action_t, self.hidden)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def evaluate_real(self, env_name=\"CartPole-v1\",\n",
    "                      num_episodes=10, max_steps=500):\n",
    "        \"\"\"Evaluate the agent in the real environment.\"\"\"\n",
    "        env = gym.make(env_name)\n",
    "        episode_rewards = []\n",
    "\n",
    "        for ep in range(num_episodes):\n",
    "            obs, _ = env.reset(seed=SEED + ep + 1000)\n",
    "            self.reset()\n",
    "            total_reward = 0\n",
    "\n",
    "            for step in range(max_steps):\n",
    "                action = self.act(obs)\n",
    "                obs, reward, terminated, truncated, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "\n",
    "            episode_rewards.append(total_reward)\n",
    "\n",
    "        env.close()\n",
    "        return episode_rewards\n",
    "\n",
    "agent = WorldModelAgent(vae, mdnrnn, controller)"
   ],
   "id": "cell_51"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "Let us evaluate our dream-trained agent in the **real** environment. Remember â€” the Controller was trained entirely inside the Memory model's imagination. Now we test whether those dream skills transfer to reality."
   ],
   "id": "cell_52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate dream-trained agent in reality\n",
    "print(\"Evaluating dream-trained agent in real CartPole...\")\n",
    "real_rewards = agent.evaluate_real(num_episodes=20)\n",
    "\n",
    "print(f\"\\nResults over 20 episodes:\")\n",
    "print(f\"  Mean reward:   {np.mean(real_rewards):.1f}\")\n",
    "print(f\"  Std reward:    {np.std(real_rewards):.1f}\")\n",
    "print(f\"  Max reward:    {np.max(real_rewards):.1f}\")\n",
    "print(f\"  Min reward:    {np.min(real_rewards):.1f}\")"
   ],
   "id": "cell_53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with a random agent baseline\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "random_rewards = []\n",
    "for ep in range(20):\n",
    "    obs, _ = env.reset(seed=SEED + ep + 2000)\n",
    "    total = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        obs, r, term, trunc, _ = env.step(env.action_space.sample())\n",
    "        total += r\n",
    "        done = term or trunc\n",
    "    random_rewards.append(total)\n",
    "env.close()\n",
    "\n",
    "print(f\"Random agent mean reward: {np.mean(random_rewards):.1f}\")\n",
    "print(f\"Dream-trained agent mean reward: {np.mean(real_rewards):.1f}\")"
   ],
   "id": "cell_54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Performance Comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart comparison\n",
    "axes[0].bar([\"Random Agent\", \"Dream-Trained Agent\"],\n",
    "            [np.mean(random_rewards), np.mean(real_rewards)],\n",
    "            color=[\"salmon\", \"mediumpurple\"],\n",
    "            edgecolor=\"black\", linewidth=0.5)\n",
    "axes[0].errorbar(\n",
    "    [0, 1],\n",
    "    [np.mean(random_rewards), np.mean(real_rewards)],\n",
    "    yerr=[np.std(random_rewards), np.std(real_rewards)],\n",
    "    fmt=\"none\", color=\"black\", capsize=5\n",
    ")\n",
    "axes[0].set_ylabel(\"Average Reward\", fontsize=12)\n",
    "axes[0].set_title(\"Random vs Dream-Trained Agent\",\n",
    "                   fontsize=14, fontweight=\"bold\")\n",
    "axes[0].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Episode-by-episode comparison\n",
    "axes[1].plot(random_rewards, \"o-\", color=\"salmon\",\n",
    "             label=\"Random\", alpha=0.7)\n",
    "axes[1].plot(real_rewards, \"s-\", color=\"mediumpurple\",\n",
    "             label=\"Dream-Trained\", alpha=0.7)\n",
    "axes[1].set_xlabel(\"Episode\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Reward\", fontsize=12)\n",
    "axes[1].set_title(\"Per-Episode Rewards\",\n",
    "                   fontsize=14, fontweight=\"bold\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_55"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ðŸŽ¯ Final Output â€” Real Environment vs. Agent's Dream\n",
    "\n",
    "This is the culmination of our notebook. Let us compare what actually happens in the real environment with what the agent *imagines* will happen. We will start from the same initial state and roll forward â€” once in reality, once in the dream."
   ],
   "id": "cell_56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one real episode and one dream episode from the same start\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs, _ = env.reset(seed=123)\n",
    "init_obs = obs.copy()\n",
    "\n",
    "# --- Real trajectory ---\n",
    "agent.reset()\n",
    "real_trajectory = [obs.copy()]\n",
    "real_actions = []\n",
    "done = False\n",
    "for step in range(100):\n",
    "    action = agent.act(obs)\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    real_trajectory.append(obs.copy())\n",
    "    real_actions.append(action)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "env.close()\n",
    "real_trajectory = np.array(real_trajectory)"
   ],
   "id": "cell_57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dream trajectory ---\n",
    "vae.eval()\n",
    "mdnrnn.eval()\n",
    "controller.eval()\n",
    "\n",
    "init_obs_t = torch.FloatTensor(init_obs).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    z, _ = vae.encode(init_obs_t)\n",
    "\n",
    "hidden = mdnrnn.init_hidden(1)\n",
    "dream_z_list = [z.cpu().numpy().squeeze()]\n",
    "dream_obs_list = [init_obs.copy()]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step in range(min(len(real_actions), 100)):\n",
    "        # Controller picks action\n",
    "        logits = controller(z, hidden[0])\n",
    "        action = logits.argmax(dim=-1)\n",
    "\n",
    "        # Memory predicts next state\n",
    "        pi, mu_m, sigma_m, hidden = mdnrnn(\n",
    "            z, action.float().unsqueeze(-1), hidden)\n",
    "        best_k = pi.argmax(dim=-1)\n",
    "        z = mu_m[0, best_k[0]].unsqueeze(0)\n",
    "\n",
    "        # Decode to observation space for visualization\n",
    "        obs_recon = vae.decode(z)\n",
    "\n",
    "        dream_z_list.append(z.cpu().numpy().squeeze())\n",
    "        dream_obs_list.append(obs_recon.cpu().numpy().squeeze())\n",
    "\n",
    "dream_obs_array = np.array(dream_obs_list)"
   ],
   "id": "cell_58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Side-by-Side: Real vs Dream Trajectories\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "labels = [\"Cart Position\", \"Cart Velocity\",\n",
    "          \"Pole Angle\", \"Pole Angular Velocity\"]\n",
    "\n",
    "n_steps = min(len(real_trajectory), len(dream_obs_array))\n",
    "\n",
    "for i, (ax, label) in enumerate(zip(axes.flat, labels)):\n",
    "    ax.plot(real_trajectory[:n_steps, i], \"b-\",\n",
    "            label=\"Real Environment\", linewidth=2, alpha=0.8)\n",
    "    ax.plot(dream_obs_array[:n_steps, i], \"r--\",\n",
    "            label=\"Agent's Dream\", linewidth=2, alpha=0.8)\n",
    "    ax.set_title(label, fontsize=13)\n",
    "    ax.set_xlabel(\"Time Step\")\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"ðŸŽ¯ Real Environment vs. Agent's Dream\",\n",
    "             fontsize=16, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Real trajectory length:  {len(real_trajectory)} steps\")\n",
    "print(f\"Dream trajectory length: {len(dream_obs_array)} steps\")"
   ],
   "id": "cell_59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Dream trajectory in latent space\n",
    "dream_z_np = np.array(dream_z_list)\n",
    "\n",
    "# Also encode the real trajectory for comparison\n",
    "with torch.no_grad():\n",
    "    real_obs_t = torch.FloatTensor(real_trajectory).to(device)\n",
    "    real_z, _ = vae.encode(real_obs_t)\n",
    "    real_z_np = real_z.cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Real trajectory in latent space\n",
    "axes[0].plot(real_z_np[:, 0], real_z_np[:, 1], \"b-o\",\n",
    "             markersize=3, alpha=0.6, linewidth=1)\n",
    "axes[0].plot(real_z_np[0, 0], real_z_np[0, 1], \"g*\",\n",
    "             markersize=15, zorder=5, label=\"Start\")\n",
    "axes[0].plot(real_z_np[-1, 0], real_z_np[-1, 1], \"r*\",\n",
    "             markersize=15, zorder=5, label=\"End\")\n",
    "axes[0].set_title(\"Real Trajectory (Latent Space)\",\n",
    "                   fontsize=13, fontweight=\"bold\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Dream trajectory in latent space\n",
    "axes[1].plot(dream_z_np[:, 0], dream_z_np[:, 1], \"r-o\",\n",
    "             markersize=3, alpha=0.6, linewidth=1)\n",
    "axes[1].plot(dream_z_np[0, 0], dream_z_np[0, 1], \"g*\",\n",
    "             markersize=15, zorder=5, label=\"Start\")\n",
    "axes[1].plot(dream_z_np[-1, 0], dream_z_np[-1, 1], \"r*\",\n",
    "             markersize=15, zorder=5, label=\"End\")\n",
    "axes[1].set_title(\"Dream Trajectory (Latent Space)\",\n",
    "                   fontsize=13, fontweight=\"bold\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Match axis limits\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Latent Dim 1\")\n",
    "    ax.set_ylabel(\"Latent Dim 2\")\n",
    "\n",
    "plt.suptitle(\"Latent Space Trajectories: Reality vs Imagination\",\n",
    "             fontsize=15, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ Congratulations!\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸŽ¯ CONGRATULATIONS!\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"You have successfully built a complete World Model!\")\n",
    "print()\n",
    "print(\"Here is what you accomplished in this notebook:\")\n",
    "print(\"  1. âœ… Collected experience from the real environment\")\n",
    "print(\"  2. âœ… Built a VAE that compresses observations to latent codes\")\n",
    "print(\"  3. âœ… Built an MDN-RNN that predicts future states\")\n",
    "print(\"  4. âœ… Built a simple linear Controller\")\n",
    "print(\"  5. âœ… Trained the Controller entirely in dreams\")\n",
    "print(\"  6. âœ… Evaluated the dream-trained agent in reality\")\n",
    "print(\"  7. âœ… Compared real vs imagined trajectories\")\n",
    "print()\n",
    "print(\"The key insight: by learning a good model of the world\")\n",
    "print(\"(V + M), an agent can train its behavior (C) without\")\n",
    "print(\"any additional interaction with the real environment.\")\n",
    "print(\"It learns by dreaming.\")\n",
    "print()\n",
    "print(\"=\" * 60)"
   ],
   "id": "cell_61"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### ðŸ¤” Reflection Questions\n",
    "\n",
    "Take a moment to think about these questions before moving on to the next notebook:\n",
    "\n",
    "1. **Model fidelity:** The dream trajectories diverge from reality over time. Why does this happen, and what determines how quickly the dream drifts from reality? How might we improve this?\n",
    "\n",
    "2. **Controller simplicity:** Ha and Schmidhuber deliberately used a simple linear controller. What are the advantages of this design choice? What happens if we make the controller more complex (e.g., a deep network)?\n",
    "\n",
    "3. **Compounding errors:** In dream training, the Memory model's prediction errors compound at each step â€” the agent acts on a slightly wrong state, which leads to a slightly more wrong next state. How does this relate to the concept of \"distribution shift\" in machine learning? Can you think of ways to mitigate this?\n",
    "\n",
    "### ðŸ”§ Optional Challenges\n",
    "\n",
    "1. **Increase the latent dimension.** Change `latent_dim` from 2 to 8 or 16. Does the VAE reconstruct better? Does the MDN-RNN predict better? Does the agent perform better in the real environment? Plot the results for different latent dimensions.\n",
    "\n",
    "2. **Try a harder environment.** Replace CartPole-v1 with `MountainCar-v0` or `Acrobot-v1`. You will need to adjust the reward proxy function and possibly the network sizes. Can the World Model learn meaningful dynamics for these environments?"
   ],
   "id": "cell_62"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Comes Next\n",
    "\n",
    "In the next notebook, we will explore how World Models evolved. Dreamer (Hafner et al., 2020) extended these ideas with:\n",
    "- **Continuous action spaces** instead of discrete ones\n",
    "- **Learned reward models** instead of hand-crafted reward proxies\n",
    "- **Latent imagination with actor-critic** instead of simple REINFORCE\n",
    "- **Recurrent State-Space Models** that are more expressive than MDN-RNNs\n",
    "\n",
    "The progression from Ha and Schmidhuber's V-M-C to Dreamer is a beautiful case study in how simple, elegant ideas get refined and scaled. We will see you there.\n",
    "\n",
    "---\n",
    "\n",
    "**Original paper:** Ha, D., & Schmidhuber, J. (2018). *World Models.* arXiv:1803.10122\n",
    "\n",
    "**Notebook by Vizuara | World Action Models Series â€” Notebook 2 of 6**"
   ],
   "id": "cell_63"
  }
 ]
}