{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Self-Attention & The Transformer Encoder ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1huSPPw4AR0ie0srQtXzs9GwRvMRBQMHr\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/00_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_00_intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Self-Attention & The Transformer Encoder from First Principles\n",
    "\n",
    "*Part 2 of the Vizuara series on Understanding BERT from Scratch*\n",
    "*Estimated time: 60 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_why_it_matters"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_why_it_matters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "The Transformer encoder is the engine that powers BERT, GPT, and virtually every modern language model. At its heart is a mechanism called **self-attention** ‚Äî a way for every word to look at every other word and decide which ones matter most.\n",
    "\n",
    "In this notebook, we will build the entire Transformer encoder **from scratch** ‚Äî no HuggingFace, no pre-built modules. By the end, you will have:\n",
    "\n",
    "1. A working **scaled dot-product attention** mechanism\n",
    "2. A **multi-head attention** module\n",
    "3. A complete **Transformer encoder block** with residual connections\n",
    "4. Beautiful **attention heatmaps** showing which words attend to which"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup ‚Äî run this cell first\n",
    "!pip install -q torch matplotlib numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Building Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_building_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_building_intuition"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Building Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_building_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_building_intuition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Imagine you are reading the sentence: *\"The delivery arrived late, but **it** was in perfect condition.\"*\n",
    "\n",
    "When you read \"it,\" your brain instantly jumps back to \"delivery\" ‚Äî that is what \"it\" refers to. Your brain does not give equal weight to every word; it **selectively attends** to the relevant ones.\n",
    "\n",
    "Self-attention does exactly this ‚Äî for every word, it computes an attention score with every other word, then uses those scores to create a weighted combination.\n",
    "\n",
    "Think of it like a library:\n",
    "- The **Query** is your search question: \"What am I looking for?\"\n",
    "- The **Key** is the label on each book: \"What information does this book contain?\"\n",
    "- The **Value** is the actual content of the book: \"Here is the information.\"\n",
    "\n",
    "You match your Query against all Keys, and the best-matching Keys point you to the most relevant Values.\n",
    "\n",
    "### ü§î Think About This\n",
    "Why do we need THREE separate matrices (Q, K, V) instead of just using the embeddings directly? Think about what would happen if Q = K = V = the embedding itself."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: The Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_the_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_the_math"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: The Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_the_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_the_math"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Scaled Dot-Product Attention\n",
    "\n",
    "Given queries $Q$, keys $K$, and values $V$, the attention output is:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Let us break this down computationally:\n",
    "- $QK^T$ computes the dot product between every query and every key. This gives us a matrix of \"relevance scores\" ‚Äî how much each word should attend to each other word.\n",
    "- Dividing by $\\sqrt{d_k}$ prevents the dot products from becoming too large (which would make softmax saturate and produce very peaked distributions).\n",
    "- Softmax normalizes each row so the attention weights sum to 1.\n",
    "- Multiplying by $V$ computes a weighted combination of the value vectors.\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Instead of computing attention once, we compute it $h$ times in parallel with different learned projections:\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W^O$$\n",
    "\n",
    "where $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$\n",
    "\n",
    "Computationally: each head learns to look for a different type of relationship. One head might learn syntax (subject-verb), another might learn coreference (what does \"it\" refer to?), and another might learn semantic similarity."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Sdpa Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_sdpa_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_sdpa_code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It ‚Äî Component by Component\n",
    "\n",
    "### 4.1 Scaled Dot-Product Attention"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "\n",
    "    Args:\n",
    "        Q: Query tensor (batch, num_heads, seq_len, d_k)\n",
    "        K: Key tensor (batch, num_heads, seq_len, d_k)\n",
    "        V: Value tensor (batch, num_heads, seq_len, d_v)\n",
    "        mask: Optional mask tensor\n",
    "\n",
    "    Returns:\n",
    "        output: Weighted values (batch, num_heads, seq_len, d_v)\n",
    "        attention_weights: Softmax attention weights (batch, num_heads, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "\n",
    "    # Step 1: Compute attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch, heads, seq, seq)\n",
    "\n",
    "    # Step 2: Scale by sqrt(d_k)\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "\n",
    "    # Step 3: Apply mask (if provided)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # Step 4: Apply softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # Step 5: Multiply by values\n",
    "    output = torch.matmul(attention_weights, V)  # (batch, heads, seq, d_v)\n",
    "\n",
    "    return output, attention_weights"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Numerical Example\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_numerical_example.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_numerical_example"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Numerical Example\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_numerical_example.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_numerical_example"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify this with the numerical example from the article."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical example from the article\n",
    "Q = torch.tensor([[[1.0, 0.0],\n",
    "                    [0.0, 1.0],\n",
    "                    [1.0, 1.0]]]).unsqueeze(1)  # (1, 1, 3, 2)\n",
    "\n",
    "K = torch.tensor([[[1.0, 1.0],\n",
    "                    [0.0, 1.0],\n",
    "                    [1.0, 0.0]]]).unsqueeze(1)\n",
    "\n",
    "V = torch.tensor([[[1.0, 2.0],\n",
    "                    [3.0, 4.0],\n",
    "                    [5.0, 6.0]]]).unsqueeze(1)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(\"Attention weights (row = query word, col = key word):\")\n",
    "print(weights[0, 0].detach().numpy().round(2))\n",
    "print(f\"\\nOutput for first word: {output[0, 0, 0].detach().numpy().round(2)}\")\n",
    "print(f\"Expected (from article): ~[3.0, 4.0]\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Attention Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_attention_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_attention_viz"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Attention Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_attention_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_attention_viz"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize the attention weights\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(weights[0, 0].detach().numpy(), cmap='Blues', vmin=0, vmax=1)\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xticks([0, 1, 2], ['Word 1', 'Word 2', 'Word 3'])\n",
    "plt.yticks([0, 1, 2], ['Word 1', 'Word 2', 'Word 3'])\n",
    "plt.xlabel(\"Key (attending TO)\")\n",
    "plt.ylabel(\"Query (attending FROM)\")\n",
    "plt.title(\"Attention Weights Matrix\")\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        val = weights[0, 0, i, j].item()\n",
    "        plt.text(j, i, f\"{val:.2f}\", ha='center', va='center',\n",
    "                 color='white' if val > 0.5 else 'black', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Multihead Attention\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_multihead_attention.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_multihead_attention"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Multihead Attention\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_multihead_attention.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_multihead_attention"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Multi-Head Attention"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention mechanism.\n",
    "\n",
    "    Instead of one attention function, we run h parallel attention \"heads\",\n",
    "    each with its own learned Q, K, V projections.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "\n",
    "        # Learned projection matrices\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)  # Output projection\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_Q(x)  # (batch, seq, d_model)\n",
    "        K = self.W_K(x)\n",
    "        V = self.W_V(x)\n",
    "\n",
    "        # Reshape to (batch, num_heads, seq_len, d_k)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Apply scaled dot-product attention\n",
    "        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Concatenate heads: (batch, seq_len, d_model)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "\n",
    "        # Final linear projection\n",
    "        output = self.W_O(attn_output)\n",
    "\n",
    "        return output, attn_weights"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-head attention\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Random input: batch=1, seq_len=5, d_model=64\n",
    "x = torch.randn(1, 5, d_model)\n",
    "output, attn_weights = mha(x)\n",
    "\n",
    "print(f\"Input shape:           {x.shape}\")\n",
    "print(f\"Output shape:          {output.shape}\")\n",
    "print(f\"Attention weights:     {attn_weights.shape}\")\n",
    "print(f\"  (batch, heads, seq, seq)\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Multihead Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_multihead_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_multihead_viz"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Multihead Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_multihead_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_multihead_viz"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize attention patterns across all heads\n",
    "fig, axes = plt.subplots(1, num_heads, figsize=(16, 4))\n",
    "words = ['Word‚ÇÅ', 'Word‚ÇÇ', 'Word‚ÇÉ', 'Word‚ÇÑ', 'Word‚ÇÖ']\n",
    "\n",
    "for head_idx in range(num_heads):\n",
    "    ax = axes[head_idx]\n",
    "    weights_np = attn_weights[0, head_idx].detach().numpy()\n",
    "    im = ax.imshow(weights_np, cmap='Blues', vmin=0, vmax=weights_np.max())\n",
    "    ax.set_title(f\"Head {head_idx + 1}\", fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(range(5))\n",
    "    ax.set_xticklabels(words, rotation=45, fontsize=8)\n",
    "    ax.set_yticks(range(5))\n",
    "    ax.set_yticklabels(words, fontsize=8)\n",
    "\n",
    "plt.suptitle(\"Multi-Head Attention ‚Äî Each Head Learns Different Patterns\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"üí° Notice how each head has a DIFFERENT attention pattern!\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Ffn Layernorm\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_ffn_layernorm.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_ffn_layernorm"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Ffn Layernorm\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_ffn_layernorm.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_ffn_layernorm"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Feed-Forward Network\n",
    "\n",
    "The Transformer encoder block also contains a position-wise feed-forward network: two linear layers with a ReLU activation in between.\n",
    "\n",
    "$$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "Computationally: this acts as a \"thinking\" step. After attention gathers relevant information from other words, the FFN processes that information independently at each position."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network.\n",
    "\n",
    "    Two linear transformations with ReLU in between.\n",
    "    The inner dimension is typically 4x the model dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.relu(self.linear1(x)))"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Layer Normalization"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer Normalization (Ba et al., 2016).\n",
    "\n",
    "    Normalizes the last dimension of the input to have\n",
    "    zero mean and unit variance, then applies learned\n",
    "    scale (gamma) and shift (beta).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Encoder Block\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_encoder_block.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_encoder_block"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Encoder Block\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_encoder_block.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_encoder_block"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 The Complete Transformer Encoder Block"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    One Transformer encoder block:\n",
    "      1. Multi-Head Self-Attention + Residual + LayerNorm\n",
    "      2. Feed-Forward Network + Residual + LayerNorm\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection and layer norm\n",
    "        attn_output, attn_weights = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # Feed-forward with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "\n",
    "        return x, attn_weights"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the full encoder block\n",
    "encoder_block = TransformerEncoderBlock(d_model=64, num_heads=4, d_ff=256)\n",
    "\n",
    "x = torch.randn(1, 5, 64)\n",
    "output, weights = encoder_block(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"‚úÖ Same shape in and out ‚Äî this is key for stacking blocks!\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo Stack Blocks\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_todo_stack_blocks.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_11_todo_stack_blocks"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo Stack Blocks\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_todo_stack_blocks.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_11_todo_stack_blocks"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîß Your Turn\n",
    "\n",
    "### TODO: Stack Multiple Encoder Blocks\n",
    "\n",
    "BERT-Base uses 12 encoder blocks stacked on top of each other. Implement the `TransformerEncoder` class that stacks N blocks."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stack of N Transformer encoder blocks.\n",
    "\n",
    "    Args:\n",
    "        num_layers: Number of encoder blocks to stack\n",
    "        d_model: Model dimension\n",
    "        num_heads: Number of attention heads\n",
    "        d_ff: Feed-forward inner dimension\n",
    "        dropout: Dropout rate\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # ============ TODO ============\n",
    "        # Create a nn.ModuleList containing num_layers\n",
    "        # TransformerEncoderBlock instances\n",
    "        # ==============================\n",
    "        self.layers = ???  # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Pass input through all encoder blocks sequentially.\n",
    "        Return the final output and attention weights from the LAST layer.\n",
    "        \"\"\"\n",
    "        attn_weights = None\n",
    "\n",
    "        # ============ TODO ============\n",
    "        # Loop through self.layers and pass x through each\n",
    "        # ==============================\n",
    "        for layer in ???:  # YOUR CODE HERE\n",
    "            x, attn_weights = ???  # YOUR CODE HERE\n",
    "\n",
    "        return x, attn_weights"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification\n",
    "encoder = TransformerEncoder(num_layers=6, d_model=64, num_heads=4, d_ff=256)\n",
    "test_input = torch.randn(2, 10, 64)  # batch=2, seq_len=10\n",
    "test_output, test_weights = encoder(test_input)\n",
    "\n",
    "assert test_output.shape == (2, 10, 64), f\"‚ùå Expected shape (2, 10, 64), got {test_output.shape}\"\n",
    "assert test_weights.shape == (2, 4, 10, 10), f\"‚ùå Expected attention shape (2, 4, 10, 10), got {test_weights.shape}\"\n",
    "print(f\"‚úÖ TransformerEncoder works! Output shape: {test_output.shape}\")\n",
    "print(f\"   6 layers stacked, each with 4 attention heads\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in encoder.parameters()):,}\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo Positional Encoding\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_todo_positional_encoding.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_12_todo_positional_encoding"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo Positional Encoding\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_todo_positional_encoding.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_12_todo_positional_encoding"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implement Positional Encoding\n",
    "\n",
    "The Transformer has no built-in notion of word order. We need to inject positional information. Implement sinusoidal positional encoding:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
    "\n",
    "Computationally: even dimensions use sine, odd dimensions use cosine, with frequencies that decrease as the dimension increases. This creates a unique \"fingerprint\" for each position."
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encoding.\n",
    "\n",
    "    Adds a fixed positional signal to the input embeddings\n",
    "    so the model knows the order of words.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # ============ TODO ============\n",
    "        # Step 1: Compute the division term: 10000^(2i/d_model)\n",
    "        #         Hint: use torch.exp and torch.arange\n",
    "        # Step 2: Apply sin to even indices (0, 2, 4, ...)\n",
    "        # Step 3: Apply cos to odd indices (1, 3, 5, ...)\n",
    "        # ==============================\n",
    "\n",
    "        div_term = ???  # YOUR CODE HERE\n",
    "        pe[:, 0::2] = ???  # YOUR CODE HERE (even indices)\n",
    "        pe[:, 1::2] = ???  # YOUR CODE HERE (odd indices)\n",
    "\n",
    "        # Register as buffer (not a parameter ‚Äî it's fixed)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
    "        return x + self.pe[:, :x.size(1)]"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification\n",
    "pos_enc = PositionalEncoding(d_model=64, max_len=100)\n",
    "test_x = torch.zeros(1, 50, 64)\n",
    "encoded = pos_enc(test_x)\n",
    "\n",
    "assert encoded.shape == (1, 50, 64), f\"‚ùå Expected shape (1, 50, 64), got {encoded.shape}\"\n",
    "assert not torch.allclose(encoded[0, 0], encoded[0, 1]), \"‚ùå Different positions should have different encodings\"\n",
    "print(\"‚úÖ Positional encoding works!\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize the positional encoding\n",
    "pe = pos_enc.pe[0, :50, :64].numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.imshow(pe.T, cmap='RdBu', aspect='auto', interpolation='nearest')\n",
    "plt.colorbar(label='Encoding Value')\n",
    "plt.xlabel(\"Position in Sequence\")\n",
    "plt.ylabel(\"Embedding Dimension\")\n",
    "plt.title(\"Sinusoidal Positional Encoding\\n(each position has a unique pattern)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Real Text Attention\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_real_text_attention.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_13_real_text_attention"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Real Text Attention\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_real_text_attention.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_13_real_text_attention"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together: Attention on Real Text\n",
    "\n",
    "Let us run our Transformer encoder on a real sentence and visualize what the attention heads learn."
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple tokenizer for our demo\n",
    "sentence = \"the cat sat on it\"\n",
    "words = sentence.split()\n",
    "word_to_idx = {w: i for i, w in enumerate(set(words))}\n",
    "idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "\n",
    "# Build a simple model\n",
    "class SimpleTransformerDemo(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        self.encoder = TransformerEncoder(num_layers, d_model, num_heads, d_ff)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.pos_encoding(x)\n",
    "        output, attn_weights = self.encoder(x)\n",
    "        return output, attn_weights\n",
    "\n",
    "vocab_size = len(word_to_idx)\n",
    "demo_model = SimpleTransformerDemo(\n",
    "    vocab_size=vocab_size, d_model=32, num_heads=4, d_ff=128, num_layers=2\n",
    ")\n",
    "\n",
    "# Encode the sentence\n",
    "input_ids = torch.tensor([[word_to_idx[w] for w in words]])\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, attn_weights = demo_model(input_ids)\n",
    "\n",
    "print(f\"Sentence: '{sentence}'\")\n",
    "print(f\"Input shape:            {input_ids.shape}\")\n",
    "print(f\"Contextual output:      {output.shape}\")\n",
    "print(f\"Attention weights:      {attn_weights.shape}\")"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Attention heatmap ‚Äî which words attend to which?\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "for head_idx in range(4):\n",
    "    ax = axes[head_idx]\n",
    "    weights_np = attn_weights[0, head_idx].detach().numpy()\n",
    "    im = ax.imshow(weights_np, cmap='Purples', vmin=0)\n",
    "    ax.set_title(f\"Head {head_idx + 1}\", fontsize=13, fontweight='bold')\n",
    "    ax.set_xticks(range(len(words)))\n",
    "    ax.set_xticklabels(words, rotation=45, fontsize=11)\n",
    "    ax.set_yticks(range(len(words)))\n",
    "    ax.set_yticklabels(words, fontsize=11)\n",
    "\n",
    "    # Annotate values\n",
    "    for i in range(len(words)):\n",
    "        for j in range(len(words)):\n",
    "            val = weights_np[i, j]\n",
    "            ax.text(j, i, f\"{val:.2f}\", ha='center', va='center',\n",
    "                    fontsize=8, color='white' if val > 0.4 else 'black')\n",
    "\n",
    "plt.suptitle(f'Self-Attention Heatmaps: \"{sentence}\"', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Each head develops a DIFFERENT attention pattern.\")\n",
    "print(\"   In a trained model, 'it' would strongly attend to 'cat' (coreference).\")"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Final Output\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_final_output.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_14_final_output"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Final Output\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_final_output.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_14_final_output"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üéØ Final Output: Interactive Attention Visualization"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a slightly larger demo to show richer patterns\n",
    "sentences = [\n",
    "    \"the cat sat on the mat and purred\",\n",
    "    \"the dog chased the cat around the yard\",\n",
    "    \"she went to the bank to deposit money\",\n",
    "]\n",
    "\n",
    "# Build a combined vocabulary\n",
    "all_words_list = list(set(w for s in sentences for w in s.split()))\n",
    "demo_vocab = {w: i for i, w in enumerate(all_words_list)}\n",
    "demo_idx_to_word = {i: w for w, i in demo_vocab.items()}\n",
    "\n",
    "demo_model_2 = SimpleTransformerDemo(\n",
    "    vocab_size=len(demo_vocab), d_model=32, num_heads=4, d_ff=128, num_layers=3\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(len(sentences), 1, figsize=(14, 4 * len(sentences)))\n",
    "\n",
    "for sent_idx, sentence in enumerate(sentences):\n",
    "    words = sentence.split()\n",
    "    ids = torch.tensor([[demo_vocab[w] for w in words]])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, attn = demo_model_2(ids)\n",
    "\n",
    "    # Average attention across all heads\n",
    "    avg_attn = attn[0].mean(dim=0).numpy()\n",
    "\n",
    "    ax = axes[sent_idx]\n",
    "    im = ax.imshow(avg_attn, cmap='viridis', vmin=0)\n",
    "    ax.set_title(f'\"{sentence}\"', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(range(len(words)))\n",
    "    ax.set_xticklabels(words, rotation=45, fontsize=10)\n",
    "    ax.set_yticks(range(len(words)))\n",
    "    ax.set_yticklabels(words, fontsize=10)\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.suptitle(\"üéØ Self-Attention Patterns (Averaged Across Heads)\", fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéâ Congratulations! You've built a complete Transformer encoder from scratch!\")\n",
    "print(\"   Next up: BERT's architecture ‚Äî input representation and pre-training objectives.\")"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/15_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_15_closing"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/15_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_15_closing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reflection and Next Steps\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "1. Why do we divide by $\\sqrt{d_k}$ in the attention formula? What would happen without it? (Hint: think about the variance of dot products as dimension grows.)\n",
    "2. If we have 12 attention heads with $d_k = 64$ each, what is the total model dimension $d_{\\text{model}}$? Why is this more efficient than having one head with $d_k = 768$?\n",
    "3. The residual connections in the encoder block add the input directly to the output. Why is this important for training deep networks?\n",
    "\n",
    "### üèÜ Optional Challenges\n",
    "1. **Masked Self-Attention**: Modify the attention function to apply a causal mask that prevents words from attending to future words. This is how GPT works (decoder-style).\n",
    "2. **Relative Position Encoding**: Instead of fixed sinusoidal encodings, implement relative position encodings where attention scores are modified based on the distance between tokens.\n",
    "3. **Attention Dropout**: Add dropout to the attention weights (after softmax, before multiplying by V). How does this affect the model?"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "chatbot"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üí¨ AI Teaching Assistant ‚Äî Click ‚ñ∂ to start\n",
    "#@markdown This AI chatbot reads your notebook and can answer questions about any concept, code, or exercise.\n",
    "\n",
    "import json as _json\n",
    "import requests as _requests\n",
    "from google.colab import output as _output\n",
    "from IPython.display import display, HTML as _HTML, Markdown as _Markdown\n",
    "\n",
    "# --- Read notebook content for context ---\n",
    "def _get_notebook_context():\n",
    "    try:\n",
    "        from google.colab import _message\n",
    "        nb = _message.blocking_request(\"get_ipynb\", request=\"\", timeout_sec=10)\n",
    "        cells = nb.get(\"ipynb\", {}).get(\"cells\", [])\n",
    "        parts = []\n",
    "        for cell in cells:\n",
    "            src = \"\".join(cell.get(\"source\", []))\n",
    "            tags = cell.get(\"metadata\", {}).get(\"tags\", [])\n",
    "            if \"chatbot\" in tags:\n",
    "                continue\n",
    "            if src.strip():\n",
    "                ct = cell.get(\"cell_type\", \"unknown\")\n",
    "                parts.append(f\"[{ct.upper()}]\\n{src}\")\n",
    "        return \"\\n\\n---\\n\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"Notebook content unavailable.\"\n",
    "\n",
    "_NOTEBOOK_CONTEXT = _get_notebook_context()\n",
    "_CHAT_HISTORY = []\n",
    "_API_URL = \"https://course-creator-brown.vercel.app/api/chat\"\n",
    "\n",
    "def _notebook_chat(question):\n",
    "    global _CHAT_HISTORY\n",
    "    try:\n",
    "        resp = _requests.post(_API_URL, json={\n",
    "            'question': question,\n",
    "            'context': _NOTEBOOK_CONTEXT[:100000],\n",
    "            'history': _CHAT_HISTORY[-10:],\n",
    "        }, timeout=60)\n",
    "        data = resp.json()\n",
    "        answer = data.get('answer', 'Sorry, I could not generate a response.')\n",
    "        _CHAT_HISTORY.append({'role': 'user', 'content': question})\n",
    "        _CHAT_HISTORY.append({'role': 'assistant', 'content': answer})\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f'Error connecting to teaching assistant: {str(e)}'\n",
    "\n",
    "_output.register_callback('notebook_chat', _notebook_chat)\n",
    "\n",
    "def ask(question):\n",
    "    \"\"\"Ask the AI teaching assistant a question about this notebook.\"\"\"\n",
    "    answer = _notebook_chat(question)\n",
    "    display(_Markdown(answer))\n",
    "\n",
    "print(\"\\u2705 AI Teaching Assistant is ready!\")\n",
    "print(\"\\U0001f4a1 Use the chat below, or call ask(\\'your question\\') in any cell.\")\n",
    "\n",
    "# --- Display chat widget ---\n",
    "display(_HTML('''<style>\n  .vc-wrap{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;max-width:100%;border-radius:16px;overflow:hidden;box-shadow:0 4px 24px rgba(0,0,0,.12);background:#fff;border:1px solid #e5e7eb}\n  .vc-hdr{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;padding:16px 20px;display:flex;align-items:center;gap:12px}\n  .vc-avatar{width:42px;height:42px;background:rgba(255,255,255,.2);border-radius:50%;display:flex;align-items:center;justify-content:center;font-size:22px}\n  .vc-hdr h3{font-size:16px;font-weight:600;margin:0}\n  .vc-hdr p{font-size:12px;opacity:.85;margin:2px 0 0}\n  .vc-msgs{height:420px;overflow-y:auto;padding:16px;background:#f8f9fb;display:flex;flex-direction:column;gap:10px}\n  .vc-msg{display:flex;flex-direction:column;animation:vc-fade .25s ease}\n  .vc-msg.user{align-items:flex-end}\n  .vc-msg.bot{align-items:flex-start}\n  .vc-bbl{max-width:85%;padding:10px 14px;border-radius:16px;font-size:14px;line-height:1.55;word-wrap:break-word}\n  .vc-msg.user .vc-bbl{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border-bottom-right-radius:4px}\n  .vc-msg.bot .vc-bbl{background:#fff;color:#1a1a2e;border:1px solid #e8e8e8;border-bottom-left-radius:4px}\n  .vc-bbl code{background:rgba(0,0,0,.07);padding:2px 6px;border-radius:4px;font-size:13px;font-family:'Fira Code',monospace}\n  .vc-bbl pre{background:#1e1e2e;color:#cdd6f4;padding:12px;border-radius:8px;overflow-x:auto;margin:8px 0;font-size:13px}\n  .vc-bbl pre code{background:none;padding:0;color:inherit}\n  .vc-bbl h3,.vc-bbl h4{margin:10px 0 4px;font-size:15px}\n  .vc-bbl ul,.vc-bbl ol{margin:4px 0;padding-left:20px}\n  .vc-bbl li{margin:2px 0}\n  .vc-chips{display:flex;flex-wrap:wrap;gap:8px;padding:0 16px 12px;background:#f8f9fb}\n  .vc-chip{background:#fff;border:1px solid #d1d5db;border-radius:20px;padding:6px 14px;font-size:12px;cursor:pointer;transition:all .15s;color:#4b5563}\n  .vc-chip:hover{border-color:#667eea;color:#667eea;background:#f0f0ff}\n  .vc-input{display:flex;padding:12px 16px;background:#fff;border-top:1px solid #eee;gap:8px}\n  .vc-input input{flex:1;padding:10px 16px;border:2px solid #e8e8e8;border-radius:24px;font-size:14px;outline:none;transition:border-color .2s}\n  .vc-input input:focus{border-color:#667eea}\n  .vc-input button{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border:none;border-radius:50%;width:42px;height:42px;cursor:pointer;display:flex;align-items:center;justify-content:center;font-size:18px;transition:transform .1s}\n  .vc-input button:hover{transform:scale(1.05)}\n  .vc-input button:disabled{opacity:.5;cursor:not-allowed;transform:none}\n  .vc-typing{display:flex;gap:5px;padding:4px 0}\n  .vc-typing span{width:8px;height:8px;background:#667eea;border-radius:50%;animation:vc-bounce 1.4s infinite ease-in-out}\n  .vc-typing span:nth-child(2){animation-delay:.2s}\n  .vc-typing span:nth-child(3){animation-delay:.4s}\n  @keyframes vc-bounce{0%,80%,100%{transform:scale(0)}40%{transform:scale(1)}}\n  @keyframes vc-fade{from{opacity:0;transform:translateY(8px)}to{opacity:1;transform:translateY(0)}}\n  .vc-note{text-align:center;font-size:11px;color:#9ca3af;padding:8px 16px 12px;background:#fff}\n</style>\n<div class=\"vc-wrap\">\n  <div class=\"vc-hdr\">\n    <div class=\"vc-avatar\">&#129302;</div>\n    <div>\n      <h3>Vizuara Teaching Assistant</h3>\n      <p>Ask me anything about this notebook</p>\n    </div>\n  </div>\n  <div class=\"vc-msgs\" id=\"vcMsgs\">\n    <div class=\"vc-msg bot\">\n      <div class=\"vc-bbl\">&#128075; Hi! I've read through this entire notebook. Ask me about any concept, code block, or exercise &mdash; I'm here to help you learn!</div>\n    </div>\n  </div>\n  <div class=\"vc-chips\" id=\"vcChips\">\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Explain the main concept</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Help with the TODO exercise</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Summarize what I learned</span>\n  </div>\n  <div class=\"vc-input\">\n    <input type=\"text\" id=\"vcIn\" placeholder=\"Ask about concepts, code, exercises...\" />\n    <button id=\"vcSend\" onclick=\"vcSendMsg()\">&#10148;</button>\n  </div>\n  <div class=\"vc-note\">AI-generated &middot; Verify important information &middot; <a href=\"#\" onclick=\"vcClear();return false\" style=\"color:#667eea\">Clear chat</a></div>\n</div>\n<script>\n(function(){\n  var msgs=document.getElementById('vcMsgs'),inp=document.getElementById('vcIn'),\n      btn=document.getElementById('vcSend'),chips=document.getElementById('vcChips');\n\n  function esc(s){var d=document.createElement('div');d.textContent=s;return d.innerHTML}\n\n  function md(t){\n    return t\n      .replace(/```(\\w*)\\n([\\s\\S]*?)```/g,function(_,l,c){return '<pre><code>'+esc(c)+'</code></pre>'})\n      .replace(/`([^`]+)`/g,'<code>$1</code>')\n      .replace(/\\*\\*([^*]+)\\*\\*/g,'<strong>$1</strong>')\n      .replace(/\\*([^*]+)\\*/g,'<em>$1</em>')\n      .replace(/^#### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^## (.+)$/gm,'<h3>$1</h3>')\n      .replace(/^\\d+\\. (.+)$/gm,'<li>$1</li>')\n      .replace(/^- (.+)$/gm,'<li>$1</li>')\n      .replace(/\\n\\n/g,'<br><br>')\n      .replace(/\\n/g,'<br>');\n  }\n\n  function addMsg(text,isUser){\n    var m=document.createElement('div');m.className='vc-msg '+(isUser?'user':'bot');\n    var b=document.createElement('div');b.className='vc-bbl';\n    b.innerHTML=isUser?esc(text):md(text);\n    m.appendChild(b);msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function showTyping(){\n    var m=document.createElement('div');m.className='vc-msg bot';m.id='vcTyping';\n    m.innerHTML='<div class=\"vc-bbl\"><div class=\"vc-typing\"><span></span><span></span><span></span></div></div>';\n    msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function hideTyping(){var e=document.getElementById('vcTyping');if(e)e.remove()}\n\n  window.vcSendMsg=function(){\n    var q=inp.value.trim();if(!q)return;\n    inp.value='';chips.style.display='none';\n    addMsg(q,true);showTyping();btn.disabled=true;\n    google.colab.kernel.invokeFunction('notebook_chat',[q],{})\n      .then(function(r){\n        hideTyping();\n        var a=r.data['application/json'];\n        addMsg(typeof a==='string'?a:JSON.stringify(a),false);\n      })\n      .catch(function(){\n        hideTyping();\n        addMsg('Sorry, I encountered an error. Please check your internet connection and try again.',false);\n      })\n      .finally(function(){btn.disabled=false;inp.focus()});\n  };\n\n  window.vcAsk=function(q){inp.value=q;vcSendMsg()};\n  window.vcClear=function(){\n    msgs.innerHTML='<div class=\"vc-msg bot\"><div class=\"vc-bbl\">&#128075; Chat cleared. Ask me anything!</div></div>';\n    chips.style.display='flex';\n  };\n\n  inp.addEventListener('keypress',function(e){if(e.key==='Enter')vcSendMsg()});\n  inp.focus();\n})();\n</script>'''))"
   ],
   "id": "vizuara_chatbot"
  }
 ]
}