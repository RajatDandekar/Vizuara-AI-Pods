{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Fine-tuning BERT for Real Tasks ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"10jLfViL5fReO1p3obUYbq8dCdXD-BOMp\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/seg_01_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Seg 01 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/seg_01_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_seg_01_intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Fine-tuning BERT for Real Tasks\n",
    "\n",
    "*Part 4 of the Vizuara series on Understanding BERT from Scratch*\n",
    "*Estimated time: 50 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ü§ñ AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** ‚Äî it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[üëâ Open AI Teaching Assistant](https://course-creator-brown.vercel.app/courses/understanding-bert-from-scratch/practice/4/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Seg 02 Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/seg_02_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_seg_02_why_it_matters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "BERT's real power is not in pre-training ‚Äî it is in **fine-tuning**. A single pre-trained BERT can be adapted to almost any NLP task with minimal modifications:\n",
    "\n",
    "- **Sentiment analysis**: Is this review positive or negative?\n",
    "- **Named entity recognition**: Which words are people, places, or organizations?\n",
    "- **Question answering**: Where in the passage is the answer?\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Load a **real pre-trained BERT** model\n",
    "2. Fine-tune it on the **IMDB movie review dataset** for sentiment classification\n",
    "3. Achieve **>90% accuracy** with just a few minutes of training\n",
    "4. **Visualize attention patterns** to see what BERT is \"looking at\"\n",
    "\n",
    "This is the payoff ‚Äî everything we built in the previous three notebooks comes together here."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Seg 03 Setup Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/seg_03_setup_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_seg_03_setup_code"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup ‚Äî run this cell first\n",
    "!pip install -q torch transformers datasets matplotlib numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Seg 04 Building Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/seg_04_building_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_seg_04_building_intuition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Remember the pre-train and fine-tune paradigm:\n",
    "\n",
    "**Pre-training** (what we did in Notebook 03): BERT reads billions of words, learning grammar, semantics, and world knowledge through MLM and NSP. This is expensive (days on TPUs) and done once.\n",
    "\n",
    "**Fine-tuning** (what we do now): We take the pre-trained BERT, add a thin task-specific layer on top, and train the whole thing on a small labeled dataset. This is cheap (minutes on a GPU).\n",
    "\n",
    "The key insight: BERT already \"understands\" language. We just need to teach it the specific task.\n",
    "\n",
    "For **classification**, we use the [CLS] token's final hidden state ‚Äî it has attended to every other token and acts as an aggregate representation of the entire input.\n",
    "\n",
    "### ü§î Think About This\n",
    "Why do we fine-tune the *entire* model (all 110M parameters) rather than just training the classification head? What would happen if we froze BERT's weights?"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Seg 05 The Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/seg_05_the_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_seg_05_the_math"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Fine-tuning for Classification\n",
    "\n",
    "Given the [CLS] token's final hidden state $\\mathbf{h}_{\\text{CLS}} \\in \\mathbb{R}^{768}$, the classification prediction is:\n",
    "\n",
    "$$P(y \\mid x) = \\text{softmax}(W \\cdot \\mathbf{h}_{\\text{CLS}} + b)$$\n",
    "\n",
    "Computationally: we project the 768-dimensional [CLS] representation down to the number of classes (2 for sentiment) using a single linear layer, then apply softmax to get probabilities.\n",
    "\n",
    "The fine-tuning loss is standard cross-entropy:\n",
    "\n",
    "$$\\mathcal{L} = -\\sum_{i} y_i \\log P(y_i \\mid x_i)$$\n",
    "\n",
    "The crucial detail: during fine-tuning, gradients flow back through the classification head *and* through all of BERT's encoder layers. This means BERT's internal representations are **adjusted** to be more useful for the specific task."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Seg 06 Loading Imdb\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/seg_06_loading_imdb.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_seg_06_loading_imdb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It ‚Äî Component by Component\n",
    "\n",
    "### 4.1 Load the IMDB Dataset"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB dataset (25K train, 25K test)\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "print(f\"Training examples: {len(dataset['train'])}\")\n",
    "print(f\"Test examples:     {len(dataset['test'])}\")\n",
    "\n",
    "# Look at a few examples\n",
    "for i in range(3):\n",
    "    text = dataset['train'][i]['text'][:150]\n",
    "    label = \"Positive\" if dataset['train'][i]['label'] == 1 else \"Negative\"\n",
    "    print(f\"\\n[{label}] {text}...\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Seg 07 Bert Tokenizer\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/seg_07_bert_tokenizer.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_seg_07_bert_tokenizer"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Tokenize with BERT's Tokenizer"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Demo: see how BERT tokenizes text\n",
    "sample_text = \"This movie was absolutely fantastic! The acting was superb.\"\n",
    "tokens = tokenizer(sample_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "print(f\"Original:  {sample_text}\")\n",
    "print(f\"Tokens:    {tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])}\")\n",
    "print(f\"Token IDs: {tokens['input_ids'][0].tolist()}\")\n",
    "print(f\"\\nNotice: [CLS] at start, [SEP] at end, all lowercase\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Seg 08 Data Preparation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/seg_08_data_preparation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_seg_08_data_preparation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "# We use a subset for faster training in this notebook\n",
    "TRAIN_SIZE = 2000  # Use 2K examples (full dataset = 25K)\n",
    "TEST_SIZE = 500\n",
    "\n",
    "def tokenize_batch(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Select subsets\n",
    "train_data = dataset['train'].shuffle(seed=42).select(range(TRAIN_SIZE))\n",
    "test_data = dataset['test'].shuffle(seed=42).select(range(TEST_SIZE))\n",
    "\n",
    "# Tokenize\n",
    "train_encoded = train_data.map(\n",
    "    lambda x: tokenizer(x['text'], padding='max_length', truncation=True, max_length=128),\n",
    "    batched=True\n",
    ")\n",
    "test_encoded = test_data.map(\n",
    "    lambda x: tokenizer(x['text'], padding='max_length', truncation=True, max_length=128),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "train_encoded.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_encoded.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "train_loader = DataLoader(train_encoded, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_encoded, batch_size=32)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches:     {len(test_loader)}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Label distribution\n",
    "labels = [d['label'] for d in train_data]\n",
    "pos = sum(labels)\n",
    "neg = len(labels) - pos\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([\"Negative\", \"Positive\"], [neg, pos], color=['coral', 'steelblue'])\n",
    "plt.title(\"Training Set Label Distribution\")\n",
    "plt.ylabel(\"Count\")\n",
    "for i, v in enumerate([neg, pos]):\n",
    "    plt.text(i, v + 20, str(v), ha='center', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Seg 09 Loading Pretrained Bert\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/seg_09_loading_pretrained_bert.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_seg_09_loading_pretrained_bert"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Load Pre-trained BERT"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT with a classification head\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters:     {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(f\"  - Token embeddings: {model.bert.embeddings.word_embeddings.weight.shape}\")\n",
    "print(f\"  - Encoder layers: {len(model.bert.encoder.layer)}\")\n",
    "print(f\"  - Classifier: {model.classifier}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Seg 10 Finetuning Loop\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/seg_10_finetuning_loop.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_seg_10_finetuning_loop"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Fine-tuning Loop"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "EPOCHS = 3\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        if (batch_idx + 1) % 25 == 0:\n",
    "            print(f\"  Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader)}, \"\n",
    "                  f\"Loss: {loss.item():.4f}, Acc: {correct/total:.3f}\")\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    avg_acc = correct / total\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accs.append(avg_acc)\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS} ‚Äî Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.3f}\\n\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Seg 11 Training Curves\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/seg_11_training_curves.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_seg_11_training_curves"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(range(1, EPOCHS+1), train_losses, 'o-', color='coral', linewidth=2)\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Training Loss\", fontsize=13, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(range(1, EPOCHS+1), train_accs, 'o-', color='steelblue', linewidth=2)\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].set_title(\"Training Accuracy\", fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylim(0.5, 1.0)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"BERT Fine-tuning Progress\", fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Seg 12 Todo Evaluation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/seg_12_todo_evaluation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_seg_12_todo_evaluation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîß Your Turn\n",
    "\n",
    "### TODO: Implement the Evaluation Function"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set.\n",
    "\n",
    "    Returns:\n",
    "        accuracy: float\n",
    "        all_predictions: list of predicted labels\n",
    "        all_labels: list of true labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Wrap the loop in torch.no_grad()\n",
    "    # Step 2: For each batch, get predictions using model(input_ids, attention_mask)\n",
    "    # Step 3: Use torch.argmax on logits to get predicted labels\n",
    "    # Step 4: Collect predictions and true labels\n",
    "    # ==============================\n",
    "\n",
    "    with ???:  # YOUR CODE HERE\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = ???  # YOUR CODE HERE\n",
    "            preds = ???  # YOUR CODE HERE\n",
    "\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    accuracy = sum(p == l for p, l in zip(all_preds, all_labels)) / len(all_labels)\n",
    "    return accuracy, all_preds, all_labels"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification\n",
    "accuracy, predictions, true_labels = evaluate(model, test_loader, device)\n",
    "assert 0 <= accuracy <= 1, \"‚ùå Accuracy should be between 0 and 1\"\n",
    "assert len(predictions) == TEST_SIZE, f\"‚ùå Expected {TEST_SIZE} predictions, got {len(predictions)}\"\n",
    "print(f\"‚úÖ Evaluation works! Test accuracy: {accuracy:.3f}\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Seg 13 Confusion Matrix\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/seg_13_confusion_matrix.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_seg_13_confusion_matrix"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Confusion matrix\n",
    "from collections import Counter\n",
    "\n",
    "tp = sum(1 for p, l in zip(predictions, true_labels) if p == 1 and l == 1)\n",
    "fp = sum(1 for p, l in zip(predictions, true_labels) if p == 1 and l == 0)\n",
    "fn = sum(1 for p, l in zip(predictions, true_labels) if p == 0 and l == 1)\n",
    "tn = sum(1 for p, l in zip(predictions, true_labels) if p == 0 and l == 0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "matrix = np.array([[tn, fp], [fn, tp]])\n",
    "im = ax.imshow(matrix, cmap='Blues')\n",
    "\n",
    "labels_text = ['Negative', 'Positive']\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels(labels_text)\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_yticklabels(labels_text)\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"Actual\")\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax.text(j, i, str(matrix[i, j]), ha='center', va='center',\n",
    "                fontsize=20, fontweight='bold',\n",
    "                color='white' if matrix[i, j] > matrix.max()/2 else 'black')\n",
    "\n",
    "plt.title(f\"Confusion Matrix (Accuracy: {accuracy:.1%})\", fontsize=13, fontweight='bold')\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Seg 14 Todo Threshold\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/seg_14_todo_threshold.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_seg_14_todo_threshold"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implement Prediction with Confidence Threshold\n",
    "\n",
    "Build a function that classifies text but only returns a prediction when it is confident enough. Otherwise, it returns \"Uncertain.\""
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_threshold(text, model, tokenizer, device, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Classify text, but only return a label if confidence exceeds threshold.\n",
    "\n",
    "    Args:\n",
    "        text: input string\n",
    "        threshold: minimum confidence to make a prediction (default 0.7)\n",
    "\n",
    "    Returns:\n",
    "        label: \"Positive\", \"Negative\", or \"Uncertain\"\n",
    "        confidence: float ‚Äî the model's confidence\n",
    "        probs: dict with 'positive' and 'negative' probabilities\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True,\n",
    "                       truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Forward pass through the model (no gradients)\n",
    "    # Step 2: Apply softmax to get probabilities\n",
    "    # Step 3: Get the maximum probability and its index\n",
    "    # Step 4: If max probability >= threshold, return the label\n",
    "    #         Otherwise, return \"Uncertain\"\n",
    "    # ==============================\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = ???  # YOUR CODE HERE\n",
    "        probs = ???  # YOUR CODE HERE: softmax on logits\n",
    "\n",
    "    neg_prob = ???  # YOUR CODE HERE\n",
    "    pos_prob = ???  # YOUR CODE HERE\n",
    "    confidence = ???  # YOUR CODE HERE: max of the two\n",
    "\n",
    "    if confidence >= threshold:\n",
    "        label = ???  # YOUR CODE HERE: \"Positive\" or \"Negative\"\n",
    "    else:\n",
    "        label = \"Uncertain\"\n",
    "\n",
    "    return label, confidence, {\"positive\": pos_prob, \"negative\": neg_prob}"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification\n",
    "label, conf, probs = classify_with_threshold(\n",
    "    \"This was the best movie I have ever seen!\",\n",
    "    model, tokenizer, device, threshold=0.6\n",
    ")\n",
    "assert label in [\"Positive\", \"Negative\", \"Uncertain\"], f\"‚ùå Invalid label: {label}\"\n",
    "assert 0 <= conf <= 1, f\"‚ùå Confidence should be in [0, 1], got {conf}\"\n",
    "assert \"positive\" in probs and \"negative\" in probs, \"‚ùå probs dict missing keys\"\n",
    "print(f\"‚úÖ Threshold classifier works! Label: {label}, Confidence: {conf:.3f}\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Seg 15 Attention Visualization\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/seg_15_attention_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_seg_15_attention_visualization"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing BERT's Attention\n",
    "\n",
    "This is the most fascinating part ‚Äî we can peek inside BERT to see what it is \"looking at\" when making predictions."
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT with attention output enabled\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True).to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "def get_attention_weights(text, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Get attention weights from all layers and heads for a given text.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # outputs.attentions is a tuple of (num_layers,) tensors,\n",
    "    # each of shape (batch, num_heads, seq_len, seq_len)\n",
    "    attentions = torch.stack(outputs.attentions)  # (layers, batch, heads, seq, seq)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "    return attentions[:, 0].cpu().numpy(), tokens  # Remove batch dim\n",
    "\n",
    "text = \"The movie was absolutely terrible and I hated every minute of it.\"\n",
    "attentions, tokens = get_attention_weights(text, tokenizer, bert_model, device)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Attention shape: {attentions.shape}\")\n",
    "print(f\"  (layers={attentions.shape[0]}, heads={attentions.shape[1]}, seq={attentions.shape[2]}, seq={attentions.shape[3]})\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Attention heatmap for a specific layer and head\n",
    "def plot_attention_head(attentions, tokens, layer, head, ax=None):\n",
    "    \"\"\"Plot attention weights for a specific layer and head.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    weights = attentions[layer, head]\n",
    "    # Only show non-padding tokens\n",
    "    n_tokens = len([t for t in tokens if t != '[PAD]'])\n",
    "    weights = weights[:n_tokens, :n_tokens]\n",
    "    display_tokens = tokens[:n_tokens]\n",
    "\n",
    "    im = ax.imshow(weights, cmap='Purples', vmin=0)\n",
    "    ax.set_xticks(range(n_tokens))\n",
    "    ax.set_xticklabels(display_tokens, rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_yticks(range(n_tokens))\n",
    "    ax.set_yticklabels(display_tokens, fontsize=9)\n",
    "    ax.set_title(f\"Layer {layer+1}, Head {head+1}\", fontsize=11, fontweight='bold')\n",
    "    return im\n",
    "\n",
    "# Show 4 interesting heads\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "layer_heads = [(0, 0), (0, 6), (5, 3), (11, 8)]\n",
    "\n",
    "for ax, (layer, head) in zip(axes.flat, layer_heads):\n",
    "    plot_attention_head(attentions, tokens, layer, head, ax)\n",
    "\n",
    "plt.suptitle(f'BERT Attention Patterns: \"{text[:50]}...\"', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Average attention across all heads for each layer\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "n_tokens = len([t for t in tokens if t != '[PAD]'])\n",
    "display_tokens = tokens[:n_tokens]\n",
    "\n",
    "for idx, layer in enumerate([0, 2, 4, 7, 9, 11]):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    avg_attn = attentions[layer].mean(axis=0)[:n_tokens, :n_tokens]\n",
    "    im = ax.imshow(avg_attn, cmap='viridis', vmin=0)\n",
    "    ax.set_title(f\"Layer {layer+1} (avg)\", fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(range(n_tokens))\n",
    "    ax.set_xticklabels(display_tokens, rotation=45, ha='right', fontsize=7)\n",
    "    ax.set_yticks(range(n_tokens))\n",
    "    ax.set_yticklabels(display_tokens, fontsize=7)\n",
    "\n",
    "plt.suptitle(\"Attention Evolves Across Layers\", fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Early layers: broad, local attention patterns\")\n",
    "print(\"   Later layers: more focused, semantically meaningful patterns\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Seg 16 Live Classifier\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/seg_16_live_classifier.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_seg_16_live_classifier"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üéØ Final Output: Live Sentiment Classifier"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive sentiment classifier\n",
    "def classify_sentiment(text, model, tokenizer, device):\n",
    "    \"\"\"Classify text as positive or negative using our fine-tuned BERT.\"\"\"\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)[0]\n",
    "\n",
    "    neg_prob = probs[0].item()\n",
    "    pos_prob = probs[1].item()\n",
    "    label = \"Positive\" if pos_prob > neg_prob else \"Negative\"\n",
    "\n",
    "    return label, pos_prob, neg_prob\n",
    "\n",
    "# Test reviews\n",
    "reviews = [\n",
    "    \"This movie was absolutely fantastic! A masterpiece of cinema.\",\n",
    "    \"Terrible film. I fell asleep halfway through. Total waste of time.\",\n",
    "    \"An okay movie. Some good parts, some boring parts.\",\n",
    "    \"The acting was superb and the story kept me engaged throughout.\",\n",
    "    \"I've never been so bored in my life. The plot made no sense.\",\n",
    "    \"A beautiful and moving film that I will never forget.\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üéØ BERT Sentiment Classifier ‚Äî Live Predictions\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 8))\n",
    "\n",
    "for idx, (review, ax) in enumerate(zip(reviews, axes.flat)):\n",
    "    label, pos_prob, neg_prob = classify_sentiment(review, model, tokenizer, device)\n",
    "    emoji = \"üòä\" if label == \"Positive\" else \"üòû\"\n",
    "\n",
    "    print(f\"\\n{emoji} [{label}] ({max(pos_prob, neg_prob):.1%} confidence)\")\n",
    "    print(f\"   \\\"{review[:60]}{'...' if len(review) > 60 else ''}\\\"\")\n",
    "\n",
    "    # Bar chart\n",
    "    colors = ['coral', 'steelblue']\n",
    "    bars = ax.barh(['Negative', 'Positive'], [neg_prob, pos_prob], color=colors)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_title(f'\"{review[:30]}...\"', fontsize=9)\n",
    "    for bar, prob in zip(bars, [neg_prob, pos_prob]):\n",
    "        ax.text(prob + 0.02, bar.get_y() + bar.get_height()/2,\n",
    "                f'{prob:.1%}', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.suptitle(\"BERT Sentiment Predictions\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéâ Congratulations! You've fine-tuned BERT for sentiment analysis!\")\n",
    "print(\"   With the full IMDB dataset, this approach achieves >93% accuracy.\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Seg 17 Bert Impact\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/seg_17_bert_impact.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_seg_17_bert_impact"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. BERT's Impact and Successors\n",
    "\n",
    "BERT (October 2018) achieved state-of-the-art on **11 NLP benchmarks simultaneously**. Its three key contributions:\n",
    "\n",
    "1. **Bidirectional pre-training works.** Deep bidirectional context produces far richer representations than left-to-right approaches.\n",
    "\n",
    "2. **Pre-train ‚Üí fine-tune paradigm.** One expensive pre-training run produces a foundation model that can be cheaply adapted to any task.\n",
    "\n",
    "3. **Transfer learning for NLP.** BERT did for NLP what ImageNet did for computer vision.\n",
    "\n",
    "BERT inspired a family of successors:\n",
    "- **RoBERTa** (2019): Dropped NSP, trained longer with more data\n",
    "- **ALBERT** (2019): Parameter sharing for efficiency\n",
    "- **DistilBERT** (2019): 60% smaller, 97% of performance via knowledge distillation\n",
    "- **ELECTRA** (2020): More sample-efficient \"replaced token detection\" objective\n",
    "\n",
    "The pre-train ‚Üí fine-tune paradigm BERT established is still the foundation of modern NLP, including GPT-4 and Claude."
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Seg 18 Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/seg_18_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_seg_18_closing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "1. We fine-tuned on only 2,000 examples and still got good results. Why does BERT transfer so well from pre-training? What would happen with only 50 examples?\n",
    "2. We used the [CLS] token for classification. What if we averaged all token representations instead? Would that be better or worse?\n",
    "3. BERT uses 12 attention heads per layer. If you could visualize what each head \"specializes in,\" what types of linguistic patterns might you find?\n",
    "\n",
    "### üèÜ Optional Challenges\n",
    "1. **Named Entity Recognition**: Modify the classification head to predict a label for *every* token (not just [CLS]). Fine-tune on the CoNLL-2003 NER dataset.\n",
    "2. **Question Answering**: Implement the SQuAD fine-tuning setup where BERT predicts the start and end positions of the answer span in a passage.\n",
    "3. **Feature Extraction**: Freeze BERT's weights and train only the classification head. Compare accuracy with full fine-tuning. How much does it drop?\n",
    "4. **Attention Analysis**: For correctly classified positive reviews, find which attention heads most strongly attend to sentiment words like \"great,\" \"amazing,\" \"terrible.\""
   ],
   "id": "cell_31"
  }
 ]
}