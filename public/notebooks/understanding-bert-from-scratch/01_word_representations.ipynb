{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Word Representations & The Need for BERT ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1-OE3rxpruDzmk8pg3auelrL_R46oxuz7\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/00_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_00_intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Word Representations & The Need for BERT\n",
    "\n",
    "*Part 1 of the Vizuara series on Understanding BERT from Scratch*\n",
    "*Estimated time: 45 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ü§ñ AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** ‚Äî it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[üëâ Open AI Teaching Assistant](https://pods.vizuara.ai/courses/understanding-bert-from-scratch/practice/1/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_why_it_matters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Every modern NLP system ‚Äî from ChatGPT to Google Search ‚Äî relies on one fundamental idea: **turning words into numbers** that capture meaning.\n",
    "\n",
    "But here is the catch: the word \"bank\" means completely different things in \"river bank\" and \"bank account.\" How do we represent words so that their meaning changes based on context?\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Build **Word2Vec from scratch** and see why static embeddings fail\n",
    "2. Understand how **ELMo** tried to fix this with bidirectional LSTMs\n",
    "3. See why BERT's approach ‚Äî **deep bidirectional attention** ‚Äî was the breakthrough we needed\n",
    "\n",
    "By the end, you will have a working Word2Vec model and a clear understanding of *why* BERT had to be invented."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Building Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_building_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_building_intuition"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup ‚Äî run this cell first\n",
    "!pip install -q torch matplotlib numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Let us start with a simple game. Look at this sentence:\n",
    "\n",
    "**\"The ___ sat on the mat and purred loudly.\"**\n",
    "\n",
    "You instantly think **cat**. But how? You used the words *after* the blank ‚Äî \"purred loudly\" ‚Äî to figure it out. You read in **both directions**.\n",
    "\n",
    "Now try these two:\n",
    "\n",
    "**\"I went to the ___ to deposit my savings.\"** ‚Üí bank (financial institution)\n",
    "\n",
    "**\"I sat on the ___ of the river and watched the sunset.\"** ‚Üí bank (river edge)\n",
    "\n",
    "Same word, completely different meanings. The surrounding context tells you which \"bank\" is meant.\n",
    "\n",
    "### ü§î Think About This\n",
    "\n",
    "If we represent each word as a single, fixed vector of numbers (like a GPS coordinate for meaning), how would we handle \"bank\"? It would need to be in *two places at once* ‚Äî near \"money\" and near \"river.\" This is the fundamental problem we are going to solve."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Word2Vec Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_word2vec_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_word2vec_math"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics of Word2Vec\n",
    "\n",
    "The most influential static word embedding method is **Word2Vec** (Mikolov et al., 2013). The key idea: *words that appear in similar contexts should have similar vectors.*\n",
    "\n",
    "Word2Vec has two variants. We will implement the **Skip-gram** model, which takes a center word and tries to predict the surrounding context words.\n",
    "\n",
    "Given a center word $w_c$ and a context word $w_o$, the probability that $w_o$ appears in the context of $w_c$ is:\n",
    "\n",
    "$$P(w_o \\mid w_c) = \\frac{\\exp(\\mathbf{u}_{w_o}^T \\mathbf{v}_{w_c})}{\\sum_{w \\in V} \\exp(\\mathbf{u}_w^T \\mathbf{v}_{w_c})}$$\n",
    "\n",
    "Computationally, this says: take the dot product between the context word's output vector $\\mathbf{u}_{w_o}$ and the center word's input vector $\\mathbf{v}_{w_c}$, then normalize over the entire vocabulary using softmax. A higher dot product means the model thinks these words are more likely to appear together.\n",
    "\n",
    "The training objective is to maximize the log-likelihood over all center-context pairs in the corpus:\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log P(w_{t+j} \\mid w_t)$$\n",
    "\n",
    "This says: for every word $w_t$ in the corpus, look at $m$ words to the left and right, and maximize the probability of predicting each of those context words."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Building Corpus\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_building_corpus.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_building_corpus"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It ‚Äî Component by Component\n",
    "\n",
    "### 4.1 Preparing a Small Corpus\n",
    "\n",
    "We will use a small corpus that includes the word \"bank\" in different contexts, so we can later see Word2Vec's polysemy problem."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small corpus with deliberate polysemy\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the rug\",\n",
    "    \"the cat chased the dog\",\n",
    "    \"the dog chased the cat\",\n",
    "    \"i went to the bank to deposit money\",\n",
    "    \"she walked to the bank to withdraw cash\",\n",
    "    \"he sat on the bank of the river\",\n",
    "    \"the river bank was covered with grass\",\n",
    "    \"the cat purred on the mat\",\n",
    "    \"the dog barked at the cat\",\n",
    "    \"money was deposited at the bank\",\n",
    "    \"the river bank had beautiful flowers\",\n",
    "    \"the mat was on the floor\",\n",
    "    \"the rug was under the dog\",\n",
    "    \"cash was withdrawn from the bank\",\n",
    "    \"grass grew along the river bank\",\n",
    "]\n",
    "\n",
    "# Tokenize\n",
    "tokenized_corpus = [sentence.split() for sentence in corpus]\n",
    "\n",
    "# Build vocabulary\n",
    "all_words = [word for sentence in tokenized_corpus for word in sentence]\n",
    "word_counts = Counter(all_words)\n",
    "vocab = sorted(word_counts.keys())\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Vocabulary: {vocab}\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Skipgram Pairs\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_skipgram_pairs.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_skipgram_pairs"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Building Skip-gram Training Pairs\n",
    "\n",
    "For each word in the corpus, we create (center_word, context_word) pairs using a sliding window."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skipgram_pairs(tokenized_corpus, word_to_idx, window_size=2):\n",
    "    \"\"\"\n",
    "    Create (center, context) pairs for Skip-gram training.\n",
    "\n",
    "    For each word in each sentence, look at 'window_size' words\n",
    "    to the left and right as context.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for sentence in tokenized_corpus:\n",
    "        for i, center_word in enumerate(sentence):\n",
    "            # Look at window_size words in each direction\n",
    "            for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
    "                if i != j:  # Skip the center word itself\n",
    "                    context_word = sentence[j]\n",
    "                    pairs.append((word_to_idx[center_word], word_to_idx[context_word]))\n",
    "    return pairs\n",
    "\n",
    "pairs = create_skipgram_pairs(tokenized_corpus, word_to_idx, window_size=2)\n",
    "print(f\"Total training pairs: {len(pairs)}\")\n",
    "print(f\"\\nFirst 5 pairs:\")\n",
    "for center_idx, context_idx in pairs[:5]:\n",
    "    print(f\"  Center: '{idx_to_word[center_idx]}' ‚Üí Context: '{idx_to_word[context_idx]}'\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization: distribution of training pairs\n",
    "center_words = [idx_to_word[p[0]] for p in pairs]\n",
    "center_counts = Counter(center_words)\n",
    "top_words = center_counts.most_common(10)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar([w for w, c in top_words], [c for w, c in top_words], color='steelblue')\n",
    "plt.title(\"Most Common Center Words in Training Pairs\")\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Number of training pairs\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: The Model\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_the_model.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_the_model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The Word2Vec Model (from Scratch)"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramWord2Vec(nn.Module):\n",
    "    \"\"\"\n",
    "    Skip-gram Word2Vec model.\n",
    "\n",
    "    Two embedding matrices:\n",
    "    - center_embeddings: vectors for center words (input)\n",
    "    - context_embeddings: vectors for context words (output)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.center_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Initialize with small random values\n",
    "        nn.init.uniform_(self.center_embeddings.weight, -0.5 / embedding_dim, 0.5 / embedding_dim)\n",
    "        nn.init.uniform_(self.context_embeddings.weight, -0.5 / embedding_dim, 0.5 / embedding_dim)\n",
    "\n",
    "    def forward(self, center_ids, context_ids):\n",
    "        # Get embeddings: (batch_size, embedding_dim)\n",
    "        center_vecs = self.center_embeddings(center_ids)\n",
    "        context_vecs = self.context_embeddings(context_ids)\n",
    "\n",
    "        # Dot product for each pair: (batch_size,)\n",
    "        scores = torch.sum(center_vecs * context_vecs, dim=1)\n",
    "\n",
    "        # Compute log-softmax over entire vocabulary\n",
    "        # For full softmax: score of center with ALL context words\n",
    "        all_context = self.context_embeddings.weight  # (vocab_size, embedding_dim)\n",
    "        all_scores = torch.matmul(center_vecs, all_context.T)  # (batch_size, vocab_size)\n",
    "\n",
    "        log_probs = torch.log_softmax(all_scores, dim=1)\n",
    "\n",
    "        # Gather the log probabilities for the actual context words\n",
    "        loss = -log_probs.gather(1, context_ids.unsqueeze(1)).squeeze(1)\n",
    "        return loss.mean()\n",
    "\n",
    "    def get_embedding(self, word_idx):\n",
    "        \"\"\"Get the learned embedding for a word.\"\"\"\n",
    "        return self.center_embeddings.weight[word_idx].detach().cpu().numpy()"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Training Word2Vec"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 32\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "model = SkipGramWord2Vec(vocab_size, EMBEDDING_DIM).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Convert pairs to tensors\n",
    "center_ids = torch.tensor([p[0] for p in pairs], dtype=torch.long).to(device)\n",
    "context_ids = torch.tensor([p[1] for p in pairs], dtype=torch.long).to(device)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    # Shuffle data\n",
    "    perm = torch.randperm(len(pairs))\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    for i in range(0, len(pairs), BATCH_SIZE):\n",
    "        batch_idx = perm[i:i+BATCH_SIZE]\n",
    "        batch_centers = center_ids[batch_idx]\n",
    "        batch_contexts = context_ids[batch_idx]\n",
    "\n",
    "        loss = model(batch_centers, batch_contexts)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Training curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses, color='steelblue', linewidth=1.5)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Word2Vec Training Loss\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Visualizing Embeddings\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_visualizing_embeddings.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_visualizing_embeddings"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Visualizing the Embeddings"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all word embeddings\n",
    "embeddings = model.center_embeddings.weight.detach().cpu().numpy()\n",
    "\n",
    "# Use PCA to project to 2D\n",
    "from numpy.linalg import svd\n",
    "\n",
    "# Center the data\n",
    "mean = embeddings.mean(axis=0)\n",
    "centered = embeddings - mean\n",
    "U, S, Vt = svd(centered, full_matrices=False)\n",
    "projected = centered @ Vt[:2].T  # Project to first 2 principal components\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, word in enumerate(vocab):\n",
    "    x, y = projected[i]\n",
    "    plt.scatter(x, y, color='steelblue', s=50, zorder=5)\n",
    "    plt.annotate(word, (x, y), fontsize=9, ha='center', va='bottom',\n",
    "                 xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "plt.title(\"Word2Vec Embeddings (PCA projection)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Polysemy Problem\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_polysemy_problem.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_polysemy_problem"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Polysemy Problem ‚Äî Word2Vec's Fatal Flaw\n",
    "\n",
    "Now let us see the fundamental limitation. The word \"bank\" appears in two very different contexts in our corpus ‚Äî financial and river. But Word2Vec gives it **one single vector**."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the embedding for \"bank\"\n",
    "bank_idx = word_to_idx[\"bank\"]\n",
    "bank_embedding = model.get_embedding(bank_idx)\n",
    "\n",
    "# Find nearest neighbors using cosine similarity\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "print(\"=== Nearest neighbors to 'bank' ===\\n\")\n",
    "similarities = []\n",
    "for word, idx in word_to_idx.items():\n",
    "    if word != \"bank\":\n",
    "        sim = cosine_similarity(bank_embedding, model.get_embedding(idx))\n",
    "        similarities.append((word, sim))\n",
    "\n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "for word, sim in similarities[:8]:\n",
    "    print(f\"  {word:12s} ‚Üí similarity: {sim:.3f}\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä The polysemy problem visualized\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: financial context sentences\n",
    "financial_words = [\"bank\", \"money\", \"deposit\", \"cash\", \"withdrawn\"]\n",
    "river_words = [\"bank\", \"river\", \"grass\", \"flowers\", \"grew\"]\n",
    "\n",
    "# Show that bank is the SAME point regardless of context\n",
    "financial_indices = [word_to_idx[w] for w in financial_words if w in word_to_idx]\n",
    "river_indices = [word_to_idx[w] for w in river_words if w in word_to_idx]\n",
    "\n",
    "# Financial context\n",
    "ax = axes[0]\n",
    "for idx in financial_indices:\n",
    "    word = idx_to_word[idx]\n",
    "    x, y = projected[idx]\n",
    "    color = 'red' if word == 'bank' else 'steelblue'\n",
    "    size = 150 if word == 'bank' else 80\n",
    "    ax.scatter(x, y, color=color, s=size, zorder=5)\n",
    "    ax.annotate(word, (x, y), fontsize=11, ha='center', va='bottom',\n",
    "                xytext=(0, 6), textcoords='offset points', fontweight='bold' if word == 'bank' else 'normal')\n",
    "ax.set_title(\"Financial Context Words\", fontsize=13)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# River context\n",
    "ax = axes[1]\n",
    "for idx in river_indices:\n",
    "    word = idx_to_word[idx]\n",
    "    x, y = projected[idx]\n",
    "    color = 'red' if word == 'bank' else 'forestgreen'\n",
    "    size = 150 if word == 'bank' else 80\n",
    "    ax.scatter(x, y, color=color, s=size, zorder=5)\n",
    "    ax.annotate(word, (x, y), fontsize=11, ha='center', va='bottom',\n",
    "                xytext=(0, 6), textcoords='offset points', fontweight='bold' if word == 'bank' else 'normal')\n",
    "ax.set_title(\"River Context Words\", fontsize=13)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"‚ö†Ô∏è 'bank' has ONE vector ‚Äî it cannot distinguish contexts!\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how \"bank\" sits at the **exact same position** in both plots. Word2Vec has no way to give \"bank\" a different representation based on its context. This is the **polysemy problem**."
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo Analogy\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_todo_analogy.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_11_todo_analogy"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üîß Your Turn\n",
    "\n",
    "### TODO: Implement a function to compute Word2Vec analogy\n",
    "\n",
    "One of the famous properties of Word2Vec is that it captures analogies:\n",
    "**king - man + woman ‚âà queen**\n",
    "\n",
    "Implement the analogy function below."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_analogy(model, word_to_idx, idx_to_word, word_a, word_b, word_c):\n",
    "    \"\"\"\n",
    "    Compute: word_a - word_b + word_c = ???\n",
    "\n",
    "    For example: king - man + woman = ???\n",
    "\n",
    "    Args:\n",
    "        word_a, word_b, word_c: strings\n",
    "    Returns:\n",
    "        The word closest to (vec_a - vec_b + vec_c)\n",
    "    \"\"\"\n",
    "    vec_a = model.get_embedding(word_to_idx[word_a])\n",
    "    vec_b = model.get_embedding(word_to_idx[word_b])\n",
    "    vec_c = model.get_embedding(word_to_idx[word_c])\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Compute the analogy vector: vec_a - vec_b + vec_c\n",
    "    # Step 2: Find the word in vocabulary whose embedding is most similar\n",
    "    #         (using cosine similarity) to the analogy vector\n",
    "    # Step 3: Exclude words a, b, c from candidates\n",
    "    # ==============================\n",
    "\n",
    "    analogy_vec = ???  # YOUR CODE HERE\n",
    "\n",
    "    best_word = None\n",
    "    best_sim = -1\n",
    "\n",
    "    for word, idx in word_to_idx.items():\n",
    "        if word in [word_a, word_b, word_c]:\n",
    "            continue\n",
    "        # YOUR CODE HERE: compute similarity and track the best match\n",
    "\n",
    "    return best_word"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification\n",
    "# With our small corpus, exact analogies are unlikely, but the function should work\n",
    "# Let's test the mechanics: \"cat\" - \"mat\" + \"rug\" should lean toward \"dog\"\n",
    "result = word_analogy(model, word_to_idx, idx_to_word, \"cat\", \"mat\", \"rug\")\n",
    "print(f\"cat - mat + rug = {result}\")\n",
    "assert result is not None, \"‚ùå Function returned None ‚Äî check your implementation\"\n",
    "assert isinstance(result, str), \"‚ùå Function should return a string (word)\"\n",
    "print(\"‚úÖ Analogy function works correctly!\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Elmo Approach\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_elmo_approach.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_12_elmo_approach"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. From Static to Contextual: The ELMo Approach\n",
    "\n",
    "ELMo (Peters et al., 2018) tried to fix the polysemy problem by using **bidirectional LSTMs**.\n",
    "\n",
    "The idea: run a left-to-right LSTM and a right-to-left LSTM over the sentence, then **concatenate** their hidden states to get a context-dependent representation."
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating the ELMo concept (simplified)\n",
    "\n",
    "class SimpleELMo(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified ELMo-style model.\n",
    "\n",
    "    Two independent LSTMs:\n",
    "    - forward LSTM: reads left-to-right\n",
    "    - backward LSTM: reads right-to-left\n",
    "\n",
    "    Their hidden states are concatenated (NOT jointly trained).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.forward_lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.backward_lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Get token embeddings\n",
    "        embeds = self.embedding(input_ids)  # (batch, seq_len, embed_dim)\n",
    "\n",
    "        # Forward LSTM (left-to-right)\n",
    "        forward_out, _ = self.forward_lstm(embeds)\n",
    "\n",
    "        # Backward LSTM (right-to-left) ‚Äî reverse the sequence\n",
    "        reversed_embeds = torch.flip(embeds, dims=[1])\n",
    "        backward_out, _ = self.backward_lstm(reversed_embeds)\n",
    "        backward_out = torch.flip(backward_out, dims=[1])  # Flip back\n",
    "\n",
    "        # Concatenate (this is ELMo's \"shallow bidirectionality\")\n",
    "        contextual = torch.cat([forward_out, backward_out], dim=-1)\n",
    "        return contextual\n",
    "\n",
    "# Create a simple ELMo\n",
    "elmo = SimpleELMo(vocab_size, embedding_dim=32, hidden_dim=32)\n",
    "\n",
    "# Get contextual representations for two sentences with \"bank\"\n",
    "sentence1 = \"i went to the bank to deposit money\".split()\n",
    "sentence2 = \"he sat on the bank of the river\".split()\n",
    "\n",
    "ids1 = torch.tensor([[word_to_idx[w] for w in sentence1]])\n",
    "ids2 = torch.tensor([[word_to_idx[w] for w in sentence2]])\n",
    "\n",
    "with torch.no_grad():\n",
    "    ctx1 = elmo(ids1)  # (1, 8, 64)\n",
    "    ctx2 = elmo(ids2)  # (1, 8, 64)\n",
    "\n",
    "# \"bank\" is at index 4 in both sentences\n",
    "bank_repr_financial = ctx1[0, 4].numpy()\n",
    "bank_repr_river = ctx2[0, 4].numpy()\n",
    "\n",
    "sim = cosine_similarity(bank_repr_financial, bank_repr_river)\n",
    "print(f\"Cosine similarity between 'bank' representations:\")\n",
    "print(f\"  Financial context vs. River context: {sim:.4f}\")\n",
    "print(f\"\\nüí° ELMo gives 'bank' DIFFERENT representations based on context!\")\n",
    "print(f\"   (similarity < 1.0 means the vectors are different)\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Elmo Visualization\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_elmo_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_13_elmo_visualization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Comparison: Word2Vec vs ELMo\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Word2Vec: same vector\n",
    "ax = axes[0]\n",
    "ax.bar(range(10), bank_embedding[:10], color='coral', alpha=0.8, label='All contexts')\n",
    "ax.set_title(\"Word2Vec: 'bank' embedding\\n(SAME for all contexts)\", fontsize=11)\n",
    "ax.set_xlabel(\"Dimension\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "ax.legend()\n",
    "ax.set_ylim(-2, 2)\n",
    "\n",
    "# ELMo: different vectors\n",
    "ax = axes[1]\n",
    "x = np.arange(10)\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, bank_repr_financial[:10], width, color='steelblue', alpha=0.8, label='Financial context')\n",
    "ax.bar(x + width/2, bank_repr_river[:10], width, color='forestgreen', alpha=0.8, label='River context')\n",
    "ax.set_title(\"ELMo: 'bank' embedding\\n(DIFFERENT per context)\", fontsize=11)\n",
    "ax.set_xlabel(\"Dimension\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "ax.legend()\n",
    "ax.set_ylim(-2, 2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo Context Similarity\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_todo_context_similarity.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_14_todo_context_similarity"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Build a Context Similarity Checker\n",
    "\n",
    "Use the ELMo model above to check how different the representation of \"bank\" is in two different contexts."
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_similarity(model, sentence1, sentence2, target_word, word_to_idx):\n",
    "    \"\"\"\n",
    "    Compare the contextual representation of target_word in two sentences.\n",
    "\n",
    "    Args:\n",
    "        model: SimpleELMo model\n",
    "        sentence1: first sentence (string)\n",
    "        sentence2: second sentence (string)\n",
    "        target_word: the word to compare (string)\n",
    "\n",
    "    Returns:\n",
    "        cosine_similarity: float between -1 and 1\n",
    "        repr1: numpy array ‚Äî representation in sentence 1\n",
    "        repr2: numpy array ‚Äî representation in sentence 2\n",
    "    \"\"\"\n",
    "    words1 = sentence1.split()\n",
    "    words2 = sentence2.split()\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Find the index of target_word in each sentence\n",
    "    # Step 2: Convert each sentence to tensor of token IDs\n",
    "    # Step 3: Pass each through the model to get contextual representations\n",
    "    # Step 4: Extract the representation at the target_word's position\n",
    "    # Step 5: Compute cosine similarity between the two representations\n",
    "    # ==============================\n",
    "\n",
    "    idx1 = ???  # YOUR CODE HERE: position of target_word in sentence1\n",
    "    idx2 = ???  # YOUR CODE HERE: position of target_word in sentence2\n",
    "\n",
    "    ids1 = torch.tensor([[word_to_idx[w] for w in words1]])\n",
    "    ids2 = torch.tensor([[word_to_idx[w] for w in words2]])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ctx1 = model(ids1)\n",
    "        ctx2 = model(ids2)\n",
    "\n",
    "    repr1 = ???  # YOUR CODE HERE: extract at idx1\n",
    "    repr2 = ???  # YOUR CODE HERE: extract at idx2\n",
    "\n",
    "    similarity = ???  # YOUR CODE HERE: cosine similarity\n",
    "\n",
    "    return similarity, repr1, repr2"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification\n",
    "sim, r1, r2 = context_similarity(\n",
    "    elmo,\n",
    "    \"i went to the bank to deposit money\",\n",
    "    \"he sat on the bank of the river\",\n",
    "    \"bank\",\n",
    "    word_to_idx\n",
    ")\n",
    "assert isinstance(sim, (float, np.floating)), \"‚ùå Should return a float\"\n",
    "assert -1 <= sim <= 1, f\"‚ùå Cosine similarity should be in [-1, 1], got {sim}\"\n",
    "print(f\"‚úÖ Context similarity works! 'bank' similarity = {sim:.4f}\")\n",
    "print(f\"   (Lower similarity = model distinguishes the contexts better)\")"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Elmo Limitation Bert Preview\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/15_elmo_limitation_bert_preview.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_15_elmo_limitation_bert_preview"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But ELMo has a limitation...\n",
    "\n",
    "ELMo's two LSTMs are trained **independently**. The forward LSTM never sees what the backward LSTM is doing, and vice versa. They are glued together at the end via concatenation ‚Äî the bidirectionality is **shallow**.\n",
    "\n",
    "What we really want is a model where **every layer** jointly considers the full left and right context. That is exactly what BERT does with self-attention, which we will build in the next notebook."
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/16_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_16_closing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üéØ Final Output: The Evolution of Word Representations"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Summary visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Panel 1: Word2Vec (static)\n",
    "ax = axes[0]\n",
    "ax.text(0.5, 0.7, '\"bank\"', ha='center', va='center', fontsize=24, fontweight='bold', color='coral')\n",
    "ax.text(0.5, 0.4, '‚Üí ONE vector', ha='center', va='center', fontsize=14, color='gray')\n",
    "ax.text(0.5, 0.25, 'Same for \"river bank\"', ha='center', va='center', fontsize=10, color='gray')\n",
    "ax.text(0.5, 0.15, 'and \"bank account\"', ha='center', va='center', fontsize=10, color='gray')\n",
    "ax.set_title(\"Word2Vec (2013)\\nStatic Embeddings\", fontsize=13, fontweight='bold')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "ax.patch.set_facecolor('#fff0f0')\n",
    "\n",
    "# Panel 2: ELMo (shallow bidirectional)\n",
    "ax = axes[1]\n",
    "ax.annotate('', xy=(0.35, 0.6), xytext=(0.1, 0.6),\n",
    "            arrowprops=dict(arrowstyle='->', color='steelblue', lw=2))\n",
    "ax.annotate('', xy=(0.65, 0.55), xytext=(0.9, 0.55),\n",
    "            arrowprops=dict(arrowstyle='->', color='orange', lw=2))\n",
    "ax.text(0.5, 0.7, '\"bank\"', ha='center', va='center', fontsize=24, fontweight='bold', color='purple')\n",
    "ax.text(0.5, 0.35, '‚Üí Context-dependent', ha='center', va='center', fontsize=14, color='gray')\n",
    "ax.text(0.5, 0.2, 'but L‚ÜíR and R‚ÜíL are', ha='center', va='center', fontsize=10, color='gray')\n",
    "ax.text(0.5, 0.1, 'trained separately (shallow)', ha='center', va='center', fontsize=10, color='gray')\n",
    "ax.set_title(\"ELMo (2018)\\nShallow Bidirectional\", fontsize=13, fontweight='bold')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "ax.patch.set_facecolor('#f0f0ff')\n",
    "\n",
    "# Panel 3: BERT (deep bidirectional)\n",
    "ax = axes[2]\n",
    "# Draw arrows in all directions\n",
    "center = (0.5, 0.6)\n",
    "for angle in range(0, 360, 45):\n",
    "    rad = np.radians(angle)\n",
    "    dx = 0.15 * np.cos(rad)\n",
    "    dy = 0.15 * np.sin(rad)\n",
    "    ax.annotate('', xy=(center[0]+dx, center[1]+dy), xytext=center,\n",
    "                arrowprops=dict(arrowstyle='->', color='green', lw=1.5))\n",
    "ax.text(0.5, 0.6, '\"bank\"', ha='center', va='center', fontsize=24, fontweight='bold', color='green',\n",
    "        bbox=dict(boxstyle='round', facecolor='white', edgecolor='green', linewidth=2))\n",
    "ax.text(0.5, 0.25, '‚Üí Deep bidirectional', ha='center', va='center', fontsize=14, color='gray')\n",
    "ax.text(0.5, 0.1, 'Every layer attends to\\nfull context jointly', ha='center', va='center', fontsize=10, color='gray')\n",
    "ax.set_title(\"BERT (2018)\\nDeep Bidirectional\", fontsize=13, fontweight='bold')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "ax.patch.set_facecolor('#f0fff0')\n",
    "\n",
    "plt.suptitle(\"The Evolution of Word Representations\", fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéâ You now understand WHY static embeddings fail and WHY we need BERT!\")\n",
    "print(\"   Next up: building the self-attention mechanism that makes BERT possible.\")"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "1. Why can't we just increase Word2Vec's embedding dimension to solve the polysemy problem? (Hint: think about what the model is optimizing for.)\n",
    "2. If ELMo concatenates forward and backward LSTMs, why is that considered \"shallow\" bidirectionality? What would \"deep\" bidirectionality look like?\n",
    "3. Word2Vec was trained on billions of words but still has the polysemy problem. Does more data help, or is it a fundamental architectural limitation?\n",
    "\n",
    "### üèÜ Optional Challenges\n",
    "1. **Negative Sampling**: Our Word2Vec uses full softmax, which is slow for large vocabularies. Implement negative sampling ‚Äî instead of normalizing over all words, randomly sample 5-10 \"negative\" context words and train a binary classifier.\n",
    "2. **CBOW Model**: Implement the Continuous Bag of Words (CBOW) variant, which predicts the center word from the context words (the reverse of Skip-gram).\n",
    "3. **Larger Corpus**: Download a real text corpus (e.g., WikiText-2) and train Word2Vec on it. Do the embeddings capture more meaningful relationships?"
   ],
   "id": "cell_34"
  }
 ]
}