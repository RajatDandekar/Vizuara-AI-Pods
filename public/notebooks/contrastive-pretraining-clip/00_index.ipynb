{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Contrastive Pretraining (CLIP-style) -- Index -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Pretraining (CLIP-style) -- Notebook Series\n",
    "\n",
    "*A Vizuara learning path for understanding CLIP from first principles*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Path\n",
    "\n",
    "This series of 3 notebooks takes you from the foundations of contrastive learning to building and evaluating a complete CLIP model.\n",
    "\n",
    "### Notebook 1: Contrastive Learning and the InfoNCE Loss\n",
    "- **Estimated time:** 45 minutes\n",
    "- **Prerequisites:** Basic PyTorch, linear algebra\n",
    "- **What you will learn:**\n",
    "  - The intuition behind contrastive learning\n",
    "  - Cosine similarity and embedding spaces\n",
    "  - The InfoNCE loss function from scratch\n",
    "  - Training a contrastive encoder on synthetic data\n",
    "- **Final output:** A trained contrastive encoder with visualized embedding clusters\n",
    "\n",
    "### Notebook 2: Building CLIP from Scratch\n",
    "- **Estimated time:** 60 minutes\n",
    "- **Prerequisites:** Notebook 1, basic understanding of Transformers\n",
    "- **What you will learn:**\n",
    "  - Vision Transformer (ViT) image encoder\n",
    "  - Transformer text encoder\n",
    "  - The complete CLIP dual-encoder architecture\n",
    "  - Training CLIP on CIFAR-10 with synthetic captions\n",
    "- **Final output:** A trained Mini-CLIP model with t-SNE visualization\n",
    "\n",
    "### Notebook 3: Zero-Shot Transfer and Evaluation\n",
    "- **Estimated time:** 45 minutes\n",
    "- **Prerequisites:** Notebooks 1 and 2\n",
    "- **What you will learn:**\n",
    "  - Zero-shot classification using text prompts\n",
    "  - Prompt engineering and ensembling\n",
    "  - Image retrieval with CLIP\n",
    "  - Understanding CLIP's limitations\n",
    "- **Final output:** Full evaluation with confusion matrix and retrieval demos"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use These Notebooks"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Welcome to the Vizuara CLIP series!\")\n",
    "print()\n",
    "print(\"Each notebook is designed to run in Google Colab with a T4 GPU.\")\n",
    "print(\"Training times are under 10 minutes per notebook.\")\n",
    "print()\n",
    "print(\"Start with Notebook 1 and work through them in order.\")\n",
    "print(\"Each notebook builds on concepts from the previous one.\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key References\n",
    "\n",
    "- Radford et al., \"Learning Transferable Visual Models From Natural Language Supervision\" (2021) -- The original CLIP paper\n",
    "- OpenCLIP: https://github.com/mlfoundations/open_clip -- Open-source implementation\n",
    "- Dosovitskiy et al., \"An Image is Worth 16x16 Words\" (2020) -- Vision Transformer (ViT)"
   ],
   "id": "cell_5"
  }
 ]
}