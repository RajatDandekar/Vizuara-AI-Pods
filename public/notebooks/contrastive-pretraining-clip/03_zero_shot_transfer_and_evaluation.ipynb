{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Zero-Shot Transfer and Evaluation -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Transfer and Evaluation: The Magic of CLIP\n",
    "\n",
    "*Part 3 of the Vizuara series on Contrastive Pretraining (CLIP-style)*\n",
    "*Estimated time: 45 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "The most remarkable property of CLIP is **zero-shot classification** -- classifying images into categories the model has never been explicitly trained on. No fine-tuning, no labeled data, just natural language prompts.\n",
    "\n",
    "This is a paradigm shift: instead of training a separate classifier for every task, you describe the task in words and CLIP figures out the rest.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Implement zero-shot classification using CLIP embeddings\n",
    "- Evaluate performance on unseen test data\n",
    "- Engineer prompts to improve accuracy\n",
    "- Build an image retrieval system\n",
    "- Understand CLIP's limitations through targeted experiments"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and GPU check\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "CIFAR10_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                    'dog', 'frog', 'horse', 'ship', 'truck']"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Let us think about how zero-shot classification works with a simple analogy.\n",
    "\n",
    "Imagine you walk into a room full of paintings. You have never seen these particular paintings before, but someone hands you a note that says \"Find the painting of a dog.\" You look around, compare each painting against your mental image of \"dog,\" and point to the one that matches best.\n",
    "\n",
    "CLIP does exactly this, but mathematically:\n",
    "1. Embed the image into a vector\n",
    "2. Embed \"a photo of a dog\" into a vector\n",
    "3. Compare them using cosine similarity\n",
    "4. Pick the label with the highest similarity\n",
    "\n",
    "The key insight is that the text encoder provides a **flexible classifier interface**. You can define any set of classes just by writing text prompts.\n",
    "\n",
    "### Think About This\n",
    "- What if we used \"a bright photograph of a cute dog\" instead of \"a photo of a dog\"? Would it matter?\n",
    "- Why might CLIP struggle with the difference between \"two cats\" and \"three cats\"?\n",
    "- Could you use CLIP for sentiment classification? How would you phrase the prompts?"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Zero-Shot Prediction\n",
    "\n",
    "Given an image $x$ and a set of $K$ class labels $\\{c_1, c_2, \\ldots, c_K\\}$, zero-shot prediction works as follows:\n",
    "\n",
    "1. Create text prompts: $t_k = \\text{\"a photo of a } c_k\\text{\"}$\n",
    "2. Encode: $z_I = f(x)$, $z_{T_k} = g(t_k)$ for each $k$\n",
    "3. Normalize: $\\hat{z}_I = z_I / \\|z_I\\|$, $\\hat{z}_{T_k} = z_{T_k} / \\|z_{T_k}\\|$\n",
    "4. Predict: $\\hat{y} = \\arg\\max_k \\frac{\\exp(\\hat{z}_I \\cdot \\hat{z}_{T_k} / \\tau)}{\\sum_{j=1}^{K} \\exp(\\hat{z}_I \\cdot \\hat{z}_{T_j} / \\tau)}$\n",
    "\n",
    "Computationally, this is a softmax over the similarities between the image embedding and each class text embedding, with temperature scaling.\n",
    "\n",
    "Let us work through a simple example with 3 classes. Suppose the similarities are:\n",
    "- $\\text{sim}(\\text{image}, \\text{\"dog\"}) = 0.85$\n",
    "- $\\text{sim}(\\text{image}, \\text{\"cat\"}) = 0.60$\n",
    "- $\\text{sim}(\\text{image}, \\text{\"car\"}) = 0.10$\n",
    "\n",
    "With $\\tau = 0.07$:\n",
    "$$p(\\text{dog}) = \\frac{e^{0.85/0.07}}{e^{0.85/0.07} + e^{0.60/0.07} + e^{0.10/0.07}} = \\frac{e^{12.14}}{e^{12.14} + e^{8.57} + e^{1.43}}$$\n",
    "\n",
    "Since $e^{12.14} \\gg e^{8.57} \\gg e^{1.43}$, essentially all probability mass goes to \"dog.\" This is exactly what we want -- the model is very confident about the correct class.\n",
    "\n",
    "### Prompt Ensembling\n",
    "\n",
    "A simple trick to improve accuracy: use multiple prompt templates per class and average the embeddings:\n",
    "\n",
    "$$\\hat{z}_{T_k} = \\frac{1}{M}\\sum_{m=1}^{M} g(\\text{template}_m(c_k))$$\n",
    "\n",
    "This reduces sensitivity to any single prompt wording."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 Rebuild the Model Architecture\n",
    "\n",
    "We need the same architecture from Notebook 2. Let us rebuild it."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim) * 0.02)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim) * 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.projection(x).flatten(2).transpose(1, 2)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        return x + self.pos_embed\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=128, num_heads=4, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)), nn.GELU(),\n",
    "            nn.Dropout(dropout), nn.Linear(int(embed_dim * mlp_ratio), embed_dim), nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n",
    "        x = x + attn_out\n",
    "        return x + self.mlp(self.norm2(x))\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, embed_dim=128, depth=4, num_heads=4, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(embed_dim, num_heads) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.projection = nn.Linear(embed_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.projection(self.norm(x)[:, 0])\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size=200, max_len=12, embed_dim=128, depth=2, num_heads=4, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_len, embed_dim) * 0.02)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(embed_dim, num_heads) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.projection = nn.Linear(embed_dim, output_dim)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        x = self.token_embed(tokens) + self.pos_embed[:, :tokens.size(1)]\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.projection(self.norm(x).mean(dim=1))\n",
    "\n",
    "class MiniCLIP(nn.Module):\n",
    "    def __init__(self, image_encoder, text_encoder, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.logit_scale = nn.Parameter(torch.log(torch.tensor(1.0 / temperature)))\n",
    "\n",
    "    def forward(self, images, tokens):\n",
    "        img_emb = F.normalize(self.image_encoder(images), dim=-1)\n",
    "        txt_emb = F.normalize(self.text_encoder(tokens), dim=-1)\n",
    "        logit_scale = self.logit_scale.exp().clamp(max=100)\n",
    "        return logit_scale * (img_emb @ txt_emb.T), img_emb, txt_emb\n",
    "\n",
    "    def clip_loss(self, logits):\n",
    "        labels = torch.arange(logits.size(0), device=logits.device)\n",
    "        return (F.cross_entropy(logits, labels) + F.cross_entropy(logits.T, labels)) / 2\n",
    "\n",
    "print(\"Architecture rebuilt.\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Simple Tokenizer"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, max_len=12):\n",
    "        self.max_len = max_len\n",
    "        self.word_to_idx = {\"<PAD>\": 0}\n",
    "        # Build vocab from all possible caption words\n",
    "        all_words = set()\n",
    "        templates = [\"a photo of a {}\", \"an image of a {}\", \"a picture of a {}\",\n",
    "                     \"a blurry photo of a {}\", \"a close-up photo of a {}\"]\n",
    "        for cls in CIFAR10_CLASSES:\n",
    "            for tmpl in templates:\n",
    "                for w in tmpl.format(cls).lower().split():\n",
    "                    all_words.add(w)\n",
    "        for idx, word in enumerate(sorted(all_words), 1):\n",
    "            self.word_to_idx[word] = idx\n",
    "        self.vocab_size = len(self.word_to_idx)\n",
    "\n",
    "    def encode(self, text):\n",
    "        words = text.lower().split()\n",
    "        tokens = [self.word_to_idx.get(w, 0) for w in words]\n",
    "        if len(tokens) < self.max_len:\n",
    "            tokens += [0] * (self.max_len - len(tokens))\n",
    "        return torch.tensor(tokens[:self.max_len])\n",
    "\n",
    "    def batch_encode(self, texts):\n",
    "        return torch.stack([self.encode(t) for t in texts])\n",
    "\n",
    "tokenizer = SimpleTokenizer()\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Train the Model (Quick Training)"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=2)\n",
    "\n",
    "# Initialize model\n",
    "clip_model = MiniCLIP(\n",
    "    ImageEncoder(output_dim=128),\n",
    "    TextEncoder(vocab_size=tokenizer.vocab_size, max_len=12, output_dim=128),\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(clip_model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "CAPTION_TEMPLATES = [\"a photo of a {}\", \"an image of a {}\", \"a picture of a {}\"]\n",
    "\n",
    "def get_caption(label_idx):\n",
    "    cls = CIFAR10_CLASSES[label_idx]\n",
    "    tmpl = CAPTION_TEMPLATES[np.random.randint(len(CAPTION_TEMPLATES))]\n",
    "    return tmpl.format(cls)"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 10 epochs\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    clip_model.train()\n",
    "    total_loss = 0\n",
    "    n = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        captions = [get_caption(l.item()) for l in labels]\n",
    "        tokens = tokenizer.batch_encode(captions).to(device)\n",
    "\n",
    "        logits, _, _ = clip_model(images, tokens)\n",
    "        loss = clip_model.clip_loss(logits)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        n += 1\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {total_loss/n:.4f}\")\n",
    "print(\"Training complete!\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO: Implement Zero-Shot Classification"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_classify(clip_model, images, class_names, tokenizer, templates=None):\n",
    "    \"\"\"\n",
    "    Classify images using zero-shot text prompts.\n",
    "\n",
    "    Args:\n",
    "        clip_model: trained MiniCLIP model\n",
    "        images: (B, C, H, W) image tensor\n",
    "        class_names: list of class name strings\n",
    "        tokenizer: text tokenizer\n",
    "        templates: list of prompt templates (default: [\"a photo of a {}\"])\n",
    "\n",
    "    Returns:\n",
    "        predictions: (B,) tensor of predicted class indices\n",
    "        probabilities: (B, K) tensor of class probabilities\n",
    "    \"\"\"\n",
    "    if templates is None:\n",
    "        templates = [\"a photo of a {}\"]\n",
    "\n",
    "    clip_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # ============ TODO ============\n",
    "        # Step 1: Encode all images through clip_model.image_encoder\n",
    "        # Step 2: Normalize image embeddings\n",
    "        # Step 3: For each class, create text prompts using templates\n",
    "        #         Average the embeddings across templates (prompt ensembling)\n",
    "        # Step 4: Normalize text embeddings\n",
    "        # Step 5: Compute similarity matrix (images x classes)\n",
    "        # Step 6: Apply softmax to get probabilities\n",
    "        # Step 7: Return argmax predictions and probabilities\n",
    "        # ==============================\n",
    "\n",
    "        predictions = ???  # YOUR CODE HERE\n",
    "        probabilities = ???  # YOUR CODE HERE\n",
    "\n",
    "    return predictions, probabilities"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "\n",
    "preds, probs = zero_shot_classify(clip_model, test_images.to(device),\n",
    "                                   CIFAR10_CLASSES, tokenizer)\n",
    "\n",
    "# Check shapes\n",
    "assert preds.shape == (100,), f\"Expected shape (100,), got {preds.shape}\"\n",
    "assert probs.shape == (100, 10), f\"Expected shape (100, 10), got {probs.shape}\"\n",
    "\n",
    "acc = (preds.cpu() == test_labels).float().mean()\n",
    "print(f\"Zero-shot accuracy on 100 test images: {acc:.1%}\")\n",
    "print(\"(Even modest accuracy shows the model learned meaningful representations!)\")\n",
    "print(\"Correct implementation!\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implement Prompt Ensembling"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_prompt_ensemble(clip_model, test_loader, class_names,\n",
    "                                    tokenizer, template_sets):\n",
    "    \"\"\"\n",
    "    Evaluate zero-shot accuracy using different prompt template sets.\n",
    "\n",
    "    Args:\n",
    "        template_sets: dict of {name: [template_list]}\n",
    "\n",
    "    Returns:\n",
    "        dict of {name: accuracy}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for name, templates in template_sets.items():\n",
    "        # ============ TODO ============\n",
    "        # Step 1: Loop over all batches in test_loader\n",
    "        # Step 2: For each batch, call zero_shot_classify with the templates\n",
    "        # Step 3: Compare predictions with ground truth labels\n",
    "        # Step 4: Compute overall accuracy\n",
    "        # ==============================\n",
    "\n",
    "        accuracy = ???  # YOUR CODE HERE\n",
    "        results[name] = accuracy\n",
    "        print(f\"  {name}: {accuracy:.1%}\")\n",
    "\n",
    "    return results"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "template_sets = {\n",
    "    \"single\": [\"a photo of a {}\"],\n",
    "    \"three\": [\"a photo of a {}\", \"an image of a {}\", \"a picture of a {}\"],\n",
    "    \"descriptive\": [\"a photo of a {}\", \"a clear photo of a {}\", \"a close-up of a {}\"],\n",
    "}\n",
    "test_loader_eval = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "print(\"Zero-shot accuracy with different prompt templates:\")\n",
    "results = evaluate_with_prompt_ensemble(clip_model, test_loader_eval,\n",
    "                                         CIFAR10_CLASSES, tokenizer, template_sets)\n",
    "print(\"\\nPrompt ensembling typically improves accuracy!\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full evaluation on CIFAR-10 test set\n",
    "clip_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Pre-compute class embeddings with ensemble\n",
    "templates = [\"a photo of a {}\", \"an image of a {}\", \"a picture of a {}\"]\n",
    "class_embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for cls in CIFAR10_CLASSES:\n",
    "        prompts = [t.format(cls) for t in templates]\n",
    "        tokens = tokenizer.batch_encode(prompts).to(device)\n",
    "        embs = F.normalize(clip_model.text_encoder(tokens), dim=-1)\n",
    "        class_embeddings.append(embs.mean(dim=0))\n",
    "\n",
    "    class_embeddings = torch.stack(class_embeddings)  # (10, 128)\n",
    "    class_embeddings = F.normalize(class_embeddings, dim=-1)\n",
    "\n",
    "    for images, labels in test_loader_eval:\n",
    "        images = images.to(device)\n",
    "        img_embs = F.normalize(clip_model.image_encoder(images), dim=-1)\n",
    "        sims = img_embs @ class_embeddings.T\n",
    "        preds = sims.argmax(dim=1).cpu()\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Overall zero-shot accuracy on CIFAR-10 test set: {accuracy:.1%}\")\n",
    "print(f\"Random chance would be: {1/10:.1%}\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class accuracy\n",
    "class_correct = {cls: 0 for cls in CIFAR10_CLASSES}\n",
    "class_total = {cls: 0 for cls in CIFAR10_CLASSES}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader_eval:\n",
    "        images = images.to(device)\n",
    "        img_embs = F.normalize(clip_model.image_encoder(images), dim=-1)\n",
    "        sims = img_embs @ class_embeddings.T\n",
    "        preds = sims.argmax(dim=1).cpu()\n",
    "        for p, l in zip(preds, labels):\n",
    "            cls_name = CIFAR10_CLASSES[l.item()]\n",
    "            class_total[cls_name] += 1\n",
    "            if p.item() == l.item():\n",
    "                class_correct[cls_name] += 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "class_accs = [class_correct[c] / class_total[c] for c in CIFAR10_CLASSES]\n",
    "colors = ['green' if a > 0.5 else 'orange' if a > 0.3 else 'red' for a in class_accs]\n",
    "ax.bar(CIFAR10_CLASSES, class_accs, color=colors, edgecolor='black', linewidth=0.5)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Per-Class Zero-Shot Accuracy', fontsize=14)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.axhline(y=0.1, color='gray', linestyle='--', label='Random chance')\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image retrieval demo: find images matching a text query\n",
    "def text_to_image_retrieval(clip_model, text_query, dataset, tokenizer,\n",
    "                             top_k=5, n_candidates=1000):\n",
    "    \"\"\"Retrieve most similar images for a text query.\"\"\"\n",
    "    clip_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode query\n",
    "        tokens = tokenizer.encode(text_query).unsqueeze(0).to(device)\n",
    "        txt_emb = F.normalize(clip_model.text_encoder(tokens), dim=-1)\n",
    "\n",
    "        # Encode candidate images\n",
    "        indices = np.random.choice(len(dataset), n_candidates, replace=False)\n",
    "        all_sims = []\n",
    "        all_imgs = []\n",
    "        all_labels = []\n",
    "\n",
    "        for start in range(0, len(indices), 256):\n",
    "            batch_idx = indices[start:start+256]\n",
    "            imgs = torch.stack([dataset[i][0] for i in batch_idx]).to(device)\n",
    "            labels = [dataset[i][1] for i in batch_idx]\n",
    "            img_embs = F.normalize(clip_model.image_encoder(imgs), dim=-1)\n",
    "            sims = (txt_emb @ img_embs.T).squeeze().cpu()\n",
    "            all_sims.append(sims)\n",
    "            all_imgs.extend([dataset[i][0] for i in batch_idx])\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "        all_sims = torch.cat(all_sims)\n",
    "        top_indices = all_sims.argsort(descending=True)[:top_k]\n",
    "\n",
    "    return top_indices, all_sims, all_imgs, all_labels"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run retrieval queries\n",
    "queries = [\"a photo of a dog\", \"a photo of a ship\", \"a photo of a truck\"]\n",
    "\n",
    "fig, axes = plt.subplots(len(queries), 6, figsize=(18, 3*len(queries)))\n",
    "for q_idx, query in enumerate(queries):\n",
    "    top_idx, sims, imgs, labels = text_to_image_retrieval(\n",
    "        clip_model, query, test_dataset, tokenizer, top_k=5\n",
    "    )\n",
    "\n",
    "    axes[q_idx, 0].text(0.5, 0.5, f'Query:\\n\"{query}\"', ha='center', va='center',\n",
    "                        fontsize=11, transform=axes[q_idx, 0].transAxes)\n",
    "    axes[q_idx, 0].axis('off')\n",
    "\n",
    "    for rank, idx in enumerate(top_idx):\n",
    "        img = imgs[idx].permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
    "        axes[q_idx, rank+1].imshow(img)\n",
    "        cls = CIFAR10_CLASSES[labels[idx]]\n",
    "        axes[q_idx, rank+1].set_title(f\"#{rank+1}: {cls}\\nsim={sims[idx]:.3f}\", fontsize=9)\n",
    "        axes[q_idx, rank+1].axis('off')\n",
    "\n",
    "plt.suptitle('Text-to-Image Retrieval Results', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "clip_model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader_eval:\n",
    "        images = images.to(device)\n",
    "        img_embs = F.normalize(clip_model.image_encoder(images), dim=-1)\n",
    "        sims = img_embs @ class_embeddings.T\n",
    "        preds = sims.argmax(dim=1).cpu()\n",
    "        all_preds.extend(preds.numpy())\n",
    "        all_true.extend(labels.numpy())\n",
    "\n",
    "cm = confusion_matrix(all_true, all_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(CIFAR10_CLASSES))\n",
    "plt.xticks(tick_marks, CIFAR10_CLASSES, rotation=45, ha='right')\n",
    "plt.yticks(tick_marks, CIFAR10_CLASSES)\n",
    "\n",
    "# Add text annotations\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'), ha='center', va='center',\n",
    "             color='white' if cm[i, j] > cm.max() / 2 else 'black', fontsize=8)\n",
    "\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.title('Zero-Shot Classification Confusion Matrix', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Zero-Shot Accuracy: {np.mean(np.array(all_preds) == np.array(all_true)):.1%}\")\n",
    "print(\"Congratulations! You have built a complete zero-shot classification system!\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "1. Which classes does the model confuse most often? Why might that be?\n",
    "2. How would you adapt this system for a completely different dataset (e.g., medical images)?\n",
    "3. What are the limits of zero-shot classification? When would fine-tuning be necessary?\n",
    "4. How does prompt engineering affect performance? What makes a good prompt?\n",
    "\n",
    "### Optional Challenges\n",
    "1. Try classifying into sub-categories: instead of \"dog,\" use \"golden retriever,\" \"poodle,\" etc. Does CLIP distinguish them?\n",
    "2. Implement image-to-text retrieval: given an image, find the best-matching caption.\n",
    "3. Try adversarial prompts: \"a photo that is NOT a dog.\" How does CLIP handle negation?\n",
    "4. Compare single-template vs. multi-template ensembling across all 10 classes."
   ],
   "id": "cell_27"
  }
 ]
}