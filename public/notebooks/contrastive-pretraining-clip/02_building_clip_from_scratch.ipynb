{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Building CLIP from Scratch -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building CLIP from Scratch: Dual Encoders and Contrastive Training\n",
    "\n",
    "*Part 2 of the Vizuara series on Contrastive Pretraining (CLIP-style)*\n",
    "*Estimated time: 60 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "CLIP (Contrastive Language-Image Pretraining) changed everything by showing that a model can learn to understand both images and text in a shared embedding space. This shared space enables remarkable capabilities: zero-shot classification, image-text retrieval, and serving as the visual backbone for modern VLMs like LLaVA and GPT-4V.\n",
    "\n",
    "In Notebook 1, we learned the contrastive loss function. Now we will build the full CLIP architecture from scratch -- an image encoder, a text encoder, and the training pipeline that connects them.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Build a Vision Transformer (ViT) image encoder from scratch\n",
    "- Build a Transformer text encoder from scratch\n",
    "- Train a complete CLIP model on CIFAR-10 with synthetic captions\n",
    "- Visualize the learned multimodal embedding space"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and GPU check\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Think of CLIP as two translators working side by side. One translator speaks \"image\" -- they can take any photograph and describe its content as a string of numbers (a vector). The other translator speaks \"text\" -- they can take any sentence and also produce a string of numbers.\n",
    "\n",
    "The magic is that both translators produce numbers in the **same language**. A photo of a dog and the text \"a photo of a dog\" both map to similar vectors. A photo of a dog and the text \"a photo of a car\" map to very different vectors.\n",
    "\n",
    "How do we train these translators? We show them millions of image-caption pairs and say: \"The vector for this image should be close to the vector for this caption, and far from the vectors of all other captions in the batch.\"\n",
    "\n",
    "### Think About This\n",
    "- Why do we need two separate encoders instead of one? Could a single network process both images and text?\n",
    "- What is the minimum information the image encoder needs to capture to match a caption?\n",
    "- Why would normalizing embeddings to unit length be important?"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### The Dual Encoder Architecture\n",
    "\n",
    "Let $f_\\theta$ be the image encoder and $g_\\phi$ be the text encoder. Given an image $x_I$ and text $x_T$:\n",
    "\n",
    "$$z_I = f_\\theta(x_I) \\in \\mathbb{R}^d, \\quad z_T = g_\\phi(x_T) \\in \\mathbb{R}^d$$\n",
    "\n",
    "Computationally, each encoder takes its input (pixels or tokens) through a neural network and outputs a $d$-dimensional embedding vector. Both vectors live in the same space.\n",
    "\n",
    "We normalize both embeddings:\n",
    "$$\\hat{z}_I = \\frac{z_I}{\\|z_I\\|}, \\quad \\hat{z}_T = \\frac{z_T}{\\|z_T\\|}$$\n",
    "\n",
    "This ensures all embeddings lie on the unit hypersphere, so cosine similarity equals the dot product.\n",
    "\n",
    "### The CLIP Loss\n",
    "\n",
    "For a batch of $N$ image-text pairs, the CLIP loss is:\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{2}\\left(-\\frac{1}{N}\\sum_{i=1}^{N}\\log\\frac{e^{\\hat{z}_{I_i}\\cdot\\hat{z}_{T_i}/\\tau}}{\\sum_{j=1}^{N}e^{\\hat{z}_{I_i}\\cdot\\hat{z}_{T_j}/\\tau}} - \\frac{1}{N}\\sum_{i=1}^{N}\\log\\frac{e^{\\hat{z}_{T_i}\\cdot\\hat{z}_{I_i}/\\tau}}{\\sum_{j=1}^{N}e^{\\hat{z}_{T_i}\\cdot\\hat{z}_{I_j}/\\tau}}\\right)$$\n",
    "\n",
    "This is a symmetric cross-entropy loss: the first term classifies images-to-texts, and the second classifies texts-to-images. The temperature $\\tau$ controls sharpness.\n",
    "\n",
    "Let us compute a quick example with $N=2$, $\\tau=0.5$. Suppose:\n",
    "$$\\hat{z}_{I_1} \\cdot \\hat{z}_{T_1} = 0.8, \\quad \\hat{z}_{I_1} \\cdot \\hat{z}_{T_2} = 0.1$$\n",
    "\n",
    "Image-to-text loss for pair 1:\n",
    "$$-\\log\\frac{e^{0.8/0.5}}{e^{0.8/0.5} + e^{0.1/0.5}} = -\\log\\frac{e^{1.6}}{e^{1.6} + e^{0.2}} = -\\log\\frac{4.95}{4.95 + 1.22} = -\\log(0.802) = 0.220$$\n",
    "\n",
    "A loss of 0.220 means the model has high confidence in the correct match. This is what we want."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 Patch Embedding (ViT Image Encoder)\n",
    "\n",
    "A Vision Transformer splits an image into patches, embeds each patch, and processes them with self-attention."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Split image into patches and embed them.\"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Conv2d with kernel_size=patch_size and stride=patch_size\n",
    "        # acts like a patch-wise linear projection\n",
    "        self.projection = nn.Conv2d(\n",
    "            in_channels, embed_dim,\n",
    "            kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "        # CLS token and positional embeddings\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim) * 0.02)\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.randn(1, self.num_patches + 1, embed_dim) * 0.02\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        # Project patches: (B, C, H, W) -> (B, embed_dim, H/P, W/P)\n",
    "        x = self.projection(x)  # (B, embed_dim, num_patches_h, num_patches_w)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
    "\n",
    "        # Prepend CLS token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (B, num_patches+1, embed_dim)\n",
    "\n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_embed\n",
    "        return x\n",
    "\n",
    "# Test patch embedding\n",
    "patch_embed = PatchEmbedding(img_size=32, patch_size=4, embed_dim=128)\n",
    "test_img = torch.randn(2, 3, 32, 32)\n",
    "patches = patch_embed(test_img)\n",
    "print(f\"Input: {test_img.shape}\")\n",
    "print(f\"Patches: {patches.shape}  (batch, num_patches+1, embed_dim)\")\n",
    "print(f\"Number of patches: {patch_embed.num_patches} = (32/4)^2\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Transformer Encoder Block"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Standard Transformer encoder block with multi-head attention.\"\"\"\n",
    "    def __init__(self, embed_dim=128, num_heads=4, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim, num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention with residual\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n",
    "        x = x + attn_out\n",
    "\n",
    "        # MLP with residual\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# Test\n",
    "block = TransformerBlock(embed_dim=128, num_heads=4)\n",
    "out = block(patches)\n",
    "print(f\"Transformer block output: {out.shape}\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Full Image Encoder"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"Vision Transformer image encoder for CLIP.\"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3,\n",
    "                 embed_dim=128, depth=4, num_heads=4, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            img_size, patch_size, in_channels, embed_dim\n",
    "        )\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.projection = nn.Linear(embed_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        # Use CLS token as image representation\n",
    "        cls_output = x[:, 0]\n",
    "        return self.projection(cls_output)\n",
    "\n",
    "image_encoder = ImageEncoder(output_dim=128)\n",
    "test_out = image_encoder(test_img)\n",
    "print(f\"Image encoder output: {test_out.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in image_encoder.parameters()):,}\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization checkpoint: check that embeddings are diverse\n",
    "with torch.no_grad():\n",
    "    random_imgs = torch.randn(20, 3, 32, 32)\n",
    "    random_embs = F.normalize(image_encoder(random_imgs), dim=-1)\n",
    "\n",
    "sim_matrix = (random_embs @ random_embs.T).numpy()\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(sim_matrix, cmap='RdYlGn', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.title('Image Embedding Similarities\\n(Should be noisy, not uniform)', fontsize=13)\n",
    "plt.xlabel('Image Index')\n",
    "plt.ylabel('Image Index')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Text Encoder"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"Transformer text encoder for CLIP.\"\"\"\n",
    "    def __init__(self, vocab_size=1000, max_len=32, embed_dim=128,\n",
    "                 depth=2, num_heads=4, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.randn(1, max_len, embed_dim) * 0.02\n",
    "        )\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.projection = nn.Linear(embed_dim, output_dim)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        B, L = tokens.shape\n",
    "        x = self.token_embed(tokens) + self.pos_embed[:, :L]\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        # Mean pooling over sequence\n",
    "        x = x.mean(dim=1)\n",
    "        return self.projection(x)\n",
    "\n",
    "text_encoder = TextEncoder(vocab_size=200, output_dim=128)\n",
    "test_tokens = torch.randint(0, 200, (2, 10))\n",
    "text_out = text_encoder(test_tokens)\n",
    "print(f\"Text encoder output: {text_out.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in text_encoder.parameters()):,}\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Full CLIP Model"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniCLIP(nn.Module):\n",
    "    \"\"\"Complete CLIP model with image and text encoders.\"\"\"\n",
    "    def __init__(self, image_encoder, text_encoder, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.logit_scale = nn.Parameter(\n",
    "            torch.log(torch.tensor(1.0 / temperature))\n",
    "        )\n",
    "\n",
    "    def forward(self, images, tokens):\n",
    "        # Encode both modalities\n",
    "        img_emb = F.normalize(self.image_encoder(images), dim=-1)\n",
    "        txt_emb = F.normalize(self.text_encoder(tokens), dim=-1)\n",
    "\n",
    "        # Compute scaled similarity matrix\n",
    "        logit_scale = self.logit_scale.exp().clamp(max=100)\n",
    "        logits = logit_scale * (img_emb @ txt_emb.T)\n",
    "        return logits, img_emb, txt_emb\n",
    "\n",
    "    def clip_loss(self, logits):\n",
    "        \"\"\"Symmetric contrastive loss.\"\"\"\n",
    "        labels = torch.arange(logits.size(0), device=logits.device)\n",
    "        loss_i2t = F.cross_entropy(logits, labels)\n",
    "        loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "        return (loss_i2t + loss_t2i) / 2\n",
    "\n",
    "clip_model = MiniCLIP(\n",
    "    ImageEncoder(output_dim=128),\n",
    "    TextEncoder(vocab_size=200, output_dim=128)\n",
    ").to(device)\n",
    "total_params = sum(p.numel() for p in clip_model.parameters())\n",
    "print(f\"Total Mini-CLIP parameters: {total_params:,}\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO: Build the Caption Tokenizer"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    \"\"\"\n",
    "    A simple word-level tokenizer for CIFAR-10 captions.\n",
    "    Vocabulary: PAD=0, then common words mapped to integers.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_len=12):\n",
    "        self.max_len = max_len\n",
    "        # ============ TODO ============\n",
    "        # Build a vocabulary from these CIFAR-10 class templates:\n",
    "        # \"a photo of a [class]\" where classes are:\n",
    "        # airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n",
    "        #\n",
    "        # Step 1: Create a word_to_idx dictionary starting with {\"<PAD>\": 0}\n",
    "        # Step 2: Add all unique words from the templates\n",
    "        # Hint: split each template into words and add unseen words\n",
    "        # ==============================\n",
    "\n",
    "        self.word_to_idx = {\"<PAD>\": 0}  # YOUR CODE to extend this\n",
    "        self.vocab_size = len(self.word_to_idx)\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert text to padded token tensor.\"\"\"\n",
    "        words = text.lower().split()\n",
    "        tokens = [self.word_to_idx.get(w, 0) for w in words]\n",
    "        # Pad or truncate\n",
    "        if len(tokens) < self.max_len:\n",
    "            tokens = tokens + [0] * (self.max_len - len(tokens))\n",
    "        else:\n",
    "            tokens = tokens[:self.max_len]\n",
    "        return torch.tensor(tokens)\n",
    "\n",
    "    def batch_encode(self, texts):\n",
    "        return torch.stack([self.encode(t) for t in texts])"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "CIFAR10_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                    'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "tokenizer = SimpleTokenizer()\n",
    "test_text = \"a photo of a dog\"\n",
    "tokens = tokenizer.encode(test_text)\n",
    "print(f\"'{test_text}' -> {tokens}\")\n",
    "assert tokens[0] != 0, \"First token should not be PAD\"\n",
    "assert tokenizer.vocab_size > 10, f\"Vocab too small: {tokenizer.vocab_size}\"\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(\"Correct! Your tokenizer works.\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implement the Training Step"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(clip_model, images, captions, optimizer, tokenizer):\n",
    "    \"\"\"\n",
    "    Perform one training step of Mini-CLIP.\n",
    "\n",
    "    Args:\n",
    "        clip_model: the MiniCLIP model\n",
    "        images: batch of images (B, C, H, W)\n",
    "        captions: list of caption strings\n",
    "        optimizer: the optimizer\n",
    "        tokenizer: text tokenizer\n",
    "\n",
    "    Returns:\n",
    "        loss value, accuracy\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Tokenize captions using tokenizer.batch_encode(captions)\n",
    "    # Step 2: Move images and tokens to device\n",
    "    # Step 3: Forward pass through clip_model to get logits\n",
    "    # Step 4: Compute clip_model.clip_loss(logits)\n",
    "    # Step 5: Backward pass and optimizer step\n",
    "    # Step 6: Compute accuracy (how often argmax matches diagonal)\n",
    "    # ==============================\n",
    "\n",
    "    loss = ???  # YOUR CODE HERE\n",
    "    accuracy = ???  # YOUR CODE HERE\n",
    "\n",
    "    return loss.item(), accuracy"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification cell\n",
    "print(\"This will be verified during the training loop in section 7.\")\n",
    "print(\"Make sure your train_step returns (loss_value, accuracy_value)\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare CIFAR-10 dataset with captions\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Caption templates for each class\n",
    "CAPTION_TEMPLATES = [\n",
    "    \"a photo of a {}\",\n",
    "    \"an image of a {}\",\n",
    "    \"a picture of a {}\",\n",
    "]\n",
    "\n",
    "def get_caption(label_idx):\n",
    "    \"\"\"Generate a caption for a CIFAR-10 label.\"\"\"\n",
    "    class_name = CIFAR10_CLASSES[label_idx]\n",
    "    template = CAPTION_TEMPLATES[np.random.randint(len(CAPTION_TEMPLATES))]\n",
    "    return template.format(class_name)\n",
    "\n",
    "# Create dataloader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=128, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")\n",
    "\n",
    "# Sample captions\n",
    "for i in range(5):\n",
    "    _, label = train_dataset[i]\n",
    "    print(f\"  Class {label} ({CIFAR10_CLASSES[label]}): '{get_caption(label)}'\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images with their captions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    img, label = train_dataset[i]\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    img_display = img.permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
    "    ax.imshow(img_display)\n",
    "    ax.set_title(get_caption(label), fontsize=9)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('CIFAR-10 Images with Generated Captions', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and optimizer\n",
    "tokenizer = SimpleTokenizer(max_len=8)\n",
    "clip_model = MiniCLIP(\n",
    "    ImageEncoder(img_size=32, patch_size=4, embed_dim=128, depth=4,\n",
    "                 num_heads=4, output_dim=128),\n",
    "    TextEncoder(vocab_size=tokenizer.vocab_size, max_len=8, embed_dim=128,\n",
    "                depth=2, num_heads=4, output_dim=128),\n",
    "    temperature=0.07\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(clip_model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in clip_model.parameters()):,}\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    clip_model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        captions = [get_caption(l.item()) for l in labels]\n",
    "        tokens = tokenizer.batch_encode(captions).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, _, _ = clip_model(images, tokens)\n",
    "        loss = clip_model.clip_loss(logits)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracy\n",
    "        with torch.no_grad():\n",
    "            preds = logits.argmax(dim=1)\n",
    "            gt = torch.arange(logits.size(0), device=device)\n",
    "            acc = (preds == gt).float().mean().item()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "        n_batches += 1\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    avg_acc = epoch_acc / n_batches\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accs.append(avg_acc)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f} | Acc: {avg_acc:.3f}\")"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(range(1, num_epochs+1), train_losses, 'b-o', linewidth=2, markersize=6)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Contrastive Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(range(1, num_epochs+1), train_accs, 'g-o', linewidth=2, markersize=6)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Contrastive Accuracy', fontsize=12)\n",
    "ax2.set_title('Training Accuracy', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned embedding space\n",
    "clip_model.eval()\n",
    "all_img_embs = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=256, shuffle=False\n",
    "    ):\n",
    "        img_emb = F.normalize(\n",
    "            clip_model.image_encoder(images.to(device)), dim=-1\n",
    "        ).cpu()\n",
    "        all_img_embs.append(img_emb)\n",
    "        all_labels.append(labels)\n",
    "        if len(all_labels) * 256 >= 5000:\n",
    "            break\n",
    "\n",
    "all_img_embs = torch.cat(all_img_embs)[:5000]\n",
    "all_labels = torch.cat(all_labels)[:5000]\n",
    "\n",
    "# t-SNE visualization\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "emb_2d = tsne.fit_transform(all_img_embs.numpy())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(emb_2d[:, 0], emb_2d[:, 1],\n",
    "                      c=all_labels.numpy(), cmap='tab10', s=5, alpha=0.6)\n",
    "plt.colorbar(scatter, ticks=range(10),\n",
    "             label='Class')\n",
    "plt.title('t-SNE of Mini-CLIP Image Embeddings\\n(Colors = CIFAR-10 classes)', fontsize=14)\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Congratulations! You have built CLIP from scratch!\")\n",
    "print(\"Similar classes should cluster together in the embedding space.\")"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "1. How does the number of Transformer layers affect the quality of embeddings? What is the minimum depth needed?\n",
    "2. Why do we use separate encoders for images and text instead of a shared one?\n",
    "3. What would happen if we increased the batch size from 128 to 1024? Why does batch size matter so much in contrastive learning?\n",
    "4. The learned temperature converges to a small value. What does this tell us about the model's confidence?\n",
    "\n",
    "### Optional Challenges\n",
    "1. Try using a ResNet instead of ViT as the image encoder. Compare the learned representations.\n",
    "2. Add more diverse caption templates and see if it improves zero-shot performance.\n",
    "3. Try training for more epochs and track zero-shot classification accuracy on the test set."
   ],
   "id": "cell_32"
  }
 ]
}