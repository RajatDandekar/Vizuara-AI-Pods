{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Contrastive Learning and the InfoNCE Loss -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Learning and the InfoNCE Loss from First Principles\n",
    "\n",
    "*Part 1 of the Vizuara series on Contrastive Pretraining (CLIP-style)*\n",
    "*Estimated time: 45 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Contrastive learning is the engine behind some of the most powerful AI models of the last few years -- CLIP, SimCLR, DINO, and many more. The core idea is deceptively simple: learn representations by pulling similar things close and pushing different things apart.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand the intuition behind contrastive learning from first principles\n",
    "- Implement the InfoNCE loss function from scratch\n",
    "- Build a simple contrastive learner on 2D data\n",
    "- Visualize how the embedding space organizes itself during training\n",
    "\n",
    "Let us see a preview of what we will build -- a model that learns to cluster similar data points together purely from contrastive supervision:"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and GPU check\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Imagine you are organizing a music library. You have thousands of songs, and you want to group them so that similar songs are nearby and dissimilar songs are far apart.\n",
    "\n",
    "One approach: pick a song, find another song that is similar (a **positive pair**), and find songs that are different (**negative pairs**). Then nudge your organization so the similar pair gets closer, and the different pairs get further apart.\n",
    "\n",
    "Now repeat this millions of times. Over time, jazz songs cluster together, rock songs cluster together, and classical music forms its own group -- all without ever labeling a single song.\n",
    "\n",
    "This is exactly what contrastive learning does, but in a high-dimensional embedding space instead of a music library shelf.\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "The beauty of contrastive learning is that you never need explicit labels. You just need to know which pairs are \"similar\" and which are \"different.\" In CLIP, this comes for free from the internet -- every image naturally comes with a caption.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "Before we write any code, ask yourself:\n",
    "- If you had to measure \"similarity\" between two vectors, what mathematical operation would you use?\n",
    "- Why might we want to normalize our vectors to unit length before comparing them?\n",
    "- What happens if all your embeddings collapse to the same point? Does that satisfy \"all positives are close\"?"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Cosine Similarity\n",
    "\n",
    "The foundation of contrastive learning is measuring similarity between vectors. We use **cosine similarity**, which measures the angle between two vectors:\n",
    "\n",
    "$$\\text{sim}(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\|\\mathbf{b}\\|}$$\n",
    "\n",
    "Computationally, this means: take the dot product of a and b, then divide by the product of their lengths. If both vectors are normalized to unit length ($\\|\\mathbf{a}\\| = \\|\\mathbf{b}\\| = 1$), then cosine similarity is just the dot product.\n",
    "\n",
    "Let us plug in some simple numbers. Suppose $\\mathbf{a} = [3, 4]$ and $\\mathbf{b} = [4, 3]$.\n",
    "\n",
    "$$\\text{sim} = \\frac{3 \\times 4 + 4 \\times 3}{\\sqrt{9+16} \\times \\sqrt{16+9}} = \\frac{24}{5 \\times 5} = \\frac{24}{25} = 0.96$$\n",
    "\n",
    "This tells us these vectors are very similar (pointing in nearly the same direction)."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us verify this computation\n",
    "a = torch.tensor([3.0, 4.0])\n",
    "b = torch.tensor([4.0, 3.0])\n",
    "\n",
    "cosine_sim = torch.dot(a, b) / (torch.norm(a) * torch.norm(b))\n",
    "print(f\"Cosine similarity: {cosine_sim.item():.4f}\")\n",
    "\n",
    "# With normalized vectors, it is just the dot product\n",
    "a_norm = F.normalize(a, dim=0)\n",
    "b_norm = F.normalize(b, dim=0)\n",
    "dot_product = torch.dot(a_norm, b_norm)\n",
    "print(f\"Dot product of normalized vectors: {dot_product.item():.4f}\")\n",
    "print(f\"Same result? {torch.allclose(cosine_sim, dot_product)}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The InfoNCE Loss\n",
    "\n",
    "Now the critical question: how do we use similarity to train a model?\n",
    "\n",
    "Given a batch of $N$ pairs, we construct an $N \\times N$ similarity matrix. For each row $i$, the diagonal entry $(i, i)$ is the positive pair, and all other entries are negatives. The loss treats this as an $N$-way classification problem:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{InfoNCE}} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\exp(\\text{sim}(z_i^a, z_i^b) / \\tau)}{\\sum_{j=1}^{N} \\exp(\\text{sim}(z_i^a, z_j^b) / \\tau)}$$\n",
    "\n",
    "Computationally, this says: for each anchor embedding $z_i^a$, compute its similarity with all $N$ candidate embeddings, scale by temperature $\\tau$, apply softmax, and take the negative log probability of the correct match.\n",
    "\n",
    "Let us work through this with $N = 3$ and $\\tau = 0.5$. Suppose our similarity scores are:\n",
    "\n",
    "| | $z_1^b$ | $z_2^b$ | $z_3^b$ |\n",
    "|---|---|---|---|\n",
    "| $z_1^a$ | **0.9** | 0.1 | -0.2 |\n",
    "\n",
    "The softmax for row 1:\n",
    "$$p_1 = \\frac{\\exp(0.9/0.5)}{\\exp(0.9/0.5) + \\exp(0.1/0.5) + \\exp(-0.2/0.5)} = \\frac{\\exp(1.8)}{\\exp(1.8) + \\exp(0.2) + \\exp(-0.4)} = \\frac{6.05}{6.05 + 1.22 + 0.67} = \\frac{6.05}{7.94} = 0.762$$\n",
    "\n",
    "The loss for this row: $-\\log(0.762) = 0.272$.\n",
    "\n",
    "This is pretty good -- the model correctly assigns high probability to the right match. A perfect model would give probability 1.0 and loss 0.0."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us verify the InfoNCE computation step by step\n",
    "sims = torch.tensor([[0.9, 0.1, -0.2],\n",
    "                      [0.2, 0.85, 0.0],\n",
    "                      [-0.1, 0.15, 0.95]])\n",
    "tau = 0.5\n",
    "\n",
    "# Scale by temperature\n",
    "scaled = sims / tau\n",
    "print(\"Scaled similarities:\")\n",
    "print(scaled)\n",
    "\n",
    "# Apply softmax along rows\n",
    "probs = F.softmax(scaled, dim=1)\n",
    "print(\"\\nSoftmax probabilities:\")\n",
    "print(probs)\n",
    "\n",
    "# The correct matches are on the diagonal\n",
    "labels = torch.arange(3)\n",
    "loss = F.cross_entropy(scaled, labels)\n",
    "print(f\"\\nInfoNCE loss: {loss.item():.4f}\")\n",
    "print(f\"(Manual check: {-torch.log(probs[0, 0]).item():.4f} for row 0)\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 The Similarity Matrix\n",
    "\n",
    "Let us start by implementing the similarity matrix computation."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_matrix(embeddings_a, embeddings_b, temperature=0.07):\n",
    "    \"\"\"\n",
    "    Compute the NxN cosine similarity matrix between two sets of embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings_a: (N, D) normalized embeddings\n",
    "        embeddings_b: (N, D) normalized embeddings\n",
    "        temperature: temperature scaling factor\n",
    "\n",
    "    Returns:\n",
    "        (N, N) scaled similarity matrix\n",
    "    \"\"\"\n",
    "    # Normalize embeddings to unit vectors\n",
    "    a_norm = F.normalize(embeddings_a, dim=-1)\n",
    "    b_norm = F.normalize(embeddings_b, dim=-1)\n",
    "\n",
    "    # Cosine similarity matrix via matrix multiplication\n",
    "    similarity = a_norm @ b_norm.T  # (N, N)\n",
    "\n",
    "    # Scale by temperature\n",
    "    return similarity / temperature\n",
    "\n",
    "# Test it\n",
    "N, D = 4, 8\n",
    "a = torch.randn(N, D)\n",
    "b = torch.randn(N, D)\n",
    "\n",
    "sim_matrix = compute_similarity_matrix(a, b, temperature=0.1)\n",
    "print(f\"Similarity matrix shape: {sim_matrix.shape}\")\n",
    "print(f\"Values range: [{sim_matrix.min():.2f}, {sim_matrix.max():.2f}]\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the similarity matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Random embeddings (before training)\n",
    "random_sims = compute_similarity_matrix(\n",
    "    torch.randn(8, 32), torch.randn(8, 32), temperature=1.0\n",
    ")\n",
    "im1 = axes[0].imshow(random_sims.detach().numpy(), cmap='RdYlGn', vmin=-1, vmax=1)\n",
    "axes[0].set_title('Random Embeddings\\n(Before Training)', fontsize=13)\n",
    "axes[0].set_xlabel('Text Index')\n",
    "axes[0].set_ylabel('Image Index')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Ideal case: identity-like matrix\n",
    "ideal = torch.eye(8) * 0.95 + torch.randn(8, 8) * 0.05\n",
    "im2 = axes[1].imshow(ideal.numpy(), cmap='RdYlGn', vmin=-1, vmax=1)\n",
    "axes[1].set_title('Aligned Embeddings\\n(After Training)', fontsize=13)\n",
    "axes[1].set_xlabel('Text Index')\n",
    "axes[1].set_ylabel('Image Index')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The InfoNCE Loss Function\n",
    "\n",
    "Now let us implement the full InfoNCE loss."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_nce_loss(embeddings_a, embeddings_b, temperature=0.07):\n",
    "    \"\"\"\n",
    "    Compute symmetric InfoNCE loss.\n",
    "\n",
    "    Args:\n",
    "        embeddings_a: (N, D) first set of embeddings (e.g., images)\n",
    "        embeddings_b: (N, D) second set of embeddings (e.g., texts)\n",
    "        temperature: temperature parameter\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    # Normalize\n",
    "    a_norm = F.normalize(embeddings_a, dim=-1)\n",
    "    b_norm = F.normalize(embeddings_b, dim=-1)\n",
    "\n",
    "    # Compute similarity matrix\n",
    "    logits = (a_norm @ b_norm.T) / temperature  # (N, N)\n",
    "\n",
    "    # Labels: the diagonal entries are the correct matches\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "\n",
    "    # Symmetric loss: a->b and b->a\n",
    "    loss_a_to_b = F.cross_entropy(logits, labels)\n",
    "    loss_b_to_a = F.cross_entropy(logits.T, labels)\n",
    "\n",
    "    return (loss_a_to_b + loss_b_to_a) / 2\n",
    "\n",
    "# Test with known good alignment\n",
    "good_a = torch.randn(4, 16)\n",
    "good_b = good_a + torch.randn(4, 16) * 0.1  # Slight noise\n",
    "loss_good = info_nce_loss(good_a, good_b, temperature=0.5)\n",
    "\n",
    "# Test with random alignment\n",
    "bad_a = torch.randn(4, 16)\n",
    "bad_b = torch.randn(4, 16)\n",
    "loss_bad = info_nce_loss(bad_a, bad_b, temperature=0.5)\n",
    "\n",
    "print(f\"Loss with aligned pairs: {loss_good.item():.4f}\")\n",
    "print(f\"Loss with random pairs:  {loss_bad.item():.4f}\")\n",
    "print(f\"Aligned loss is lower? {loss_good < loss_bad}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO: Implement Temperature-Scaled Softmax Visualization"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_temperature_effect(similarities, temperatures):\n",
    "    \"\"\"\n",
    "    Visualize how temperature affects the softmax distribution.\n",
    "\n",
    "    Args:\n",
    "        similarities: 1D tensor of similarity scores\n",
    "        temperatures: list of temperature values to compare\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(temperatures), figsize=(5*len(temperatures), 4))\n",
    "\n",
    "    for idx, tau in enumerate(temperatures):\n",
    "        # ============ TODO ============\n",
    "        # Step 1: Scale similarities by temperature (divide by tau)\n",
    "        # Step 2: Apply softmax to get probabilities\n",
    "        # Step 3: Plot as a bar chart on axes[idx]\n",
    "        # ==============================\n",
    "\n",
    "        scaled = ???  # YOUR CODE HERE\n",
    "        probs = ???   # YOUR CODE HERE\n",
    "\n",
    "        axes[idx].bar(range(len(probs)), probs.detach().numpy(), color='steelblue')\n",
    "        axes[idx].set_title(f'tau = {tau}', fontsize=13)\n",
    "        axes[idx].set_xlabel('Index')\n",
    "        axes[idx].set_ylabel('Probability')\n",
    "        axes[idx].set_ylim(0, 1.05)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test data: one high similarity, rest low\n",
    "test_sims = torch.tensor([0.9, 0.2, 0.1, -0.1, 0.0])"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "test_sims = torch.tensor([0.9, 0.2, 0.1, -0.1, 0.0])\n",
    "visualize_temperature_effect(test_sims, [0.07, 0.5, 2.0])\n",
    "# You should see: sharp peak at tau=0.07, moderate at tau=0.5, flat at tau=2.0\n",
    "print(\"Check: Does the distribution get sharper as temperature decreases?\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implement Contrastive Accuracy"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_accuracy(embeddings_a, embeddings_b, temperature=0.07):\n",
    "    \"\"\"\n",
    "    Compute how often the model picks the correct match.\n",
    "    For each image, the predicted text is the one with highest similarity.\n",
    "\n",
    "    Args:\n",
    "        embeddings_a: (N, D) first set of embeddings\n",
    "        embeddings_b: (N, D) second set of embeddings\n",
    "        temperature: temperature parameter\n",
    "\n",
    "    Returns:\n",
    "        Accuracy as a float between 0 and 1\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Normalize both sets of embeddings\n",
    "    # Step 2: Compute the NxN similarity matrix\n",
    "    # Step 3: For each row, find the index of maximum similarity (argmax)\n",
    "    # Step 4: Compare with ground truth labels (0, 1, 2, ..., N-1)\n",
    "    # Step 5: Return the fraction of correct matches\n",
    "    # ==============================\n",
    "\n",
    "    accuracy = ???  # YOUR CODE HERE\n",
    "    return accuracy"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "aligned_a = torch.randn(10, 32)\n",
    "aligned_b = aligned_a + torch.randn(10, 32) * 0.01  # Nearly identical\n",
    "acc = contrastive_accuracy(aligned_a, aligned_b)\n",
    "assert acc > 0.9, f\"Expected high accuracy for aligned pairs, got {acc}\"\n",
    "print(f\"Accuracy for aligned pairs: {acc:.2f}\")\n",
    "\n",
    "random_a = torch.randn(10, 32)\n",
    "random_b = torch.randn(10, 32)\n",
    "acc_rand = contrastive_accuracy(random_a, random_b)\n",
    "print(f\"Accuracy for random pairs: {acc_rand:.2f}\")\n",
    "print(\"Correct! Aligned pairs should have high accuracy, random pairs should be near 1/N\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us build a complete contrastive learning system on synthetic 2D data. We will create data points from different clusters and learn an encoder that maps them into an embedding space where similar points cluster together."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic paired data\n",
    "# Each \"pair\" consists of a point and a slightly perturbed version of it\n",
    "def generate_contrastive_pairs(n_clusters=5, points_per_cluster=20, noise=0.3):\n",
    "    \"\"\"Generate paired data from clusters for contrastive learning.\"\"\"\n",
    "    centers = torch.randn(n_clusters, 2) * 3  # Cluster centers\n",
    "    anchors = []\n",
    "    positives = []\n",
    "    labels = []\n",
    "\n",
    "    for c in range(n_clusters):\n",
    "        for _ in range(points_per_cluster):\n",
    "            point = centers[c] + torch.randn(2) * 0.5\n",
    "            augmented = point + torch.randn(2) * noise\n",
    "            anchors.append(point)\n",
    "            positives.append(augmented)\n",
    "            labels.append(c)\n",
    "\n",
    "    return (torch.stack(anchors), torch.stack(positives),\n",
    "            torch.tensor(labels))\n",
    "\n",
    "anchors, positives, labels = generate_contrastive_pairs()\n",
    "print(f\"Generated {len(anchors)} pairs from 5 clusters\")\n",
    "print(f\"Input dimension: {anchors.shape[1]}\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the raw data\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "colors = plt.cm.Set2(labels.numpy() / labels.max().item())\n",
    "\n",
    "axes[0].scatter(anchors[:, 0].numpy(), anchors[:, 1].numpy(),\n",
    "                c=colors, s=40, label='Anchors', marker='o')\n",
    "axes[0].set_title('Anchor Points (by cluster)', fontsize=13)\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "\n",
    "axes[1].scatter(anchors[:, 0].numpy(), anchors[:, 1].numpy(),\n",
    "                c=colors, s=30, marker='o', alpha=0.5, label='Anchors')\n",
    "axes[1].scatter(positives[:, 0].numpy(), positives[:, 1].numpy(),\n",
    "                c=colors, s=30, marker='^', alpha=0.5, label='Positives')\n",
    "for i in range(0, len(anchors), 5):\n",
    "    axes[1].plot([anchors[i, 0], positives[i, 0]],\n",
    "                 [anchors[i, 1], positives[i, 1]], 'k-', alpha=0.1)\n",
    "axes[1].set_title('Contrastive Pairs (linked)', fontsize=13)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple encoder network\n",
    "class ContrastiveEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=64, embed_dim=16):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "encoder = ContrastiveEncoder(input_dim=2, hidden_dim=64, embed_dim=16)\n",
    "print(f\"Encoder parameters: {sum(p.numel() for p in encoder.parameters()):,}\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n",
    "temperature = 0.1\n",
    "num_epochs = 200\n",
    "batch_size = 32\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle data\n",
    "    perm = torch.randperm(len(anchors))\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    for start in range(0, len(anchors), batch_size):\n",
    "        idx = perm[start:start + batch_size]\n",
    "        batch_a = anchors[idx]\n",
    "        batch_p = positives[idx]\n",
    "\n",
    "        # Encode both views\n",
    "        emb_a = encoder(batch_a)\n",
    "        emb_p = encoder(batch_p)\n",
    "\n",
    "        # Compute contrastive loss\n",
    "        loss = info_nce_loss(emb_a, emb_p, temperature=temperature)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses, color='steelblue', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('InfoNCE Loss', fontsize=12)\n",
    "plt.title('Contrastive Learning Training Progress', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned embeddings\n",
    "with torch.no_grad():\n",
    "    learned_embeddings = encoder(anchors)\n",
    "    learned_norm = F.normalize(learned_embeddings, dim=-1)\n",
    "\n",
    "# Use PCA to project to 2D for visualization\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "emb_2d = pca.fit_transform(learned_norm.numpy())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before training (raw input)\n",
    "axes[0].scatter(anchors[:, 0].numpy(), anchors[:, 1].numpy(),\n",
    "                c=colors, s=40)\n",
    "axes[0].set_title('Input Space (Raw Data)', fontsize=13)\n",
    "\n",
    "# After training (learned embeddings)\n",
    "axes[1].scatter(emb_2d[:, 0], emb_2d[:, 1], c=colors, s=40)\n",
    "axes[1].set_title('Learned Embedding Space\\n(After Contrastive Training)', fontsize=13)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Clusters should be more clearly separated in the embedding space!\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration: use the learned encoder for retrieval\n",
    "with torch.no_grad():\n",
    "    all_embeddings = F.normalize(encoder(anchors), dim=-1)\n",
    "\n",
    "# Pick a query point and find its nearest neighbors\n",
    "query_idx = 0\n",
    "query_emb = all_embeddings[query_idx:query_idx+1]\n",
    "similarities = (query_emb @ all_embeddings.T).squeeze()\n",
    "\n",
    "# Top 5 most similar\n",
    "top_k = 5\n",
    "top_indices = similarities.argsort(descending=True)[1:top_k+1]\n",
    "\n",
    "print(f\"Query point cluster: {labels[query_idx].item()}\")\n",
    "print(f\"Top-{top_k} most similar points:\")\n",
    "for rank, idx in enumerate(top_indices):\n",
    "    print(f\"  Rank {rank+1}: cluster={labels[idx].item()}, similarity={similarities[idx]:.3f}\")\n",
    "    correct = \"correct\" if labels[idx] == labels[query_idx] else \"WRONG\"\n",
    "    print(f\"    -> {correct}\")\n",
    "\n",
    "# Compute retrieval accuracy across all points\n",
    "correct = 0\n",
    "for i in range(len(anchors)):\n",
    "    q = all_embeddings[i:i+1]\n",
    "    sims = (q @ all_embeddings.T).squeeze()\n",
    "    nearest = sims.argsort(descending=True)[1]\n",
    "    if labels[nearest] == labels[i]:\n",
    "        correct += 1\n",
    "retrieval_acc = correct / len(anchors)\n",
    "print(f\"\\nOverall nearest-neighbor retrieval accuracy: {retrieval_acc:.1%}\")\n",
    "print(\"Congratulations! You have built a contrastive learning system from scratch!\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "1. What would happen if we used a very small batch size (e.g., 2)? How would the number of negatives affect learning?\n",
    "2. Why is temperature important? What happens if tau is too small or too large?\n",
    "3. Could the embeddings collapse to all be the same vector? Why or why not?\n",
    "4. How is InfoNCE different from a standard triplet loss?\n",
    "\n",
    "### Optional Challenges\n",
    "1. Try different temperature values (0.01, 0.1, 0.5, 1.0) and plot the final embedding quality for each.\n",
    "2. Increase the number of clusters to 20. Does the model still separate them well?\n",
    "3. Implement a version where the encoder is a deeper network. Does depth help?"
   ],
   "id": "cell_30"
  }
 ]
}