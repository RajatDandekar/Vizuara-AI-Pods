{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Factorized Space-Time Attention â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1ventuhdj998YNr_9KusKPNX2VFJg7Av1\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Factorized Space-Time Attention for Video Diffusion\n",
    "\n",
    "*Part 2 of the Vizuara series on Diffusion Models for Video Generation*\n",
    "*Estimated time: 45 minutes*\n",
    "\n",
    "In Notebook 1, we built a simple video diffusion model using 3D convolutions. It worked â€” but it was slow and inflexible. In this notebook, we will build the **real** architecture behind modern video diffusion models: **factorized spatial-temporal attention**. This is the core architectural innovation used by Video Diffusion Models (VDM), Imagen Video, Stable Video Diffusion, and many others.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand why full 3D attention is impractical for video\n",
    "- Build spatial self-attention (per-frame) and temporal self-attention (across-frames) from scratch\n",
    "- See how factorized attention achieves **16x savings** over full 3D attention\n",
    "- Implement a training strategy that leverages pretrained image models\n",
    "- Train a factorized video U-Net on Moving MNIST and compare it to the 3D conv approach"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_why_it_matters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Think about how a film editor works. They do not look at every pixel of every frame simultaneously â€” that would be overwhelming. Instead, they work in two passes:\n",
    "\n",
    "1. **Spatial pass:** Look at each frame individually â€” is the composition good? Are the colors right? Is the lighting consistent?\n",
    "2. **Temporal pass:** Play the frames in sequence â€” is the motion smooth? Do objects move naturally? Are there any visual jumps?\n",
    "\n",
    "This is exactly the idea behind **factorized attention** for video diffusion. Instead of trying to jointly attend over all space-time positions (which is absurdly expensive), we split the attention into two cheaper operations:\n",
    "- **Spatial attention** within each frame (what does each frame look like?)\n",
    "- **Temporal attention** across frames (how do things move over time?)\n",
    "\n",
    "This simple factorization is what makes video diffusion models practical. Without it, the computational cost would be prohibitive for any reasonable resolution."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_intuition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### ðŸ¤” Think About This\n",
    "\n",
    "Before we dive into the math, consider this thought experiment:\n",
    "\n",
    "You have a 16-frame video at 32Ã—32 resolution. If you wanted every pixel to \"talk to\" every other pixel across all frames (full 3D attention), how many attention pairs would that be?\n",
    "\n",
    "- Total positions: 16 Ã— 32 Ã— 32 = **16,384**\n",
    "- Attention pairs: 16,384Â² â‰ˆ **268 million**\n",
    "\n",
    "Now consider the factorized approach:\n",
    "- Spatial attention (per frame): 32 Ã— 32 = 1,024 positions, repeated 16 times â†’ 16 Ã— 1,024Â² â‰ˆ **16.8 million**\n",
    "- Temporal attention (per position): 16 frames, repeated 1,024 times â†’ 1,024 Ã— 16Â² â‰ˆ **262,000**\n",
    "\n",
    "Total: ~**17 million** vs ~**268 million** â€” that is a **16Ã— reduction**!\n",
    "\n",
    "The savings grow even more dramatically at higher resolutions. For a 256Ã—256 video, the ratio is over **1000Ã—**.\n",
    "\n",
    "### âœ‹ Stop and Think\n",
    "\n",
    "Before scrolling down, ask yourself:\n",
    "1. What information might we lose by factorizing? Can spatial-only attention capture an object moving across frames?\n",
    "2. Why do we do spatial attention first and temporal attention second (and not the other way)?\n",
    "3. Could there be scenarios where full 3D attention is actually better?\n",
    "\n",
    "*Take 2 minutes. Then scroll down.*"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_math"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Self-Attention Recap\n",
    "\n",
    "Recall that self-attention computes:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) \\cdot V$$\n",
    "\n",
    "where Q (queries), K (keys), and V (values) are linear projections of the input features.\n",
    "\n",
    "Computationally, this says: for each query position, compute a weighted sum over all value positions, where the weights are determined by the similarity between the query and each key, scaled by $\\sqrt{d_k}$ to prevent the dot products from becoming too large.\n",
    "\n",
    "### Spatial Self-Attention\n",
    "\n",
    "For spatial attention, we reshape the video features of shape $(B, T, H, W, C)$ so that the **time axis is merged with the batch axis**:\n",
    "\n",
    "$$\\mathbf{x}_{\\text{spatial}} \\in \\mathbb{R}^{(B \\cdot T) \\times (H \\cdot W) \\times C}$$\n",
    "\n",
    "Each frame is processed independently. The sequence length is $H \\times W$ â€” the number of spatial positions in one frame.\n",
    "\n",
    "This means frame 1 attends only to itself, frame 2 attends only to itself, and so on. No cross-frame information flows during spatial attention.\n",
    "\n",
    "### Temporal Self-Attention\n",
    "\n",
    "For temporal attention, we reshape so that the **spatial axes are merged with the batch axis**:\n",
    "\n",
    "$$\\mathbf{x}_{\\text{temporal}} \\in \\mathbb{R}^{(B \\cdot H \\cdot W) \\times T \\times C}$$\n",
    "\n",
    "Now, for each spatial position $(h, w)$, we have a sequence of $T$ features across time. The attention lets each frame \"look at\" all other frames at that same spatial position.\n",
    "\n",
    "Computationally, the pixel at position $(3, 7)$ in frame 5 can attend to position $(3, 7)$ in frames 1, 2, 3, ..., T â€” but NOT to position $(10, 15)$ in any frame. This is the key restriction that makes it efficient.\n",
    "\n",
    "### Complexity Comparison\n",
    "\n",
    "| Approach | Sequence Length | Complexity |\n",
    "|----------|----------------|------------|\n",
    "| Full 3D | $T \\cdot H \\cdot W$ | $O((T \\cdot H \\cdot W)^2)$ |\n",
    "| Spatial only | $H \\cdot W$ (Ã—T) | $O(T \\cdot (H \\cdot W)^2)$ |\n",
    "| Temporal only | $T$ (Ã—HÂ·W) | $O(H \\cdot W \\cdot T^2)$ |\n",
    "| **Factorized** | Both | $O(T \\cdot (H \\cdot W)^2 + H \\cdot W \\cdot T^2)$ |"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Building Components\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_building_components.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_building_components"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It â€” Component by Component"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Self Attention\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_self_attention.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_self_attention"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Multi-Head Self-Attention\n",
    "\n",
    "First, let us build a general-purpose multi-head self-attention module. We will reuse this for both spatial and temporal attention â€” the only difference is how we reshape the input."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard multi-head self-attention.\n",
    "    Input: (batch, seq_len, dim)\n",
    "    Output: (batch, seq_len, dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        assert dim % num_heads == 0, \"dim must be divisible by num_heads\"\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        # Project to Q, K, V\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, heads, N, head_dim)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        return self.proj(out)"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify it works with a small example."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Quick test\n",
    "attn = MultiHeadSelfAttention(dim=32, num_heads=4).to(device)\n",
    "test_input = torch.randn(2, 16, 32, device=device)  # batch=2, seq=16, dim=32\n",
    "test_output = attn(test_input)\n",
    "print(f\"Input shape:  {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "assert test_input.shape == test_output.shape\n",
    "print(\"âœ… Self-attention module works!\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Spatial Attention\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_spatial_attention.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_spatial_attention"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Spatial Attention Block\n",
    "\n",
    "Now let us wrap the self-attention into a **spatial attention** block. The key idea: merge the time axis into the batch dimension, so each frame is processed independently."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-attention over spatial positions within each frame.\n",
    "    Input: (B, T, H, W, C) -> reshape to (B*T, H*W, C) -> attend -> reshape back\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attn = MultiHeadSelfAttention(dim, num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, H, W, C)\n",
    "        B, T, H, W, C = x.shape\n",
    "\n",
    "        # Merge time into batch: each frame is independent\n",
    "        x_flat = x.reshape(B * T, H * W, C)\n",
    "\n",
    "        # Self-attention with residual connection\n",
    "        x_flat = x_flat + self.attn(self.norm(x_flat))\n",
    "\n",
    "        # Reshape back\n",
    "        return x_flat.reshape(B, T, H, W, C)"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualize what spatial attention \"sees\"\n",
    "# Each frame is processed independently â€” no cross-frame information\n",
    "B, T, H, W, C = 1, 4, 8, 8, 32\n",
    "test_video = torch.randn(B, T, H, W, C, device=device)\n",
    "\n",
    "spatial_block = SpatialAttentionBlock(dim=C).to(device)\n",
    "output = spatial_block(test_video)\n",
    "\n",
    "print(f\"Input shape:  {test_video.shape}  (B, T, H, W, C)\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Verify frames are processed independently:\n",
    "# If we change frame 3, only frame 3's output should change\n",
    "test_video_modified = test_video.clone()\n",
    "test_video_modified[:, 2] = torch.randn(B, H, W, C, device=device)\n",
    "\n",
    "output_orig = spatial_block(test_video)\n",
    "output_mod = spatial_block(test_video_modified)\n",
    "\n",
    "# Frames 0, 1, 3 should be unchanged\n",
    "for t in [0, 1, 3]:\n",
    "    diff = (output_orig[:, t] - output_mod[:, t]).abs().max().item()\n",
    "    print(f\"  Frame {t} max diff: {diff:.6f} (should be ~0)\")\n",
    "\n",
    "diff_f2 = (output_orig[:, 2] - output_mod[:, 2]).abs().max().item()\n",
    "print(f\"  Frame 2 max diff: {diff_f2:.4f} (should be large)\")\n",
    "print(\"âœ… Spatial attention processes each frame independently!\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Temporal Attention\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_temporal_attention.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_temporal_attention"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Temporal Attention Block\n",
    "\n",
    "Next, the **temporal attention** block. Here, we merge the spatial axes into the batch dimension, so each spatial position attends across all frames."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-attention across frames at each spatial position.\n",
    "    Input: (B, T, H, W, C) -> reshape to (B*H*W, T, C) -> attend -> reshape back\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attn = MultiHeadSelfAttention(dim, num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, H, W, C)\n",
    "        B, T, H, W, C = x.shape\n",
    "\n",
    "        # Merge spatial into batch: each position attends across time\n",
    "        x_flat = x.permute(0, 2, 3, 1, 4).reshape(B * H * W, T, C)\n",
    "\n",
    "        # Self-attention with residual connection\n",
    "        x_flat = x_flat + self.attn(self.norm(x_flat))\n",
    "\n",
    "        # Reshape back\n",
    "        return x_flat.reshape(B, H, W, T, C).permute(0, 3, 1, 2, 4)"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualize what temporal attention \"sees\"\n",
    "temporal_block = TemporalAttentionBlock(dim=C).to(device)\n",
    "output_temporal = temporal_block(test_video)\n",
    "\n",
    "print(f\"Input shape:  {test_video.shape}  (B, T, H, W, C)\")\n",
    "print(f\"Output shape: {output_temporal.shape}\")\n",
    "\n",
    "# Verify: changing one spatial position should NOT affect other positions\n",
    "test_video_2 = test_video.clone()\n",
    "test_video_2[:, :, 5, 5] = torch.randn(B, T, C, device=device)\n",
    "\n",
    "out_a = temporal_block(test_video)\n",
    "out_b = temporal_block(test_video_2)\n",
    "\n",
    "# Position (3,3) should be unchanged\n",
    "diff_pos = (out_a[:, :, 3, 3] - out_b[:, :, 3, 3]).abs().max().item()\n",
    "print(f\"  Position (3,3) max diff: {diff_pos:.6f} (should be ~0)\")\n",
    "\n",
    "# Position (5,5) should change\n",
    "diff_mod = (out_a[:, :, 5, 5] - out_b[:, :, 5, 5]).abs().max().item()\n",
    "print(f\"  Position (5,5) max diff: {diff_mod:.4f} (should be large)\")\n",
    "print(\"âœ… Temporal attention processes each spatial position independently across time!\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Factorized Block\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_factorized_block.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_factorized_block"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Factorized Space-Time Block\n",
    "\n",
    "Now we combine spatial and temporal attention into a single **factorized space-time block**. This is the fundamental building block of modern video diffusion architectures."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizedSpaceTimeBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single block of factorized spatial-temporal attention.\n",
    "    1. Spatial self-attention (within each frame)\n",
    "    2. Temporal self-attention (across frames at each position)\n",
    "    3. Feedforward network\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=4, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.spatial_attn = SpatialAttentionBlock(dim, num_heads)\n",
    "        self.temporal_attn = TemporalAttentionBlock(dim, num_heads)\n",
    "\n",
    "        # Feedforward network\n",
    "        self.norm_ff = nn.LayerNorm(dim)\n",
    "        hidden_dim = int(dim * mlp_ratio)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, H, W, C)\n",
    "        # Step 1: Spatial attention (per frame)\n",
    "        x = self.spatial_attn(x)\n",
    "\n",
    "        # Step 2: Temporal attention (across frames)\n",
    "        x = self.temporal_attn(x)\n",
    "\n",
    "        # Step 3: Feedforward with residual\n",
    "        B, T, H, W, C = x.shape\n",
    "        x_flat = x.reshape(B * T * H * W, C)\n",
    "        x_flat = x_flat + self.ff(self.norm_ff(x_flat))\n",
    "        return x_flat.reshape(B, T, H, W, C)"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Test the full factorized block\n",
    "block = FactorizedSpaceTimeBlock(dim=32, num_heads=4).to(device)\n",
    "test_input = torch.randn(2, 4, 8, 8, 32, device=device)\n",
    "test_output = block(test_input)\n",
    "\n",
    "print(f\"Input:  {test_input.shape}\")\n",
    "print(f\"Output: {test_output.shape}\")\n",
    "\n",
    "num_params = sum(p.numel() for p in block.parameters())\n",
    "print(f\"Parameters: {num_params:,}\")\n",
    "print(\"âœ… Factorized space-time block works!\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Cost Comparison\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_cost_comparison.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_cost_comparison"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ðŸ“Š Computational Cost Comparison\n",
    "\n",
    "Let us empirically verify the cost savings of factorized vs full 3D attention."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Full3DAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Full 3D self-attention over all space-time positions.\n",
    "    Input: (B, T, H, W, C) -> reshape to (B, T*H*W, C) -> attend\n",
    "    WARNING: Very expensive for large inputs!\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attn = MultiHeadSelfAttention(dim, num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, H, W, C = x.shape\n",
    "        x_flat = x.reshape(B, T * H * W, C)\n",
    "        x_flat = x_flat + self.attn(self.norm(x_flat))\n",
    "        return x_flat.reshape(B, T, H, W, C)"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare computation time: Factorized vs Full 3D\n",
    "results = {}\n",
    "resolutions = [(4, 8, 8), (4, 16, 16), (8, 16, 16), (8, 32, 32)]\n",
    "\n",
    "for T, H, W in resolutions:\n",
    "    label = f\"T={T}, {H}x{W}\"\n",
    "    seq_len = T * H * W\n",
    "    x = torch.randn(1, T, H, W, 32, device=device)\n",
    "\n",
    "    # Factorized\n",
    "    fact_block = FactorizedSpaceTimeBlock(dim=32).to(device)\n",
    "    torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = fact_block(x)\n",
    "    torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "    fact_time = (time.time() - start) / 10\n",
    "\n",
    "    # Full 3D (skip if too large)\n",
    "    if seq_len <= 4096:\n",
    "        full_block = Full3DAttention(dim=32).to(device)\n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        start = time.time()\n",
    "        for _ in range(10):\n",
    "            _ = full_block(x)\n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        full_time = (time.time() - start) / 10\n",
    "        speedup = full_time / fact_time\n",
    "    else:\n",
    "        full_time = float('nan')\n",
    "        speedup = float('nan')\n",
    "\n",
    "    results[label] = (fact_time, full_time, speedup)\n",
    "    print(f\"{label} (seq={seq_len:>5}): \"\n",
    "          f\"Factorized={fact_time*1000:.1f}ms, \"\n",
    "          f\"Full3D={full_time*1000:.1f}ms, \"\n",
    "          f\"Speedup={speedup:.1f}x\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualize the scaling\n",
    "labels = list(results.keys())\n",
    "fact_times = [results[l][0]*1000 for l in labels]\n",
    "full_times = [results[l][1]*1000 for l in labels]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x_pos = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x_pos - width/2, fact_times, width, label='Factorized', color='#4c72b0')\n",
    "bars2 = ax.bar(x_pos + width/2, full_times, width, label='Full 3D', color='#dd8452')\n",
    "\n",
    "ax.set_xlabel('Video Resolution')\n",
    "ax.set_ylabel('Time per forward pass (ms)')\n",
    "ax.set_title('Factorized vs Full 3D Attention â€” Computation Time')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(labels, rotation=15)\n",
    "ax.legend()\n",
    "\n",
    "# Add speedup annotations\n",
    "for i, label in enumerate(labels):\n",
    "    speedup = results[label][2]\n",
    "    if not np.isnan(speedup):\n",
    "        ax.annotate(f'{speedup:.1f}x faster',\n",
    "                   xy=(x_pos[i], max(fact_times[i], full_times[i])),\n",
    "                   xytext=(0, 10), textcoords='offset points',\n",
    "                   ha='center', fontsize=9, color='green', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"ðŸ’¡ The savings grow dramatically with resolution!\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Full Unet\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_full_unet.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_full_unet"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Building a Factorized Video U-Net\n",
    "\n",
    "Now let us put it all together into a complete U-Net architecture with factorized attention. We will compare it against the 3D conv model from Notebook 1.\n",
    "\n",
    "### 6.1 Dataset (same Moving MNIST)"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_moving_mnist_video(num_frames=16, size=32, digit_size=12):\n",
    "    \"\"\"Create a single Moving MNIST video: a white digit bouncing around.\"\"\"\n",
    "    video = np.zeros((num_frames, size, size), dtype=np.float32)\n",
    "\n",
    "    # Random starting position and velocity\n",
    "    x = np.random.randint(0, size - digit_size)\n",
    "    y = np.random.randint(0, size - digit_size)\n",
    "    vx = np.random.choice([-2, -1, 1, 2])\n",
    "    vy = np.random.choice([-2, -1, 1, 2])\n",
    "\n",
    "    # Simple digit pattern (circle-like blob)\n",
    "    digit = np.zeros((digit_size, digit_size), dtype=np.float32)\n",
    "    center = digit_size // 2\n",
    "    for i in range(digit_size):\n",
    "        for j in range(digit_size):\n",
    "            dist = ((i - center)**2 + (j - center)**2) ** 0.5\n",
    "            if dist < center:\n",
    "                digit[i, j] = 1.0\n",
    "\n",
    "    for t in range(num_frames):\n",
    "        # Place digit\n",
    "        video[t, y:y+digit_size, x:x+digit_size] = digit\n",
    "\n",
    "        # Move with bouncing\n",
    "        x += vx\n",
    "        y += vy\n",
    "        if x <= 0 or x >= size - digit_size:\n",
    "            vx = -vx\n",
    "            x = max(0, min(size - digit_size, x))\n",
    "        if y <= 0 or y >= size - digit_size:\n",
    "            vy = -vy\n",
    "            y = max(0, min(size - digit_size, y))\n",
    "\n",
    "    return video\n",
    "\n",
    "def create_dataset(num_videos=512, num_frames=16, size=32):\n",
    "    \"\"\"Create a dataset of Moving MNIST videos.\"\"\"\n",
    "    videos = np.stack([create_moving_mnist_video(num_frames, size) for _ in range(num_videos)])\n",
    "    # Shape: (N, T, H, W) -> (N, 1, T, H, W) for channel dim\n",
    "    return torch.tensor(videos).unsqueeze(1)\n",
    "\n",
    "# Create dataset\n",
    "dataset = create_dataset(num_videos=512, num_frames=8, size=32)\n",
    "print(f\"Dataset shape: {dataset.shape}  (N, C, T, H, W)\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualize some training samples\n",
    "fig, axes = plt.subplots(3, 8, figsize=(14, 5))\n",
    "for row in range(3):\n",
    "    for col in range(8):\n",
    "        axes[row, col].imshow(dataset[row, 0, col].numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "        axes[row, col].axis('off')\n",
    "        if row == 0:\n",
    "            axes[row, col].set_title(f'f={col}', fontsize=9)\n",
    "fig.suptitle('Moving MNIST Training Samples', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Factorized Video U-Net"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalTimeEmbed(nn.Module):\n",
    "    \"\"\"Sinusoidal timestep embedding.\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        half = self.dim // 2\n",
    "        freqs = torch.exp(-math.log(10000) * torch.arange(half, device=t.device) / half)\n",
    "        args = t[:, None].float() * freqs[None, :]\n",
    "        emb = torch.cat([args.sin(), args.cos()], dim=-1)\n",
    "        return self.mlp(emb)"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizedVideoUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple U-Net with factorized spatial-temporal attention.\n",
    "\n",
    "    Architecture:\n",
    "    - Encoder: 2D conv (spatial) â†’ factorized attention\n",
    "    - Bottleneck: factorized attention\n",
    "    - Decoder: factorized attention â†’ 2D conv (spatial)\n",
    "\n",
    "    All convolutions are 2D (per frame). Temporal modeling is\n",
    "    handled entirely by the temporal attention layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, base_dim=32, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.time_embed = SinusoidalTimeEmbed(base_dim)\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, base_dim, 3, padding=1),\n",
    "            nn.GroupNorm(4, base_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.enc1_attn = FactorizedSpaceTimeBlock(base_dim, num_heads)\n",
    "\n",
    "        self.enc2_conv = nn.Sequential(\n",
    "            nn.Conv2d(base_dim, base_dim * 2, 3, stride=2, padding=1),\n",
    "            nn.GroupNorm(4, base_dim * 2),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.enc2_attn = FactorizedSpaceTimeBlock(base_dim * 2, num_heads)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = FactorizedSpaceTimeBlock(base_dim * 2, num_heads)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec2_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_dim * 4, base_dim, 2, stride=2),\n",
    "            nn.GroupNorm(4, base_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.dec2_attn = FactorizedSpaceTimeBlock(base_dim, num_heads)\n",
    "\n",
    "        self.out_conv = nn.Conv2d(base_dim * 2, in_channels, 1)\n",
    "\n",
    "        # Time projection layers\n",
    "        self.time_proj1 = nn.Linear(base_dim, base_dim)\n",
    "        self.time_proj2 = nn.Linear(base_dim, base_dim * 2)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        x: (B, C, T, H, W) - noisy video\n",
    "        t: (B,) - diffusion timestep\n",
    "        Returns: (B, C, T, H, W) - predicted noise\n",
    "        \"\"\"\n",
    "        B, C, T, H, W = x.shape\n",
    "        t_emb = self.time_embed(t)  # (B, base_dim)\n",
    "\n",
    "        # Process each frame with 2D convs, then do factorized attention\n",
    "        # Reshape: (B, C, T, H, W) -> (B*T, C, H, W) for 2D convs\n",
    "        x_frames = x.permute(0, 2, 1, 3, 4).reshape(B * T, C, H, W)\n",
    "\n",
    "        # Encoder block 1\n",
    "        h1 = self.enc1_conv(x_frames)  # (B*T, base_dim, H, W)\n",
    "        # Add time embedding\n",
    "        t1 = self.time_proj1(t_emb)[:, :, None, None]  # (B, base_dim, 1, 1)\n",
    "        t1 = t1.repeat(1, 1, 1, 1).repeat_interleave(T, dim=0)  # (B*T, base_dim, 1, 1)\n",
    "        h1 = h1 + t1\n",
    "        # Reshape for attention: (B*T, C, H, W) -> (B, T, H, W, C)\n",
    "        h1_attn = h1.reshape(B, T, -1, H, W).permute(0, 1, 3, 4, 2)\n",
    "        h1_attn = self.enc1_attn(h1_attn)\n",
    "        h1 = h1_attn.permute(0, 1, 4, 2, 3).reshape(B * T, -1, H, W)\n",
    "\n",
    "        # Encoder block 2 (with downsampling)\n",
    "        h2 = self.enc2_conv(h1)  # (B*T, base_dim*2, H/2, W/2)\n",
    "        H2, W2 = H // 2, W // 2\n",
    "        t2 = self.time_proj2(t_emb)[:, :, None, None].repeat_interleave(T, dim=0)\n",
    "        h2 = h2 + t2\n",
    "        h2_attn = h2.reshape(B, T, -1, H2, W2).permute(0, 1, 3, 4, 2)\n",
    "        h2_attn = self.enc2_attn(h2_attn)\n",
    "        h2 = h2_attn.permute(0, 1, 4, 2, 3).reshape(B * T, -1, H2, W2)\n",
    "\n",
    "        # Bottleneck\n",
    "        bot = h2.reshape(B, T, -1, H2, W2).permute(0, 1, 3, 4, 2)\n",
    "        bot = self.bottleneck(bot)\n",
    "        bot = bot.permute(0, 1, 4, 2, 3).reshape(B * T, -1, H2, W2)\n",
    "\n",
    "        # Decoder with skip connection\n",
    "        dec = torch.cat([bot, h2], dim=1)  # Skip connection\n",
    "        dec = self.dec2_conv(dec)  # (B*T, base_dim, H, W)\n",
    "        dec_attn = dec.reshape(B, T, -1, H, W).permute(0, 1, 3, 4, 2)\n",
    "        dec_attn = self.dec2_attn(dec_attn)\n",
    "        dec = dec_attn.permute(0, 1, 4, 2, 3).reshape(B * T, -1, H, W)\n",
    "\n",
    "        # Final output with skip connection\n",
    "        out = self.out_conv(torch.cat([dec, h1], dim=1))  # (B*T, C, H, W)\n",
    "        return out.reshape(B, T, C, H, W).permute(0, 2, 1, 3, 4)"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model = FactorizedVideoUNet(in_channels=1, base_dim=32).to(device)\n",
    "test_x = torch.randn(2, 1, 8, 32, 32, device=device)\n",
    "test_t = torch.randint(0, 1000, (2,), device=device)\n",
    "test_out = model(test_x, test_t)\n",
    "\n",
    "print(f\"Input:  {test_x.shape}\")\n",
    "print(f\"Output: {test_out.shape}\")\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Parameters: {num_params:,}\")\n",
    "print(\"âœ… Factorized Video U-Net works!\")"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_todo_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_11_todo_training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ðŸ”§ Your Turn â€” Implement the Training Step\n",
    "\n",
    "### TODO: Complete the training function\n",
    "\n",
    "This is the same diffusion training loop from Notebook 1, but now with our factorized attention model. Complete the missing pieces:"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"Cosine noise schedule (same as Notebook 1).\"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clamp(betas, 0.0001, 0.999)\n",
    "\n",
    "# Precompute schedule values\n",
    "num_timesteps = 500\n",
    "betas = cosine_beta_schedule(num_timesteps).to(device)\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
    "\n",
    "\n",
    "def train_one_step(model, optimizer, video_batch):\n",
    "    \"\"\"\n",
    "    Perform one training step of the diffusion model.\n",
    "\n",
    "    Args:\n",
    "        model: the noise prediction network\n",
    "        optimizer: Adam optimizer\n",
    "        video_batch: clean videos, shape (B, 1, T, H, W)\n",
    "\n",
    "    Returns:\n",
    "        loss value (float)\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    B = video_batch.shape[0]\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Sample random timesteps for each video in the batch\n",
    "    #         t should be integers in [0, num_timesteps)\n",
    "    # Step 2: Sample random Gaussian noise (same shape as video_batch)\n",
    "    # Step 3: Create the noisy video using the forward diffusion formula:\n",
    "    #         noisy = sqrt_alphas_cumprod[t] * clean + sqrt_one_minus_alphas_cumprod[t] * noise\n",
    "    #         (Hint: you need to reshape the schedule values for broadcasting)\n",
    "    # Step 4: Predict the noise using the model\n",
    "    # Step 5: Compute MSE loss between predicted and actual noise\n",
    "    # Step 6: Backpropagate and update weights\n",
    "    # ==============================\n",
    "\n",
    "    t = ???  # YOUR CODE HERE (Step 1)\n",
    "    noise = ???  # YOUR CODE HERE (Step 2)\n",
    "    noisy_video = ???  # YOUR CODE HERE (Step 3)\n",
    "    predicted_noise = ???  # YOUR CODE HERE (Step 4)\n",
    "    loss = ???  # YOUR CODE HERE (Step 5)\n",
    "\n",
    "    loss.backward()  # Step 6\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Verification: Test your implementation\n",
    "_test_model = FactorizedVideoUNet(in_channels=1, base_dim=32).to(device)\n",
    "_test_opt = torch.optim.Adam(_test_model.parameters(), lr=1e-3)\n",
    "_test_batch = dataset[:4].to(device)\n",
    "\n",
    "try:\n",
    "    _test_loss = train_one_step(_test_model, _test_opt, _test_batch)\n",
    "    assert isinstance(_test_loss, float), \"Loss should be a float\"\n",
    "    assert 0 < _test_loss < 10, f\"Loss {_test_loss} seems wrong (expected 0-10 range)\"\n",
    "    print(f\"âœ… Training step works! Loss = {_test_loss:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(\"Hint: Make sure t has shape (B,), noise has same shape as video_batch,\")\n",
    "    print(\"and schedule values are reshaped to (B, 1, 1, 1, 1) for broadcasting.\")\n",
    "\n",
    "del _test_model, _test_opt"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Training Results\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_training_results.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_12_training_results"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training the Model"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "model = FactorizedVideoUNet(in_channels=1, base_dim=32).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "\n",
    "num_epochs = 30\n",
    "batch_size = 16\n",
    "losses = []\n",
    "\n",
    "print(\"Training factorized video diffusion model...\")\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "\n",
    "    # Shuffle dataset\n",
    "    perm = torch.randperm(len(dataset))\n",
    "\n",
    "    for i in range(0, len(dataset) - batch_size + 1, batch_size):\n",
    "        batch = dataset[perm[i:i+batch_size]].to(device)\n",
    "        loss = train_one_step(model, optimizer, batch)\n",
    "        epoch_losses.append(loss)\n",
    "\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"  Epoch {epoch+1}/{num_epochs} â€” Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Training curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Factorized Video U-Net â€” Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Sampling"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_videos(model, num_samples=8, num_frames=8, size=32, num_channels=1):\n",
    "    \"\"\"Generate videos using DDPM sampling.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Start from pure noise\n",
    "    x = torch.randn(num_samples, num_channels, num_frames, size, size, device=device)\n",
    "\n",
    "    for t_idx in reversed(range(num_timesteps)):\n",
    "        t = torch.full((num_samples,), t_idx, device=device, dtype=torch.long)\n",
    "\n",
    "        # Predict noise\n",
    "        predicted_noise = model(x, t)\n",
    "\n",
    "        # DDPM update step\n",
    "        beta_t = betas[t_idx]\n",
    "        alpha_t = alphas[t_idx]\n",
    "        alpha_cumprod_t = alphas_cumprod[t_idx]\n",
    "\n",
    "        # Mean of p(x_{t-1} | x_t)\n",
    "        coeff1 = 1.0 / torch.sqrt(alpha_t)\n",
    "        coeff2 = beta_t / torch.sqrt(1.0 - alpha_cumprod_t)\n",
    "        mean = coeff1 * (x - coeff2 * predicted_noise)\n",
    "\n",
    "        # Add noise (except at t=0)\n",
    "        if t_idx > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "            sigma = torch.sqrt(beta_t)\n",
    "            x = mean + sigma * noise\n",
    "        else:\n",
    "            x = mean\n",
    "\n",
    "    model.train()\n",
    "    return x.clamp(0, 1)\n",
    "\n",
    "print(\"Generating videos...\")\n",
    "generated = sample_videos(model, num_samples=8)\n",
    "print(f\"Generated shape: {generated.shape}\")"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Display generated videos\n",
    "fig, axes = plt.subplots(4, 8, figsize=(14, 7))\n",
    "for row in range(4):\n",
    "    for col in range(8):\n",
    "        axes[row, col].imshow(generated[row, 0, col].cpu().numpy(),\n",
    "                             cmap='gray', vmin=0, vmax=1)\n",
    "        axes[row, col].axis('off')\n",
    "        if row == 0:\n",
    "            axes[row, col].set_title(f'f={col}', fontsize=9)\n",
    "    axes[row, 0].set_ylabel(f'Video {row+1}', fontsize=10, rotation=0, labelpad=45)\n",
    "\n",
    "fig.suptitle('Generated Videos â€” Factorized Attention Model', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo Causal\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_todo_causal.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_13_todo_causal"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ðŸ”§ Your Turn â€” Experiment with Attention Masking\n",
    "\n",
    "### TODO: Implement causal temporal attention\n",
    "\n",
    "In the current temporal attention, every frame can attend to every other frame (bidirectional). But what if we want **causal** temporal attention, where each frame can only attend to past frames? This is useful for autoregressive video generation."
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalTemporalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal self-attention with a causal mask.\n",
    "    Frame t can only attend to frames 0, 1, ..., t (not future frames).\n",
    "\n",
    "    Input: (B, T, H, W, C)\n",
    "    Output: (B, T, H, W, C)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, H, W, C = x.shape\n",
    "\n",
    "        # Merge spatial into batch\n",
    "        x_flat = x.permute(0, 2, 3, 1, 4).reshape(B * H * W, T, C)\n",
    "        x_normed = self.norm(x_flat)\n",
    "\n",
    "        BHW, T, C = x_normed.shape\n",
    "        qkv = self.qkv(x_normed).reshape(BHW, T, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        # ============ TODO ============\n",
    "        # Create a causal mask that prevents attending to future frames.\n",
    "        # The mask should be a (T, T) boolean tensor where:\n",
    "        #   mask[i, j] = True if frame i should NOT attend to frame j\n",
    "        #   (i.e., True for j > i â€” future frames)\n",
    "        # Apply the mask by setting masked positions to -inf before softmax\n",
    "        # ==============================\n",
    "\n",
    "        causal_mask = ???  # YOUR CODE HERE\n",
    "        attn = ???  # YOUR CODE HERE: apply the mask\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).reshape(BHW, T, C)\n",
    "        out = self.proj(out)\n",
    "\n",
    "        # Residual + reshape back\n",
    "        x_flat = x_flat + out\n",
    "        return x_flat.reshape(B, H, W, T, C).permute(0, 3, 1, 2, 4)"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Verification\n",
    "causal = CausalTemporalAttention(dim=32, num_heads=4).to(device)\n",
    "test_in = torch.randn(1, 4, 4, 4, 32, device=device)\n",
    "\n",
    "try:\n",
    "    test_out = causal(test_in)\n",
    "    assert test_out.shape == test_in.shape, f\"Shape mismatch: {test_out.shape}\"\n",
    "\n",
    "    # Verify causality: changing frame 3 should NOT affect frames 0, 1, 2\n",
    "    test_in_mod = test_in.clone()\n",
    "    test_in_mod[:, 3] = torch.randn(1, 4, 4, 32, device=device)\n",
    "\n",
    "    out_orig = causal(test_in)\n",
    "    out_mod = causal(test_in_mod)\n",
    "\n",
    "    for t in range(3):\n",
    "        diff = (out_orig[:, t] - out_mod[:, t]).abs().max().item()\n",
    "        assert diff < 1e-5, f\"Frame {t} changed when it shouldn't have! diff={diff}\"\n",
    "\n",
    "    diff_3 = (out_orig[:, 3] - out_mod[:, 3]).abs().max().item()\n",
    "    assert diff_3 > 0.01, \"Frame 3 didn't change when it should have!\"\n",
    "\n",
    "    print(\"âœ… Causal temporal attention is correct!\")\n",
    "    print(f\"   Frames 0-2 unaffected by changes to frame 3 âœ“\")\n",
    "    print(f\"   Frame 3 properly updated âœ“\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(\"Hint: Use torch.triu to create an upper triangular mask,\")\n",
    "    print(\"then apply it with masked_fill(-inf) before softmax.\")"
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Final Output\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_final_output.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_14_final_output"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ðŸŽ¯ Final Output â€” Side-by-Side Comparison\n",
    "\n",
    "Let us compare the factorized attention model with a simpler baseline to appreciate the architectural differences."
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a final gallery of videos\n",
    "print(\"Generating final showcase...\")\n",
    "final_videos = sample_videos(model, num_samples=8)\n",
    "\n",
    "fig, axes = plt.subplots(8, 8, figsize=(14, 14))\n",
    "for row in range(8):\n",
    "    for col in range(8):\n",
    "        axes[row, col].imshow(final_videos[row, 0, col].cpu().numpy(),\n",
    "                             cmap='gray', vmin=0, vmax=1)\n",
    "        axes[row, col].axis('off')\n",
    "        if row == 0:\n",
    "            axes[row, col].set_title(f'Frame {col}', fontsize=9)\n",
    "    axes[row, 0].set_ylabel(f'Video {row+1}', fontsize=9, rotation=0, labelpad=40)\n",
    "\n",
    "fig.suptitle('ðŸŽ¯ Generated Videos â€” Factorized Space-Time Attention\\n'\n",
    "             'Each row is a separate video showing temporal coherence', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"ðŸŽ‰ You've built factorized space-time attention from scratch!\")"
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Reflection\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/15_reflection.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_15_reflection"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Reflection and Next Steps\n",
    "\n",
    "### ðŸ¤” Reflection Questions\n",
    "\n",
    "1. **Why spatial first?** We do spatial attention before temporal attention. What would happen if we reversed the order? Would the model still work? Would training be different?\n",
    "\n",
    "2. **Information bottleneck:** In factorized attention, a pixel at (3, 7) in frame 5 can only \"communicate\" with other frames through temporal attention at position (3, 7). What information might be lost? How could we mitigate this?\n",
    "\n",
    "3. **Causal vs bidirectional:** We implemented both causal and bidirectional temporal attention. When would you use each? Think about autoregressive generation vs. denoising.\n",
    "\n",
    "4. **Scaling behavior:** As video resolution increases, which component becomes the bottleneck â€” spatial or temporal attention? What does this tell us about designing architectures for high-resolution video?\n",
    "\n",
    "### ðŸ† Optional Challenges\n",
    "\n",
    "1. **Alternating order:** Modify the factorized block to alternate â€” odd layers do spatialâ†’temporal, even layers do temporalâ†’spatial. Does this help?\n",
    "\n",
    "2. **Local temporal attention:** Instead of attending to ALL frames, restrict temporal attention to a sliding window of Â±3 frames. How does this affect quality vs speed?\n",
    "\n",
    "3. **Cross-frame spatial attention:** Allow limited cross-frame communication in spatial attention by including features from adjacent frames as extra keys/values.\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In Notebook 3, we will tackle two final pieces of the puzzle:\n",
    "- **Latent Video Diffusion:** Compressing videos with a VAE before running diffusion, for massive efficiency gains\n",
    "- **Diffusion Transformers (DiT):** Replacing the U-Net entirely with a Transformer using spacetime patches"
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "chatbot"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸ’¬ AI Teaching Assistant â€” Click â–¶ to start\n",
    "#@markdown This AI chatbot reads your notebook and can answer questions about any concept, code, or exercise.\n",
    "\n",
    "import json as _json\n",
    "import requests as _requests\n",
    "from google.colab import output as _output\n",
    "from IPython.display import display, HTML as _HTML, Markdown as _Markdown\n",
    "\n",
    "# --- Read notebook content for context ---\n",
    "def _get_notebook_context():\n",
    "    try:\n",
    "        from google.colab import _message\n",
    "        nb = _message.blocking_request(\"get_ipynb\", request=\"\", timeout_sec=10)\n",
    "        cells = nb.get(\"ipynb\", {}).get(\"cells\", [])\n",
    "        parts = []\n",
    "        for cell in cells:\n",
    "            src = \"\".join(cell.get(\"source\", []))\n",
    "            tags = cell.get(\"metadata\", {}).get(\"tags\", [])\n",
    "            if \"chatbot\" in tags:\n",
    "                continue\n",
    "            if src.strip():\n",
    "                ct = cell.get(\"cell_type\", \"unknown\")\n",
    "                parts.append(f\"[{ct.upper()}]\\n{src}\")\n",
    "        return \"\\n\\n---\\n\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"Notebook content unavailable.\"\n",
    "\n",
    "_NOTEBOOK_CONTEXT = _get_notebook_context()\n",
    "_CHAT_HISTORY = []\n",
    "_API_URL = \"https://course-creator-brown.vercel.app/api/chat\"\n",
    "\n",
    "def _notebook_chat(question):\n",
    "    global _CHAT_HISTORY\n",
    "    try:\n",
    "        resp = _requests.post(_API_URL, json={\n",
    "            'question': question,\n",
    "            'context': _NOTEBOOK_CONTEXT[:100000],\n",
    "            'history': _CHAT_HISTORY[-10:],\n",
    "        }, timeout=60)\n",
    "        data = resp.json()\n",
    "        answer = data.get('answer', 'Sorry, I could not generate a response.')\n",
    "        _CHAT_HISTORY.append({'role': 'user', 'content': question})\n",
    "        _CHAT_HISTORY.append({'role': 'assistant', 'content': answer})\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f'Error connecting to teaching assistant: {str(e)}'\n",
    "\n",
    "_output.register_callback('notebook_chat', _notebook_chat)\n",
    "\n",
    "def ask(question):\n",
    "    \"\"\"Ask the AI teaching assistant a question about this notebook.\"\"\"\n",
    "    answer = _notebook_chat(question)\n",
    "    display(_Markdown(answer))\n",
    "\n",
    "print(\"\\u2705 AI Teaching Assistant is ready!\")\n",
    "print(\"\\U0001f4a1 Use the chat below, or call ask(\\'your question\\') in any cell.\")\n",
    "\n",
    "# --- Display chat widget ---\n",
    "display(_HTML('''<style>\n  .vc-wrap{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;max-width:100%;border-radius:16px;overflow:hidden;box-shadow:0 4px 24px rgba(0,0,0,.12);background:#fff;border:1px solid #e5e7eb}\n  .vc-hdr{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;padding:16px 20px;display:flex;align-items:center;gap:12px}\n  .vc-avatar{width:42px;height:42px;background:rgba(255,255,255,.2);border-radius:50%;display:flex;align-items:center;justify-content:center;font-size:22px}\n  .vc-hdr h3{font-size:16px;font-weight:600;margin:0}\n  .vc-hdr p{font-size:12px;opacity:.85;margin:2px 0 0}\n  .vc-msgs{height:420px;overflow-y:auto;padding:16px;background:#f8f9fb;display:flex;flex-direction:column;gap:10px}\n  .vc-msg{display:flex;flex-direction:column;animation:vc-fade .25s ease}\n  .vc-msg.user{align-items:flex-end}\n  .vc-msg.bot{align-items:flex-start}\n  .vc-bbl{max-width:85%;padding:10px 14px;border-radius:16px;font-size:14px;line-height:1.55;word-wrap:break-word}\n  .vc-msg.user .vc-bbl{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border-bottom-right-radius:4px}\n  .vc-msg.bot .vc-bbl{background:#fff;color:#1a1a2e;border:1px solid #e8e8e8;border-bottom-left-radius:4px}\n  .vc-bbl code{background:rgba(0,0,0,.07);padding:2px 6px;border-radius:4px;font-size:13px;font-family:'Fira Code',monospace}\n  .vc-bbl pre{background:#1e1e2e;color:#cdd6f4;padding:12px;border-radius:8px;overflow-x:auto;margin:8px 0;font-size:13px}\n  .vc-bbl pre code{background:none;padding:0;color:inherit}\n  .vc-bbl h3,.vc-bbl h4{margin:10px 0 4px;font-size:15px}\n  .vc-bbl ul,.vc-bbl ol{margin:4px 0;padding-left:20px}\n  .vc-bbl li{margin:2px 0}\n  .vc-chips{display:flex;flex-wrap:wrap;gap:8px;padding:0 16px 12px;background:#f8f9fb}\n  .vc-chip{background:#fff;border:1px solid #d1d5db;border-radius:20px;padding:6px 14px;font-size:12px;cursor:pointer;transition:all .15s;color:#4b5563}\n  .vc-chip:hover{border-color:#667eea;color:#667eea;background:#f0f0ff}\n  .vc-input{display:flex;padding:12px 16px;background:#fff;border-top:1px solid #eee;gap:8px}\n  .vc-input input{flex:1;padding:10px 16px;border:2px solid #e8e8e8;border-radius:24px;font-size:14px;outline:none;transition:border-color .2s}\n  .vc-input input:focus{border-color:#667eea}\n  .vc-input button{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border:none;border-radius:50%;width:42px;height:42px;cursor:pointer;display:flex;align-items:center;justify-content:center;font-size:18px;transition:transform .1s}\n  .vc-input button:hover{transform:scale(1.05)}\n  .vc-input button:disabled{opacity:.5;cursor:not-allowed;transform:none}\n  .vc-typing{display:flex;gap:5px;padding:4px 0}\n  .vc-typing span{width:8px;height:8px;background:#667eea;border-radius:50%;animation:vc-bounce 1.4s infinite ease-in-out}\n  .vc-typing span:nth-child(2){animation-delay:.2s}\n  .vc-typing span:nth-child(3){animation-delay:.4s}\n  @keyframes vc-bounce{0%,80%,100%{transform:scale(0)}40%{transform:scale(1)}}\n  @keyframes vc-fade{from{opacity:0;transform:translateY(8px)}to{opacity:1;transform:translateY(0)}}\n  .vc-note{text-align:center;font-size:11px;color:#9ca3af;padding:8px 16px 12px;background:#fff}\n</style>\n<div class=\"vc-wrap\">\n  <div class=\"vc-hdr\">\n    <div class=\"vc-avatar\">&#129302;</div>\n    <div>\n      <h3>Vizuara Teaching Assistant</h3>\n      <p>Ask me anything about this notebook</p>\n    </div>\n  </div>\n  <div class=\"vc-msgs\" id=\"vcMsgs\">\n    <div class=\"vc-msg bot\">\n      <div class=\"vc-bbl\">&#128075; Hi! I've read through this entire notebook. Ask me about any concept, code block, or exercise &mdash; I'm here to help you learn!</div>\n    </div>\n  </div>\n  <div class=\"vc-chips\" id=\"vcChips\">\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Explain the main concept</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Help with the TODO exercise</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Summarize what I learned</span>\n  </div>\n  <div class=\"vc-input\">\n    <input type=\"text\" id=\"vcIn\" placeholder=\"Ask about concepts, code, exercises...\" />\n    <button id=\"vcSend\" onclick=\"vcSendMsg()\">&#10148;</button>\n  </div>\n  <div class=\"vc-note\">AI-generated &middot; Verify important information &middot; <a href=\"#\" onclick=\"vcClear();return false\" style=\"color:#667eea\">Clear chat</a></div>\n</div>\n<script>\n(function(){\n  var msgs=document.getElementById('vcMsgs'),inp=document.getElementById('vcIn'),\n      btn=document.getElementById('vcSend'),chips=document.getElementById('vcChips');\n\n  function esc(s){var d=document.createElement('div');d.textContent=s;return d.innerHTML}\n\n  function md(t){\n    return t\n      .replace(/```(\\w*)\\n([\\s\\S]*?)```/g,function(_,l,c){return '<pre><code>'+esc(c)+'</code></pre>'})\n      .replace(/`([^`]+)`/g,'<code>$1</code>')\n      .replace(/\\*\\*([^*]+)\\*\\*/g,'<strong>$1</strong>')\n      .replace(/\\*([^*]+)\\*/g,'<em>$1</em>')\n      .replace(/^#### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^## (.+)$/gm,'<h3>$1</h3>')\n      .replace(/^\\d+\\. (.+)$/gm,'<li>$1</li>')\n      .replace(/^- (.+)$/gm,'<li>$1</li>')\n      .replace(/\\n\\n/g,'<br><br>')\n      .replace(/\\n/g,'<br>');\n  }\n\n  function addMsg(text,isUser){\n    var m=document.createElement('div');m.className='vc-msg '+(isUser?'user':'bot');\n    var b=document.createElement('div');b.className='vc-bbl';\n    b.innerHTML=isUser?esc(text):md(text);\n    m.appendChild(b);msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function showTyping(){\n    var m=document.createElement('div');m.className='vc-msg bot';m.id='vcTyping';\n    m.innerHTML='<div class=\"vc-bbl\"><div class=\"vc-typing\"><span></span><span></span><span></span></div></div>';\n    msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function hideTyping(){var e=document.getElementById('vcTyping');if(e)e.remove()}\n\n  window.vcSendMsg=function(){\n    var q=inp.value.trim();if(!q)return;\n    inp.value='';chips.style.display='none';\n    addMsg(q,true);showTyping();btn.disabled=true;\n    google.colab.kernel.invokeFunction('notebook_chat',[q],{})\n      .then(function(r){\n        hideTyping();\n        var a=r.data['application/json'];\n        addMsg(typeof a==='string'?a:JSON.stringify(a),false);\n      })\n      .catch(function(){\n        hideTyping();\n        addMsg('Sorry, I encountered an error. Please check your internet connection and try again.',false);\n      })\n      .finally(function(){btn.disabled=false;inp.focus()});\n  };\n\n  window.vcAsk=function(q){inp.value=q;vcSendMsg()};\n  window.vcClear=function(){\n    msgs.innerHTML='<div class=\"vc-msg bot\"><div class=\"vc-bbl\">&#128075; Chat cleared. Ask me anything!</div></div>';\n    chips.style.display='flex';\n  };\n\n  inp.addEventListener('keypress',function(e){if(e.key==='Enter')vcSendMsg()});\n  inp.focus();\n})();\n</script>'''))"
   ],
   "id": "vizuara_chatbot"
  }
 ]
}