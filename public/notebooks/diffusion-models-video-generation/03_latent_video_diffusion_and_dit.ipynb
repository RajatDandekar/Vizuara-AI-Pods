{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Latent Video Diffusion & DiT ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1rhbO-3WtvU0YjQYQrCebpE_IATSZyRCX\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Latent Video Diffusion and the Diffusion Transformer (DiT)\n",
    "\n",
    "*Part 3 of the Vizuara series on Diffusion Models for Video Generation*\n",
    "*Estimated time: 50 minutes*\n",
    "\n",
    "In Notebooks 1 and 2, we built video diffusion models that operate directly on pixel space. This works for our 32√ó32 Moving MNIST examples, but real videos are 256√ó256 or higher ‚Äî making pixel-space diffusion prohibitively expensive.\n",
    "\n",
    "In this notebook, we tackle the two biggest ideas in modern video generation:\n",
    "1. **Latent Video Diffusion** ‚Äî compress the video into a tiny latent space first, then run diffusion there\n",
    "2. **Diffusion Transformers (DiT)** ‚Äî replace the U-Net entirely with a Transformer over spacetime patches\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Build a simple video VAE (encoder + decoder)\n",
    "- Run diffusion in latent space for massive efficiency gains\n",
    "- Implement text conditioning via cross-attention\n",
    "- Build a mini Diffusion Transformer with spacetime patches\n",
    "- Generate text-conditioned videos from your DiT model"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ü§ñ AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** ‚Äî it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[üëâ Open AI Teaching Assistant](https://pods.vizuara.ai/courses/diffusion-models-video-generation/practice/3/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_why_it_matters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Let us revisit the numbers from the article.\n",
    "\n",
    "A 16-frame video at 256√ó256 resolution: $16 \\times 256 \\times 256 \\times 3 = 3{,}145{,}728$ values.\n",
    "\n",
    "After encoding with a VAE (spatial downsampling 8√ó, 4 latent channels): $16 \\times 32 \\times 32 \\times 4 = 65{,}536$ values.\n",
    "\n",
    "That is a **48√ó compression**. The diffusion model now operates on a tensor that is 48 times smaller. Training is faster, inference is faster, and memory usage drops dramatically.\n",
    "\n",
    "On top of this, the Diffusion Transformer (DiT) architecture further simplifies the design. Instead of carefully engineering spatial convolutions, temporal convolutions, and separate attention blocks in a U-Net, we simply:\n",
    "1. Cut the video into spacetime patches\n",
    "2. Flatten them into a token sequence\n",
    "3. Run a standard Transformer\n",
    "\n",
    "This is the architecture behind Sora, and it scales predictably with model size ‚Äî a property inherited from the language modeling world."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_intuition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### The VAE Analogy\n",
    "\n",
    "Think of the VAE as a skilled summarizer. Imagine you need to email a friend about a movie you watched. You would not send them the raw video file (too large). Instead, you would write a concise summary ‚Äî capturing the key plot points, visual style, and emotional beats in a much smaller representation. Your friend (the decoder) can then reconstruct a mental image of the movie from your summary.\n",
    "\n",
    "The video VAE does the same thing with learned representations. The encoder compresses each frame from 256√ó256√ó3 to 32√ó32√ó4 ‚Äî keeping the essential visual information while discarding redundancy.\n",
    "\n",
    "### The DiT Analogy\n",
    "\n",
    "Think of the shift from U-Net to Transformer like the shift from specialized to general-purpose computing. A U-Net is like a custom-built circuit ‚Äî highly optimized for the task but rigid in structure. A Transformer is like a general-purpose processor ‚Äî you can make it bigger, train it on more data, and it just keeps getting better. This is why language models (which are Transformers) scale so well, and it is why the video generation field is moving in the same direction.\n",
    "\n",
    "### ü§î Think About This\n",
    "\n",
    "Before we proceed:\n",
    "1. What information might the VAE lose during compression? How would this affect video quality?\n",
    "2. Why do spacetime patches mix spatial and temporal information, while factorized attention keeps them separate?\n",
    "3. If you had unlimited compute, would you still use a VAE, or run diffusion directly in pixel space?\n",
    "\n",
    "*Take 2 minutes. Then scroll down.*"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_math"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Video VAE\n",
    "\n",
    "The encoder maps a video $\\mathbf{v}$ to a latent representation $\\mathbf{z}$:\n",
    "\n",
    "$$\\mathbf{z} = \\mathcal{E}(\\mathbf{v}), \\quad \\mathbf{v} \\in \\mathbb{R}^{T \\times H \\times W \\times 3}, \\quad \\mathbf{z} \\in \\mathbb{R}^{T \\times h \\times w \\times c}$$\n",
    "\n",
    "where $h = H / f_s$, $w = W / f_s$, and $f_s$ is the spatial downsampling factor (typically 8).\n",
    "\n",
    "Computationally: take each frame, run it through a series of strided convolutions that progressively halve the spatial resolution (3 times for factor 8: 256‚Üí128‚Üí64‚Üí32), while expanding the channel dimension.\n",
    "\n",
    "The decoder inverts this:\n",
    "\n",
    "$$\\hat{\\mathbf{v}} = \\mathcal{D}(\\mathbf{z})$$\n",
    "\n",
    "The VAE is trained with a reconstruction loss plus a KL divergence regularizer:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{VAE}} = \\|\\mathbf{v} - \\hat{\\mathbf{v}}\\|^2 + \\beta \\cdot D_{\\text{KL}}(q(\\mathbf{z}|\\mathbf{v}) \\| p(\\mathbf{z}))$$\n",
    "\n",
    "The first term ensures the reconstruction is accurate. The second term keeps the latent space well-organized (close to a standard Gaussian), which is crucial because the diffusion model will sample from this space.\n",
    "\n",
    "Let us plug in numbers. If $\\|\\mathbf{v} - \\hat{\\mathbf{v}}\\|^2 = 0.015$ (good reconstruction) and $D_{\\text{KL}} = 3.2$ with $\\beta = 0.001$:\n",
    "\n",
    "$\\mathcal{L}_{\\text{VAE}} = 0.015 + 0.001 \\times 3.2 = 0.015 + 0.0032 = 0.0182$\n",
    "\n",
    "The reconstruction loss dominates ‚Äî we want high-quality decoding above all.\n",
    "\n",
    "### Spacetime Patches for DiT\n",
    "\n",
    "Given a video $\\mathbf{v} \\in \\mathbb{R}^{T \\times H \\times W \\times C}$, we divide it into non-overlapping 3D patches of size $t_p \\times h_p \\times w_p$:\n",
    "\n",
    "$$\\text{Number of tokens} = \\frac{T}{t_p} \\times \\frac{H}{h_p} \\times \\frac{W}{w_p}$$\n",
    "\n",
    "Each patch is flattened and linearly projected to the Transformer's hidden dimension $d$:\n",
    "\n",
    "$$z_i = \\text{Linear}(\\text{flatten}(p_i)) \\in \\mathbb{R}^d$$\n",
    "\n",
    "For a 16-frame video at 32√ó32 latent resolution with patches of size $2 \\times 4 \\times 4$:\n",
    "\n",
    "$\\frac{16}{2} \\times \\frac{32}{4} \\times \\frac{32}{4} = 8 \\times 8 \\times 8 = 512 \\text{ tokens}$\n",
    "\n",
    "Each token encodes a small 3D cube of spacetime ‚Äî containing information about both spatial content and temporal evolution within that cube.\n",
    "\n",
    "### Cross-Attention for Text Conditioning\n",
    "\n",
    "To condition on text, we add cross-attention layers where queries come from the video tokens and keys/values come from text embeddings:\n",
    "\n",
    "$$\\text{CrossAttn}(Q_{\\text{video}}, K_{\\text{text}}, V_{\\text{text}}) = \\text{softmax}\\left(\\frac{Q_{\\text{video}} K_{\\text{text}}^T}{\\sqrt{d_k}}\\right) \\cdot V_{\\text{text}}$$\n",
    "\n",
    "Computationally: each video patch looks at the full text description and extracts the information relevant to that spacetime location. A patch showing a dog's legs would attend strongly to the word \"running\", while a patch showing the sky would attend to \"sunny day\"."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Building Encoder\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_building_encoder.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_building_encoder"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It ‚Äî Component by Component"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Encoder Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_encoder_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_encoder_code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Video VAE ‚Äî Encoder\n",
    "\n",
    "We will build a simple video VAE that compresses spatial dimensions. For our 32√ó32 Moving MNIST, we will downsample by 4√ó to get 8√ó8 latents."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes video frames into a latent representation.\n",
    "    Processes each frame independently with 2D convolutions.\n",
    "\n",
    "    Input: (B, C_in, T, H, W)\n",
    "    Output: mean and log_var, each (B, C_latent, T, H/f, W/f)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, latent_channels=4, base_dim=32):\n",
    "        super().__init__()\n",
    "        # Downsample 4x: 32 -> 16 -> 8\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, base_dim, 3, stride=2, padding=1),\n",
    "            nn.GroupNorm(4, base_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(base_dim, base_dim * 2, 3, stride=2, padding=1),\n",
    "            nn.GroupNorm(4, base_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(base_dim * 2, base_dim * 2, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        # Output mean and log_var\n",
    "        self.to_mean = nn.Conv2d(base_dim * 2, latent_channels, 1)\n",
    "        self.to_logvar = nn.Conv2d(base_dim * 2, latent_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T, H, W)\n",
    "        B, C, T, H, W = x.shape\n",
    "        # Process each frame independently\n",
    "        x_frames = x.permute(0, 2, 1, 3, 4).reshape(B * T, C, H, W)\n",
    "        h = self.encoder(x_frames)\n",
    "        mean = self.to_mean(h)\n",
    "        logvar = self.to_logvar(h)\n",
    "        # Reshape back: (B*T, C_lat, H/f, W/f) -> (B, C_lat, T, H/f, W/f)\n",
    "        _, C_lat, Hf, Wf = mean.shape\n",
    "        mean = mean.reshape(B, T, C_lat, Hf, Wf).permute(0, 2, 1, 3, 4)\n",
    "        logvar = logvar.reshape(B, T, C_lat, Hf, Wf).permute(0, 2, 1, 3, 4)\n",
    "        return mean, logvar\n",
    "\n",
    "    def sample(self, mean, logvar):\n",
    "        \"\"\"Reparameterization trick.\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + std * eps"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the encoder\n",
    "encoder = VideoEncoder(in_channels=1, latent_channels=4).to(device)\n",
    "test_video = torch.randn(2, 1, 8, 32, 32, device=device)\n",
    "mean, logvar = encoder(test_video)\n",
    "z = encoder.sample(mean, logvar)\n",
    "\n",
    "print(f\"Input video:  {test_video.shape}  (B, C, T, H, W)\")\n",
    "print(f\"Latent mean:  {mean.shape}  (B, C_lat, T, H/4, W/4)\")\n",
    "print(f\"Latent sample: {z.shape}\")\n",
    "compression = test_video.numel() / z.numel()\n",
    "print(f\"Compression ratio: {compression:.1f}x\")\n",
    "print(\"‚úÖ Encoder works!\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Decoder\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_decoder.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_decoder"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Video VAE ‚Äî Decoder"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decodes latent representation back to video frames.\n",
    "\n",
    "    Input: (B, C_latent, T, H/f, W/f)\n",
    "    Output: (B, C_out, T, H, W)\n",
    "    \"\"\"\n",
    "    def __init__(self, out_channels=1, latent_channels=4, base_dim=32):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(latent_channels, base_dim * 2, 3, padding=1),\n",
    "            nn.GroupNorm(4, base_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(base_dim * 2, base_dim, 2, stride=2),\n",
    "            nn.GroupNorm(4, base_dim),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(base_dim, base_dim, 2, stride=2),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(base_dim, out_channels, 1),\n",
    "            nn.Sigmoid()  # Output in [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # z: (B, C_lat, T, H/f, W/f)\n",
    "        B, C_lat, T, Hf, Wf = z.shape\n",
    "        z_frames = z.permute(0, 2, 1, 3, 4).reshape(B * T, C_lat, Hf, Wf)\n",
    "        decoded = self.decoder(z_frames)\n",
    "        _, C_out, H, W = decoded.shape\n",
    "        return decoded.reshape(B, T, C_out, H, W).permute(0, 2, 1, 3, 4)"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test encoder ‚Üí decoder roundtrip\n",
    "decoder = VideoDecoder(out_channels=1, latent_channels=4).to(device)\n",
    "reconstructed = decoder(z)\n",
    "\n",
    "print(f\"Original video:      {test_video.shape}\")\n",
    "print(f\"Latent:              {z.shape}\")\n",
    "print(f\"Reconstructed video: {reconstructed.shape}\")\n",
    "print(\"‚úÖ Full VAE roundtrip works!\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Vae Pipeline Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_vae_pipeline_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_vae_pipeline_viz"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize the compression pipeline\n",
    "fig, axes = plt.subplots(3, 8, figsize=(14, 5))\n",
    "\n",
    "# Create a sample video\n",
    "from collections import OrderedDict\n",
    "sample_video = torch.randn(1, 1, 8, 32, 32, device=device).clamp(0, 1)\n",
    "\n",
    "# Original\n",
    "for col in range(8):\n",
    "    axes[0, col].imshow(sample_video[0, 0, col].cpu().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "    axes[0, col].axis('off')\n",
    "    if col == 0:\n",
    "        axes[0, col].set_ylabel('Original\\n32√ó32', fontsize=10, rotation=0, labelpad=50)\n",
    "\n",
    "# Latent (show first channel)\n",
    "mean_vis, logvar_vis = encoder(sample_video)\n",
    "z_vis = encoder.sample(mean_vis, logvar_vis)\n",
    "for col in range(8):\n",
    "    axes[1, col].imshow(z_vis[0, 0, col].cpu().detach().numpy(), cmap='viridis')\n",
    "    axes[1, col].axis('off')\n",
    "    if col == 0:\n",
    "        axes[1, col].set_ylabel('Latent\\n8√ó8', fontsize=10, rotation=0, labelpad=50)\n",
    "\n",
    "# Reconstructed\n",
    "recon_vis = decoder(z_vis)\n",
    "for col in range(8):\n",
    "    axes[2, col].imshow(recon_vis[0, 0, col].cpu().detach().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "    axes[2, col].axis('off')\n",
    "    if col == 0:\n",
    "        axes[2, col].set_ylabel('Decoded\\n32√ó32', fontsize=10, rotation=0, labelpad=50)\n",
    "\n",
    "fig.suptitle('Video VAE Pipeline: Encode ‚Üí Latent ‚Üí Decode', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"üí° Latent space is {z_vis.shape[-2]}√ó{z_vis.shape[-1]} ‚Äî 4x smaller in each spatial dimension\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Train Vae\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_train_vae.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_train_vae"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training the VAE"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_moving_mnist_video(num_frames=16, size=32, digit_size=12):\n",
    "    \"\"\"Create a single Moving MNIST video.\"\"\"\n",
    "    video = np.zeros((num_frames, size, size), dtype=np.float32)\n",
    "    x = np.random.randint(0, size - digit_size)\n",
    "    y = np.random.randint(0, size - digit_size)\n",
    "    vx = np.random.choice([-2, -1, 1, 2])\n",
    "    vy = np.random.choice([-2, -1, 1, 2])\n",
    "\n",
    "    digit = np.zeros((digit_size, digit_size), dtype=np.float32)\n",
    "    center = digit_size // 2\n",
    "    for i in range(digit_size):\n",
    "        for j in range(digit_size):\n",
    "            if ((i - center)**2 + (j - center)**2) ** 0.5 < center:\n",
    "                digit[i, j] = 1.0\n",
    "\n",
    "    for t in range(num_frames):\n",
    "        video[t, y:y+digit_size, x:x+digit_size] = digit\n",
    "        x += vx; y += vy\n",
    "        if x <= 0 or x >= size - digit_size: vx = -vx; x = max(0, min(size - digit_size, x))\n",
    "        if y <= 0 or y >= size - digit_size: vy = -vy; y = max(0, min(size - digit_size, y))\n",
    "    return video\n",
    "\n",
    "def create_dataset(num_videos=512, num_frames=8, size=32):\n",
    "    videos = np.stack([create_moving_mnist_video(num_frames, size) for _ in range(num_videos)])\n",
    "    return torch.tensor(videos).unsqueeze(1)\n",
    "\n",
    "dataset = create_dataset(512, 8, 32)\n",
    "print(f\"Dataset: {dataset.shape}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the VAE\n",
    "encoder = VideoEncoder(in_channels=1, latent_channels=4).to(device)\n",
    "decoder = VideoDecoder(out_channels=1, latent_channels=4).to(device)\n",
    "\n",
    "vae_params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "vae_optimizer = torch.optim.Adam(vae_params, lr=3e-4)\n",
    "beta_kl = 0.0001  # Small KL weight ‚Äî we prioritize reconstruction\n",
    "\n",
    "vae_losses = []\n",
    "print(\"Training Video VAE...\")\n",
    "\n",
    "for epoch in range(40):\n",
    "    epoch_loss = 0\n",
    "    perm = torch.randperm(len(dataset))\n",
    "\n",
    "    for i in range(0, len(dataset) - 16 + 1, 16):\n",
    "        batch = dataset[perm[i:i+16]].to(device)\n",
    "\n",
    "        mean, logvar = encoder(batch)\n",
    "        z = encoder.sample(mean, logvar)\n",
    "        recon = decoder(z)\n",
    "\n",
    "        # Reconstruction loss\n",
    "        recon_loss = F.mse_loss(recon, batch)\n",
    "\n",
    "        # KL divergence\n",
    "        kl_loss = -0.5 * torch.mean(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "\n",
    "        loss = recon_loss + beta_kl * kl_loss\n",
    "\n",
    "        vae_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        vae_optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / (len(dataset) // 16)\n",
    "    vae_losses.append(avg_loss)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"  Epoch {epoch+1}/40 ‚Äî Loss: {avg_loss:.4f} \"\n",
    "              f\"(Recon: {recon_loss.item():.4f}, KL: {kl_loss.item():.2f})\")\n",
    "\n",
    "print(\"VAE training complete!\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä VAE reconstruction quality\n",
    "fig, axes = plt.subplots(2, 8, figsize=(14, 3.5))\n",
    "\n",
    "sample = dataset[:1].to(device)\n",
    "with torch.no_grad():\n",
    "    mean, logvar = encoder(sample)\n",
    "    z = encoder.sample(mean, logvar)\n",
    "    recon = decoder(z)\n",
    "\n",
    "for col in range(8):\n",
    "    axes[0, col].imshow(sample[0, 0, col].cpu().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "    axes[0, col].axis('off')\n",
    "    axes[1, col].imshow(recon[0, 0, col].cpu().detach().numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "    axes[1, col].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Original', fontsize=10, rotation=0, labelpad=45)\n",
    "axes[1, 0].set_ylabel('Reconstructed', fontsize=10, rotation=0, labelpad=45)\n",
    "fig.suptitle('VAE Reconstruction Quality', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Latent Diffusion Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_latent_diffusion_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_latent_diffusion_intro"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Latent Video Diffusion\n",
    "\n",
    "Now that we have a working VAE, let us run diffusion in the latent space. The idea is simple:\n",
    "1. Encode videos to latents using the frozen VAE encoder\n",
    "2. Train a diffusion model to denoise in latent space\n",
    "3. At inference: sample noise ‚Üí denoise in latent space ‚Üí decode to pixels\n",
    "\n",
    "### 5.1 Pre-encode the Dataset"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the VAE and encode all training videos to latent space\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    latent_dataset = []\n",
    "    for i in range(0, len(dataset), 32):\n",
    "        batch = dataset[i:i+32].to(device)\n",
    "        mean, logvar = encoder(batch)\n",
    "        z = encoder.sample(mean, logvar)\n",
    "        latent_dataset.append(z.cpu())\n",
    "\n",
    "latent_dataset = torch.cat(latent_dataset, dim=0)\n",
    "print(f\"Pixel dataset:  {dataset.shape}    ({dataset.numel():,} values)\")\n",
    "print(f\"Latent dataset: {latent_dataset.shape}  ({latent_dataset.numel():,} values)\")\n",
    "print(f\"Compression: {dataset.numel() / latent_dataset.numel():.1f}x\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Latent Unet\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_latent_unet.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_latent_unet"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Simple Latent Diffusion Model\n",
    "\n",
    "For the latent diffusion model, we will use a small 2D U-Net that processes each frame of the latent independently, plus temporal mixing layers."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDiffusionUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A U-Net for denoising in latent space.\n",
    "    Input: (B, C_lat, T, H_lat, W_lat) ‚Äî noisy latent video\n",
    "    Output: (B, C_lat, T, H_lat, W_lat) ‚Äî predicted noise\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_channels=4, base_dim=64):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(base_dim, base_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(base_dim * 4, base_dim)\n",
    "        )\n",
    "        self.time_dim = base_dim\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(latent_channels, base_dim, 3, padding=1),\n",
    "            nn.GroupNorm(4, base_dim), nn.GELU())\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(base_dim, base_dim * 2, 3, stride=2, padding=1),\n",
    "            nn.GroupNorm(4, base_dim * 2), nn.GELU())\n",
    "\n",
    "        # Temporal mixing (1D conv across frames)\n",
    "        self.temporal1 = nn.Conv1d(base_dim, base_dim, 3, padding=1)\n",
    "        self.temporal2 = nn.Conv1d(base_dim * 2, base_dim * 2, 3, padding=1)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_dim * 4, base_dim, 2, stride=2),\n",
    "            nn.GroupNorm(4, base_dim), nn.GELU())\n",
    "        self.out = nn.Conv2d(base_dim * 2, latent_channels, 1)\n",
    "\n",
    "        self.time_proj1 = nn.Linear(base_dim, base_dim)\n",
    "        self.time_proj2 = nn.Linear(base_dim, base_dim * 2)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        B, C, T, H, W = x.shape\n",
    "\n",
    "        # Time embedding\n",
    "        half = self.time_dim // 2\n",
    "        freqs = torch.exp(-math.log(10000) * torch.arange(half, device=t.device) / half)\n",
    "        t_emb = torch.cat([\n",
    "            (t[:, None].float() * freqs[None]).sin(),\n",
    "            (t[:, None].float() * freqs[None]).cos()\n",
    "        ], dim=-1)\n",
    "        t_emb = self.time_mlp(t_emb)\n",
    "\n",
    "        # Process frames with 2D convs\n",
    "        x_frames = x.permute(0, 2, 1, 3, 4).reshape(B * T, C, H, W)\n",
    "\n",
    "        h1 = self.enc1(x_frames)  # (B*T, base, H, W)\n",
    "        # Add time embedding\n",
    "        te1 = self.time_proj1(t_emb).repeat_interleave(T, 0)[:, :, None, None]\n",
    "        h1 = h1 + te1\n",
    "\n",
    "        # Temporal mixing on h1\n",
    "        _, C1, H1, W1 = h1.shape\n",
    "        h1_t = h1.reshape(B, T, C1, H1, W1).permute(0, 3, 4, 2, 1)  # (B, H, W, C, T)\n",
    "        h1_t = h1_t.reshape(B * H1 * W1, C1, T)\n",
    "        h1_t = h1_t + self.temporal1(h1_t)\n",
    "        h1_t = h1_t.reshape(B, H1, W1, C1, T).permute(0, 4, 3, 1, 2)  # (B, T, C, H, W)\n",
    "        h1 = h1_t.reshape(B * T, C1, H1, W1)\n",
    "\n",
    "        h2 = self.enc2(h1)  # (B*T, base*2, H/2, W/2)\n",
    "        te2 = self.time_proj2(t_emb).repeat_interleave(T, 0)[:, :, None, None]\n",
    "        h2 = h2 + te2\n",
    "\n",
    "        # Temporal mixing on h2\n",
    "        _, C2, H2, W2 = h2.shape\n",
    "        h2_t = h2.reshape(B, T, C2, H2, W2).permute(0, 3, 4, 2, 1).reshape(B * H2 * W2, C2, T)\n",
    "        h2_t = h2_t + self.temporal2(h2_t)\n",
    "        h2_t = h2_t.reshape(B, H2, W2, C2, T).permute(0, 4, 3, 1, 2)\n",
    "        h2 = h2_t.reshape(B * T, C2, H2, W2)\n",
    "\n",
    "        # Decode with skip\n",
    "        dec = self.dec2(torch.cat([h2, h2], dim=1))\n",
    "        out = self.out(torch.cat([dec, h1], dim=1))\n",
    "\n",
    "        return out.reshape(B, T, -1, H, W).permute(0, 2, 1, 3, 4)"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "latent_model = LatentDiffusionUNet(latent_channels=4, base_dim=64).to(device)\n",
    "test_z = torch.randn(2, 4, 8, 8, 8, device=device)\n",
    "test_t = torch.randint(0, 500, (2,), device=device)\n",
    "test_out = latent_model(test_z, test_t)\n",
    "\n",
    "print(f\"Latent input:  {test_z.shape}\")\n",
    "print(f\"Noise output:  {test_out.shape}\")\n",
    "params = sum(p.numel() for p in latent_model.parameters())\n",
    "print(f\"Parameters: {params:,}\")\n",
    "print(\"‚úÖ Latent diffusion model works!\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Train Latent\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_train_latent.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_11_train_latent"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Train Latent Diffusion"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffusion schedule\n",
    "num_timesteps = 500\n",
    "betas = torch.linspace(0.0001, 0.02, num_timesteps, device=device)\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
    "\n",
    "latent_model = LatentDiffusionUNet(latent_channels=4, base_dim=64).to(device)\n",
    "optimizer = torch.optim.Adam(latent_model.parameters(), lr=2e-4)\n",
    "\n",
    "latent_losses = []\n",
    "print(\"Training Latent Diffusion Model...\")\n",
    "\n",
    "for epoch in range(30):\n",
    "    epoch_loss = 0\n",
    "    perm = torch.randperm(len(latent_dataset))\n",
    "\n",
    "    for i in range(0, len(latent_dataset) - 16 + 1, 16):\n",
    "        batch = latent_dataset[perm[i:i+16]].to(device)\n",
    "        B = batch.shape[0]\n",
    "\n",
    "        t = torch.randint(0, num_timesteps, (B,), device=device)\n",
    "        noise = torch.randn_like(batch)\n",
    "\n",
    "        # Forward diffusion in latent space\n",
    "        sqrt_ac = sqrt_alphas_cumprod[t][:, None, None, None, None]\n",
    "        sqrt_omac = sqrt_one_minus_alphas_cumprod[t][:, None, None, None, None]\n",
    "        noisy_latent = sqrt_ac * batch + sqrt_omac * noise\n",
    "\n",
    "        # Predict noise\n",
    "        pred_noise = latent_model(noisy_latent, t)\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg = epoch_loss / (len(latent_dataset) // 16)\n",
    "    latent_losses.append(avg)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"  Epoch {epoch+1}/30 ‚Äî Loss: {avg:.4f}\")\n",
    "\n",
    "print(\"Latent diffusion training complete!\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Training curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(latent_losses, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (in latent space)')\n",
    "plt.title('Latent Video Diffusion ‚Äî Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Latent Sampling\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_latent_sampling.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_12_latent_sampling"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Sampling from Latent Diffusion"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_latent_diffusion(model, decoder, num_samples=8, num_frames=8,\n",
    "                             latent_h=8, latent_w=8, latent_c=4):\n",
    "    \"\"\"Sample videos via latent diffusion + VAE decoding.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Start from noise in latent space (much smaller!)\n",
    "    z = torch.randn(num_samples, latent_c, num_frames, latent_h, latent_w, device=device)\n",
    "\n",
    "    for t_idx in reversed(range(num_timesteps)):\n",
    "        t = torch.full((num_samples,), t_idx, device=device, dtype=torch.long)\n",
    "        pred_noise = model(z, t)\n",
    "\n",
    "        beta_t = betas[t_idx]\n",
    "        alpha_t = alphas[t_idx]\n",
    "        alpha_cumprod_t = alphas_cumprod[t_idx]\n",
    "\n",
    "        coeff1 = 1.0 / torch.sqrt(alpha_t)\n",
    "        coeff2 = beta_t / torch.sqrt(1.0 - alpha_cumprod_t)\n",
    "        mean = coeff1 * (z - coeff2 * pred_noise)\n",
    "\n",
    "        if t_idx > 0:\n",
    "            z = mean + torch.sqrt(beta_t) * torch.randn_like(z)\n",
    "        else:\n",
    "            z = mean\n",
    "\n",
    "    # Decode latents to pixels\n",
    "    videos = decoder(z)\n",
    "    model.train()\n",
    "    return videos.clamp(0, 1)\n",
    "\n",
    "print(\"Generating videos via latent diffusion...\")\n",
    "generated_latent = sample_latent_diffusion(latent_model, decoder)\n",
    "print(f\"Generated: {generated_latent.shape}\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Display latent diffusion results\n",
    "fig, axes = plt.subplots(4, 8, figsize=(14, 7))\n",
    "for row in range(4):\n",
    "    for col in range(8):\n",
    "        axes[row, col].imshow(generated_latent[row, 0, col].cpu().numpy(),\n",
    "                             cmap='gray', vmin=0, vmax=1)\n",
    "        axes[row, col].axis('off')\n",
    "        if row == 0:\n",
    "            axes[row, col].set_title(f'f={col}', fontsize=9)\n",
    "    axes[row, 0].set_ylabel(f'Video {row+1}', fontsize=9, rotation=0, labelpad=40)\n",
    "\n",
    "fig.suptitle('Generated Videos ‚Äî Latent Video Diffusion\\n'\n",
    "             'Diffusion runs in 8√ó8 latent space, decoded to 32√ó32 pixels', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Dit Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_dit_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_13_dit_intro"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Diffusion Transformer (DiT)\n",
    "\n",
    "Now let us build the other major innovation: replacing the U-Net with a Transformer. This is the architecture behind Sora.\n",
    "\n",
    "### 6.1 Spacetime Patchification"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacetimePatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert a video into a sequence of spacetime patch tokens.\n",
    "\n",
    "    Input: (B, C, T, H, W)\n",
    "    Output: (B, num_patches, embed_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=4, embed_dim=128,\n",
    "                 patch_t=2, patch_h=4, patch_w=4):\n",
    "        super().__init__()\n",
    "        self.patch_t = patch_t\n",
    "        self.patch_h = patch_h\n",
    "        self.patch_w = patch_w\n",
    "        patch_dim = in_channels * patch_t * patch_h * patch_w\n",
    "        self.proj = nn.Linear(patch_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, T, H, W = x.shape\n",
    "        pt, ph, pw = self.patch_t, self.patch_h, self.patch_w\n",
    "\n",
    "        # Reshape into patches\n",
    "        nt, nh, nw = T // pt, H // ph, W // pw\n",
    "        x = x.reshape(B, C, nt, pt, nh, ph, nw, pw)\n",
    "        x = x.permute(0, 2, 4, 6, 1, 3, 5, 7)  # (B, nt, nh, nw, C, pt, ph, pw)\n",
    "        x = x.reshape(B, nt * nh * nw, C * pt * ph * pw)  # (B, num_patches, patch_dim)\n",
    "\n",
    "        return self.proj(x), (nt, nh, nw)"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize patchification\n",
    "patcher = SpacetimePatchEmbed(in_channels=4, embed_dim=128,\n",
    "                               patch_t=2, patch_h=4, patch_w=4).to(device)\n",
    "test_latent = torch.randn(1, 4, 8, 8, 8, device=device)\n",
    "tokens, (nt, nh, nw) = patcher(test_latent)\n",
    "\n",
    "print(f\"Input latent: {test_latent.shape}  (B, C, T, H, W)\")\n",
    "print(f\"Patches: {nt}t √ó {nh}h √ó {nw}w = {nt*nh*nw} tokens\")\n",
    "print(f\"Token sequence: {tokens.shape}  (B, num_tokens, embed_dim)\")\n",
    "print(f\"\\nüí° Each token encodes a {patcher.patch_t}√ó{patcher.patch_h}√ó{patcher.patch_w} \"\n",
    "      f\"cube of spacetime!\")"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo Unpatchify\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_todo_unpatchify.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_14_todo_unpatchify"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß TODO: Implement Spacetime Unpatchification\n",
    "\n",
    "Patchification converts a video into tokens. After the Transformer processes these tokens, we need to convert them **back** into a video tensor. This is the inverse operation ‚Äî unpatchification."
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpatchify(tokens, num_channels, nt, nh, nw, patch_t, patch_h, patch_w):\n",
    "    \"\"\"\n",
    "    Convert a sequence of patch tokens back into a video tensor.\n",
    "\n",
    "    Args:\n",
    "        tokens: (B, num_patches, patch_dim) ‚Äî output from Transformer head\n",
    "                where patch_dim = num_channels * patch_t * patch_h * patch_w\n",
    "        num_channels: number of channels (e.g., 4 for latent space)\n",
    "        nt, nh, nw: number of patches along time, height, width\n",
    "        patch_t, patch_h, patch_w: patch dimensions\n",
    "\n",
    "    Returns:\n",
    "        video: (B, num_channels, T, H, W)\n",
    "               where T = nt*patch_t, H = nh*patch_h, W = nw*patch_w\n",
    "    \"\"\"\n",
    "    B = tokens.shape[0]\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Reshape tokens from (B, nt*nh*nw, patch_dim)\n",
    "    #         to (B, nt, nh, nw, C, patch_t, patch_h, patch_w)\n",
    "    # Step 2: Permute to (B, C, nt, patch_t, nh, patch_h, nw, patch_w)\n",
    "    #         This interleaves the grid indices with patch indices\n",
    "    # Step 3: Reshape to (B, C, T, H, W) where T=nt*pt, H=nh*ph, W=nw*pw\n",
    "    # ==============================\n",
    "\n",
    "    video = ???  # YOUR CODE HERE\n",
    "\n",
    "    return video"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification\n",
    "torch.manual_seed(42)\n",
    "B, C, T, H, W = 2, 4, 8, 8, 8\n",
    "pt, ph, pw = 2, 4, 4\n",
    "_nt, _nh, _nw = T // pt, H // ph, W // pw\n",
    "patch_dim = C * pt * ph * pw\n",
    "test_tokens = torch.randn(B, _nt * _nh * _nw, patch_dim, device=device)\n",
    "\n",
    "try:\n",
    "    result = unpatchify(test_tokens, C, _nt, _nh, _nw, pt, ph, pw)\n",
    "    assert result.shape == (B, C, T, H, W), f\"Wrong shape: {result.shape}, expected ({B}, {C}, {T}, {H}, {W})\"\n",
    "\n",
    "    # Verify roundtrip: patchify ‚Üí unpatchify should recover original\n",
    "    orig = torch.randn(B, C, T, H, W, device=device)\n",
    "    tokens_rt, (_nt2, _nh2, _nw2) = patcher(orig)\n",
    "    # Need to invert the linear projection for a true roundtrip, so just check shape\n",
    "    print(f\"‚úÖ Unpatchify works! Output shape: {result.shape}\")\n",
    "    print(f\"   {_nt}√ó{_nh}√ó{_nw} patches ‚Üí {T}√ó{H}√ó{W} video\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Hint: reshape to (B, nt, nh, nw, C, pt, ph, pw),\")\n",
    "    print(\"then permute to (B, C, nt, pt, nh, ph, nw, pw),\")\n",
    "    print(\"then reshape to (B, C, nt*pt, nh*ph, nw*pw)\")"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Dit Block\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/15_dit_block.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_15_dit_block"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 DiT Block"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiTBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer block for the Diffusion Transformer.\n",
    "    Contains: self-attention + cross-attention (for conditioning) + feedforward.\n",
    "    Timestep information is injected via adaptive layer norm (adaLN).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=4, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        # Self-attention\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.self_attn_qkv = nn.Linear(dim, dim * 3)\n",
    "        self.self_attn_proj = nn.Linear(dim, dim)\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        # Cross-attention (for text conditioning)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.cross_q = nn.Linear(dim, dim)\n",
    "        self.cross_kv = nn.Linear(dim, dim * 2)\n",
    "        self.cross_proj = nn.Linear(dim, dim)\n",
    "\n",
    "        # Feedforward\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "        hidden = int(dim * mlp_ratio)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim, hidden), nn.GELU(), nn.Linear(hidden, dim))\n",
    "\n",
    "        # AdaLN modulation from timestep\n",
    "        self.adaLN = nn.Sequential(\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim, dim * 6)  # 6 = scale+shift for 3 norms\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t_emb, context=None):\n",
    "        \"\"\"\n",
    "        x: (B, N, D) ‚Äî patch tokens\n",
    "        t_emb: (B, D) ‚Äî timestep embedding\n",
    "        context: (B, L, D) ‚Äî text embeddings (optional)\n",
    "        \"\"\"\n",
    "        # AdaLN parameters from timestep\n",
    "        ada = self.adaLN(t_emb)  # (B, 6*D)\n",
    "        s1, b1, s2, b2, s3, b3 = ada.chunk(6, dim=-1)\n",
    "\n",
    "        # Self-attention with adaLN\n",
    "        h = self.norm1(x) * (1 + s1.unsqueeze(1)) + b1.unsqueeze(1)\n",
    "        B, N, D = h.shape\n",
    "        qkv = self.self_attn_qkv(h).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        h = (attn @ v).transpose(1, 2).reshape(B, N, D)\n",
    "        h = self.self_attn_proj(h)\n",
    "        x = x + h\n",
    "\n",
    "        # Cross-attention (if conditioning context provided)\n",
    "        if context is not None:\n",
    "            h = self.norm2(x) * (1 + s2.unsqueeze(1)) + b2.unsqueeze(1)\n",
    "            q = self.cross_q(h).reshape(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "            kv = self.cross_kv(context).reshape(B, -1, 2, self.num_heads, self.head_dim)\n",
    "            kv = kv.permute(2, 0, 3, 1, 4)\n",
    "            k_c, v_c = kv.unbind(0)\n",
    "            attn = (q @ k_c.transpose(-2, -1)) * self.scale\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            h = (attn @ v_c).transpose(1, 2).reshape(B, N, D)\n",
    "            x = x + self.cross_proj(h)\n",
    "\n",
    "        # Feedforward with adaLN\n",
    "        h = self.norm3(x) * (1 + s3.unsqueeze(1)) + b3.unsqueeze(1)\n",
    "        x = x + self.ff(h)\n",
    "\n",
    "        return x"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Full Dit\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/16_full_dit.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_16_full_dit"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Full Mini-DiT Model"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniDiT(nn.Module):\n",
    "    \"\"\"\n",
    "    A minimal Diffusion Transformer for video generation.\n",
    "\n",
    "    Architecture:\n",
    "    1. Spacetime patchify the latent video\n",
    "    2. Add positional embeddings\n",
    "    3. Pass through N DiT blocks (self-attn + cross-attn + FFN)\n",
    "    4. Unpatchify back to latent video shape\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_channels=4, embed_dim=128, num_heads=4,\n",
    "                 depth=4, patch_t=2, patch_h=4, patch_w=4,\n",
    "                 context_dim=64):\n",
    "        super().__init__()\n",
    "        self.patch_embed = SpacetimePatchEmbed(\n",
    "            latent_channels, embed_dim, patch_t, patch_h, patch_w)\n",
    "        self.patch_t = patch_t\n",
    "        self.patch_h = patch_h\n",
    "        self.patch_w = patch_w\n",
    "        self.latent_channels = latent_channels\n",
    "\n",
    "        # Positional embedding (learnable)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 512, embed_dim) * 0.02)\n",
    "\n",
    "        # Timestep embedding\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "        self.time_dim = embed_dim\n",
    "\n",
    "        # Context projection (for text conditioning)\n",
    "        self.context_proj = nn.Linear(context_dim, embed_dim)\n",
    "\n",
    "        # DiT blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            DiTBlock(embed_dim, num_heads) for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        # Output: project back to patch dimension\n",
    "        self.norm_out = nn.LayerNorm(embed_dim)\n",
    "        patch_dim = latent_channels * patch_t * patch_h * patch_w\n",
    "        self.head = nn.Linear(embed_dim, patch_dim)\n",
    "\n",
    "    def forward(self, x, t, context=None):\n",
    "        \"\"\"\n",
    "        x: (B, C, T, H, W) ‚Äî noisy latent video\n",
    "        t: (B,) ‚Äî diffusion timestep\n",
    "        context: (B, L, context_dim) ‚Äî text embeddings (optional)\n",
    "        \"\"\"\n",
    "        B, C, T, H, W = x.shape\n",
    "\n",
    "        # Patchify\n",
    "        tokens, (nt, nh, nw) = self.patch_embed(x)\n",
    "        num_tokens = tokens.shape[1]\n",
    "\n",
    "        # Add positional embedding\n",
    "        tokens = tokens + self.pos_embed[:, :num_tokens]\n",
    "\n",
    "        # Timestep embedding\n",
    "        half = self.time_dim // 2\n",
    "        freqs = torch.exp(-math.log(10000) * torch.arange(half, device=t.device) / half)\n",
    "        t_emb = torch.cat([\n",
    "            (t[:, None].float() * freqs[None]).sin(),\n",
    "            (t[:, None].float() * freqs[None]).cos()\n",
    "        ], dim=-1)\n",
    "        t_emb = self.time_embed(t_emb)\n",
    "\n",
    "        # Project context if provided\n",
    "        if context is not None:\n",
    "            context = self.context_proj(context)\n",
    "\n",
    "        # DiT blocks\n",
    "        for block in self.blocks:\n",
    "            tokens = block(tokens, t_emb, context)\n",
    "\n",
    "        # Unpatchify\n",
    "        tokens = self.head(self.norm_out(tokens))\n",
    "\n",
    "        # Reshape back to (B, C, T, H, W)\n",
    "        pt, ph, pw = self.patch_t, self.patch_h, self.patch_w\n",
    "        tokens = tokens.reshape(B, nt, nh, nw, C, pt, ph, pw)\n",
    "        tokens = tokens.permute(0, 4, 1, 5, 2, 6, 3, 7)  # (B, C, nt, pt, nh, ph, nw, pw)\n",
    "        out = tokens.reshape(B, C, T, H, W)\n",
    "\n",
    "        return out"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the DiT\n",
    "dit = MiniDiT(latent_channels=4, embed_dim=128, num_heads=4,\n",
    "              depth=4, context_dim=64).to(device)\n",
    "test_z = torch.randn(2, 4, 8, 8, 8, device=device)\n",
    "test_t = torch.randint(0, 500, (2,), device=device)\n",
    "test_ctx = torch.randn(2, 10, 64, device=device)  # 10 \"text\" tokens\n",
    "\n",
    "out_uncond = dit(test_z, test_t)\n",
    "out_cond = dit(test_z, test_t, context=test_ctx)\n",
    "\n",
    "print(f\"Input latent:     {test_z.shape}\")\n",
    "print(f\"Output (uncond):  {out_uncond.shape}\")\n",
    "print(f\"Output (cond):    {out_cond.shape}\")\n",
    "params = sum(p.numel() for p in dit.parameters())\n",
    "print(f\"DiT parameters: {params:,}\")\n",
    "print(\"‚úÖ Mini DiT works ‚Äî with and without text conditioning!\")"
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo Cross Attention\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/17_todo_cross_attention.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_17_todo_cross_attention"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üîß Your Turn ‚Äî Implement Cross-Attention\n",
    "\n",
    "### TODO: Complete the cross-attention function\n",
    "\n",
    "Cross-attention is how the video model \"looks at\" the text description. Complete the implementation below:"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_attention(query, key, value, num_heads=4):\n",
    "    \"\"\"\n",
    "    Compute multi-head cross-attention.\n",
    "\n",
    "    Args:\n",
    "        query: (B, N_q, D) ‚Äî queries from video features\n",
    "        key:   (B, N_kv, D) ‚Äî keys from text embeddings\n",
    "        value: (B, N_kv, D) ‚Äî values from text embeddings\n",
    "        num_heads: number of attention heads\n",
    "\n",
    "    Returns:\n",
    "        (B, N_q, D) ‚Äî video features enriched with text information\n",
    "    \"\"\"\n",
    "    B, N_q, D = query.shape\n",
    "    _, N_kv, _ = key.shape\n",
    "    head_dim = D // num_heads\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Reshape query to (B, num_heads, N_q, head_dim)\n",
    "    # Step 2: Reshape key to (B, num_heads, N_kv, head_dim)\n",
    "    # Step 3: Reshape value to (B, num_heads, N_kv, head_dim)\n",
    "    # Step 4: Compute attention scores: (q @ k^T) / sqrt(head_dim)\n",
    "    # Step 5: Apply softmax over the key dimension (last dim)\n",
    "    # Step 6: Compute weighted sum: attn_weights @ value\n",
    "    # Step 7: Reshape output back to (B, N_q, D)\n",
    "    # ==============================\n",
    "\n",
    "    q = ???  # YOUR CODE HERE (Step 1)\n",
    "    k = ???  # YOUR CODE HERE (Step 2)\n",
    "    v = ???  # YOUR CODE HERE (Step 3)\n",
    "\n",
    "    scores = ???  # YOUR CODE HERE (Step 4)\n",
    "    attn_weights = ???  # YOUR CODE HERE (Step 5)\n",
    "    out = ???  # YOUR CODE HERE (Step 6)\n",
    "\n",
    "    output = ???  # YOUR CODE HERE (Step 7)\n",
    "    return output"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification\n",
    "torch.manual_seed(42)\n",
    "B, N_q, N_kv, D = 2, 16, 8, 32\n",
    "test_q = torch.randn(B, N_q, D, device=device)\n",
    "test_k = torch.randn(B, N_kv, D, device=device)\n",
    "test_v = torch.randn(B, N_kv, D, device=device)\n",
    "\n",
    "try:\n",
    "    result = cross_attention(test_q, test_k, test_v, num_heads=4)\n",
    "    assert result.shape == (B, N_q, D), f\"Wrong shape: {result.shape}, expected ({B}, {N_q}, {D})\"\n",
    "\n",
    "    # Verify attention is over key dimension\n",
    "    # Each query should produce a different output even with same keys\n",
    "    test_q2 = torch.randn(B, N_q, D, device=device)\n",
    "    result2 = cross_attention(test_q2, test_k, test_v, num_heads=4)\n",
    "    assert not torch.allclose(result, result2, atol=1e-3), \"Different queries should give different outputs\"\n",
    "\n",
    "    print(f\"‚úÖ Cross-attention works! Output shape: {result.shape}\")\n",
    "    print(f\"   {N_q} video tokens attended to {N_kv} text tokens\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Hint: reshape using .reshape(B, N, num_heads, head_dim).transpose(1, 2)\")"
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Train Dit\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/18_train_dit.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_18_train_dit"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training the DiT with \"Text\" Conditioning\n",
    "\n",
    "For our Moving MNIST demo, we will create simple conditioning labels (e.g., velocity direction) as a stand-in for real text embeddings."
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labeled dataset: encode velocity direction as a simple \"text\" embedding\n",
    "def create_labeled_dataset(num_videos=512, num_frames=8, size=32, embed_dim=64):\n",
    "    \"\"\"Create Moving MNIST with direction labels as conditioning.\"\"\"\n",
    "    videos = []\n",
    "    labels = []\n",
    "\n",
    "    for _ in range(num_videos):\n",
    "        video = np.zeros((num_frames, size, size), dtype=np.float32)\n",
    "        digit_size = 12\n",
    "        x = np.random.randint(0, size - digit_size)\n",
    "        y = np.random.randint(0, size - digit_size)\n",
    "\n",
    "        # 4 directions: right, left, down, up\n",
    "        direction = np.random.randint(0, 4)\n",
    "        vx = [2, -2, 0, 0][direction]\n",
    "        vy = [0, 0, 2, -2][direction]\n",
    "\n",
    "        digit = np.zeros((digit_size, digit_size), dtype=np.float32)\n",
    "        center = digit_size // 2\n",
    "        for i in range(digit_size):\n",
    "            for j in range(digit_size):\n",
    "                if ((i - center)**2 + (j - center)**2) ** 0.5 < center:\n",
    "                    digit[i, j] = 1.0\n",
    "\n",
    "        for t in range(num_frames):\n",
    "            video[t, y:y+digit_size, x:x+digit_size] = digit\n",
    "            x += vx; y += vy\n",
    "            if x <= 0 or x >= size - digit_size: vx = -vx; x = max(0, min(size - digit_size, x))\n",
    "            if y <= 0 or y >= size - digit_size: vy = -vy; y = max(0, min(size - digit_size, y))\n",
    "\n",
    "        videos.append(video)\n",
    "\n",
    "        # Create a simple \"text embedding\" from direction\n",
    "        label = np.zeros(embed_dim, dtype=np.float32)\n",
    "        label[direction * (embed_dim // 4):(direction + 1) * (embed_dim // 4)] = 1.0\n",
    "        labels.append(label)\n",
    "\n",
    "    videos = torch.tensor(np.stack(videos)).unsqueeze(1)\n",
    "    labels = torch.tensor(np.stack(labels))\n",
    "    return videos, labels\n",
    "\n",
    "cond_videos, cond_labels = create_labeled_dataset(512, 8, 32, 64)\n",
    "print(f\"Videos: {cond_videos.shape}, Labels: {cond_labels.shape}\")\n",
    "print(\"Directions: right=0, left=1, down=2, up=3\")"
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode to latent space\n",
    "with torch.no_grad():\n",
    "    cond_latents = []\n",
    "    for i in range(0, len(cond_videos), 32):\n",
    "        batch = cond_videos[i:i+32].to(device)\n",
    "        mean, logvar = encoder(batch)\n",
    "        z = encoder.sample(mean, logvar)\n",
    "        cond_latents.append(z.cpu())\n",
    "    cond_latents = torch.cat(cond_latents)\n",
    "print(f\"Latent dataset: {cond_latents.shape}\")"
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the DiT with conditioning\n",
    "dit = MiniDiT(latent_channels=4, embed_dim=128, num_heads=4,\n",
    "              depth=4, context_dim=64).to(device)\n",
    "optimizer = torch.optim.Adam(dit.parameters(), lr=2e-4)\n",
    "\n",
    "dit_losses = []\n",
    "print(\"Training Mini DiT with direction conditioning...\")\n",
    "\n",
    "for epoch in range(30):\n",
    "    epoch_loss = 0\n",
    "    perm = torch.randperm(len(cond_latents))\n",
    "\n",
    "    for i in range(0, len(cond_latents) - 16 + 1, 16):\n",
    "        idx = perm[i:i+16]\n",
    "        batch = cond_latents[idx].to(device)\n",
    "        labels = cond_labels[idx].to(device)\n",
    "        B = batch.shape[0]\n",
    "\n",
    "        # Context: direction label reshaped as 1-token sequence\n",
    "        context = labels.unsqueeze(1)  # (B, 1, 64)\n",
    "\n",
    "        t = torch.randint(0, num_timesteps, (B,), device=device)\n",
    "        noise = torch.randn_like(batch)\n",
    "        sqrt_ac = sqrt_alphas_cumprod[t][:, None, None, None, None]\n",
    "        sqrt_omac = sqrt_one_minus_alphas_cumprod[t][:, None, None, None, None]\n",
    "        noisy = sqrt_ac * batch + sqrt_omac * noise\n",
    "\n",
    "        pred = dit(noisy, t, context)\n",
    "        loss = F.mse_loss(pred, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg = epoch_loss / (len(cond_latents) // 16)\n",
    "    dit_losses.append(avg)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"  Epoch {epoch+1}/30 ‚Äî Loss: {avg:.4f}\")\n",
    "\n",
    "print(\"DiT training complete!\")"
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Training curve comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].plot(latent_losses, 'b-', linewidth=2, label='Latent U-Net')\n",
    "axes[0].plot(dit_losses, 'r-', linewidth=2, label='DiT')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Architecture comparison\n",
    "archs = ['Pixel U-Net\\n(NB1)', 'Factorized\\nU-Net (NB2)', 'Latent\\nU-Net', 'DiT']\n",
    "desc = ['3D conv in\\npixel space', 'Spatial+Temporal\\nattn in pixels', 'U-Net in\\nlatent space', 'Transformer\\nin latent space']\n",
    "\n",
    "axes[1].barh(archs, [1, 2, 3, 4], color=['#dd8452', '#55a868', '#4c72b0', '#c44e52'])\n",
    "axes[1].set_xlabel('Approach Sophistication ‚Üí')\n",
    "axes[1].set_title('Video Diffusion Architecture Evolution')\n",
    "for i, d in enumerate(desc):\n",
    "    axes[1].text(0.5, i, d, va='center', fontsize=8, color='white', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Conditioned Sampling\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/19_conditioned_sampling.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_19_conditioned_sampling"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Conditioned Sampling from DiT"
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_dit(dit_model, decoder, direction, num_samples=4,\n",
    "               num_frames=8, latent_h=8, latent_w=8, latent_c=4):\n",
    "    \"\"\"Sample videos conditioned on a direction label.\"\"\"\n",
    "    dit_model.eval()\n",
    "\n",
    "    # Create conditioning\n",
    "    context_dim = 64\n",
    "    label = torch.zeros(num_samples, context_dim, device=device)\n",
    "    label[:, direction * (context_dim // 4):(direction + 1) * (context_dim // 4)] = 1.0\n",
    "    context = label.unsqueeze(1)  # (B, 1, 64)\n",
    "\n",
    "    z = torch.randn(num_samples, latent_c, num_frames, latent_h, latent_w, device=device)\n",
    "\n",
    "    for t_idx in reversed(range(num_timesteps)):\n",
    "        t = torch.full((num_samples,), t_idx, device=device, dtype=torch.long)\n",
    "        pred_noise = dit_model(z, t, context)\n",
    "\n",
    "        beta_t = betas[t_idx]\n",
    "        alpha_t = alphas[t_idx]\n",
    "        alpha_cumprod_t = alphas_cumprod[t_idx]\n",
    "\n",
    "        coeff1 = 1.0 / torch.sqrt(alpha_t)\n",
    "        coeff2 = beta_t / torch.sqrt(1.0 - alpha_cumprod_t)\n",
    "        mean = coeff1 * (z - coeff2 * pred_noise)\n",
    "\n",
    "        if t_idx > 0:\n",
    "            z = mean + torch.sqrt(beta_t) * torch.randn_like(z)\n",
    "        else:\n",
    "            z = mean\n",
    "\n",
    "    videos = decoder(z)\n",
    "    dit_model.train()\n",
    "    return videos.clamp(0, 1)\n",
    "\n",
    "# Generate for each direction\n",
    "direction_names = ['Right ‚Üí', 'Left ‚Üê', 'Down ‚Üì', 'Up ‚Üë']\n",
    "print(\"Generating conditioned videos for each direction...\")\n",
    "all_gen = []\n",
    "for d in range(4):\n",
    "    vids = sample_dit(dit, decoder, direction=d, num_samples=2)\n",
    "    all_gen.append(vids)\n",
    "    print(f\"  Generated {direction_names[d]}\")"
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Final Output\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/20_final_output.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_20_final_output"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üéØ Final Output ‚Äî Text-Conditioned Video Generation"
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display conditioned generation results\n",
    "fig, axes = plt.subplots(8, 8, figsize=(14, 14))\n",
    "\n",
    "for d in range(4):\n",
    "    for sample in range(2):\n",
    "        row = d * 2 + sample\n",
    "        for col in range(8):\n",
    "            axes[row, col].imshow(\n",
    "                all_gen[d][sample, 0, col].cpu().numpy(),\n",
    "                cmap='gray', vmin=0, vmax=1)\n",
    "            axes[row, col].axis('off')\n",
    "            if row == 0:\n",
    "                axes[row, col].set_title(f'Frame {col}', fontsize=9)\n",
    "        axes[row, 0].set_ylabel(f'{direction_names[d]}',\n",
    "                               fontsize=10, rotation=0, labelpad=50,\n",
    "                               color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][d])\n",
    "\n",
    "fig.suptitle('üéØ Conditioned Video Generation with Mini DiT\\n'\n",
    "             'Each pair of rows shows a different motion direction', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéâ Congratulations! You've built:\")\n",
    "print(\"  1. A Video VAE (48x compression)\")\n",
    "print(\"  2. Latent Video Diffusion (efficient denoising)\")\n",
    "print(\"  3. A Diffusion Transformer with spacetime patches\")\n",
    "print(\"  4. Text-conditioned video generation via cross-attention\")\n",
    "print(\"\\nThis is the same architecture family behind Sora!\")"
   ],
   "id": "cell_51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Reflection\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/21_reflection.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_21_reflection"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Reflection and Next Steps\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "\n",
    "1. **VAE quality tradeoff:** We used a small $\\beta_{\\text{KL}}$ to prioritize reconstruction. What happens if we increase it? How would this affect the diffusion model's job?\n",
    "\n",
    "2. **Patch size matters:** We used $2 \\times 4 \\times 4$ patches. What happens with smaller patches (more tokens, slower but more detail) vs larger patches (fewer tokens, faster but less detail)?\n",
    "\n",
    "3. **Factorized vs DiT:** The factorized U-Net separates spatial and temporal attention. The DiT mixes them via spacetime patches. When might one approach be better than the other?\n",
    "\n",
    "4. **Scaling:** Sora reportedly uses a much larger DiT (billions of parameters). What changes when you scale up? Do you expect the same architecture to work, just bigger?\n",
    "\n",
    "5. **Real text conditioning:** We used a simple one-hot direction label. How would you integrate a real text encoder like CLIP? What additional challenges arise?\n",
    "\n",
    "### üèÜ Optional Challenges\n",
    "\n",
    "1. **3D VAE:** Extend the VAE to compress temporally as well (reduce 8 frames to 4 latent frames). How does this affect reconstruction quality and diffusion efficiency?\n",
    "\n",
    "2. **Classifier-free guidance:** Implement classifier-free guidance by randomly dropping the conditioning during training (replace with zeros 10% of the time). At inference, interpolate between conditional and unconditional predictions.\n",
    "\n",
    "3. **Longer videos:** Try generating 16 or 32 frame videos. Does the DiT handle longer sequences gracefully?\n",
    "\n",
    "### Series Summary\n",
    "\n",
    "Across these three notebooks, you have built the complete toolkit for video diffusion:\n",
    "- **Notebook 1:** Video diffusion basics ‚Äî forward/reverse process, 3D convolutions, DDPM sampling\n",
    "- **Notebook 2:** Factorized attention ‚Äî spatial and temporal attention, computational efficiency\n",
    "- **Notebook 3:** Modern architectures ‚Äî Video VAE, latent diffusion, DiT with spacetime patches, text conditioning\n",
    "\n",
    "These are the core building blocks behind every modern video generation system, from Stable Video Diffusion to Sora."
   ],
   "id": "cell_52"
  }
 ]
}