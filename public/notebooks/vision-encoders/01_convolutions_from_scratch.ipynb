{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Convolutions from Scratch -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutions from Scratch -- Vizuara\n",
    "\n",
    "**How do neural networks learn to see? We build the convolution operation from first principles, then stack them into a CNN that classifies images.**\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Understand what a convolution is by implementing it from scratch\n",
    "2. Visualize learned filters and feature maps\n",
    "3. Build a complete CNN and train it on CIFAR-10\n",
    "4. Understand max pooling, ReLU, and feature hierarchies\n",
    "\n",
    "**Runtime:** Google Colab (GPU recommended, T4 is sufficient)\n",
    "**Estimated time:** 45-60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Every modern vision system -- from self-driving cars to medical imaging AI -- relies on a simple idea: **slide a small filter across an image and compute a weighted sum at each position.** This operation, the convolution, is the building block of computer vision.\n",
    "\n",
    "But why does sliding a small filter work for understanding images? The key insight is that images have **local structure**. Edges, corners, and textures are all local patterns -- they do not depend on what is happening on the other side of the image. A vertical edge looks the same whether it appears in the top-left or bottom-right corner.\n",
    "\n",
    "By the end of this notebook, you will have implemented convolutions from scratch, visualized what CNNs actually learn, and trained a working image classifier. Let us begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Let us start with a simple thought experiment. Imagine you have a small magnifying glass that you slide across a photograph. At each position, you look at the small patch of pixels under the magnifying glass, perform a calculation, and write down a single number.\n",
    "\n",
    "When you have done this for every position, you have a new grid of numbers -- a **feature map** -- that captures some local pattern from the original image.\n",
    "\n",
    "The calculation we perform at each position is a **weighted sum**: multiply each pixel by a corresponding weight, then add them all up. The set of weights is called a **filter** or **kernel**.\n",
    "\n",
    "Let us see this concretely with a simple 5x5 image and a 3x3 filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple 5x5 image with a vertical edge\n",
    "image = np.array([\n",
    "    [0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 1, 1],\n",
    "], dtype=np.float32)\n",
    "\n",
    "# A vertical edge detector filter\n",
    "vertical_edge_filter = np.array([\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1],\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Visualize both\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "axes[0].imshow(image, cmap='gray', vmin=0, vmax=1)\n",
    "axes[0].set_title('Input Image (5x5)', fontsize=12)\n",
    "axes[0].set_xticks(range(5))\n",
    "axes[0].set_yticks(range(5))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        axes[0].text(j, i, f'{image[i,j]:.0f}', ha='center', va='center',\n",
    "                    color='red' if image[i,j] < 0.5 else 'blue', fontsize=11)\n",
    "\n",
    "axes[1].imshow(vertical_edge_filter, cmap='RdBu', vmin=-1, vmax=1)\n",
    "axes[1].set_title('Vertical Edge Filter (3x3)', fontsize=12)\n",
    "axes[1].set_xticks(range(3))\n",
    "axes[1].set_yticks(range(3))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[1].text(j, i, f'{vertical_edge_filter[i,j]:.0f}', ha='center',\n",
    "                    va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "The 2D convolution operation can be written as:\n",
    "\n",
    "$$O(i, j) = \\sum_{m=0}^{k-1} \\sum_{n=0}^{k-1} I(i+m, j+n) \\cdot K(m, n)$$\n",
    "\n",
    "where:\n",
    "- $I$ is the input image\n",
    "- $K$ is the filter (kernel) of size $k \\times k$\n",
    "- $O$ is the output feature map\n",
    "\n",
    "Let us plug in some simple numbers. For our 5x5 image and 3x3 filter above, the output at position (0,0) is:\n",
    "\n",
    "$$O(0,0) = (0 \\times -1) + (0 \\times 0) + (1 \\times 1) + (0 \\times -1) + (0 \\times 0) + (1 \\times 1) + (0 \\times -1) + (0 \\times 0) + (1 \\times 1) = 3$$\n",
    "\n",
    "This value of 3 tells us there is a strong vertical edge at this position. The filter assigns negative weights to the left side and positive weights to the right side -- so when pixels transition from dark (0) to bright (1), the output is large.\n",
    "\n",
    "Now let us implement this from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_manual(image, kernel):\n",
    "    \"\"\"\n",
    "    Perform 2D convolution manually (no padding, stride=1).\n",
    "\n",
    "    Args:\n",
    "        image: 2D numpy array (H, W)\n",
    "        kernel: 2D numpy array (kH, kW)\n",
    "\n",
    "    Returns:\n",
    "        output: 2D numpy array\n",
    "    \"\"\"\n",
    "    H, W = image.shape\n",
    "    kH, kW = kernel.shape\n",
    "    out_H = H - kH + 1\n",
    "    out_W = W - kW + 1\n",
    "    output = np.zeros((out_H, out_W))\n",
    "\n",
    "    for i in range(out_H):\n",
    "        for j in range(out_W):\n",
    "            # Extract the local patch\n",
    "            patch = image[i:i+kH, j:j+kW]\n",
    "            # Element-wise multiply and sum\n",
    "            output[i, j] = np.sum(patch * kernel)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Apply our vertical edge detector\n",
    "output = conv2d_manual(image, vertical_edge_filter)\n",
    "\n",
    "print(\"Input image (5x5):\")\n",
    "print(image)\n",
    "print(f\"\\nFilter (3x3):\")\n",
    "print(vertical_edge_filter)\n",
    "print(f\"\\nOutput feature map ({output.shape[0]}x{output.shape[1]}):\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the output is a 3x3 feature map (the image shrinks because the filter cannot extend beyond the edges). The middle column has values of 3, which corresponds exactly to where the vertical edge is in the original image. This is exactly what we want.\n",
    "\n",
    "Let us visualize the convolution operation step by step and try different filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the convolution result\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "axes[0].imshow(image, cmap='gray')\n",
    "axes[0].set_title('Input Image')\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        axes[0].text(j, i, f'{image[i,j]:.0f}', ha='center', va='center', fontsize=10)\n",
    "\n",
    "axes[1].imshow(vertical_edge_filter, cmap='RdBu', vmin=-1, vmax=1)\n",
    "axes[1].set_title('Vertical Edge Filter')\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[1].text(j, i, f'{vertical_edge_filter[i,j]:.0f}', ha='center', va='center', fontsize=12)\n",
    "\n",
    "axes[2].imshow(output, cmap='hot')\n",
    "axes[2].set_title('Feature Map (Edge Detected!)')\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[2].text(j, i, f'{output[i,j]:.0f}', ha='center', va='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Now try different filters\n",
    "horizontal_edge = np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]], dtype=np.float32)\n",
    "sharpen = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]], dtype=np.float32)\n",
    "blur = np.ones((3, 3), dtype=np.float32) / 9.0\n",
    "\n",
    "# Create a more interesting test image\n",
    "test_img = np.zeros((8, 8), dtype=np.float32)\n",
    "test_img[2:6, 2:6] = 1  # White square in center\n",
    "\n",
    "filters = {'Vertical Edge': vertical_edge_filter, 'Horizontal Edge': horizontal_edge,\n",
    "           'Sharpen': sharpen, 'Blur': blur}\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(18, 3))\n",
    "axes[0].imshow(test_img, cmap='gray')\n",
    "axes[0].set_title('Input (8x8)')\n",
    "\n",
    "for idx, (name, filt) in enumerate(filters.items()):\n",
    "    result = conv2d_manual(test_img, filt)\n",
    "    axes[idx+1].imshow(result, cmap='hot')\n",
    "    axes[idx+1].set_title(name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "Now that we understand the basic convolution, let us build the three key components of a CNN:\n",
    "\n",
    "### Component 1: Convolution Layer (with learnable filters)\n",
    "\n",
    "In a real CNN, the filter values are **not hand-designed** -- they are **learned** during training. The network discovers which patterns are most useful for the task. Let us implement a convolution layer using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component 1: Understanding PyTorch's Conv2d\n",
    "\n",
    "# Create a single convolution layer\n",
    "conv_layer = nn.Conv2d(\n",
    "    in_channels=1,     # Grayscale input (1 channel)\n",
    "    out_channels=4,    # Learn 4 different filters\n",
    "    kernel_size=3,     # 3x3 filters\n",
    "    padding=1,         # Add padding to maintain spatial size\n",
    "    bias=False         # No bias for clarity\n",
    ")\n",
    "\n",
    "# Inspect the learnable parameters\n",
    "print(f\"Filter shape: {conv_layer.weight.shape}\")\n",
    "print(f\"  -> {conv_layer.weight.shape[0]} filters\")\n",
    "print(f\"  -> each filter: {conv_layer.weight.shape[2]}x{conv_layer.weight.shape[3]}\")\n",
    "print(f\"  -> Total parameters: {conv_layer.weight.numel()}\")\n",
    "\n",
    "# Apply to a random 8x8 image\n",
    "x = torch.randn(1, 1, 8, 8)  # (batch=1, channels=1, H=8, W=8)\n",
    "y = conv_layer(x)\n",
    "print(f\"\\nInput shape:  {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}  (4 feature maps, same spatial size due to padding)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 2: ReLU Activation\n",
    "\n",
    "After convolution, we apply the **ReLU** (Rectified Linear Unit) activation function, which simply sets all negative values to zero:\n",
    "\n",
    "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "\n",
    "Let us plug in some numbers: $\\text{ReLU}(-3) = 0$, $\\text{ReLU}(0) = 0$, $\\text{ReLU}(5) = 5$.\n",
    "\n",
    "This introduces non-linearity -- without it, stacking multiple convolutions would be equivalent to a single convolution (linear operations compose into linear operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component 2: ReLU activation\n",
    "x_sample = torch.tensor([-3.0, -1.0, 0.0, 1.0, 3.0, 5.0])\n",
    "x_relu = F.relu(x_sample)\n",
    "\n",
    "print(\"Input:\", x_sample.numpy())\n",
    "print(\"ReLU: \", x_relu.numpy())\n",
    "\n",
    "# Visualize ReLU\n",
    "x_range = torch.linspace(-5, 5, 100)\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(x_range, F.relu(x_range), 'b-', linewidth=2)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.title('ReLU Activation: max(0, x)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 3: Max Pooling\n",
    "\n",
    "Max pooling reduces the spatial dimensions by keeping only the **maximum value** in each local region:\n",
    "\n",
    "$$\\text{MaxPool}(i, j) = \\max(O(2i, 2j), O(2i{+}1, 2j), O(2i, 2j{+}1), O(2i{+}1, 2j{+}1))$$\n",
    "\n",
    "Let us plug in some numbers. Given a 4x4 feature map with a 2x2 pooling window:\n",
    "\n",
    "```\n",
    "6  2 | 3  1\n",
    "1  7 | 0  4\n",
    "-----------\n",
    "8  0 | 5  2\n",
    "3  1 | 2  9\n",
    "```\n",
    "\n",
    "Max pooling gives: `[[7, 4], [8, 9]]` -- we keep the strongest activation from each 2x2 block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component 3: Max Pooling\n",
    "feature_map = torch.tensor([[\n",
    "    [6., 2., 3., 1.],\n",
    "    [1., 7., 0., 4.],\n",
    "    [8., 0., 5., 2.],\n",
    "    [3., 1., 2., 9.]\n",
    "]]).unsqueeze(0)  # Shape: (1, 1, 4, 4)\n",
    "\n",
    "pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "pooled = pool(feature_map)\n",
    "\n",
    "print(\"Before max pooling (4x4):\")\n",
    "print(feature_map.squeeze().numpy())\n",
    "print(f\"\\nAfter max pooling (2x2):\")\n",
    "print(pooled.squeeze().numpy())\n",
    "print(f\"\\nShape: {feature_map.shape} -> {pooled.shape}\")\n",
    "print(\"Spatial dimensions halved, strongest features preserved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Implement a Horizontal Edge Detector\n",
    "\n",
    "Complete the function below to create a horizontal edge detection filter and apply it to a test image. A horizontal edge detector should detect transitions from dark to bright going top-to-bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_horizontal_edges(image):\n",
    "    \"\"\"\n",
    "    Apply a horizontal edge detection filter to the image.\n",
    "\n",
    "    Args:\n",
    "        image: 2D numpy array\n",
    "\n",
    "    Returns:\n",
    "        output: 2D numpy array (feature map with horizontal edges highlighted)\n",
    "\n",
    "    Hint: The horizontal edge filter is the transpose of the vertical edge filter.\n",
    "          It should have negative values in the top row and positive values in the bottom row.\n",
    "    \"\"\"\n",
    "    # TODO: Define a 3x3 horizontal edge detection filter\n",
    "    # horizontal_filter = np.array([...])\n",
    "\n",
    "    # TODO: Apply convolution using conv2d_manual\n",
    "    # output = ...\n",
    "\n",
    "    # return output\n",
    "    pass\n",
    "\n",
    "# Test image with horizontal edge\n",
    "test_h = np.array([\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 1],\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Uncomment when ready:\n",
    "# result = detect_horizontal_edges(test_h)\n",
    "# plt.figure(figsize=(8, 3))\n",
    "# plt.subplot(1, 2, 1); plt.imshow(test_h, cmap='gray'); plt.title('Input')\n",
    "# plt.subplot(1, 2, 2); plt.imshow(result, cmap='hot'); plt.title('Horizontal Edges')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification cell for TODO 1\n",
    "# Expected: The center row of the output should have high positive values (edge detected)\n",
    "# Run this after completing TODO 1 to verify:\n",
    "# assert result is not None, \"Function returned None\"\n",
    "# assert result.shape == (3, 3), f\"Expected (3,3), got {result.shape}\"\n",
    "# print(\"Center row values:\", result[1, :])\n",
    "# print(\"Expected: all values should be 3.0 (strong horizontal edge)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Build a Custom CNN\n",
    "\n",
    "Complete the CNN architecture below. Fill in the missing layers to create a 3-layer CNN for CIFAR-10 classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A 3-layer CNN for CIFAR-10 classification.\n",
    "\n",
    "    Architecture:\n",
    "        Conv2d(3, 32, 3, padding=1) -> ReLU -> MaxPool2d(2)\n",
    "        Conv2d(32, 64, 3, padding=1) -> ReLU -> MaxPool2d(2)\n",
    "        Conv2d(64, 128, 3, padding=1) -> ReLU -> MaxPool2d(2)\n",
    "        Flatten -> Linear(128*4*4, 256) -> ReLU -> Linear(256, 10)\n",
    "\n",
    "    Hint: CIFAR-10 images are 32x32x3. After 3 max-pool layers with stride 2,\n",
    "          the spatial size becomes 32/2/2/2 = 4.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: Define the convolutional layers\n",
    "        # self.conv1 = nn.Conv2d(...)\n",
    "        # self.conv2 = nn.Conv2d(...)\n",
    "        # self.conv3 = nn.Conv2d(...)\n",
    "        # self.pool = nn.MaxPool2d(...)\n",
    "        # self.fc1 = nn.Linear(...)\n",
    "        # self.fc2 = nn.Linear(...)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass\n",
    "        # x = self.pool(F.relu(self.conv1(x)))\n",
    "        # x = self.pool(F.relu(self.conv2(x)))\n",
    "        # x = self.pool(F.relu(self.conv3(x)))\n",
    "        # x = x.flatten(1)\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        # x = self.fc2(x)\n",
    "        # return x\n",
    "        pass\n",
    "\n",
    "# Verification:\n",
    "# model = MyCNN()\n",
    "# test_input = torch.randn(2, 3, 32, 32)\n",
    "# test_output = model(test_input)\n",
    "# print(f\"Input shape: {test_input.shape}\")\n",
    "# print(f\"Output shape: {test_output.shape}\")\n",
    "# assert test_output.shape == (2, 10), f\"Expected (2, 10), got {test_output.shape}\"\n",
    "# print(\"Architecture looks correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Now let us build a complete CNN and train it on CIFAR-10. We will use the pre-built `SimpleCNN` architecture from the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 4 * 4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Count parameters\n",
    "model = SimpleCNN().to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"SimpleCNN: {total_params:,} parameters\")\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "print(f\"Training samples: {len(trainset)}, Test samples: {len(testset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_losses, test_accs = [], []\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(trainloader)\n",
    "    train_acc = 100. * correct / total\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    test_acc = 100. * correct / total\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}: Loss={train_loss:.3f}, '\n",
    "          f'Train Acc={train_acc:.1f}%, Test Acc={test_acc:.1f}%')\n",
    "\n",
    "print(f'\\nFinal test accuracy: {test_accs[-1]:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization checkpoint 1: Training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.plot(train_losses, 'b-', linewidth=2)\n",
    "ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss'); ax1.set_title('Training Loss')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.plot(test_accs, 'r-', linewidth=2)\n",
    "ax2.set_xlabel('Epoch'); ax2.set_ylabel('Accuracy (%)'); ax2.set_title('Test Accuracy')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output\n",
    "\n",
    "Let us visualize what the CNN actually learned by looking at the first-layer filters and the feature maps they produce. This is one of the most fascinating aspects of CNNs -- the network discovers edge detectors, color detectors, and texture detectors on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization checkpoint 2: Visualize learned filters\n",
    "first_conv_weights = model.features[0].weight.data.cpu()\n",
    "print(f\"First conv layer filters: {first_conv_weights.shape}\")\n",
    "\n",
    "fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "fig.suptitle('Learned First-Layer Filters (32 filters, 3x3 each)', fontsize=14)\n",
    "\n",
    "for i in range(32):\n",
    "    ax = axes[i // 8, i % 8]\n",
    "    # Normalize filter for visualization\n",
    "    w = first_conv_weights[i].permute(1, 2, 0).numpy()\n",
    "    w = (w - w.min()) / (w.max() - w.min() + 1e-8)\n",
    "    ax.imshow(w)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'F{i+1}', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization checkpoint 3: Feature maps for a sample image\n",
    "model.eval()\n",
    "sample_img, sample_label = testset[0]\n",
    "sample_img_gpu = sample_img.unsqueeze(0).to(device)\n",
    "\n",
    "# Get feature maps after each conv layer\n",
    "with torch.no_grad():\n",
    "    fm1 = F.relu(model.features[0](sample_img_gpu))\n",
    "    fm2 = F.relu(model.features[3](model.features[2](fm1)))  # after pool+conv\n",
    "    fm3 = F.relu(model.features[6](model.features[5](fm2)))  # after pool+conv\n",
    "\n",
    "# Show original image\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "img_show = sample_img.permute(1, 2, 0).numpy()\n",
    "img_show = (img_show - img_show.min()) / (img_show.max() - img_show.min())\n",
    "axes[0].imshow(img_show)\n",
    "axes[0].set_title(f'Original: {classes[sample_label]}')\n",
    "\n",
    "# Show feature maps from each layer (average across channels)\n",
    "for idx, (fm, title) in enumerate([(fm1, 'Layer 1'), (fm2, 'Layer 2'), (fm3, 'Layer 3')]):\n",
    "    axes[idx+1].imshow(fm.squeeze().mean(0).cpu().numpy(), cmap='viridis')\n",
    "    axes[idx+1].set_title(f'{title} ({fm.shape[1]} channels)')\n",
    "    axes[idx+1].axis('off')\n",
    "\n",
    "plt.suptitle('Feature Maps: From Edges to Semantic Features', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice how:\")\n",
    "print(\"- Layer 1: captures edges and simple color patterns (high spatial resolution)\")\n",
    "print(\"- Layer 2: captures textures and combinations of edges (lower resolution)\")\n",
    "print(\"- Layer 3: captures high-level object features (very abstract)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**What we built:** We started with the raw convolution operation, implemented it from scratch, understood its mathematics, and then built a complete CNN that classifies images with 75-80% accuracy on CIFAR-10.\n",
    "\n",
    "**Key takeaways:**\n",
    "1. A convolution is simply a weighted sum over a local neighborhood\n",
    "2. The filter values are learned -- the network discovers useful patterns automatically\n",
    "3. ReLU adds non-linearity; max pooling reduces spatial dimensions\n",
    "4. Stacking convolutions creates a feature hierarchy: edges -> textures -> parts -> objects\n",
    "\n",
    "**Reflection questions:**\n",
    "- Why do you think the first-layer filters look like edge detectors? What would happen if we initialized them differently?\n",
    "- What happens to the test accuracy if we remove all max pooling layers? Why?\n",
    "- How would the feature maps change if we trained on a different dataset (e.g., medical images)?\n",
    "\n",
    "**Next:** In the next notebook, we will explore the limitations of CNNs and build a Vision Transformer from scratch -- an architecture that processes the *entire image at once* through self-attention."
   ]
  }
 ]
}