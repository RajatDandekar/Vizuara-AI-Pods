{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Vision Transformers from Scratch -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformers from Scratch -- Vizuara\n",
    "\n",
    "**We build the Vision Transformer (ViT) from first principles: patch embeddings, self-attention, and the full encoder -- all implemented manually before using PyTorch.**\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Understand why CNNs struggle with global context\n",
    "2. Implement patch embeddings from scratch\n",
    "3. Build the self-attention mechanism step by step\n",
    "4. Assemble a complete Vision Transformer\n",
    "\n",
    "**Runtime:** Google Colab (GPU recommended, T4 is sufficient)\n",
    "**Estimated time:** 60-75 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebook, we built a CNN that classifies images by sliding small filters across them. This works remarkably well, but it has a fundamental limitation: **each filter only sees a tiny local patch.**\n",
    "\n",
    "Consider an image of a bird flying over the ocean. To correctly classify it, the network needs to understand the relationship between the bird (top of image) and the water (bottom of image). In a CNN, this information must pass through many layers before these distant regions can \"communicate.\" In a Vision Transformer, every part of the image can attend to every other part in a single operation.\n",
    "\n",
    "This is the key idea behind the Vision Transformer: **treat an image as a sequence of patches and let every patch attend to every other patch.** By the end of this notebook, you will have built one from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Let us think about images differently. Instead of processing pixels with sliding filters, what if we:\n",
    "\n",
    "1. **Cut the image into fixed-size patches** (like puzzle pieces)\n",
    "2. **Flatten each patch** into a vector\n",
    "3. **Treat each patch as a \"word\"** in a sentence\n",
    "4. **Use the Transformer** (from NLP) to process this \"sentence\"\n",
    "\n",
    "This is exactly the idea from the 2020 paper \"An Image is Worth 16x16 Words.\" Let us see this concretely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample CIFAR-10 image\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "sample_img, sample_label = dataset[0]\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(f\"Image shape: {sample_img.shape}\")\n",
    "print(f\"Label: {classes[sample_label]}\")\n",
    "\n",
    "# Visualize the image and its patches\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(sample_img.permute(1, 2, 0))\n",
    "axes[0].set_title(f'Original Image: {classes[sample_label]}')\n",
    "\n",
    "# Image with patch grid overlay\n",
    "axes[1].imshow(sample_img.permute(1, 2, 0))\n",
    "patch_size = 4  # Using 4x4 patches for 32x32 images\n",
    "for i in range(0, 32, patch_size):\n",
    "    axes[1].axhline(y=i, color='red', linewidth=1)\n",
    "    axes[1].axvline(x=i, color='red', linewidth=1)\n",
    "axes[1].set_title(f'Divided into {(32//patch_size)**2} patches ({patch_size}x{patch_size})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "num_patches = (32 // patch_size) ** 2\n",
    "print(f\"\\nWith patch_size={patch_size}: {num_patches} patches\")\n",
    "print(f\"Each patch: {patch_size}x{patch_size}x3 = {patch_size*patch_size*3} values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Patch Embedding\n",
    "\n",
    "Given an image of size $H \\times W$ with $C$ channels, and patch size $P$:\n",
    "\n",
    "1. Number of patches: $N = \\frac{H \\times W}{P^2}$\n",
    "2. Each patch is flattened: $\\mathbf{x}_i^{\\text{patch}} \\in \\mathbb{R}^{P^2 C}$\n",
    "3. Linear projection: $\\mathbf{z}_i = \\mathbf{E} \\cdot \\mathbf{x}_i^{\\text{patch}} + \\mathbf{e}_i^{\\text{pos}}$\n",
    "\n",
    "where $\\mathbf{E} \\in \\mathbb{R}^{D \\times (P^2 C)}$ is the projection matrix and $\\mathbf{e}_i^{\\text{pos}}$ is the positional embedding.\n",
    "\n",
    "Let us plug in some numbers for our CIFAR-10 images:\n",
    "- Image: $32 \\times 32 \\times 3$, Patch size: $P = 4$\n",
    "- Each patch: $4 \\times 4 \\times 3 = 48$ values\n",
    "- Number of patches: $\\frac{32 \\times 32}{4^2} = 64$ patches\n",
    "- Projection to dimension $D = 64$\n",
    "\n",
    "So the image becomes a sequence of 64 tokens, each of dimension 64.\n",
    "\n",
    "### Self-Attention\n",
    "\n",
    "The attention mechanism computes:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "where Q, K, V are linear projections of the input. The scaling by $\\sqrt{d_k}$ prevents the dot products from becoming too large.\n",
    "\n",
    "Let us implement each component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### Component 1: Patch Embedding (from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(image, patch_size):\n",
    "    \"\"\"\n",
    "    Extract non-overlapping patches from an image.\n",
    "\n",
    "    Args:\n",
    "        image: tensor of shape (C, H, W)\n",
    "        patch_size: int (P)\n",
    "\n",
    "    Returns:\n",
    "        patches: tensor of shape (N, P*P*C) where N = (H*W) / P^2\n",
    "    \"\"\"\n",
    "    C, H, W = image.shape\n",
    "    assert H % patch_size == 0 and W % patch_size == 0\n",
    "\n",
    "    # Reshape image into grid of patches\n",
    "    # (C, H, W) -> (C, H/P, P, W/P, P)\n",
    "    patches = image.reshape(C, H // patch_size, patch_size, W // patch_size, patch_size)\n",
    "    # Rearrange: (H/P, W/P, C, P, P) -> flatten last 3 dims\n",
    "    patches = patches.permute(1, 3, 0, 2, 4).contiguous()\n",
    "    patches = patches.reshape(-1, C * patch_size * patch_size)\n",
    "\n",
    "    return patches\n",
    "\n",
    "# Extract patches from our sample image\n",
    "patch_size = 4\n",
    "patches = extract_patches(sample_img, patch_size)\n",
    "print(f\"Image shape: {sample_img.shape}\")\n",
    "print(f\"Patches shape: {patches.shape}\")\n",
    "print(f\"  -> {patches.shape[0]} patches, each with {patches.shape[1]} values\")\n",
    "\n",
    "# Visualize the first 16 patches\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "fig.suptitle(f'First 16 patches (out of {patches.shape[0]})', fontsize=14)\n",
    "for i in range(16):\n",
    "    ax = axes[i // 8, i % 8]\n",
    "    patch = patches[i].reshape(3, patch_size, patch_size).permute(1, 2, 0).numpy()\n",
    "    ax.imshow(patch)\n",
    "    ax.set_title(f'P{i}', fontsize=9)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now implement the full patch embedding with linear projection\n",
    "\n",
    "class PatchEmbeddingManual(nn.Module):\n",
    "    \"\"\"Patch embedding implemented step by step.\"\"\"\n",
    "\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        patch_dim = in_channels * patch_size * patch_size\n",
    "\n",
    "        # Linear projection: (P*P*C) -> D\n",
    "        self.projection = nn.Linear(patch_dim, embed_dim)\n",
    "\n",
    "        # Learnable positional embeddings\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim) * 0.02)\n",
    "\n",
    "        # Learnable [CLS] token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # Step 1: Extract patches\n",
    "        patches = []\n",
    "        for img in x:\n",
    "            patches.append(extract_patches(img, self.patch_size))\n",
    "        patches = torch.stack(patches)  # (B, N, P*P*C)\n",
    "\n",
    "        # Step 2: Linear projection\n",
    "        embeddings = self.projection(patches)  # (B, N, D)\n",
    "\n",
    "        # Step 3: Prepend [CLS] token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, D)\n",
    "        embeddings = torch.cat([cls_tokens, embeddings], dim=1)  # (B, N+1, D)\n",
    "\n",
    "        # Step 4: Add positional embeddings\n",
    "        embeddings = embeddings + self.pos_embedding  # (B, N+1, D)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "# Test it\n",
    "patch_embed = PatchEmbeddingManual(img_size=32, patch_size=4, embed_dim=64)\n",
    "test_batch = sample_img.unsqueeze(0)  # (1, 3, 32, 32)\n",
    "output = patch_embed(test_batch)\n",
    "print(f\"Input: {test_batch.shape}\")\n",
    "print(f\"Output: {output.shape}\")\n",
    "print(f\"  -> {output.shape[1]} tokens (1 CLS + {output.shape[1]-1} patches)\")\n",
    "print(f\"  -> each token is {output.shape[2]}-dimensional\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 2: Self-Attention (from scratch)\n",
    "\n",
    "Now let us implement self-attention step by step. This is the core mechanism that allows every patch to communicate with every other patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionManual(nn.Module):\n",
    "    \"\"\"Self-attention implemented step by step for pedagogical clarity.\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim=64, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads  # d_k\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        B, N, D = x.shape\n",
    "        H = self.num_heads\n",
    "        d_k = self.head_dim\n",
    "\n",
    "        # Step 1: Project to Q, K, V\n",
    "        Q = self.W_q(x)  # (B, N, D)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        # Step 2: Reshape for multi-head: (B, N, D) -> (B, H, N, d_k)\n",
    "        Q = Q.reshape(B, N, H, d_k).transpose(1, 2)\n",
    "        K = K.reshape(B, N, H, d_k).transpose(1, 2)\n",
    "        V = V.reshape(B, N, H, d_k).transpose(1, 2)\n",
    "\n",
    "        # Step 3: Compute attention scores: QK^T / sqrt(d_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_k ** 0.5)\n",
    "        # scores shape: (B, H, N, N)\n",
    "\n",
    "        # Step 4: Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        # attention_weights shape: (B, H, N, N)\n",
    "\n",
    "        # Step 5: Weighted sum of values\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        # output shape: (B, H, N, d_k)\n",
    "\n",
    "        # Step 6: Reshape back and project\n",
    "        output = output.transpose(1, 2).reshape(B, N, D)\n",
    "        output = self.W_o(output)\n",
    "\n",
    "        if return_attention:\n",
    "            return output, attention_weights\n",
    "        return output\n",
    "\n",
    "# Test it with our patch embeddings\n",
    "attn = SelfAttentionManual(embed_dim=64, num_heads=4)\n",
    "embeddings = patch_embed(test_batch)\n",
    "attn_output, attn_weights = attn(embeddings, return_attention=True)\n",
    "\n",
    "print(f\"Input embeddings: {embeddings.shape}\")\n",
    "print(f\"Attention output: {attn_output.shape}\")\n",
    "print(f\"Attention weights: {attn_weights.shape}\")\n",
    "print(f\"  -> {attn_weights.shape[1]} heads, each producing a {attn_weights.shape[2]}x{attn_weights.shape[3]} attention map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization checkpoint 1: Attention patterns\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "fig.suptitle('Attention Weights from 4 Heads (CLS token attending to all patches)', fontsize=13)\n",
    "\n",
    "for head in range(4):\n",
    "    # Get CLS token's attention to all patches (row 0 of attention matrix)\n",
    "    cls_attn = attn_weights[0, head, 0, 1:].detach().numpy()  # Exclude CLS-to-CLS\n",
    "    # Reshape to spatial grid\n",
    "    grid_size = int(np.sqrt(len(cls_attn)))\n",
    "    attn_map = cls_attn.reshape(grid_size, grid_size)\n",
    "\n",
    "    axes[head].imshow(attn_map, cmap='hot')\n",
    "    axes[head].set_title(f'Head {head+1}')\n",
    "    axes[head].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Each head attends to different parts of the image -- this is the power of multi-head attention.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Compute Attention Manually for 3 Patches\n",
    "\n",
    "Given these Q, K, V matrices for 3 patches (4-dimensional each), compute the attention output step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_manual(Q, K, V):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention manually.\n",
    "\n",
    "    Args:\n",
    "        Q: tensor of shape (N, d_k) -- queries\n",
    "        K: tensor of shape (N, d_k) -- keys\n",
    "        V: tensor of shape (N, d_k) -- values\n",
    "\n",
    "    Returns:\n",
    "        output: tensor of shape (N, d_k) -- attention output\n",
    "        weights: tensor of shape (N, N) -- attention weights\n",
    "\n",
    "    Steps:\n",
    "        1. Compute QK^T (matrix multiplication)\n",
    "        2. Scale by 1/sqrt(d_k)\n",
    "        3. Apply softmax row-wise\n",
    "        4. Multiply by V\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "\n",
    "    # TODO: Step 1 - Compute raw scores\n",
    "    # scores = ...\n",
    "\n",
    "    # TODO: Step 2 - Scale\n",
    "    # scores = scores / ...\n",
    "\n",
    "    # TODO: Step 3 - Softmax\n",
    "    # weights = F.softmax(scores, dim=...)\n",
    "\n",
    "    # TODO: Step 4 - Weighted sum\n",
    "    # output = torch.matmul(weights, V)\n",
    "\n",
    "    # return output, weights\n",
    "    pass\n",
    "\n",
    "# Test with known values\n",
    "Q_test = torch.tensor([[1., 0., 1., 0.],\n",
    "                        [0., 1., 0., 1.],\n",
    "                        [1., 1., 0., 0.]])\n",
    "K_test = torch.tensor([[1., 1., 0., 0.],\n",
    "                        [0., 0., 1., 1.],\n",
    "                        [1., 0., 1., 0.]])\n",
    "V_test = torch.tensor([[1., 0., 0., 1.],\n",
    "                        [0., 1., 1., 0.],\n",
    "                        [1., 1., 0., 0.]])\n",
    "\n",
    "# Uncomment when ready:\n",
    "# output, weights = compute_attention_manual(Q_test, K_test, V_test)\n",
    "# print(\"Attention weights:\")\n",
    "# print(weights.numpy().round(3))\n",
    "# print(\"\\nOutput:\")\n",
    "# print(output.numpy().round(3))\n",
    "# print(\"\\nExpected row 0 weights: ~[0.24, 0.24, 0.52] (patch 1 attends most to patch 3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Add Layer Normalization and MLP to Create a Transformer Block\n",
    "\n",
    "Complete the Transformer encoder block below. Each block has:\n",
    "- LayerNorm -> Multi-Head Self-Attention -> Residual\n",
    "- LayerNorm -> MLP (2 linear layers with GELU) -> Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer encoder block.\n",
    "\n",
    "    Architecture:\n",
    "        x' = MSA(LN(x)) + x       (attention with residual)\n",
    "        z  = MLP(LN(x')) + x'      (feedforward with residual)\n",
    "\n",
    "    Hint: MLP has two linear layers with GELU activation.\n",
    "          Hidden dim is typically 4x the embed_dim.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=64, num_heads=4, mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        # TODO: Define components\n",
    "        # self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        # self.attn = SelfAttentionManual(embed_dim, num_heads)\n",
    "        # self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        # self.mlp = nn.Sequential(\n",
    "        #     nn.Linear(embed_dim, embed_dim * mlp_ratio),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(embed_dim * mlp_ratio, embed_dim),\n",
    "        # )\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward with residual connections\n",
    "        # x = self.attn(self.norm1(x)) + x    # Attention + residual\n",
    "        # x = self.mlp(self.norm2(x)) + x      # MLP + residual\n",
    "        # return x\n",
    "        pass\n",
    "\n",
    "# Verification:\n",
    "# block = TransformerBlock(embed_dim=64, num_heads=4)\n",
    "# test_input = torch.randn(2, 65, 64)  # (batch=2, tokens=65, dim=64)\n",
    "# test_output = block(test_input)\n",
    "# assert test_output.shape == test_input.shape, \"Shape mismatch!\"\n",
    "# print(f\"Input: {test_input.shape} -> Output: {test_output.shape}\")\n",
    "# print(\"Transformer block working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us assemble the complete Vision Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockFull(nn.Module):\n",
    "    \"\"\"Complete Transformer encoder block.\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = SelfAttentionManual(embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * mlp_ratio),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * mlp_ratio, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(self.norm1(x)) + x\n",
    "        x = self.mlp(self.norm2(x)) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleViT(nn.Module):\n",
    "    \"\"\"Complete Vision Transformer.\"\"\"\n",
    "\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3,\n",
    "                 embed_dim=128, num_heads=4, num_layers=4, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        patch_dim = in_channels * patch_size * patch_size\n",
    "\n",
    "        # Patch embedding (using Conv2d for efficiency)\n",
    "        self.patch_embed = nn.Conv2d(in_channels, embed_dim,\n",
    "                                      kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        # CLS token and positional embeddings\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.randn(1, self.num_patches + 1, embed_dim) * 0.02)\n",
    "\n",
    "        # Transformer encoder blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlockFull(embed_dim, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Final normalization and classification head\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)       # (B, D, H/P, W/P)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, N, D)\n",
    "\n",
    "        # Prepend CLS token\n",
    "        cls = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls, x], dim=1)\n",
    "\n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Classification\n",
    "        x = self.norm(x)\n",
    "        cls_output = x[:, 0]  # CLS token\n",
    "        return self.head(cls_output)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "vit = SimpleViT(img_size=32, patch_size=4, embed_dim=128,\n",
    "                num_heads=4, num_layers=4, num_classes=10).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in vit.parameters())\n",
    "print(f\"SimpleViT: {total_params:,} parameters\")\n",
    "print(f\"\\nArchitecture components:\")\n",
    "print(f\"  Patch embedding: Conv2d(3, 128, kernel_size=4, stride=4)\")\n",
    "print(f\"  Patches: {vit.num_patches}\")\n",
    "print(f\"  Transformer blocks: 4\")\n",
    "print(f\"  Embed dim: 128, Heads: 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(vit.parameters(), lr=3e-4, weight_decay=0.05)\n",
    "\n",
    "train_losses, test_accs = [], []\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    vit.train()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = vit(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(trainloader)\n",
    "    train_acc = 100. * correct / total\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    vit.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = vit(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    test_acc = 100. * correct / total\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}: Loss={train_loss:.3f}, '\n",
    "          f'Train={train_acc:.1f}%, Test={test_acc:.1f}%')\n",
    "\n",
    "print(f'\\nBest test accuracy: {max(test_accs):.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization checkpoint 2: Training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.plot(train_losses, 'b-', linewidth=2)\n",
    "ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss'); ax1.set_title('Training Loss')\n",
    "ax1.grid(alpha=0.3)\n",
    "ax2.plot(test_accs, 'r-', linewidth=2)\n",
    "ax2.set_xlabel('Epoch'); ax2.set_ylabel('Accuracy (%)'); ax2.set_title('Test Accuracy')\n",
    "ax2.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output\n",
    "\n",
    "Let us visualize the attention maps -- these show which patches the model focuses on when making predictions. We will look at the CLS token's attention to all patches in the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization checkpoint 3: Attention maps on real images\n",
    "vit.eval()\n",
    "fig, axes = plt.subplots(3, 5, figsize=(18, 10))\n",
    "fig.suptitle('ViT Attention Maps: Where the Model Looks', fontsize=15)\n",
    "\n",
    "for row in range(3):\n",
    "    idx = row * 100  # Pick different images\n",
    "    img, label = testset[idx]\n",
    "    img_gpu = img.unsqueeze(0).to(device)\n",
    "\n",
    "    # Get attention from the last block\n",
    "    with torch.no_grad():\n",
    "        x = vit.patch_embed(img_gpu).flatten(2).transpose(1, 2)\n",
    "        cls = vit.cls_token.expand(1, -1, -1)\n",
    "        x = torch.cat([cls, x], dim=1) + vit.pos_embed\n",
    "        for block in vit.blocks[:-1]:\n",
    "            x = block(x)\n",
    "        # Get attention from last block\n",
    "        attn_out, attn_w = vit.blocks[-1].attn(vit.blocks[-1].norm1(x), return_attention=True)\n",
    "\n",
    "    # Show original image\n",
    "    img_show = img.permute(1, 2, 0).numpy()\n",
    "    img_show = (img_show - img_show.min()) / (img_show.max() - img_show.min())\n",
    "    axes[row, 0].imshow(img_show)\n",
    "    axes[row, 0].set_title(f'{classes[label]}')\n",
    "    axes[row, 0].axis('off')\n",
    "\n",
    "    # Show attention from each head (CLS to patches)\n",
    "    for head in range(4):\n",
    "        cls_attn = attn_w[0, head, 0, 1:].cpu().numpy()\n",
    "        grid_size = int(np.sqrt(len(cls_attn)))\n",
    "        attn_map = cls_attn.reshape(grid_size, grid_size)\n",
    "        # Upsample attention map to image size\n",
    "        attn_map_resized = np.kron(attn_map, np.ones((32//grid_size, 32//grid_size)))\n",
    "\n",
    "        axes[row, head+1].imshow(img_show)\n",
    "        axes[row, head+1].imshow(attn_map_resized, cmap='hot', alpha=0.5)\n",
    "        axes[row, head+1].set_title(f'Head {head+1}')\n",
    "        axes[row, head+1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each attention head learns to focus on different parts of the image.\")\n",
    "print(\"Some heads attend to the object; others attend to background or edges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**What we built:** We implemented the entire Vision Transformer from scratch:\n",
    "- Patch extraction and embedding\n",
    "- Self-attention with multi-head attention\n",
    "- Transformer encoder blocks with residual connections\n",
    "- Full ViT model trained on CIFAR-10\n",
    "\n",
    "**Key takeaways:**\n",
    "1. Vision Transformers treat images as sequences of patches -- just like words in a sentence\n",
    "2. Self-attention lets every patch attend to every other patch, capturing global relationships\n",
    "3. Multi-head attention allows the model to attend to different aspects simultaneously\n",
    "4. ViTs need sufficient data to match CNN performance (they lack built-in inductive biases)\n",
    "\n",
    "**Reflection questions:**\n",
    "- What happens if you double the patch size from 4 to 8? How does this affect the number of tokens and the accuracy?\n",
    "- Why does the ViT need positional embeddings while a CNN does not?\n",
    "- If you remove the CLS token and instead average all patch outputs, how does the accuracy change?\n",
    "\n",
    "**Next:** In the next notebook, we will directly compare CNNs and ViTs on the same task, exploring their different strengths and trade-offs."
   ]
  }
 ]
}