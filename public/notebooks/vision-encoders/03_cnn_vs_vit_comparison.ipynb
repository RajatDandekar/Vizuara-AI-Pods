{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "CNN vs. Vision Transformer: A Hands-On Comparison -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN vs. Vision Transformer: A Hands-On Comparison -- Vizuara\n",
    "\n",
    "**We train both a CNN and a ViT on the same dataset and directly compare their behavior, accuracy, computational cost, and what they learn.**\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Train both architectures under identical conditions\n",
    "2. Compare accuracy, parameter count, and training speed\n",
    "3. Analyze how data size affects each architecture differently\n",
    "4. Visualize the differences in learned representations\n",
    "\n",
    "**Runtime:** Google Colab (GPU, T4 sufficient)\n",
    "**Estimated time:** 45-60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "CNNs and Vision Transformers are the two dominant paradigms for visual understanding. Choosing between them is one of the most important architectural decisions in modern computer vision. But the choice is not always obvious -- it depends on your dataset size, computational budget, and the type of visual reasoning your task requires.\n",
    "\n",
    "In this notebook, we will run a controlled experiment: **same data, same training schedule, same evaluation** -- different architecture. By the end, you will have a clear intuition for when to use each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "The key difference between CNNs and ViTs can be summarized in one sentence: **CNNs have strong built-in assumptions about images; ViTs learn everything from data.**\n",
    "\n",
    "CNNs assume:\n",
    "- **Locality**: nearby pixels are more related than distant ones\n",
    "- **Translation equivariance**: a cat looks the same regardless of where it appears\n",
    "\n",
    "These assumptions are incredibly useful -- they act as a form of regularization. But they also limit what the network can learn.\n",
    "\n",
    "ViTs make almost no assumptions. They must learn locality, translation equivariance, and spatial structure entirely from the training data. This gives them more flexibility but requires more data.\n",
    "\n",
    "Let us set up both architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define both architectures\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(), nn.Linear(128*4*4, 256), nn.ReLU(), nn.Linear(256, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))\n",
    "\n",
    "\n",
    "class SimpleViT(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, embed_dim=128,\n",
    "                 num_heads=4, num_layers=4, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_embed = nn.Conv2d(3, embed_dim, patch_size, stride=patch_size)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches+1, embed_dim)*0.02)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads, dim_feedforward=embed_dim*4,\n",
    "            activation='gelu', batch_first=True, norm_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x).flatten(2).transpose(1, 2)\n",
    "        x = torch.cat([self.cls_token.expand(B, -1, -1), x], dim=1) + self.pos_embed\n",
    "        x = self.norm(self.transformer(x))\n",
    "        return self.head(x[:, 0])\n",
    "\n",
    "\n",
    "# Instantiate both\n",
    "cnn = SimpleCNN().to(device)\n",
    "vit = SimpleViT().to(device)\n",
    "\n",
    "cnn_params = sum(p.numel() for p in cnn.parameters())\n",
    "vit_params = sum(p.numel() for p in vit.parameters())\n",
    "\n",
    "print(f\"SimpleCNN: {cnn_params:,} parameters\")\n",
    "print(f\"SimpleViT: {vit_params:,} parameters\")\n",
    "print(f\"ViT/CNN ratio: {vit_params/cnn_params:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "The key mathematical difference:\n",
    "\n",
    "**CNN**: Each output neuron is a function of a **local** region:\n",
    "$$y_{i,j} = \\sum_{m,n} W_{m,n} \\cdot x_{i+m, j+n}$$\n",
    "\n",
    "**ViT**: Each output token is a function of **all** input tokens:\n",
    "$$y_i = \\sum_j \\alpha_{ij} \\cdot V_j, \\quad \\alpha_{ij} = \\frac{\\exp(Q_i \\cdot K_j / \\sqrt{d})}{\\sum_k \\exp(Q_i \\cdot K_k / \\sqrt{d})}$$\n",
    "\n",
    "Let us plug in complexity numbers:\n",
    "- For a 32x32 image with 3x3 convolutions: each output pixel depends on **9 values** (local)\n",
    "- For a 32x32 image with 4x4 patches (64 patches): each patch attends to **64 other patches** (global)\n",
    "\n",
    "This global attention is both the ViT's greatest strength and its computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component: The Training Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified training function for fair comparison\n",
    "def train_model(model, trainloader, testloader, epochs=20, lr=3e-4):\n",
    "    \"\"\"Train a model and return metrics.\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.05)\n",
    "\n",
    "    history = {'train_loss': [], 'test_acc': [], 'epoch_time': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        train_loss = running_loss / len(trainloader)\n",
    "        train_acc = 100. * correct / total\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                _, predicted = model(images).max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        test_acc = 100. * correct / total\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'  Epoch {epoch+1}/{epochs}: Loss={train_loss:.3f}, '\n",
    "                  f'Test={test_acc:.1f}%, Time={epoch_time:.1f}s')\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "print(f\"Training: {len(trainset)}, Test: {len(testset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Predict Which Architecture Wins\n",
    "\n",
    "Before running the experiment, write down your predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in your predictions before running the training!\n",
    "predictions = {\n",
    "    \"which_has_higher_accuracy\": \"___\",  # \"CNN\" or \"ViT\"\n",
    "    \"which_trains_faster\": \"___\",         # \"CNN\" or \"ViT\"\n",
    "    \"accuracy_gap_percent\": \"___\",        # e.g., \"5\" for a 5% gap\n",
    "    \"reasoning\": \"___\"\n",
    "}\n",
    "\n",
    "# Print your predictions\n",
    "for key, value in predictions.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"\\nNow let us see if you are right!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN\n",
    "print(\"=\" * 50)\n",
    "print(\"Training CNN...\")\n",
    "print(\"=\" * 50)\n",
    "cnn = SimpleCNN().to(device)\n",
    "cnn_history = train_model(cnn, trainloader, testloader, epochs=20)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Training ViT...\")\n",
    "print(\"=\" * 50)\n",
    "vit = SimpleViT().to(device)\n",
    "vit_history = train_model(vit, trainloader, testloader, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization checkpoint 1: Side-by-side comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(cnn_history['train_loss'], 'b-', label='CNN', linewidth=2)\n",
    "axes[0].plot(vit_history['train_loss'], 'r-', label='ViT', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss'); axes[0].legend(); axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(cnn_history['test_acc'], 'b-', label='CNN', linewidth=2)\n",
    "axes[1].plot(vit_history['test_acc'], 'r-', label='ViT', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Test Accuracy (%)')\n",
    "axes[1].set_title('Test Accuracy'); axes[1].legend(); axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Time per epoch\n",
    "axes[2].bar(['CNN', 'ViT'],\n",
    "            [np.mean(cnn_history['epoch_time']), np.mean(vit_history['epoch_time'])],\n",
    "            color=['#4a90d9', '#e74c3c'])\n",
    "axes[2].set_ylabel('Seconds per Epoch')\n",
    "axes[2].set_title('Training Speed')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  CNN: {max(cnn_history['test_acc']):.1f}% accuracy, \"\n",
    "      f\"{np.mean(cnn_history['epoch_time']):.1f}s/epoch\")\n",
    "print(f\"  ViT: {max(vit_history['test_acc']):.1f}% accuracy, \"\n",
    "      f\"{np.mean(vit_history['epoch_time']):.1f}s/epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization checkpoint 2: Data efficiency experiment\n",
    "# Train both models on subsets of increasing size\n",
    "\n",
    "subset_sizes = [1000, 5000, 10000, 25000, 50000]\n",
    "cnn_accs_by_size = []\n",
    "vit_accs_by_size = []\n",
    "\n",
    "print(\"Data Efficiency Experiment\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for size in subset_sizes:\n",
    "    # Create subset\n",
    "    indices = torch.randperm(len(trainset))[:size]\n",
    "    subset = torch.utils.data.Subset(trainset, indices)\n",
    "    subloader = torch.utils.data.DataLoader(subset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "    # Train CNN\n",
    "    cnn_sub = SimpleCNN().to(device)\n",
    "    h = train_model(cnn_sub, subloader, testloader, epochs=10)\n",
    "    cnn_accs_by_size.append(max(h['test_acc']))\n",
    "\n",
    "    # Train ViT\n",
    "    vit_sub = SimpleViT().to(device)\n",
    "    h = train_model(vit_sub, subloader, testloader, epochs=10)\n",
    "    vit_accs_by_size.append(max(h['test_acc']))\n",
    "\n",
    "    print(f\"  {size:,} samples: CNN={cnn_accs_by_size[-1]:.1f}%, \"\n",
    "          f\"ViT={vit_accs_by_size[-1]:.1f}%\")\n",
    "\n",
    "# Plot data efficiency\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(subset_sizes, cnn_accs_by_size, 'bo-', label='CNN', linewidth=2, markersize=8)\n",
    "plt.plot(subset_sizes, vit_accs_by_size, 'rs-', label='ViT', linewidth=2, markersize=8)\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.title('Data Efficiency: CNN vs. ViT')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: CNNs perform better with small datasets due to inductive biases.\")\n",
    "print(\"ViTs catch up and eventually surpass CNNs as data increases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Analyze the Results\n",
    "\n",
    "Answer these questions based on the experiments above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in your analysis after running the experiments\n",
    "analysis = {\n",
    "    \"which_model_won_with_full_data\": \"___\",\n",
    "    \"which_model_won_with_1000_samples\": \"___\",\n",
    "    \"at_what_dataset_size_did_vit_catch_up\": \"___\",\n",
    "    \"why_does_cnn_perform_better_with_small_data\": \"___\",\n",
    "    \"main_takeaway\": \"___\"\n",
    "}\n",
    "\n",
    "for key, value in analysis.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output\n",
    "\n",
    "Let us create a comprehensive summary visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization checkpoint 3: Comprehensive comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('CNN vs. Vision Transformer: Complete Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Parameter comparison\n",
    "cnn_p = sum(p.numel() for p in SimpleCNN().parameters())\n",
    "vit_p = sum(p.numel() for p in SimpleViT().parameters())\n",
    "axes[0, 0].bar(['CNN', 'ViT'], [cnn_p/1e6, vit_p/1e6], color=['#4a90d9', '#e74c3c'])\n",
    "axes[0, 0].set_ylabel('Parameters (Millions)')\n",
    "axes[0, 0].set_title('Model Size')\n",
    "\n",
    "# 2. Accuracy comparison\n",
    "axes[0, 1].bar(['CNN', 'ViT'],\n",
    "               [max(cnn_history['test_acc']), max(vit_history['test_acc'])],\n",
    "               color=['#4a90d9', '#e74c3c'])\n",
    "axes[0, 1].set_ylabel('Test Accuracy (%)')\n",
    "axes[0, 1].set_title('Best Accuracy (Full Dataset)')\n",
    "axes[0, 1].set_ylim(60, 95)\n",
    "\n",
    "# 3. Training speed\n",
    "axes[1, 0].bar(['CNN', 'ViT'],\n",
    "               [np.mean(cnn_history['epoch_time']),\n",
    "                np.mean(vit_history['epoch_time'])],\n",
    "               color=['#4a90d9', '#e74c3c'])\n",
    "axes[1, 0].set_ylabel('Seconds per Epoch')\n",
    "axes[1, 0].set_title('Training Speed')\n",
    "\n",
    "# 4. Data efficiency\n",
    "axes[1, 1].plot(subset_sizes, cnn_accs_by_size, 'bo-', label='CNN', linewidth=2)\n",
    "axes[1, 1].plot(subset_sizes, vit_accs_by_size, 'rs-', label='ViT', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Dataset Size')\n",
    "axes[1, 1].set_ylabel('Accuracy (%)')\n",
    "axes[1, 1].set_title('Data Efficiency')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].set_xscale('log')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"  CNN: {cnn_p:,} params, {max(cnn_history['test_acc']):.1f}% accuracy, \"\n",
    "      f\"{np.mean(cnn_history['epoch_time']):.1f}s/epoch\")\n",
    "print(f\"  ViT: {vit_p:,} params, {max(vit_history['test_acc']):.1f}% accuracy, \"\n",
    "      f\"{np.mean(vit_history['epoch_time']):.1f}s/epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**What we learned:**\n",
    "- CNNs and ViTs have different strengths that depend on the data regime\n",
    "- With limited data, CNN's inductive biases give it a significant advantage\n",
    "- With more data, ViT's flexibility allows it to learn richer representations\n",
    "- Hybrid approaches that combine both are often the best practical choice\n",
    "\n",
    "**Reflection questions:**\n",
    "- If you were building a production image classifier with only 5,000 labeled images, which architecture would you choose and why?\n",
    "- The original ViT paper needed 300M images to beat CNNs on ImageNet. DeiT later achieved competitive results with just ImageNet (1.2M). What changed?\n",
    "- Modern architectures like ConvNeXt bring ViT design principles back to CNNs. Does this blur the distinction between the two paradigms?\n",
    "\n",
    "**Next:** In the next notebook, we will dive deeper into attention visualization and explore what Vision Transformers actually learn about images."
   ]
  }
 ]
}