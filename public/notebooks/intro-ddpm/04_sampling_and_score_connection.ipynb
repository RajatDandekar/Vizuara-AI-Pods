{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Sampling, DDIM, and Score Connection â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"17rFuCNZUUY1xHrMq1WTamV-JWh_IDZe8\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/04_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_02_motivation_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Motivation Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_02_motivation_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling, DDIM, and Score Connection â€” Vizuara\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "We have built and trained a complete DDPM. But there are two critical topics we have not explored yet. First, DDPM sampling requires 1000 neural network evaluations per image â€” can we do it faster? Second, how does noise prediction connect to the broader world of score-based generative models?\n",
    "\n",
    "In this notebook, we will:\n",
    "- Analyze the DDPM sampling algorithm in detail\n",
    "- Implement DDIM (Denoising Diffusion Implicit Models) for 10-50x faster sampling\n",
    "- Derive the connection between noise prediction and score functions\n",
    "- Implement Langevin dynamics sampling to see the score connection in action\n",
    "- Compare DDPM vs DDIM vs Langevin sampling quality\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "Imagine you are a sculptor. DDPM sampling is like carving a statue from a block of marble using 1000 tiny chisel taps â€” each tap removes a tiny bit of material. The result is beautiful, but it takes forever.\n",
    "\n",
    "DDIM is like using bigger chisel strokes â€” you take 50 well-placed cuts instead of 1000 tiny ones. The result is almost as good, but 20x faster.\n",
    "\n",
    "Score-based sampling is a different technique entirely â€” instead of chiseling, you use magnets. You place magnets at the locations where the statue should be, and the marble dust naturally arranges itself into the right shape. Remarkably, this is mathematically equivalent to what DDPM is doing."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_03_lib_params_setup",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Lib Params Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_03_lib_params_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision matplotlib numpy -q"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Diffusion parameters\n",
    "T = 1000\n",
    "betas = torch.linspace(1e-4, 0.02, T).to(device)\n",
    "alphas = (1.0 - betas).to(device)\n",
    "alpha_bars = torch.cumprod(alphas, dim=0).to(device)"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_04_unet_model_def",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Unet Model Def\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_04_unet_model_def.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained model (or use the one from Notebook 3)\n",
    "# For this notebook, we provide a minimal U-Net and pretrain it quickly\n",
    "\n",
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, t):\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t[:, None].float() * emb[None, :]\n",
    "        return torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.time_mlp = nn.Linear(time_dim, out_ch)\n",
    "        self.norm1 = nn.GroupNorm(8, out_ch)\n",
    "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
    "        self.shortcut = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "    def forward(self, x, t_emb):\n",
    "        h = self.norm1(F.relu(self.conv1(x)))\n",
    "        h = h + F.relu(self.time_mlp(t_emb))[:, :, None, None]\n",
    "        h = self.norm2(F.relu(self.conv2(h)))\n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, base_ch=64, time_dim=256):\n",
    "        super().__init__()\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalTimeEmbedding(base_ch), nn.Linear(base_ch, time_dim),\n",
    "            nn.ReLU(), nn.Linear(time_dim, time_dim))\n",
    "        self.enc1 = ResBlock(in_ch, base_ch, time_dim)\n",
    "        self.enc2 = ResBlock(base_ch, base_ch*2, time_dim)\n",
    "        self.enc3 = ResBlock(base_ch*2, base_ch*4, time_dim)\n",
    "        self.down1 = nn.Conv2d(base_ch, base_ch, 4, 2, 1)\n",
    "        self.down2 = nn.Conv2d(base_ch*2, base_ch*2, 4, 2, 1)\n",
    "        self.down3 = nn.Conv2d(base_ch*4, base_ch*4, 4, 2, 1)\n",
    "        self.bottleneck = ResBlock(base_ch*4, base_ch*4, time_dim)\n",
    "        self.up3 = nn.ConvTranspose2d(base_ch*4, base_ch*4, 4, 2, 1)\n",
    "        self.up2 = nn.ConvTranspose2d(base_ch*2, base_ch*2, 4, 2, 1)\n",
    "        self.up1 = nn.ConvTranspose2d(base_ch, base_ch, 4, 2, 1)\n",
    "        self.dec3 = ResBlock(base_ch*8, base_ch*2, time_dim)\n",
    "        self.dec2 = ResBlock(base_ch*4, base_ch, time_dim)\n",
    "        self.dec1 = ResBlock(base_ch*2, base_ch, time_dim)\n",
    "        self.final = nn.Conv2d(base_ch, in_ch, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_embed(t)\n",
    "        e1 = self.enc1(x, t_emb)\n",
    "        e2 = self.enc2(self.down1(e1), t_emb)\n",
    "        e3 = self.enc3(self.down2(e2), t_emb)\n",
    "        b = self.bottleneck(self.down3(e3), t_emb)\n",
    "        d3 = self.dec3(torch.cat([self.up3(b), e3], 1), t_emb)\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], 1), t_emb)\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], 1), t_emb)\n",
    "        return self.final(d1)\n",
    "\n",
    "model = UNet().to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_05_math_ddpm_review",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Math Ddpm Review\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_05_math_ddpm_review.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_06_math_ddim_sampling",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Math Ddim Sampling\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_06_math_ddim_sampling.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_07_math_score_connection",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Math Score Connection\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_07_math_score_connection.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### DDPM Sampling (Review)\n",
    "\n",
    "The DDPM sampling rule at each step is:\n",
    "\n",
    "$$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t)\\right) + \\sigma_t z$$\n",
    "\n",
    "where $z \\sim \\mathcal{N}(0, I)$ and $\\sigma_t = \\sqrt{\\beta_t}$.\n",
    "\n",
    "Let us plug in numbers at $t=500$: $\\alpha_t=0.99$, $\\beta_t=0.01$, $\\bar{\\alpha}_t=0.5$.\n",
    "If $x_t = 0.4$ and $\\epsilon_\\theta = 0.3$, $z = 0.1$:\n",
    "- $x_{t-1} = \\frac{1}{0.995}(0.4 - \\frac{0.01}{0.707} \\times 0.3) + 0.1 \\times 0.1 = 1.005 \\times 0.396 + 0.01 = 0.408$\n",
    "\n",
    "### DDIM Sampling\n",
    "\n",
    "DDIM removes the stochastic noise term, making sampling **deterministic**:\n",
    "\n",
    "$$x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}\\left(\\frac{x_t - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_\\theta}{\\sqrt{\\bar{\\alpha}_t}}\\right) + \\sqrt{1-\\bar{\\alpha}_{t-1}} \\cdot \\epsilon_\\theta$$\n",
    "\n",
    "The key insight: DDIM can skip timesteps! Instead of sampling at every $t \\in \\{1000, 999, ..., 1\\}$, we can use a subsequence like $\\{1000, 980, 960, ..., 20\\}$ (50 steps instead of 1000).\n",
    "\n",
    "### The Score Connection\n",
    "\n",
    "The noise prediction and score function are related by:\n",
    "\n",
    "$$\\epsilon_\\theta(x_t, t) = -\\sqrt{1-\\bar{\\alpha}_t} \\cdot \\nabla_{x_t} \\log q(x_t)$$\n",
    "\n",
    "This means predicting noise is equivalent to estimating the score (gradient of log-probability). The DDPM sampling update is actually discretized Langevin dynamics!\n",
    "\n",
    "## 4. Let's Build It â€” Component by Component\n",
    "\n",
    "### Step 1: DDPM Sampler"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_08_ddpm_sampler_impl",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Ddpm Sampler Impl\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_08_ddpm_sampler_impl.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_ddpm(model, n_samples=16, img_shape=(1, 32, 32)):\n",
    "    \"\"\"Standard DDPM sampling: 1000 steps.\"\"\"\n",
    "    model.eval()\n",
    "    x = torch.randn(n_samples, *img_shape, device=device)\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        t_batch = torch.full((n_samples,), t, device=device, dtype=torch.long)\n",
    "        eps = model(x, t_batch)\n",
    "\n",
    "        alpha_t = alphas[t]\n",
    "        alpha_bar_t = alpha_bars[t]\n",
    "        beta_t = betas[t]\n",
    "\n",
    "        x = (1 / torch.sqrt(alpha_t)) * (x - beta_t / torch.sqrt(1 - alpha_bar_t) * eps)\n",
    "        if t > 0:\n",
    "            x += torch.sqrt(beta_t) * torch.randn_like(x)\n",
    "\n",
    "    return x"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_09_ddim_sampler_impl",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Ddim Sampler Impl\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_09_ddim_sampler_impl.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: DDIM Sampler"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_ddim(model, n_samples=16, img_shape=(1, 32, 32), num_steps=50):\n",
    "    \"\"\"\n",
    "    DDIM sampling: deterministic, with configurable number of steps.\n",
    "    Much faster than DDPM while maintaining quality.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    x = torch.randn(n_samples, *img_shape, device=device)\n",
    "\n",
    "    # Create a subsequence of timesteps\n",
    "    step_size = T // num_steps\n",
    "    timesteps = list(range(0, T, step_size))\n",
    "    timesteps = list(reversed(timesteps))\n",
    "\n",
    "    for i in range(len(timesteps) - 1):\n",
    "        t = timesteps[i]\n",
    "        t_prev = timesteps[i + 1]\n",
    "\n",
    "        t_batch = torch.full((n_samples,), t, device=device, dtype=torch.long)\n",
    "        eps = model(x, t_batch)\n",
    "\n",
    "        alpha_bar_t = alpha_bars[t]\n",
    "        alpha_bar_t_prev = alpha_bars[t_prev]\n",
    "\n",
    "        # DDIM update (deterministic â€” no random noise added)\n",
    "        x0_pred = (x - torch.sqrt(1 - alpha_bar_t) * eps) / torch.sqrt(alpha_bar_t)\n",
    "        x = (torch.sqrt(alpha_bar_t_prev) * x0_pred\n",
    "             + torch.sqrt(1 - alpha_bar_t_prev) * eps)\n",
    "\n",
    "    return x"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_10_quick_training",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Quick Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_10_quick_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick training (5 epochs for demonstration)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "print(\"Quick training (5 epochs)...\")\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (images, _) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        t = torch.randint(0, T, (images.shape[0],), device=device)\n",
    "        noise = torch.randn_like(images)\n",
    "        sqrt_ab = torch.sqrt(alpha_bars[t]).view(-1,1,1,1)\n",
    "        sqrt_1_ab = torch.sqrt(1-alpha_bars[t]).view(-1,1,1,1)\n",
    "        x_t = sqrt_ab * images + sqrt_1_ab * noise\n",
    "        pred = model(x_t, t)\n",
    "        loss = F.mse_loss(pred, noise)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"  Epoch {epoch+1}: Loss = {total_loss / len(dataloader):.4f}\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_11_todo1_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Narration: Todo1 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_11_todo1_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** The loss should decrease over epochs, indicating the model is learning to predict noise.\n",
    "\n",
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Implement Eta-Controlled DDIM\n",
    "\n",
    "DDIM has a parameter $\\eta$ that interpolates between deterministic ($\\eta=0$) and stochastic ($\\eta=1$, equivalent to DDPM) sampling."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_12_todo1_impl",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo1 Impl\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_12_todo1_impl.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_ddim_eta(model, n_samples=16, img_shape=(1, 32, 32), num_steps=50, eta=0.0):\n",
    "    \"\"\"\n",
    "    TODO: Implement DDIM with controllable stochasticity via eta.\n",
    "\n",
    "    When eta=0: deterministic (standard DDIM)\n",
    "    When eta=1: stochastic (equivalent to DDPM)\n",
    "\n",
    "    The update rule with eta is:\n",
    "    sigma_t = eta * sqrt((1-alpha_bar_{t-1})/(1-alpha_bar_t)) * sqrt(beta_t)\n",
    "    x_{t-1} = sqrt(alpha_bar_{t-1}) * x0_pred\n",
    "            + sqrt(1 - alpha_bar_{t-1} - sigma_t^2) * eps_pred\n",
    "            + sigma_t * z  (where z ~ N(0,I))\n",
    "\n",
    "    Returns: generated samples\n",
    "    \"\"\"\n",
    "    # HINT: Start with the DDIM code above and add the sigma_t term\n",
    "\n",
    "    pass  # YOUR CODE HERE"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_13_todo2_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Narration: Todo2 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_13_todo2_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Compare Sampling Speed and Quality"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_14_todo2_impl",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo2 Impl\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_14_todo2_impl.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_samplers(model, num_ddim_steps_list=[10, 25, 50, 100, 200]):\n",
    "    \"\"\"\n",
    "    TODO: Compare DDPM vs DDIM at different step counts.\n",
    "\n",
    "    1. For each method, time the sampling process\n",
    "    2. Generate 16 samples with each method\n",
    "    3. Display results in a grid with timing information\n",
    "    4. Discuss: at what point does DDIM quality match DDPM?\n",
    "\n",
    "    Expected: DDIM with 50 steps should produce quality close to DDPM with 1000.\n",
    "    \"\"\"\n",
    "    # HINT: Use time.time() to measure wall-clock time\n",
    "\n",
    "    pass  # YOUR CODE HERE"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_15_speed_comparison_run",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Speed Comparison Run\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_15_speed_comparison_run.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "### Speed Comparison"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare DDPM (1000 steps) vs DDIM (50 steps)\n",
    "print(\"Sampling with DDPM (1000 steps)...\")\n",
    "start = time.time()\n",
    "ddpm_samples = sample_ddpm(model, n_samples=8)\n",
    "ddpm_time = time.time() - start\n",
    "print(f\"  DDPM time: {ddpm_time:.1f}s\")\n",
    "\n",
    "print(\"Sampling with DDIM (50 steps)...\")\n",
    "start = time.time()\n",
    "ddim_samples = sample_ddim(model, n_samples=8, num_steps=50)\n",
    "ddim_time = time.time() - start\n",
    "print(f\"  DDIM time: {ddim_time:.1f}s\")\n",
    "\n",
    "print(f\"\\nSpeedup: {ddpm_time / ddim_time:.1f}x faster!\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize side by side\n",
    "fig, axes = plt.subplots(2, 8, figsize=(24, 6))\n",
    "for i in range(8):\n",
    "    axes[0][i].imshow(ddpm_samples[i].squeeze().cpu().clamp(-1,1).numpy(), cmap='gray')\n",
    "    axes[0][i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0][i].set_ylabel('DDPM\\n(1000 steps)', fontsize=12, rotation=0, labelpad=80)\n",
    "\n",
    "    axes[1][i].imshow(ddim_samples[i].squeeze().cpu().clamp(-1,1).numpy(), cmap='gray')\n",
    "    axes[1][i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1][i].set_ylabel('DDIM\\n(50 steps)', fontsize=12, rotation=0, labelpad=80)\n",
    "\n",
    "plt.suptitle(f'DDPM ({ddpm_time:.1f}s) vs DDIM ({ddim_time:.1f}s) â€” {ddpm_time/ddim_time:.0f}x Speedup', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_16_score_connection_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Score Connection Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_16_score_connection_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "### The Score Function Connection"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the connection: epsilon prediction = scaled score\n",
    "def visualize_score_connection(model, x_0, t_val=500):\n",
    "    \"\"\"\n",
    "    Show that noise prediction is equivalent to score estimation.\n",
    "\n",
    "    score = -epsilon_theta / sqrt(1 - alpha_bar_t)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    x_0 = x_0.to(device)\n",
    "    t = torch.tensor([t_val], device=device)\n",
    "\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_ab = torch.sqrt(alpha_bars[t_val])\n",
    "    sqrt_1_ab = torch.sqrt(1 - alpha_bars[t_val])\n",
    "    x_t = sqrt_ab * x_0 + sqrt_1_ab * noise\n",
    "\n",
    "    with torch.no_grad():\n",
    "        eps_pred = model(x_t, t)\n",
    "\n",
    "    # Score = -eps / sqrt(1 - alpha_bar_t)\n",
    "    score = -eps_pred / sqrt_1_ab\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    axes[0].imshow(x_0.squeeze().cpu().numpy(), cmap='gray')\n",
    "    axes[0].set_title('Original xâ‚€')\n",
    "    axes[1].imshow(x_t.squeeze().cpu().clamp(-1,1).numpy(), cmap='gray')\n",
    "    axes[1].set_title(f'Noisy x_t (t={t_val})')\n",
    "    axes[2].imshow(eps_pred.squeeze().cpu().numpy(), cmap='RdBu')\n",
    "    axes[2].set_title('Predicted Noise ÎµÌ‚')\n",
    "    axes[3].imshow(score.squeeze().cpu().numpy(), cmap='RdBu')\n",
    "    axes[3].set_title('Score âˆ‡log q(x_t)\\n= -ÎµÌ‚ / âˆš(1-á¾±_t)')\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "    plt.suptitle('Noise Prediction â†” Score Function', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "sample = dataset[0][0].unsqueeze(0)\n",
    "visualize_score_connection(model, sample, t_val=300)"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_17_ddim_steps_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Ddim Steps Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_17_ddim_steps_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DDIM at different step counts\n",
    "fig, axes = plt.subplots(4, 8, figsize=(24, 12))\n",
    "for row, num_steps in enumerate([10, 25, 50, 200]):\n",
    "    samples = sample_ddim(model, n_samples=8, num_steps=num_steps)\n",
    "    for col in range(8):\n",
    "        axes[row][col].imshow(samples[col].squeeze().cpu().clamp(-1,1).numpy(), cmap='gray')\n",
    "        axes[row][col].axis('off')\n",
    "    axes[row][0].set_ylabel(f'{num_steps} steps', fontsize=12, rotation=0, labelpad=60)\n",
    "\n",
    "plt.suptitle('DDIM Sampling Quality at Different Step Counts', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_18_final_output",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Final Output\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_18_final_output.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the best samples we can\n",
    "print(\"Generating final showcase samples...\")\n",
    "final_ddpm = sample_ddpm(model, n_samples=16)\n",
    "final_ddim = sample_ddim(model, n_samples=16, num_steps=50)\n",
    "\n",
    "fig, axes = plt.subplots(2, 16, figsize=(32, 4))\n",
    "for i in range(16):\n",
    "    axes[0][i].imshow(final_ddpm[i].squeeze().cpu().clamp(-1,1).numpy(), cmap='gray')\n",
    "    axes[0][i].axis('off')\n",
    "    axes[1][i].imshow(final_ddim[i].squeeze().cpu().clamp(-1,1).numpy(), cmap='gray')\n",
    "    axes[1][i].axis('off')\n",
    "axes[0][0].set_ylabel('DDPM', fontsize=12, rotation=0, labelpad=40)\n",
    "axes[1][0].set_ylabel('DDIM', fontsize=12, rotation=0, labelpad=40)\n",
    "plt.suptitle('Final Generated Samples: DDPM (1000 steps) vs DDIM (50 steps)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('ddpm_vs_ddim_final.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"DDPM: 1000 steps â€” highest quality, slower\")\n",
    "print(f\"DDIM: 50 steps â€” nearly identical quality, 20x faster\")\n",
    "print(f\"Score Connection: Îµ_Î¸(x_t, t) = -âˆš(1-á¾±_t) Â· âˆ‡log q(x_t)\")\n",
    "print(\"Diffusion models and score-based models are two sides of the same coin!\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_19_closing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Wrap-Up: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_19_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Key Takeaways\n",
    "1. DDPM sampling requires T=1000 steps â€” each step is one neural network forward pass\n",
    "2. DDIM enables deterministic sampling in far fewer steps (50-100) with minimal quality loss\n",
    "3. Noise prediction is mathematically equivalent to score function estimation\n",
    "4. This unification reveals that DDPMs are performing discretized Langevin dynamics\n",
    "\n",
    "### Reflection Questions\n",
    "- Why is deterministic (DDIM) sampling useful beyond just speed? Think about image editing applications.\n",
    "- The score function points toward high-probability regions. How does this help generate realistic images?\n",
    "- How would you extend DDPM to generate images conditioned on text descriptions (like DALL-E)?\n",
    "- What limits current diffusion models â€” is it the architecture, the noise schedule, or the sampling method?\n",
    "\n",
    "### What Comes Next\n",
    "The DDPM framework we built here is the foundation for modern image generation systems:\n",
    "- **DALL-E 2:** DDPM + CLIP text embeddings for text-to-image generation\n",
    "- **Stable Diffusion:** DDPM in the latent space of a VAE for efficiency\n",
    "- **Imagen:** Cascaded DDPMs at increasing resolutions for high-fidelity output\n",
    "\n",
    "The core principle remains the same: learn to predict noise, then iteratively denoise."
   ],
   "id": "cell_22"
  }
 ]
}