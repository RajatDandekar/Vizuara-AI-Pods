{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "The Reverse Process and DDPM Loss â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1RJjttCvltRK-j5XaI_Tp752cibGKRYMf\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/02_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_02_why_reverse_matters",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why Reverse Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_02_why_reverse_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_03_building_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Building Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_03_building_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Reverse Process and DDPM Loss â€” Vizuara\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebook, we learned how to systematically destroy images with the forward diffusion process. Now comes the exciting part â€” learning to **reverse** this process. If we can undo each tiny step of noise addition, we can start from pure random noise and generate entirely new images.\n",
    "\n",
    "The DDPM paper by Ho et al. (2020) showed that this reverse process can be trained with a surprisingly simple loss function: just predict the noise that was added. This single insight transformed diffusion models from a theoretical curiosity into a practical powerhouse.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand why the reverse posterior is tractable when conditioned on $x_0$\n",
    "- Derive the DDPM loss from the variational lower bound\n",
    "- Implement the noise prediction loss\n",
    "- Train a simple denoiser and verify it learns to predict noise\n",
    "- See the connection between noise prediction and score matching\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "Let us think about what it means to denoise an image. Imagine you have a photograph of a cat with some static noise overlay. A human can easily recognize the cat and mentally \"remove\" the noise. But how would a neural network do it?\n",
    "\n",
    "The key insight is this: if we know exactly what noise was added, we can subtract it to recover the clean image. So instead of learning to directly produce the clean image, we train the network to predict the noise.\n",
    "\n",
    "Let us set up our environment and build on the forward process from Notebook 1."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision matplotlib numpy -q"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Recreate the forward diffusion from Notebook 1\n",
    "T = 1000\n",
    "betas = torch.linspace(1e-4, 0.02, T)\n",
    "alphas = 1.0 - betas\n",
    "alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "def forward_diffusion(x_0, t, noise=None):\n",
    "    \"\"\"Add noise to x_0 at timestep t.\"\"\"\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_0)\n",
    "    sqrt_ab = torch.sqrt(alpha_bars[t]).view(-1, 1, 1, 1)\n",
    "    sqrt_1_ab = torch.sqrt(1 - alpha_bars[t]).view(-1, 1, 1, 1)\n",
    "    x_t = sqrt_ab * x_0 + sqrt_1_ab * noise\n",
    "    return x_t, noise\n",
    "\n",
    "# Load MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_04_true_posterior_math",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: True Posterior Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_04_true_posterior_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_05_ddpm_loss_math",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Ddpm Loss Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_05_ddpm_loss_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### The True Posterior\n",
    "\n",
    "We want to learn $p(x_{t-1} | x_t)$ â€” given a noisy image, produce a slightly less noisy version. Computing this directly is intractable. But if we also know the original image $x_0$, the posterior becomes a simple Gaussian:\n",
    "\n",
    "$$q(x_{t-1} | x_t, x_0) = \\mathcal{N}(x_{t-1}; \\tilde{\\mu}_t, \\tilde{\\beta}_t I)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\\tilde{\\mu}_t = \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t}{1 - \\bar{\\alpha}_t} x_0 + \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} x_t$$\n",
    "\n",
    "Let us plug in numbers. At $t=500$: $\\alpha_t = 0.99$, $\\bar{\\alpha}_t = 0.5$, $\\bar{\\alpha}_{t-1} = 0.505$, $\\beta_t = 0.01$.\n",
    "- Coefficient of $x_0$: $\\frac{\\sqrt{0.505} \\times 0.01}{0.5} = \\frac{0.711 \\times 0.01}{0.5} = 0.0142$\n",
    "- Coefficient of $x_t$: $\\frac{\\sqrt{0.99} \\times 0.495}{0.5} = \\frac{0.995 \\times 0.495}{0.5} = 0.985$\n",
    "\n",
    "So $\\tilde{\\mu}_t = 0.0142 \\cdot x_0 + 0.985 \\cdot x_t$. The posterior mean mostly relies on $x_t$ â€” which makes sense because $x_t$ already contains most of the useful information at intermediate noise levels.\n",
    "\n",
    "### From Means to Noise Prediction\n",
    "\n",
    "Since we cannot access $x_0$ during generation, we reparameterize. Using $x_0 = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}}(x_t - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon)$, the loss simplifies to:\n",
    "\n",
    "$$L_{\\text{simple}} = \\mathbb{E}_{t, x_0, \\epsilon}\\left[\\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\right]$$\n",
    "\n",
    "This is the DDPM loss. It says: predict the noise that was added, and minimize the squared error. That is it!\n",
    "\n",
    "Let us verify with numbers. If true noise $\\epsilon = 0.3$ and predicted $\\epsilon_\\theta = 0.25$:\n",
    "- Loss = $(0.3 - 0.25)^2 = 0.0025$\n",
    "- Perfect prediction: loss = 0\n",
    "\n",
    "## 4. Let's Build It â€” Component by Component\n",
    "\n",
    "### Step 1: Compute the True Posterior"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_06_code_posterior_mean",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Code Posterior Mean\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_06_code_posterior_mean.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_posterior_mean(x_0, x_t, t):\n",
    "    \"\"\"\n",
    "    Compute the true posterior mean mu_tilde.\n",
    "\n",
    "    mu_tilde = (sqrt(alpha_bar_{t-1}) * beta_t) / (1 - alpha_bar_t) * x_0\n",
    "             + (sqrt(alpha_t) * (1 - alpha_bar_{t-1})) / (1 - alpha_bar_t) * x_t\n",
    "    \"\"\"\n",
    "    alpha_bar_t = alpha_bars[t].view(-1, 1, 1, 1)\n",
    "    alpha_bar_t_prev = alpha_bars[t - 1].view(-1, 1, 1, 1) if t.min() > 0 else torch.ones_like(alpha_bar_t)\n",
    "    alpha_t = alphas[t].view(-1, 1, 1, 1)\n",
    "    beta_t = betas[t].view(-1, 1, 1, 1)\n",
    "\n",
    "    coeff_x0 = torch.sqrt(alpha_bar_t_prev) * beta_t / (1 - alpha_bar_t)\n",
    "    coeff_xt = torch.sqrt(alpha_t) * (1 - alpha_bar_t_prev) / (1 - alpha_bar_t)\n",
    "\n",
    "    return coeff_x0 * x_0 + coeff_xt * x_t"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_07_visualize_posterior",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Visualize Posterior\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_07_visualize_posterior.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Visualize the True Posterior in Action"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that if we KNOW x_0, we can compute the reverse step\n",
    "sample = dataset[0][0].unsqueeze(0)  # [1, 1, 28, 28]\n",
    "\n",
    "# Go to t=500\n",
    "t_val = torch.tensor([500])\n",
    "x_500, noise = forward_diffusion(sample, t_val)\n",
    "\n",
    "# Compute posterior mean (using known x_0)\n",
    "mu_tilde = compute_posterior_mean(sample, x_500, t_val)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "axes[0].imshow(sample.squeeze().numpy(), cmap='gray')\n",
    "axes[0].set_title('Original xâ‚€')\n",
    "axes[1].imshow(x_500.squeeze().clamp(-1, 1).detach().numpy(), cmap='gray')\n",
    "axes[1].set_title('Noisy xâ‚…â‚€â‚€')\n",
    "axes[2].imshow(mu_tilde.squeeze().clamp(-1, 1).detach().numpy(), cmap='gray')\n",
    "axes[2].set_title('Posterior Mean Î¼Ìƒâ‚…â‚€â‚€\\n(using known xâ‚€)')\n",
    "axes[3].imshow(noise.squeeze().numpy(), cmap='gray')\n",
    "axes[3].set_title('True Noise Îµ')\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "plt.suptitle('True Posterior: One Step of Denoising (with known xâ‚€)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_08_simple_noise_predictor",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Simple Noise Predictor\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_08_simple_noise_predictor.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build a Simple Noise Predictor\n",
    "\n",
    "Before using a full U-Net, let us build a minimal CNN that can predict noise."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNoisePredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple CNN that predicts noise given a noisy image and timestep.\n",
    "    This is NOT a U-Net â€” just a minimal model to verify the loss works.\n",
    "    \"\"\"\n",
    "    def __init__(self, T=1000, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        # Timestep embedding\n",
    "        self.time_embed = nn.Embedding(T, hidden_dim)\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(1, hidden_dim, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1)\n",
    "\n",
    "        # Decoder\n",
    "        self.conv4 = nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(hidden_dim, 1, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Get time embedding and reshape for broadcasting\n",
    "        t_emb = self.time_embed(t)  # [B, hidden_dim]\n",
    "        t_emb = t_emb.view(-1, t_emb.shape[1], 1, 1)  # [B, hidden_dim, 1, 1]\n",
    "\n",
    "        # Forward pass\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = h + t_emb  # Add time embedding\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h = F.relu(self.conv3(h))\n",
    "        h = F.relu(self.conv4(h))\n",
    "        h = self.conv5(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "model = SimpleNoisePredictor(T=T)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement the DDPM Training Step"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_09_ddpm_train_step",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Ddpm Train Step\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_09_ddpm_train_step.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpm_train_step(model, x_0, optimizer):\n",
    "    \"\"\"One training step: sample t, add noise, predict noise, compute MSE.\"\"\"\n",
    "    batch_size = x_0.shape[0]\n",
    "\n",
    "    # 1. Sample random timesteps\n",
    "    t = torch.randint(0, T, (batch_size,))\n",
    "\n",
    "    # 2. Sample random noise\n",
    "    noise = torch.randn_like(x_0)\n",
    "\n",
    "    # 3. Create noisy images\n",
    "    x_t, _ = forward_diffusion(x_0, t, noise)\n",
    "\n",
    "    # 4. Predict noise\n",
    "    predicted_noise = model(x_t, t)\n",
    "\n",
    "    # 5. Compute MSE loss\n",
    "    loss = F.mse_loss(predicted_noise, noise)\n",
    "\n",
    "    # 6. Optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_10_checkpoint_training",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Checkpoint Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_10_checkpoint_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** This is the entire DDPM training procedure. Sample noise, add it to images, predict it, minimize MSE. Beautifully simple.\n",
    "\n",
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Train and Monitor the Loss"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_11_todo_1_train_loss",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo Train Loss\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_11_todo_1_train_loss.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_plot(model, dataloader, num_epochs=5, lr=1e-3):\n",
    "    \"\"\"\n",
    "    TODO: Train the noise predictor and plot the loss curve.\n",
    "\n",
    "    1. Create an Adam optimizer\n",
    "    2. Loop over epochs and batches\n",
    "    3. Call ddpm_train_step for each batch\n",
    "    4. Record the loss every 50 steps\n",
    "    5. Plot the loss curve\n",
    "    6. Print the final loss\n",
    "\n",
    "    Expected: Loss should decrease from ~1.0 to ~0.3-0.5\n",
    "    \"\"\"\n",
    "    # HINT: The loss should steadily decrease, showing the model\n",
    "    # is learning to predict noise better over time.\n",
    "\n",
    "    pass  # YOUR CODE HERE"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_12_todo_1_after",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo After\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_12_todo_1_after.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Visualize Noise Predictions"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_13_todo_2_visualize_noise",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo Visualize Noise\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_13_todo_2_visualize_noise.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_noise_predictions(model, x_0, timesteps=[100, 300, 500, 700, 900]):\n",
    "    \"\"\"\n",
    "    TODO: For a given clean image, show the noise prediction at different timesteps.\n",
    "\n",
    "    For each timestep:\n",
    "    1. Add noise to x_0 to get x_t (using forward_diffusion)\n",
    "    2. Predict the noise using the model\n",
    "    3. Show side-by-side: x_t, true noise, predicted noise, |error|\n",
    "\n",
    "    This will reveal how well the model predicts noise at different levels.\n",
    "    \"\"\"\n",
    "    # HINT: Use model.eval() and torch.no_grad() for inference\n",
    "\n",
    "    pass  # YOUR CODE HERE"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_14_todo_2_after",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo After\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_14_todo_2_after.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_15_putting_it_all_together",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Putting It All Together\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_15_putting_it_all_together.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us train our simple model and see the noise prediction in action."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_16_training_results_loss",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Training Results Loss\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_16_training_results_loss.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for a few epochs\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    epoch_losses = []\n",
    "    for batch_idx, (images, _) in enumerate(dataloader):\n",
    "        loss = ddpm_train_step(model, images, optimizer)\n",
    "        epoch_losses.append(loss)\n",
    "        if batch_idx % 200 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss:.4f}\")\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    losses.extend(epoch_losses)\n",
    "    print(f\"Epoch {epoch+1} average loss: {avg_loss:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses[::10])\n",
    "plt.xlabel('Training Steps (x10)')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('DDPM Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_17_visualize_noise_detailed",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Visualize Noise Detailed\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_17_visualize_noise_detailed.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model's noise predictions\n",
    "model.eval()\n",
    "sample = dataset[0][0].unsqueeze(0)\n",
    "\n",
    "fig, axes = plt.subplots(4, 5, figsize=(20, 16))\n",
    "timesteps = [50, 200, 400, 600, 900]\n",
    "row_labels = ['Noisy xâ‚œ', 'True Noise Îµ', 'Predicted ÎµÌ‚', '|Error|']\n",
    "\n",
    "for col, t in enumerate(timesteps):\n",
    "    t_tensor = torch.tensor([t])\n",
    "    noise = torch.randn_like(sample)\n",
    "    x_t, _ = forward_diffusion(sample, t_tensor, noise)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_noise = model(x_t, t_tensor)\n",
    "\n",
    "    error = (noise - pred_noise).abs()\n",
    "\n",
    "    axes[0][col].imshow(x_t.squeeze().clamp(-1, 1).detach().numpy(), cmap='gray')\n",
    "    axes[0][col].set_title(f't={t}')\n",
    "    axes[1][col].imshow(noise.squeeze().numpy(), cmap='gray')\n",
    "    axes[2][col].imshow(pred_noise.squeeze().detach().numpy(), cmap='gray')\n",
    "    axes[3][col].imshow(error.squeeze().detach().numpy(), cmap='hot')\n",
    "\n",
    "for row, label in enumerate(row_labels):\n",
    "    axes[row][0].set_ylabel(label, fontsize=12, rotation=0, labelpad=80, va='center')\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Noise Prediction at Different Timesteps', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_18_final_output_reconstruction",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Final Output Reconstruction\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_18_final_output_reconstruction.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** You should see that the predicted noise pattern roughly matches the true noise, especially at lower timesteps where there is more signal. The error map should show that the model is learning!\n",
    "\n",
    "## 8. Final Output"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the key insight: noise prediction enables mean estimation\n",
    "model.eval()\n",
    "sample = dataset[5][0].unsqueeze(0)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "timesteps = [100, 300, 500, 700, 900]\n",
    "\n",
    "for col, t in enumerate(timesteps):\n",
    "    t_tensor = torch.tensor([t])\n",
    "    noise = torch.randn_like(sample)\n",
    "    x_t, _ = forward_diffusion(sample, t_tensor, noise)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_noise = model(x_t, t_tensor)\n",
    "\n",
    "    # Reconstruct x_0 from predicted noise\n",
    "    sqrt_ab = torch.sqrt(alpha_bars[t])\n",
    "    sqrt_1_ab = torch.sqrt(1 - alpha_bars[t])\n",
    "    x_0_pred = (x_t - sqrt_1_ab * pred_noise) / sqrt_ab\n",
    "\n",
    "    axes[0][col].imshow(x_t.squeeze().clamp(-1, 1).detach().numpy(), cmap='gray')\n",
    "    axes[0][col].set_title(f'Noisy x_t (t={t})')\n",
    "    axes[1][col].imshow(x_0_pred.squeeze().clamp(-1, 1).detach().numpy(), cmap='gray')\n",
    "    axes[1][col].set_title(f'Predicted xâ‚€')\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('From Noise Prediction to Image Reconstruction', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\nBy predicting the noise, we can estimate the original clean image from any noise level!\")\n",
    "print(\"This is the core insight of DDPM â€” training a generative model reduces to denoising.\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_19_closing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Wrap-Up: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_19_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Key Takeaways\n",
    "1. The true reverse posterior $q(x_{t-1}|x_t, x_0)$ is a Gaussian with known mean and variance\n",
    "2. The DDPM loss simplifies to predicting the added noise: $\\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2$\n",
    "3. Noise prediction is equivalent to estimating the score function $\\nabla \\log p(x_t)$\n",
    "4. A simple CNN can already learn basic noise prediction â€” U-Nets make it much better\n",
    "\n",
    "### Reflection Questions\n",
    "- Why does predicting noise work better than directly predicting $x_0$?\n",
    "- At which timesteps do you think noise prediction is hardest? Why?\n",
    "- How does the loss function relate to the variational lower bound (ELBO)?\n",
    "\n",
    "### What is Next\n",
    "In the next notebook, we will build a proper U-Net architecture with timestep embeddings and train a full DDPM that can actually generate convincing images from pure noise."
   ],
   "id": "cell_22"
  }
 ]
}