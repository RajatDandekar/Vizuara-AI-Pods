{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Forward Diffusion Process ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1K3QJmxvgc0_ZwXawSeV_oQll-6WeibaO\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/01_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_02_why_matter_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why Matter Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_02_why_matter_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Diffusion Process ‚Äî Vizuara\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "Generative models have transformed AI, but how do we train a model to create images from pure noise? The key insight of diffusion models is that if we can learn how noise is systematically added to images, we can learn to reverse the process.\n",
    "\n",
    "In this notebook, we will build the **forward diffusion process** from scratch. This is the foundation of all diffusion models ‚Äî the process that takes clean images and gradually transforms them into pure Gaussian noise.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand why Gaussian noise is the right choice for corruption\n",
    "- Implement the step-by-step forward process\n",
    "- Derive and implement the closed-form \"skip to any timestep\" formula\n",
    "- Visualize how images are destroyed at different noise levels\n",
    "- Build the noise schedule that controls the rate of corruption\n",
    "\n",
    "Let us begin!\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "Let us start with a simple analogy. Imagine you have a cup of black coffee. You add a single drop of milk. The coffee looks almost the same. Add another drop ‚Äî still barely noticeable. But if you keep adding drops, eventually the coffee becomes uniformly light brown. The original \"structure\" of pure black coffee has been completely destroyed.\n",
    "\n",
    "This is exactly what happens in forward diffusion. We add tiny amounts of random noise at each step. After enough steps, the original image is completely unrecognizable ‚Äî replaced by pure random noise.\n",
    "\n",
    "Let us visualize this with actual images. First, let us set up our environment."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_03_setup_packages_data",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Narration: Setup Packages Data\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_03_setup_packages_data.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision matplotlib numpy -q"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Grab a single image\n",
    "sample_image = dataset[0][0]  # Shape: [1, 28, 28]\n",
    "print(f\"Image shape: {sample_image.shape}\")\n",
    "print(f\"Pixel range: [{sample_image.min():.2f}, {sample_image.max():.2f}]\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_04_visualize_clean_image",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Visualize Clean Image\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_04_visualize_clean_image.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at our clean image before we start adding noise."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(sample_image.squeeze(), cmap='gray')\n",
    "plt.title('Original Image (x‚ÇÄ)')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_05_math_step_by_step_equation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Math Step By Step Equation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_05_math_step_by_step_equation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "Now let us understand the mathematics behind the forward process.\n",
    "\n",
    "At each time step $t$, we take the image from step $t-1$ and add noise according to:\n",
    "\n",
    "$$q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1-\\beta_t} \\cdot \\mathbf{x}_{t-1}, \\beta_t \\mathbf{I})$$\n",
    "\n",
    "This means: sample $x_t$ from a Gaussian with:\n",
    "- **Mean:** $\\sqrt{1-\\beta_t} \\cdot x_{t-1}$ (scaled-down version of previous image)\n",
    "- **Variance:** $\\beta_t$ (amount of noise)\n",
    "\n",
    "Let us plug in numbers. If pixel value $x_{t-1} = 0.8$ and $\\beta_t = 0.01$:\n",
    "- Mean = $\\sqrt{0.99} \\times 0.8 = 0.995 \\times 0.8 = 0.796$\n",
    "- Std = $\\sqrt{0.01} = 0.1$\n",
    "- So $x_t \\sim \\mathcal{N}(0.796, 0.01)$\n",
    "\n",
    "The mean shrinks slightly, and noise is added. After many steps, the signal disappears.\n",
    "\n",
    "**The key trick:** We define $\\alpha_t = 1 - \\beta_t$ and $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$.\n",
    "\n",
    "Then we get the closed-form formula:\n",
    "\n",
    "$$\\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t} \\cdot \\mathbf{x}_0 + \\sqrt{1-\\bar{\\alpha}_t} \\cdot \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\mathbf{I})$$\n",
    "\n",
    "This is a massive shortcut ‚Äî we can jump directly from $x_0$ to any $x_t$ without iterating through all intermediate steps!\n",
    "\n",
    "Let us verify with numbers. If $\\bar{\\alpha}_t = 0.5$ and $x_0 = 0.8$, $\\epsilon = 0.3$:\n",
    "- $x_t = \\sqrt{0.5} \\times 0.8 + \\sqrt{0.5} \\times 0.3 = 0.566 + 0.212 = 0.778$\n",
    "- About 70.7% original signal + 70.7% noise. This makes sense for a mid-level noise amount.\n",
    "\n",
    "## 4. Let's Build It ‚Äî Component by Component\n",
    "\n",
    "### Step 1: Define the Noise Schedule\n",
    "\n",
    "The noise schedule determines $\\beta_t$ at each time step. We use a **linear schedule** from $\\beta_1 = 10^{-4}$ to $\\beta_T = 0.02$."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_07_noise_schedule_implementation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Noise Schedule Implementation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_07_noise_schedule_implementation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000  # Total diffusion steps\n",
    "\n",
    "# Linear noise schedule\n",
    "beta_start = 1e-4\n",
    "beta_end = 0.02\n",
    "betas = torch.linspace(beta_start, beta_end, T)\n",
    "\n",
    "# Compute alphas and cumulative alpha products\n",
    "alphas = 1.0 - betas\n",
    "alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "print(f\"beta range: [{betas[0]:.6f}, {betas[-1]:.4f}]\")\n",
    "print(f\"alpha_bar at t=0:   {alpha_bars[0]:.6f}\")\n",
    "print(f\"alpha_bar at t=250: {alpha_bars[250]:.6f}\")\n",
    "print(f\"alpha_bar at t=500: {alpha_bars[500]:.6f}\")\n",
    "print(f\"alpha_bar at t=750: {alpha_bars[750]:.6f}\")\n",
    "print(f\"alpha_bar at t=999: {alpha_bars[999]:.6f}\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_08_noise_schedule_visualization",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Noise Schedule Visualization\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_08_noise_schedule_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize how $\\bar{\\alpha}_t$ decreases over time."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(alpha_bars.numpy())\n",
    "plt.xlabel('Time step t')\n",
    "plt.ylabel('·æ±‚Çú (cumulative alpha)')\n",
    "plt.title('How Much Original Signal Remains at Each Time Step')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='50% signal')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_09_step_by_step_intro_func",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Step By Step Intro Func\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_09_step_by_step_intro_func.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** You should see $\\bar{\\alpha}_t$ starting near 1.0 (almost no noise) and decreasing towards 0 (pure noise). The curve shows that most of the signal is destroyed in the first half of the schedule.\n",
    "\n",
    "### Step 2: Implement Step-by-Step Forward Process\n",
    "\n",
    "Let us first implement the naive iterative approach, step by step."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_step(x_prev, t):\n",
    "    \"\"\"Apply one step of forward diffusion: q(x_t | x_{t-1})\"\"\"\n",
    "    beta_t = betas[t]\n",
    "    mean = torch.sqrt(1 - beta_t) * x_prev\n",
    "    noise = torch.randn_like(x_prev)\n",
    "    x_t = mean + torch.sqrt(beta_t) * noise\n",
    "    return x_t"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_10_step_by_step_visualization",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Step By Step Visualization\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_10_step_by_step_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply forward diffusion step by step\n",
    "x = sample_image.clone()\n",
    "noisy_images_stepwise = [x.clone()]\n",
    "\n",
    "for t in range(T):\n",
    "    x = forward_step(x, t)\n",
    "    if t in [0, 99, 249, 499, 749, 999]:\n",
    "        noisy_images_stepwise.append(x.clone())\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 7, figsize=(21, 3))\n",
    "titles = ['t=0', 't=1', 't=100', 't=250', 't=500', 't=750', 't=1000']\n",
    "for ax, img, title in zip(axes, noisy_images_stepwise, titles):\n",
    "    ax.imshow(img.squeeze().clamp(-1, 1).numpy(), cmap='gray')\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Step-by-Step Forward Diffusion', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_11_closed_form_intro_func",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Closed Form Intro Func\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_11_closed_form_intro_func.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement the Closed-Form Forward Process\n",
    "\n",
    "Now let us implement the efficient closed-form version."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion_closed_form(x_0, t, noise=None):\n",
    "    \"\"\"\n",
    "    Jump directly from x_0 to x_t using the closed-form formula.\n",
    "\n",
    "    x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon\n",
    "    \"\"\"\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_0)\n",
    "\n",
    "    sqrt_alpha_bar = torch.sqrt(alpha_bars[t])\n",
    "    sqrt_one_minus_alpha_bar = torch.sqrt(1 - alpha_bars[t])\n",
    "\n",
    "    x_t = sqrt_alpha_bar * x_0 + sqrt_one_minus_alpha_bar * noise\n",
    "    return x_t, noise"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_12_closed_form_visualization",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Closed Form Visualization\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_12_closed_form_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jump to different time steps directly\n",
    "timesteps = [0, 50, 100, 250, 500, 750, 999]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(timesteps), figsize=(3 * len(timesteps), 3))\n",
    "for ax, t in zip(axes, timesteps):\n",
    "    x_t, _ = forward_diffusion_closed_form(sample_image, t)\n",
    "    ax.imshow(x_t.squeeze().clamp(-1, 1).numpy(), cmap='gray')\n",
    "    ax.set_title(f't={t}\\n·æ±={alpha_bars[t]:.3f}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Closed-Form Forward Diffusion (Direct Jump)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_13_todo1_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo1 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_13_todo1_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Both methods should produce visually similar results ‚Äî the image gradually dissolving into noise. But the closed-form version computes each noisy image independently in one operation.\n",
    "\n",
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Experiment with Different Noise Schedules\n",
    "\n",
    "Replace the linear schedule with a **cosine schedule** (proposed by Nichol & Dhariwal, 2021). The cosine schedule provides a more gradual noise increase."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_14_todo1_implementation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo1 Implementation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_14_todo1_implementation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_beta_schedule(T, s=0.008):\n",
    "    \"\"\"\n",
    "    Cosine noise schedule from Nichol & Dhariwal (2021).\n",
    "\n",
    "    TODO: Implement the cosine schedule.\n",
    "\n",
    "    The formula is:\n",
    "    alpha_bar_t = cos((t/T + s) / (1 + s) * pi/2)^2\n",
    "\n",
    "    Then compute betas from alpha_bars:\n",
    "    beta_t = 1 - alpha_bar_t / alpha_bar_{t-1}\n",
    "    Clip beta_t to be at most 0.999\n",
    "\n",
    "    Returns:\n",
    "        betas: tensor of shape (T,)\n",
    "    \"\"\"\n",
    "    # HINT: Use torch.cos and torch.clamp\n",
    "    # Step 1: Create timestep array from 0 to T\n",
    "    # Step 2: Compute f(t) = cos((t/T + s)/(1+s) * pi/2)^2\n",
    "    # Step 3: Compute alpha_bars = f(t) / f(0)\n",
    "    # Step 4: Compute betas = 1 - alpha_bars[t] / alpha_bars[t-1]\n",
    "    # Step 5: Clip betas to [0, 0.999]\n",
    "\n",
    "    pass  # YOUR CODE HERE"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_15_todo1_verification",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo1 Verification\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_15_todo1_verification.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification cell for TODO 1\n",
    "cosine_betas = cosine_beta_schedule(T)\n",
    "if cosine_betas is not None:\n",
    "    cosine_alphas = 1.0 - cosine_betas\n",
    "    cosine_alpha_bars = torch.cumprod(cosine_alphas, dim=0)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(alpha_bars.numpy(), label='Linear Schedule')\n",
    "    plt.plot(cosine_alpha_bars.numpy(), label='Cosine Schedule')\n",
    "    plt.xlabel('Time step t')\n",
    "    plt.ylabel('·æ±‚Çú')\n",
    "    plt.title('Linear vs Cosine Noise Schedules')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    print(\"The cosine schedule should decrease more slowly at first, then faster at the end.\")\n",
    "else:\n",
    "    print(\"TODO 1 not yet implemented.\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_16_todo2_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo2 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_16_todo2_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Verify the Closed-Form Formula\n",
    "\n",
    "Prove empirically that the closed-form and step-by-step methods produce the same distribution."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_17_todo2_implementation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo2 Implementation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_17_todo2_implementation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_closed_form(x_0, t, num_samples=10000):\n",
    "    \"\"\"\n",
    "    TODO: Sample x_t using both methods many times and compare the distributions.\n",
    "\n",
    "    1. Use forward_step() iteratively t times to get samples of x_t\n",
    "    2. Use forward_diffusion_closed_form() to get samples of x_t\n",
    "    3. Compare the mean and std of both sample sets\n",
    "    4. Plot histograms of pixel values from both methods\n",
    "\n",
    "    If the closed-form is correct, both distributions should match.\n",
    "    \"\"\"\n",
    "    # HINT: Pick a single pixel location, e.g., [0, 14, 14]\n",
    "    # Sample it many times with both methods and compare\n",
    "\n",
    "    pass  # YOUR CODE HERE"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_18_putting_it_together_class",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Putting It Together Class\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_18_putting_it_together_class.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us combine everything into a complete forward diffusion module."
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardDiffusion:\n",
    "    \"\"\"Complete forward diffusion process for DDPM.\"\"\"\n",
    "\n",
    "    def __init__(self, T=1000, beta_start=1e-4, beta_end=0.02, schedule='linear'):\n",
    "        self.T = T\n",
    "\n",
    "        if schedule == 'linear':\n",
    "            self.betas = torch.linspace(beta_start, beta_end, T)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown schedule: {schedule}\")\n",
    "\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "        # Precompute useful quantities\n",
    "        self.sqrt_alpha_bars = torch.sqrt(self.alpha_bars)\n",
    "        self.sqrt_one_minus_alpha_bars = torch.sqrt(1.0 - self.alpha_bars)\n",
    "\n",
    "    def add_noise(self, x_0, t, noise=None):\n",
    "        \"\"\"Add noise to x_0 at timestep t.\"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "\n",
    "        # Handle batch of different timesteps\n",
    "        if isinstance(t, int):\n",
    "            sqrt_ab = self.sqrt_alpha_bars[t]\n",
    "            sqrt_1_ab = self.sqrt_one_minus_alpha_bars[t]\n",
    "        else:\n",
    "            sqrt_ab = self.sqrt_alpha_bars[t].view(-1, 1, 1, 1)\n",
    "            sqrt_1_ab = self.sqrt_one_minus_alpha_bars[t].view(-1, 1, 1, 1)\n",
    "\n",
    "        x_t = sqrt_ab * x_0 + sqrt_1_ab * noise\n",
    "        return x_t, noise\n",
    "\n",
    "    def visualize(self, x_0, num_steps=8):\n",
    "        \"\"\"Visualize the forward process at evenly spaced timesteps.\"\"\"\n",
    "        timesteps = torch.linspace(0, self.T - 1, num_steps).long()\n",
    "\n",
    "        fig, axes = plt.subplots(1, num_steps, figsize=(3 * num_steps, 3))\n",
    "        for ax, t in zip(axes, timesteps):\n",
    "            x_t, _ = self.add_noise(x_0, t.item())\n",
    "            ax.imshow(x_t.squeeze().clamp(-1, 1).numpy(), cmap='gray')\n",
    "            ax.set_title(f't={t.item()}\\n·æ±={self.alpha_bars[t]:.3f}')\n",
    "            ax.axis('off')\n",
    "        plt.suptitle('Forward Diffusion Process', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Create the forward process and visualize\n",
    "forward = ForwardDiffusion(T=1000)\n",
    "forward.visualize(sample_image)"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_19_batch_visualization",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Batch Visualization\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_19_batch_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "Let us test our forward process on a batch of different MNIST digits."
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a batch of images\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "batch_images, batch_labels = next(iter(dataloader))\n",
    "\n",
    "# Show the original batch\n",
    "fig, axes = plt.subplots(2, 8, figsize=(24, 6))\n",
    "for i in range(8):\n",
    "    # Original\n",
    "    axes[0][i].imshow(batch_images[i].squeeze().numpy(), cmap='gray')\n",
    "    axes[0][i].set_title(f'Original (digit {batch_labels[i].item()})')\n",
    "    axes[0][i].axis('off')\n",
    "\n",
    "    # At t=500\n",
    "    x_noisy, _ = forward.add_noise(batch_images[i:i+1], 500)\n",
    "    axes[1][i].imshow(x_noisy.squeeze().clamp(-1, 1).numpy(), cmap='gray')\n",
    "    axes[1][i].set_title(f't=500')\n",
    "    axes[1][i].axis('off')\n",
    "\n",
    "plt.suptitle('Forward Diffusion on MNIST Batch (t=0 vs t=500)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_20_statistical_verification",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Narration: Statistical Verification\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_20_statistical_verification.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical verification: at t=T, x_T should be approximately standard Gaussian\n",
    "x_T, _ = forward.add_noise(batch_images, T - 1)\n",
    "print(f\"x_T statistics:\")\n",
    "print(f\"  Mean:  {x_T.mean().item():.4f}  (expected: ~0.0)\")\n",
    "print(f\"  Std:   {x_T.std().item():.4f}  (expected: ~1.0)\")\n",
    "print(f\"  Min:   {x_T.min().item():.4f}\")\n",
    "print(f\"  Max:   {x_T.max().item():.4f}\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_21_final_output_visualization",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Final Output Visualization\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_21_final_output_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** At $t = T$, the mean should be close to 0 and the std close to 1.0, confirming that our forward process correctly produces approximately standard Gaussian noise.\n",
    "\n",
    "## 8. Final Output"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a beautiful visualization showing one image being destroyed over time\n",
    "fig, axes = plt.subplots(2, 10, figsize=(30, 6))\n",
    "timesteps = [0, 50, 100, 200, 300, 400, 500, 700, 850, 999]\n",
    "\n",
    "for row, img_idx in enumerate([0, 3]):\n",
    "    img = dataset[img_idx][0]\n",
    "    for col, t in enumerate(timesteps):\n",
    "        x_t, _ = forward.add_noise(img, t)\n",
    "        axes[row][col].imshow(x_t.squeeze().clamp(-1, 1).numpy(), cmap='gray')\n",
    "        axes[row][col].set_title(f't={t}', fontsize=10)\n",
    "        axes[row][col].axis('off')\n",
    "\n",
    "plt.suptitle('Forward Diffusion Process ‚Äî From Clean Image to Pure Noise', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\nForward diffusion complete! We can now jump to any noise level in a single step.\")\n",
    "print(\"Next: Learn how to REVERSE this process to generate new images.\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_22_closing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Wrap-Up: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_22_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Key Takeaways\n",
    "1. The forward process adds Gaussian noise gradually over T steps\n",
    "2. The closed-form formula $x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon$ lets us skip to any timestep directly\n",
    "3. The noise schedule controls how quickly the signal is destroyed\n",
    "4. At $t = T$, the noisy image is approximately standard Gaussian noise\n",
    "\n",
    "### Reflection Questions\n",
    "- Why do we scale the mean down by $\\sqrt{1-\\beta_t}$ instead of just adding noise? What would happen if we did not scale?\n",
    "- What is the effect of choosing a larger $\\beta_T$ value? How would it change the quality of generated images?\n",
    "- Why is the closed-form formula so important for training efficiency?\n",
    "\n",
    "### What is Next\n",
    "In the next notebook, we will tackle the reverse question: given a noisy image, how do we learn to remove the noise? This brings us to the DDPM loss function and the elegant insight that training a diffusion model reduces to predicting noise."
   ],
   "id": "cell_28"
  }
 ]
}