{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "U-Net and Full DDPM Training â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1_yOuaRupWcvvBB5tNnjVrtDllXqg6x4Q\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/03_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_01_setup",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_01_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_02_why_unet",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why Unet\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_02_why_unet.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net and Full DDPM Training â€” Vizuara\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebooks, we built the forward diffusion process and trained a simple CNN to predict noise. But simple CNNs are not powerful enough to generate realistic images. The DDPM paper uses a **U-Net architecture** â€” an encoder-decoder network with skip connections that can process images at multiple resolutions while preserving fine spatial details.\n",
    "\n",
    "In this notebook, we will:\n",
    "- Build a complete U-Net noise predictor with timestep conditioning\n",
    "- Understand why skip connections and multi-scale processing are essential\n",
    "- Train a full DDPM on MNIST and generate images from pure noise\n",
    "- Implement the complete sampling loop\n",
    "\n",
    "This is where everything comes together â€” by the end, you will have a working diffusion model that generates handwritten digits from scratch.\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "Why do we need a U-Net specifically? Consider what the noise predictor must do:\n",
    "\n",
    "At **high noise levels** (large $t$), the image is almost pure noise. The model needs to make **global decisions** â€” is this going to be a \"3\" or a \"7\"? This requires understanding the big picture.\n",
    "\n",
    "At **low noise levels** (small $t$), the image is mostly clean. The model needs to refine **local details** â€” sharpening edges, fixing individual pixels. This requires fine-grained spatial information.\n",
    "\n",
    "The U-Net handles both by processing at multiple resolutions:\n",
    "- **Encoder (downsampling):** Captures global structure by progressively reducing resolution\n",
    "- **Decoder (upsampling):** Reconstructs details by progressively increasing resolution\n",
    "- **Skip connections:** Pass fine-grained spatial information directly from encoder to decoder\n",
    "\n",
    "Let us build it step by step."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_03_install_and_params",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Install And Params\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_03_install_and_params.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision matplotlib numpy -q"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Setup diffusion parameters\n",
    "T = 1000\n",
    "betas = torch.linspace(1e-4, 0.02, T).to(device)\n",
    "alphas = (1.0 - betas).to(device)\n",
    "alpha_bars = torch.cumprod(alphas, dim=0).to(device)\n",
    "\n",
    "def forward_diffusion(x_0, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_0)\n",
    "    sqrt_ab = torch.sqrt(alpha_bars[t]).view(-1, 1, 1, 1)\n",
    "    sqrt_1_ab = torch.sqrt(1 - alpha_bars[t]).view(-1, 1, 1, 1)\n",
    "    x_t = sqrt_ab * x_0 + sqrt_1_ab * noise\n",
    "    return x_t, noise\n",
    "\n",
    "# Load MNIST (padded to 32x32 for cleaner U-Net downsampling)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(2),  # 28x28 -> 32x32\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_04_timestep_embedding_math",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Timestep Embedding Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_04_timestep_embedding_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Sinusoidal Timestep Embedding\n",
    "\n",
    "The model needs to know the noise level. We embed the timestep $t$ using sinusoidal positional encodings (the same idea from Transformers):\n",
    "\n",
    "$$\\text{PE}(t, 2i) = \\sin(t / 10000^{2i/d})$$\n",
    "$$\\text{PE}(t, 2i+1) = \\cos(t / 10000^{2i/d})$$\n",
    "\n",
    "Let us plug in numbers. For $t=500$, $d=64$, $i=0$:\n",
    "- $\\text{PE}(500, 0) = \\sin(500 / 10000^{0/64}) = \\sin(500/1) = \\sin(500) \\approx -0.468$\n",
    "- $\\text{PE}(500, 1) = \\cos(500 / 10000^{0/64}) = \\cos(500) \\approx 0.884$\n",
    "\n",
    "For $i=16$: $10000^{32/64} = 10000^{0.5} = 100$\n",
    "- $\\text{PE}(500, 32) = \\sin(500/100) = \\sin(5) \\approx -0.959$\n",
    "\n",
    "Different $i$ values create features that oscillate at different frequencies, giving the model a rich representation of the timestep.\n",
    "\n",
    "### U-Net Architecture Summary\n",
    "\n",
    "The U-Net processes at 4 resolution levels: 32x32, 16x16, 8x8, and 4x4. At each level:\n",
    "1. Two convolutional blocks with time embedding injection\n",
    "2. Downsampling (encoder) or upsampling (decoder)\n",
    "3. Skip connections between matching encoder/decoder levels\n",
    "\n",
    "## 4. Let's Build It â€” Component by Component\n",
    "\n",
    "### Step 1: Sinusoidal Timestep Embedding"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_05_sinusoidal_embedding_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Sinusoidal Embedding Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_05_sinusoidal_embedding_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional embedding for timesteps.\"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None].float() * emb[None, :]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "        return emb"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_06_visualize_embedding",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Visualize Embedding\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_06_visualize_embedding.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the time embedding\n",
    "time_emb = SinusoidalTimeEmbedding(64)\n",
    "t_values = torch.arange(0, T, 10)\n",
    "embeddings = time_emb(t_values).detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(embeddings.T, aspect='auto', cmap='RdBu')\n",
    "plt.xlabel('Timestep t')\n",
    "plt.ylabel('Embedding Dimension')\n",
    "plt.title('Sinusoidal Timestep Embeddings')\n",
    "plt.colorbar(label='Value')\n",
    "plt.show()\n",
    "print(\"Each column is a unique embedding â€” the model can distinguish all 1000 timesteps.\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_07_residual_block",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Residual Block\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_07_residual_block.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Residual Convolutional Block"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"Residual block with time embedding injection.\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, time_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.time_mlp = nn.Linear(time_dim, out_ch)\n",
    "        self.norm1 = nn.GroupNorm(8, out_ch)\n",
    "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
    "\n",
    "        # Shortcut if dimensions change\n",
    "        self.shortcut = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        h = self.norm1(F.relu(self.conv1(x)))\n",
    "\n",
    "        # Inject time embedding\n",
    "        t = F.relu(self.time_mlp(t_emb))\n",
    "        h = h + t[:, :, None, None]\n",
    "\n",
    "        h = self.norm2(F.relu(self.conv2(h)))\n",
    "        return h + self.shortcut(x)"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_08_full_unet",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Full Unet\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_08_full_unet.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Full U-Net"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net for DDPM noise prediction.\n",
    "\n",
    "    Architecture:\n",
    "    - 4 resolution levels: 32 -> 16 -> 8 -> 4\n",
    "    - Channel progression: 1 -> 64 -> 128 -> 256 -> 512\n",
    "    - Skip connections between matching encoder/decoder levels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch=1, base_ch=64, time_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalTimeEmbedding(base_ch),\n",
    "            nn.Linear(base_ch, time_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "        )\n",
    "\n",
    "        # Encoder (downsampling path)\n",
    "        self.enc1 = ResBlock(in_ch, base_ch, time_dim)          # 32x32\n",
    "        self.enc2 = ResBlock(base_ch, base_ch * 2, time_dim)    # 16x16\n",
    "        self.enc3 = ResBlock(base_ch * 2, base_ch * 4, time_dim)  # 8x8\n",
    "\n",
    "        self.down1 = nn.Conv2d(base_ch, base_ch, 4, stride=2, padding=1)\n",
    "        self.down2 = nn.Conv2d(base_ch * 2, base_ch * 2, 4, stride=2, padding=1)\n",
    "        self.down3 = nn.Conv2d(base_ch * 4, base_ch * 4, 4, stride=2, padding=1)\n",
    "\n",
    "        # Bottleneck (4x4)\n",
    "        self.bottleneck = ResBlock(base_ch * 4, base_ch * 4, time_dim)\n",
    "\n",
    "        # Decoder (upsampling path)\n",
    "        self.up3 = nn.ConvTranspose2d(base_ch * 4, base_ch * 4, 4, stride=2, padding=1)\n",
    "        self.up2 = nn.ConvTranspose2d(base_ch * 2, base_ch * 2, 4, stride=2, padding=1)\n",
    "        self.up1 = nn.ConvTranspose2d(base_ch, base_ch, 4, stride=2, padding=1)\n",
    "\n",
    "        # Decoder blocks (input channels doubled due to skip connections)\n",
    "        self.dec3 = ResBlock(base_ch * 8, base_ch * 2, time_dim)   # 8x8\n",
    "        self.dec2 = ResBlock(base_ch * 4, base_ch, time_dim)       # 16x16\n",
    "        self.dec1 = ResBlock(base_ch * 2, base_ch, time_dim)       # 32x32\n",
    "\n",
    "        # Final output\n",
    "        self.final = nn.Conv2d(base_ch, in_ch, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Time embedding\n",
    "        t_emb = self.time_embed(t)\n",
    "\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x, t_emb)        # 32x32, base_ch\n",
    "        e2 = self.enc2(self.down1(e1), t_emb)  # 16x16, base_ch*2\n",
    "        e3 = self.enc3(self.down2(e2), t_emb)  # 8x8, base_ch*4\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(self.down3(e3), t_emb)  # 4x4, base_ch*4\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        d3 = self.dec3(torch.cat([self.up3(b), e3], dim=1), t_emb)   # 8x8\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1), t_emb)  # 16x16\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1), t_emb)  # 32x32\n",
    "\n",
    "        return self.final(d1)\n",
    "\n",
    "\n",
    "model = UNet(in_ch=1, base_ch=64).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"U-Net parameters: {total_params:,}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_09_unet_test",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Narration: Unet Test\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_09_unet_test.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test: verify the model produces correct output shape\n",
    "test_x = torch.randn(4, 1, 32, 32).to(device)\n",
    "test_t = torch.randint(0, T, (4,)).to(device)\n",
    "test_out = model(test_x, test_t)\n",
    "print(f\"Input shape:  {test_x.shape}\")\n",
    "print(f\"Output shape: {test_out.shape}\")\n",
    "assert test_x.shape == test_out.shape, \"Input and output shapes must match!\"\n",
    "print(\"Shape check passed!\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_10_todo1_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo1 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_10_todo1_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** The model should have roughly 5-10 million parameters and produce output with the same shape as the input.\n",
    "\n",
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Add Self-Attention to the Bottleneck\n",
    "\n",
    "Self-attention allows the model to capture long-range dependencies. This is especially useful at low resolutions where global context matters most."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_11_todo1_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo1 Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_11_todo1_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    TODO: Implement self-attention for the U-Net bottleneck.\n",
    "\n",
    "    1. Project input to Q, K, V using 1x1 convolutions\n",
    "    2. Reshape to (B, C, H*W) for attention computation\n",
    "    3. Compute attention: softmax(Q^T K / sqrt(d)) * V\n",
    "    4. Reshape back and add residual connection\n",
    "\n",
    "    Args:\n",
    "        channels: Number of input/output channels\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        # HINT: Use nn.Conv2d with kernel_size=1 for Q, K, V projections\n",
    "        # HINT: Scale attention by 1/sqrt(channels)\n",
    "\n",
    "        pass  # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        # HINT: Remember to reshape from (B,C,H,W) to (B,C,H*W) for attention\n",
    "        pass  # YOUR CODE HERE"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_12_todo2_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo2 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_12_todo2_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement Gradient Accumulation for Larger Effective Batch Size"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_13_todo2_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo2 Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_13_todo2_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_gradient_accumulation(model, dataloader, optimizer, accum_steps=4, num_epochs=1):\n",
    "    \"\"\"\n",
    "    TODO: Train with gradient accumulation to simulate larger batch sizes.\n",
    "\n",
    "    1. Zero gradients at the start\n",
    "    2. For each batch, compute loss and call loss.backward()\n",
    "    3. Only call optimizer.step() every accum_steps batches\n",
    "    4. Scale the loss by 1/accum_steps before backward\n",
    "\n",
    "    This lets you train with effective batch size = actual_batch * accum_steps\n",
    "    without needing more GPU memory.\n",
    "    \"\"\"\n",
    "    # HINT: Use (batch_idx + 1) % accum_steps == 0 to decide when to step\n",
    "\n",
    "    pass  # YOUR CODE HERE"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_14_training_loop",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Training Loop\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_14_training_loop.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete training loop\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "losses = []\n",
    "\n",
    "print(\"Training DDPM...\")\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "\n",
    "    for batch_idx, (images, _) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        batch_size = images.shape[0]\n",
    "\n",
    "        # Sample random timesteps\n",
    "        t = torch.randint(0, T, (batch_size,), device=device)\n",
    "\n",
    "        # Sample noise and create noisy images\n",
    "        noise = torch.randn_like(images)\n",
    "        x_t, _ = forward_diffusion(images, t, noise)\n",
    "\n",
    "        # Predict noise\n",
    "        predicted_noise = model(x_t, t)\n",
    "\n",
    "        # DDPM loss\n",
    "        loss = F.mse_loss(predicted_noise, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "        if batch_idx % 200 == 0:\n",
    "            print(f\"  Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    losses.extend(epoch_losses)\n",
    "    print(f\"Epoch {epoch+1}/10 â€” Average Loss: {avg_loss:.4f}\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_15_plot_loss",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Plot Loss\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_15_plot_loss.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(np.convolve(losses, np.ones(50)/50, mode='valid'))\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('MSE Loss (smoothed)')\n",
    "plt.title('DDPM Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_16_sampling_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Sampling Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_16_sampling_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "### Sampling: Generating Images from Noise"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_17_sample_ddpm_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Sample Ddpm Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_17_sample_ddpm_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_ddpm(model, n_samples=16, img_shape=(1, 32, 32)):\n",
    "    \"\"\"Generate images using the trained DDPM.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Start from pure noise\n",
    "    x = torch.randn(n_samples, *img_shape, device=device)\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        t_batch = torch.full((n_samples,), t, device=device, dtype=torch.long)\n",
    "\n",
    "        # Predict noise\n",
    "        predicted_noise = model(x, t_batch)\n",
    "\n",
    "        # Compute denoised image\n",
    "        alpha_t = alphas[t]\n",
    "        alpha_bar_t = alpha_bars[t]\n",
    "        beta_t = betas[t]\n",
    "\n",
    "        x = (1 / torch.sqrt(alpha_t)) * (\n",
    "            x - (beta_t / torch.sqrt(1 - alpha_bar_t)) * predicted_noise\n",
    "        )\n",
    "\n",
    "        # Add noise (except at last step)\n",
    "        if t > 0:\n",
    "            x = x + torch.sqrt(beta_t) * torch.randn_like(x)\n",
    "\n",
    "    return x"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_18_generate_samples",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Generate Samples\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_18_generate_samples.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "print(\"Generating samples (this takes a minute)...\")\n",
    "samples = sample_ddpm(model, n_samples=16)\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(24, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = samples[i].squeeze().cpu().clamp(-1, 1).numpy()\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Generated Samples from Trained DDPM', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_19_visualize_trajectory",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Visualize Trajectory\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_19_visualize_trajectory.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the progressive denoising for a single sample\n",
    "@torch.no_grad()\n",
    "def sample_with_trajectory(model, img_shape=(1, 32, 32), save_every=100):\n",
    "    \"\"\"Generate one image and save intermediate steps.\"\"\"\n",
    "    model.eval()\n",
    "    x = torch.randn(1, *img_shape, device=device)\n",
    "    trajectory = [x.clone()]\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        t_batch = torch.tensor([t], device=device)\n",
    "        predicted_noise = model(x, t_batch)\n",
    "\n",
    "        alpha_t = alphas[t]\n",
    "        alpha_bar_t = alpha_bars[t]\n",
    "        beta_t = betas[t]\n",
    "\n",
    "        x = (1 / torch.sqrt(alpha_t)) * (\n",
    "            x - (beta_t / torch.sqrt(1 - alpha_bar_t)) * predicted_noise\n",
    "        )\n",
    "        if t > 0:\n",
    "            x = x + torch.sqrt(beta_t) * torch.randn_like(x)\n",
    "\n",
    "        if t % save_every == 0:\n",
    "            trajectory.append(x.clone())\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "trajectory = sample_with_trajectory(model, save_every=100)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(trajectory), figsize=(3 * len(trajectory), 3))\n",
    "for i, (ax, img) in enumerate(zip(axes, trajectory)):\n",
    "    t_val = T - i * 100 if i < len(trajectory) - 1 else 0\n",
    "    ax.imshow(img.squeeze().cpu().clamp(-1, 1).numpy(), cmap='gray')\n",
    "    ax.set_title(f't={t_val}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Progressive Denoising â€” From Pure Noise to Generated Image', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_20_final_output",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Final Output\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_20_final_output.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a beautiful 4x8 grid of diverse samples\n",
    "print(\"Generating final sample grid...\")\n",
    "final_samples = sample_ddpm(model, n_samples=32)\n",
    "\n",
    "fig, axes = plt.subplots(4, 8, figsize=(24, 12))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = final_samples[i].squeeze().cpu().clamp(-1, 1).numpy()\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('DDPM Generated Digits â€” Trained from Scratch on MNIST', fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig('ddpm_generated_samples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nModel: U-Net with {total_params:,} parameters\")\n",
    "print(f\"Training: {len(losses)} steps, final loss: {losses[-1]:.4f}\")\n",
    "print(f\"Sampling: {T} denoising steps per image\")\n",
    "print(\"Generated 32 unique digits from pure Gaussian noise!\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_21_reflection",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Reflection\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_21_reflection.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Key Takeaways\n",
    "1. The U-Net's multi-scale architecture with skip connections is ideal for noise prediction\n",
    "2. Sinusoidal time embeddings let the model adapt its behavior to different noise levels\n",
    "3. Training is just MSE between true and predicted noise â€” no adversarial training needed\n",
    "4. Sampling requires T = 1000 forward passes, which is slow but produces high-quality results\n",
    "\n",
    "### Reflection Questions\n",
    "- Why do skip connections improve generation quality? What would happen without them?\n",
    "- How does the model \"decide\" what digit to generate when starting from random noise?\n",
    "- What are the trade-offs between using more diffusion steps T and fewer?\n",
    "- How would you modify this architecture for color images (3 channels)?\n",
    "\n",
    "### What is Next\n",
    "In the final notebook, we will explore the connection between DDPM and score-based generative models, implement faster sampling with DDIM, and discuss how this framework scales to produce stunning results like Stable Diffusion and DALL-E 2."
   ],
   "id": "cell_25"
  }
 ]
}