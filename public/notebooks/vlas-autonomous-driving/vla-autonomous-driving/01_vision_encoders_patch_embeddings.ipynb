{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Vision Encoders & Patch Embeddings â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1IFTVQVhXDAoHIk3Ewmz5AKLUo0lFt1N5\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_why_it_matters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Encoders & Patch Embeddings: How VLAs \"See\" the Road\n",
    "\n",
    "*Part 1 of the Vizuara series on Vision-Language-Action Models for Autonomous Driving*\n",
    "*Estimated time: 60 minutes | Runs on: Google Colab (T4 GPU)*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://pods.vizuara.ai/courses/vla-autonomous-driving/practice/1/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Let us start with a simple question: how does a self-driving car *see*?\n",
    "\n",
    "A human driver glances at the road ahead, checks the mirrors, notices a pedestrian stepping off the curb, spots a red brake light two cars ahead â€” all in a fraction of a second. We process rich visual information effortlessly. But for an autonomous vehicle, the raw camera feed is just a grid of numbers â€” millions of pixel values streaming in from 4 to 8 cameras with 360-degree coverage.\n",
    "\n",
    "**Vision-Language-Action models (VLAs)** are a new class of models that take these camera images, convert them into compact \"feature tokens\" (just like words in a sentence), feed them to a language model, and output driving actions â€” steering, acceleration, braking. The first and arguably most critical step in this entire pipeline is the **Vision Encoder**: the component that converts raw pixels into meaningful representations.\n",
    "\n",
    "The dominant approach? The **Vision Transformer (ViT)**. It splits each camera image into a grid of small patches, treats each patch as a \"word,\" and uses self-attention to understand how the patches relate to each other.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Build a complete Vision Transformer from scratch in PyTorch\n",
    "- Train it on real images and achieve reasonable accuracy\n",
    "- Visualize **attention heatmaps** showing exactly which parts of the image the model focuses on\n",
    "- Understand every mathematical operation happening inside\n",
    "\n",
    "Let us get started."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Building Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_building_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_building_intuition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Before we touch any code or equations, let us build a strong mental model for what is happening.\n",
    "\n",
    "**Analogy 1: Reading a newspaper page.** Imagine you pick up a newspaper. You do not read every single letter from top-left to bottom-right. Your eyes jump to headlines, photographs, bold text, and key paragraphs. You quickly understand the \"layout\" â€” where the important stuff is. A Vision Transformer does something remarkably similar with images. It divides the image into patches and learns which patches matter and how they relate to each other.\n",
    "\n",
    "**Analogy 2: Tiles in a mosaic.** Think of a large mosaic on a wall. Each small tile contains a tiny piece of the picture â€” maybe a sliver of blue sky, or a fragment of a face. Individually, each tile tells you very little. But your brain naturally groups tiles together: \"these blue tiles are sky,\" \"these tan tiles form a face,\" \"these red tiles are a flower.\" The transformer's self-attention mechanism does the same thing â€” it figures out which tiles (patches) are related. In driving, this means: \"this patch has a red light AND this patch shows a car bumper ahead, so there is a car braking in front of us.\"\n",
    "\n",
    "### Think About This\n",
    "\n",
    "Why patches instead of individual pixels? This comes down to computational cost.\n",
    "\n",
    "Consider a typical driving camera image: 320 x 576 pixels. That is 184,320 pixels. Self-attention computes relationships between *every pair* of tokens, which is O(n^2). So:\n",
    "- **Pixel-level attention:** 184,320^2 = ~34 billion operations. Not feasible.\n",
    "- **With 16x16 patches:** we get (320/16) x (576/16) = 20 x 36 = 720 patches. And 720^2 = 518,400 operations. Entirely manageable.\n",
    "\n",
    "Patches give us a 65,000x reduction in the number of attention computations. This is exactly what makes Vision Transformers practical."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Mathematics\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_mathematics.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_mathematics"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "Now let us look at the three key mathematical operations inside a Vision Transformer. We will take each one, explain what it does computationally, and work through a concrete numerical example.\n",
    "\n",
    "### 3.1 Patch Embedding\n",
    "\n",
    "$$\\mathbf{z}_i = \\mathbf{W}_p \\cdot \\text{flatten}(\\text{patch}_i) + \\mathbf{b}_p$$\n",
    "\n",
    "What does this say? Take each 16 x 16 x 3 patch (that is 16 x 16 pixels across 3 color channels = 768 values), flatten it into a single vector, and multiply by a learned weight matrix $\\mathbf{W}_p$ to project it into a $d$-dimensional embedding space. Then add a bias term. This is literally a linear projection â€” a matrix multiplication followed by an addition.\n",
    "\n",
    "**Let us plug in some simple numbers.** Suppose we have a tiny 4 x 4 x 1 patch (grayscale, so 1 channel). Flattened, this gives us a vector of 16 values. Say our embedding dimension is $d = 3$:\n",
    "\n",
    "$$\\text{patch} = [0.2, 0.5, 0.1, 0.8, \\ldots] \\quad (\\text{16 values})$$\n",
    "\n",
    "$$\\mathbf{W}_p \\in \\mathbb{R}^{3 \\times 16}, \\quad \\mathbf{b}_p \\in \\mathbb{R}^{3}$$\n",
    "\n",
    "$$\\mathbf{z}_i = \\mathbf{W}_p \\cdot \\text{patch} + \\mathbf{b}_p = [1.3, -0.7, 0.4]$$\n",
    "\n",
    "So a 16-dimensional raw patch gets compressed into a 3-dimensional embedding vector. In practice, the embedding dimension is much larger (e.g., 128 or 768), but the operation is exactly the same â€” a matrix multiply.\n",
    "\n",
    "### 3.2 Positional Encoding\n",
    "\n",
    "$$\\mathbf{z}_i' = \\mathbf{z}_i + \\mathbf{e}_i^{\\text{pos}}$$\n",
    "\n",
    "After embedding each patch, we add a **learnable position vector** so the model knows *where* each patch came from in the original image. Without positional encoding, the model would see a bag of patches with no spatial information â€” it could not distinguish \"car in the top-left\" from \"car in the bottom-right.\"\n",
    "\n",
    "**Worked example.** Suppose patch 0 (top-left corner) has embedding $\\mathbf{z}_0 = [1.3, -0.7, 0.4]$ and its learnable position vector is $\\mathbf{e}_0^{\\text{pos}} = [0.1, 0.2, -0.1]$:\n",
    "\n",
    "$$\\mathbf{z}_0' = [1.3 + 0.1, \\; -0.7 + 0.2, \\; 0.4 + (-0.1)] = [1.4, -0.5, 0.3]$$\n",
    "\n",
    "Patch 5 (somewhere in the middle) would have a different position vector $\\mathbf{e}_5^{\\text{pos}} = [-0.3, 0.5, 0.6]$, so even if two patches had identical pixel content, their final representations would differ based on where they sit in the image. This is exactly what we want.\n",
    "\n",
    "### 3.3 Self-Attention\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "This is the heart of the transformer. Let us break it down computationally:\n",
    "\n",
    "- **Q (Query):** Each patch asks \"what am I looking for?\"\n",
    "- **K (Key):** Each patch declares \"what do I contain?\"\n",
    "- **$QK^T$ (dot product):** Measures compatibility between every pair of patches. High score means \"these two patches are relevant to each other.\"\n",
    "- **$\\sqrt{d_k}$ (scaling):** Prevents the dot products from growing too large, which would push softmax into regions where gradients vanish.\n",
    "- **Softmax:** Converts raw scores into attention weights that sum to 1 â€” a probability distribution over patches.\n",
    "- **Multiply by V (Value):** Produces a weighted combination of all patches, emphasizing the most relevant ones.\n",
    "\n",
    "**Worked example with 3 patches, $d_k = 2$.** Suppose:\n",
    "\n",
    "$$Q = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix}, \\quad K = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\\\ 1 & 0 \\end{bmatrix}, \\quad V = \\begin{bmatrix} 0.5 & 0.3 \\\\ 0.8 & 0.1 \\\\ 0.2 & 0.9 \\end{bmatrix}$$\n",
    "\n",
    "Step 1 â€” Compute $QK^T$:\n",
    "\n",
    "$$QK^T = \\begin{bmatrix} 1 & 0 & 1 \\\\ 1 & 1 & 0 \\\\ 2 & 1 & 1 \\end{bmatrix}$$\n",
    "\n",
    "Step 2 â€” Scale by $\\sqrt{d_k} = \\sqrt{2} \\approx 1.41$:\n",
    "\n",
    "$$\\frac{QK^T}{\\sqrt{2}} = \\begin{bmatrix} 0.71 & 0 & 0.71 \\\\ 0.71 & 0.71 & 0 \\\\ 1.41 & 0.71 & 0.71 \\end{bmatrix}$$\n",
    "\n",
    "Step 3 â€” Apply softmax to each row (we get attention weights):\n",
    "\n",
    "Row 1: $[0.40, 0.20, 0.40]$ â€” Patch 1 attends equally to patches 1 and 3, less to patch 2.\n",
    "\n",
    "Step 4 â€” Multiply by V: the output for patch 1 would be $0.40 \\times [0.5, 0.3] + 0.20 \\times [0.8, 0.1] + 0.40 \\times [0.2, 0.9] = [0.44, 0.50]$.\n",
    "\n",
    "This tells us that patch 1's new representation is a weighted blend of all patches, with more weight on patches 1 and 3. In a driving scene, this is how the model learns that a brake light patch and a bumper patch should attend to each other."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Start Building\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_start_building.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_start_building"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let Us Build It â€” Component by Component\n",
    "\n",
    "Now we have all the intuition and math. Let us implement each component in PyTorch, step by step, and visualize what each one does.\n",
    "\n",
    "### Setup"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup â€” run this first!\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Patch Embedding\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_patch_embedding.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_patch_embedding"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Patch Embedding\n",
    "\n",
    "The first step is splitting the image into patches and projecting each one into an embedding vector. There is a neat trick here: a `Conv2d` layer with `kernel_size = stride = patch_size` is *mathematically equivalent* to extracting non-overlapping patches and applying a linear projection to each. Let us implement this."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Split an image into non-overlapping patches and project to embedding dimension.\n",
    "\n",
    "    A Conv2d with kernel_size=stride=patch_size extracts patches and applies\n",
    "    a linear projection in one operation. This is equivalent to:\n",
    "      1. Slice the image into patch_size x patch_size blocks\n",
    "      2. Flatten each block to a vector of length (channels * patch_size^2)\n",
    "      3. Multiply by a weight matrix W of shape (embed_dim, patch_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        # Conv2d trick: kernel_size=stride=patch_size\n",
    "        self.projection = nn.Conv2d(\n",
    "            in_channels, embed_dim,\n",
    "            kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, channels, height, width)\n",
    "        x = self.projection(x)    # (batch, embed_dim, n_patches_h, n_patches_w)\n",
    "        x = x.flatten(2)          # (batch, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2)     # (batch, n_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "# Quick test\n",
    "patch_embed = PatchEmbedding()\n",
    "dummy_img = torch.randn(1, 3, 32, 32)\n",
    "out = patch_embed(dummy_img)\n",
    "print(f\"Input shape:  {dummy_img.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Number of patches: {patch_embed.n_patches}\")\n",
    "print(f\"Each patch is now a {out.shape[-1]}-dimensional vector\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Patch Visualization\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_patch_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_patch_visualization"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us actually *see* what patch extraction looks like on a real image."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize patch extraction on a CIFAR-10 image\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "sample_img, label = dataset[0]\n",
    "cifar_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(sample_img.permute(1, 2, 0).numpy())\n",
    "axes[0].set_title(f'Original Image (32x32) â€” \"{cifar_classes[label]}\"', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Image with patch grid overlay\n",
    "img_with_grid = sample_img.permute(1, 2, 0).numpy().copy()\n",
    "axes[1].imshow(img_with_grid)\n",
    "patch_size = 4\n",
    "for i in range(0, 32, patch_size):\n",
    "    axes[1].axhline(y=i, color='red', linewidth=0.8)\n",
    "    axes[1].axvline(x=i, color='red', linewidth=0.8)\n",
    "axes[1].set_title(f'Divided into {(32//patch_size)**2} patches (4x4 each)', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all 64 individual patches in a grid\n",
    "patch_size = 4\n",
    "img_np = sample_img.permute(1, 2, 0).numpy()\n",
    "n_patches_per_side = 32 // patch_size  # 8\n",
    "\n",
    "fig, axes = plt.subplots(n_patches_per_side, n_patches_per_side, figsize=(6, 6))\n",
    "for i in range(n_patches_per_side):\n",
    "    for j in range(n_patches_per_side):\n",
    "        patch = img_np[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n",
    "        axes[i][j].imshow(patch)\n",
    "        axes[i][j].axis('off')\n",
    "plt.suptitle(\n",
    "    f'Image split into {n_patches_per_side}x{n_patches_per_side} = {n_patches_per_side**2} patches\\n'\n",
    "    f'Each patch: {patch_size}x{patch_size} pixels = {patch_size*patch_size*3} values (RGB)',\n",
    "    fontsize=12\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Each patch has {patch_size}x{patch_size}x3 = {patch_size*patch_size*3} raw values\")\n",
    "print(f\"After projection, each patch becomes a 128-dimensional embedding vector\")\n",
    "print(f\"So we go from {32*32*3:,} pixel values to {n_patches_per_side**2} x 128 = {n_patches_per_side**2 * 128:,} values\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how each individual patch is tiny and hard to interpret on its own â€” just like a single mosaic tile. The transformer's job is to learn how these tiles fit together."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Positional Encoding\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_positional_encoding.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_positional_encoding"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Positional Encoding\n",
    "\n",
    "Now the question is: once we have patch embeddings, how does the model know *where* each patch came from? If we just pass a bag of patch vectors, the model has no idea whether a patch is from the top-left corner (sky) or bottom-center (road). We solve this by adding **learnable positional embeddings**.\n",
    "\n",
    "We also introduce a special **[CLS] token** â€” an extra learnable vector prepended to the sequence. This token has no corresponding image patch. Instead, it learns to aggregate information from all patches through attention, and its final representation is used for classification."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Learnable positional encoding for patch tokens, plus a CLS token.\"\"\"\n",
    "    def __init__(self, n_patches, embed_dim):\n",
    "        super().__init__()\n",
    "        # CLS token: a learnable vector for classification\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        # One learnable position embedding per token (patches + CLS)\n",
    "        self.position_embeddings = nn.Parameter(\n",
    "            torch.randn(1, n_patches + 1, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # Expand CLS token for the entire batch\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        # Prepend CLS token to the patch sequence\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (batch, n_patches+1, embed_dim)\n",
    "        # Add positional encoding\n",
    "        x = x + self.position_embeddings\n",
    "        return x\n",
    "\n",
    "# Quick test\n",
    "pos_enc = PositionalEncoding(n_patches=64, embed_dim=128)\n",
    "dummy_patches = torch.randn(2, 64, 128)  # batch of 2, 64 patches, 128-dim\n",
    "out = pos_enc(dummy_patches)\n",
    "print(f\"Input:  {dummy_patches.shape} (batch, patches, embed_dim)\")\n",
    "print(f\"Output: {out.shape} (batch, patches+CLS, embed_dim)\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the positional encoding similarity matrix. Before training, the position vectors are random. After training, we would expect nearby patches to have similar position encodings â€” because neighboring patches tend to contain related visual content."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize positional encoding similarity\n",
    "pos_enc_viz = PositionalEncoding(n_patches=64, embed_dim=128)\n",
    "pos_embeds = pos_enc_viz.position_embeddings.squeeze(0).detach()\n",
    "\n",
    "# Compute cosine similarity between all position pairs\n",
    "similarity = F.cosine_similarity(\n",
    "    pos_embeds.unsqueeze(0), pos_embeds.unsqueeze(1), dim=-1\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(similarity.numpy(), cmap='viridis')\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.title('Positional Encoding Similarity (before training)\\n'\n",
    "          'Position 0 = CLS token, Positions 1-64 = image patches')\n",
    "plt.xlabel('Token Position')\n",
    "plt.ylabel('Token Position')\n",
    "plt.show()\n",
    "\n",
    "print(\"Before training, the similarity is essentially random noise.\")\n",
    "print(\"After training, we would see a structured pattern: nearby patches\")\n",
    "print(\"would have high similarity, and distant patches would have low similarity.\")\n",
    "print(\"The CLS token (position 0) would show moderate similarity to all patches.\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly what we want â€” a mechanism for the model to understand spatial layout."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Multi Head Attention\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_multi_head_attention.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_multi_head_attention"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Multi-Head Self-Attention\n",
    "\n",
    "Now we arrive at the core mechanism. Self-attention lets every patch \"look at\" every other patch and decide which ones are relevant. Multi-head attention does this multiple times in parallel, with each \"head\" potentially learning to focus on different types of relationships.\n",
    "\n",
    "Why multiple heads? Think of it this way: one head might learn to focus on color relationships (\"these patches are all blue sky\"), another on edges (\"these patches form a continuous lane marking\"), and another on semantic groupings (\"these patches all belong to the same vehicle\"). Having multiple heads gives the model richer, more diverse representations."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention mechanism.\n",
    "\n",
    "    Projects input to Q, K, V for each head, computes scaled dot-product\n",
    "    attention, concatenates heads, and projects back.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = embed_dim // n_heads\n",
    "        assert embed_dim % n_heads == 0, \"embed_dim must be divisible by n_heads\"\n",
    "\n",
    "        # Single linear layer to compute Q, K, V for all heads at once\n",
    "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attn_weights = None  # Store for visualization later\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "\n",
    "        # Compute Q, K, V for all heads in one matrix multiplication\n",
    "        qkv = self.qkv(x)  # (batch, seq_len, 3 * embed_dim)\n",
    "        qkv = qkv.reshape(batch_size, seq_len, 3, self.n_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch, heads, seq_len, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scale = self.head_dim ** 0.5\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / scale\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        self.attn_weights = attn_weights.detach()  # Save for visualization\n",
    "\n",
    "        # Apply attention weights to values\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "\n",
    "        # Concatenate heads and apply final projection\n",
    "        attn_output = attn_output.transpose(1, 2).reshape(batch_size, seq_len, embed_dim)\n",
    "        output = self.projection(attn_output)\n",
    "        return output\n",
    "\n",
    "# Quick test\n",
    "mha = MultiHeadAttention(embed_dim=128, n_heads=4)\n",
    "dummy_input = torch.randn(1, 65, 128)  # 64 patches + 1 CLS token\n",
    "out = mha(dummy_input)\n",
    "print(f\"Input:  {dummy_input.shape}\")\n",
    "print(f\"Output: {out.shape}\")\n",
    "print(f\"Attention weights shape: {mha.attn_weights.shape}\")\n",
    "print(f\"  -> {mha.attn_weights.shape[1]} heads, each computing {mha.attn_weights.shape[2]}x{mha.attn_weights.shape[3]} attention matrix\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Transformer Block\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_transformer_block.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_transformer_block"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Transformer Block\n",
    "\n",
    "A single transformer block combines self-attention with a feed-forward network (FFN), plus residual connections and layer normalization. The residual connections are critical â€” they allow gradients to flow directly through the network, making it possible to stack many layers.\n",
    "\n",
    "We use the **pre-norm** architecture (LayerNorm before attention/FFN), which is the standard in modern Vision Transformers. It tends to train more stably than post-norm."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A single transformer encoder block: LayerNorm + Attention + Residual + LayerNorm + FFN + Residual.\"\"\"\n",
    "    def __init__(self, embed_dim, n_heads, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = MultiHeadAttention(embed_dim, n_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-norm: normalize before attention and FFN\n",
    "        x = x + self.attention(self.norm1(x))  # Residual + Attention\n",
    "        x = x + self.mlp(self.norm2(x))        # Residual + FFN\n",
    "        return x\n",
    "\n",
    "# Quick test\n",
    "block = TransformerBlock(embed_dim=128, n_heads=4)\n",
    "out = block(dummy_input)\n",
    "print(f\"Input:  {dummy_input.shape}\")\n",
    "print(f\"Output: {out.shape} (same shape â€” transformer preserves dimensions)\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Complete Vit\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_complete_vit.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_11_complete_vit"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Complete Vision Transformer\n",
    "\n",
    "Now we have all the pieces. Let us assemble the full ViT: Patch Embedding, Positional Encoding, a stack of Transformer Blocks, and a classification head."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleViT(nn.Module):\n",
    "    \"\"\"A simple Vision Transformer for image classification.\n",
    "\n",
    "    Architecture:\n",
    "        Image -> PatchEmbedding -> PositionalEncoding -> N x TransformerBlock\n",
    "              -> LayerNorm -> CLS token -> Linear classifier -> logits\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3,\n",
    "                 n_classes=10, embed_dim=128, n_heads=4, n_layers=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        n_patches = self.patch_embed.n_patches\n",
    "        self.pos_encoding = PositionalEncoding(n_patches, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Stack of transformer blocks\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[TransformerBlock(embed_dim, n_heads, dropout=dropout)\n",
    "              for _ in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.classifier = nn.Linear(embed_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)       # (batch, n_patches, embed_dim)\n",
    "        x = self.pos_encoding(x)      # (batch, n_patches+1, embed_dim) â€” CLS prepended\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)       # (batch, n_patches+1, embed_dim)\n",
    "        x = self.norm(x)\n",
    "        cls_output = x[:, 0]          # Take CLS token output\n",
    "        logits = self.classifier(cls_output)  # (batch, n_classes)\n",
    "        return logits\n",
    "\n",
    "# Create model and inspect\n",
    "model = SimpleViT().to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters:     {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nArchitecture summary:\")\n",
    "print(f\"  Patch size: 4x4\")\n",
    "print(f\"  Number of patches: 64\")\n",
    "print(f\"  Embedding dim: 128\")\n",
    "print(f\"  Attention heads: 4\")\n",
    "print(f\"  Transformer layers: 4\")\n",
    "print(f\"  Sequence length: 65 (64 patches + 1 CLS token)\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad â€” we have a complete Vision Transformer in under 100 lines of code. In a real autonomous driving VLA like RT-2 or OpenVLA, this same architecture (scaled up significantly) processes each camera image and produces the feature tokens that the language model reads. The concept is identical."
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo1\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_todo1.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_12_todo1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "Now it is your turn to implement two core operations from scratch. These exercises will cement your understanding of what is happening inside the ViT.\n",
    "\n",
    "### TODO 1: Implement Scaled Dot-Product Attention from Scratch\n",
    "\n",
    "We saw the attention formula earlier: $\\text{softmax}(QK^T / \\sqrt{d_k}) \\cdot V$. Now implement it yourself. No peeking at the `MultiHeadAttention` class above!"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention from scratch.\n",
    "\n",
    "    Args:\n",
    "        Q: Query tensor of shape (batch, heads, seq_len, head_dim)\n",
    "        K: Key tensor of shape (batch, heads, seq_len, head_dim)\n",
    "        V: Value tensor of shape (batch, heads, seq_len, head_dim)\n",
    "\n",
    "    Returns:\n",
    "        output: (batch, heads, seq_len, head_dim)\n",
    "        attn_weights: (batch, heads, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Compute attention scores: Q @ K^T (transpose last two dims of K)\n",
    "    # Step 2: Scale by sqrt(d_k)\n",
    "    # Step 3: Apply softmax over the last dimension (each query attends to all keys)\n",
    "    # Step 4: Multiply attention weights by V to get weighted values\n",
    "    # ==============================\n",
    "\n",
    "    scores = ???       # YOUR CODE HERE â€” shape: (batch, heads, seq_len, seq_len)\n",
    "    attn_weights = ??? # YOUR CODE HERE â€” shape: (batch, heads, seq_len, seq_len)\n",
    "    output = ???       # YOUR CODE HERE â€” shape: (batch, heads, seq_len, head_dim)\n",
    "\n",
    "    return output, attn_weights"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification cell â€” run this to check your implementation\n",
    "Q_test = torch.randn(1, 2, 4, 8)\n",
    "K_test = torch.randn(1, 2, 4, 8)\n",
    "V_test = torch.randn(1, 2, 4, 8)\n",
    "\n",
    "out, weights = scaled_dot_product_attention(Q_test, K_test, V_test)\n",
    "assert out.shape == (1, 2, 4, 8), f\"Wrong output shape: {out.shape}, expected (1, 2, 4, 8)\"\n",
    "assert weights.shape == (1, 2, 4, 4), f\"Wrong attention shape: {weights.shape}, expected (1, 2, 4, 4)\"\n",
    "assert torch.allclose(weights.sum(dim=-1), torch.ones(1, 2, 4), atol=1e-5), \\\n",
    "    \"Attention weights don't sum to 1 along the last dimension\"\n",
    "print(\"Your attention implementation is correct!\")\n",
    "print(f\"  Output shape: {out.shape}\")\n",
    "print(f\"  Attention weights shape: {weights.shape}\")\n",
    "print(f\"  Weights sum per query: {weights.sum(dim=-1)[0, 0]} (should be all 1.0)\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo2\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_todo2.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_13_todo2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement Patch Embedding Without the Conv2d Trick\n",
    "\n",
    "In our `PatchEmbedding` class, we used a Conv2d shortcut. But what is it actually doing? Let us implement it the explicit way: manually extract patches, flatten them, and apply a linear projection."
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_patch_embed(image, patch_size, projection_weight, projection_bias):\n",
    "    \"\"\"\n",
    "    Manually extract patches and project them â€” no Conv2d shortcut.\n",
    "\n",
    "    Args:\n",
    "        image: (batch, channels, height, width)\n",
    "        patch_size: int â€” size of each square patch\n",
    "        projection_weight: (embed_dim, patch_dim) where patch_dim = channels * patch_size^2\n",
    "        projection_bias: (embed_dim,)\n",
    "\n",
    "    Returns:\n",
    "        patch_embeddings: (batch, n_patches, embed_dim)\n",
    "    \"\"\"\n",
    "    batch, channels, height, width = image.shape\n",
    "    n_patches_h = height // patch_size\n",
    "    n_patches_w = width // patch_size\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Extract patches using unfold or manual slicing\n",
    "    #         Hint: image.unfold(dim, size, step) extracts sliding windows\n",
    "    #         You need to unfold along both height (dim=2) and width (dim=3)\n",
    "    #         Then reshape so each patch is flattened to (channels * patch_size^2,)\n",
    "    # Step 2: Apply linear projection: patches @ weight^T + bias\n",
    "    # ==============================\n",
    "\n",
    "    patches = ???       # YOUR CODE HERE â€” shape: (batch, n_patches, patch_dim)\n",
    "    embeddings = ???    # YOUR CODE HERE â€” shape: (batch, n_patches, embed_dim)\n",
    "\n",
    "    return embeddings"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification cell\n",
    "test_img = torch.randn(2, 3, 32, 32)\n",
    "embed_dim = 128\n",
    "patch_size = 4\n",
    "patch_dim = 3 * patch_size * patch_size  # 48\n",
    "W = torch.randn(embed_dim, patch_dim)\n",
    "b = torch.randn(embed_dim)\n",
    "\n",
    "result = manual_patch_embed(test_img, patch_size=patch_size, projection_weight=W, projection_bias=b)\n",
    "assert result.shape == (2, 64, 128), f\"Wrong shape: {result.shape}, expected (2, 64, 128)\"\n",
    "\n",
    "# Bonus: verify it gives same result as Conv2d\n",
    "conv = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size, bias=True)\n",
    "with torch.no_grad():\n",
    "    conv.weight.copy_(W.reshape(embed_dim, 3, patch_size, patch_size))\n",
    "    conv.bias.copy_(b)\n",
    "conv_result = conv(test_img).flatten(2).transpose(1, 2)\n",
    "manual_result = manual_patch_embed(test_img, patch_size, W, b)\n",
    "match = torch.allclose(conv_result, manual_result, atol=1e-5)\n",
    "print(f\"Manual patch embedding works! Shape: {result.shape}\")\n",
    "print(f\"Matches Conv2d output: {match}\")\n",
    "if match:\n",
    "    print(\"You now understand exactly what Conv2d does under the hood.\")"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_14_training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training on CIFAR-10\n",
    "\n",
    "Enough building â€” let us train our Vision Transformer and see how it performs. We will use CIFAR-10: 60,000 tiny 32x32 images across 10 classes (airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, trucks). It is small enough to train quickly on a T4 GPU but complex enough to be a meaningful test."
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data with augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset):,}\")\n",
    "print(f\"Test samples:     {len(test_dataset):,}\")\n",
    "print(f\"Classes: {cifar_classes}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop â€” should take ~5 minutes on a T4 GPU\n",
    "model = SimpleViT(\n",
    "    img_size=32, patch_size=4, in_channels=3,\n",
    "    n_classes=10, embed_dim=128, n_heads=4, n_layers=4\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses = []\n",
    "test_accs = []\n",
    "\n",
    "print(\"Training SimpleViT on CIFAR-10...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for epoch in range(10):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = logits.max(1)\n",
    "        correct_train += predicted.eq(labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_acc = 100. * correct_train / total_train\n",
    "    train_losses.append(avg_loss)\n",
    "    scheduler.step()\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            _, predicted = logits.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = 100. * correct / total\n",
    "    test_accs.append(acc)\n",
    "    print(f\"Epoch {epoch+1:2d}/10 | Loss: {avg_loss:.4f} | Train Acc: {train_acc:.1f}% | Test Acc: {acc:.1f}%\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Final test accuracy: {test_accs[-1]:.1f}%\")"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a small ViT trained from scratch on only 50,000 images with no pretraining, getting around 70-75% accuracy is reasonable. State-of-the-art ViTs pretrained on ImageNet-21k and fine-tuned on CIFAR-10 achieve over 99%, but they have orders of magnitude more parameters and training data. The important thing is that our model *learns* â€” the attention mechanism works."
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Training Results\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/15_training_results.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_15_training_results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(range(1, 11), train_losses, 'b-o', linewidth=2, markersize=6)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Training Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss over Epochs', fontsize=13)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(range(1, 11), test_accs, 'r-o', linewidth=2, markersize=6)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Test Accuracy over Epochs', fontsize=13)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best test accuracy: {max(test_accs):.1f}% (epoch {test_accs.index(max(test_accs))+1})\")"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly what we want â€” the loss decreases steadily and accuracy improves. Our Vision Transformer is learning to classify images by attending to informative patches."
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Attention Heatmaps\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/16_attention_heatmaps.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_16_attention_heatmaps"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Output: Attention Heatmaps\n",
    "\n",
    "This is the most exciting part. Let us look *inside* the trained model and see where it focuses its attention. We will extract the attention weights from the last transformer layer and visualize which patches the CLS token attends to. This tells us which parts of the image the model considers most important for classification.\n",
    "\n",
    "In a real driving VLA, these same attention patterns would reveal which parts of the road scene the model focuses on â€” brake lights, lane markings, pedestrians, traffic signs."
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention maps from the trained model\n",
    "model.eval()\n",
    "\n",
    "# Get a batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "test_images, test_labels = next(dataiter)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "# Register forward hooks to capture attention weights from each layer\n",
    "attention_maps = []\n",
    "def hook_fn(module, input, output):\n",
    "    if hasattr(module, 'attn_weights') and module.attn_weights is not None:\n",
    "        attention_maps.append(module.attn_weights.cpu())\n",
    "\n",
    "hooks = []\n",
    "for block in model.transformer:\n",
    "    hooks.append(block.attention.register_forward_hook(hook_fn))\n",
    "\n",
    "# Forward pass to trigger hooks\n",
    "with torch.no_grad():\n",
    "    logits = model(test_images)\n",
    "    predictions = logits.argmax(dim=-1).cpu()\n",
    "\n",
    "# Clean up hooks\n",
    "for h in hooks:\n",
    "    h.remove()\n",
    "\n",
    "print(f\"Captured attention maps from {len(attention_maps)} transformer layers\")\n",
    "print(f\"Each attention map shape: {attention_maps[0].shape}\")\n",
    "print(f\"  -> (batch={attention_maps[0].shape[0]}, heads={attention_maps[0].shape[1]}, \"\n",
    "      f\"seq_len={attention_maps[0].shape[2]}, seq_len={attention_maps[0].shape[3]})\")"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention heatmaps for 4 sample images\n",
    "from PIL import Image\n",
    "\n",
    "# Unnormalize images for display\n",
    "unnorm = transforms.Normalize(\n",
    "    mean=[-0.4914/0.2470, -0.4822/0.2435, -0.4465/0.2616],\n",
    "    std=[1/0.2470, 1/0.2435, 1/0.2616]\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(12, 14))\n",
    "column_titles = ['Input Image', 'Attention Heatmap\\n(CLS token, last layer)', 'Attention Overlay']\n",
    "\n",
    "for col, title in enumerate(column_titles):\n",
    "    axes[0, col].set_title(title, fontsize=12, fontweight='bold')\n",
    "\n",
    "for idx in range(4):\n",
    "    # --- Column 1: Original image ---\n",
    "    img = unnorm(test_images[idx].cpu()).clamp(0, 1)\n",
    "    axes[idx, 0].imshow(img.permute(1, 2, 0).numpy())\n",
    "    pred_class = cifar_classes[predictions[idx]]\n",
    "    true_class = cifar_classes[test_labels[idx]]\n",
    "    color = 'green' if pred_class == true_class else 'red'\n",
    "    axes[idx, 0].set_xlabel(f'True: {true_class} | Pred: {pred_class}',\n",
    "                            fontsize=10, color=color)\n",
    "    axes[idx, 0].set_xticks([])\n",
    "    axes[idx, 0].set_yticks([])\n",
    "\n",
    "    # --- Column 2: Attention heatmap ---\n",
    "    # Last layer attention: CLS token (position 0) attending to all patches (positions 1+)\n",
    "    last_attn = attention_maps[-1][idx]  # (heads, seq_len, seq_len)\n",
    "    cls_attn = last_attn[:, 0, 1:]       # (heads, n_patches) â€” CLS attending to patches\n",
    "    avg_attn = cls_attn.mean(0)           # Average across heads -> (n_patches,)\n",
    "\n",
    "    # Reshape to 2D spatial grid\n",
    "    n_patches_side = int(avg_attn.shape[0] ** 0.5)\n",
    "    attn_map = avg_attn.reshape(n_patches_side, n_patches_side).numpy()\n",
    "\n",
    "    axes[idx, 1].imshow(attn_map, cmap='hot', interpolation='bilinear')\n",
    "    axes[idx, 1].set_xticks([])\n",
    "    axes[idx, 1].set_yticks([])\n",
    "\n",
    "    # --- Column 3: Overlay ---\n",
    "    img_np = img.permute(1, 2, 0).numpy()\n",
    "    # Resize attention map to image size for overlay\n",
    "    attn_resized = np.array(\n",
    "        Image.fromarray((attn_map * 255 / attn_map.max()).astype(np.uint8)).resize(\n",
    "            (32, 32), Image.BILINEAR\n",
    "        )\n",
    "    ) / 255.0\n",
    "\n",
    "    axes[idx, 2].imshow(img_np)\n",
    "    axes[idx, 2].imshow(attn_resized, cmap='hot', alpha=0.5)\n",
    "    axes[idx, 2].set_xticks([])\n",
    "    axes[idx, 2].set_yticks([])\n",
    "\n",
    "plt.suptitle('What Does the Vision Transformer Focus On?', fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The bright/hot regions show where the model pays the most attention.\")\n",
    "print(\"Notice how it tends to focus on the object itself rather than the background.\")\n",
    "print()\n",
    "print(\"In a real driving VLA, these attention patterns would highlight:\")\n",
    "print(\"  - Other vehicles (especially brake lights)\")\n",
    "print(\"  - Lane markings and road boundaries\")\n",
    "print(\"  - Traffic signs and signals\")\n",
    "print(\"  - Pedestrians and cyclists\")"
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: compare attention patterns across different heads\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 6))\n",
    "\n",
    "sample_idx = 0\n",
    "img = unnorm(test_images[sample_idx].cpu()).clamp(0, 1)\n",
    "\n",
    "# Show original image\n",
    "axes[0, 0].imshow(img.permute(1, 2, 0).numpy())\n",
    "axes[0, 0].set_title(f'Original\\n({cifar_classes[test_labels[sample_idx]]})', fontsize=10)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Show each head's attention from the last layer\n",
    "last_attn = attention_maps[-1][sample_idx]  # (heads, seq_len, seq_len)\n",
    "n_heads = last_attn.shape[0]\n",
    "\n",
    "for head in range(n_heads):\n",
    "    row = (head + 1) // 4\n",
    "    col = (head + 1) % 4\n",
    "\n",
    "    head_attn = last_attn[head, 0, 1:]  # CLS attention to patches\n",
    "    n_side = int(head_attn.shape[0] ** 0.5)\n",
    "    attn_grid = head_attn.reshape(n_side, n_side).numpy()\n",
    "\n",
    "    axes[row, col].imshow(attn_grid, cmap='hot', interpolation='bilinear')\n",
    "    axes[row, col].set_title(f'Head {head + 1}', fontsize=10)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "# Clear unused subplots\n",
    "for i in range(n_heads + 1, 8):\n",
    "    row = i // 4\n",
    "    col = i % 4\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle('Different Attention Heads Focus on Different Things', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each head learns a different attention pattern:\")\n",
    "print(\"  Some heads might focus on edges and contours\")\n",
    "print(\"  Others might attend to color regions or textures\")\n",
    "print(\"  This diversity is exactly why multi-head attention is powerful\")"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is truly amazing. With just a few hundred lines of code, we have built a model that learns to \"see\" â€” it automatically discovers which parts of an image are important, just like a human driver scanning the road."
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Connecting To Driving\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/17_connecting_to_driving.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_17_connecting_to_driving"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Connecting to Autonomous Driving VLAs\n",
    "\n",
    "Let us take a step back and connect what we built to how real autonomous driving VLAs work.\n",
    "\n",
    "In a production VLA like RT-2, OpenVLA, or DriveVLM:\n",
    "\n",
    "1. **Multiple cameras** (4-8) capture 360-degree views around the vehicle\n",
    "2. **Each camera image** is processed by a Vision Transformer (much larger than ours â€” typically ViT-Large or ViT-Huge with ~300M-600M parameters)\n",
    "3. The ViT produces **feature tokens** â€” one per patch, exactly like our model\n",
    "4. These tokens are **concatenated** across all cameras, giving the language model a complete visual representation of the driving scene\n",
    "5. The language model processes these visual tokens alongside text instructions (e.g., \"turn left at the next intersection\") and outputs **driving actions** (steering angle, acceleration, braking)\n",
    "\n",
    "The patch embedding and attention mechanisms we implemented are the exact same operations running in these production systems. The only differences are scale (more parameters, higher resolution images, larger patches) and the downstream components (language model + action head instead of a simple classifier)."
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale comparison: our toy ViT vs. real driving ViTs\n",
    "print(\"=\" * 60)\n",
    "print(\"Scale Comparison: Our ViT vs. Production Driving ViTs\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'':20s} {'Ours':>15s} {'ViT-Large':>15s} {'ViT-Huge':>15s}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Image size':20s} {'32x32':>15s} {'224x224':>15s} {'336x336':>15s}\")\n",
    "print(f\"{'Patch size':20s} {'4x4':>15s} {'16x16':>15s} {'14x14':>15s}\")\n",
    "print(f\"{'Num patches':20s} {'64':>15s} {'196':>15s} {'576':>15s}\")\n",
    "print(f\"{'Embed dim':20s} {'128':>15s} {'1024':>15s} {'1280':>15s}\")\n",
    "print(f\"{'Heads':20s} {'4':>15s} {'16':>15s} {'16':>15s}\")\n",
    "print(f\"{'Layers':20s} {'4':>15s} {'24':>15s} {'32':>15s}\")\n",
    "print(f\"{'Parameters':20s} {f'{total_params:,}':>15s} {'304M':>15s} {'632M':>15s}\")\n",
    "print(f\"{'Cameras':20s} {'1':>15s} {'6-8':>15s} {'6-8':>15s}\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\\nThe concepts are identical â€” only the scale changes.\")\n",
    "print(\"You now understand the core of how VLAs process visual input.\")"
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/18_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_18_closing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "\n",
    "1. **Patch size tradeoff:** We used 4x4 patches on 32x32 images (64 patches). What would change if we used 2x2 patches (256 patches)? We would get finer spatial resolution â€” each patch captures more localized details. But the attention computation grows quadratically: 256^2 = 65,536 vs. 64^2 = 4,096. That is a 16x increase in attention cost. There is a fundamental tradeoff between resolution and computational cost.\n",
    "\n",
    "2. **Why CLS token?** Why use a special CLS token instead of, say, averaging all patch embeddings? In the first transformer layer, before the model has learned anything useful, averaging random patch representations would give a noisy mess. The CLS token starts as a learnable \"query\" that progressively accumulates information from all patches through attention. It acts as a dedicated summarization slot. That said, some recent ViT variants do use global average pooling instead of CLS, and they work well too.\n",
    "\n",
    "3. **Driving scene attention patterns:** Would you expect the attention pattern for a highway scene to look different from a city intersection? Absolutely. On a highway, the model would likely attend strongly to the lane markings, the car directly ahead, and the road curvature. At a city intersection, attention would be much more distributed â€” pedestrians, traffic lights, crossing vehicles, turn lanes. This is the beauty of self-attention: it adapts dynamically to the scene.\n",
    "\n",
    "### Optional Challenges\n",
    "\n",
    "1. **Larger patches:** Change `patch_size` to 8 (giving only 16 patches) and retrain. How does accuracy change? What do the attention maps look like with so few patches?\n",
    "\n",
    "2. **More heads:** Try 8 heads instead of 4. Visualize each head's attention pattern separately â€” do different heads learn to focus on different visual features?\n",
    "\n",
    "3. **Multi-camera fusion:** This is closer to real driving VLAs. Modify the model to process 4 images simultaneously (simulating front, left, right, and rear cameras). Concatenate their patch tokens into a single sequence before the transformer layers. Does the attention mechanism learn cross-camera relationships?"
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick starter for Challenge 1\n",
    "print(\"Challenge 1: Larger patches\")\n",
    "print(\"Change patch_size from 4 to 8:\")\n",
    "print(\"  - Patches: 64 -> 16\")\n",
    "print(\"  - Each patch covers: 4x4=16 pixels -> 8x8=64 pixels\")\n",
    "print(\"  - Attention matrix: 65x65 -> 17x17 (much smaller)\")\n",
    "print()\n",
    "print(\"Try it: model_8 = SimpleViT(patch_size=8).to(device)\")\n",
    "print(\"Then retrain and compare accuracy + attention maps\")"
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is it for Part 1. You have built a complete Vision Transformer from scratch, trained it, and visualized its attention patterns. You now understand the first and most critical component of how VLAs \"see\" the road.\n",
    "\n",
    "In production autonomous driving systems, everything we built here runs at scale â€” larger images, more cameras, bigger models â€” but the fundamental operations are identical: split images into patches, embed them, add positions, run self-attention, and produce feature tokens that downstream models can understand.\n",
    "\n",
    "Thanks for working through this notebook!"
   ],
   "id": "cell_46"
  }
 ]
}