{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Building a Mini VLA (RT-2 Style) ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1rJ_FlomWsaD7qy9Uby4XRTEnGrgSi6F1\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Mini VLA: Unifying Vision, Language, and Action\n",
    "\n",
    "*Part 3 of the Vizuara series on VLAs for Autonomous Driving*\n",
    "*Estimated time: 75 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ü§ñ AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** ‚Äî it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[üëâ Open AI Teaching Assistant](https://pods.vizuara.ai/courses/vla-autonomous-driving/practice/3/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_why_it_matters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Let us start with the big picture. In the previous notebooks, we built two separate components: a vision encoder that converts camera images into compact feature tokens, and an action tokenizer that converts continuous steering angles into discrete tokens. Both are powerful on their own, but neither can drive a car.\n",
    "\n",
    "Why? Because driving requires all three capabilities **simultaneously**: the model must **see** the road (vision), **understand** what is happening and what to do (language), and **act** on that understanding by outputting a trajectory (action). A vision encoder alone sees objects but cannot reason about them. An action tokenizer alone produces outputs but has no idea what is happening on the road.\n",
    "\n",
    "What if we could build **one single model** that takes in a camera image, reads a text command like \"go to the red circle,\" and directly outputs a driving trajectory? This is precisely what a Vision-Language-Action (VLA) model does. This is the same principle behind RT-2 (Google DeepMind), EMMA (Waymo), and NVIDIA's Alpamayo -- the most advanced driving models in the world.\n",
    "\n",
    "By the end of this notebook, you will have built a complete mini VLA from scratch. You will feed it an image and a text command, and it will output a trajectory. Let us get started."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Building Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_building_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_building_intuition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Think of a multilingual translator who can also cook. They can read a recipe in French (text), look at photos of the ingredients (vision), and then actually prepare the dish (action). They do not need three separate brains for reading, seeing, and cooking -- it all happens in one integrated mind where each capability informs the others. If the recipe says \"sear until golden brown,\" they **look** at the pan to judge the color, **understand** what \"golden brown\" means from the text, and **act** by flipping the food at the right moment.\n",
    "\n",
    "A VLA model works exactly the same way. It \"reads\" the road scene through its vision encoder, \"understands\" the driving instruction through its language backbone, and \"cooks up\" a trajectory through its action decoder. All in one unified architecture.\n",
    "\n",
    "Now here is the key insight from RT-2 that makes this unification so elegant. If we put action tokens into the same vocabulary as words, then generating an action is **literally the same computation** as generating the next word in a sentence. The model does not know the difference between outputting the word \"turn\" and outputting the action token for \"turn the wheel 15 degrees.\" To the transformer, both are just the next token to predict. This is why driving IS language -- the same attention mechanism, the same softmax, the same autoregressive generation loop.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "Why is it valuable to have **one model** that handles vision, language, AND action? Why not three separate specialists?\n",
    "\n",
    "Consider two reasons:\n",
    "\n",
    "1. **Shared representations.** When the model sees a red traffic light, the visual feature for \"red light\" can directly inform both the language understanding (\"the traffic light is red, I should stop\") and the action output (\"apply brakes, decelerate to zero\"). With separate specialists, you would need to explicitly pass messages between them -- and any miscommunication means errors cascade.\n",
    "\n",
    "2. **Emergent capabilities.** A unified model can generalize to novel commands it has never seen during training. If you tell RT-2 to \"pick up the object closest to the blue ball\" -- an instruction it was never trained on -- it can do it. The vision-language grounding and action generation work together to handle novel combinations. Three separate specialists could never achieve this."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Math Cross Attention\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_math_cross_attention.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_math_cross_attention"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "Before we write any code, let us understand the two key mathematical operations that make a VLA tick.\n",
    "\n",
    "### 3.1 Cross-Attention: How Vision and Language Talk to Each Other\n",
    "\n",
    "The core fusion mechanism in a VLA is **cross-attention**. The idea is simple: the text tokens \"ask questions\" about the image, and the visual tokens provide the answers.\n",
    "\n",
    "$$\\text{CrossAttn}(Q_{\\text{text}}, K_{\\text{vis}}, V_{\\text{vis}}) = \\text{softmax}\\left(\\frac{Q_{\\text{text}} K_{\\text{vis}}^T}{\\sqrt{d_k}}\\right) V_{\\text{vis}}$$\n",
    "\n",
    "Let us break this down:\n",
    "\n",
    "- $Q_{\\text{text}}$ are the **query** vectors from the text encoder. Think of each query as a question: \"Is there a red light in the scene?\"\n",
    "- $K_{\\text{vis}}$ are the **key** vectors from the vision encoder. Each key represents what a particular image patch contains.\n",
    "- $V_{\\text{vis}}$ are the **value** vectors from the vision encoder. These carry the actual visual information to transfer.\n",
    "- $d_k$ is the dimension of the key vectors. We divide by $\\sqrt{d_k}$ to keep the numbers from getting too large.\n",
    "\n",
    "Let us plug in some simple numbers to see how this works. Suppose we have 2 text tokens and 3 visual tokens, each with dimension $d_k = 4$:\n",
    "\n",
    "$$Q_{\\text{text}} = \\begin{bmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\end{bmatrix}, \\quad K_{\\text{vis}} = \\begin{bmatrix} 1 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 \\\\ 1 & 0 & 1 & 0 \\end{bmatrix}$$\n",
    "\n",
    "Step 1: Compute $Q K^T$ (each text token scores against each visual token):\n",
    "\n",
    "$$Q K^T = \\begin{bmatrix} 1 \\cdot 1 + 0 \\cdot 1 + 1 \\cdot 0 + 0 \\cdot 0 & \\cdots \\\\ \\cdots & \\cdots \\end{bmatrix} = \\begin{bmatrix} 1 & 1 & 2 \\\\ 1 & 1 & 0 \\end{bmatrix}$$\n",
    "\n",
    "Step 2: Scale by $\\sqrt{d_k} = \\sqrt{4} = 2$:\n",
    "\n",
    "$$\\frac{Q K^T}{\\sqrt{d_k}} = \\begin{bmatrix} 0.5 & 0.5 & 1.0 \\\\ 0.5 & 0.5 & 0.0 \\end{bmatrix}$$\n",
    "\n",
    "Step 3: Apply softmax across visual tokens (each row sums to 1):\n",
    "\n",
    "$$\\text{weights} \\approx \\begin{bmatrix} 0.26 & 0.26 & 0.43 \\\\ 0.36 & 0.36 & 0.22 \\end{bmatrix}$$\n",
    "\n",
    "This tells us that text token 1 attends most strongly to visual token 3 (weight 0.43), while text token 2 attends equally to visual tokens 1 and 2. The text is \"looking\" at the parts of the image most relevant to its query. This is exactly what we want.\n",
    "\n",
    "### 3.2 Autoregressive Action Generation\n",
    "\n",
    "The second key idea is how the model generates actions. Just like GPT generates one word at a time, a VLA generates one action token at a time:\n",
    "\n",
    "$$p(a_{1:T} \\mid I, \\ell) = \\prod_{t=1}^{T} p_\\theta(a_t \\mid a_{<t}, I, \\ell)$$\n",
    "\n",
    "Here $I$ is the image, $\\ell$ is the language command, and $a_{1:T}$ is the sequence of action tokens (e.g., trajectory waypoints). Each action $a_t$ is conditioned on the image, the text, AND all previous actions.\n",
    "\n",
    "Let us plug in numbers. Suppose we generate $T = 3$ waypoint tokens, and at each step the model assigns probabilities:\n",
    "\n",
    "- $p(a_1 \\mid I, \\ell) = 0.7$ -- first waypoint, fairly confident\n",
    "- $p(a_2 \\mid a_1, I, \\ell) = 0.9$ -- second waypoint, very confident given the first\n",
    "- $p(a_3 \\mid a_1, a_2, I, \\ell) = 0.6$ -- third waypoint, less certain\n",
    "\n",
    "The probability of the full trajectory:\n",
    "\n",
    "$$p(a_{1:3} \\mid I, \\ell) = 0.7 \\times 0.9 \\times 0.6 = 0.378$$\n",
    "\n",
    "During training, we maximize this probability for expert trajectories. A model that assigns higher probability to expert actions will produce better driving trajectories at inference time.\n",
    "\n",
    "In our mini VLA, we will simplify this by predicting all waypoints at once (regression instead of autoregressive). But the principle remains: image + text goes in, trajectory comes out."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Lets Build\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_lets_build.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_lets_build"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let us Build It -- Component by Component\n",
    "\n",
    "Now let us roll up our sleeves and build our mini VLA. We will construct each piece separately, verify it works, and then assemble everything into a unified model.\n",
    "\n",
    "### 4.0 Setup"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Synthetic Dataset\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_synthetic_dataset.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_synthetic_dataset"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create a Synthetic Multimodal Dataset\n",
    "\n",
    "We need data with three modalities: images, text commands, and target trajectories. We will create a simple but illustrative synthetic dataset: colored shapes on a canvas, text commands like \"go to red circle\" or \"go to blue square,\" and target trajectories that move from the center toward the named object.\n",
    "\n",
    "This is a simplified version of what real driving VLAs face -- they see a scene (image), receive an instruction (\"turn left at the intersection\"), and must produce a trajectory to execute it."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scene(img_size=32, n_objects=3):\n",
    "    \"\"\"\n",
    "    Generate a synthetic scene with colored shapes.\n",
    "    Returns the image tensor and a list of object metadata.\n",
    "    \"\"\"\n",
    "    image = torch.zeros(3, img_size, img_size)\n",
    "    objects = []\n",
    "\n",
    "    colors = {\n",
    "        'red': torch.tensor([1.0, 0.0, 0.0]),\n",
    "        'green': torch.tensor([0.0, 1.0, 0.0]),\n",
    "        'blue': torch.tensor([0.0, 0.0, 1.0]),\n",
    "    }\n",
    "    shapes = ['circle', 'square']\n",
    "\n",
    "    for _ in range(n_objects):\n",
    "        color_name = np.random.choice(list(colors.keys()))\n",
    "        shape = np.random.choice(shapes)\n",
    "        cx = np.random.randint(6, img_size - 6)\n",
    "        cy = np.random.randint(6, img_size - 6)\n",
    "        color_val = colors[color_name]\n",
    "\n",
    "        # Draw the shape on the image\n",
    "        for dx in range(-3, 4):\n",
    "            for dy in range(-3, 4):\n",
    "                px, py = cx + dx, cy + dy\n",
    "                if 0 <= px < img_size and 0 <= py < img_size:\n",
    "                    if shape == 'circle' and dx*dx + dy*dy <= 9:\n",
    "                        image[:, py, px] = color_val\n",
    "                    elif shape == 'square':\n",
    "                        image[:, py, px] = color_val\n",
    "\n",
    "        objects.append({\n",
    "            'color': color_name, 'shape': shape,\n",
    "            'x': cx / img_size, 'y': cy / img_size  # Normalized to [0, 1]\n",
    "        })\n",
    "\n",
    "    return image, objects\n",
    "\n",
    "\n",
    "def generate_command_and_target(objects):\n",
    "    \"\"\"\n",
    "    Pick a random object, create a text command, and generate\n",
    "    a straight-line trajectory from center to that object.\n",
    "    \"\"\"\n",
    "    target_obj = np.random.choice(objects)\n",
    "    command = f\"go to {target_obj['color']} {target_obj['shape']}\"\n",
    "\n",
    "    # Trajectory: 10 waypoints from center (0.5, 0.5) to object position\n",
    "    n_waypoints = 10\n",
    "    start = np.array([0.5, 0.5])\n",
    "    end = np.array([target_obj['x'], target_obj['y']])\n",
    "\n",
    "    t = np.linspace(0, 1, n_waypoints).reshape(-1, 1)\n",
    "    trajectory = start + t * (end - start)\n",
    "\n",
    "    return command, torch.FloatTensor(trajectory)"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize a few examples to make sure our data looks right."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample scenes with commands and trajectories\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(4):\n",
    "    np.random.seed(i + 100)  # Different seeds for variety\n",
    "    image, objects = generate_scene()\n",
    "    command, traj = generate_command_and_target(objects)\n",
    "\n",
    "    # Top row: raw scenes\n",
    "    axes[0, i].imshow(image.permute(1, 2, 0).numpy())\n",
    "    axes[0, i].set_title(f'Scene {i+1}', fontsize=11)\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    # Bottom row: scenes with trajectory overlay\n",
    "    axes[1, i].imshow(image.permute(1, 2, 0).numpy())\n",
    "    traj_np = traj.numpy()\n",
    "    axes[1, i].plot(\n",
    "        traj_np[:, 0] * 32, traj_np[:, 1] * 32,\n",
    "        'w-o', linewidth=2, markersize=4\n",
    "    )\n",
    "    axes[1, i].set_title(f'\"{command}\"', fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Synthetic Scenes: Top = Raw, Bottom = Command + Trajectory\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "np.random.seed(42)  # Reset seed"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see colored shapes (red, green, blue circles and squares) scattered on a black canvas, with white trajectories going from the center to the target object. This is our toy version of \"see a scene, receive an instruction, plan a path.\" Not bad, right?"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Vision Encoder\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_vision_encoder.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_vision_encoder"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Vision Encoder -- How the Model \"Sees\"\n",
    "\n",
    "Our vision encoder converts a raw image into a sequence of **feature tokens** -- compact vector representations that capture what each spatial region of the image contains. In real VLAs like Alpamayo, this would be a Vision Transformer (ViT). For our mini version, we use a simple CNN that produces a grid of spatial features."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniVisionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple CNN-based vision encoder.\n",
    "    Converts a (3, 32, 32) image into 64 spatial tokens of dimension embed_dim.\n",
    "    Think of it as a simplified ViT -- each output token summarizes\n",
    "    a spatial patch of the image.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=32, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.ReLU(),  # 32->16\n",
    "            nn.Conv2d(64, embed_dim, 3, stride=2, padding=1), nn.ReLU(),  # 16->8\n",
    "        )\n",
    "        self.n_tokens = 64  # 8 x 8 spatial grid\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.conv(x)  # (batch, embed_dim, 8, 8)\n",
    "        batch = features.size(0)\n",
    "        # Reshape spatial grid into a sequence of tokens\n",
    "        tokens = features.reshape(batch, features.size(1), -1)  # (batch, embed_dim, 64)\n",
    "        tokens = tokens.permute(0, 2, 1)  # (batch, 64, embed_dim)\n",
    "        return tokens"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check\n",
    "test_encoder = MiniVisionEncoder(embed_dim=64)\n",
    "dummy_img = torch.randn(2, 3, 32, 32)\n",
    "vision_tokens = test_encoder(dummy_img)\n",
    "print(f\"Input image shape:  {dummy_img.shape}\")\n",
    "print(f\"Output tokens shape: {vision_tokens.shape}\")\n",
    "print(f\"Number of spatial tokens: {vision_tokens.shape[1]} (8x8 grid)\")\n",
    "print(f\"Each token dimension: {vision_tokens.shape[2]}\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each 32x32 image becomes 64 tokens (an 8x8 spatial grid), where each token is a 64-dimensional vector summarizing what that region of the image contains. This is analogous to how a ViT converts image patches into token sequences."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Text Encoder\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_text_encoder.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_text_encoder"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Text Encoder -- How the Model \"Understands\"\n",
    "\n",
    "The text encoder converts word sequences into contextual embeddings. In a full VLA, this would be a large language model like LLaMA or Gemma. Our mini version uses a simple embedding layer plus self-attention -- enough to capture which words matter in the command."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniTextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode text commands into contextual embeddings.\n",
    "    Uses embedding + positional encoding + self-attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=50, embed_dim=64, max_len=10):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_len, embed_dim) * 0.02)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads=4, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        x = self.embedding(token_ids)  # (batch, seq_len, embed_dim)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pos_embed[:, :seq_len, :]\n",
    "        attn_out, _ = self.attn(x, x, x)  # Self-attention\n",
    "        x = self.norm(x + attn_out)\n",
    "        return x  # (batch, seq_len, embed_dim)"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a simple tokenizer to convert text commands into integer IDs."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our vocabulary: all words that can appear in commands\n",
    "vocab = {\n",
    "    '<pad>': 0, 'go': 1, 'to': 2, 'avoid': 3,\n",
    "    'red': 4, 'green': 5, 'blue': 6,\n",
    "    'circle': 7, 'square': 8, 'the': 9\n",
    "}\n",
    "\n",
    "def tokenize_command(command, vocab, max_len=10):\n",
    "    \"\"\"Convert a text command to padded token IDs.\"\"\"\n",
    "    words = command.lower().split()\n",
    "    tokens = [vocab.get(w, 0) for w in words]\n",
    "    tokens = tokens[:max_len] + [0] * max(0, max_len - len(tokens))\n",
    "    return torch.LongTensor(tokens)\n",
    "\n",
    "# Test it\n",
    "cmd = \"go to red circle\"\n",
    "tokens = tokenize_command(cmd, vocab)\n",
    "print(f'Command: \"{cmd}\"')\n",
    "print(f'Token IDs: {tokens.tolist()}')\n",
    "print(f'Decoded: {[k for t in tokens.tolist() for k, v in vocab.items() if v == t][:4]}')"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Cross Attention Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_cross_attention_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_cross_attention_code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Cross-Attention -- Where Vision Meets Language\n",
    "\n",
    "This is the heart of a VLA. Cross-attention allows the text features to \"look at\" the visual features and extract the information they need. When the command says \"go to **red circle**,\" the word \"red\" should attend strongly to image patches that contain red objects."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Text tokens attend to visual tokens.\n",
    "    Q = text features (the questions)\n",
    "    K, V = visual features (the answers)\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, n_heads, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, text_tokens, vision_tokens):\n",
    "        # Text queries attend to vision keys and values\n",
    "        attn_out, attn_weights = self.attn(\n",
    "            text_tokens, vision_tokens, vision_tokens\n",
    "        )\n",
    "        # Residual connection + layer norm\n",
    "        fused = self.norm(text_tokens + attn_out)\n",
    "        return fused, attn_weights"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output has the same shape as `text_tokens`, but now each text token carries information from the image. The word \"red\" now \"knows\" where the red objects are. The word \"circle\" now \"knows\" which patches are circular. This is how the model builds a joint vision-language representation."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Action Decoder\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_action_decoder.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_action_decoder"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Action Decoder -- How the Model \"Acts\"\n",
    "\n",
    "The action decoder takes the fused vision-language features and produces trajectory waypoints. In RT-2 and EMMA, this would be autoregressive token generation. In Alpamayo, this would be a diffusion decoder. For our mini VLA, we use a simple MLP that regresses waypoint coordinates directly -- conceptually the same idea, just simpler."
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decode fused vision-language features into trajectory waypoints.\n",
    "    Pools the fused sequence and maps to (n_waypoints, 2) coordinates.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, n_waypoints=10, action_dim=2):\n",
    "        super().__init__()\n",
    "        self.n_waypoints = n_waypoints\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_waypoints * action_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, fused_features):\n",
    "        # Mean-pool over the sequence dimension\n",
    "        pooled = fused_features.mean(dim=1)  # (batch, embed_dim)\n",
    "        waypoints = self.decoder(pooled)  # (batch, n_waypoints * 2)\n",
    "        waypoints = waypoints.reshape(-1, self.n_waypoints, 2)\n",
    "        return waypoints  # (batch, n_waypoints, 2)"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Full Vla Assembly\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_full_vla_assembly.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_full_vla_assembly"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 The Complete Mini VLA\n",
    "\n",
    "Now we assemble all four components into one unified model. This is the moment where vision, language, and action come together."
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniVLA(nn.Module):\n",
    "    \"\"\"\n",
    "    A minimal Vision-Language-Action model.\n",
    "    Image + Text Command --> Trajectory\n",
    "\n",
    "    Pipeline:\n",
    "      1. Vision encoder: image --> visual tokens\n",
    "      2. Text encoder: command --> text features\n",
    "      3. Cross-attention: text features query visual tokens --> fused features\n",
    "      4. Action decoder: fused features --> trajectory waypoints\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=50, embed_dim=64, n_waypoints=10):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = MiniVisionEncoder(embed_dim=embed_dim)\n",
    "        self.text_encoder = MiniTextEncoder(vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "        self.cross_attention = CrossAttention(embed_dim=embed_dim)\n",
    "        self.action_decoder = ActionDecoder(embed_dim=embed_dim, n_waypoints=n_waypoints)\n",
    "\n",
    "    def forward(self, image, text_tokens):\n",
    "        # Step 1: Encode the image into visual tokens\n",
    "        vision_tokens = self.vision_encoder(image)\n",
    "        # Step 2: Encode the text command into contextual features\n",
    "        text_features = self.text_encoder(text_tokens)\n",
    "        # Step 3: Cross-attend -- text looks at the image\n",
    "        fused, attn_weights = self.cross_attention(text_features, vision_tokens)\n",
    "        # Step 4: Decode fused features into a trajectory\n",
    "        trajectory = self.action_decoder(fused)\n",
    "        return trajectory, attn_weights\n",
    "\n",
    "\n",
    "# Instantiate our mini VLA\n",
    "vla = MiniVLA(vocab_size=len(vocab), embed_dim=64, n_waypoints=10).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in vla.parameters())\n",
    "print(f\"Mini VLA total parameters: {total_params:,}\")\n",
    "print(f\"\\nComponent breakdown:\")\n",
    "for name, module in [\n",
    "    ('Vision Encoder', vla.vision_encoder),\n",
    "    ('Text Encoder', vla.text_encoder),\n",
    "    ('Cross-Attention', vla.cross_attention),\n",
    "    ('Action Decoder', vla.action_decoder),\n",
    "]:\n",
    "    n = sum(p.numel() for p in module.parameters())\n",
    "    print(f\"  {name}: {n:,} ({100*n/total_params:.1f}%)\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our mini VLA has around 100K parameters -- tiny compared to Alpamayo's 10.5 billion, but it has the exact same architecture: vision encoder, text encoder, cross-attention fusion, and action decoder. The principles are identical."
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_11_training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Mini VLA\n",
    "\n",
    "Now let us generate a large synthetic dataset and train our model to follow text commands.\n",
    "\n",
    "### 5.1 Generate the Dataset"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training and test data\n",
    "n_samples = 8000\n",
    "images_list, commands_list, trajectories_list = [], [], []\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    img, objs = generate_scene()\n",
    "    cmd, traj = generate_command_and_target(objs)\n",
    "    cmd_tokens = tokenize_command(cmd, vocab)\n",
    "    images_list.append(img)\n",
    "    commands_list.append(cmd_tokens)\n",
    "    trajectories_list.append(traj)\n",
    "\n",
    "all_images = torch.stack(images_list)\n",
    "all_commands = torch.stack(commands_list)\n",
    "all_trajectories = torch.stack(trajectories_list)\n",
    "\n",
    "# Train/test split\n",
    "n_train = 6000\n",
    "train_imgs, test_imgs = all_images[:n_train], all_images[n_train:]\n",
    "train_cmds, test_cmds = all_commands[:n_train], all_commands[n_train:]\n",
    "train_trajs, test_trajs = all_trajectories[:n_train], all_trajectories[n_train:]\n",
    "\n",
    "print(f\"Training samples: {n_train}\")\n",
    "print(f\"Test samples:     {n_samples - n_train}\")\n",
    "print(f\"Image shape:      {all_images.shape}\")\n",
    "print(f\"Command shape:    {all_commands.shape}\")\n",
    "print(f\"Trajectory shape: {all_trajectories.shape}\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Training Loop\n",
    "\n",
    "We train with MSE loss on trajectory waypoints. This is behavioral cloning in its simplest form: the model learns to imitate the \"expert\" trajectories in our dataset."
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vla.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "batch_size = 128\n",
    "n_epochs = 30\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # --- Training ---\n",
    "    vla.train()\n",
    "    epoch_loss = 0\n",
    "    perm = torch.randperm(n_train)\n",
    "    n_batches = 0\n",
    "\n",
    "    for i in range(0, n_train, batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        imgs = train_imgs[idx].to(device)\n",
    "        cmds = train_cmds[idx].to(device)\n",
    "        trajs = train_trajs[idx].to(device)\n",
    "\n",
    "        pred_trajs, _ = vla(imgs, cmds)\n",
    "        loss = F.mse_loss(pred_trajs, trajs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_train_loss = epoch_loss / n_batches\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    vla.eval()\n",
    "    with torch.no_grad():\n",
    "        # Evaluate on test set in batches\n",
    "        test_loss_sum = 0\n",
    "        test_batches = 0\n",
    "        for i in range(0, len(test_imgs), batch_size):\n",
    "            t_imgs = test_imgs[i:i+batch_size].to(device)\n",
    "            t_cmds = test_cmds[i:i+batch_size].to(device)\n",
    "            t_trajs = test_trajs[i:i+batch_size].to(device)\n",
    "            t_pred, _ = vla(t_imgs, t_cmds)\n",
    "            test_loss_sum += F.mse_loss(t_pred, t_trajs).item()\n",
    "            test_batches += 1\n",
    "        avg_test_loss = test_loss_sum / test_batches\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{n_epochs} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.6f} | \"\n",
    "              f\"Test Loss: {avg_test_loss:.6f}\")"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Training Curves\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_training_curves.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_12_training_curves"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(train_losses, 'b-', linewidth=2, label='Train')\n",
    "ax1.plot(test_losses, 'r--', linewidth=2, label='Test')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('MSE Loss', fontsize=12)\n",
    "ax1.set_title('Mini VLA Training Progress', fontsize=13)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Also plot on log scale\n",
    "ax2.semilogy(train_losses, 'b-', linewidth=2, label='Train')\n",
    "ax2.semilogy(test_losses, 'r--', linewidth=2, label='Test')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('MSE Loss (log scale)', fontsize=12)\n",
    "ax2.set_title('Training Progress (Log Scale)', fontsize=13)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal train loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"Final test loss:  {test_losses[-1]:.6f}\")"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the loss decreasing steadily on both train and test sets. The model is learning to map images and text commands to trajectories. This is exactly what we want."
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo1 Cross Attention\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_todo1_cross_attention.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_13_todo1_cross_attention"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Your Turn -- Exercises\n",
    "\n",
    "Now it is your turn to get your hands dirty. We have two exercises that will deepen your understanding of the VLA architecture.\n",
    "\n",
    "### TODO 1: Implement Cross-Attention from Scratch\n",
    "\n",
    "In Section 4.4, we used PyTorch's built-in `MultiheadAttention`. Now let us implement the core cross-attention computation ourselves to make sure we truly understand what is happening under the hood."
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_cross_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Compute cross-attention: text queries attend to visual features.\n",
    "\n",
    "    Args:\n",
    "        Q: (batch, text_len, d) -- text query vectors\n",
    "        K: (batch, vis_len, d) -- visual key vectors\n",
    "        V: (batch, vis_len, d) -- visual value vectors\n",
    "\n",
    "    Returns:\n",
    "        output: (batch, text_len, d) -- text tokens enriched with visual info\n",
    "        weights: (batch, text_len, vis_len) -- attention weights\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Compute attention scores by multiplying Q with K transposed\n",
    "    #         Hint: use torch.bmm or the @ operator. K^T means swap last two dims.\n",
    "    # Step 2: Scale by sqrt(d_k) to prevent softmax saturation\n",
    "    # Step 3: Apply softmax over the visual dimension (last dim, dim=-1)\n",
    "    # Step 4: Multiply weights by V to get the output\n",
    "    # ==============================\n",
    "\n",
    "    scores = ???    # YOUR CODE: Q @ K^T, shape (batch, text_len, vis_len)\n",
    "    scores = ???    # YOUR CODE: divide by sqrt(d_k)\n",
    "    weights = ???   # YOUR CODE: softmax over last dim\n",
    "    output = ???    # YOUR CODE: weights @ V, shape (batch, text_len, d)\n",
    "\n",
    "    return output, weights"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification cell -- run this after completing TODO 1\n",
    "Q_test = torch.randn(2, 5, 32)   # 2 batches, 5 text tokens, 32-dim\n",
    "K_test = torch.randn(2, 64, 32)  # 2 batches, 64 visual tokens, 32-dim\n",
    "V_test = torch.randn(2, 64, 32)\n",
    "\n",
    "out, w = manual_cross_attention(Q_test, K_test, V_test)\n",
    "\n",
    "assert out.shape == (2, 5, 32), f\"Wrong output shape: {out.shape}, expected (2, 5, 32)\"\n",
    "assert w.shape == (2, 5, 64), f\"Wrong weight shape: {w.shape}, expected (2, 5, 64)\"\n",
    "assert torch.allclose(w.sum(-1), torch.ones(2, 5), atol=1e-5), \\\n",
    "    \"Attention weights don't sum to 1 across visual tokens!\"\n",
    "\n",
    "# Verify it matches PyTorch's implementation\n",
    "mha = nn.MultiheadAttention(32, num_heads=1, batch_first=True, bias=False)\n",
    "# Use identity projections for comparison\n",
    "with torch.no_grad():\n",
    "    mha.in_proj_weight.copy_(torch.eye(32).repeat(3, 1))\n",
    "    mha.out_proj.weight.copy_(torch.eye(32))\n",
    "\n",
    "pytorch_out, pytorch_w = mha(Q_test, K_test, V_test)\n",
    "our_out, our_w = manual_cross_attention(Q_test, K_test, V_test)\n",
    "\n",
    "print(\"All assertions passed!\")\n",
    "print(f\"Output shape: {out.shape} -- each text token is now a weighted sum of visual features\")\n",
    "print(f\"Weight shape: {w.shape} -- each text token has a distribution over 64 visual tokens\")\n",
    "print(f\"Weights sum:  {w.sum(-1)[0, 0].item():.4f} (should be 1.0)\")\n",
    "print(f\"\\nEach text token now 'sees' a weighted combination of visual features!\")"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo1 Verify\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_todo1_verify.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_14_todo1_verify"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement the Complete VLA Forward Pass\n",
    "\n",
    "Now let us wire up the entire VLA pipeline manually. This exercise ensures you understand the complete data flow from raw inputs to trajectory output."
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo2 Forward Pass\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/15_todo2_forward_pass.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_15_todo2_forward_pass"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vla_forward(image, text_tokens, vision_enc, text_enc, cross_attn_fn, action_dec):\n",
    "    \"\"\"\n",
    "    Run the full VLA pipeline: image + text --> trajectory.\n",
    "\n",
    "    Args:\n",
    "        image: (batch, 3, 32, 32) -- raw camera image\n",
    "        text_tokens: (batch, seq_len) -- tokenized text command\n",
    "        vision_enc: MiniVisionEncoder instance\n",
    "        text_enc: MiniTextEncoder instance\n",
    "        cross_attn_fn: your manual_cross_attention function\n",
    "        action_dec: ActionDecoder instance\n",
    "\n",
    "    Returns:\n",
    "        trajectory: (batch, n_waypoints, 2)\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Pass the image through the vision encoder to get visual tokens\n",
    "    #         Result shape: (batch, 64, embed_dim)\n",
    "    #\n",
    "    # Step 2: Pass the text tokens through the text encoder to get text features\n",
    "    #         Result shape: (batch, seq_len, embed_dim)\n",
    "    #\n",
    "    # Step 3: Cross-attend: use your manual_cross_attention function\n",
    "    #         Q = text features, K = V = visual tokens\n",
    "    #         Result shape: (batch, seq_len, embed_dim)\n",
    "    #\n",
    "    # Step 4: Decode the fused features into a trajectory\n",
    "    #         Result shape: (batch, n_waypoints, 2)\n",
    "    # ==============================\n",
    "\n",
    "    visual_tokens = ???    # YOUR CODE: Step 1\n",
    "    text_features = ???    # YOUR CODE: Step 2\n",
    "    fused, _ = ???         # YOUR CODE: Step 3\n",
    "    trajectory = ???       # YOUR CODE: Step 4\n",
    "\n",
    "    return trajectory"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification cell -- run this after completing TODO 2\n",
    "test_img = torch.randn(2, 3, 32, 32).to(device)\n",
    "test_cmd = torch.randint(0, len(vocab), (2, 10)).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    traj = vla_forward(\n",
    "        test_img, test_cmd,\n",
    "        vla.vision_encoder, vla.text_encoder,\n",
    "        manual_cross_attention, vla.action_decoder\n",
    "    )\n",
    "\n",
    "assert traj.shape == (2, 10, 2), f\"Wrong shape: {traj.shape}, expected (2, 10, 2)\"\n",
    "print(\"Full VLA pipeline works!\")\n",
    "print(f\"Input:  image {test_img.shape} + text {test_cmd.shape}\")\n",
    "print(f\"Output: trajectory {traj.shape}\")\n",
    "print(f\"\\nThis is the complete VLA data flow:\")\n",
    "print(f\"  Image (3, 32, 32) --> Vision Encoder --> 64 visual tokens\")\n",
    "print(f\"  Text  (10,)       --> Text Encoder   --> 10 text features\")\n",
    "print(f\"  Cross-Attention: text queries visual tokens --> 10 fused features\")\n",
    "print(f\"  Action Decoder: fused features --> 10 waypoints x 2D\")\n",
    "print(f\"\\nYou have built a complete VLA from scratch!\")"
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo2 Verify\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/16_todo2_verify.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_16_todo2_verify"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Trajectory Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/17_trajectory_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_17_trajectory_viz"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing the Trained VLA\n",
    "\n",
    "Let us now put our trained model through its paces. We will visualize three things: (1) predicted vs target trajectories, (2) cross-attention heatmaps showing where the model \"looks,\" and (3) how performance varies across different commands.\n",
    "\n",
    "### 7.1 Predicted vs Target Trajectories"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vla.eval()\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "\n",
    "np.random.seed(999)\n",
    "for i in range(5):\n",
    "    img, objs = generate_scene()\n",
    "    cmd, target_traj = generate_command_and_target(objs)\n",
    "    cmd_tokens = tokenize_command(cmd, vocab)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_traj, attn_w = vla(\n",
    "            img.unsqueeze(0).to(device),\n",
    "            cmd_tokens.unsqueeze(0).to(device)\n",
    "        )\n",
    "\n",
    "    pred_np = pred_traj.squeeze(0).cpu().numpy()\n",
    "    target_np = target_traj.numpy()\n",
    "\n",
    "    # Top row: scene with both trajectories\n",
    "    axes[0, i].imshow(img.permute(1, 2, 0).numpy())\n",
    "    axes[0, i].plot(\n",
    "        target_np[:, 0]*32, target_np[:, 1]*32,\n",
    "        'g-o', markersize=3, linewidth=2, label='Target'\n",
    "    )\n",
    "    axes[0, i].plot(\n",
    "        pred_np[:, 0]*32, pred_np[:, 1]*32,\n",
    "        'r--s', markersize=3, linewidth=2, label='VLA Prediction'\n",
    "    )\n",
    "    axes[0, i].set_title(f'\"{cmd}\"', fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    axes[0, i].legend(fontsize=7, loc='lower right')\n",
    "\n",
    "    # Bottom row: cross-attention heatmap\n",
    "    # Average attention weights across heads and text tokens\n",
    "    attn_map = attn_w.squeeze(0).cpu()\n",
    "    # attn_map shape: (n_heads * text_len, 64) or (text_len, 64)\n",
    "    if attn_map.dim() == 3:\n",
    "        attn_2d = attn_map.mean(0).mean(0).reshape(8, 8).numpy()\n",
    "    else:\n",
    "        attn_2d = attn_map.mean(0).reshape(8, 8).numpy()\n",
    "\n",
    "    axes[1, i].imshow(img.permute(1, 2, 0).numpy())\n",
    "    # Upsample attention to image size for overlay\n",
    "    import torch.nn.functional as F_interp\n",
    "    attn_upsampled = F_interp.interpolate(\n",
    "        torch.tensor(attn_2d).unsqueeze(0).unsqueeze(0).float(),\n",
    "        size=(32, 32), mode='bilinear', align_corners=False\n",
    "    ).squeeze().numpy()\n",
    "    attn_upsampled = attn_upsampled / (attn_upsampled.max() + 1e-8)\n",
    "    axes[1, i].imshow(attn_upsampled, cmap='hot', alpha=0.5)\n",
    "    axes[1, i].set_title(\"Where VLA looks\", fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Mini VLA: Image + Text Command --> Driving Trajectory\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "np.random.seed(42)"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top row shows predicted (red dashed) vs target (green solid) trajectories. The bottom row shows cross-attention heatmaps -- the brighter regions are where the model focuses its attention given the text command. Ideally, the model should attend to the region containing the target object mentioned in the command."
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Quantitative Evaluation by Command Type\n",
    "\n",
    "Let us measure how well the model performs for different color-shape combinations."
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate per-command performance\n",
    "vla.eval()\n",
    "command_errors = {}\n",
    "\n",
    "np.random.seed(42)\n",
    "for i in range(len(test_imgs)):\n",
    "    img = test_imgs[i]\n",
    "    cmd_tokens = test_cmds[i]\n",
    "    target = test_trajs[i]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred, _ = vla(\n",
    "            img.unsqueeze(0).to(device),\n",
    "            cmd_tokens.unsqueeze(0).to(device)\n",
    "        )\n",
    "\n",
    "    # Compute average waypoint error (in pixel units, *32)\n",
    "    error = ((pred.squeeze(0).cpu() - target) * 32).pow(2).sum(-1).sqrt().mean().item()\n",
    "\n",
    "    # Reconstruct command from tokens\n",
    "    words = []\n",
    "    for t in cmd_tokens.tolist():\n",
    "        for word, idx in vocab.items():\n",
    "            if idx == t and t != 0:\n",
    "                words.append(word)\n",
    "    cmd_str = ' '.join(words)\n",
    "\n",
    "    if cmd_str not in command_errors:\n",
    "        command_errors[cmd_str] = []\n",
    "    command_errors[cmd_str].append(error)\n",
    "\n",
    "# Plot average error per command\n",
    "cmds_sorted = sorted(command_errors.keys(),\n",
    "                     key=lambda c: np.mean(command_errors[c]))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "means = [np.mean(command_errors[c]) for c in cmds_sorted]\n",
    "stds = [np.std(command_errors[c]) for c in cmds_sorted]\n",
    "colors_bar = ['#2ecc71' if m < 3 else '#e74c3c' if m > 5 else '#f39c12' for m in means]\n",
    "\n",
    "bars = ax.bar(range(len(cmds_sorted)), means, yerr=stds,\n",
    "              color=colors_bar, edgecolor='black', capsize=3, alpha=0.8)\n",
    "ax.set_xticks(range(len(cmds_sorted)))\n",
    "ax.set_xticklabels(cmds_sorted, rotation=45, ha='right', fontsize=9)\n",
    "ax.set_ylabel('Average Waypoint Error (pixels)', fontsize=12)\n",
    "ax.set_title('VLA Performance by Command Type', fontsize=13)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "overall_error = np.mean([e for errors in command_errors.values() for e in errors])\n",
    "print(f\"Overall average waypoint error: {overall_error:.2f} pixels\")\n",
    "print(f\"Image size: 32x32 pixels\")\n",
    "print(f\"Relative error: {overall_error/32*100:.1f}% of image width\")"
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us how well the model handles different command types. Some color-shape combinations may be easier than others -- for example, red circles on a black background create high contrast, while green shapes might be harder to distinguish."
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Attention Highlight\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/18_attention_highlight.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_18_attention_highlight"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Attention Analysis -- Does the Model Look at the Right Things?\n",
    "\n",
    "One of the most powerful properties of cross-attention is interpretability. Let us verify that our model is actually \"looking\" at the correct objects when given different commands."
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scene with well-separated objects at known positions\n",
    "np.random.seed(7)\n",
    "torch.manual_seed(7)\n",
    "\n",
    "# Fixed scene with clear object placement\n",
    "img_fixed = torch.zeros(3, 32, 32)\n",
    "# Red circle at top-left (8, 8)\n",
    "for dx in range(-3, 4):\n",
    "    for dy in range(-3, 4):\n",
    "        if dx*dx + dy*dy <= 9:\n",
    "            img_fixed[0, 8+dy, 8+dx] = 1.0\n",
    "# Blue square at bottom-right (24, 24)\n",
    "for dx in range(-3, 4):\n",
    "    for dy in range(-3, 4):\n",
    "        img_fixed[2, 24+dy, 24+dx] = 1.0\n",
    "# Green circle at top-right (24, 8)\n",
    "for dx in range(-3, 4):\n",
    "    for dy in range(-3, 4):\n",
    "        if dx*dx + dy*dy <= 9:\n",
    "            img_fixed[1, 8+dy, 24+dx] = 1.0\n",
    "\n",
    "commands = [\"go to red circle\", \"go to blue square\", \"go to green circle\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for col, cmd in enumerate(commands):\n",
    "    cmd_tokens = tokenize_command(cmd, vocab)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_traj, attn_w = vla(\n",
    "            img_fixed.unsqueeze(0).to(device),\n",
    "            cmd_tokens.unsqueeze(0).to(device)\n",
    "        )\n",
    "\n",
    "    pred_np = pred_traj.squeeze(0).cpu().numpy()\n",
    "\n",
    "    # Top: scene + trajectory\n",
    "    axes[0, col].imshow(img_fixed.permute(1, 2, 0).numpy())\n",
    "    axes[0, col].plot(pred_np[:, 0]*32, pred_np[:, 1]*32, 'w--o',\n",
    "                      markersize=4, linewidth=2, label='VLA')\n",
    "    axes[0, col].set_title(f'\"{cmd}\"', fontsize=12)\n",
    "    axes[0, col].axis('off')\n",
    "    axes[0, col].legend(fontsize=9)\n",
    "\n",
    "    # Bottom: attention heatmap\n",
    "    attn_map = attn_w.squeeze(0).cpu()\n",
    "    if attn_map.dim() == 3:\n",
    "        attn_2d = attn_map.mean(0).mean(0).reshape(8, 8).numpy()\n",
    "    else:\n",
    "        attn_2d = attn_map.mean(0).reshape(8, 8).numpy()\n",
    "\n",
    "    attn_upsampled = F.interpolate(\n",
    "        torch.tensor(attn_2d).unsqueeze(0).unsqueeze(0).float(),\n",
    "        size=(32, 32), mode='bilinear', align_corners=False\n",
    "    ).squeeze().numpy()\n",
    "    attn_upsampled = attn_upsampled / (attn_upsampled.max() + 1e-8)\n",
    "\n",
    "    axes[1, col].imshow(img_fixed.permute(1, 2, 0).numpy())\n",
    "    axes[1, col].imshow(attn_upsampled, cmap='jet', alpha=0.6)\n",
    "    axes[1, col].set_title('Attention Heatmap', fontsize=12)\n",
    "    axes[1, col].axis('off')\n",
    "\n",
    "plt.suptitle(\"Same Scene, Different Commands --> Different Attention + Trajectories\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how the SAME image produces DIFFERENT trajectories based on the text command.\")\n",
    "print(\"This is the power of cross-attention: language steers where the model looks,\")\n",
    "print(\"and where it looks determines where it drives.\")"
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is truly the essence of a VLA. The same visual scene produces entirely different behaviors based on the language command. The text tells the model what to attend to, and the attended visual features determine the trajectory. This is exactly the same mechanism that allows RT-2 to follow novel instructions -- the cross-attention bridges the gap between seeing and doing, with language as the bridge."
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/19_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_19_closing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reflection and Next Steps\n",
    "\n",
    "### What We Built\n",
    "\n",
    "Let us take a step back and appreciate what we have accomplished. We built a complete Vision-Language-Action model from scratch:\n",
    "\n",
    "1. **Vision Encoder** -- converts raw images into 64 spatial feature tokens\n",
    "2. **Text Encoder** -- converts word sequences into contextual embeddings\n",
    "3. **Cross-Attention** -- fuses vision and language so text can \"look at\" the image\n",
    "4. **Action Decoder** -- maps fused features to trajectory waypoints\n",
    "\n",
    "This is the exact same architecture used by RT-2, EMMA, and Alpamayo -- the only difference is scale. Our model has ~100K parameters; Alpamayo has 10.5 billion. But the data flow is identical: image + text in, trajectory out, with cross-attention as the fusion mechanism.\n",
    "\n",
    "### Reflection Questions\n",
    "\n",
    "1. **Cross-attention vs concatenation.** Our model uses cross-attention to fuse vision and language. What would happen if we simply concatenated the visual and text tokens into one long sequence and applied self-attention? When would cross-attention be strictly better? (Hint: think about what happens when the text is very short but the image has many tokens.)\n",
    "\n",
    "2. **Regression vs tokenization.** We predicted waypoints directly using MSE regression. How would the architecture change if we used action tokenization (from Notebook 2) instead? What are the tradeoffs? (Hint: tokenization allows the model to express **uncertainty** -- it can assign probability mass to multiple possible actions. Regression predicts only one.)\n",
    "\n",
    "3. **Emergent generalization.** RT-2 can follow commands like \"pick up the object closest to the blue ball\" even though it was never trained on that phrase. What property of large pre-trained VLMs enables this? Could our tiny model do the same? Why or why not?\n",
    "\n",
    "### Optional Challenges\n",
    "\n",
    "1. **Add \"avoid\" commands.** Modify `generate_command_and_target` to create \"avoid red circle\" commands where the trajectory should curve **away** from the named object. Train the model on mixed \"go to\" and \"avoid\" data. Does it learn both behaviors?\n",
    "\n",
    "2. **Autoregressive generation.** Instead of predicting all 10 waypoints at once, modify the action decoder to predict them one at a time, each conditioned on the previous waypoints. This is closer to how RT-2 and EMMA actually generate actions.\n",
    "\n",
    "3. **Multi-step reasoning.** Create scenes with 5+ objects and commands like \"go to the red circle near the blue square.\" This requires the model to understand spatial relationships -- a significant step up in complexity.\n",
    "\n",
    "4. **Diffusion decoder.** Replace the MLP action decoder with a simple diffusion decoder that starts from Gaussian noise and iteratively refines it into a trajectory. This is what Alpamayo uses.\n",
    "\n",
    "You have now built a complete Vision-Language-Action model from scratch. This is the same principle behind every major driving VLA -- one model that sees, understands, and acts. The rest is scale."
   ],
   "id": "cell_49"
  }
 ]
}