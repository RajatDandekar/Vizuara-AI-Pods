{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Action Tokenization & Behavioral Cloning â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1DvX5XWC8mluYULlxP19dfwrDdQIYawHu\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Tokenization & Behavioral Cloning: Teaching a Language Model to Drive by Imitation\n",
    "\n",
    "**Notebook 2 â€” VLAs for Autonomous Driving Series | Vizuara**\n",
    "\n",
    "**Estimated time: ~60 minutes**\n",
    "\n",
    "Let us start with a simple question. Language models output words â€” sequences of discrete tokens like \"the\", \"cat\", \"sat\". But a car needs continuous signals: steer 15.3 degrees left, accelerate at 2.1 m/s squared, follow a smooth curve through the intersection. How do we bridge this gap? How do we get a model that \"speaks\" in English to also \"speak\" in steering angles?\n",
    "\n",
    "The answer is beautifully simple: we turn driving actions into words. We discretize continuous steering angles into bins and add them to the model's vocabulary as new tokens. Then we train the model to imitate expert drivers, predicting the correct action token at each timestep â€” exactly the way it would predict the next word in a sentence.\n",
    "\n",
    "By the end of this notebook, you will build a complete action tokenization system, train a behavioral cloning model that learns to imitate expert driving trajectories, and see it in action across five different driving maneuvers."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://course-creator-brown.vercel.app/courses/vla-autonomous-driving/practice/2/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_why_it_matters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 â€” Why Does This Matter?\n",
    "\n",
    "Here is the core tension. Systems like RT-2 from Google DeepMind and EMMA from Waymo are built on large language models â€” models with a fixed vocabulary of text tokens. These models are extraordinarily good at understanding images, following instructions, and reasoning about situations. But they were never designed to output continuous numbers like \"steer 14.7 degrees.\"\n",
    "\n",
    "So researchers asked: **what if we just made steering angles part of the vocabulary?**\n",
    "\n",
    "This is action tokenization. Instead of outputting the continuous value 15.0 degrees, the model outputs **token 170** â€” a discrete symbol that represents the bin containing 15.0 degrees. The car's controller then converts token 170 back to approximately 15.0 degrees and turns the wheel.\n",
    "\n",
    "This approach is powerful because it lets us use the **exact same training recipe** as language modeling. The model predicts the next token in a sequence â€” sometimes that token is a word describing the scene, and sometimes it is an action token telling the car what to do. Same architecture, same loss function, same training loop. This is exactly what we want.\n",
    "\n",
    "But the question remains: does discretizing continuous actions lose too much precision? And how do we actually train a model to imitate expert drivers? Let us find out."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_intuition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 â€” Building Intuition (No Code)\n",
    "\n",
    "### The Sheet Music Analogy\n",
    "\n",
    "Imagine you are listening to a guitar solo â€” a continuous, flowing melody with subtle bends, vibrato, and slides between notes. Now imagine writing that solo as sheet music. Sheet music uses discrete symbols: quarter notes, half notes, specific pitches on a staff. You lose some of the continuous nuance â€” the exact micro-bend between two frets, the precise vibrato speed. But if your notation system is detailed enough (32nd notes, bend markers, dynamics), you can recreate the solo with remarkable fidelity.\n",
    "\n",
    "Action tokenization does the same thing for driving. The continuous \"melody\" of steering is written as discrete \"notes\" â€” bin indices â€” that a language model can read and produce. With enough bins (256 is typical), the notation is precise enough that the reconstruction error is imperceptible to the car's physics.\n",
    "\n",
    "### The Thermometer Analogy\n",
    "\n",
    "Think of a thermometer. The actual temperature outside is continuous â€” it might be exactly 72.347 degrees Fahrenheit. But your weather app shows \"72 degrees.\" It rounds to the nearest integer. With 256 bins spanning a 90-degree steering range, we are rounding to the nearest 0.35 degrees. That is less than the mechanical play in most steering systems. The car cannot even tell the difference.\n",
    "\n",
    "### How It Fits Into the VLA Pipeline\n",
    "\n",
    "In a Vision-Language-Action model for driving:\n",
    "1. A camera image goes in.\n",
    "2. A language description of the driving task goes in (e.g., \"follow the road, turn left at the intersection\").\n",
    "3. The model outputs a sequence of action tokens â€” one per timestep.\n",
    "4. Each action token is detokenized back to a continuous steering angle.\n",
    "5. The car executes the steering command.\n",
    "\n",
    "The training data consists of triplets: (camera image, language description, expert action). We train the model to predict the expert's action token given the image and language â€” this is **behavioral cloning**.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "If we use 256 bins for a steering range of [-45 degrees, 45 degrees], what is the maximum tokenization error? The total range is 90 degrees, divided into 255 intervals (256 bins means 255 gaps between them). That gives us 90 / 255 = 0.353 degrees per bin. The maximum error from rounding is half a bin width: **0.176 degrees**. That is remarkably precise â€” far below what a human driver can consistently achieve."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Mathematics\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_mathematics.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_mathematics"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 â€” The Mathematics\n",
    "\n",
    "Let us now formalize these ideas. We will work through three equations, and for each one, we will plug in concrete numbers so you can trace through the computation yourself.\n",
    "\n",
    "### Equation 1: Action Tokenization\n",
    "\n",
    "Given a continuous action $a$ (say, a steering angle), we convert it to a discrete token index:\n",
    "\n",
    "$$a_{\\text{token}} = \\text{round}\\left(\\frac{a - a_{\\min}}{a_{\\max} - a_{\\min}} \\times (N_{\\text{bins}} - 1)\\right)$$\n",
    "\n",
    "Let us break this down computationally, step by step:\n",
    "\n",
    "1. **Subtract $a_{\\min}$**: shifts the range so it starts at zero.\n",
    "2. **Divide by $(a_{\\max} - a_{\\min})$**: normalizes to $[0, 1]$.\n",
    "3. **Multiply by $(N_{\\text{bins}} - 1)$**: scales to $[0, N_{\\text{bins}} - 1]$.\n",
    "4. **Round**: snaps to the nearest integer. That integer IS the token.\n",
    "\n",
    "**Worked example:** A steering angle of 15 degrees, with range $[-45, 45]$ and 256 bins.\n",
    "\n",
    "$$a_{\\text{token}} = \\text{round}\\left(\\frac{15 - (-45)}{45 - (-45)} \\times (256 - 1)\\right) = \\text{round}\\left(\\frac{60}{90} \\times 255\\right) = \\text{round}(170.0) = 170$$\n",
    "\n",
    "So steering 15 degrees to the right maps to **token 170**. This is exactly what we want.\n",
    "\n",
    "### Equation 2: Detokenization (The Inverse)\n",
    "\n",
    "To convert a token back to a continuous action:\n",
    "\n",
    "$$a = \\frac{a_{\\text{token}}}{N_{\\text{bins}} - 1} \\times (a_{\\max} - a_{\\min}) + a_{\\min}$$\n",
    "\n",
    "**Worked example:** Token 170 back to a steering angle.\n",
    "\n",
    "$$a = \\frac{170}{255} \\times (45 - (-45)) + (-45) = 0.6667 \\times 90 + (-45) = 60.0 - 45.0 = 15.0Â°$$\n",
    "\n",
    "Perfect roundtrip. Token 170 reconstructs exactly to 15.0 degrees. Not all values will reconstruct perfectly â€” values that fall between bin centers will have a small rounding error â€” but with 256 bins, the maximum error is only 0.176 degrees.\n",
    "\n",
    "### Equation 3: Behavioral Cloning Loss\n",
    "\n",
    "Now, how do we train a model to imitate expert drivers? At each timestep $t$, the model sees an image $I_t$ and language instruction $\\ell$, and predicts a probability distribution over all $N_{\\text{bins}}$ action tokens. The behavioral cloning loss is:\n",
    "\n",
    "$$\\mathcal{L} = -\\sum_{t=1}^{T} \\log p_\\theta(a_t \\mid I_t, \\ell)$$\n",
    "\n",
    "This is cross-entropy loss. For each timestep, the model outputs a distribution over 256 bins. The loss measures how much probability mass the model places on the **correct** token â€” the one the expert chose. If the model assigns probability 1.0 to the correct token, $\\log(1.0) = 0$, and the loss is zero. If it assigns probability 0.01, $\\log(0.01) = -4.6$, and the loss is large. The model learns to concentrate its predictions on the expert's choices.\n",
    "\n",
    "**Worked example:** Suppose $T = 3$ timesteps, and the model assigns these probabilities to the expert's chosen tokens:\n",
    "\n",
    "| Timestep | Expert Token | Model's Probability | $-\\log p$ |\n",
    "|----------|-------------|-------------------|-----------|\n",
    "| $t=1$ | 170 | 0.8 | 0.223 |\n",
    "| $t=2$ | 128 | 0.5 | 0.693 |\n",
    "| $t=3$ | 200 | 0.9 | 0.105 |\n",
    "\n",
    "$$\\mathcal{L} = 0.223 + 0.693 + 0.105 = 1.021$$\n",
    "\n",
    "If the model improved and assigned probability 0.95 to all three expert tokens, the loss would drop to $3 \\times 0.051 = 0.154$. Lower loss means the model is more confident about the expert's actions. This is exactly what we want."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Setup And Tokenizer\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_setup_and_tokenizer.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_setup_and_tokenizer"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 â€” Let Us Build It, Component by Component\n",
    "\n",
    "Now let us implement everything we just discussed. We will build four components: the tokenizer, a synthetic driving environment, a training dataset generator, and a behavioral cloning model."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Setup"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup â€” run this cell first\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Action Tokenizer\n",
    "\n",
    "This is the bridge between the continuous world of steering angles and the discrete world of language model tokens. Let us implement the exact formulas from Section 3."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionTokenizer:\n",
    "    \"\"\"\n",
    "    Convert continuous actions (steering angles) to discrete tokens and back.\n",
    "\n",
    "    This is exactly how RT-2 and EMMA handle action outputs â€”\n",
    "    they discretize continuous actions into bins and treat them as vocabulary tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, action_min: float, action_max: float, n_bins: int = 256):\n",
    "        self.action_min = action_min\n",
    "        self.action_max = action_max\n",
    "        self.n_bins = n_bins\n",
    "        self.bin_width = (action_max - action_min) / (n_bins - 1)\n",
    "\n",
    "    def tokenize(self, action: float) -> int:\n",
    "        \"\"\"Continuous action --> discrete token index.\"\"\"\n",
    "        # Step 1-2: Normalize to [0, 1]\n",
    "        normalized = (action - self.action_min) / (self.action_max - self.action_min)\n",
    "        # Step 3: Scale to [0, n_bins - 1]\n",
    "        scaled = normalized * (self.n_bins - 1)\n",
    "        # Step 4: Round and clamp\n",
    "        token = int(round(scaled))\n",
    "        return max(0, min(self.n_bins - 1, token))\n",
    "\n",
    "    def detokenize(self, token: int) -> float:\n",
    "        \"\"\"Discrete token index --> continuous action.\"\"\"\n",
    "        normalized = token / (self.n_bins - 1)\n",
    "        return normalized * (self.action_max - self.action_min) + self.action_min\n",
    "\n",
    "    def tokenize_batch(self, actions: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Batch tokenization for training tensors.\"\"\"\n",
    "        normalized = (actions - self.action_min) / (self.action_max - self.action_min)\n",
    "        tokens = torch.round(normalized * (self.n_bins - 1)).long()\n",
    "        return tokens.clamp(0, self.n_bins - 1)\n",
    "\n",
    "    def detokenize_batch(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Batch detokenization for inference tensors.\"\"\"\n",
    "        normalized = tokens.float() / (self.n_bins - 1)\n",
    "        return normalized * (self.action_max - self.action_min) + self.action_min\n",
    "\n",
    "# --- Verify with our worked example ---\n",
    "tokenizer_demo = ActionTokenizer(action_min=-45.0, action_max=45.0, n_bins=256)\n",
    "\n",
    "# Forward: 15 degrees --> token\n",
    "token = tokenizer_demo.tokenize(15.0)\n",
    "print(f\"Steering angle 15.0Â° --> Token {token}\")\n",
    "\n",
    "# Inverse: token --> degrees\n",
    "recovered = tokenizer_demo.detokenize(token)\n",
    "print(f\"Token {token} --> Steering angle {recovered:.1f}Â°\")\n",
    "\n",
    "# Edge cases\n",
    "print(f\"\\nEdge cases:\")\n",
    "print(f\"  -45.0Â° --> Token {tokenizer_demo.tokenize(-45.0)}\")\n",
    "print(f\"   0.0Â°  --> Token {tokenizer_demo.tokenize(0.0)}\")\n",
    "print(f\"  45.0Â°  --> Token {tokenizer_demo.tokenize(45.0)}\")\n",
    "print(f\"\\nBin width: {tokenizer_demo.bin_width:.3f}Â°\")\n",
    "print(f\"Max roundtrip error: {tokenizer_demo.bin_width / 2:.3f}Â°\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Tokenizer Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_tokenizer_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_tokenizer_viz"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualize how precise this tokenization actually is. We will compute the roundtrip error for every angle in the range, and also show how the number of bins affects precision."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tokenization precision\n",
    "tokenizer_viz = ActionTokenizer(action_min=-45.0, action_max=45.0, n_bins=256)\n",
    "\n",
    "angles = np.linspace(-45, 45, 1000)\n",
    "roundtrip_errors = []\n",
    "for a in angles:\n",
    "    token = tokenizer_viz.tokenize(a)\n",
    "    recovered = tokenizer_viz.detokenize(token)\n",
    "    roundtrip_errors.append(abs(a - recovered))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Left: roundtrip error across the range\n",
    "ax1.plot(angles, roundtrip_errors, 'steelblue', alpha=0.7, linewidth=1)\n",
    "ax1.axhline(y=tokenizer_viz.bin_width / 2, color='crimson', linestyle='--',\n",
    "            linewidth=2, label=f'Max error: {tokenizer_viz.bin_width / 2:.3f}Â°')\n",
    "ax1.set_xlabel('Original Steering Angle (Â°)', fontsize=12)\n",
    "ax1.set_ylabel('Roundtrip Error (Â°)', fontsize=12)\n",
    "ax1.set_title('Tokenization Error Across Full Range', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: precision vs number of bins\n",
    "bin_counts = [8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "max_errors = [90.0 / (2 * (b - 1)) for b in bin_counts]\n",
    "colors = ['#d32f2f' if e > 1.0 else '#f57c00' if e > 0.5 else '#388e3c' for e in max_errors]\n",
    "bars = ax2.bar(range(len(bin_counts)), max_errors, color=colors, edgecolor='white', linewidth=0.5)\n",
    "ax2.set_xticks(range(len(bin_counts)))\n",
    "ax2.set_xticklabels(bin_counts)\n",
    "ax2.set_xlabel('Number of Bins', fontsize=12)\n",
    "ax2.set_ylabel('Max Error (Â°)', fontsize=12)\n",
    "ax2.set_title('Precision vs. Number of Bins', fontsize=13, fontweight='bold')\n",
    "ax2.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5, label='0.5Â° threshold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, max_errors):\n",
    "    ax2.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1,\n",
    "             f'{val:.2f}Â°', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"With 256 bins, the max steering error is only 0.176Â° â€” barely noticeable!\")\n",
    "print(\"Even 64 bins gives sub-degree precision. This is why tokenization works so well.\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Driving Env\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_driving_env.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_driving_env"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Synthetic Driving Environment\n",
    "\n",
    "We need expert driving data to train our behavioral cloning model. In a real system like EMMA, this comes from thousands of hours of human driving. For our notebook, we will create a simple 2D driving environment with five trajectory types: straight driving, lane changes, left turns, right turns, and S-curves."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDrivingEnv:\n",
    "    \"\"\"\n",
    "    A simple 2D driving environment that generates expert trajectories.\n",
    "\n",
    "    In a real VLA system, expert data comes from human drivers.\n",
    "    Here we use analytical curves to simulate expert behavior.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_waypoints: int = 50):\n",
    "        self.n_waypoints = n_waypoints\n",
    "\n",
    "    def generate_expert_trajectory(self, trajectory_type: str = 'lane_change') -> np.ndarray:\n",
    "        \"\"\"Generate an expert trajectory as (n_waypoints, 2) array of [x, y] positions.\"\"\"\n",
    "        t = np.linspace(0, 1, self.n_waypoints)\n",
    "\n",
    "        if trajectory_type == 'straight':\n",
    "            x = t * 50\n",
    "            y = np.zeros_like(t)\n",
    "        elif trajectory_type == 'lane_change':\n",
    "            x = t * 50\n",
    "            y = 3.5 * (1 / (1 + np.exp(-10 * (t - 0.5))))  # Smooth sigmoid\n",
    "        elif trajectory_type == 'left_turn':\n",
    "            theta = t * np.pi / 2\n",
    "            r = 20\n",
    "            x = r * np.sin(theta)\n",
    "            y = r * (1 - np.cos(theta))\n",
    "        elif trajectory_type == 'right_turn':\n",
    "            theta = t * np.pi / 2\n",
    "            r = 20\n",
    "            x = r * np.sin(theta)\n",
    "            y = -r * (1 - np.cos(theta))\n",
    "        elif trajectory_type == 's_curve':\n",
    "            x = t * 50\n",
    "            y = 3.0 * np.sin(2 * np.pi * t)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown trajectory type: {trajectory_type}\")\n",
    "\n",
    "        # Add small noise for realistic variety\n",
    "        x += np.random.normal(0, 0.1, len(t))\n",
    "        y += np.random.normal(0, 0.1, len(t))\n",
    "\n",
    "        return np.stack([x, y], axis=1)  # (n_waypoints, 2)\n",
    "\n",
    "    def trajectory_to_actions(self, trajectory: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Convert waypoint trajectory to steering angles between consecutive points.\"\"\"\n",
    "        dx = np.diff(trajectory[:, 0])\n",
    "        dy = np.diff(trajectory[:, 1])\n",
    "        angles = np.degrees(np.arctan2(dy, dx))\n",
    "        return angles  # (n_waypoints - 1,)\n",
    "\n",
    "env = SimpleDrivingEnv(n_waypoints=50)\n",
    "print(f\"Environment created with {env.n_waypoints} waypoints per trajectory.\")\n",
    "print(f\"Each trajectory produces {env.n_waypoints - 1} steering actions.\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Trajectory Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_trajectory_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_trajectory_viz"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the five trajectory types. Each one represents a different driving maneuver that our behavioral cloning model will need to learn."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all five expert trajectory types\n",
    "fig, axes = plt.subplots(1, 5, figsize=(22, 4))\n",
    "trajectory_types = ['straight', 'lane_change', 'left_turn', 'right_turn', 's_curve']\n",
    "colors = ['#1976d2', '#388e3c', '#f57c00', '#d32f2f', '#7b1fa2']\n",
    "\n",
    "np.random.seed(42)  # Consistent visualization\n",
    "for ax, ttype, color in zip(axes, trajectory_types, colors):\n",
    "    # Plot 3 samples to show variance\n",
    "    for sample in range(3):\n",
    "        traj = env.generate_expert_trajectory(ttype)\n",
    "        alpha = 1.0 if sample == 0 else 0.3\n",
    "        lw = 2.5 if sample == 0 else 1.0\n",
    "        ax.plot(traj[:, 0], traj[:, 1], color=color, linewidth=lw, alpha=alpha)\n",
    "        if sample == 0:\n",
    "            ax.plot(traj[0, 0], traj[0, 1], 'o', color='green', markersize=8, zorder=5)\n",
    "            ax.plot(traj[-1, 0], traj[-1, 1], 's', color='red', markersize=8, zorder=5)\n",
    "\n",
    "    ax.set_title(ttype.replace('_', ' ').title(), fontsize=13, fontweight='bold')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlabel('x (m)')\n",
    "    ax.set_ylabel('y (m)')\n",
    "\n",
    "# Add a legend manually\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='green', linestyle='None', markersize=8, label='Start'),\n",
    "    Line2D([0], [0], marker='s', color='red', linestyle='None', markersize=8, label='End'),\n",
    "    Line2D([0], [0], color='gray', linewidth=1, alpha=0.4, label='Variations (noise)')\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='lower center', ncol=3, fontsize=11,\n",
    "           bbox_to_anchor=(0.5, -0.05))\n",
    "\n",
    "plt.suptitle(\"Expert Driving Trajectories â€” Our Training Data\", fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each trajectory type represents a different driving maneuver.\")\n",
    "print(\"The noise across samples simulates natural driving variation.\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Dataset\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_dataset.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_dataset"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Generating the Training Dataset\n",
    "\n",
    "Now we need to create training data in the format our model expects: pairs of (context features, tokenized action sequences). In a real VLA, the context features would come from a vision encoder processing camera images. Here, we use a simplified representation: a one-hot encoding of the trajectory type plus the first few waypoints as spatial context."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(env, tokenizer, n_samples=5000):\n",
    "    \"\"\"\n",
    "    Generate a training dataset of (context_features, action_token_sequences).\n",
    "\n",
    "    Each sample consists of:\n",
    "      - features: one-hot trajectory type (5) + first 5 waypoints flattened (10) = 15 dims\n",
    "      - tokens: tokenized steering angles for the full trajectory (49 timesteps)\n",
    "    \"\"\"\n",
    "    trajectory_types = ['straight', 'lane_change', 'left_turn', 'right_turn', 's_curve']\n",
    "    type_to_idx = {t: i for i, t in enumerate(trajectory_types)}\n",
    "\n",
    "    all_features = []\n",
    "    all_tokens = []\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        # Pick a random maneuver type\n",
    "        ttype = np.random.choice(trajectory_types)\n",
    "        traj = env.generate_expert_trajectory(ttype)\n",
    "        actions = env.trajectory_to_actions(traj)\n",
    "\n",
    "        # Build context features\n",
    "        type_onehot = np.zeros(5)\n",
    "        type_onehot[type_to_idx[ttype]] = 1.0\n",
    "        context = traj[:5].flatten()  # First 5 waypoints as observation\n",
    "        features = np.concatenate([type_onehot, context])  # 15-dim feature vector\n",
    "\n",
    "        # Tokenize the expert's steering actions\n",
    "        action_tokens = [tokenizer.tokenize(a) for a in actions]\n",
    "\n",
    "        all_features.append(features)\n",
    "        all_tokens.append(action_tokens)\n",
    "\n",
    "    features_tensor = torch.FloatTensor(np.array(all_features))\n",
    "    tokens_tensor = torch.LongTensor(np.array(all_tokens))\n",
    "\n",
    "    return features_tensor, tokens_tensor\n",
    "\n",
    "# Build the dataset\n",
    "# Using [-90, 90] range to accommodate all possible steering angles from our trajectories\n",
    "tokenizer = ActionTokenizer(action_min=-90.0, action_max=90.0, n_bins=256)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "features, tokens = generate_dataset(env, tokenizer, n_samples=5000)\n",
    "\n",
    "# Train/test split\n",
    "N_TRAIN = 4000\n",
    "train_features, test_features = features[:N_TRAIN], features[N_TRAIN:]\n",
    "train_tokens, test_tokens = tokens[:N_TRAIN], tokens[N_TRAIN:]\n",
    "\n",
    "print(f\"Dataset generated:\")\n",
    "print(f\"  Training samples:       {N_TRAIN}\")\n",
    "print(f\"  Test samples:           {len(test_features)}\")\n",
    "print(f\"  Feature dimensionality: {features.shape[1]}\")\n",
    "print(f\"  Action sequence length: {tokens.shape[1]} timesteps\")\n",
    "print(f\"  Token vocabulary size:  {tokenizer.n_bins} bins\")\n",
    "print(f\"\\nToken distribution â€” min: {tokens.min().item()}, max: {tokens.max().item()}, \"\n",
    "      f\"mean: {tokens.float().mean().item():.1f}\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Model\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_model.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The Behavioral Cloning Model\n",
    "\n",
    "Now for the model itself. Our behavioral cloning model takes context features as input and predicts a probability distribution over 256 bins for each of the 49 action timesteps. This is exactly analogous to how a language model predicts a probability distribution over vocabulary tokens for each position in a sentence.\n",
    "\n",
    "The key architectural choice: we use separate linear heads for each timestep. In a real VLA like EMMA, this would be an autoregressive transformer that predicts one action token at a time, conditioned on all previous tokens. Our simplified version predicts all timesteps in parallel, but the training objective â€” cross-entropy over discrete bins â€” is identical."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BehavioralCloningModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A model that learns to predict expert action tokens from context features.\n",
    "\n",
    "    Architecture:\n",
    "      - Encoder: 2-layer MLP that processes context features\n",
    "      - Action heads: one linear layer per timestep, predicting a distribution over bins\n",
    "\n",
    "    In a real VLA, the encoder would be a vision-language model (e.g., PaLM-E),\n",
    "    and the action heads would be an autoregressive transformer decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, n_actions: int, n_bins: int = 256, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.n_actions = n_actions\n",
    "        self.n_bins = n_bins\n",
    "\n",
    "        # Shared feature encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "        # One classification head per action timestep\n",
    "        # Each head predicts a distribution over n_bins\n",
    "        self.action_heads = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, n_bins) for _ in range(n_actions)\n",
    "        ])\n",
    "\n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: (batch_size, input_dim)\n",
    "        Returns:\n",
    "            logits: (batch_size, n_actions, n_bins) â€” unnormalized log-probabilities\n",
    "        \"\"\"\n",
    "        h = self.encoder(features)\n",
    "        logits = torch.stack([head(h) for head in self.action_heads], dim=1)\n",
    "        return logits\n",
    "\n",
    "# Create the model\n",
    "N_ACTIONS = 49  # n_waypoints - 1\n",
    "model = BehavioralCloningModel(\n",
    "    input_dim=15,\n",
    "    n_actions=N_ACTIONS,\n",
    "    n_bins=256,\n",
    "    hidden_dim=256\n",
    ").to(DEVICE)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model created with {n_params:,} parameters\")\n",
    "print(f\"  Input:  15-dimensional context features\")\n",
    "print(f\"  Output: {N_ACTIONS} action distributions, each over {256} bins\")\n",
    "print(f\"  This is equivalent to predicting {N_ACTIONS} 'words' from a vocabulary of {256}.\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Training Loop â€” Behavioral Cloning\n",
    "\n",
    "Here is where the behavioral cloning loss from Section 3 comes to life. For every batch, we compute the cross-entropy between the model's predicted distributions and the expert's chosen tokens. The model learns to assign high probability to the expert's actions."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training ---\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 30\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    # --- Train ---\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "    perm = torch.randperm(N_TRAIN)\n",
    "\n",
    "    for i in range(0, N_TRAIN, BATCH_SIZE):\n",
    "        idx = perm[i:i + BATCH_SIZE]\n",
    "        batch_features = train_features[idx].to(DEVICE)\n",
    "        batch_tokens = train_tokens[idx].to(DEVICE)\n",
    "\n",
    "        logits = model(batch_features)  # (batch, 49, 256)\n",
    "\n",
    "        # Behavioral cloning loss: cross-entropy over all action positions\n",
    "        loss = F.cross_entropy(\n",
    "            logits.reshape(-1, 256),       # (batch * 49, 256)\n",
    "            batch_tokens.reshape(-1)       # (batch * 49,)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_train_loss = epoch_loss / n_batches\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # --- Evaluate ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_logits = model(test_features.to(DEVICE))\n",
    "        test_loss = F.cross_entropy(\n",
    "            test_logits.reshape(-1, 256),\n",
    "            test_tokens.to(DEVICE).reshape(-1)\n",
    "        ).item()\n",
    "\n",
    "        # Accuracy: how often does the model predict the exact right bin?\n",
    "        pred_tokens = test_logits.argmax(dim=-1)\n",
    "        accuracy = (pred_tokens.cpu() == test_tokens).float().mean().item()\n",
    "\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:2d}/{N_EPOCHS} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Test Loss: {test_loss:.4f} | \"\n",
    "              f\"Test Accuracy: {accuracy:.1%}\")\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Final test loss: {test_losses[-1]:.4f}\")\n",
    "print(f\"Final test accuracy: {test_accuracies[-1]:.1%}\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Training Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_training_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_11_training_viz"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the training progress and a sample prediction. The loss should decrease steadily as the model learns to imitate the expert's steering patterns."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves and a sample prediction\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(train_losses, 'b-', linewidth=2, label='Train Loss')\n",
    "axes[0].plot(test_losses, 'r-', linewidth=2, label='Test Loss')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "axes[0].set_title('Behavioral Cloning Training', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curve\n",
    "axes[1].plot(test_accuracies, 'g-', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Exact Token Accuracy', fontsize=12)\n",
    "axes[1].set_title('Test Accuracy (Exact Bin Match)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Sample prediction: expert vs predicted steering\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_logits = model(test_features[:1].to(DEVICE))\n",
    "    sample_pred_tokens = sample_logits.argmax(dim=-1).squeeze(0).cpu()\n",
    "\n",
    "pred_angles = [tokenizer.detokenize(t.item()) for t in sample_pred_tokens]\n",
    "true_angles = [tokenizer.detokenize(t.item()) for t in test_tokens[0]]\n",
    "\n",
    "axes[2].plot(true_angles, 'g-', linewidth=2.5, label='Expert', alpha=0.8)\n",
    "axes[2].plot(pred_angles, 'r--', linewidth=2, label='Model Prediction')\n",
    "axes[2].set_xlabel('Timestep', fontsize=12)\n",
    "axes[2].set_ylabel('Steering Angle (Â°)', fontsize=12)\n",
    "axes[2].set_title('Expert vs. Predicted Steering', fontsize=13, fontweight='bold')\n",
    "axes[2].legend(fontsize=11)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The model has learned to predict steering sequences that closely match the expert.\")\n",
    "print(\"This is exactly what behavioral cloning achieves â€” imitation through supervised learning.\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo1 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_todo1_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_12_todo1_intro"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 â€” Your Turn\n",
    "\n",
    "Now it is time for you to implement the core ideas yourself. These exercises test the two most important concepts from this notebook: action tokenization and the behavioral cloning loss.\n",
    "\n",
    "### TODO 1: Implement Tokenization from Scratch\n",
    "\n",
    "Write the tokenization function without using the ActionTokenizer class. This forces you to internalize the formula."
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenize(action: float, action_min: float, action_max: float, n_bins: int) -> int:\n",
    "    \"\"\"\n",
    "    Convert a continuous action to a discrete token.\n",
    "\n",
    "    Args:\n",
    "        action:     the continuous action value (e.g., steering angle in degrees)\n",
    "        action_min: minimum of the action range\n",
    "        action_max: maximum of the action range\n",
    "        n_bins:     number of discrete bins\n",
    "\n",
    "    Returns:\n",
    "        token: int in [0, n_bins - 1]\n",
    "\n",
    "    Steps:\n",
    "        1. Normalize action to [0, 1] range\n",
    "        2. Scale to [0, n_bins - 1]\n",
    "        3. Round to nearest integer\n",
    "        4. Clamp to valid range\n",
    "    \"\"\"\n",
    "    # ============ YOUR CODE HERE ============\n",
    "    # Step 1: Normalize\n",
    "    # normalized = ???\n",
    "\n",
    "    # Step 2: Scale\n",
    "    # scaled = ???\n",
    "\n",
    "    # Step 3: Round\n",
    "    # token = ???\n",
    "\n",
    "    # Step 4: Clamp\n",
    "    # token = ???\n",
    "\n",
    "    # ========================================\n",
    "\n",
    "    token = ???  # Replace with your implementation\n",
    "\n",
    "    return token\n",
    "\n",
    "\n",
    "def my_detokenize(token: int, action_min: float, action_max: float, n_bins: int) -> float:\n",
    "    \"\"\"\n",
    "    Convert a discrete token back to a continuous action.\n",
    "\n",
    "    Args:\n",
    "        token:      the discrete token index\n",
    "        action_min: minimum of the action range\n",
    "        action_max: maximum of the action range\n",
    "        n_bins:     number of discrete bins\n",
    "\n",
    "    Returns:\n",
    "        action: float â€” the reconstructed continuous value\n",
    "    \"\"\"\n",
    "    # ============ YOUR CODE HERE ============\n",
    "    # Step 1: Normalize token to [0, 1]\n",
    "    # Step 2: Scale to [action_min, action_max]\n",
    "    # ========================================\n",
    "\n",
    "    action = ???  # Replace with your implementation\n",
    "\n",
    "    return action"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo1 Verify\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_todo1_verify.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_13_todo1_verify"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification â€” all assertions must pass\n",
    "assert my_tokenize(15.0, -45.0, 45.0, 256) == 170, \"15Â° should map to token 170\"\n",
    "assert my_tokenize(-45.0, -45.0, 45.0, 256) == 0, \"-45Â° should map to token 0\"\n",
    "assert my_tokenize(45.0, -45.0, 45.0, 256) == 255, \"45Â° should map to token 255\"\n",
    "assert my_tokenize(0.0, -45.0, 45.0, 256) == 128, \"0Â° should map to token 128 (midpoint)\"\n",
    "\n",
    "assert abs(my_detokenize(170, -45.0, 45.0, 256) - 15.0) < 0.01, \"Token 170 should reconstruct to ~15Â°\"\n",
    "assert abs(my_detokenize(0, -45.0, 45.0, 256) - (-45.0)) < 0.01, \"Token 0 should reconstruct to -45Â°\"\n",
    "assert abs(my_detokenize(255, -45.0, 45.0, 256) - 45.0) < 0.01, \"Token 255 should reconstruct to 45Â°\"\n",
    "\n",
    "print(\"Your tokenization and detokenization are both correct!\")\n",
    "print(\"You have implemented the exact same algorithm used in RT-2 and EMMA.\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo2 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_todo2_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_14_todo2_intro"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement the Behavioral Cloning Loss\n",
    "\n",
    "This is the loss function that drives the entire training process. Implement it from scratch using PyTorch's cross-entropy."
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def behavioral_cloning_loss(predicted_logits: torch.Tensor, expert_tokens: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the behavioral cloning loss.\n",
    "\n",
    "    This is cross-entropy over action tokens â€” the same loss used in language modeling,\n",
    "    applied to driving actions instead of words.\n",
    "\n",
    "    Args:\n",
    "        predicted_logits: (batch_size, n_actions, n_bins) â€” model's raw output\n",
    "        expert_tokens:    (batch_size, n_actions)         â€” expert's chosen tokens\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar tensor â€” average cross-entropy loss\n",
    "\n",
    "    Hint: F.cross_entropy expects inputs of shape (N, C) and targets of shape (N,)\n",
    "          where C is the number of classes (bins). You need to reshape accordingly.\n",
    "    \"\"\"\n",
    "    # ============ YOUR CODE HERE ============\n",
    "    # Step 1: Get the shapes\n",
    "    # batch_size, n_actions, n_bins = predicted_logits.shape\n",
    "\n",
    "    # Step 2: Reshape logits to (batch_size * n_actions, n_bins)\n",
    "    # flat_logits = ???\n",
    "\n",
    "    # Step 3: Reshape expert tokens to (batch_size * n_actions,)\n",
    "    # flat_tokens = ???\n",
    "\n",
    "    # Step 4: Compute cross-entropy loss\n",
    "    # loss = F.cross_entropy(flat_logits, flat_tokens)\n",
    "    # ========================================\n",
    "\n",
    "    loss = ???  # Replace with your implementation\n",
    "\n",
    "    return loss"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo2 Verify\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/15_todo2_verify.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_15_todo2_verify"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "torch.manual_seed(42)\n",
    "test_logits_v = torch.randn(4, 10, 256)   # 4 samples, 10 actions, 256 bins\n",
    "test_expert_v = torch.randint(0, 256, (4, 10))  # Expert tokens\n",
    "\n",
    "loss_v = behavioral_cloning_loss(test_logits_v, test_expert_v)\n",
    "\n",
    "assert loss_v.ndim == 0, f\"Loss should be a scalar, got ndim={loss_v.ndim}\"\n",
    "assert loss_v.item() > 0, f\"Loss should be positive, got {loss_v.item()}\"\n",
    "assert not torch.isnan(loss_v), \"Loss should not be NaN\"\n",
    "assert abs(loss_v.item() - F.cross_entropy(\n",
    "    test_logits_v.reshape(-1, 256), test_expert_v.reshape(-1)\n",
    ").item()) < 1e-5, \"Loss does not match expected value\"\n",
    "\n",
    "print(f\"Loss computed correctly: {loss_v.item():.4f}\")\n",
    "print(\"Your implementation matches PyTorch's cross_entropy â€” this is the exact loss\")\n",
    "print(\"used to train behavioral cloning models like EMMA.\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Final Eval\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/16_final_eval.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_16_final_eval"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6 â€” Final Evaluation: Expert vs. Learned Trajectories\n",
    "\n",
    "Now let us see the full picture. We will compare the expert trajectories against our model's learned trajectories for all five driving maneuvers. This is the moment of truth â€” can our model, trained purely by imitating experts, actually reproduce their driving behavior?"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison: expert vs. model across all 5 maneuvers\n",
    "model.eval()\n",
    "trajectory_types = ['straight', 'lane_change', 'left_turn', 'right_turn', 's_curve']\n",
    "type_to_idx = {t: i for i, t in enumerate(trajectory_types)}\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(24, 9))\n",
    "np.random.seed(123)  # Fresh seed for evaluation trajectories\n",
    "\n",
    "for col, ttype in enumerate(trajectory_types):\n",
    "    # Generate an expert trajectory\n",
    "    expert_traj = env.generate_expert_trajectory(ttype)\n",
    "    expert_actions = env.trajectory_to_actions(expert_traj)\n",
    "\n",
    "    # Build context features (same format as training)\n",
    "    type_onehot = np.zeros(5)\n",
    "    type_onehot[type_to_idx[ttype]] = 1.0\n",
    "    context = expert_traj[:5].flatten()\n",
    "    features_np = np.concatenate([type_onehot, context])\n",
    "    features_t = torch.FloatTensor(features_np).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # Model prediction\n",
    "    with torch.no_grad():\n",
    "        pred_logits = model(features_t)\n",
    "        pred_token_seq = pred_logits.argmax(dim=-1).squeeze(0).cpu()\n",
    "\n",
    "    pred_actions = np.array([tokenizer.detokenize(t.item()) for t in pred_token_seq])\n",
    "\n",
    "    # Reconstruct predicted trajectory from steering angles\n",
    "    pred_traj = [expert_traj[0].copy()]\n",
    "    for a in pred_actions:\n",
    "        dx = np.cos(np.radians(a))\n",
    "        dy = np.sin(np.radians(a))\n",
    "        pred_traj.append(pred_traj[-1] + np.array([dx, dy]))\n",
    "    pred_traj = np.array(pred_traj)\n",
    "\n",
    "    # --- Top row: trajectory comparison ---\n",
    "    axes[0, col].plot(expert_traj[:, 0], expert_traj[:, 1], 'g-', linewidth=2.5,\n",
    "                      label='Expert', alpha=0.8)\n",
    "    axes[0, col].plot(pred_traj[:, 0], pred_traj[:, 1], 'r--', linewidth=2,\n",
    "                      label='Learned')\n",
    "    axes[0, col].plot(expert_traj[0, 0], expert_traj[0, 1], 'ko', markersize=8, zorder=5)\n",
    "    axes[0, col].set_title(ttype.replace('_', ' ').title(), fontsize=13, fontweight='bold')\n",
    "    axes[0, col].set_aspect('equal')\n",
    "    axes[0, col].grid(True, alpha=0.3)\n",
    "    axes[0, col].legend(fontsize=9, loc='best')\n",
    "    if col == 0:\n",
    "        axes[0, col].set_ylabel('y position (m)', fontsize=11)\n",
    "\n",
    "    # --- Bottom row: steering angle comparison ---\n",
    "    axes[1, col].plot(expert_actions, 'g-', linewidth=2, label='Expert', alpha=0.8)\n",
    "    axes[1, col].plot(pred_actions, 'r--', linewidth=1.5, label='Learned')\n",
    "    axes[1, col].set_xlabel('Timestep', fontsize=11)\n",
    "    axes[1, col].grid(True, alpha=0.3)\n",
    "    axes[1, col].legend(fontsize=9, loc='best')\n",
    "    if col == 0:\n",
    "        axes[1, col].set_ylabel('Steering Angle (Â°)', fontsize=11)\n",
    "\n",
    "plt.suptitle(\"Behavioral Cloning Results: Expert vs. Learned Driving\",\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute overall error statistics\n",
    "all_errors = []\n",
    "np.random.seed(456)\n",
    "for _ in range(100):\n",
    "    ttype = np.random.choice(trajectory_types)\n",
    "    traj = env.generate_expert_trajectory(ttype)\n",
    "    actions = env.trajectory_to_actions(traj)\n",
    "\n",
    "    type_onehot = np.zeros(5)\n",
    "    type_onehot[type_to_idx[ttype]] = 1.0\n",
    "    context = traj[:5].flatten()\n",
    "    feat = np.concatenate([type_onehot, context])\n",
    "    feat_t = torch.FloatTensor(feat).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(feat_t)\n",
    "        preds = logits.argmax(dim=-1).squeeze(0).cpu()\n",
    "\n",
    "    pred_a = np.array([tokenizer.detokenize(t.item()) for t in preds])\n",
    "    error = np.abs(pred_a - actions)\n",
    "    all_errors.extend(error.tolist())\n",
    "\n",
    "all_errors = np.array(all_errors)\n",
    "print(f\"\\nOverall steering angle error across 100 random test trajectories:\")\n",
    "print(f\"  Mean error:   {all_errors.mean():.2f}Â°\")\n",
    "print(f\"  Median error: {np.median(all_errors):.2f}Â°\")\n",
    "print(f\"  95th pctl:    {np.percentile(all_errors, 95):.2f}Â°\")\n",
    "print(f\"\\nThe model has learned to imitate expert driving through pure behavioral cloning.\")\n",
    "print(\"In a real system like EMMA, the same cross-entropy training objective is used â€”\")\n",
    "print(\"but with a vision transformer encoder processing real camera images.\")"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also create an animated visualization showing the model's learned driving in action. This makes the behavioral cloning result tangible â€” you can watch the car follow the learned trajectory in real time."
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animated comparison: expert vs. learned trajectory\n",
    "np.random.seed(42)\n",
    "demo_traj = env.generate_expert_trajectory('s_curve')\n",
    "demo_actions = env.trajectory_to_actions(demo_traj)\n",
    "\n",
    "type_onehot = np.zeros(5)\n",
    "type_onehot[type_to_idx['s_curve']] = 1.0\n",
    "context = demo_traj[:5].flatten()\n",
    "feat = np.concatenate([type_onehot, context])\n",
    "feat_t = torch.FloatTensor(feat).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(feat_t)\n",
    "    pred_tok = logits.argmax(dim=-1).squeeze(0).cpu()\n",
    "\n",
    "pred_act = np.array([tokenizer.detokenize(t.item()) for t in pred_tok])\n",
    "pred_traj_anim = [demo_traj[0].copy()]\n",
    "for a in pred_act:\n",
    "    dx = np.cos(np.radians(a))\n",
    "    dy = np.sin(np.radians(a))\n",
    "    pred_traj_anim.append(pred_traj_anim[-1] + np.array([dx, dy]))\n",
    "pred_traj_anim = np.array(pred_traj_anim)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "def animate(frame):\n",
    "    ax.clear()\n",
    "    # Full expert path (faded)\n",
    "    ax.plot(demo_traj[:, 0], demo_traj[:, 1], 'g-', linewidth=1, alpha=0.3, label='Expert (full)')\n",
    "\n",
    "    # Expert up to current frame\n",
    "    end = min(frame + 1, len(demo_traj))\n",
    "    ax.plot(demo_traj[:end, 0], demo_traj[:end, 1], 'g-', linewidth=2.5, label='Expert')\n",
    "\n",
    "    # Learned up to current frame\n",
    "    end_p = min(frame + 1, len(pred_traj_anim))\n",
    "    ax.plot(pred_traj_anim[:end_p, 0], pred_traj_anim[:end_p, 1], 'r--', linewidth=2, label='Learned')\n",
    "\n",
    "    # Current position markers\n",
    "    if end > 0:\n",
    "        ax.plot(demo_traj[end-1, 0], demo_traj[end-1, 1], 'go', markersize=12, zorder=5)\n",
    "    if end_p > 0:\n",
    "        ax.plot(pred_traj_anim[end_p-1, 0], pred_traj_anim[end_p-1, 1], 'rs', markersize=10, zorder=5)\n",
    "\n",
    "    ax.set_xlim(-5, 55)\n",
    "    ax.set_ylim(-6, 6)\n",
    "    ax.set_xlabel('x (m)', fontsize=12)\n",
    "    ax.set_ylabel('y (m)', fontsize=12)\n",
    "    ax.set_title(f'S-Curve: Expert vs. Learned (t={frame})', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='upper right', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    return []\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, frames=len(demo_traj), interval=100, blit=True)\n",
    "plt.close(fig)\n",
    "HTML(anim.to_jshtml())"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/17_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_17_closing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7 â€” Reflection and Next Steps\n",
    "\n",
    "### What We Built\n",
    "\n",
    "In this notebook, we implemented two fundamental building blocks used in state-of-the-art VLA driving systems:\n",
    "\n",
    "1. **Action Tokenization** â€” converting continuous steering angles into discrete tokens that a language model can predict. We showed that with 256 bins, the quantization error is negligible (less than 0.18 degrees).\n",
    "\n",
    "2. **Behavioral Cloning** â€” training a model to imitate expert drivers by minimizing cross-entropy loss over action tokens. The model learns to assign high probability to the expert's chosen actions, exactly like a language model learns to predict the next word.\n",
    "\n",
    "### Reflection Questions\n",
    "\n",
    "1. **Bin count tradeoffs:** We used 256 bins. What would happen with only 8 bins? The max error would jump to ~6.4 degrees â€” large enough to cause noticeable weaving. With 4096 bins? The error drops to 0.011 degrees, but the model must predict over a much larger vocabulary, which can slow convergence. 256 is the sweet spot for most driving applications.\n",
    "\n",
    "2. **Expert quality matters:** Behavioral cloning faithfully imitates whatever the expert does â€” including mistakes. If the expert occasionally swerves unnecessarily, the model learns that too. This is both a strength (simple, stable training) and a weakness (garbage in, garbage out). Systems like DAgger address this by iteratively collecting corrections.\n",
    "\n",
    "3. **Why cross-entropy and not MSE?** Cross-entropy treats action prediction as classification over bins, which handles multimodal distributions naturally. If there are two valid steering angles at a junction (slight left or slight right), cross-entropy can assign probability mass to both. MSE would average them, predicting \"go straight\" â€” which might be the worst option. This is a critical advantage for real-world driving.\n",
    "\n",
    "### Optional Challenges\n",
    "\n",
    "1. **Multi-dimensional tokenization:** Extend the tokenizer to handle both steering AND acceleration simultaneously. With 256 bins each, you need $256 \\times 256 = 65{,}536$ joint tokens â€” or you can tokenize each dimension independently (just 512 tokens total). Which approach do RT-2 and EMMA use, and why?\n",
    "\n",
    "2. **Autoregressive prediction:** Replace the parallel action heads with an LSTM or Transformer that predicts one action token at a time, conditioned on all previous predictions. This is closer to how real VLAs work. Does it improve trajectory smoothness?\n",
    "\n",
    "3. **Covariate shift experiment:** After training, roll out the model in the environment: at each step, use the model's *own predicted* position (not the expert's) to compute the next action. Watch what happens as small errors accumulate. This is the **covariate shift** problem â€” the model encounters states it never saw during training because it was always shown the expert's states. How quickly do trajectories diverge?"
   ],
   "id": "cell_34"
  }
 ]
}