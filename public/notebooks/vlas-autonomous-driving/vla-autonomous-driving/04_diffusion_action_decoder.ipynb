{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Diffusion Action Decoder (Alpamayo Style) â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1VLFlUCL9vCbtUm2KD6CbL9RdcA0VWmBm\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_why_it_matters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Action Decoder: Sculpting Trajectories from Noise\n",
    "\n",
    "*Part 4 of the Vizuara series on VLAs for Autonomous Driving*\n",
    "*Estimated time: 75 minutes | Runs on: Google Colab (T4 GPU)*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://course-creator-brown.vercel.app/courses/vla-autonomous-driving/practice/4/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In Notebook 2, we built an action tokenizer that discretizes continuous driving commands into tokens. It works â€” but a steering angle of 14.8 degrees and 15.2 degrees both map to the same token. At highway speeds, that 0.4-degree difference means drifting into an adjacent lane over a few seconds. For safety-critical driving, discretization artifacts are not acceptable.\n",
    "\n",
    "NVIDIA's Alpamayo takes a radically different approach: a **diffusion action decoder** (2.3B params) that generates infinitely smooth, continuous trajectories without any discretization. It takes internal representations from the VLM backbone and produces 64 waypoints at 10 Hz â€” 6.4 seconds of future driving, each with position (x, y, z) and rotation.\n",
    "\n",
    "The core idea: start with random noise and iteratively refine it into a clean trajectory â€” like sculpting a statue out of marble. By the end, you will build a conditional trajectory diffusion model, train it, and watch an animation of noise being sculpted into smooth driving paths."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_intuition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "**Analogy: The sculptor.** You cannot carve a perfect statue in a single strike. Instead, you work iteratively â€” remove large chunks for the rough form, refine contours, then smooth the surface. Diffusion models work the same way: start with pure noise (the marble) and iteratively denoise until a clean trajectory emerges.\n",
    "\n",
    "**Why not predict the trajectory directly?** Consider a car at a T-intersection. It could turn left, right, or go straight. A regression model trained with MSE loss would average these options and output a trajectory that goes straight into the building across the street. A diffusion model captures the *full distribution* â€” each sample can produce any valid option. This **multimodality** is why diffusion is the method of choice for trajectory generation.\n",
    "\n",
    "**The key insight.** Instead of learning to generate trajectories from scratch (very hard), we learn to *remove noise* (much easier). If I show you a slightly noisy trajectory and ask \"what noise was added?\", that is simpler than \"generate a perfect trajectory from nothing.\" Chain 100 such denoising steps together and we go from pure noise to a clean trajectory.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "Why 64 waypoints at 10 Hz? That is 6.4 seconds. At highway speeds (30 m/s), the model plans ~192 meters ahead â€” enough for lane changes and merges, but not so far that predictions become unreliable."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_math"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### 3.1 Forward Diffusion (Adding Noise)\n",
    "\n",
    "$$q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} \\, x_0, (1 - \\bar{\\alpha}_t) \\mathbf{I})$$\n",
    "\n",
    "We can jump from clean trajectory $x_0$ to any noise level $t$ in one step. $\\bar{\\alpha}_t$ controls signal retention: at $t=0$, $\\bar{\\alpha}_0 \\approx 1$ (unchanged); at $t=T$, $\\bar{\\alpha}_T \\approx 0$ (pure noise). In practice:\n",
    "\n",
    "$$x_t = \\sqrt{\\bar{\\alpha}_t} \\cdot x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\mathbf{I})$$\n",
    "\n",
    "**Worked example.** Waypoint $x_0 = [2.0, 0.5]$, $\\sqrt{\\bar{\\alpha}_t} = 0.8$, $\\sqrt{1-\\bar{\\alpha}_t} = 0.6$, noise $\\epsilon = [0.3, -0.1]$:\n",
    "\n",
    "$$x_t = 0.8 \\times [2.0, 0.5] + 0.6 \\times [0.3, -0.1] = [1.6, 0.4] + [0.18, -0.06] = [1.78, 0.34]$$\n",
    "\n",
    "### 3.2 Noise Schedule\n",
    "\n",
    "$$\\beta_t = \\beta_{\\min} + \\frac{t}{T}(\\beta_{\\max} - \\beta_{\\min}), \\quad \\alpha_t = 1 - \\beta_t, \\quad \\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$$\n",
    "\n",
    "$\\beta_t$ grows linearly from $0.0001$ to $0.02$. The cumulative product $\\bar{\\alpha}_t$ gives total signal remaining.\n",
    "\n",
    "**Worked example.** $T=100$: at $t=0$, $\\bar{\\alpha}_0 = 0.9999$ (99.99% signal). At $t=50$, $\\bar{\\alpha}_{50} \\approx 0.62$ (62%). At $t=99$, $\\bar{\\alpha}_{99} \\approx 0.006$ (pure noise).\n",
    "\n",
    "### 3.3 Training Objective\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}_{t, x_0, \\epsilon} \\left[ \\| \\epsilon - \\epsilon_\\theta(x_t, t, c) \\|^2 \\right]$$\n",
    "\n",
    "Given noisy trajectory $x_t$, timestep $t$, and conditioning $c$ (scene features), predict the noise $\\epsilon$. Just MSE between predicted and actual noise.\n",
    "\n",
    "**Worked example.** True $\\epsilon = [0.3, -0.1]$, predicted $\\hat{\\epsilon} = [0.25, -0.08]$. Loss: $(0.3-0.25)^2 + (-0.1+0.08)^2 = 0.0029$.\n",
    "\n",
    "### 3.4 Sampling (Denoising)\n",
    "\n",
    "$$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t, c) \\right) + \\sigma_t z$$\n",
    "\n",
    "Starting from $x_T$ (noise), apply this repeatedly. The predicted noise tells us which direction to step; $\\sigma_t z$ adds stochasticity for diversity.\n",
    "\n",
    "**Worked example.** At $t=50$: $x_{50}=[0.5,1.2]$, $\\hat{\\epsilon}=[0.3,-0.4]$, $\\beta_{50}=0.0101$, $\\sqrt{\\alpha_{50}}=0.9949$, $\\sqrt{1-\\bar{\\alpha}_{50}}=0.616$:\n",
    "\n",
    "$$x_{49} = \\frac{1}{0.9949}([0.5-0.0049, 1.2+0.0066]) + \\sigma z = [0.4975, 1.2127] + \\text{small noise}$$\n",
    "\n",
    "One step closer. After 100 steps: a smooth driving path."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Setup And Schedule\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_setup_and_schedule.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_setup_and_schedule"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let Us Build It\n",
    "\n",
    "### Setup"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Noise Schedule"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseSchedule:\n",
    "    \"\"\"Linear noise schedule â€” precomputes all quantities for training and sampling.\"\"\"\n",
    "    def __init__(self, n_timesteps=100, beta_min=1e-4, beta_max=0.02):\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.betas = torch.linspace(beta_min, beta_max, n_timesteps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
    "        self.sqrt_alpha_bars = torch.sqrt(self.alpha_bars)\n",
    "        self.sqrt_one_minus_alpha_bars = torch.sqrt(1.0 - self.alpha_bars)\n",
    "        self.sqrt_alphas = torch.sqrt(self.alphas)"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualize the noise schedule\n",
    "schedule = NoiseSchedule(n_timesteps=100)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(schedule.betas.numpy(), 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Timestep t'); axes[0].set_ylabel(r'$\\beta_t$')\n",
    "axes[0].set_title(r'Noise Rate $\\beta_t$'); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(schedule.alpha_bars.numpy(), 'r-', linewidth=2)\n",
    "axes[1].set_xlabel('Timestep t'); axes[1].set_ylabel(r'$\\bar{\\alpha}_t$')\n",
    "axes[1].set_title(r'Cumulative Signal $\\bar{\\alpha}_t$'); axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(schedule.sqrt_one_minus_alpha_bars.numpy(), 'g-', linewidth=2)\n",
    "axes[2].set_xlabel('Timestep t'); axes[2].set_ylabel(r'$\\sqrt{1-\\bar{\\alpha}_t}$')\n",
    "axes[2].set_title('Noise Level'); axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout(); plt.show()\n",
    "print(\"At t=0, signal preserved. At t=99, pure noise. This is our sculpting plan.\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Forward Diffusion\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_forward_diffusion.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_forward_diffusion"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Forward Diffusion"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion(x_0, t, schedule, noise=None):\n",
    "    \"\"\"Add noise to clean trajectories: x_t = sqrt(a_bar)*x_0 + sqrt(1-a_bar)*noise.\"\"\"\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_0)\n",
    "    sqrt_alpha_bar = schedule.sqrt_alpha_bars[t].reshape(-1, 1, 1)\n",
    "    sqrt_one_minus = schedule.sqrt_one_minus_alpha_bars[t].reshape(-1, 1, 1)\n",
    "    x_t = sqrt_alpha_bar * x_0 + sqrt_one_minus * noise\n",
    "    return x_t, noise"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Watch a trajectory dissolve into noise\n",
    "t_param = np.linspace(0, 1, 64)\n",
    "clean_traj = np.stack([t_param * 50, 5 * np.sin(2 * np.pi * t_param)], axis=1)\n",
    "x_0 = torch.FloatTensor(clean_traj).unsqueeze(0)\n",
    "\n",
    "timesteps_to_show = [0, 10, 25, 50, 75, 99]\n",
    "fig, axes = plt.subplots(1, 6, figsize=(20, 3))\n",
    "torch.manual_seed(42)\n",
    "for ax, ts in zip(axes, timesteps_to_show):\n",
    "    x_t, _ = forward_diffusion(x_0, torch.tensor([ts]), schedule)\n",
    "    traj = x_t.squeeze(0).numpy()\n",
    "    ax.plot(clean_traj[:, 0], clean_traj[:, 1], 'g-', alpha=0.3)\n",
    "    ax.plot(traj[:, 0], traj[:, 1], 'r-', linewidth=1.5)\n",
    "    ax.set_title(f't = {ts}', fontweight='bold')\n",
    "    ax.set_xlim(-20, 70); ax.set_ylim(-20, 20)\n",
    "    ax.grid(True, alpha=0.3); ax.set_aspect('equal')\n",
    "plt.suptitle(\"Forward Diffusion: Clean â†’ Noise\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout(); plt.show()"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Denoiser Network\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_denoiser_network.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_denoiser_network"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Denoising Network"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(nn.Module):\n",
    "    \"\"\"Sinusoidal timestep embedding (same trick as original Transformer).\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, t):\n",
    "        half = self.dim // 2\n",
    "        emb = torch.exp(torch.arange(half, device=t.device) * -(np.log(10000) / (half - 1)))\n",
    "        emb = t.unsqueeze(1) * emb.unsqueeze(0)\n",
    "        return torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "\n",
    "class TrajectoryDenoiser(nn.Module):\n",
    "    \"\"\"Predict noise in a trajectory, conditioned on timestep and scene features.\"\"\"\n",
    "    def __init__(self, traj_dim=2, hidden_dim=128, n_waypoints=64, condition_dim=32):\n",
    "        super().__init__()\n",
    "        self.n_waypoints = n_waypoints\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalPosEmb(hidden_dim), nn.Linear(hidden_dim, hidden_dim), nn.GELU())\n",
    "        self.cond_proj = nn.Linear(condition_dim, hidden_dim)\n",
    "        input_dim = n_waypoints * traj_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim + hidden_dim * 2, hidden_dim * 4), nn.GELU(),\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 4), nn.GELU(),\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2), nn.GELU(),\n",
    "            nn.Linear(hidden_dim * 2, input_dim))\n",
    "\n",
    "    def forward(self, x_t, t, condition):\n",
    "        batch = x_t.size(0)\n",
    "        x_flat = x_t.reshape(batch, -1)\n",
    "        t_emb = self.time_embed(t.float())\n",
    "        c_emb = self.cond_proj(condition)\n",
    "        noise_pred = self.net(torch.cat([x_flat, t_emb, c_emb], dim=-1))\n",
    "        return noise_pred.reshape(batch, self.n_waypoints, 2)"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "model_test = TrajectoryDenoiser().to(device)\n",
    "out = model_test(torch.randn(4,64,2).to(device), torch.tensor([10,20,50,90]).to(device),\n",
    "                 torch.randn(4,32).to(device))\n",
    "print(f\"Input: (4, 64, 2) â†’ Output: {out.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model_test.parameters()):,}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Synthetic Data\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_synthetic_data.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_synthetic_data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Synthetic Trajectory Data\n",
    "\n",
    "We generate five maneuver types â€” straight, lane change, left/right turns, S-curves â€” with random speed and amplitude variation. Each trajectory gets a condition vector (one-hot type + parameters), mimicking how Alpamayo conditions on VLM scene features."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory_dataset(n_samples=5000, n_waypoints=64):\n",
    "    \"\"\"Generate diverse 2D driving trajectories with condition labels.\"\"\"\n",
    "    trajectories, conditions = [], []\n",
    "    types = ['straight', 'lane_change', 'left_turn', 'right_turn', 's_curve']\n",
    "    for _ in range(n_samples):\n",
    "        t = np.linspace(0, 1, n_waypoints)\n",
    "        idx = np.random.randint(len(types))\n",
    "        speed = np.random.uniform(0.8, 1.2)\n",
    "        amp = np.random.uniform(0.8, 1.2)\n",
    "        if types[idx] == 'straight':\n",
    "            x, y = t * 50 * speed, np.random.normal(0, 0.3, n_waypoints).cumsum() * 0.1\n",
    "        elif types[idx] == 'lane_change':\n",
    "            x, y = t * 50 * speed, 3.5 * amp / (1 + np.exp(-10 * (t - 0.5)))\n",
    "        elif types[idx] == 'left_turn':\n",
    "            r, theta = 20*amp, t*np.pi/2; x, y = r*np.sin(theta), r*(1-np.cos(theta))\n",
    "        elif types[idx] == 'right_turn':\n",
    "            r, theta = 20*amp, t*np.pi/2; x, y = r*np.sin(theta), -r*(1-np.cos(theta))\n",
    "        else:\n",
    "            x, y = t * 50 * speed, 4.0 * amp * np.sin(2 * np.pi * t)\n",
    "        x += np.random.normal(0, 0.05, n_waypoints)\n",
    "        y += np.random.normal(0, 0.05, n_waypoints)\n",
    "        cond = np.zeros(32); cond[idx] = 1.0; cond[5] = speed; cond[6] = amp\n",
    "        trajectories.append(np.stack([x, y], axis=1)); conditions.append(cond)\n",
    "    return torch.FloatTensor(np.array(trajectories)), torch.FloatTensor(np.array(conditions))\n",
    "\n",
    "torch.manual_seed(42); np.random.seed(42)\n",
    "trajectories, conditions = generate_trajectory_dataset(5000, 64)\n",
    "print(f\"Dataset: {trajectories.shape[0]} trajectories, {trajectories.shape[1]} waypoints each\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualize training data\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "names = ['Straight', 'Lane Change', 'Left Turn', 'Right Turn', 'S-Curve']\n",
    "for col in range(5):\n",
    "    for j, traj in enumerate(trajectories[conditions[:, col] == 1.0][:8]):\n",
    "        axes[col].plot(traj[:, 0].numpy(), traj[:, 1].numpy(), alpha=0.5, linewidth=1)\n",
    "    axes[col].set_title(names[col], fontweight='bold'); axes[col].grid(True, alpha=0.3)\n",
    "    axes[col].set_aspect('equal')\n",
    "plt.suptitle(\"Training Data: Five Maneuver Types\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout(); plt.show()"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to normalize the trajectories before training â€” diffusion models work best when data has zero mean and unit variance."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize (diffusion works best with zero mean, unit variance)\n",
    "traj_mean = trajectories.mean(dim=(0, 1), keepdim=True)\n",
    "traj_std = trajectories.std(dim=(0, 1), keepdim=True) + 1e-6\n",
    "trajectories_norm = (trajectories - traj_mean) / traj_std\n",
    "print(f\"Normalized â€” mean: {trajectories_norm.mean():.4f}, std: {trajectories_norm.std():.4f}\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Training\n",
    "\n",
    "For each batch: sample random timesteps, add noise, predict noise, compute MSE, backpropagate. That is the entire algorithm."
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoiser = TrajectoryDenoiser(traj_dim=2, hidden_dim=128, n_waypoints=64, condition_dim=32).to(device)\n",
    "schedule = NoiseSchedule(n_timesteps=100)\n",
    "optimizer = torch.optim.Adam(denoiser.parameters(), lr=1e-3)\n",
    "batch_size, n_epochs = 128, 50\n",
    "losses = []\n",
    "print(f\"Parameters: {sum(p.numel() for p in denoiser.parameters()):,}\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    denoiser.train()\n",
    "    perm = torch.randperm(len(trajectories_norm))\n",
    "    epoch_loss, n_batches = 0, 0\n",
    "    for i in range(0, len(trajectories_norm), batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        x_0 = trajectories_norm[idx].to(device)\n",
    "        cond = conditions[idx].to(device)\n",
    "        t = torch.randint(0, schedule.n_timesteps, (len(idx),), device=device)\n",
    "        noise = torch.randn_like(x_0)\n",
    "        sqrt_ab = schedule.sqrt_alpha_bars[t].reshape(-1,1,1).to(device)\n",
    "        sqrt_1mab = schedule.sqrt_one_minus_alpha_bars[t].reshape(-1,1,1).to(device)\n",
    "        x_t = sqrt_ab * x_0 + sqrt_1mab * noise\n",
    "        loss = F.mse_loss(denoiser(x_t, t, cond), noise)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        epoch_loss += loss.item(); n_batches += 1\n",
    "    losses.append(epoch_loss / n_batches)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{n_epochs} | Loss: {losses[-1]:.6f}\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Training curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch'); plt.ylabel('MSE Loss')\n",
    "plt.title('Diffusion Denoiser Training'); plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "print(\"Lower loss = better noise prediction = smoother trajectories.\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Sampling\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_sampling.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_sampling"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Sampling (Reverse Diffusion)\n",
    "\n",
    "Start from pure noise. At each step, predict noise and subtract it. After 100 steps: a clean trajectory."
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_trajectories(denoiser, schedule, condition, n_samples=1,\n",
    "                        n_waypoints=64, device='cpu', return_intermediates=False):\n",
    "    \"\"\"Generate trajectories by iterative denoising.\"\"\"\n",
    "    denoiser.eval()\n",
    "    x = torch.randn(n_samples, n_waypoints, 2, device=device)\n",
    "    intermediates = [x.cpu().clone()]\n",
    "    for t in reversed(range(schedule.n_timesteps)):\n",
    "        t_batch = torch.full((n_samples,), t, device=device, dtype=torch.long)\n",
    "        noise_pred = denoiser(x, t_batch, condition)\n",
    "        beta_t = schedule.betas[t].to(device)\n",
    "        sqrt_alpha_t = schedule.sqrt_alphas[t].to(device)\n",
    "        sqrt_1m_ab = schedule.sqrt_one_minus_alpha_bars[t].to(device)\n",
    "        x = (1 / sqrt_alpha_t) * (x - (beta_t / sqrt_1m_ab) * noise_pred)\n",
    "        if t > 0:\n",
    "            x = x + torch.sqrt(beta_t) * torch.randn_like(x)\n",
    "        if return_intermediates and t % 5 == 0:\n",
    "            intermediates.append(x.cpu().clone())\n",
    "    return (x, intermediates) if return_intermediates else x"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo1 Forward\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_todo1_forward.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_todo1_forward"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Implement Forward Diffusion\n",
    "\n",
    "Implement $x_t = \\sqrt{\\bar{\\alpha}_t} \\cdot x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\epsilon$."
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_forward_diffusion(x_0, t, sqrt_alpha_bars, sqrt_one_minus_alpha_bars):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x_0: (batch, n_waypoints, 2) â€” clean trajectories\n",
    "        t: (batch,) â€” timestep indices\n",
    "        sqrt_alpha_bars, sqrt_one_minus_alpha_bars: precomputed schedule values\n",
    "    Returns: (x_t, noise) â€” noisy trajectories and the noise added\n",
    "\n",
    "    Hints: torch.randn_like for noise, index with t, reshape to (-1,1,1) for broadcasting\n",
    "    \"\"\"\n",
    "    # ============ YOUR CODE HERE ============\n",
    "    noise = ???       # Step 1: Sample noise ~ N(0, I)\n",
    "    sqrt_ab = ???     # Step 2: Look up sqrt(alpha_bar_t), reshape to (batch, 1, 1)\n",
    "    sqrt_1mab = ???   # Step 3: Look up sqrt(1 - alpha_bar_t), reshape\n",
    "    x_t = ???         # Step 4: x_t = sqrt_ab * x_0 + sqrt_1mab * noise\n",
    "    # ========================================\n",
    "    return x_t, noise"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Verification\n",
    "torch.manual_seed(123)\n",
    "x_test = torch.randn(4, 64, 2)\n",
    "t_test = torch.tensor([0, 25, 50, 99])\n",
    "x_t, eps = my_forward_diffusion(x_test, t_test, schedule.sqrt_alpha_bars,\n",
    "                                 schedule.sqrt_one_minus_alpha_bars)\n",
    "assert x_t.shape == x_test.shape, f\"Wrong shape: {x_t.shape}\"\n",
    "diff_t0 = (x_t[0] - x_test[0]).abs().max().item()\n",
    "assert diff_t0 < 0.15, f\"At t=0, x_t should â‰ˆ x_0 (diff={diff_t0:.3f})\"\n",
    "assert (x_t[3] - x_test[3]).abs().mean().item() > 0.5, \"At t=99, should be mostly noise\"\n",
    "print(f\"âœ… Forward diffusion correct! t=0 deviation: {diff_t0:.4f}\")"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo2 Denoise Step\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_todo2_denoise_step.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_11_todo2_denoise_step"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement a Denoising Step\n",
    "\n",
    "Implement one reverse step: $x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta) + \\sigma_t z$"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_denoise_step(x_t, t, noise_pred, schedule):\n",
    "    \"\"\"\n",
    "    One reverse diffusion step: x_t â†’ x_{t-1}.\n",
    "    Args:\n",
    "        x_t: (batch, n_waypoints, 2), t: int, noise_pred: (batch, n_waypoints, 2)\n",
    "    Hints: Get beta_t, sqrt_alpha_t, sqrt_1m_alpha_bar from schedule.\n",
    "           If t > 0, add sigma_t * z where sigma_t = sqrt(beta_t).\n",
    "    \"\"\"\n",
    "    # ============ YOUR CODE HERE ============\n",
    "    beta_t = ???\n",
    "    sqrt_alpha_t = ???\n",
    "    sqrt_1m_alpha_bar = ???\n",
    "    x_prev = ???  # mean = (1/sqrt_alpha_t) * (x_t - (beta_t/sqrt_1m_alpha_bar) * noise_pred)\n",
    "    if t > 0:\n",
    "        x_prev = ???  # add sigma_t * z\n",
    "    # ========================================\n",
    "    return x_prev"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Verification\n",
    "torch.manual_seed(456)\n",
    "x_noisy = torch.randn(2, 64, 2)\n",
    "fake_noise = torch.randn(2, 64, 2)\n",
    "x_out = my_denoise_step(x_noisy, t=50, noise_pred=fake_noise, schedule=schedule)\n",
    "assert x_out.shape == x_noisy.shape, f\"Wrong shape: {x_out.shape}\"\n",
    "assert not torch.allclose(x_out, x_noisy, atol=1e-6), \"Output should differ from input\"\n",
    "x_final = my_denoise_step(x_noisy, t=0, noise_pred=fake_noise, schedule=schedule)\n",
    "assert x_final.shape == x_noisy.shape\n",
    "print(\"âœ… Denoising step correct! You have all pieces for a full sampling loop.\")"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo3 Sampling Loop\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_todo3_sampling_loop.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_12_todo3_sampling_loop"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 3 (Bonus): Full Sampling Loop"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_sample(denoiser, schedule, condition, n_waypoints=64, device='cpu'):\n",
    "    \"\"\"Generate a trajectory from pure noise using your denoise step.\"\"\"\n",
    "    denoiser.eval()\n",
    "    # ============ YOUR CODE HERE ============\n",
    "    x = ???  # Start from pure noise: (1, n_waypoints, 2)\n",
    "    with torch.no_grad():\n",
    "        for t in reversed(range(schedule.n_timesteps)):\n",
    "            t_batch = ???       # (1,) tensor of current timestep\n",
    "            noise_pred = ???    # denoiser(x, t_batch, condition)\n",
    "            x = ???             # my_denoise_step(x, t, noise_pred, schedule)\n",
    "    # ========================================\n",
    "    return x"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your sampling loop\n",
    "cond_test = torch.zeros(1, 32).to(device)\n",
    "cond_test[0, 0] = 1.0; cond_test[0, 5] = 1.0; cond_test[0, 6] = 1.0\n",
    "result = my_sample(denoiser, schedule, cond_test, device=device)\n",
    "result_np = (result.cpu() * traj_std + traj_mean).squeeze(0).numpy()\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(result_np[:, 0], result_np[:, 1], 'b-o', markersize=2, linewidth=1.5)\n",
    "plt.plot(result_np[0, 0], result_np[0, 1], 'go', markersize=10, label='Start')\n",
    "plt.plot(result_np[-1, 0], result_np[-1, 1], 'ro', markersize=10, label='End')\n",
    "plt.title(\"Your Generated Trajectory\", fontweight='bold')\n",
    "plt.legend(); plt.grid(True, alpha=0.3); plt.axis('equal'); plt.tight_layout(); plt.show()\n",
    "print(\"Smooth trajectory? Your diffusion model works end to end!\")"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: All Maneuvers And Animation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_all_maneuvers_and_animation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_13_all_maneuvers_and_animation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Output: All Maneuver Types + Animation"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Generate all five maneuver types\n",
    "type_labels = ['Straight', 'Lane Change', 'Left Turn', 'Right Turn', 'S-Curve']\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "\n",
    "for col, label in enumerate(type_labels):\n",
    "    cond = torch.zeros(1, 32).to(device)\n",
    "    cond[0, col] = 1.0; cond[0, 5] = 1.0; cond[0, 6] = 1.0\n",
    "    final_traj, intermediates = sample_trajectories(\n",
    "        denoiser, schedule, cond, n_samples=1, n_waypoints=64,\n",
    "        device=device, return_intermediates=True)\n",
    "    final_np = (final_traj.cpu() * traj_std + traj_mean).squeeze(0).numpy()\n",
    "\n",
    "    axes[0, col].plot(final_np[:, 0], final_np[:, 1], 'b-', linewidth=2)\n",
    "    axes[0, col].plot(final_np[0, 0], final_np[0, 1], 'go', markersize=8)\n",
    "    axes[0, col].plot(final_np[-1, 0], final_np[-1, 1], 'ro', markersize=8)\n",
    "    axes[0, col].set_title(label, fontweight='bold')\n",
    "    axes[0, col].grid(True, alpha=0.3); axes[0, col].set_aspect('equal')\n",
    "\n",
    "    n_show = min(6, len(intermediates))\n",
    "    idxs = np.linspace(0, len(intermediates)-1, n_show, dtype=int)\n",
    "    colors = plt.cm.Blues(np.linspace(0.2, 1.0, n_show))\n",
    "    for j, si in enumerate(idxs):\n",
    "        ti = (intermediates[si] * traj_std + traj_mean).squeeze(0).numpy()\n",
    "        axes[1, col].plot(ti[:, 0], ti[:, 1], '-', color=colors[j],\n",
    "                         alpha=0.4+0.6*(j/(n_show-1)), linewidth=1)\n",
    "    axes[1, col].set_title('Denoising Steps'); axes[1, col].grid(True, alpha=0.3)\n",
    "    axes[1, col].set_aspect('equal')\n",
    "\n",
    "plt.suptitle(\"Diffusion: From Noise to Smooth Driving Paths\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout(); plt.show()\n",
    "print(\"This is exactly what we want â€” diverse, smooth trajectories for each maneuver type.\")"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the showstopper: an animation of the full denoising process."
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture all 100 denoising steps for a lane change\n",
    "cond_anim = torch.zeros(1, 32).to(device)\n",
    "cond_anim[0, 1] = 1.0; cond_anim[0, 5] = 1.0; cond_anim[0, 6] = 1.0\n",
    "denoiser.eval(); torch.manual_seed(42)\n",
    "x = torch.randn(1, 64, 2, device=device)\n",
    "all_steps = [x.cpu().clone()]\n",
    "with torch.no_grad():\n",
    "    for t in reversed(range(schedule.n_timesteps)):\n",
    "        t_batch = torch.full((1,), t, device=device, dtype=torch.long)\n",
    "        noise_pred = denoiser(x, t_batch, cond_anim)\n",
    "        beta_t = schedule.betas[t].to(device)\n",
    "        x = (1/schedule.sqrt_alphas[t].to(device)) * (\n",
    "            x - (beta_t/schedule.sqrt_one_minus_alpha_bars[t].to(device)) * noise_pred)\n",
    "        if t > 0: x = x + torch.sqrt(beta_t) * torch.randn_like(x)\n",
    "        all_steps.append(x.cpu().clone())\n",
    "print(f\"Captured {len(all_steps)} snapshots\")"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us animate the full denoising process â€” watch a lane-change trajectory materialize from pure noise, one step at a time."
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¬ Animate: noise â†’ trajectory\n",
    "fig_anim, ax_anim = plt.subplots(figsize=(10, 5))\n",
    "def update(frame):\n",
    "    ax_anim.clear()\n",
    "    traj = (all_steps[frame] * traj_std + traj_mean).squeeze(0).numpy()\n",
    "    ax_anim.plot(traj[:, 0], traj[:, 1], 'b-o', markersize=2, linewidth=1.5)\n",
    "    ax_anim.plot(traj[0, 0], traj[0, 1], 'go', markersize=8)\n",
    "    ax_anim.plot(traj[-1, 0], traj[-1, 1], 'ro', markersize=8)\n",
    "    p = frame / (len(all_steps) - 1)\n",
    "    phase = (\"Pure Noise\" if p < 0.05 else \"Rough Shape...\" if p < 0.3 else\n",
    "             \"Sculpting...\" if p < 0.7 else \"Refining...\" if p < 0.95 else \"Clean Trajectory!\")\n",
    "    ax_anim.set_title(f\"Step {frame}/{len(all_steps)-1} â€” {phase}\", fontsize=13, fontweight='bold')\n",
    "    ax_anim.set_xlim(-30, 80); ax_anim.set_ylim(-15, 15)\n",
    "    ax_anim.grid(True, alpha=0.3); ax_anim.set_aspect('equal')\n",
    "    ax_anim.set_xlabel('x (meters)'); ax_anim.set_ylabel('y (meters)')\n",
    "anim = animation.FuncAnimation(fig_anim, update, frames=len(all_steps), interval=50)\n",
    "HTML(anim.to_jshtml())"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Multimodality\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_multimodality.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_14_multimodality"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multimodality: The Killer Feature"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š 10 trajectories from the SAME condition â€” demonstrating distribution capture\n",
    "cond_multi = torch.zeros(10, 32).to(device)\n",
    "cond_multi[:, 2] = 1.0; cond_multi[:, 5] = 1.0; cond_multi[:, 6] = 1.0\n",
    "torch.manual_seed(0)\n",
    "multi = sample_trajectories(denoiser, schedule, cond_multi, n_samples=10,\n",
    "                            n_waypoints=64, device=device)\n",
    "multi = (multi.cpu() * traj_std + traj_mean).numpy()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(10):\n",
    "    c = plt.cm.tab10(i/10)\n",
    "    plt.plot(multi[i,:,0], multi[i,:,1], '-', color=c, linewidth=1.5, alpha=0.7, label=f'Sample {i+1}')\n",
    "    plt.plot(multi[i,0,0], multi[i,0,1], 'o', color=c, markersize=6)\n",
    "plt.title(\"10 Left Turns from Same Condition\\n(Different noise â†’ different valid trajectories)\",\n",
    "          fontsize=13, fontweight='bold')\n",
    "plt.xlabel('x (m)'); plt.ylabel('y (m)')\n",
    "plt.legend(fontsize=8); plt.grid(True, alpha=0.3); plt.axis('equal')\n",
    "plt.tight_layout(); plt.show()\n",
    "print(\"All valid left turns, all different. Regression would give ONE average (wrong) trajectory.\")\n",
    "print(\"Diffusion captures the full distribution â€” this is exactly what we want.\")"
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/15_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_15_closing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reflection and Next Steps\n",
    "\n",
    "### What We Built\n",
    "\n",
    "A complete diffusion action decoder: noise schedule, forward diffusion, denoising network, and reverse sampling. The same principle powers Alpamayo's 2.3B-parameter decoder â€” differences are in scale (MLP vs. transformer), dimensionality (2D vs. 3D poses), and conditioning (one-hot vs. VLM features). The core algorithm is identical.\n",
    "\n",
    "### Reflection Questions\n",
    "\n",
    "1. We used 100 steps. With 10? Faster but lower quality. DDIM achieves good quality in 10 steps â€” critical for real-time driving at 10 Hz.\n",
    "2. Why 6.4 seconds? At 30 m/s, that is ~192m ahead. How would you choose the horizon for city vs. highway driving?\n",
    "3. We showed multimodality, but a real car must pick ONE trajectory. How would you score candidates for safety, comfort, and efficiency?\n",
    "\n",
    "### Optional Challenges\n",
    "\n",
    "1. **DDIM Sampling:** Deterministic sampler â€” reduce from 100 to 10 steps with minimal quality loss.\n",
    "2. **Classifier-Free Guidance:** Drop condition 10% during training. At inference: $\\hat{\\epsilon} = \\epsilon_{\\text{unc}} + s(\\epsilon_{\\text{cond}} - \\epsilon_{\\text{unc}})$ for stronger conditioning.\n",
    "3. **3D Trajectories:** Extend to (x, y, z) for hills and overpasses â€” what Alpamayo actually outputs.\n",
    "4. **Cosine Schedule:** $\\bar{\\alpha}_t = \\cos(\\frac{t/T+s}{1+s}\\frac{\\pi}{2})^2$ â€” spends more steps at intermediate noise levels."
   ],
   "id": "cell_43"
  }
 ]
}