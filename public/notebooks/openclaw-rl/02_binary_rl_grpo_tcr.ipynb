{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Binary RL with GRPO-TCR ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1RJjttCvltRK-j5XaI_Tp752cibGKRYMf\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/02_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_01_setup",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_01_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Binary RL with GRPO-TCR: Scoring Responses and Training from Feedback\n",
    "\n",
    "*Part 2 of the Vizuara series on OpenClaw-RL*\n",
    "*Estimated time: 50 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ü§ñ AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** ‚Äî it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[üëâ Open AI Teaching Assistant](https://pods.vizuara.ai/courses/openclaw-rl/practice/2/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_02_why_this_matters",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why This Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_02_why_this_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebook, we built a system that extracts training samples from conversations. Each sample tells us what the assistant said and how the user reacted.\n",
    "\n",
    "But how do we actually **use** this data to improve the model?\n",
    "\n",
    "We need two things:\n",
    "1. A way to **score** each response (Was it good? Bad? Neutral?)\n",
    "2. A way to **update** the model so it produces more good responses and fewer bad ones\n",
    "\n",
    "This is where **Binary RL with GRPO-TCR** comes in. GRPO (Group-Relative Policy Optimization) eliminates the need for a critic network ‚Äî cutting memory and compute costs by ~50%. The TCR recipe (Token-level loss + Clip-higher + Reward shaping) makes it work reliably in practice.\n",
    "\n",
    "By the end of this notebook, you will have implemented:\n",
    "- A **Process Reward Model (PRM)** with majority voting\n",
    "- **GRPO advantage computation** from scratch\n",
    "- The **clip-higher** asymmetric clipping mechanism\n",
    "- **Overlong reward shaping** for length control\n",
    "- The complete **GRPO-TCR loss function**\n",
    "- A working training loop on synthetic conversation data"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Teaser: We'll train a model where responses improve over iterations\n",
    "# Starting: random rewards, no learning\n",
    "# After GRPO-TCR: model learns to prefer good responses over bad ones!"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_03_building_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Building Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_03_building_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Let us build some intuition before we touch any math.\n",
    "\n",
    "Imagine you are a teacher grading essays. You have 4 students who each wrote an essay on the same topic.\n",
    "\n",
    "- Student A wrote an excellent essay ‚Üí Grade: A+\n",
    "- Student B wrote a terrible essay ‚Üí Grade: F\n",
    "- Student C wrote a good essay ‚Üí Grade: A\n",
    "- Student D wrote an average essay ‚Üí Grade: C\n",
    "\n",
    "Now, instead of giving absolute grades, you **rank them relative to each other**:\n",
    "- A and C are above average ‚Üí encourage this kind of writing\n",
    "- B is far below average ‚Üí strongly discourage this\n",
    "- D is slightly below average ‚Üí mildly discourage this\n",
    "\n",
    "This is the core idea of GRPO: **group-relative advantages**. We do not need an absolute standard (a critic network). We just need to know which responses in the group were better than others.\n",
    "\n",
    "But how do we get those initial grades? In OpenClaw-RL, a **Process Reward Model (PRM)** acts as the grader. It is a separate language model that looks at the (response, user feedback) pair and decides: good, neutral, or bad.\n",
    "\n",
    "One evaluation can be noisy (maybe the PRM had a bad day). So we run it **multiple times** and take a **majority vote** ‚Äî just like having multiple teachers grade the same essay.\n",
    "\n",
    "### ü§î Think About This\n",
    "\n",
    "If the PRM is correct 70% of the time, and we run 5 evaluations, what is the probability that the majority vote gives the correct answer? (Hint: it is significantly higher than 70%.)"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_04_mathematics",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Mathematics\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_04_mathematics.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### 3.1 PRM Majority Voting\n",
    "\n",
    "For each (response, next-state) pair, the PRM runs $m$ evaluations and produces votes $v_1, v_2, \\ldots, v_m \\in \\{-1, 0, +1\\}$.\n",
    "\n",
    "The final reward is determined by majority vote:\n",
    "\n",
    "$$r = \\text{mode}(\\{v_1, v_2, \\ldots, v_m\\})$$\n",
    "\n",
    "Computationally: count how many votes are +1, 0, and -1. Whichever category has the most votes wins.\n",
    "\n",
    "For example, with $m = 5$ votes: $[+1, +1, -1, +1, 0]$. Count: three +1, one -1, one 0. The majority is +1, so $r = +1$.\n",
    "\n",
    "### 3.2 Group-Relative Advantages\n",
    "\n",
    "Given $G$ responses to the same prompt with rewards $r_1, r_2, \\ldots, r_G$, the normalized advantage is:\n",
    "\n",
    "$$\\hat{A}_i = \\frac{r_i - \\text{mean}(\\{r_1, \\ldots, r_G\\})}{\\text{std}(\\{r_1, \\ldots, r_G\\})}$$\n",
    "\n",
    "Computationally: subtract the group mean from each reward, then divide by the standard deviation. This centers the advantages around zero, so the model knows which responses were above average and which were below.\n",
    "\n",
    "### 3.3 The GRPO Clipped Surrogate Loss\n",
    "\n",
    "$$J_{\\text{GRPO}}(\\theta) = \\mathbb{E}\\left[\\min\\left(\\rho_t \\hat{A}_t,\\; \\text{clip}(\\rho_t, 1-\\epsilon_{\\text{low}}, 1+\\epsilon_{\\text{high}}) \\hat{A}_t\\right) - \\beta D_{\\text{KL}}(\\pi_\\theta \\| \\pi_{\\text{ref}})\\right]$$\n",
    "\n",
    "where $\\rho_t = \\frac{\\pi_\\theta(a_t \\mid s_t)}{\\pi_{\\text{ref}}(a_t \\mid s_t)}$ is the probability ratio between the new and reference policies.\n",
    "\n",
    "Computationally: we compute how much the new policy differs from the old one at each token. If it changed too much (ratio too high or too low), the clip mechanism prevents the update from being too aggressive. This keeps training stable.\n",
    "\n",
    "### 3.4 Overlong Reward Shaping\n",
    "\n",
    "$$r_{\\text{length}}(y) = \\begin{cases} 0 & \\text{if } |y| \\leq L_{\\max} - L_{\\text{cache}} \\\\ \\frac{(L_{\\max} - L_{\\text{cache}}) - |y|}{L_{\\text{cache}}} & \\text{if } L_{\\max} - L_{\\text{cache}} < |y| \\leq L_{\\max} \\\\ -1 & \\text{if } |y| > L_{\\max} \\end{cases}$$\n",
    "\n",
    "Computationally: responses within the safe zone get no penalty. Responses approaching the limit get a linearly increasing penalty. Responses exceeding the limit get the maximum penalty of -1."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_05_setup_imports",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Transition: Setup Imports\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_05_setup_imports.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It ‚Äî Component by Component\n",
    "\n",
    "### 4.1 Setup and Imports"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Using device: {device}\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_06_prm_majority_voting",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Prm Majority Voting\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_06_prm_majority_voting.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Process Reward Model (PRM) with Majority Voting\n",
    "\n",
    "Let us build a simulated PRM. In the real system, the PRM is a separate language model. Here, we simulate it with a noisy oracle that is correct ~70% of the time."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessRewardModel:\n",
    "    \"\"\"\n",
    "    Simulated Process Reward Model (PRM) with majority voting.\n",
    "    In production, this would be a separate language model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, accuracy: float = 0.7, num_votes: int = 5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            accuracy: Probability of the PRM giving the correct verdict\n",
    "            num_votes: Number of evaluations (m) for majority voting\n",
    "        \"\"\"\n",
    "        self.accuracy = accuracy\n",
    "        self.num_votes = num_votes\n",
    "\n",
    "    def evaluate_single(self, true_quality: int) -> int:\n",
    "        \"\"\"\n",
    "        Single PRM evaluation. Returns +1 (good), 0 (neutral), or -1 (bad).\n",
    "\n",
    "        Args:\n",
    "            true_quality: The actual quality of the response (-1, 0, or +1)\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.accuracy:\n",
    "            return true_quality  # Correct evaluation\n",
    "        else:\n",
    "            # Random incorrect evaluation\n",
    "            options = [-1, 0, 1]\n",
    "            options.remove(true_quality)\n",
    "            return np.random.choice(options)\n",
    "\n",
    "    def evaluate_with_majority_voting(self, true_quality: int) -> Tuple[int, List[int]]:\n",
    "        \"\"\"\n",
    "        Run m evaluations and return majority vote.\n",
    "\n",
    "        Args:\n",
    "            true_quality: The actual quality of the response\n",
    "\n",
    "        Returns:\n",
    "            (final_reward, list_of_individual_votes)\n",
    "        \"\"\"\n",
    "        votes = [self.evaluate_single(true_quality) for _ in range(self.num_votes)]\n",
    "        vote_counts = Counter(votes)\n",
    "        majority_reward = vote_counts.most_common(1)[0][0]\n",
    "        return majority_reward, votes\n",
    "\n",
    "# Test the PRM\n",
    "prm = ProcessRewardModel(accuracy=0.7, num_votes=5)\n",
    "\n",
    "# Simulate evaluating a good response\n",
    "print(\"Evaluating a GOOD response (true quality = +1):\")\n",
    "for trial in range(5):\n",
    "    reward, votes = prm.evaluate_with_majority_voting(true_quality=+1)\n",
    "    print(f\"  Trial {trial+1}: votes={votes} ‚Üí majority={reward:+d}\")\n",
    "\n",
    "print(\"\\nEvaluating a BAD response (true quality = -1):\")\n",
    "for trial in range(5):\n",
    "    reward, votes = prm.evaluate_with_majority_voting(true_quality=-1)\n",
    "    print(f\"  Trial {trial+1}: votes={votes} ‚Üí majority={reward:+d}\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Visualization: How Majority Voting Improves Accuracy"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_majority_voting_accuracy(prm_accuracy, num_votes_list, num_trials=1000):\n",
    "    \"\"\"Measure how majority voting improves PRM accuracy.\"\"\"\n",
    "    results = {}\n",
    "    for num_votes in num_votes_list:\n",
    "        prm = ProcessRewardModel(accuracy=prm_accuracy, num_votes=num_votes)\n",
    "        correct = 0\n",
    "        for _ in range(num_trials):\n",
    "            true_quality = np.random.choice([-1, 0, 1])\n",
    "            reward, _ = prm.evaluate_with_majority_voting(true_quality)\n",
    "            if reward == true_quality:\n",
    "                correct += 1\n",
    "        results[num_votes] = correct / num_trials\n",
    "    return results\n",
    "\n",
    "# Test with different numbers of votes\n",
    "num_votes_list = [1, 3, 5, 7, 9, 11, 15, 21]\n",
    "results = measure_majority_voting_accuracy(0.7, num_votes_list)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(list(results.keys()), list(results.values()), 'o-',\n",
    "        color='#3498db', linewidth=2, markersize=8)\n",
    "ax.axhline(y=0.7, color='#e74c3c', linestyle='--', label='Single PRM accuracy (70%)')\n",
    "ax.set_xlabel('Number of Majority Votes (m)', fontsize=12)\n",
    "ax.set_ylabel('Effective Accuracy', fontsize=12)\n",
    "ax.set_title('Majority Voting Dramatically Improves PRM Accuracy', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Effective accuracy with majority voting:\")\n",
    "for m, acc in results.items():\n",
    "    improvement = acc - 0.7\n",
    "    print(f\"  m={m:2d}: {acc:.1%} ({improvement:+.1%} vs single eval)\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_07_grpo_advantages",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Grpo Advantages\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_07_grpo_advantages.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 GRPO Advantage Computation\n",
    "\n",
    "Now let us implement the group-relative advantage normalization:"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grpo_advantages(rewards: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute group-relative advantages.\n",
    "\n",
    "    Args:\n",
    "        rewards: Tensor of shape (G,) with rewards for G responses\n",
    "\n",
    "    Returns:\n",
    "        advantages: Tensor of shape (G,) with normalized advantages\n",
    "    \"\"\"\n",
    "    mean = rewards.mean()\n",
    "    std = rewards.std()\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if std < 1e-8:\n",
    "        return torch.zeros_like(rewards)\n",
    "\n",
    "    advantages = (rewards - mean) / std\n",
    "    return advantages\n",
    "\n",
    "# Worked example from the article\n",
    "rewards = torch.tensor([1.0, -1.0, 1.0, 0.0])\n",
    "\n",
    "advantages = compute_grpo_advantages(rewards)\n",
    "\n",
    "print(\"GRPO Advantage Computation:\")\n",
    "print(f\"  Rewards:    {rewards.tolist()}\")\n",
    "print(f\"  Mean:       {rewards.mean():.2f}\")\n",
    "print(f\"  Std:        {rewards.std():.2f}\")\n",
    "print(f\"  Advantages: {[f'{a:.2f}' for a in advantages.tolist()]}\")\n",
    "print()\n",
    "print(\"  Response 1 (r=+1): Advantage = {:.2f} ‚Üí INCREASE probability\".format(advantages[0]))\n",
    "print(\"  Response 2 (r=-1): Advantage = {:.2f} ‚Üí DECREASE probability\".format(advantages[1]))\n",
    "print(\"  Response 3 (r=+1): Advantage = {:.2f} ‚Üí INCREASE probability\".format(advantages[2]))\n",
    "print(\"  Response 4 (r= 0): Advantage = {:.2f} ‚Üí Slightly decrease\".format(advantages[3]))"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Visualization: How Advantages Distribute Across a Group"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Rewards vs Advantages\n",
    "x = np.arange(len(rewards))\n",
    "width = 0.35\n",
    "axes[0].bar(x - width/2, rewards.numpy(), width, label='Raw Rewards',\n",
    "            color='#3498db', alpha=0.8)\n",
    "axes[0].bar(x + width/2, advantages.numpy(), width, label='GRPO Advantages',\n",
    "            color='#e74c3c', alpha=0.8)\n",
    "axes[0].set_xlabel('Response Index')\n",
    "axes[0].set_title('Raw Rewards vs Group-Relative Advantages', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Distribution over many groups\n",
    "all_advantages = []\n",
    "for _ in range(500):\n",
    "    r = torch.tensor(np.random.choice([-1.0, 0.0, 1.0], size=8))\n",
    "    a = compute_grpo_advantages(r)\n",
    "    all_advantages.extend(a.tolist())\n",
    "\n",
    "axes[1].hist(all_advantages, bins=50, color='#9b59b6', alpha=0.7, edgecolor='white')\n",
    "axes[1].set_xlabel('Advantage Value')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Distribution of GRPO Advantages (500 groups)', fontsize=12, fontweight='bold')\n",
    "axes[1].axvline(x=0, color='#e74c3c', linestyle='--', label='Zero (average)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_08_overlong_reward",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Overlong Reward\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_08_overlong_reward.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Overlong Reward Shaping\n",
    "\n",
    "Responses that are too long should be penalized ‚Äî but smoothly, not with a hard cutoff:"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlong_reward(response_length: int, L_max: int = 1000, L_cache: int = 200) -> float:\n",
    "    \"\"\"\n",
    "    Compute the overlong reward penalty.\n",
    "\n",
    "    Args:\n",
    "        response_length: Number of tokens in the response\n",
    "        L_max: Maximum allowed response length\n",
    "        L_cache: Size of the penalty transition zone\n",
    "\n",
    "    Returns:\n",
    "        Penalty in [-1, 0]\n",
    "    \"\"\"\n",
    "    safe_zone = L_max - L_cache\n",
    "\n",
    "    if response_length <= safe_zone:\n",
    "        return 0.0    # No penalty ‚Äî within safe zone\n",
    "    elif response_length <= L_max:\n",
    "        # Linear penalty in the transition zone\n",
    "        return (safe_zone - response_length) / L_cache\n",
    "    else:\n",
    "        return -1.0   # Maximum penalty ‚Äî exceeded limit\n",
    "\n",
    "# Visualize the penalty function\n",
    "lengths = np.arange(0, 1300)\n",
    "penalties = [overlong_reward(l) for l in lengths]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(lengths, penalties, linewidth=2.5, color='#e74c3c')\n",
    "ax.axvline(x=800, color='#2ecc71', linestyle='--', alpha=0.7, label='Safe zone boundary (L_max - L_cache)')\n",
    "ax.axvline(x=1000, color='#e74c3c', linestyle='--', alpha=0.7, label='Maximum length (L_max)')\n",
    "ax.fill_between(lengths[:801], 0, [penalties[i] for i in range(801)],\n",
    "                alpha=0.1, color='#2ecc71', label='Safe zone (no penalty)')\n",
    "ax.fill_between(lengths[800:1001], 0, penalties[800:1001],\n",
    "                alpha=0.1, color='#f39c12', label='Transition zone')\n",
    "ax.fill_between(lengths[1000:], -1, penalties[1000:],\n",
    "                alpha=0.1, color='#e74c3c', label='Over-limit zone')\n",
    "ax.set_xlabel('Response Length (tokens)', fontsize=12)\n",
    "ax.set_ylabel('Length Penalty', fontsize=12)\n",
    "ax.set_title('Overlong Reward Shaping ‚Äî Smooth Penalty for Long Responses', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower left', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(-1.15, 0.15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Worked examples\n",
    "print(\"Overlong reward examples (L_max=1000, L_cache=200):\")\n",
    "for length in [500, 700, 800, 850, 900, 950, 1000, 1100]:\n",
    "    r = overlong_reward(length)\n",
    "    print(f\"  {length:4d} tokens ‚Üí penalty = {r:+.2f}\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_09_clip_higher",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Clip Higher\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_09_clip_higher.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 The Clip-Higher Mechanism\n",
    "\n",
    "Standard PPO uses symmetric clipping: $[1-\\epsilon, 1+\\epsilon]$. OpenClaw-RL uses **asymmetric clipping** where the upper bound is larger ‚Äî this allows the model to more aggressively increase the probability of good responses while being conservative about decreasing probabilities."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_higher(ratio: torch.Tensor, eps_low: float = 0.2, eps_high: float = 0.28) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Asymmetric clipping ‚Äî clip-higher technique.\n",
    "\n",
    "    Args:\n",
    "        ratio: Policy ratio tensor (pi_new / pi_old)\n",
    "        eps_low: Lower bound offset (conservative on negative updates)\n",
    "        eps_high: Upper bound offset (more exploratory on positive updates)\n",
    "\n",
    "    Returns:\n",
    "        Clipped ratio\n",
    "    \"\"\"\n",
    "    lower = 1.0 - eps_low\n",
    "    upper = 1.0 + eps_high\n",
    "    return torch.clamp(ratio, lower, upper)\n",
    "\n",
    "# Visualize symmetric vs asymmetric clipping\n",
    "ratios = torch.linspace(0.3, 2.0, 200)\n",
    "\n",
    "symmetric_clipped = torch.clamp(ratios, 0.8, 1.2)  # Standard PPO\n",
    "asymmetric_clipped = clip_higher(ratios)  # OpenClaw-RL\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(ratios.numpy(), ratios.numpy(), '--', color='gray', alpha=0.5, label='Unclipped')\n",
    "ax.plot(ratios.numpy(), symmetric_clipped.numpy(), linewidth=2.5,\n",
    "        color='#3498db', label='Symmetric clip [0.8, 1.2]')\n",
    "ax.plot(ratios.numpy(), asymmetric_clipped.numpy(), linewidth=2.5,\n",
    "        color='#e74c3c', label='Clip-higher [0.8, 1.28]')\n",
    "ax.axvline(x=1.0, color='gray', linestyle=':', alpha=0.3)\n",
    "ax.set_xlabel('Policy Ratio (œÅ)', fontsize=12)\n",
    "ax.set_ylabel('Clipped Ratio', fontsize=12)\n",
    "ax.set_title('Clip-Higher: Asymmetric Clipping for Exploration', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: clip-higher allows larger positive updates (up to 1.28)\")\n",
    "print(\"while keeping negative updates conservative (down to 0.8).\")\n",
    "print(\"This expands the exploration budget without risking catastrophic forgetting.\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_10_todo_clipped_loss",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Narration: Todo Clipped Loss\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_10_todo_clipped_loss.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîß Your Turn\n",
    "\n",
    "### TODO 1: Implement the GRPO Clipped Surrogate Loss\n",
    "\n",
    "This is the core loss function. Implement it step by step:"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_clipped_loss(\n",
    "    log_probs_new: torch.Tensor,    # Log probs under current policy\n",
    "    log_probs_ref: torch.Tensor,    # Log probs under reference policy\n",
    "    advantages: torch.Tensor,        # Group-relative advantages (per-token, broadcasted)\n",
    "    eps_low: float = 0.2,\n",
    "    eps_high: float = 0.28,\n",
    "    beta_kl: float = 0.01\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the GRPO clipped surrogate loss with clip-higher and KL penalty.\n",
    "\n",
    "    Args:\n",
    "        log_probs_new: shape (batch, seq_len) ‚Äî log œÄ_Œ∏(a_t | s_t)\n",
    "        log_probs_ref: shape (batch, seq_len) ‚Äî log œÄ_ref(a_t | s_t)\n",
    "        advantages: shape (batch,) ‚Äî group-relative advantages\n",
    "        eps_low: Lower clipping bound offset\n",
    "        eps_high: Upper clipping bound offset (higher = more exploration)\n",
    "        beta_kl: KL divergence penalty coefficient\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss value (to be MAXIMIZED, so negate for gradient descent)\n",
    "\n",
    "    Steps:\n",
    "        1. Compute the probability ratio: œÅ_t = exp(log_new - log_ref)\n",
    "        2. Compute unclipped objective: œÅ_t * A_hat\n",
    "        3. Compute clipped objective: clip(œÅ_t, 1-eps_low, 1+eps_high) * A_hat\n",
    "        4. Take the minimum of unclipped and clipped (pessimistic bound)\n",
    "        5. Compute KL penalty: beta * mean(log_new - log_ref)\n",
    "        6. Return mean(min_objective) - KL_penalty\n",
    "    \"\"\"\n",
    "    # Expand advantages to match token dimension\n",
    "    # advantages shape: (batch,) ‚Üí (batch, 1)\n",
    "    adv = advantages.unsqueeze(-1)\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Compute ratio œÅ = exp(log_probs_new - log_probs_ref)\n",
    "    # Step 2: Compute unclipped term = œÅ * advantages\n",
    "    # Step 3: Compute clipped_ratio using clip_higher\n",
    "    # Step 4: Compute clipped term = clipped_ratio * advantages\n",
    "    # Step 5: Take element-wise minimum\n",
    "    # Step 6: Compute KL penalty = beta_kl * mean(log_probs_new - log_probs_ref)\n",
    "    # Step 7: Return mean(min_term) - kl_penalty\n",
    "    # ==============================\n",
    "\n",
    "    loss = ???  # YOUR CODE HERE\n",
    "\n",
    "    return loss\n",
    "\n",
    "# ‚úÖ Verification\n",
    "batch, seq = 4, 10\n",
    "log_new = torch.randn(batch, seq) * 0.1 - 2.0  # Typical log-prob values\n",
    "log_ref = torch.randn(batch, seq) * 0.1 - 2.0\n",
    "advs = torch.tensor([0.9, -1.5, 0.9, -0.3])\n",
    "\n",
    "loss = grpo_clipped_loss(log_new, log_ref, advs)\n",
    "assert loss.dim() == 0, f\"‚ùå Loss should be scalar, got shape {loss.shape}\"\n",
    "assert not torch.isnan(loss), \"‚ùå Loss is NaN!\"\n",
    "print(f\"‚úÖ GRPO loss computed successfully: {loss.item():.4f}\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_11_todo_tcr_loss",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Narration: Todo Tcr Loss\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_11_todo_tcr_loss.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement the Full GRPO-TCR Loss\n",
    "\n",
    "Now combine everything ‚Äî PRM rewards, advantages, clipping, and reward shaping:"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_tcr_loss(\n",
    "    log_probs_new: torch.Tensor,   # (batch, seq_len)\n",
    "    log_probs_ref: torch.Tensor,   # (batch, seq_len)\n",
    "    response_lengths: torch.Tensor, # (batch,) ‚Äî length of each response in tokens\n",
    "    prm_rewards: torch.Tensor,      # (batch,) ‚Äî PRM majority vote rewards\n",
    "    eps_low: float = 0.2,\n",
    "    eps_high: float = 0.28,\n",
    "    beta_kl: float = 0.01,\n",
    "    L_max: int = 1000,\n",
    "    L_cache: int = 200,\n",
    ") -> Tuple[torch.Tensor, dict]:\n",
    "    \"\"\"\n",
    "    Full GRPO-TCR loss: Token-level + Clip-higher + Reward shaping.\n",
    "\n",
    "    Args:\n",
    "        log_probs_new, log_probs_ref: Policy log-probabilities\n",
    "        response_lengths: Token counts per response\n",
    "        prm_rewards: PRM majority vote rewards per response\n",
    "        eps_low, eps_high: Clipping bounds\n",
    "        beta_kl: KL penalty coefficient\n",
    "        L_max, L_cache: Overlong reward shaping parameters\n",
    "\n",
    "    Returns:\n",
    "        (loss, metrics_dict)\n",
    "\n",
    "    Steps:\n",
    "        1. Compute overlong penalties for each response\n",
    "        2. Combine PRM rewards with length penalties\n",
    "        3. Compute GRPO advantages from combined rewards\n",
    "        4. Compute clipped surrogate loss with KL penalty\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Compute overlong penalties for each response\n",
    "    # Step 2: combined_reward = prm_reward + length_penalty\n",
    "    # Step 3: advantages = compute_grpo_advantages(combined_rewards)\n",
    "    # Step 4: loss = grpo_clipped_loss(log_new, log_ref, advantages, ...)\n",
    "    # ==============================\n",
    "\n",
    "    loss = ???  # YOUR CODE HERE\n",
    "    metrics = ???  # YOUR CODE HERE\n",
    "\n",
    "    return loss, metrics\n",
    "\n",
    "# ‚úÖ Verification\n",
    "batch, seq = 8, 20\n",
    "log_new = torch.randn(batch, seq) * 0.1 - 2.0\n",
    "log_ref = torch.randn(batch, seq) * 0.1 - 2.0\n",
    "lengths = torch.tensor([500, 800, 900, 1100, 600, 750, 950, 400]).float()\n",
    "prm_rewards = torch.tensor([1.0, -1.0, 1.0, 0.0, 1.0, -1.0, 0.0, 1.0])\n",
    "\n",
    "loss, metrics = grpo_tcr_loss(log_new, log_ref, lengths, prm_rewards)\n",
    "assert loss.dim() == 0, f\"‚ùå Loss should be scalar\"\n",
    "assert \"mean_advantage\" in metrics, \"‚ùå Metrics should include mean_advantage\"\n",
    "print(f\"‚úÖ GRPO-TCR loss: {loss.item():.4f}\")\n",
    "print(f\"   Mean advantage: {metrics['mean_advantage']:.4f}\")\n",
    "print(f\"   Mean length penalty: {metrics['mean_length_penalty']:.4f}\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_12_training_loop",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Training Loop\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_12_training_loop.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together ‚Äî A Complete Training Loop\n",
    "\n",
    "Let us build a small-scale training loop that demonstrates GRPO-TCR in action. We will use a simple model that learns to prefer certain token patterns over others."
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePolicyModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A tiny policy model for demonstration.\n",
    "    Maps a prompt embedding to a distribution over responses.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=100, hidden_size=64, max_seq_len=20):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.transformer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size, nhead=4, dim_feedforward=128, batch_first=True\n",
    "        )\n",
    "        self.head = nn.Linear(hidden_size, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"Compute log-probabilities for each position.\"\"\"\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.transformer(x)\n",
    "        logits = self.head(x)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        return log_probs\n",
    "\n",
    "    def get_response_log_probs(self, input_ids, response_ids):\n",
    "        \"\"\"\n",
    "        Get the log-probability of generating specific response tokens.\n",
    "\n",
    "        Args:\n",
    "            input_ids: (batch, prompt_len)\n",
    "            response_ids: (batch, response_len)\n",
    "\n",
    "        Returns:\n",
    "            log_probs: (batch, response_len) ‚Äî log p(response_t | context)\n",
    "        \"\"\"\n",
    "        full_ids = torch.cat([input_ids, response_ids], dim=1)\n",
    "        all_log_probs = self.forward(full_ids)\n",
    "        # Get log-probs for the response portion only\n",
    "        prompt_len = input_ids.shape[1]\n",
    "        response_log_probs = all_log_probs[:, prompt_len-1:-1, :]  # Shifted by 1\n",
    "        # Gather the log-probs for the actual response tokens\n",
    "        selected = response_log_probs.gather(2, response_ids.unsqueeze(-1)).squeeze(-1)\n",
    "        return selected\n",
    "\n",
    "# Create model\n",
    "vocab_size = 100\n",
    "model = SimplePolicyModel(vocab_size=vocab_size).to(device)\n",
    "ref_model = SimplePolicyModel(vocab_size=vocab_size).to(device)\n",
    "ref_model.load_state_dict(model.state_dict())  # Same initial weights\n",
    "\n",
    "print(f\"‚úÖ Policy model: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"‚úÖ Reference model frozen (same architecture)\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loop"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_batch(batch_size=8, prompt_len=5, response_len=15, vocab_size=100):\n",
    "    \"\"\"Generate a synthetic batch of (prompt, response, reward) triples.\"\"\"\n",
    "    prompts = torch.randint(0, vocab_size, (batch_size, prompt_len))\n",
    "    responses = torch.randint(0, vocab_size, (batch_size, response_len))\n",
    "\n",
    "    # Simulate PRM rewards: responses with more low-value tokens are \"bad\"\n",
    "    # (This is artificial ‚Äî in reality, the PRM evaluates actual response quality)\n",
    "    rewards = []\n",
    "    for resp in responses:\n",
    "        low_token_ratio = (resp < vocab_size // 3).float().mean().item()\n",
    "        if low_token_ratio > 0.5:\n",
    "            rewards.append(-1.0)\n",
    "        elif low_token_ratio < 0.3:\n",
    "            rewards.append(1.0)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "    rewards = torch.tensor(rewards)\n",
    "\n",
    "    lengths = torch.full((batch_size,), response_len, dtype=torch.float)\n",
    "    return prompts.to(device), responses.to(device), rewards.to(device), lengths.to(device)\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "num_steps = 200\n",
    "losses = []\n",
    "mean_advantages = []\n",
    "\n",
    "print(\"Training with GRPO-TCR...\")\n",
    "for step in range(num_steps):\n",
    "    prompts, responses, prm_rewards, lengths = generate_synthetic_batch()\n",
    "\n",
    "    # Get log-probs from current and reference policy\n",
    "    log_probs_new = model.get_response_log_probs(prompts, responses)\n",
    "    with torch.no_grad():\n",
    "        log_probs_ref = ref_model.get_response_log_probs(prompts, responses)\n",
    "\n",
    "    # Compute GRPO-TCR loss\n",
    "    combined = prm_rewards + torch.tensor(\n",
    "        [overlong_reward(int(l.item())) for l in lengths], device=device\n",
    "    )\n",
    "    advantages = compute_grpo_advantages(combined)\n",
    "\n",
    "    loss = grpo_clipped_loss(log_probs_new, log_probs_ref, advantages)\n",
    "    neg_loss = -loss  # We maximize the objective, so minimize the negative\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    neg_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    mean_advantages.append(advantages.mean().item())\n",
    "\n",
    "    if (step + 1) % 50 == 0:\n",
    "        print(f\"  Step {step+1}/{num_steps}: loss={loss.item():.4f}, \"\n",
    "              f\"mean_adv={advantages.mean().item():.4f}\")\n",
    "\n",
    "print(\"‚úÖ Training complete!\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_13_training_results",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Training Results\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_13_training_results.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üìä Training Results"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training loss curve\n",
    "window = 10\n",
    "smoothed_losses = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "axes[0].plot(smoothed_losses, linewidth=2, color='#3498db')\n",
    "axes[0].set_xlabel('Training Step')\n",
    "axes[0].set_ylabel('GRPO Objective')\n",
    "axes[0].set_title('GRPO-TCR Training Curve', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Policy divergence from reference\n",
    "with torch.no_grad():\n",
    "    test_prompts, test_responses, _, _ = generate_synthetic_batch(batch_size=32)\n",
    "    new_lp = model.get_response_log_probs(test_prompts, test_responses)\n",
    "    ref_lp = ref_model.get_response_log_probs(test_prompts, test_responses)\n",
    "    kl_div = (new_lp - ref_lp).mean(dim=1)\n",
    "\n",
    "axes[1].hist(kl_div.cpu().numpy(), bins=30, color='#e74c3c', alpha=0.7, edgecolor='white')\n",
    "axes[1].axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "axes[1].set_xlabel('KL Divergence from Reference')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Policy Divergence Distribution', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean KL divergence from reference: {kl_div.mean().item():.4f}\")\n",
    "print(f\"Max KL divergence: {kl_div.max().item():.4f}\")\n",
    "print(\"(Small KL = policy hasn't drifted too far = good stability)\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üéØ Final Output: GRPO-TCR Component Summary"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the full GRPO-TCR pipeline visually\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. PRM majority voting\n",
    "prm = ProcessRewardModel(accuracy=0.7, num_votes=7)\n",
    "good_results = [prm.evaluate_with_majority_voting(+1)[0] for _ in range(100)]\n",
    "bad_results = [prm.evaluate_with_majority_voting(-1)[0] for _ in range(100)]\n",
    "\n",
    "axes[0, 0].bar(['Good ‚Üí +1', 'Good ‚Üí 0', 'Good ‚Üí -1'],\n",
    "               [good_results.count(1), good_results.count(0), good_results.count(-1)],\n",
    "               color=['#2ecc71', '#f39c12', '#e74c3c'], alpha=0.8)\n",
    "axes[0, 0].set_title('PRM Majority Voting (true: GOOD)', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Count (out of 100)')\n",
    "\n",
    "# 2. Advantage distribution\n",
    "sample_rewards = torch.tensor([1.0, -1.0, 1.0, 0.0, 1.0, -1.0, 0.0, 1.0])\n",
    "sample_advs = compute_grpo_advantages(sample_rewards)\n",
    "colors = ['#2ecc71' if a > 0 else '#e74c3c' for a in sample_advs]\n",
    "axes[0, 1].bar(range(len(sample_advs)), sample_advs.numpy(), color=colors, alpha=0.8)\n",
    "axes[0, 1].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[0, 1].set_title('Group-Relative Advantages', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Response Index')\n",
    "\n",
    "# 3. Clip-higher comparison\n",
    "ratios = torch.linspace(0.3, 2.0, 100)\n",
    "sym = torch.clamp(ratios, 0.8, 1.2)\n",
    "asym = clip_higher(ratios)\n",
    "axes[1, 0].plot(ratios.numpy(), ratios.numpy(), '--', color='gray', alpha=0.5)\n",
    "axes[1, 0].plot(ratios.numpy(), sym.numpy(), linewidth=2, label='Symmetric')\n",
    "axes[1, 0].plot(ratios.numpy(), asym.numpy(), linewidth=2, label='Clip-higher')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].set_title('Clip-Higher vs Symmetric', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 4. Training curve\n",
    "axes[1, 1].plot(smoothed_losses, linewidth=2, color='#9b59b6')\n",
    "axes[1, 1].set_title('GRPO-TCR Training Convergence', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Step')\n",
    "axes[1, 1].set_ylabel('Objective')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('GRPO-TCR: Complete Binary RL Pipeline', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéâ Congratulations! You've built the Binary RL pipeline from scratch!\")\n",
    "print(\"   ‚úÖ Process Reward Model with majority voting\")\n",
    "print(\"   ‚úÖ Group-relative advantage computation\")\n",
    "print(\"   ‚úÖ Clip-higher asymmetric clipping\")\n",
    "print(\"   ‚úÖ Overlong reward shaping\")\n",
    "print(\"   ‚úÖ Complete GRPO-TCR training loop\")"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_14_reflection",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Narration: Reflection\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_14_reflection.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "1. Why does asymmetric clipping (clip-higher) help with exploration? What would happen if $\\epsilon_{\\text{high}}$ was very large (say, 10)?\n",
    "2. The overlong reward shaping uses a linear penalty in the transition zone. What would happen with a quadratic penalty instead? Would it be better or worse?\n",
    "3. In our demo, we used synthetic rewards. In the real system, PRM rewards come from a language model. What happens if the PRM is biased (e.g., it always rates longer responses higher)?\n",
    "\n",
    "### üèÜ Optional Challenges\n",
    "1. **Dynamic clip bounds**: Implement a version where $\\epsilon_{\\text{high}}$ decreases over training (start exploratory, become conservative).\n",
    "2. **Reward aggregation**: Instead of simple majority voting, implement weighted voting where more confident PRM evaluations count more.\n",
    "3. **Group size ablation**: Run the training loop with different group sizes (G=2, 4, 8, 16) and compare convergence speed."
   ],
   "id": "cell_32"
  }
 ]
}