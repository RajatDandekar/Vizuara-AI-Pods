{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "The RLAnything Closed Loop â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"17rFuCNZUUY1xHrMq1WTamV-JWh_IDZe8\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/04_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ The RLAnything Closed Loop: Co-Optimizing Policy, Reward Model, and Environment\n",
    "\n",
    "*Part 4 of the Vizuara series on OpenClaw-RL*\n",
    "*Estimated time: 60 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://pods.vizuara.ai/courses/openclaw-rl/practice/4/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_01_why_matters",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_01_why_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebooks, we built two training paradigms: Binary RL (scalar feedback) and OPD (token-level feedback). Both treat the **Process Reward Model (PRM)** as a fixed judge â€” the policy improves, but the judge stays the same.\n",
    "\n",
    "But what if the judge itself could get better over time?\n",
    "\n",
    "Think about it: as the policy improves, it generates higher-quality responses. These higher-quality responses contain more nuanced information for the reward model to learn from. And a better reward model provides more precise feedback for the policy.\n",
    "\n",
    "This is the **RLAnything** framework â€” a closed loop where **policy**, **reward model**, and **environment** all co-optimize simultaneously.\n",
    "\n",
    "By the end of this notebook, you will have implemented:\n",
    "- **Integrated rewards** combining outcome signals with step-wise PRM scores\n",
    "- **Consistency feedback** that trains the reward model alongside the policy\n",
    "- **Automatic difficulty adaptation** that keeps the environment challenging\n",
    "- A complete **closed-loop training pipeline** where all three components improve together\n",
    "- Visualization of the co-evolution dynamics"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ Teaser: Watch all three components improve together\n",
    "# Step 0:   Policy: 45% success | PRM accuracy: 65% | Difficulty: 0.3\n",
    "# Step 50:  Policy: 62% success | PRM accuracy: 73% | Difficulty: 0.5\n",
    "# Step 100: Policy: 78% success | PRM accuracy: 82% | Difficulty: 0.7\n",
    "# Step 150: Policy: 85% success | PRM accuracy: 88% | Difficulty: 0.8"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_02_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_02_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Let us use an analogy.\n",
    "\n",
    "Imagine a **student**, a **teacher**, and a **textbook author** working together:\n",
    "\n",
    "- **The student** (policy) solves problems from the textbook\n",
    "- **The teacher** (reward model) grades the student's work\n",
    "- **The textbook author** (environment) creates new problems\n",
    "\n",
    "At first:\n",
    "- The student is mediocre, solving only easy problems\n",
    "- The teacher's grading is somewhat inaccurate\n",
    "- The textbook has a mix of easy and hard problems\n",
    "\n",
    "Over time, something beautiful happens:\n",
    "- The student improves â†’ now the easy problems are too easy\n",
    "- The teacher sees better work â†’ learns to distinguish good from great (not just good from bad)\n",
    "- The textbook author notices the student acing easy problems â†’ generates harder ones\n",
    "- The harder problems force the student to improve further\n",
    "- And the cycle continues...\n",
    "\n",
    "This is **co-optimization**. No single component drives improvement alone â€” they lift each other up.\n",
    "\n",
    "### ðŸ¤” Think About This\n",
    "\n",
    "What would happen if we only improved the policy but kept the reward model fixed? At some point, the policy would surpass the reward model's ability to evaluate it. The reward model would assign random scores to high-quality responses, and training would plateau or even degrade. This is why co-optimization matters."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_03_mathematics",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Mathematics\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_03_mathematics.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### 3.1 Integrated Reward\n",
    "\n",
    "Instead of relying solely on the PRM, we combine **outcome rewards** (task-level success/failure) with **step-wise PRM scores**:\n",
    "\n",
    "$$R_{\\tau_i} = O_\\tau + \\frac{\\lambda}{m}\\sum_{j=1}^{m} S_{\\tau_i, j}$$\n",
    "\n",
    "where:\n",
    "- $O_\\tau \\in \\{-1, +1\\}$ is the outcome reward (did the task succeed overall?)\n",
    "- $S_{\\tau_i, j} \\in \\{-1, +1\\}$ is the $j$-th PRM evaluation for step $i$\n",
    "- $m$ is the number of PRM evaluations\n",
    "- $\\lambda$ (default 1) balances outcome vs step-wise signals\n",
    "\n",
    "Computationally: the integrated reward augments the binary outcome with fine-grained step scores. A step that was individually good ($S > 0$) in a failed task ($O = -1$) gets a less harsh penalty than a step that was individually bad.\n",
    "\n",
    "Example: Task succeeded ($O = +1$), 3 PRM evaluations for this step: $[+1, +1, -1]$.\n",
    "\n",
    "$R_{\\tau_i} = 1 + \\frac{1}{3}(1 + 1 + (-1)) = 1 + 0.33 = 1.33$\n",
    "\n",
    "### 3.2 Consistency Feedback for the Reward Model\n",
    "\n",
    "How does the PRM improve? Through a **self-consistency signal**:\n",
    "\n",
    "$$R_{S_{\\tau_i, j}} = R_{\\tau_i} \\cdot S_{\\tau_i, j}$$\n",
    "\n",
    "Computationally: if the PRM agrees with the integrated reward (both positive or both negative), it gets a positive consistency reward. If it disagrees, it gets penalized.\n",
    "\n",
    "Example: If $R_{\\tau_i} = 1.33$ and $S_{\\tau_i, j} = +1$: consistency reward = $1.33 \\times 1 = 1.33$ (correct, rewarded).\n",
    "If $S_{\\tau_i, j} = -1$: consistency reward = $1.33 \\times (-1) = -1.33$ (incorrect, penalized).\n",
    "\n",
    "### 3.3 Reward Precision Bound\n",
    "\n",
    "As the number of PRM evaluations $m$ increases, the precision approaches 1:\n",
    "\n",
    "$$A \\geq 1 - \\exp\\left(-\\frac{m(\\mu - 1)^2}{4}\\right)$$\n",
    "\n",
    "where $\\mu = p_+ + p_-$ is the sum of true positive and true negative rates.\n",
    "\n",
    "Computationally: even a mediocre PRM (70% accuracy) becomes highly reliable with enough evaluations. With $m = 20$: precision > 95%."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_04_setup_build",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Setup Build\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_04_setup_build.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It â€” Component by Component\n",
    "\n",
    "### 4.1 Setup and Imports"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ… Using device: {device}\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_05_integrated_reward",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Integrated Reward\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_05_integrated_reward.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Integrated Reward Computer"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegratedRewardComputer:\n",
    "    \"\"\"\n",
    "    Computes integrated rewards combining outcome and step-wise PRM signals.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lambda_weight: float = 1.0, num_prm_votes: int = 3):\n",
    "        self.lambda_weight = lambda_weight\n",
    "        self.num_prm_votes = num_prm_votes\n",
    "\n",
    "    def compute_integrated_reward(\n",
    "        self,\n",
    "        outcome: float,\n",
    "        prm_scores: List[float]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Compute integrated reward for a single step.\n",
    "\n",
    "        R_tau_i = O_tau + (lambda / m) * sum(S_tau_i_j)\n",
    "\n",
    "        Args:\n",
    "            outcome: Overall task outcome (-1 or +1)\n",
    "            prm_scores: List of PRM evaluations for this step\n",
    "\n",
    "        Returns:\n",
    "            Integrated reward value\n",
    "        \"\"\"\n",
    "        m = len(prm_scores)\n",
    "        step_signal = self.lambda_weight / m * sum(prm_scores)\n",
    "        return outcome + step_signal\n",
    "\n",
    "    def compute_consistency_reward(\n",
    "        self,\n",
    "        integrated_reward: float,\n",
    "        prm_score: float\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Compute consistency reward for the PRM.\n",
    "\n",
    "        R_S = R_tau_i * S_tau_i_j\n",
    "\n",
    "        If PRM agrees with integrated reward â†’ positive (PRM was right)\n",
    "        If PRM disagrees â†’ negative (PRM was wrong)\n",
    "\n",
    "        Args:\n",
    "            integrated_reward: The integrated reward for this step\n",
    "            prm_score: This particular PRM evaluation\n",
    "\n",
    "        Returns:\n",
    "            Consistency reward for the PRM\n",
    "        \"\"\"\n",
    "        return integrated_reward * prm_score\n",
    "\n",
    "# Demonstrate\n",
    "reward_computer = IntegratedRewardComputer(lambda_weight=1.0, num_prm_votes=3)\n",
    "\n",
    "# Example 1: Successful task, PRM mostly agrees\n",
    "outcome = +1.0\n",
    "prm_scores = [+1, +1, -1]\n",
    "R = reward_computer.compute_integrated_reward(outcome, prm_scores)\n",
    "print(f\"Example 1: Successful task, PRM scores = {prm_scores}\")\n",
    "print(f\"  Integrated reward: R = {outcome} + (1/3)({sum(prm_scores)}) = {R:.2f}\")\n",
    "for j, s in enumerate(prm_scores):\n",
    "    c = reward_computer.compute_consistency_reward(R, s)\n",
    "    status = \"âœ… correct\" if c > 0 else \"âŒ wrong\"\n",
    "    print(f\"  PRM vote {j+1} (S={s:+d}): consistency = {R:.2f} Ã— {s} = {c:.2f} ({status})\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Example 2: Failed task, PRM says step was good\n",
    "outcome = -1.0\n",
    "prm_scores = [+1, +1, +1]\n",
    "R = reward_computer.compute_integrated_reward(outcome, prm_scores)\n",
    "print(f\"Example 2: Failed task, PRM scores = {prm_scores}\")\n",
    "print(f\"  Integrated reward: R = {outcome} + (1/3)({sum(prm_scores)}) = {R:.2f}\")\n",
    "for j, s in enumerate(prm_scores):\n",
    "    c = reward_computer.compute_consistency_reward(R, s)\n",
    "    print(f\"  PRM vote {j+1} (S={s:+d}): consistency = {R:.2f} Ã— {s} = {c:.2f}\")\n",
    "print(f\"  Note: R = {R:.2f}, PRM said +1. Product is {R:.2f} Ã— 1 = {R:.2f}\")\n",
    "print(f\"  This is slightly positive because the step might have been good\")\n",
    "print(f\"  even though the overall task failed.\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_06_reward_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Reward Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_06_reward_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Visualization: Integrated Rewards Across Different Scenarios"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\n",
    "    {\"outcome\": +1, \"prm_scores\": [+1, +1, +1], \"label\": \"Success + PRM agrees\"},\n",
    "    {\"outcome\": +1, \"prm_scores\": [+1, +1, -1], \"label\": \"Success + PRM mostly agrees\"},\n",
    "    {\"outcome\": +1, \"prm_scores\": [-1, -1, +1], \"label\": \"Success + PRM mostly disagrees\"},\n",
    "    {\"outcome\": -1, \"prm_scores\": [-1, -1, -1], \"label\": \"Failure + PRM agrees\"},\n",
    "    {\"outcome\": -1, \"prm_scores\": [-1, +1, -1], \"label\": \"Failure + PRM mostly agrees\"},\n",
    "    {\"outcome\": -1, \"prm_scores\": [+1, +1, +1], \"label\": \"Failure + PRM disagrees\"},\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Integrated rewards\n",
    "labels = [s[\"label\"] for s in scenarios]\n",
    "integrated = [reward_computer.compute_integrated_reward(s[\"outcome\"], s[\"prm_scores\"])\n",
    "              for s in scenarios]\n",
    "colors = ['#2ecc71' if r > 0 else '#e74c3c' for r in integrated]\n",
    "\n",
    "y_pos = range(len(scenarios))\n",
    "axes[0].barh(y_pos, integrated, color=colors, edgecolor='white', linewidth=2, alpha=0.8)\n",
    "axes[0].set_yticks(y_pos)\n",
    "axes[0].set_yticklabels(labels, fontsize=9)\n",
    "axes[0].axvline(x=0, color='black', linewidth=0.5)\n",
    "axes[0].set_xlabel('Integrated Reward')\n",
    "axes[0].set_title('Integrated Rewards: Outcome + PRM', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Right: Consistency rewards for different PRM scores\n",
    "consistency_data = {}\n",
    "for r_val in np.linspace(-2, 2, 20):\n",
    "    for s_val in [-1, +1]:\n",
    "        key = f\"S={s_val:+d}\"\n",
    "        if key not in consistency_data:\n",
    "            consistency_data[key] = {\"R\": [], \"C\": []}\n",
    "        consistency_data[key][\"R\"].append(r_val)\n",
    "        consistency_data[key][\"C\"].append(r_val * s_val)\n",
    "\n",
    "for key, data in consistency_data.items():\n",
    "    color = '#2ecc71' if '+1' in key else '#e74c3c'\n",
    "    axes[1].plot(data[\"R\"], data[\"C\"], linewidth=2.5, label=key, color=color)\n",
    "\n",
    "axes[1].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[1].axvline(x=0, color='black', linewidth=0.5)\n",
    "axes[1].set_xlabel('Integrated Reward (R)', fontsize=12)\n",
    "axes[1].set_ylabel('Consistency Reward', fontsize=12)\n",
    "axes[1].set_title('PRM Consistency Rewards', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_07_environment",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Environment\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_07_environment.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The Simulated Environment with Difficulty Adaptation"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveEnvironment:\n",
    "    \"\"\"\n",
    "    An environment that generates tasks and adapts difficulty\n",
    "    based on the policy's success rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size=50, seq_len=10,\n",
    "                 difficulty: float = 0.3,\n",
    "                 easy_threshold: float = 0.8,\n",
    "                 hard_threshold: float = 0.2,\n",
    "                 adapt_rate: float = 0.05):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            difficulty: Current difficulty level (0 = easiest, 1 = hardest)\n",
    "            easy_threshold: If success > this, increase difficulty\n",
    "            hard_threshold: If success < this, decrease difficulty\n",
    "            adapt_rate: How much to change difficulty each adaptation step\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.difficulty = difficulty\n",
    "        self.easy_threshold = easy_threshold\n",
    "        self.hard_threshold = hard_threshold\n",
    "        self.adapt_rate = adapt_rate\n",
    "        self.success_history = deque(maxlen=50)\n",
    "        self.difficulty_history = [difficulty]\n",
    "\n",
    "    def generate_task(self, batch_size: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Generate a batch of tasks with current difficulty.\n",
    "\n",
    "        Higher difficulty = more complex target patterns.\n",
    "\n",
    "        Returns:\n",
    "            (inputs, targets) tensors\n",
    "        \"\"\"\n",
    "        inputs = torch.randint(0, self.vocab_size, (batch_size, self.seq_len)).to(device)\n",
    "\n",
    "        # Difficulty controls target pattern complexity\n",
    "        if self.difficulty < 0.3:\n",
    "            # Easy: target is a simple repeating pattern\n",
    "            base_pattern = torch.arange(self.seq_len) % 5\n",
    "            targets = base_pattern.unsqueeze(0).expand(batch_size, -1).to(device)\n",
    "        elif self.difficulty < 0.6:\n",
    "            # Medium: target depends on input (first token modulates)\n",
    "            modulator = inputs[:, 0:1] % 5\n",
    "            base = torch.arange(self.seq_len).unsqueeze(0).to(device)\n",
    "            targets = (base + modulator) % self.vocab_size\n",
    "        else:\n",
    "            # Hard: target is a complex function of multiple input tokens\n",
    "            targets = (inputs * 3 + torch.roll(inputs, 1, dims=1) * 2) % self.vocab_size\n",
    "\n",
    "        return inputs, targets\n",
    "\n",
    "    def evaluate_outcome(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Evaluate task outcomes.\n",
    "\n",
    "        Returns:\n",
    "            outcomes: (batch,) tensor of +1 (success) or -1 (failure)\n",
    "        \"\"\"\n",
    "        match_rate = (predictions == targets).float().mean(dim=1)\n",
    "        # Success threshold increases with difficulty\n",
    "        threshold = 0.3 + 0.3 * self.difficulty\n",
    "        outcomes = torch.where(match_rate > threshold,\n",
    "                              torch.ones_like(match_rate),\n",
    "                              -torch.ones_like(match_rate))\n",
    "        return outcomes\n",
    "\n",
    "    def adapt_difficulty(self, outcomes: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Adapt difficulty based on recent success rate.\n",
    "\n",
    "        If success > 80%: tasks are too easy â†’ increase difficulty\n",
    "        If success < 20%: tasks are too hard â†’ decrease difficulty\n",
    "        \"\"\"\n",
    "        success_rate = (outcomes > 0).float().mean().item()\n",
    "        self.success_history.append(success_rate)\n",
    "\n",
    "        if len(self.success_history) >= 10:\n",
    "            recent_success = np.mean(list(self.success_history)[-10:])\n",
    "\n",
    "            if recent_success > self.easy_threshold:\n",
    "                self.difficulty = min(1.0, self.difficulty + self.adapt_rate)\n",
    "            elif recent_success < self.hard_threshold:\n",
    "                self.difficulty = max(0.0, self.difficulty - self.adapt_rate)\n",
    "\n",
    "        self.difficulty_history.append(self.difficulty)\n",
    "\n",
    "    def get_stats(self):\n",
    "        if len(self.success_history) > 0:\n",
    "            return {\n",
    "                \"difficulty\": self.difficulty,\n",
    "                \"recent_success\": np.mean(list(self.success_history)[-10:]),\n",
    "                \"total_tasks\": len(self.success_history),\n",
    "            }\n",
    "        return {\"difficulty\": self.difficulty, \"recent_success\": 0, \"total_tasks\": 0}\n",
    "\n",
    "# Test the environment\n",
    "env = AdaptiveEnvironment(difficulty=0.3)\n",
    "inputs, targets = env.generate_task(batch_size=4)\n",
    "print(f\"Environment difficulty: {env.difficulty}\")\n",
    "print(f\"Generated task shapes: inputs={inputs.shape}, targets={targets.shape}\")\n",
    "print(f\"Sample input:  {inputs[0].tolist()}\")\n",
    "print(f\"Sample target: {targets[0].tolist()}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_08_prm_policy",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Prm Policy\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_08_prm_policy.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The Simulated PRM (Trainable)"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainablePRM(nn.Module):\n",
    "    \"\"\"\n",
    "    A trainable Process Reward Model.\n",
    "    In production, this is a language model. Here we use a simple classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size=50, hidden_size=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 3),  # Output: [-1, 0, +1] logits\n",
    "        )\n",
    "        self.label_map = {0: -1, 1: 0, 2: +1}\n",
    "\n",
    "    def forward(self, step_features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict step quality.\n",
    "\n",
    "        Args:\n",
    "            step_features: (batch, input_size) â€” features describing a step\n",
    "\n",
    "        Returns:\n",
    "            logits: (batch, 3) â€” logits for [-1, 0, +1]\n",
    "        \"\"\"\n",
    "        return self.net(step_features)\n",
    "\n",
    "    def predict_score(self, step_features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get the predicted score (-1, 0, or +1).\"\"\"\n",
    "        logits = self.forward(step_features)\n",
    "        indices = logits.argmax(dim=-1)\n",
    "        # Map 0â†’-1, 1â†’0, 2â†’+1\n",
    "        scores = indices.float() - 1.0\n",
    "        return scores\n",
    "\n",
    "    def evaluate_with_voting(self, step_features: torch.Tensor, num_votes: int = 3):\n",
    "        \"\"\"\n",
    "        Majority voting with dropout-based diversity.\n",
    "        In production, this uses sampling. Here we use dropout.\n",
    "        \"\"\"\n",
    "        self.train()  # Enable dropout for diversity\n",
    "        votes = []\n",
    "        for _ in range(num_votes):\n",
    "            with torch.no_grad():\n",
    "                score = self.predict_score(step_features)\n",
    "                votes.append(score)\n",
    "\n",
    "        self.eval()\n",
    "        votes_stack = torch.stack(votes, dim=0)  # (num_votes, batch)\n",
    "\n",
    "        # Majority vote per sample\n",
    "        majority = []\n",
    "        for b in range(step_features.shape[0]):\n",
    "            sample_votes = votes_stack[:, b].tolist()\n",
    "            from collections import Counter\n",
    "            most_common = Counter(sample_votes).most_common(1)[0][0]\n",
    "            majority.append(most_common)\n",
    "\n",
    "        return torch.tensor(majority, device=step_features.device), votes_stack\n",
    "\n",
    "# Test PRM\n",
    "prm_model = TrainablePRM(input_size=50).to(device)\n",
    "test_features = torch.randn(4, 50).to(device)\n",
    "scores, votes = prm_model.evaluate_with_voting(test_features)\n",
    "print(f\"PRM scores: {scores.tolist()}\")\n",
    "print(f\"PRM votes shape: {votes.shape}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 The Policy Model"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyModel(nn.Module):\n",
    "    \"\"\"Simple policy model for the closed-loop demo.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size=50, hidden_size=64, seq_len=10):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.head = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.layers(self.embed(x))\n",
    "        return self.head(h)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.forward(x).argmax(dim=-1)\n",
    "\n",
    "    def get_log_probs(self, x, targets):\n",
    "        logits = self.forward(x)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        selected = log_probs.gather(2, targets.unsqueeze(-1)).squeeze(-1)\n",
    "        return selected\n",
    "\n",
    "policy = PolicyModel().to(device)\n",
    "ref_policy = PolicyModel().to(device)\n",
    "ref_policy.load_state_dict(policy.state_dict())\n",
    "\n",
    "print(f\"âœ… Policy model: {sum(p.numel() for p in policy.parameters()):,} params\")\n",
    "print(f\"âœ… PRM model:    {sum(p.numel() for p in prm_model.parameters()):,} params\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_09_todo_section",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo Section\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_09_todo_section.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ðŸ”§ Your Turn\n",
    "\n",
    "### TODO 1: Implement the Consistency-Based PRM Update\n",
    "\n",
    "The PRM improves by receiving consistency rewards. Implement the PRM training step:"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_prm_step(\n",
    "    prm: TrainablePRM,\n",
    "    prm_optimizer: torch.optim.Optimizer,\n",
    "    step_features: torch.Tensor,    # (batch, feature_size)\n",
    "    integrated_rewards: torch.Tensor, # (batch,) â€” integrated rewards for each step\n",
    "    prm_scores: torch.Tensor,         # (batch,) â€” PRM's own predictions\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Train the PRM using consistency feedback.\n",
    "\n",
    "    The PRM should learn to agree with the integrated reward signal.\n",
    "    When R * S > 0, the PRM was correct â†’ reinforce this behavior.\n",
    "    When R * S < 0, the PRM was wrong â†’ correct this behavior.\n",
    "\n",
    "    Args:\n",
    "        prm: The trainable PRM model\n",
    "        prm_optimizer: PRM optimizer\n",
    "        step_features: Features describing each step\n",
    "        integrated_rewards: Ground truth integrated rewards\n",
    "        prm_scores: PRM's predictions for these steps\n",
    "\n",
    "    Returns:\n",
    "        Loss value\n",
    "\n",
    "    Steps:\n",
    "        1. Compute consistency rewards: R * S for each sample\n",
    "        2. Convert integrated_rewards to classification targets:\n",
    "           R > 0 â†’ class 2 (+1), R == 0 â†’ class 1 (0), R < 0 â†’ class 0 (-1)\n",
    "        3. Get PRM logits from forward pass\n",
    "        4. Compute weighted cross-entropy loss (weight by |consistency_reward|)\n",
    "        5. Backprop and step\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Create classification targets from integrated_rewards\n",
    "    #   R > 0.5 â†’ class 2, -0.5 < R < 0.5 â†’ class 1, R < -0.5 â†’ class 0\n",
    "    # Step 2: Get PRM logits\n",
    "    # Step 3: Compute cross-entropy loss\n",
    "    # Step 4: Weight by |consistency_reward| for emphasis\n",
    "    # Step 5: Backprop\n",
    "    # ==============================\n",
    "\n",
    "    loss = ???  # YOUR CODE HERE\n",
    "\n",
    "    return loss\n",
    "\n",
    "# âœ… Verification (run after implementing)\n",
    "# prm_opt = torch.optim.Adam(prm_model.parameters(), lr=1e-3)\n",
    "# test_features = torch.randn(8, 50).to(device)\n",
    "# test_rewards = torch.tensor([1.33, -0.67, 1.0, -1.33, 0.33, 1.0, -1.0, 0.67]).to(device)\n",
    "# test_scores = torch.tensor([1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0]).to(device)\n",
    "# loss = train_prm_step(prm_model, prm_opt, test_features, test_rewards, test_scores)\n",
    "# assert isinstance(loss, float), \"âŒ Should return float loss\"\n",
    "# print(f\"âœ… PRM training step works! Loss: {loss:.4f}\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement the Difficulty Adaptation Logic"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_difficulty_step(\n",
    "    env: AdaptiveEnvironment,\n",
    "    recent_outcomes: torch.Tensor,\n",
    ") -> Tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Decide whether to adapt environment difficulty.\n",
    "\n",
    "    Args:\n",
    "        env: The adaptive environment\n",
    "        recent_outcomes: Recent task outcomes (+1/-1)\n",
    "\n",
    "    Returns:\n",
    "        (new_difficulty, action_taken)\n",
    "        action_taken is one of: \"increased\", \"decreased\", \"unchanged\"\n",
    "\n",
    "    Rules:\n",
    "        - If recent success rate > env.easy_threshold â†’ increase difficulty\n",
    "        - If recent success rate < env.hard_threshold â†’ decrease difficulty\n",
    "        - Otherwise â†’ no change\n",
    "        - Difficulty must stay in [0, 1]\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Compute success rate from recent_outcomes\n",
    "    # Step 2: Compare to thresholds\n",
    "    # Step 3: Adjust env.difficulty accordingly\n",
    "    # Step 4: Return new difficulty and action string\n",
    "    # ==============================\n",
    "\n",
    "    new_diff = ???  # YOUR CODE HERE\n",
    "    action = ???    # YOUR CODE HERE\n",
    "\n",
    "    return new_diff, action\n",
    "\n",
    "# âœ… Verification\n",
    "# test_env = AdaptiveEnvironment(difficulty=0.5)\n",
    "# easy_outcomes = torch.ones(20)  # 100% success\n",
    "# new_diff, action = adapt_difficulty_step(test_env, easy_outcomes)\n",
    "# assert action == \"increased\", f\"âŒ Should increase, got {action}\"\n",
    "# assert new_diff > 0.5, f\"âŒ Difficulty should increase from 0.5\"\n",
    "# print(f\"âœ… Difficulty adaptation works! {0.5:.2f} â†’ {new_diff:.2f} ({action})\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_10_closed_loop",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Closed Loop\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_10_closed_loop.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together â€” The Complete Closed Loop"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_closed_loop_training(\n",
    "    num_steps: int = 200,\n",
    "    batch_size: int = 16,\n",
    "    policy_lr: float = 3e-4,\n",
    "    prm_lr: float = 1e-3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the complete RLAnything closed-loop training.\n",
    "\n",
    "    Three components co-optimize:\n",
    "    1. Policy learns to solve tasks better\n",
    "    2. PRM learns to evaluate more accurately\n",
    "    3. Environment adapts difficulty to keep things challenging\n",
    "    \"\"\"\n",
    "    # Initialize all components\n",
    "    env = AdaptiveEnvironment(difficulty=0.3)\n",
    "    policy = PolicyModel().to(device)\n",
    "    ref_policy = PolicyModel().to(device)\n",
    "    ref_policy.load_state_dict(policy.state_dict())\n",
    "    prm = TrainablePRM(input_size=50).to(device)\n",
    "\n",
    "    policy_opt = torch.optim.Adam(policy.parameters(), lr=policy_lr)\n",
    "    prm_opt = torch.optim.Adam(prm.parameters(), lr=prm_lr)\n",
    "\n",
    "    reward_computer = IntegratedRewardComputer(lambda_weight=1.0, num_prm_votes=3)\n",
    "\n",
    "    # Metrics tracking\n",
    "    metrics = {\n",
    "        \"policy_success\": [],\n",
    "        \"prm_accuracy\": [],\n",
    "        \"difficulty\": [],\n",
    "        \"integrated_rewards\": [],\n",
    "        \"policy_loss\": [],\n",
    "    }\n",
    "\n",
    "    print(\"ðŸ”„ Starting RLAnything closed-loop training...\\n\")\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # === Step 1: Environment generates tasks ===\n",
    "        inputs, targets = env.generate_task(batch_size)\n",
    "\n",
    "        # === Step 2: Policy generates predictions ===\n",
    "        with torch.no_grad():\n",
    "            predictions = policy.predict(inputs)\n",
    "            outcomes = env.evaluate_outcome(predictions, targets)\n",
    "            success_rate = (outcomes > 0).float().mean().item()\n",
    "\n",
    "        # === Step 3: PRM evaluates each step ===\n",
    "        # Create step features from (input, prediction, target)\n",
    "        step_features = torch.zeros(batch_size, 50).to(device)\n",
    "        step_features[:, :10] = inputs.float() / 50.0\n",
    "        step_features[:, 10:20] = predictions.float() / 50.0\n",
    "        step_features[:, 20:30] = targets.float() / 50.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prm_scores, prm_votes = prm.evaluate_with_voting(step_features, num_votes=3)\n",
    "\n",
    "        # === Step 4: Compute integrated rewards ===\n",
    "        integrated_rewards = []\n",
    "        for b in range(batch_size):\n",
    "            prm_vote_list = prm_votes[:, b].tolist()\n",
    "            R = reward_computer.compute_integrated_reward(\n",
    "                outcomes[b].item(), prm_vote_list\n",
    "            )\n",
    "            integrated_rewards.append(R)\n",
    "        integrated_rewards_t = torch.tensor(integrated_rewards, device=device)\n",
    "\n",
    "        # === Step 5: Train policy with GRPO ===\n",
    "        log_probs_new = policy.get_log_probs(inputs, targets)\n",
    "        with torch.no_grad():\n",
    "            log_probs_ref = ref_policy.get_log_probs(inputs, targets)\n",
    "\n",
    "        # Compute advantages from integrated rewards\n",
    "        mean_r = integrated_rewards_t.mean()\n",
    "        std_r = integrated_rewards_t.std() + 1e-8\n",
    "        advantages = (integrated_rewards_t - mean_r) / std_r\n",
    "\n",
    "        ratio = torch.exp(log_probs_new - log_probs_ref.detach())\n",
    "        adv_expanded = advantages.unsqueeze(1).expand_as(ratio)\n",
    "        unclipped = ratio * adv_expanded\n",
    "        clipped = torch.clamp(ratio, 0.8, 1.28) * adv_expanded\n",
    "        policy_loss = -torch.min(unclipped, clipped).mean()\n",
    "\n",
    "        policy_opt.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n",
    "        policy_opt.step()\n",
    "\n",
    "        # === Step 6: Train PRM with consistency feedback ===\n",
    "        # Target: PRM should agree with integrated reward direction\n",
    "        prm_targets = torch.zeros(batch_size, dtype=torch.long, device=device)\n",
    "        prm_targets[integrated_rewards_t > 0.5] = 2   # Class +1\n",
    "        prm_targets[integrated_rewards_t < -0.5] = 0   # Class -1\n",
    "        prm_targets[(integrated_rewards_t >= -0.5) & (integrated_rewards_t <= 0.5)] = 1  # Class 0\n",
    "\n",
    "        prm_logits = prm(step_features)\n",
    "        prm_loss = F.cross_entropy(prm_logits, prm_targets)\n",
    "\n",
    "        prm_opt.zero_grad()\n",
    "        prm_loss.backward()\n",
    "        prm_opt.step()\n",
    "\n",
    "        # Compute PRM accuracy\n",
    "        with torch.no_grad():\n",
    "            prm_preds = prm_logits.argmax(dim=-1)\n",
    "            prm_accuracy = (prm_preds == prm_targets).float().mean().item()\n",
    "\n",
    "        # === Step 7: Adapt environment difficulty ===\n",
    "        env.adapt_difficulty(outcomes)\n",
    "\n",
    "        # === Record metrics ===\n",
    "        metrics[\"policy_success\"].append(success_rate)\n",
    "        metrics[\"prm_accuracy\"].append(prm_accuracy)\n",
    "        metrics[\"difficulty\"].append(env.difficulty)\n",
    "        metrics[\"integrated_rewards\"].append(integrated_rewards_t.mean().item())\n",
    "        metrics[\"policy_loss\"].append(policy_loss.item())\n",
    "\n",
    "        if (step + 1) % 50 == 0:\n",
    "            print(f\"  Step {step+1}/{num_steps}:\")\n",
    "            print(f\"    Policy success: {success_rate:.1%}\")\n",
    "            print(f\"    PRM accuracy:   {prm_accuracy:.1%}\")\n",
    "            print(f\"    Difficulty:      {env.difficulty:.2f}\")\n",
    "            print(f\"    Mean reward:     {integrated_rewards_t.mean():.2f}\")\n",
    "            print()\n",
    "\n",
    "    print(\"âœ… Closed-loop training complete!\")\n",
    "    return metrics, env\n",
    "\n",
    "# Run the training\n",
    "metrics, final_env = run_closed_loop_training(num_steps=200)"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_11_coevolution_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Coevolution Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_11_coevolution_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ðŸ“Š Visualizing the Co-Evolution"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "window = 10\n",
    "\n",
    "# 1. Policy success rate\n",
    "success_smooth = np.convolve(metrics[\"policy_success\"],\n",
    "                              np.ones(window)/window, mode='valid')\n",
    "axes[0, 0].plot(success_smooth, linewidth=2.5, color='#2ecc71')\n",
    "axes[0, 0].set_ylabel('Success Rate', fontsize=12)\n",
    "axes[0, 0].set_title('Policy Performance', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "\n",
    "# 2. PRM accuracy\n",
    "prm_smooth = np.convolve(metrics[\"prm_accuracy\"],\n",
    "                          np.ones(window)/window, mode='valid')\n",
    "axes[0, 1].plot(prm_smooth, linewidth=2.5, color='#3498db')\n",
    "axes[0, 1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0, 1].set_title('PRM Accuracy (co-evolving)', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "\n",
    "# 3. Environment difficulty\n",
    "axes[1, 0].plot(metrics[\"difficulty\"], linewidth=2.5, color='#e74c3c')\n",
    "axes[1, 0].set_xlabel('Training Step', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Difficulty', fontsize=12)\n",
    "axes[1, 0].set_title('Environment Difficulty (auto-adapting)', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_ylim(0, 1)\n",
    "\n",
    "# 4. All three together (normalized)\n",
    "success_norm = np.array(metrics[\"policy_success\"]) / max(metrics[\"policy_success\"])\n",
    "prm_norm = np.array(metrics[\"prm_accuracy\"]) / max(max(metrics[\"prm_accuracy\"]), 1e-8)\n",
    "diff_norm = np.array(metrics[\"difficulty\"]) / max(max(metrics[\"difficulty\"]), 1e-8)\n",
    "\n",
    "axes[1, 1].plot(np.convolve(success_norm, np.ones(window)/window, mode='valid'),\n",
    "                linewidth=2, color='#2ecc71', label='Policy')\n",
    "axes[1, 1].plot(np.convolve(prm_norm, np.ones(window)/window, mode='valid'),\n",
    "                linewidth=2, color='#3498db', label='PRM')\n",
    "axes[1, 1].plot(np.convolve(diff_norm, np.ones(window)/window, mode='valid'),\n",
    "                linewidth=2, color='#e74c3c', label='Difficulty')\n",
    "axes[1, 1].set_xlabel('Training Step', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Normalized Value', fontsize=12)\n",
    "axes[1, 1].set_title('Co-Evolution: All Three Components', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=11)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('RLAnything Closed Loop â€” Co-Optimization Dynamics',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_12_precision_bound",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Precision Bound\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_12_precision_bound.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Visualization: The Reward Precision Bound"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision_bound(m_values, mu):\n",
    "    \"\"\"Compute the reward precision lower bound.\"\"\"\n",
    "    return [1 - np.exp(-m * (mu - 1)**2 / 4) for m in m_values]\n",
    "\n",
    "m_values = np.arange(1, 101)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for mu, color, label in [\n",
    "    (1.1, '#e74c3c', 'Î¼=1.1 (barely better than random)'),\n",
    "    (1.2, '#f39c12', 'Î¼=1.2 (mediocre PRM)'),\n",
    "    (1.35, '#3498db', 'Î¼=1.35 (p+=0.7, p-=0.65)'),\n",
    "    (1.5, '#2ecc71', 'Î¼=1.5 (good PRM)'),\n",
    "    (1.8, '#9b59b6', 'Î¼=1.8 (excellent PRM)'),\n",
    "]:\n",
    "    precision = compute_precision_bound(m_values, mu)\n",
    "    ax.plot(m_values, precision, linewidth=2, color=color, label=label)\n",
    "\n",
    "ax.axhline(y=0.95, color='gray', linestyle='--', alpha=0.5, label='95% precision')\n",
    "ax.set_xlabel('Number of PRM Evaluations (m)', fontsize=12)\n",
    "ax.set_ylabel('Precision Lower Bound (A)', fontsize=12)\n",
    "ax.set_title('Reward Precision Converges with More PRM Evaluations', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10, loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Specific numerical examples from the article\n",
    "print(\"Precision bounds for Î¼=1.35 (p+=0.7, p-=0.65):\")\n",
    "for m in [5, 10, 20, 50, 100]:\n",
    "    A = 1 - np.exp(-m * (1.35 - 1)**2 / 4)\n",
    "    print(f\"  m={m:3d}: A â‰¥ {A:.3f}\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_13_final_output",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Final Output\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_13_final_output.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ðŸŽ¯ Final Output: The Complete RLAnything System"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Create a visual summary of the closed-loop system\n",
    "gs = fig.add_gridspec(2, 3, hspace=0.4, wspace=0.3)\n",
    "\n",
    "# Top-left: Policy improvement\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "success_smooth = np.convolve(metrics[\"policy_success\"],\n",
    "                              np.ones(10)/10, mode='valid')\n",
    "ax1.fill_between(range(len(success_smooth)), success_smooth,\n",
    "                 alpha=0.3, color='#2ecc71')\n",
    "ax1.plot(success_smooth, linewidth=2, color='#2ecc71')\n",
    "ax1.set_title('ðŸŽ¯ Policy', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Success Rate')\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Top-center: PRM improvement\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "prm_smooth = np.convolve(metrics[\"prm_accuracy\"],\n",
    "                          np.ones(10)/10, mode='valid')\n",
    "ax2.fill_between(range(len(prm_smooth)), prm_smooth,\n",
    "                 alpha=0.3, color='#3498db')\n",
    "ax2.plot(prm_smooth, linewidth=2, color='#3498db')\n",
    "ax2.set_title('âš–ï¸ Reward Model', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# Top-right: Environment adaptation\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.fill_between(range(len(metrics[\"difficulty\"])), metrics[\"difficulty\"],\n",
    "                 alpha=0.3, color='#e74c3c')\n",
    "ax3.plot(metrics[\"difficulty\"], linewidth=2, color='#e74c3c')\n",
    "ax3.set_title('ðŸŒ Environment', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Difficulty')\n",
    "ax3.set_ylim(0, 1)\n",
    "\n",
    "# Bottom: Integrated reward evolution\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "reward_smooth = np.convolve(metrics[\"integrated_rewards\"],\n",
    "                             np.ones(10)/10, mode='valid')\n",
    "ax4.plot(reward_smooth, linewidth=2.5, color='#9b59b6')\n",
    "ax4.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax4.set_xlabel('Training Step', fontsize=12)\n",
    "ax4.set_ylabel('Mean Integrated Reward', fontsize=12)\n",
    "ax4.set_title('Integrated Reward (Outcome + PRM)', fontsize=12, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('ðŸŽ‰ RLAnything: Complete Closed-Loop Co-Optimization',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final statistics\n",
    "final_success = np.mean(metrics[\"policy_success\"][-20:])\n",
    "final_prm_acc = np.mean(metrics[\"prm_accuracy\"][-20:])\n",
    "final_difficulty = metrics[\"difficulty\"][-1]\n",
    "initial_success = np.mean(metrics[\"policy_success\"][:20])\n",
    "initial_prm_acc = np.mean(metrics[\"prm_accuracy\"][:20])\n",
    "\n",
    "print(\"ðŸŽ‰ Congratulations! You've built the RLAnything closed loop from scratch!\\n\")\n",
    "print(\"ðŸ“Š Training Summary:\")\n",
    "print(f\"   Policy success:  {initial_success:.1%} â†’ {final_success:.1%} \"\n",
    "      f\"({final_success - initial_success:+.1%})\")\n",
    "print(f\"   PRM accuracy:    {initial_prm_acc:.1%} â†’ {final_prm_acc:.1%} \"\n",
    "      f\"({final_prm_acc - initial_prm_acc:+.1%})\")\n",
    "print(f\"   Difficulty:      0.30 â†’ {final_difficulty:.2f}\")\n",
    "print(f\"\\n   All three components improved together â€” this is co-optimization! ðŸš€\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_14_reflection_and_series_wrap",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Wrap-Up: Reflection And Series Wrap\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_14_reflection_and_series_wrap.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### ðŸ¤” Reflection Questions\n",
    "1. What would happen if we made the PRM too powerful (100% accurate from the start)? Would co-optimization still help?\n",
    "2. The difficulty adaptation uses fixed thresholds (80% and 20%). What would happen with tighter thresholds (e.g., 60% and 40%)? Would the system oscillate?\n",
    "3. In the real OpenClaw-RL system, the user is the \"environment\" â€” they generate tasks by chatting. How does this differ from our simulated environment? What challenges does this introduce?\n",
    "\n",
    "### ðŸ† Optional Challenges\n",
    "1. **Multi-step trajectories**: Extend the system to handle multi-step tasks where each step has its own integrated reward.\n",
    "2. **PRM ensemble**: Instead of one PRM, train multiple PRMs and use their agreement as an additional signal.\n",
    "3. **Catastrophic forgetting**: Add a regularization term that prevents the policy from forgetting old skills while learning new preferences."
   ],
   "id": "cell_30"
  }
 ]
}