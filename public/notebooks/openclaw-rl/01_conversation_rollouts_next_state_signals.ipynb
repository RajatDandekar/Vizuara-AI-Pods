{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Conversation Rollouts and Next-State Signals ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1K3QJmxvgc0_ZwXawSeV_oQll-6WeibaO\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/01_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_01_setup",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_01_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Conversation Rollouts and Next-State Signals: Turning Chat into Training Data\n",
    "\n",
    "*Part 1 of the Vizuara series on OpenClaw-RL*\n",
    "*Estimated time: 45 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ü§ñ AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** ‚Äî it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[üëâ Open AI Teaching Assistant](https://pods.vizuara.ai/courses/openclaw-rl/practice/1/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_02_why_it_matters",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_02_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Every time you chat with an AI assistant, you are generating valuable training signal ‚Äî but current systems throw it all away.\n",
    "\n",
    "When you say \"No, I wanted Python not JavaScript,\" that correction contains precise information about your preferences. When you say \"Perfect, thanks!\", that confirmation tells the model it did something right.\n",
    "\n",
    "**OpenClaw-RL captures this signal.** In this notebook, we will build the **rollout collection** system from scratch ‚Äî the component that transforms your everyday conversations into structured training data.\n",
    "\n",
    "By the end of this notebook, you will have a working `ConversationRolloutCollector` that:\n",
    "- Tracks multi-turn conversation sessions\n",
    "- Classifies each turn as trainable or non-trainable\n",
    "- Extracts **next-state signals** ‚Äî where the user's next message becomes the feedback for the model's previous response\n",
    "- Produces ready-to-train samples with (prompt, response, feedback) triples"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Teaser: Here's what we'll build\n",
    "# Input: A raw multi-turn conversation\n",
    "# Output: Structured training samples with natural feedback signals\n",
    "\n",
    "# conversation = [\n",
    "#     {\"role\": \"user\", \"content\": \"Write me a sorting function in Python\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Here's a JavaScript sort: ...\"},\n",
    "#     {\"role\": \"user\", \"content\": \"No, I said Python not JavaScript\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Sorry! Here's the Python version: ...\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Perfect, thanks!\"},\n",
    "# ]\n",
    "#\n",
    "# collector.process(conversation) ‚Üí\n",
    "#   Sample 1: response=\"JS sort\", feedback=\"No, I said Python\" ‚Üí NEGATIVE\n",
    "#   Sample 2: response=\"Python sort\", feedback=\"Perfect, thanks!\" ‚Üí POSITIVE"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_03_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_03_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Let us think about what happens in a typical conversation with an AI assistant.\n",
    "\n",
    "You ask something. The model responds. Then you react to that response.\n",
    "\n",
    "That reaction ‚Äî your next message ‚Äî is incredibly informative:\n",
    "\n",
    "- **\"Great, exactly what I needed!\"** ‚Üí The model did well. Positive signal.\n",
    "- **\"No, I meant the other thing.\"** ‚Üí The model misunderstood. Negative signal with direction.\n",
    "- **\"Can you also add error handling?\"** ‚Üí The model did okay, but there is room for improvement. Neutral/mildly positive signal.\n",
    "- **\"That code has a bug on line 3.\"** ‚Üí Specific negative signal with a precise correction.\n",
    "\n",
    "This is the **next-state signal** idea. In traditional reinforcement learning, we need a separate reward function to tell the agent how well it did. In OpenClaw-RL, the reward is already there ‚Äî embedded naturally in the conversation flow.\n",
    "\n",
    "But not every message in a conversation is useful for training. Some messages are:\n",
    "- System messages or metadata\n",
    "- The user asking for clarification (not reacting to a response)\n",
    "- Tool calls or function results\n",
    "\n",
    "We need to classify each turn and extract only the useful training pairs.\n",
    "\n",
    "### ü§î Think About This\n",
    "\n",
    "Before we start coding, consider this question:\n",
    "\n",
    "In a 10-turn conversation, how many training samples do you think we can extract? Is it 10? 5? Just 1?\n",
    "\n",
    "Think about which turns have a clear (response, feedback) pair and which do not."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_04_math",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_04_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "The rollout collection system does not involve heavy mathematics, but it does have a formal structure.\n",
    "\n",
    "A **conversation session** is a sequence of turns:\n",
    "\n",
    "$$\\tau = \\{(r_1, m_1), (r_2, m_2), \\ldots, (r_T, m_T)\\}$$\n",
    "\n",
    "where $r_t \\in \\{\\text{user}, \\text{assistant}, \\text{system}\\}$ is the role and $m_t$ is the message content.\n",
    "\n",
    "A turn $(r_t, m_t)$ is **trainable** (main-line) if:\n",
    "1. $r_t = \\text{assistant}$ (the model produced a response)\n",
    "2. There exists a subsequent user turn $(r_{t+k}, m_{t+k})$ where $r_{t+k} = \\text{user}$ and $k$ is minimal\n",
    "3. That user turn is a **reaction** to the assistant's response (not a new topic)\n",
    "\n",
    "The **next-state signal** for a trainable assistant turn at position $t$ is:\n",
    "\n",
    "$$\\text{feedback}_t = m_{t+k}$$\n",
    "\n",
    "where $t+k$ is the position of the next user message after turn $t$.\n",
    "\n",
    "Computationally, this means: for every assistant response, we look ahead in the conversation to find the user's next message, and treat that message as natural feedback.\n",
    "\n",
    "The resulting training sample is a triple:\n",
    "\n",
    "$$s_t = (\\text{context}_t, \\text{response}_t, \\text{feedback}_t)$$\n",
    "\n",
    "where $\\text{context}_t$ is everything the model saw before generating $\\text{response}_t$."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_05_data_structures",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Data Structures\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_05_data_structures.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It ‚Äî Component by Component\n",
    "\n",
    "### 4.1 Setup and Imports"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "import hashlib\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from typing import Optional\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All imports ready!\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Define the Data Structures\n",
    "\n",
    "First, let us define the core data types. In OpenClaw-RL, every turn has a **role**, a **classification**, and optional metadata."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurnRole(Enum):\n",
    "    \"\"\"Who sent this message?\"\"\"\n",
    "    USER = \"user\"\n",
    "    ASSISTANT = \"assistant\"\n",
    "    SYSTEM = \"system\"\n",
    "    TOOL = \"tool\"\n",
    "\n",
    "class TurnClassification(Enum):\n",
    "    \"\"\"Is this turn useful for training?\"\"\"\n",
    "    MAIN_LINE = \"main_line\"     # Trainable ‚Äî core interaction turn\n",
    "    SIDE = \"side\"               # Non-trainable ‚Äî metadata, system, or off-topic\n",
    "    UNCLASSIFIED = \"unclassified\"\n",
    "\n",
    "@dataclass\n",
    "class ConversationTurn:\n",
    "    \"\"\"A single turn in a conversation.\"\"\"\n",
    "    role: TurnRole\n",
    "    content: str\n",
    "    turn_index: int\n",
    "    classification: TurnClassification = TurnClassification.UNCLASSIFIED\n",
    "    timestamp: Optional[str] = None\n",
    "    is_tool_call: bool = False\n",
    "\n",
    "    def __repr__(self):\n",
    "        cls = self.classification.value[:4].upper()\n",
    "        return f\"Turn {self.turn_index} [{self.role.value:>9}] ({cls}): {self.content[:60]}...\"\n",
    "\n",
    "@dataclass\n",
    "class TrainingSample:\n",
    "    \"\"\"A structured training sample extracted from a conversation.\"\"\"\n",
    "    session_id: str\n",
    "    context: list           # All turns before the response\n",
    "    response: str           # The assistant's response\n",
    "    feedback: str           # The user's next message (next-state signal)\n",
    "    response_turn_index: int\n",
    "    feedback_turn_index: int\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"TrainingSample(response='{self.response[:40]}...', \"\n",
    "                f\"feedback='{self.feedback[:40]}...')\")\n",
    "\n",
    "print(\"‚úÖ Data structures defined!\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a sample conversation to work with:"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A realistic multi-turn conversation\n",
    "sample_conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write me a function to sort a list in Python.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Here's a sorting function in JavaScript:\\n\\nfunction sortArray(arr) {\\n  return arr.sort((a, b) => a - b);\\n}\"},\n",
    "    {\"role\": \"user\", \"content\": \"No, I said Python not JavaScript. Please use Python.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sorry about that! Here's the Python version:\\n\\ndef sort_list(lst):\\n    return sorted(lst)\"},\n",
    "    {\"role\": \"user\", \"content\": \"Perfect, thanks! Can you also add a reverse parameter?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here you go:\\n\\ndef sort_list(lst, reverse=False):\\n    return sorted(lst, reverse=reverse)\"},\n",
    "    {\"role\": \"user\", \"content\": \"Great, exactly what I needed!\"},\n",
    "]\n",
    "\n",
    "print(f\"Sample conversation has {len(sample_conversation)} turns\")\n",
    "for i, turn in enumerate(sample_conversation):\n",
    "    content_preview = turn['content'][:60].replace('\\n', ' ')\n",
    "    print(f\"  Turn {i}: [{turn['role']:>9}] {content_preview}...\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_06_classifier",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Classifier\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_06_classifier.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The Turn Classifier\n",
    "\n",
    "Now let us build the turn classifier. This is the component that decides which turns are **main-line** (trainable) and which are **side** (non-trainable).\n",
    "\n",
    "The rules are:\n",
    "1. **System turns** ‚Üí always SIDE (they are instructions, not interactions)\n",
    "2. **Tool turns** ‚Üí always SIDE (tool results are not conversational feedback)\n",
    "3. **Assistant turns** ‚Üí MAIN_LINE if followed by a user reaction; SIDE otherwise\n",
    "4. **User turns** ‚Üí MAIN_LINE if they follow an assistant turn (they might be feedback); SIDE if they follow a system/tool turn"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurnClassifier:\n",
    "    \"\"\"Classifies conversation turns as main-line (trainable) or side (non-trainable).\"\"\"\n",
    "\n",
    "    # Keywords that indicate a message is metadata, not real feedback\n",
    "    METADATA_KEYWORDS = [\n",
    "        \"function_call\", \"tool_result\", \"system:\",\n",
    "        \"[METADATA]\", \"[INTERNAL]\"\n",
    "    ]\n",
    "\n",
    "    def classify_turns(self, raw_turns: list) -> list:\n",
    "        \"\"\"\n",
    "        Classify each turn in a conversation.\n",
    "\n",
    "        Args:\n",
    "            raw_turns: List of dicts with 'role' and 'content' keys\n",
    "\n",
    "        Returns:\n",
    "            List of ConversationTurn objects with classifications set\n",
    "        \"\"\"\n",
    "        turns = []\n",
    "        for i, raw in enumerate(raw_turns):\n",
    "            role = TurnRole(raw[\"role\"])\n",
    "            is_tool = raw.get(\"is_tool_call\", False)\n",
    "            turn = ConversationTurn(\n",
    "                role=role,\n",
    "                content=raw[\"content\"],\n",
    "                turn_index=i,\n",
    "                is_tool_call=is_tool,\n",
    "                timestamp=raw.get(\"timestamp\", datetime.now().isoformat())\n",
    "            )\n",
    "            turns.append(turn)\n",
    "\n",
    "        # Now classify each turn based on context\n",
    "        for i, turn in enumerate(turns):\n",
    "            turn.classification = self._classify_single(turn, i, turns)\n",
    "\n",
    "        return turns\n",
    "\n",
    "    def _classify_single(self, turn, index, all_turns):\n",
    "        \"\"\"Classify a single turn based on its role and context.\"\"\"\n",
    "        # Rule 1: System and tool turns are always side\n",
    "        if turn.role == TurnRole.SYSTEM:\n",
    "            return TurnClassification.SIDE\n",
    "        if turn.is_tool_call or turn.role == TurnRole.TOOL:\n",
    "            return TurnClassification.SIDE\n",
    "\n",
    "        # Rule 2: Check for metadata content\n",
    "        if self._is_metadata(turn.content):\n",
    "            return TurnClassification.SIDE\n",
    "\n",
    "        # Rule 3: Assistant turns are main-line if followed by a user turn\n",
    "        if turn.role == TurnRole.ASSISTANT:\n",
    "            has_user_followup = any(\n",
    "                t.role == TurnRole.USER\n",
    "                for t in all_turns[index + 1:]\n",
    "            )\n",
    "            return TurnClassification.MAIN_LINE if has_user_followup else TurnClassification.SIDE\n",
    "\n",
    "        # Rule 4: User turns are main-line if they follow an assistant turn\n",
    "        if turn.role == TurnRole.USER:\n",
    "            if index > 0 and all_turns[index - 1].role == TurnRole.ASSISTANT:\n",
    "                return TurnClassification.MAIN_LINE\n",
    "            # First user message (the initial prompt) is also main-line\n",
    "            if index == 0 or (index > 0 and all_turns[index - 1].role == TurnRole.SYSTEM):\n",
    "                return TurnClassification.MAIN_LINE\n",
    "            return TurnClassification.SIDE\n",
    "\n",
    "        return TurnClassification.SIDE\n",
    "\n",
    "    def _is_metadata(self, content: str) -> bool:\n",
    "        \"\"\"Check if content looks like metadata rather than real conversation.\"\"\"\n",
    "        content_lower = content.lower()\n",
    "        return any(kw.lower() in content_lower for kw in self.METADATA_KEYWORDS)\n",
    "\n",
    "# Test the classifier\n",
    "classifier = TurnClassifier()\n",
    "classified_turns = classifier.classify_turns(sample_conversation)\n",
    "\n",
    "print(\"Classified turns:\")\n",
    "for turn in classified_turns:\n",
    "    emoji = \"üü¢\" if turn.classification == TurnClassification.MAIN_LINE else \"‚ö™\"\n",
    "    print(f\"  {emoji} {turn}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_07_viz_flow",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Viz Flow\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_07_viz_flow.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Visualization: Conversation Flow with Classifications\n",
    "\n",
    "Let us visualize which turns are trainable and which are not."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_conversation_flow(turns):\n",
    "    \"\"\"Visualize a conversation with turn classifications.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, len(turns) * 0.8 + 1))\n",
    "\n",
    "    colors = {\n",
    "        TurnClassification.MAIN_LINE: '#2ecc71',  # Green\n",
    "        TurnClassification.SIDE: '#95a5a6',         # Gray\n",
    "    }\n",
    "    role_x = {\n",
    "        TurnRole.SYSTEM: 0.1,\n",
    "        TurnRole.USER: 0.3,\n",
    "        TurnRole.ASSISTANT: 0.7,\n",
    "    }\n",
    "\n",
    "    for i, turn in enumerate(turns):\n",
    "        y = len(turns) - i - 1\n",
    "        x = role_x.get(turn.role, 0.5)\n",
    "        color = colors.get(turn.classification, '#bdc3c7')\n",
    "\n",
    "        # Draw the turn box\n",
    "        content_short = turn.content[:50].replace('\\n', ' ')\n",
    "        bbox = dict(boxstyle=\"round,pad=0.3\", facecolor=color, alpha=0.7)\n",
    "        ax.text(x, y, f\"[{turn.role.value}] {content_short}...\",\n",
    "                fontsize=9, ha='center', va='center', bbox=bbox,\n",
    "                fontfamily='monospace')\n",
    "\n",
    "        # Draw arrow from previous turn\n",
    "        if i > 0:\n",
    "            prev_y = len(turns) - (i - 1) - 1\n",
    "            prev_x = role_x.get(turns[i-1].role, 0.5)\n",
    "            ax.annotate('', xy=(x, y + 0.3), xytext=(prev_x, prev_y - 0.3),\n",
    "                       arrowprops=dict(arrowstyle='->', color='#34495e', lw=1.5))\n",
    "\n",
    "    # Legend\n",
    "    main_patch = mpatches.Patch(color='#2ecc71', alpha=0.7, label='Main-line (trainable)')\n",
    "    side_patch = mpatches.Patch(color='#95a5a6', alpha=0.7, label='Side (non-trainable)')\n",
    "    ax.legend(handles=[main_patch, side_patch], loc='upper right', fontsize=10)\n",
    "\n",
    "    ax.set_xlim(-0.1, 1.1)\n",
    "    ax.set_ylim(-0.8, len(turns) + 0.2)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Conversation Flow with Turn Classifications', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_conversation_flow(classified_turns)"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_08_extractor",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Extractor\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_08_extractor.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The Next-State Signal Extractor\n",
    "\n",
    "Now for the key insight of OpenClaw-RL. For every assistant response, we extract the **next-state signal** ‚Äî the user's next message that serves as natural feedback."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextStateExtractor:\n",
    "    \"\"\"Extracts next-state signals from classified conversation turns.\"\"\"\n",
    "\n",
    "    def extract_pairs(self, turns: list) -> list:\n",
    "        \"\"\"\n",
    "        For each main-line assistant turn, find the next user turn\n",
    "        and create a (response, feedback) pair.\n",
    "\n",
    "        Args:\n",
    "            turns: List of classified ConversationTurn objects\n",
    "\n",
    "        Returns:\n",
    "            List of (assistant_turn, user_feedback_turn) pairs\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "\n",
    "        for i, turn in enumerate(turns):\n",
    "            # Only process main-line assistant turns\n",
    "            if turn.role != TurnRole.ASSISTANT:\n",
    "                continue\n",
    "            if turn.classification != TurnClassification.MAIN_LINE:\n",
    "                continue\n",
    "\n",
    "            # Look ahead to find the next user turn\n",
    "            feedback_turn = self._find_next_user_turn(turns, i)\n",
    "\n",
    "            if feedback_turn is not None:\n",
    "                pairs.append((turn, feedback_turn))\n",
    "\n",
    "        return pairs\n",
    "\n",
    "    def _find_next_user_turn(self, turns, start_index):\n",
    "        \"\"\"Find the next user turn after position start_index.\"\"\"\n",
    "        for j in range(start_index + 1, len(turns)):\n",
    "            if turns[j].role == TurnRole.USER:\n",
    "                return turns[j]\n",
    "        return None\n",
    "\n",
    "# Extract next-state pairs\n",
    "extractor = NextStateExtractor()\n",
    "pairs = extractor.extract_pairs(classified_turns)\n",
    "\n",
    "print(f\"Extracted {len(pairs)} (response, feedback) pairs:\\n\")\n",
    "for i, (response_turn, feedback_turn) in enumerate(pairs):\n",
    "    print(f\"Pair {i + 1}:\")\n",
    "    print(f\"  üì§ Assistant: {response_turn.content[:80]}...\")\n",
    "    print(f\"  üì• Feedback:  {feedback_turn.content[:80]}...\")\n",
    "    print()"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§î Think About This\n",
    "\n",
    "Look at the pairs we extracted:\n",
    "- Pair 1: The assistant wrote JavaScript, and the user corrected it ‚Üí **negative signal**\n",
    "- Pair 2: The assistant fixed it to Python, and the user said \"Perfect\" ‚Üí **positive signal**\n",
    "- Pair 3: The assistant added the reverse parameter, and the user confirmed ‚Üí **positive signal**\n",
    "\n",
    "Notice how we got 3 training samples from a single 8-turn conversation ‚Äî all without any manual labeling!"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_09_todo_context",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo Context\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_09_todo_context.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîß Your Turn\n",
    "\n",
    "### TODO 1: Build the Context Assembler\n",
    "\n",
    "The training sample needs the full **context** ‚Äî everything the model saw before generating its response. This includes all previous turns in the conversation.\n",
    "\n",
    "Implement the `build_context` function:"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(turns: list, response_turn_index: int) -> list:\n",
    "    \"\"\"\n",
    "    Build the context (conversation history) that the model saw\n",
    "    before generating the response at response_turn_index.\n",
    "\n",
    "    Args:\n",
    "        turns: All conversation turns\n",
    "        response_turn_index: Index of the assistant's response turn\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with 'role' and 'content' for all turns\n",
    "        BEFORE the response (not including the response itself)\n",
    "\n",
    "    Example:\n",
    "        If turns are [system, user, assistant, user, assistant]\n",
    "        and response_turn_index = 4, the context should include\n",
    "        turns at indices [0, 1, 2, 3]\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Collect all turns before response_turn_index\n",
    "    # Return them as a list of {\"role\": ..., \"content\": ...} dicts\n",
    "    # ==============================\n",
    "\n",
    "    context = ???  # YOUR CODE HERE\n",
    "\n",
    "    return context\n",
    "\n",
    "# ‚úÖ Verification\n",
    "test_context = build_context(classified_turns, 2)  # Context for first assistant response\n",
    "assert len(test_context) == 2, f\"‚ùå Expected 2 turns in context, got {len(test_context)}\"\n",
    "assert test_context[0][\"role\"] == \"system\", f\"‚ùå First turn should be system, got {test_context[0]['role']}\"\n",
    "assert test_context[1][\"role\"] == \"user\", f\"‚ùå Second turn should be user, got {test_context[1]['role']}\"\n",
    "print(\"‚úÖ Context assembler works correctly!\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_10_todo_collector",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo Collector\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_10_todo_collector.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement the Complete Rollout Collector\n",
    "\n",
    "Now combine everything into a single `ConversationRolloutCollector` class:"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationRolloutCollector:\n",
    "    \"\"\"\n",
    "    The complete rollout collection pipeline.\n",
    "    Takes raw conversations and produces structured training samples.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.classifier = TurnClassifier()\n",
    "        self.extractor = NextStateExtractor()\n",
    "        self.sessions = {}  # session_id -> list of samples\n",
    "\n",
    "    def process_conversation(self, raw_turns: list, session_id: str = None) -> list:\n",
    "        \"\"\"\n",
    "        Process a raw conversation into training samples.\n",
    "\n",
    "        Args:\n",
    "            raw_turns: List of dicts with 'role' and 'content'\n",
    "            session_id: Optional session identifier\n",
    "\n",
    "        Returns:\n",
    "            List of TrainingSample objects\n",
    "\n",
    "        Steps:\n",
    "            1. Generate session_id if not provided\n",
    "            2. Classify all turns\n",
    "            3. Extract (response, feedback) pairs\n",
    "            4. Build context for each pair\n",
    "            5. Create TrainingSample objects\n",
    "        \"\"\"\n",
    "        if session_id is None:\n",
    "            session_id = str(uuid.uuid4())[:8]\n",
    "\n",
    "        # ============ TODO ============\n",
    "        # Step 1: Classify turns using self.classifier\n",
    "        # Step 2: Extract pairs using self.extractor\n",
    "        # Step 3: For each pair, build context and create TrainingSample\n",
    "        # Step 4: Store in self.sessions and return\n",
    "        # ==============================\n",
    "\n",
    "        samples = ???  # YOUR CODE HERE\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def get_all_samples(self) -> list:\n",
    "        \"\"\"Return all training samples across all sessions.\"\"\"\n",
    "        all_samples = []\n",
    "        for samples in self.sessions.values():\n",
    "            all_samples.extend(samples)\n",
    "        return all_samples\n",
    "\n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Return statistics about collected rollouts.\"\"\"\n",
    "        all_samples = self.get_all_samples()\n",
    "        return {\n",
    "            \"total_sessions\": len(self.sessions),\n",
    "            \"total_samples\": len(all_samples),\n",
    "            \"avg_samples_per_session\": len(all_samples) / max(len(self.sessions), 1),\n",
    "        }\n",
    "\n",
    "# ‚úÖ Verification\n",
    "collector = ConversationRolloutCollector()\n",
    "samples = collector.process_conversation(sample_conversation, session_id=\"test_001\")\n",
    "\n",
    "assert len(samples) == 3, f\"‚ùå Expected 3 training samples, got {len(samples)}\"\n",
    "assert samples[0].feedback == \"No, I said Python not JavaScript. Please use Python.\", \\\n",
    "    f\"‚ùå First feedback should be the correction, got: {samples[0].feedback[:50]}\"\n",
    "assert samples[1].feedback == \"Perfect, thanks! Can you also add a reverse parameter?\", \\\n",
    "    f\"‚ùå Second feedback should be positive\"\n",
    "print(f\"‚úÖ Rollout collector works! Extracted {len(samples)} samples from 1 conversation.\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_11_putting_together",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Putting Together\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_11_putting_together.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us now test our collector on multiple realistic conversations:"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple conversation sessions\n",
    "conversations = [\n",
    "    {\n",
    "        \"session_id\": \"session_001\",\n",
    "        \"turns\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What's the capital of France?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Correct! And what about Germany?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"The capital of Germany is Berlin.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Right, thanks!\"},\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"session_id\": \"session_002\",\n",
    "        \"turns\": [\n",
    "            {\"role\": \"user\", \"content\": \"Explain recursion in simple terms.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Recursion is when a function calls itself. It's like looking into two mirrors facing each other ‚Äî the reflection contains itself infinitely.\"},\n",
    "            {\"role\": \"user\", \"content\": \"That's a great analogy! Can you show me a code example?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Sure! Here's a recursive factorial function:\\n\\ndef factorial(n):\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n - 1)\"},\n",
    "            {\"role\": \"user\", \"content\": \"Perfect, very clear.\"},\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"session_id\": \"session_003\",\n",
    "        \"turns\": [\n",
    "            {\"role\": \"user\", \"content\": \"Help me write a REST API in Flask.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Here's how to create a REST API using Django:\\n\\nfrom django.http import JsonResponse...\"},\n",
    "            {\"role\": \"user\", \"content\": \"I asked for Flask, not Django. Please use Flask.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"My apologies! Here's the Flask version:\\n\\nfrom flask import Flask, jsonify\\n\\napp = Flask(__name__)\\n\\n@app.route('/api/items')\\ndef get_items():\\n    return jsonify({'items': []})\"},\n",
    "            {\"role\": \"user\", \"content\": \"Now that's what I wanted. Add a POST endpoint too.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Here's the updated version with both GET and POST:\\n\\n@app.route('/api/items', methods=['GET', 'POST'])\\ndef items():\\n    if request.method == 'POST':\\n        data = request.get_json()\\n        return jsonify(data), 201\\n    return jsonify({'items': []})\"},\n",
    "            {\"role\": \"user\", \"content\": \"Excellent work!\"},\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "# Process all conversations\n",
    "collector = ConversationRolloutCollector()\n",
    "all_samples = []\n",
    "\n",
    "for conv in conversations:\n",
    "    samples = collector.process_conversation(conv[\"turns\"], conv[\"session_id\"])\n",
    "    all_samples.extend(samples)\n",
    "    print(f\"Session {conv['session_id']}: {len(samples)} training samples\")\n",
    "\n",
    "stats = collector.get_stats()\n",
    "print(f\"\\nüìä Overall Statistics:\")\n",
    "print(f\"  Total sessions: {stats['total_sessions']}\")\n",
    "print(f\"  Total samples: {stats['total_samples']}\")\n",
    "print(f\"  Avg samples per session: {stats['avg_samples_per_session']:.1f}\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_12_analysis",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Analysis\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_12_analysis.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Data Analysis\n",
    "\n",
    "Let us analyze the quality of our extracted training data:"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feedback_signals(samples):\n",
    "    \"\"\"Analyze the distribution of feedback signals in our training data.\"\"\"\n",
    "\n",
    "    # Simple heuristic-based sentiment classification\n",
    "    positive_keywords = [\"perfect\", \"great\", \"correct\", \"thanks\", \"excellent\",\n",
    "                         \"good\", \"right\", \"exactly\", \"wonderful\", \"clear\"]\n",
    "    negative_keywords = [\"no\", \"wrong\", \"not\", \"incorrect\", \"don't\",\n",
    "                         \"didn't\", \"mistake\", \"error\", \"instead\"]\n",
    "\n",
    "    sentiments = []\n",
    "    for sample in samples:\n",
    "        feedback_lower = sample.feedback.lower()\n",
    "        pos_score = sum(1 for kw in positive_keywords if kw in feedback_lower)\n",
    "        neg_score = sum(1 for kw in negative_keywords if kw in feedback_lower)\n",
    "\n",
    "        if pos_score > neg_score:\n",
    "            sentiments.append(\"positive\")\n",
    "        elif neg_score > pos_score:\n",
    "            sentiments.append(\"negative\")\n",
    "        else:\n",
    "            sentiments.append(\"neutral\")\n",
    "\n",
    "    return sentiments\n",
    "\n",
    "sentiments = analyze_feedback_signals(all_samples)\n",
    "\n",
    "# Visualize the distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Sentiment distribution\n",
    "colors_map = {\"positive\": \"#2ecc71\", \"negative\": \"#e74c3c\", \"neutral\": \"#f39c12\"}\n",
    "sentiment_counts = {s: sentiments.count(s) for s in [\"positive\", \"negative\", \"neutral\"]}\n",
    "bars = axes[0].bar(sentiment_counts.keys(), sentiment_counts.values(),\n",
    "                   color=[colors_map[s] for s in sentiment_counts.keys()],\n",
    "                   edgecolor='white', linewidth=2)\n",
    "axes[0].set_title(\"Feedback Signal Distribution\", fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "for bar, count in zip(bars, sentiment_counts.values()):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                str(count), ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Right: Samples per session\n",
    "session_counts = [len(s) for s in collector.sessions.values()]\n",
    "session_labels = list(collector.sessions.keys())\n",
    "axes[1].barh(session_labels, session_counts, color='#3498db', edgecolor='white', linewidth=2)\n",
    "axes[1].set_title(\"Training Samples per Session\", fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel(\"Number of Samples\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Feedback Signal Analysis:\")\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    pct = 100 * count / len(sentiments)\n",
    "    print(f\"  {sentiment.capitalize():>8}: {count} ({pct:.0f}%)\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Visualization: The Next-State Signal Flow"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_next_state_flow(samples, max_samples=5):\n",
    "    \"\"\"Visualize how next-state signals connect responses to feedback.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, max(len(samples[:max_samples]) * 2, 4)))\n",
    "\n",
    "    for i, sample in enumerate(samples[:max_samples]):\n",
    "        y = len(samples[:max_samples]) - i - 1\n",
    "\n",
    "        # Draw response box\n",
    "        resp_text = sample.response[:45].replace('\\n', ' ')\n",
    "        ax.text(0.15, y * 2, f\"ü§ñ {resp_text}...\", fontsize=9,\n",
    "                ha='left', va='center',\n",
    "                bbox=dict(boxstyle=\"round\", facecolor='#3498db', alpha=0.3))\n",
    "\n",
    "        # Draw feedback box\n",
    "        fb_text = sample.feedback[:45].replace('\\n', ' ')\n",
    "        sentiment = analyze_feedback_signals([sample])[0]\n",
    "        fb_color = {\"positive\": \"#2ecc71\", \"negative\": \"#e74c3c\", \"neutral\": \"#f39c12\"}[sentiment]\n",
    "        ax.text(0.15, y * 2 - 0.7, f\"üë§ {fb_text}...\", fontsize=9,\n",
    "                ha='left', va='center',\n",
    "                bbox=dict(boxstyle=\"round\", facecolor=fb_color, alpha=0.3))\n",
    "\n",
    "        # Draw arrow\n",
    "        ax.annotate('next-state\\nsignal',\n",
    "                    xy=(0.13, y * 2 - 0.5), xytext=(0.13, y * 2 - 0.2),\n",
    "                    arrowprops=dict(arrowstyle='->', color='#e74c3c', lw=2),\n",
    "                    fontsize=7, ha='center', color='#e74c3c')\n",
    "\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(-1.5, len(samples[:max_samples]) * 2)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Next-State Signal Flow: Response ‚Üí Feedback', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_next_state_flow(all_samples)"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_13_jsonl",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Jsonl\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_13_jsonl.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. JSONL Logging ‚Äî Just Like OpenClaw-RL\n",
    "\n",
    "OpenClaw-RL logs all rollouts to JSONL files for analysis and debugging. Let us implement this:"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_to_jsonl(samples, filepath=\"rollout_log.jsonl\"):\n",
    "    \"\"\"Export training samples to JSONL format, matching OpenClaw-RL's logging.\"\"\"\n",
    "    records = []\n",
    "    for sample in samples:\n",
    "        record = {\n",
    "            \"session_id\": sample.session_id,\n",
    "            \"response_turn\": sample.response_turn_index,\n",
    "            \"feedback_turn\": sample.feedback_turn_index,\n",
    "            \"context\": sample.context,\n",
    "            \"response\": sample.response,\n",
    "            \"feedback\": sample.feedback,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }\n",
    "        records.append(record)\n",
    "\n",
    "    # Write to JSONL\n",
    "    with open(filepath, 'w') as f:\n",
    "        for record in records:\n",
    "            f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "    print(f\"üìù Logged {len(records)} training samples to {filepath}\")\n",
    "    return records\n",
    "\n",
    "# Export our samples\n",
    "records = samples_to_jsonl(all_samples)\n",
    "\n",
    "# Show one record\n",
    "print(\"\\nSample JSONL record:\")\n",
    "print(json.dumps(records[0], indent=2)[:500])"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_14_final_output",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Final Output\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_14_final_output.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üéØ Final Output: The At-Least-One Guarantee\n",
    "\n",
    "OpenClaw-RL guarantees that **every session contributes at least one training sample**. Let us verify this property and show our final system in action:"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the at-least-one guarantee\n",
    "def test_at_least_one_guarantee():\n",
    "    \"\"\"Verify that every processed session produces at least one sample.\"\"\"\n",
    "    test_collector = ConversationRolloutCollector()\n",
    "\n",
    "    # Even a minimal conversation should produce a sample\n",
    "    minimal_conversations = [\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Hi there!\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Hello! How can I help?\"},\n",
    "            {\"role\": \"user\", \"content\": \"Just testing.\"},\n",
    "        ],\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": \"You are helpful.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"4.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Correct!\"},\n",
    "        ],\n",
    "    ]\n",
    "\n",
    "    all_pass = True\n",
    "    for i, conv in enumerate(minimal_conversations):\n",
    "        samples = test_collector.process_conversation(conv, f\"minimal_{i}\")\n",
    "        passed = len(samples) >= 1\n",
    "        emoji = \"‚úÖ\" if passed else \"‚ùå\"\n",
    "        print(f\"  {emoji} Conversation {i}: {len(samples)} sample(s)\")\n",
    "        all_pass = all_pass and passed\n",
    "\n",
    "    return all_pass\n",
    "\n",
    "print(\"üîç Testing at-least-one guarantee:\")\n",
    "guarantee_holds = test_at_least_one_guarantee()\n",
    "\n",
    "if guarantee_holds:\n",
    "    print(\"\\n‚úÖ At-least-one guarantee holds for all test cases!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Guarantee violated ‚Äî some sessions produced no samples.\")"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Final summary visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "\n",
    "# Pipeline flow\n",
    "steps = [\n",
    "    (\"Raw\\nConversation\", \"#ecf0f1\"),\n",
    "    (\"Turn\\nClassification\", \"#3498db\"),\n",
    "    (\"Next-State\\nExtraction\", \"#2ecc71\"),\n",
    "    (\"Training\\nSamples\", \"#e74c3c\"),\n",
    "    (\"JSONL\\nLog\", \"#9b59b6\"),\n",
    "]\n",
    "\n",
    "for i, (label, color) in enumerate(steps):\n",
    "    x = i * 2.2\n",
    "    rect = mpatches.FancyBboxPatch((x, 0), 1.8, 1.2,\n",
    "                                     boxstyle=\"round,pad=0.1\",\n",
    "                                     facecolor=color, alpha=0.7)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x + 0.9, 0.6, label, ha='center', va='center',\n",
    "            fontsize=10, fontweight='bold', color='white' if color != '#ecf0f1' else 'black')\n",
    "\n",
    "    if i < len(steps) - 1:\n",
    "        ax.annotate('', xy=((i+1)*2.2, 0.6), xytext=(x + 1.8, 0.6),\n",
    "                    arrowprops=dict(arrowstyle='->', lw=2, color='#34495e'))\n",
    "\n",
    "ax.set_xlim(-0.3, len(steps) * 2.2)\n",
    "ax.set_ylim(-0.3, 1.7)\n",
    "ax.axis('off')\n",
    "ax.set_title('OpenClaw-RL Rollout Collection Pipeline ‚Äî Built from Scratch!',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "stats = collector.get_stats()\n",
    "print(f\"\\nüéâ Congratulations! You've built the rollout collection pipeline from scratch!\")\n",
    "print(f\"   Processed {stats['total_sessions']} sessions ‚Üí {stats['total_samples']} training samples\")\n",
    "print(f\"   Average: {stats['avg_samples_per_session']:.1f} samples per session\")\n",
    "print(f\"   Every conversation you have with your AI now generates training data! üöÄ\")"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_15_closing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Wrap-Up: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_15_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Reflection and Next Steps\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "1. What happens if a user asks a completely new question (changes topic) instead of providing feedback on the assistant's response? Should that be treated as a next-state signal?\n",
    "2. In OpenClaw-RL, the rollout collector runs **asynchronously** ‚Äî it processes conversations while the model continues serving. What challenges does this introduce?\n",
    "3. We used simple keyword matching for sentiment classification. In the full system, a PRM (Process Reward Model) handles this. What advantages would a learned model have over keywords?\n",
    "\n",
    "### üèÜ Optional Challenges\n",
    "1. **Handle tool calls**: Extend the turn classifier to handle conversations that include tool calls (e.g., the assistant calling a calculator or web search).\n",
    "2. **Sliding window context**: For very long conversations, implement a sliding window that keeps only the last N turns as context.\n",
    "3. **Session splitting**: Implement logic to split a single long conversation into multiple sessions when the user changes topic."
   ],
   "id": "cell_34"
  }
 ]
}