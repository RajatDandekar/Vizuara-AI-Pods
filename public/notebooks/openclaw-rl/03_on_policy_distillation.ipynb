{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "On-Policy Distillation with Hindsight Hints â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1_yOuaRupWcvvBB5tNnjVrtDllXqg6x4Q\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/03_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ On-Policy Distillation: Learning *What* to Improve with Hindsight Hints\n",
    "\n",
    "*Part 3 of the Vizuara series on OpenClaw-RL*\n",
    "*Estimated time: 55 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://pods.vizuara.ai/courses/openclaw-rl/practice/3/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_01_why_this_matters",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why This Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_01_why_this_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebook, we built Binary RL â€” a system that tells the model \"that was good\" or \"that was bad.\" But it never tells the model **what** to do differently.\n",
    "\n",
    "Imagine a music teacher. One teacher just says \"Wrong note!\" after every mistake. Another teacher says \"You played F# instead of F natural â€” flatten your finger on the fourth fret.\" Which teacher helps you improve faster?\n",
    "\n",
    "**On-Policy Distillation (OPD)** is the second teacher. Instead of reducing feedback to a scalar (+1 or -1), it extracts a **textual hint** from the user's correction and uses it to create rich, **token-level** training signals.\n",
    "\n",
    "By the end of this notebook, you will have implemented:\n",
    "- **Hindsight hint extraction** from user feedback\n",
    "- **Enhanced prompt construction** (original prompt + hint)\n",
    "- **Teacher-student log-probability comparison** at the token level\n",
    "- **Token-level advantage computation** ($A_t = \\log \\pi_{\\text{teacher}} - \\log \\pi_{\\text{student}}$)\n",
    "- A full OPD training loop with a side-by-side comparison against Binary RL"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ Teaser: Token-level advantages show EXACTLY which tokens need to change\n",
    "#\n",
    "# Token:     \"Here\"  \"is\"  \"a\"  \"JavaScript\"  \"sorting\"  \"function\"\n",
    "# Advantage:  +0.1   +0.1  +0.0    +3.8         +0.2       +0.1\n",
    "#                                    â†‘\n",
    "#                         This token should DEFINITELY change!\n",
    "#                      (Teacher with hint would say \"Python\" here)"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_02_building_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Building Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_02_building_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Let us think carefully about what information is lost when we reduce feedback to a scalar.\n",
    "\n",
    "Consider this conversation:\n",
    "\n",
    "**User:** \"Write me a function to sort a list in Python.\"\n",
    "\n",
    "**Assistant:** \"Here is a sorting function in JavaScript:\n",
    "```\n",
    "function sortArray(arr) { return arr.sort((a, b) => a - b); }\n",
    "```\"\n",
    "\n",
    "**User:** \"No, I said Python not JavaScript.\"\n",
    "\n",
    "With **Binary RL**, the system assigns reward = -1 to the entire response. Every token gets the same negative signal. The model learns: \"This whole response was bad.\" But *why* was it bad? Was it the function name? The logic? The formatting? Binary RL cannot say.\n",
    "\n",
    "With **OPD**, the system:\n",
    "1. Reads the user's correction: \"No, I said Python not JavaScript\"\n",
    "2. Extracts a **hindsight hint**: \"The user wants Python code, not JavaScript\"\n",
    "3. Feeds the original prompt + hint to the same model\n",
    "4. The model (now acting as a \"teacher\" because it has the hint) would generate Python code\n",
    "5. Compares the teacher's token probabilities to the student's token probabilities\n",
    "\n",
    "The result? A **token-level advantage map** that says: \"The tokens 'JavaScript', 'function', 'arr.sort', etc. should all change. But 'sorting' and 'function' are fine concepts.\"\n",
    "\n",
    "This is dramatically more informative than a single -1.\n",
    "\n",
    "### ðŸ¤” Think About This\n",
    "\n",
    "Here is a subtle point: the \"teacher\" in OPD is **the same model** as the student. The only difference is that the teacher sees the enhanced prompt (with the hint). Why is this important? Why not use a different, stronger model as the teacher?\n",
    "\n",
    "(Answer: using the same model ensures the token-level differences are meaningful â€” they reflect what *this specific model* would do differently with more information, not what a completely different model would do.)"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_03_mathematics",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Mathematics\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_03_mathematics.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### 3.1 Hindsight Hint Extraction\n",
    "\n",
    "Given a user feedback message $f$, a judge model extracts a short textual hint $h$:\n",
    "\n",
    "$$h = \\text{Judge}(f)$$\n",
    "\n",
    "For example:\n",
    "- $f$ = \"No, I said Python not JavaScript\" â†’ $h$ = \"Use Python instead of JavaScript\"\n",
    "- $f$ = \"That is too verbose, be more concise\" â†’ $h$ = \"Keep the response short and direct\"\n",
    "\n",
    "### 3.2 Enhanced Prompt\n",
    "\n",
    "The enhanced prompt concatenates the original prompt with the hint:\n",
    "\n",
    "$$s_{\\text{enhanced}} = s \\oplus h$$\n",
    "\n",
    "where $\\oplus$ denotes concatenation.\n",
    "\n",
    "### 3.3 Token-Level Advantage\n",
    "\n",
    "The advantage at token position $t$ is the log-probability gap between teacher and student:\n",
    "\n",
    "$$A_t = \\log \\pi_{\\text{teacher}}(a_t \\mid s_{\\text{enhanced}}) - \\log \\pi_{\\theta}(a_t \\mid s)$$\n",
    "\n",
    "Computationally: at each token, we compare how confident the teacher (with the hint) is versus the student (without the hint). A large positive $A_t$ means the teacher is much more confident â€” this token needs a big correction. A near-zero $A_t$ means both agree â€” no correction needed.\n",
    "\n",
    "Let us work through a concrete example. At token position $t$:\n",
    "- Teacher (with hint) assigns log-prob = $-0.5$ to \"Python\"\n",
    "- Student (without hint) assigns log-prob = $-2.3$ to \"Python\"\n",
    "- $A_t = -0.5 - (-2.3) = 1.8$ â†’ Strong signal: \"increase the probability of this token\"\n",
    "\n",
    "At another position:\n",
    "- Teacher assigns $-1.2$, Student assigns $-1.0$\n",
    "- $A_t = -1.2 - (-1.0) = -0.2$ â†’ Tiny signal: \"student was already fine here\"\n",
    "\n",
    "### 3.4 OPD Loss\n",
    "\n",
    "The OPD loss uses the same PPO-style clipped surrogate as Binary RL, but with **token-level advantages** instead of a single broadcasted scalar:\n",
    "\n",
    "$$J_{\\text{OPD}}(\\theta) = \\mathbb{E}\\left[\\min\\left(\\rho_t A_t,\\; \\text{clip}(\\rho_t, 1-\\epsilon, 1+\\epsilon) A_t\\right)\\right]$$\n",
    "\n",
    "The key difference from Binary RL: each token has its own advantage $A_t$, so the model receives directional guidance at every position."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_04_setup_and_imports",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Setup And Imports\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_04_setup_and_imports.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It â€” Component by Component\n",
    "\n",
    "### 4.1 Setup and Imports"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from typing import List, Tuple, Optional\n",
    "import re\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ… Using device: {device}\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_05_hint_extractor",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Hint Extractor\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_05_hint_extractor.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Hindsight Hint Extractor\n",
    "\n",
    "In the real system, hint extraction is done by a judge LLM. Here we build a rule-based version that captures the core idea:"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HindsightHintExtractor:\n",
    "    \"\"\"\n",
    "    Extracts actionable hints from user feedback.\n",
    "    In production, this is done by a judge LLM with majority voting.\n",
    "    Here we use pattern matching to demonstrate the concept.\n",
    "    \"\"\"\n",
    "\n",
    "    # Patterns that indicate specific corrections\n",
    "    CORRECTION_PATTERNS = [\n",
    "        (r\"(?:no|not)\\s*,?\\s*(?:I\\s+)?(?:said|asked|wanted|meant)\\s+(.+)\",\n",
    "         \"The user wants: {}\"),\n",
    "        (r\"(?:use|try|switch to)\\s+(.+?)(?:\\s+instead)?\",\n",
    "         \"Use {} instead\"),\n",
    "        (r\"(?:too|very)\\s+(verbose|long|short|brief|formal|casual)\",\n",
    "         \"Adjust tone: be less {}\"),\n",
    "        (r\"(?:should|must|need to)\\s+(.+)\",\n",
    "         \"Important requirement: {}\"),\n",
    "    ]\n",
    "\n",
    "    # Hints that are too trivial to be useful\n",
    "    TRIVIAL_HINTS = [\"ok\", \"fine\", \"sure\", \"yes\", \"no\", \"thanks\"]\n",
    "\n",
    "    def extract_hint(self, feedback: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Extract a hindsight hint from user feedback.\n",
    "\n",
    "        Args:\n",
    "            feedback: The user's response after the assistant's message\n",
    "\n",
    "        Returns:\n",
    "            A short textual hint, or None if feedback is not corrective\n",
    "        \"\"\"\n",
    "        feedback_lower = feedback.lower().strip()\n",
    "\n",
    "        # Filter out trivial feedback\n",
    "        if feedback_lower in self.TRIVIAL_HINTS:\n",
    "            return None\n",
    "\n",
    "        # Try each pattern\n",
    "        for pattern, template in self.CORRECTION_PATTERNS:\n",
    "            match = re.search(pattern, feedback_lower)\n",
    "            if match:\n",
    "                extracted = match.group(1).strip()\n",
    "                hint = template.format(extracted)\n",
    "                return hint\n",
    "\n",
    "        # If no pattern matches but feedback contains negative sentiment,\n",
    "        # use the feedback itself as the hint\n",
    "        negative_words = [\"wrong\", \"incorrect\", \"bad\", \"no\", \"not\", \"don't\"]\n",
    "        if any(word in feedback_lower for word in negative_words):\n",
    "            return f\"User correction: {feedback[:100]}\"\n",
    "\n",
    "        return None\n",
    "\n",
    "    def extract_with_voting(self, feedback: str, num_votes: int = 3) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Extract hints with majority voting (simulated).\n",
    "        In production, the judge LLM generates m hints and the\n",
    "        longest, most informative one is kept.\n",
    "\n",
    "        Args:\n",
    "            feedback: User feedback message\n",
    "            num_votes: Number of extraction attempts\n",
    "\n",
    "        Returns:\n",
    "            The best hint (longest non-trivial one), or None\n",
    "        \"\"\"\n",
    "        hints = []\n",
    "        for _ in range(num_votes):\n",
    "            hint = self.extract_hint(feedback)\n",
    "            if hint is not None:\n",
    "                hints.append(hint)\n",
    "\n",
    "        if not hints:\n",
    "            return None\n",
    "\n",
    "        # Keep the longest, most informative hint\n",
    "        return max(hints, key=len)\n",
    "\n",
    "# Test the hint extractor\n",
    "extractor = HindsightHintExtractor()\n",
    "\n",
    "test_feedbacks = [\n",
    "    \"No, I said Python not JavaScript.\",\n",
    "    \"Use Flask instead of Django.\",\n",
    "    \"That's too verbose, be more concise.\",\n",
    "    \"You should add error handling.\",\n",
    "    \"Perfect, thanks!\",\n",
    "    \"Wrong, the answer is 42.\",\n",
    "    \"Great job!\",\n",
    "]\n",
    "\n",
    "print(\"Hindsight Hint Extraction:\")\n",
    "for fb in test_feedbacks:\n",
    "    hint = extractor.extract_hint(fb)\n",
    "    emoji = \"ðŸ’¡\" if hint else \"âšª\"\n",
    "    print(f\"  {emoji} \\\"{fb}\\\"\")\n",
    "    print(f\"     â†’ Hint: {hint or '(no correction detected)'}\\n\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_06_enhanced_prompt_and_log_probs",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Enhanced Prompt And Log Probs\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_06_enhanced_prompt_and_log_probs.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Enhanced Prompt Construction"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedPromptBuilder:\n",
    "    \"\"\"\n",
    "    Constructs enhanced prompts by appending hindsight hints.\n",
    "    The enhanced prompt is what the model SHOULD have seen to get it right.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hint_prefix: str = \"\\n[HINT: \", hint_suffix: str = \"]\\n\"):\n",
    "        self.hint_prefix = hint_prefix\n",
    "        self.hint_suffix = hint_suffix\n",
    "\n",
    "    def build(self, original_prompt: str, hint: str) -> str:\n",
    "        \"\"\"\n",
    "        Build an enhanced prompt by appending the hint.\n",
    "\n",
    "        Args:\n",
    "            original_prompt: The original user prompt\n",
    "            hint: The extracted hindsight hint\n",
    "\n",
    "        Returns:\n",
    "            Enhanced prompt with hint appended\n",
    "        \"\"\"\n",
    "        return original_prompt + self.hint_prefix + hint + self.hint_suffix\n",
    "\n",
    "# Demonstrate\n",
    "builder = EnhancedPromptBuilder()\n",
    "\n",
    "original = \"Write me a function to sort a list.\"\n",
    "hint = \"The user wants: Python not JavaScript\"\n",
    "enhanced = builder.build(original, hint)\n",
    "\n",
    "print(\"Original prompt:\")\n",
    "print(f\"  \\\"{original}\\\"\\n\")\n",
    "print(\"Enhanced prompt (with hint):\")\n",
    "print(f\"  \\\"{enhanced}\\\"\")\n",
    "print(\"\\nThe teacher model sees this enhanced prompt and generates a better response!\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Token-Level Log-Probability Computation\n",
    "\n",
    "Now the core mechanic: computing log-probabilities for both the teacher and student models."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenLogProbComputer:\n",
    "    \"\"\"\n",
    "    Computes token-level log-probabilities for teacher and student.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size=50, hidden_size=32, max_seq_len=30):\n",
    "        \"\"\"Create a simple model for demonstration.\"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Simplified teacher/student as embedding + linear\n",
    "        # In reality, these are the SAME model with different inputs\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, vocab_size),\n",
    "        ).to(device)\n",
    "\n",
    "    def get_log_probs(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get log-probabilities for each token position.\n",
    "\n",
    "        Args:\n",
    "            input_ids: (batch, seq_len) â€” token IDs\n",
    "\n",
    "        Returns:\n",
    "            log_probs: (batch, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        logits = self.model(input_ids)\n",
    "        return F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    def get_token_log_probs(self, input_ids: torch.Tensor, target_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get log-probability of specific target tokens at each position.\n",
    "\n",
    "        Args:\n",
    "            input_ids: (batch, seq_len) â€” input context\n",
    "            target_ids: (batch, seq_len) â€” target tokens to score\n",
    "\n",
    "        Returns:\n",
    "            selected_log_probs: (batch, seq_len)\n",
    "        \"\"\"\n",
    "        all_log_probs = self.get_log_probs(input_ids)\n",
    "        # Gather the log-prob for each target token\n",
    "        selected = all_log_probs.gather(2, target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "        return selected\n",
    "\n",
    "# Create the model\n",
    "computer = TokenLogProbComputer()\n",
    "\n",
    "# Demo: compute log-probs for teacher vs student\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "# Same response tokens for both teacher and student\n",
    "response_ids = torch.randint(0, 50, (batch_size, seq_len)).to(device)\n",
    "\n",
    "# Student sees the original prompt\n",
    "student_input = torch.randint(0, 50, (batch_size, seq_len)).to(device)\n",
    "# Teacher sees the enhanced prompt (different tokens due to hint)\n",
    "teacher_input = torch.randint(0, 50, (batch_size, seq_len)).to(device)\n",
    "\n",
    "student_log_probs = computer.get_token_log_probs(student_input, response_ids)\n",
    "teacher_log_probs = computer.get_token_log_probs(teacher_input, response_ids)\n",
    "\n",
    "print(f\"Student log-probs shape: {student_log_probs.shape}\")\n",
    "print(f\"Teacher log-probs shape: {teacher_log_probs.shape}\")\n",
    "print(f\"\\nSample student log-probs: {student_log_probs[0, :5].detach().cpu().tolist()}\")\n",
    "print(f\"Sample teacher log-probs: {teacher_log_probs[0, :5].detach().cpu().tolist()}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_07_token_advantages_and_heatmap",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Token Advantages And Heatmap\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_07_token_advantages_and_heatmap.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Token-Level Advantage Computation\n",
    "\n",
    "This is the heart of OPD. At each token position, we compute how much the teacher (with hint) disagrees with the student (without hint):"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_token_advantages(\n",
    "    teacher_log_probs: torch.Tensor,\n",
    "    student_log_probs: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute token-level OPD advantages.\n",
    "\n",
    "    A_t = log Ï€_teacher(a_t | s + hint) - log Ï€_student(a_t | s)\n",
    "\n",
    "    Args:\n",
    "        teacher_log_probs: (batch, seq_len) â€” teacher's per-token log-probs\n",
    "        student_log_probs: (batch, seq_len) â€” student's per-token log-probs\n",
    "\n",
    "    Returns:\n",
    "        advantages: (batch, seq_len) â€” token-level advantages\n",
    "    \"\"\"\n",
    "    return teacher_log_probs - student_log_probs\n",
    "\n",
    "# Compute advantages\n",
    "advantages = compute_token_advantages(teacher_log_probs, student_log_probs)\n",
    "\n",
    "print(f\"Token-level advantages shape: {advantages.shape}\")\n",
    "print(f\"Sample advantages: {advantages[0].detach().cpu().numpy().round(3)}\")\n",
    "print(f\"\\nMean advantage: {advantages.mean().item():.4f}\")\n",
    "print(f\"Max advantage:  {advantages.max().item():.4f}\")\n",
    "print(f\"Min advantage:  {advantages.min().item():.4f}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Visualization: Token-Level Advantage Heatmap"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_token_advantages(advantages, tokens=None, title=\"Token-Level OPD Advantages\"):\n",
    "    \"\"\"Visualize token-level advantages as a heatmap.\"\"\"\n",
    "    adv_np = advantages.detach().cpu().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 3))\n",
    "\n",
    "    # Normalize colormap around zero\n",
    "    vmax = max(abs(adv_np.min()), abs(adv_np.max()))\n",
    "    norm = mcolors.TwoSlopeNorm(vmin=-vmax, vcenter=0, vmax=vmax)\n",
    "\n",
    "    im = ax.imshow(adv_np, cmap='RdYlGn', norm=norm, aspect='auto')\n",
    "    plt.colorbar(im, ax=ax, label='Advantage (green=increase, red=decrease)')\n",
    "\n",
    "    if tokens:\n",
    "        ax.set_xticks(range(len(tokens)))\n",
    "        ax.set_xticklabels(tokens, rotation=45, ha='right', fontsize=9)\n",
    "    else:\n",
    "        ax.set_xlabel('Token Position')\n",
    "\n",
    "    ax.set_ylabel('Batch')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a simulated meaningful example\n",
    "# Simulate a scenario where the teacher strongly disagrees at position 3 (\"JavaScript\")\n",
    "simulated_teacher_lp = torch.tensor([[-1.0, -0.8, -0.5, -0.5, -0.9, -0.7, -1.1, -0.6]])\n",
    "simulated_student_lp = torch.tensor([[-1.1, -0.9, -0.6, -2.3, -1.0, -0.8, -1.2, -0.7]])\n",
    "simulated_adv = compute_token_advantages(simulated_teacher_lp, simulated_student_lp)\n",
    "\n",
    "tokens = [\"Here\", \"is\", \"a\", \"Python\", \"sorting\", \"function\", \"that\", \"works\"]\n",
    "visualize_token_advantages(simulated_adv, tokens=tokens,\n",
    "                           title=\"OPD Advantage: Teacher Strongly Prefers 'Python' at Position 3\")\n",
    "\n",
    "print(\"Token-by-token breakdown:\")\n",
    "for i, (tok, adv) in enumerate(zip(tokens, simulated_adv[0].tolist())):\n",
    "    direction = \"â†‘ increase\" if adv > 0.1 else \"â†“ decrease\" if adv < -0.1 else \"â†’ keep\"\n",
    "    print(f\"  Token '{tok}': A_t = {adv:+.2f}  ({direction})\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_08_stop_and_think",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Stop And Think\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_08_stop_and_think.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ‹ Stop and Think\n",
    "\n",
    "Look at the advantage values above. Notice that:\n",
    "- Position 3 (\"Python\") has $A_t = 1.8$ â€” a very large positive advantage\n",
    "- Most other positions have small values near zero\n",
    "\n",
    "This is the power of OPD: it pinpoints **exactly which tokens** need to change. Binary RL would assign the same scalar to all 8 tokens. OPD gives each token its own gradient direction.\n",
    "\n",
    "*Take a moment to appreciate this before continuing.*"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_09_todo1_opd_loss",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo1 Opd Loss\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_09_todo1_opd_loss.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ðŸ”§ Your Turn\n",
    "\n",
    "### TODO 1: Implement the OPD Loss Function\n",
    "\n",
    "Combine token-level advantages with the clipped surrogate loss:"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opd_loss(\n",
    "    log_probs_new: torch.Tensor,     # (batch, seq_len) â€” current policy\n",
    "    log_probs_ref: torch.Tensor,     # (batch, seq_len) â€” reference policy\n",
    "    token_advantages: torch.Tensor,   # (batch, seq_len) â€” per-token OPD advantages\n",
    "    eps: float = 0.2,\n",
    "    mask: torch.Tensor = None,        # (batch, seq_len) â€” optional padding mask\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the OPD clipped surrogate loss.\n",
    "\n",
    "    Unlike Binary RL where advantages are per-response (scalar),\n",
    "    OPD advantages are per-token (vector).\n",
    "\n",
    "    Args:\n",
    "        log_probs_new: Log probs under current policy\n",
    "        log_probs_ref: Log probs under reference policy\n",
    "        token_advantages: Token-level advantages from teacher-student gap\n",
    "        eps: Clipping bound\n",
    "        mask: Binary mask (1 for real tokens, 0 for padding)\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss value\n",
    "\n",
    "    Steps:\n",
    "        1. Compute ratio: Ï_t = exp(log_new - log_ref)\n",
    "        2. Unclipped term: Ï_t * A_t  (element-wise, both are per-token)\n",
    "        3. Clipped term: clip(Ï_t) * A_t\n",
    "        4. Min of unclipped and clipped\n",
    "        5. Apply mask if provided\n",
    "        6. Return mean over all tokens\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Compute ratio\n",
    "    # Step 2: Unclipped objective\n",
    "    # Step 3: Clipped objective\n",
    "    # Step 4: Pessimistic bound (min)\n",
    "    # Step 5: Apply mask and average\n",
    "    # ==============================\n",
    "\n",
    "    loss = ???  # YOUR CODE HERE\n",
    "\n",
    "    return loss\n",
    "\n",
    "# âœ… Verification\n",
    "batch, seq = 4, 10\n",
    "log_new = torch.randn(batch, seq) * 0.1 - 2.0\n",
    "log_ref = torch.randn(batch, seq) * 0.1 - 2.0\n",
    "tok_advs = torch.randn(batch, seq) * 0.5  # Token-level advantages\n",
    "\n",
    "loss = opd_loss(log_new, log_ref, tok_advs)\n",
    "assert loss.dim() == 0, f\"âŒ Loss should be scalar, got shape {loss.shape}\"\n",
    "assert not torch.isnan(loss), \"âŒ Loss is NaN!\"\n",
    "print(f\"âœ… OPD loss computed successfully: {loss.item():.4f}\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_10_todo2_hint_filtering",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo2 Hint Filtering\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_10_todo2_hint_filtering.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement Hint Quality Filtering\n",
    "\n",
    "Not all hints are useful. Trivial hints like \"the response was wrong\" add no information beyond what Binary RL already provides. Implement a quality filter:"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_hints(hints: List[Optional[str]], min_length: int = 10) -> List[Optional[str]]:\n",
    "    \"\"\"\n",
    "    Filter hints by quality. Keep only informative hints.\n",
    "\n",
    "    Args:\n",
    "        hints: List of extracted hints (some may be None)\n",
    "        min_length: Minimum character length for a hint to be considered informative\n",
    "\n",
    "    Returns:\n",
    "        Filtered list where low-quality hints are replaced with None\n",
    "\n",
    "    Rules:\n",
    "        1. None hints stay None\n",
    "        2. Hints shorter than min_length characters â†’ None (too trivial)\n",
    "        3. Hints that are purely negative without direction â†’ None\n",
    "           (e.g., \"That was wrong\" has no actionable information)\n",
    "        4. All other hints are kept\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Filter each hint based on the rules above\n",
    "    # ==============================\n",
    "\n",
    "    filtered = ???  # YOUR CODE HERE\n",
    "\n",
    "    return filtered\n",
    "\n",
    "# âœ… Verification\n",
    "test_hints = [\n",
    "    \"The user wants: Python not JavaScript\",   # Good â€” specific correction\n",
    "    \"wrong\",                                     # Bad â€” too short, no direction\n",
    "    None,                                        # None â€” no hint extracted\n",
    "    \"Adjust tone: be less verbose and more concise when explaining code\", # Good\n",
    "    \"bad\",                                       # Bad â€” too short\n",
    "    \"Use Flask instead of Django for the web framework\",  # Good\n",
    "]\n",
    "\n",
    "filtered = filter_hints(test_hints)\n",
    "assert filtered[0] is not None, \"âŒ First hint should be kept\"\n",
    "assert filtered[1] is None, \"âŒ 'wrong' should be filtered out\"\n",
    "assert filtered[2] is None, \"âŒ None should stay None\"\n",
    "assert filtered[3] is not None, \"âŒ Long specific hint should be kept\"\n",
    "assert filtered[4] is None, \"âŒ 'bad' should be filtered out\"\n",
    "assert filtered[5] is not None, \"âŒ Specific framework hint should be kept\"\n",
    "print(\"âœ… Hint quality filtering works correctly!\")\n",
    "print(f\"   Kept {sum(1 for h in filtered if h is not None)}/{len(test_hints)} hints\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_11_full_pipeline",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Full Pipeline\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_11_full_pipeline.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together â€” The Full OPD Pipeline"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnPolicyDistillationPipeline:\n",
    "    \"\"\"\n",
    "    The complete OPD pipeline: from user feedback to token-level training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, vocab_size=50, hidden_size=32):\n",
    "        self.hint_extractor = HindsightHintExtractor()\n",
    "        self.prompt_builder = EnhancedPromptBuilder()\n",
    "        self.model = model\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def process_sample(self, prompt: str, response: str, feedback: str):\n",
    "        \"\"\"\n",
    "        Process a single (prompt, response, feedback) triple.\n",
    "\n",
    "        Returns:\n",
    "            dict with keys: hint, enhanced_prompt, token_advantages, or None if no hint\n",
    "        \"\"\"\n",
    "        # Step 1: Extract hint\n",
    "        hint = self.hint_extractor.extract_with_voting(feedback)\n",
    "        if hint is None:\n",
    "            return None\n",
    "\n",
    "        # Step 2: Build enhanced prompt\n",
    "        enhanced = self.prompt_builder.build(prompt, hint)\n",
    "\n",
    "        return {\n",
    "            \"hint\": hint,\n",
    "            \"original_prompt\": prompt,\n",
    "            \"enhanced_prompt\": enhanced,\n",
    "            \"response\": response,\n",
    "            \"feedback\": feedback,\n",
    "        }\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = OnPolicyDistillationPipeline(computer)\n",
    "\n",
    "# Process several examples\n",
    "examples = [\n",
    "    {\n",
    "        \"prompt\": \"Write a sorting function\",\n",
    "        \"response\": \"function sortArray(arr) { return arr.sort(); }\",\n",
    "        \"feedback\": \"No, I said Python not JavaScript.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain machine learning\",\n",
    "        \"response\": \"Machine learning is a comprehensive field that encompasses...(500 words)\",\n",
    "        \"feedback\": \"That's too verbose, be more concise.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is 2+2?\",\n",
    "        \"response\": \"2+2 = 4\",\n",
    "        \"feedback\": \"Thanks!\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Help me with my Flask app\",\n",
    "        \"response\": \"Here's how to do it with Django...\",\n",
    "        \"feedback\": \"Use Flask instead of Django.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"OPD Pipeline â€” Processing Examples:\\n\")\n",
    "for ex in examples:\n",
    "    result = pipeline.process_sample(**ex)\n",
    "    if result:\n",
    "        print(f\"  âœ… Prompt: \\\"{ex['prompt']}\\\"\")\n",
    "        print(f\"     Hint: \\\"{result['hint']}\\\"\")\n",
    "        print(f\"     Enhanced: \\\"{result['enhanced_prompt']}\\\"\\n\")\n",
    "    else:\n",
    "        print(f\"  âšª Prompt: \\\"{ex['prompt']}\\\"\")\n",
    "        print(f\"     Feedback: \\\"{ex['feedback']}\\\" â†’ No actionable hint\\n\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_12_training_comparison",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Training Comparison\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_12_training_comparison.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training: OPD vs Binary RL Comparison\n",
    "\n",
    "Let us compare both approaches on a synthetic task where OPD should shine:"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple teacher and student models\n",
    "class SimpleSeqModel(nn.Module):\n",
    "    \"\"\"A tiny sequence model for OPD demonstration.\"\"\"\n",
    "    def __init__(self, vocab_size=50, hidden=32):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden)\n",
    "        self.fc1 = nn.Linear(hidden, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(self.embed(x)))\n",
    "        return F.log_softmax(self.fc2(h), dim=-1)\n",
    "\n",
    "    def get_token_log_probs(self, input_ids, target_ids):\n",
    "        log_probs = self.forward(input_ids)\n",
    "        return log_probs.gather(2, target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "# Training comparison\n",
    "vocab_size = 50\n",
    "seq_len = 15\n",
    "\n",
    "student_model = SimpleSeqModel(vocab_size).to(device)\n",
    "ref_model = SimpleSeqModel(vocab_size).to(device)\n",
    "ref_model.load_state_dict(student_model.state_dict())\n",
    "\n",
    "# Binary RL baseline (separate copy)\n",
    "binary_model = SimpleSeqModel(vocab_size).to(device)\n",
    "binary_model.load_state_dict(student_model.state_dict())\n",
    "\n",
    "opt_opd = torch.optim.Adam(student_model.parameters(), lr=3e-4)\n",
    "opt_binary = torch.optim.Adam(binary_model.parameters(), lr=3e-4)\n",
    "\n",
    "# Generate synthetic data where OPD has an advantage\n",
    "# The \"correct\" tokens are known, so we can compute teacher log-probs\n",
    "target_pattern = torch.arange(seq_len).to(device) % vocab_size  # Repeating pattern\n",
    "\n",
    "opd_losses = []\n",
    "binary_losses = []\n",
    "opd_accuracies = []\n",
    "binary_accuracies = []\n",
    "\n",
    "num_steps = 150\n",
    "batch_size = 16\n",
    "\n",
    "print(\"Training OPD vs Binary RL...\\n\")\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Generate batch\n",
    "    inputs = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)\n",
    "    targets = target_pattern.unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "    # === OPD Training ===\n",
    "    student_lp = student_model.get_token_log_probs(inputs, targets)\n",
    "    with torch.no_grad():\n",
    "        ref_lp = ref_model.get_token_log_probs(inputs, targets)\n",
    "        # Teacher log-probs (simulated: teacher is more confident about correct tokens)\n",
    "        teacher_lp = ref_lp + torch.randn_like(ref_lp) * 0.1 + 0.5  # Teacher is better\n",
    "\n",
    "    token_advs = teacher_lp - student_lp.detach()\n",
    "    ratio = torch.exp(student_lp - ref_lp.detach())\n",
    "    unclipped = ratio * token_advs\n",
    "    clipped = torch.clamp(ratio, 0.8, 1.2) * token_advs\n",
    "    opd_obj = torch.min(unclipped, clipped).mean()\n",
    "\n",
    "    opt_opd.zero_grad()\n",
    "    (-opd_obj).backward()\n",
    "    torch.nn.utils.clip_grad_norm_(student_model.parameters(), 1.0)\n",
    "    opt_opd.step()\n",
    "    opd_losses.append(opd_obj.item())\n",
    "\n",
    "    # === Binary RL Training ===\n",
    "    binary_lp = binary_model.get_token_log_probs(inputs, targets)\n",
    "    with torch.no_grad():\n",
    "        ref_lp_b = ref_model.get_token_log_probs(inputs, targets)\n",
    "\n",
    "    # Binary rewards: +1 if most tokens match target pattern\n",
    "    preds = binary_model.forward(inputs).argmax(dim=-1)\n",
    "    match_rate = (preds == targets).float().mean(dim=1)\n",
    "    rewards = (match_rate > 0.3).float() * 2 - 1  # +1 if >30% match, else -1\n",
    "\n",
    "    # Broadcast scalar reward to all tokens\n",
    "    mean_r, std_r = rewards.mean(), rewards.std() + 1e-8\n",
    "    advs = ((rewards - mean_r) / std_r).unsqueeze(1).expand_as(binary_lp)\n",
    "\n",
    "    ratio_b = torch.exp(binary_lp - ref_lp_b.detach())\n",
    "    unclipped_b = ratio_b * advs\n",
    "    clipped_b = torch.clamp(ratio_b, 0.8, 1.2) * advs\n",
    "    binary_obj = torch.min(unclipped_b, clipped_b).mean()\n",
    "\n",
    "    opt_binary.zero_grad()\n",
    "    (-binary_obj).backward()\n",
    "    torch.nn.utils.clip_grad_norm_(binary_model.parameters(), 1.0)\n",
    "    opt_binary.step()\n",
    "    binary_losses.append(binary_obj.item())\n",
    "\n",
    "    # Compute accuracies\n",
    "    with torch.no_grad():\n",
    "        opd_preds = student_model.forward(inputs).argmax(dim=-1)\n",
    "        binary_preds = binary_model.forward(inputs).argmax(dim=-1)\n",
    "        opd_accuracies.append((opd_preds == targets).float().mean().item())\n",
    "        binary_accuracies.append((binary_preds == targets).float().mean().item())\n",
    "\n",
    "    if (step + 1) % 50 == 0:\n",
    "        print(f\"Step {step+1}: OPD acc={opd_accuracies[-1]:.3f}, \"\n",
    "              f\"Binary acc={binary_accuracies[-1]:.3f}\")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_13_visualization_comparison",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Visualization Comparison\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_13_visualization_comparison.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Visualization: OPD vs Binary RL"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy curves\n",
    "window = 5\n",
    "opd_smooth = np.convolve(opd_accuracies, np.ones(window)/window, mode='valid')\n",
    "binary_smooth = np.convolve(binary_accuracies, np.ones(window)/window, mode='valid')\n",
    "\n",
    "axes[0].plot(opd_smooth, linewidth=2.5, color='#2ecc71', label='OPD (token-level)')\n",
    "axes[0].plot(binary_smooth, linewidth=2.5, color='#3498db', label='Binary RL (scalar)')\n",
    "axes[0].set_xlabel('Training Step', fontsize=12)\n",
    "axes[0].set_ylabel('Token Accuracy', fontsize=12)\n",
    "axes[0].set_title('OPD vs Binary RL: Learning Speed', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Token-level advantage distribution at the end of training\n",
    "with torch.no_grad():\n",
    "    test_inputs = torch.randint(0, vocab_size, (32, seq_len)).to(device)\n",
    "    test_targets = target_pattern.unsqueeze(0).expand(32, -1)\n",
    "    final_student_lp = student_model.get_token_log_probs(test_inputs, test_targets)\n",
    "    final_teacher_lp = ref_model.get_token_log_probs(test_inputs, test_targets) + 0.5\n",
    "    final_advs = (final_teacher_lp - final_student_lp).cpu().numpy().flatten()\n",
    "\n",
    "axes[1].hist(final_advs, bins=40, color='#9b59b6', alpha=0.7, edgecolor='white')\n",
    "axes[1].axvline(x=0, color='#e74c3c', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Token-Level Advantage', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_title('Final Token Advantage Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final OPD accuracy:       {opd_accuracies[-1]:.3f}\")\n",
    "print(f\"Final Binary RL accuracy: {binary_accuracies[-1]:.3f}\")\n",
    "improvement = opd_accuracies[-1] - binary_accuracies[-1]\n",
    "print(f\"OPD advantage:            {improvement:+.3f}\")"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_14_final_output",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Final Output\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_14_final_output.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ðŸŽ¯ Final Output: The Complete OPD Pipeline Visualization"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Hint extraction success rate\n",
    "hint_results = {\"Extracted\": 0, \"No hint\": 0}\n",
    "test_fbs = [\n",
    "    \"No, use Python\", \"Great!\", \"Too verbose\", \"Thanks\",\n",
    "    \"Wrong framework\", \"Perfect\", \"Should add tests\", \"OK\",\n",
    "    \"Not what I asked for\", \"Looks good\",\n",
    "]\n",
    "for fb in test_fbs:\n",
    "    hint = extractor.extract_hint(fb)\n",
    "    if hint:\n",
    "        hint_results[\"Extracted\"] += 1\n",
    "    else:\n",
    "        hint_results[\"No hint\"] += 1\n",
    "\n",
    "axes[0, 0].bar(hint_results.keys(), hint_results.values(),\n",
    "               color=['#2ecc71', '#95a5a6'], edgecolor='white', linewidth=2)\n",
    "axes[0, 0].set_title('Hint Extraction Rate', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "# 2. Token advantage heatmap\n",
    "sample_advs = torch.tensor([[0.1, 0.0, -0.1, 1.8, 0.2, 0.1, -0.2, 0.0]])\n",
    "im = axes[0, 1].imshow(sample_advs.numpy(), cmap='RdYlGn', aspect='auto',\n",
    "                        vmin=-2, vmax=2)\n",
    "axes[0, 1].set_xticks(range(8))\n",
    "axes[0, 1].set_xticklabels([\"Here\", \"is\", \"a\", \"Python\", \"sort\", \"fn\", \"that\", \"works\"],\n",
    "                            fontsize=9)\n",
    "plt.colorbar(im, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Token-Level Advantages', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 3. OPD vs Binary learning curves\n",
    "axes[1, 0].plot(opd_smooth, linewidth=2, color='#2ecc71', label='OPD')\n",
    "axes[1, 0].plot(binary_smooth, linewidth=2, color='#3498db', label='Binary RL')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].set_title('Learning Speed Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Signal density comparison\n",
    "categories = ['Signal\\nGranularity', 'Information\\nDensity', 'Compute\\nCost']\n",
    "opd_vals = [5, 5, 4]      # Token-level, rich, higher\n",
    "binary_vals = [2, 2, 2]    # Scalar, coarse, lower\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.3\n",
    "axes[1, 1].bar(x - width/2, binary_vals, width, label='Binary RL',\n",
    "               color='#3498db', alpha=0.8)\n",
    "axes[1, 1].bar(x + width/2, opd_vals, width, label='OPD',\n",
    "               color='#2ecc71', alpha=0.8)\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(categories)\n",
    "axes[1, 1].set_ylabel('Score (1-5)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].set_title('Binary RL vs OPD Comparison', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('On-Policy Distillation: Complete Pipeline', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸŽ‰ Congratulations! You've built On-Policy Distillation from scratch!\")\n",
    "print(\"   âœ… Hindsight hint extraction from user feedback\")\n",
    "print(\"   âœ… Enhanced prompt construction\")\n",
    "print(\"   âœ… Teacher-student log-probability comparison\")\n",
    "print(\"   âœ… Token-level advantage computation\")\n",
    "print(\"   âœ… OPD loss function with clipped surrogate\")\n",
    "print(\"   âœ… Demonstrated OPD's advantage over Binary RL\")"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_15_reflection_and_next_steps",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Wrap-Up: Reflection And Next Steps\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_15_reflection_and_next_steps.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### ðŸ¤” Reflection Questions\n",
    "1. In OPD, the teacher and student are the **same model**. What would happen if we used a much larger model as the teacher? Would that be better or worse?\n",
    "2. The hint quality filter discards short hints. Can you think of a case where a very short hint is still highly informative? (e.g., \"Python\" as a hint)\n",
    "3. OPD requires computing log-probabilities twice (once for teacher, once for student). How could we reduce this compute cost?\n",
    "\n",
    "### ðŸ† Optional Challenges\n",
    "1. **Weighted token advantages**: Instead of raw $A_t = \\log \\pi_{\\text{teacher}} - \\log \\pi_{\\text{student}}$, implement a version where advantages are weighted by the teacher's confidence.\n",
    "2. **Adaptive hint selection**: Instead of always keeping the longest hint, implement a scoring function that balances hint length with specificity.\n",
    "3. **Mixed training**: Implement a training loop that alternates between Binary RL steps (for samples without hints) and OPD steps (for samples with hints)."
   ],
   "id": "cell_31"
  }
 ]
}