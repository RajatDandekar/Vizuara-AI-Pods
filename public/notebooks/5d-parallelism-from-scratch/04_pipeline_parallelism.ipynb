{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Pipeline Parallelism from Scratch â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1b5CC6G-W_RszA4l3kZ1pQHVSrFdbh7aI\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Pipeline Parallelism: The GPU Assembly Line\n",
    "\n",
    "*Part 4 of the Vizuara series on 5D Parallelism from Scratch*\n",
    "*Estimated time: 45 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://course-creator-brown.vercel.app/courses/5d-parallelism-from-scratch/practice/4/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"\\nðŸš€ Welcome to Pipeline Parallelism from Scratch!\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_why_it_matters",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebooks, we saw that **Data Parallelism** splits the training data across GPUs, and **Tensor Parallelism** splits individual weight matrices within a layer. But what happens when your model has so many layers that the total stack does not fit on a single GPU â€” even if each individual layer is small enough?\n",
    "\n",
    "This is where **Pipeline Parallelism** comes in. The idea is beautifully simple: split the model by **depth**. GPU 0 gets layers 1â€“8, GPU 1 gets layers 9â€“16, GPU 2 gets layers 17â€“24, and GPU 3 gets layers 25â€“32. Each GPU is a specialist â€” it only handles its assigned stage.\n",
    "\n",
    "Every major LLM training run (Llama 3, GPT-4, DeepSeek-V3) uses pipeline parallelism. By the end of this notebook, you will understand exactly how it works, why \"bubbles\" are the central challenge, and how clever scheduling (GPipe and 1F1B) keeps GPUs busy.\n",
    "\n",
    "**Here is a preview of what we will build** â€” an interactive pipeline schedule simulator:"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ Teaser: Preview of the final output\n",
    "# (We will build all of this from scratch below!)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
    "P, M = 4, 8\n",
    "\n",
    "for ax_idx, title in enumerate([\"GPipe\", \"1F1B\"]):\n",
    "    ax = axes[ax_idx]\n",
    "    grid = np.zeros((P, 2 * M + 2 * (P - 1)))\n",
    "    ax.imshow(grid, cmap='Greys', aspect='auto', vmin=0, vmax=1)\n",
    "    ax.set_title(f\"{title} Schedule (P={P}, M={M})\", fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel(\"Stage (GPU)\")\n",
    "    ax.set_xlabel(\"Time Step\")\n",
    "    ax.set_yticks(range(P))\n",
    "    ax.set_yticklabels([f\"GPU {i}\" for i in range(P)])\n",
    "\n",
    "fig.suptitle(\"Pipeline Schedules â€” We Will Build These!\", fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_car_factory",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Car Factory\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_car_factory.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition â€” The Car Factory\n",
    "\n",
    "Before we write a single line of code, let us build a mental model.\n",
    "\n",
    "**Imagine a car factory with four stations:**\n",
    "\n",
    "| Station | Task | Specialist |\n",
    "|---------|------|-----------|\n",
    "| Station 1 | Build the chassis | GPU 0 |\n",
    "| Station 2 | Add the engine | GPU 1 |\n",
    "| Station 3 | Paint the body | GPU 2 |\n",
    "| Station 4 | Install the interior | GPU 3 |\n",
    "\n",
    "Each station handles one stage of the assembly. A car starts at Station 1, moves to Station 2, then Station 3, and finally Station 4.\n",
    "\n",
    "**The key insight:** As soon as Station 1 finishes the chassis for Car #1, it can immediately start on Car #2 â€” it does not need to wait for Car #1 to be fully assembled. Meanwhile, Station 2 begins working on Car #1's engine. This overlap is the whole point of a pipeline.\n",
    "\n",
    "**But here is the catch.** When only *one* car is being built:\n",
    "- Station 1 works. Stations 2, 3, 4 sit idle.\n",
    "- Station 2 works. Stations 1, 3, 4 sit idle.\n",
    "- Station 3 works. Stations 1, 2, 4 sit idle.\n",
    "- Station 4 works. Stations 1, 2, 3 sit idle.\n",
    "\n",
    "Three out of four stations are always idle. That is **75% wasted time**. This idle time is called the **pipeline bubble**."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_think_about_this",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Think About This\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_think_about_this.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤” Think About This\n",
    "\n",
    "Before moving on, try to answer these questions in your head:\n",
    "\n",
    "1. If you have 4 stations and send **100 cars** through the factory, roughly what fraction of time is wasted on idle stations?\n",
    "2. What if you only send **2 cars**?\n",
    "3. Is there a way to completely eliminate the bubble?\n",
    "\n",
    "*Take a minute. Then scroll down.*"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us check our intuition with quick calculations\n",
    "print(\"Car Factory Bubble Check:\")\n",
    "print(f\"  100 cars, 4 stations: bubble = {3/103*100:.1f}%  (tiny!)\")\n",
    "print(f\"    2 cars, 4 stations: bubble = {3/5*100:.1f}%  (bad!)\")\n",
    "print(f\"    1 car,  4 stations: bubble = {3/4*100:.1f}% (terrible!)\")\n",
    "print()\n",
    "print(\"ðŸ’¡ You can never fully eliminate the bubble (as long as P > 1),\")\n",
    "print(\"   but you can make it negligibly small with many micro-batches.\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_mathematics_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Mathematics Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_mathematics_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "Now let us formalize this intuition.\n",
    "\n",
    "### 3.1 The Bubble Fraction\n",
    "\n",
    "With $P$ pipeline stages and $M$ micro-batches, the pipeline bubble fraction is:\n",
    "\n",
    "$$\\text{Bubble fraction} = \\frac{P - 1}{P - 1 + M}$$\n",
    "\n",
    "This equation says: the \"startup cost\" is $P - 1$ time steps (waiting for the pipeline to fill up), and the total useful work takes $P - 1 + M$ time steps. The fraction of time wasted is the startup cost divided by the total."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us plug in some simple numbers to build intuition\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'P (stages)':<12} {'M (micro-batches)':<20} {'Bubble %':<12} {'Efficiency %'}\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "examples = [\n",
    "    (4, 1),    # Worst case: single micro-batch\n",
    "    (4, 4),    # Equal to number of stages\n",
    "    (4, 8),    # Moderate\n",
    "    (4, 12),   # Good\n",
    "    (4, 32),   # Excellent\n",
    "    (8, 1),    # 8 stages, 1 micro-batch\n",
    "    (8, 32),   # 8 stages, many micro-batches\n",
    "    (16, 64),  # Large-scale\n",
    "]\n",
    "\n",
    "for P, M in examples:\n",
    "    bubble = (P - 1) / (P - 1 + M)\n",
    "    efficiency = 1 - bubble\n",
    "    print(f\"{P:<12} {M:<20} {bubble*100:<12.1f} {efficiency*100:.1f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key takeaway: More micro-batches â†’ smaller bubble!\")\n",
    "print(\"   The bubble becomes negligible when M >> P.\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_formula_trace",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Formula Trace\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_formula_trace.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Understanding the Formula Computationally\n",
    "\n",
    "Let us trace through exactly what happens with $P = 4$ stages and $M = 1$ micro-batch:\n",
    "\n",
    "| Time step | GPU 0 | GPU 1 | GPU 2 | GPU 3 |\n",
    "|-----------|-------|-------|-------|-------|\n",
    "| t=0       | F(1)  | idle  | idle  | idle  |\n",
    "| t=1       | idle  | F(1)  | idle  | idle  |\n",
    "| t=2       | idle  | idle  | F(1)  | idle  |\n",
    "| t=3       | idle  | idle  | idle  | F(1)  |\n",
    "| t=4       | idle  | idle  | idle  | B(1)  |\n",
    "| t=5       | idle  | idle  | B(1)  | idle  |\n",
    "| t=6       | idle  | B(1)  | idle  | idle  |\n",
    "| t=7       | B(1)  | idle  | idle  | idle  |\n",
    "\n",
    "Total cells: $4 \\times 8 = 32$. Active cells: $8$ (4 forward + 4 backward). Idle cells: $24$. Bubble = $24/32 = 75\\%$.\n",
    "\n",
    "Now with $M = 4$ micro-batches, many of those idle cells get filled by the next micro-batch's forward or backward pass."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_memory_analysis",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Memory Analysis\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_memory_analysis.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Memory Analysis: GPipe vs 1F1B\n",
    "\n",
    "There are two major pipeline schedules, and they differ dramatically in **peak memory**:\n",
    "\n",
    "**GPipe** (all forwards first, then all backwards):\n",
    "$$\\text{Peak memory} = M \\times A$$\n",
    "where $A$ is the activation memory per micro-batch. GPipe must store activations for ALL $M$ micro-batches simultaneously, because no backward pass runs until all forwards are complete.\n",
    "\n",
    "**1F1B** (interleave one forward, one backward):\n",
    "$$\\text{Peak memory} = P \\times A$$\n",
    "Since backward passes start as soon as possible, activations are released early. In steady state, only $P$ micro-batches worth of activations are stored at once."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us see the memory difference concretely\n",
    "\n",
    "print(\"Memory Comparison: GPipe vs 1F1B\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'P':<6} {'M':<6} {'GPipe (Ã—A)':<15} {'1F1B (Ã—A)':<15} {'Savings'}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "scenarios = [\n",
    "    (4, 4), (4, 8), (4, 16),\n",
    "    (4, 32), (8, 32), (16, 64),\n",
    "]\n",
    "\n",
    "for P, M in scenarios:\n",
    "    gpipe_mem = M\n",
    "    onef1b_mem = P\n",
    "    savings = (1 - onef1b_mem / gpipe_mem) * 100\n",
    "    print(f\"{P:<6} {M:<6} {gpipe_mem:<15} {onef1b_mem:<15} {savings:.0f}% less\")\n",
    "\n",
    "print(\"\\nðŸ’¡ 1F1B has the SAME bubble fraction as GPipe,\")\n",
    "print(\"   but MUCH lower peak memory!\")\n",
    "print(\"   This is why 1F1B is used in practice (Megatron-LM, DeepSpeed).\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_07_build_stages",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_build_stages.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let Us Build It â€” Component by Component\n",
    "\n",
    "Time to get our hands dirty. We will build a pipeline parallelism simulator step by step.\n",
    "\n",
    "### 4.1 Defining a Simple Model and Pipeline Stages\n",
    "\n",
    "Let us create a small model (8 linear layers) and split it into 4 stages of 2 layers each."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLayer(nn.Module):\n",
    "    \"\"\"A single linear layer with ReLU â€” represents one Transformer layer.\"\"\"\n",
    "    def __init__(self, dim, layer_id):\n",
    "        super().__init__()\n",
    "        self.layer_id = layer_id\n",
    "        self.linear = nn.Linear(dim, dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.linear(x))\n",
    "\n",
    "\n",
    "class PipelineStage(nn.Module):\n",
    "    \"\"\"A pipeline stage: consecutive layers assigned to one GPU.\"\"\"\n",
    "    def __init__(self, layers, stage_id):\n",
    "        super().__init__()\n",
    "        self.stage_id = stage_id\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an 8-layer model, split into 4 stages of 2 layers each\n",
    "DIM = 64\n",
    "NUM_LAYERS = 8\n",
    "NUM_STAGES = 4\n",
    "LAYERS_PER_STAGE = NUM_LAYERS // NUM_STAGES\n",
    "\n",
    "all_layers = [SimpleLayer(DIM, i) for i in range(NUM_LAYERS)]\n",
    "\n",
    "stages = []\n",
    "for s in range(NUM_STAGES):\n",
    "    start = s * LAYERS_PER_STAGE\n",
    "    end = start + LAYERS_PER_STAGE\n",
    "    stage = PipelineStage(all_layers[start:end], stage_id=s)\n",
    "    stages.append(stage)\n",
    "\n",
    "print(\"Pipeline Stages:\")\n",
    "for s, stage in enumerate(stages):\n",
    "    layer_ids = [l.layer_id for l in stage.layers]\n",
    "    params = sum(p.numel() for p in stage.parameters())\n",
    "    print(f\"  Stage {s} (GPU {s}): Layers {layer_ids}, Params: {params:,}\")\n",
    "\n",
    "total = sum(p.numel() for s in stages for p in s.parameters())\n",
    "print(f\"\\nTotal parameters: {total:,}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_08_data_flow",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_data_flow.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Running Data Through the Pipeline\n",
    "\n",
    "Let us see how activations flow from one stage to the next â€” this is the core of pipeline parallelism."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample input and pass it through the pipeline\n",
    "batch_size = 4\n",
    "x = torch.randn(batch_size, DIM)\n",
    "\n",
    "print(\"Forward pass through the pipeline:\")\n",
    "print(f\"  Input shape: {x.shape}\")\n",
    "print(f\"  Input mean: {x.mean().item():.4f}\\n\")\n",
    "\n",
    "activation = x\n",
    "for stage in stages:\n",
    "    activation = stage(activation)\n",
    "    print(f\"  After Stage {stage.stage_id}: shape={activation.shape}, \"\n",
    "          f\"mean={activation.mean().item():.4f}\")\n",
    "\n",
    "print(f\"\\nâœ… Activations flow: Stage 0 â†’ Stage 1 â†’ Stage 2 â†’ Stage 3\")\n",
    "print(f\"   In real PP, each arrow is a GPU-to-GPU data transfer.\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_09_naive_schedule",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_naive_schedule.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The Naive Schedule (No Pipelining)\n",
    "\n",
    "Before we build clever schedules, let us implement the **worst case**: process one micro-batch all the way through before starting the next. This makes the bubble problem painfully visible."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_naive_schedule(P, M):\n",
    "    \"\"\"\n",
    "    Naive pipeline schedule: each micro-batch completes fully\n",
    "    (all forwards + all backwards) before the next starts.\n",
    "\n",
    "    Returns: (schedule grid, total time steps)\n",
    "    Values: positive = forward(mb), negative = backward(mb), 0 = idle\n",
    "    \"\"\"\n",
    "    total_time = M * 2 * P\n",
    "    schedule = np.zeros((P, total_time), dtype=int)\n",
    "\n",
    "    for m in range(M):\n",
    "        base_time = m * 2 * P\n",
    "        # Forward pass: staircase down\n",
    "        for s in range(P):\n",
    "            schedule[s][base_time + s] = m + 1\n",
    "        # Backward pass: staircase up\n",
    "        for s in range(P):\n",
    "            schedule[P - 1 - s][base_time + P + s] = -(m + 1)\n",
    "\n",
    "    return schedule, total_time"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display the naive schedule\n",
    "P, M = 4, 4\n",
    "naive_schedule, naive_time = generate_naive_schedule(P, M)\n",
    "\n",
    "total_cells = P * naive_time\n",
    "active_cells = np.count_nonzero(naive_schedule)\n",
    "idle_cells = total_cells - active_cells\n",
    "bubble_pct = idle_cells / total_cells * 100\n",
    "\n",
    "print(f\"Naive Schedule: P={P} stages, M={M} micro-batches\")\n",
    "print(f\"  Total time steps: {naive_time}\")\n",
    "print(f\"  Total cells: {total_cells}\")\n",
    "print(f\"  Active cells: {active_cells}\")\n",
    "print(f\"  Idle cells: {idle_cells}\")\n",
    "print(f\"  Bubble: {bubble_pct:.1f}%\")\n",
    "print(f\"\\nâš ï¸ That is {bubble_pct:.0f}% wasted GPU time â€” terrible!\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_10_naive_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_naive_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“Š **Visualization Checkpoint 1**: Let us see the naive schedule as a colored grid. First, we need a reusable plotting function."
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cell_color_and_label(val, M):\n",
    "    \"\"\"Return (color, label) for a schedule cell value.\"\"\"\n",
    "    if val > 0:\n",
    "        return (0.2, 0.4, 0.4 + 0.5*(val/M)), f\"F{val}\"\n",
    "    elif val < 0:\n",
    "        return (0.4 + 0.5*(abs(val)/M), 0.5, 0.2), f\"B{abs(val)}\"\n",
    "    return (0.85, 0.85, 0.85), \"\"\n",
    "\n",
    "\n",
    "def plot_schedule(schedule, total_time, P, M,\n",
    "                  title=\"Pipeline Schedule\", ax=None, show_legend=True):\n",
    "    \"\"\"Plot a pipeline schedule as a colored grid.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(max(14, total_time*0.5), 3))\n",
    "\n",
    "    for s in range(P):\n",
    "        for t in range(total_time):\n",
    "            color, label = _cell_color_and_label(schedule[s][t], M)\n",
    "            rect = plt.Rectangle((t, s), 1, 1, facecolor=color,\n",
    "                                 edgecolor='white', linewidth=1.5)\n",
    "            ax.add_patch(rect)\n",
    "            if label:\n",
    "                ax.text(t+0.5, s+0.5, label, ha='center', va='center',\n",
    "                        fontsize=7, fontweight='bold', color='white')\n",
    "\n",
    "    _format_schedule_axes(ax, total_time, P, title, show_legend)\n",
    "    return ax"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_schedule_axes(ax, total_time, P, title, show_legend):\n",
    "    \"\"\"Helper to format axes for schedule plots.\"\"\"\n",
    "    ax.set_xlim(0, total_time)\n",
    "    ax.set_ylim(0, P)\n",
    "    ax.set_yticks([i + 0.5 for i in range(P)])\n",
    "    ax.set_yticklabels([f\"GPU {i}\" for i in range(P)])\n",
    "    ax.set_xlabel(\"Time Step\", fontsize=11)\n",
    "    ax.set_ylabel(\"Stage\", fontsize=11)\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    if show_legend:\n",
    "        fwd = mpatches.Patch(color=(0.2, 0.4, 0.7), label='Forward')\n",
    "        bwd = mpatches.Patch(color=(0.7, 0.5, 0.2), label='Backward')\n",
    "        idle = mpatches.Patch(color=(0.85, 0.85, 0.85), label='Idle (Bubble)')\n",
    "        ax.legend(handles=[fwd, bwd, idle], loc='upper right', fontsize=8)"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualize the naive schedule\n",
    "fig, ax = plt.subplots(figsize=(16, 3.5))\n",
    "plot_schedule(naive_schedule, naive_time, P, M,\n",
    "              title=f\"Naive Schedule (P={P}, M={M}) â€” {bubble_pct:.0f}% Bubble!\",\n",
    "              ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each micro-batch must fully complete before the next one starts.\")\n",
    "print(\"Most GPUs are idle most of the time. We can do MUCH better.\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_11_gpipe_schedule",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_gpipe_schedule.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 GPipe Schedule\n",
    "\n",
    "**GPipe** was the first practical pipeline schedule. The idea: split the mini-batch into $M$ micro-batches, run ALL forward passes first (in a staggered fashion), then run ALL backward passes.\n",
    "\n",
    "This fills in many of the idle cells from the naive schedule. The forward passes cascade down the stages like a waterfall, and then the backward passes cascade back up."
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpipe_schedule(P, M):\n",
    "    \"\"\"\n",
    "    GPipe schedule: all forwards first, then all backwards.\n",
    "    Micro-batch m enters stage s at time m + s (forward phase).\n",
    "    \"\"\"\n",
    "    fwd_end = M + P - 2\n",
    "    total_time = 2 * (M + P - 1)\n",
    "    schedule = np.zeros((P, total_time), dtype=int)\n",
    "\n",
    "    # Forward passes: micro-batch m enters stage s at time m + s\n",
    "    for m in range(M):\n",
    "        for s in range(P):\n",
    "            schedule[s][m + s] = m + 1\n",
    "\n",
    "    # Backward passes: run in reverse after all forwards\n",
    "    bwd_start = fwd_end + 1\n",
    "    for m in range(M):\n",
    "        for s in range(P):\n",
    "            t = bwd_start + (M - 1 - m) + (P - 1 - s)\n",
    "            schedule[s][t] = -(m + 1)\n",
    "\n",
    "    return schedule, total_time"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and analyze the GPipe schedule\n",
    "P, M = 4, 8\n",
    "gpipe_schedule, gpipe_time = generate_gpipe_schedule(P, M)\n",
    "\n",
    "total_cells = P * gpipe_time\n",
    "active_cells = np.count_nonzero(gpipe_schedule)\n",
    "idle_cells = total_cells - active_cells\n",
    "gpipe_bubble_pct = idle_cells / total_cells * 100\n",
    "theoretical_bubble = (P - 1) / (P - 1 + M) * 100\n",
    "\n",
    "print(f\"GPipe Schedule: P={P} stages, M={M} micro-batches\")\n",
    "print(f\"  Total time steps: {gpipe_time}\")\n",
    "print(f\"  Active cells: {active_cells} (expected: {2*P*M})\")\n",
    "print(f\"  Bubble: {gpipe_bubble_pct:.1f}%\")\n",
    "print(f\"  Theoretical bubble fraction: {theoretical_bubble:.1f}%\")\n",
    "print(f\"\\nðŸ’¡ Much better than the naive schedule!\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_12_gpipe_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_gpipe_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“Š **Visualization Checkpoint 2**: The GPipe schedule grid."
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualize the GPipe schedule\n",
    "fig, ax = plt.subplots(figsize=(18, 4))\n",
    "plot_schedule(gpipe_schedule, gpipe_time, P, M,\n",
    "              title=f\"GPipe Schedule (P={P}, M={M}) â€” Bubble: {gpipe_bubble_pct:.1f}%\",\n",
    "              ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Notice the pattern:\")\n",
    "print(\"  - All forward passes (blue) run first in a staircase\")\n",
    "print(\"  - Then all backward passes (orange) in a reverse staircase\")\n",
    "print(\"  - Gray cells in the corners are the bubble\")\n",
    "print(f\"\\nâš ï¸ Peak memory: GPipe stores activations for ALL {M} micro-batches = {M}Ã—A\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_13_1f1b_schedule",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_1f1b_schedule.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 The 1F1B Schedule\n",
    "\n",
    "**1F1B (One-Forward-One-Backward)** is the schedule used by Megatron-LM and most production systems. The idea is to **interleave** forward and backward passes so that activations can be released as soon as their corresponding backward pass completes.\n",
    "\n",
    "The schedule has three phases:\n",
    "\n",
    "1. **Warmup phase**: Fill the pipeline with forward passes (each stage does progressively fewer warmup forwards)\n",
    "2. **Steady state**: Alternate one forward and one backward pass\n",
    "3. **Cooldown phase**: Drain remaining backward passes"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_1f1b_schedule(P, M):\n",
    "    \"\"\"1F1B: warmup forwards, then alternate 1F+1B, then drain backwards.\"\"\"\n",
    "    total_time = 2 * (M + P - 1)\n",
    "    schedule = np.zeros((P, total_time), dtype=int)\n",
    "\n",
    "    for s in range(P):\n",
    "        num_warmup = P - 1 - s\n",
    "        t, fwd_idx, bwd_idx = s, 0, 0\n",
    "\n",
    "        for _ in range(num_warmup):  # Phase 1: warmup\n",
    "            if fwd_idx < M:\n",
    "                schedule[s][t] = fwd_idx + 1; fwd_idx += 1; t += 1\n",
    "\n",
    "        while fwd_idx < M:  # Phase 2: steady state (1F, 1B)\n",
    "            schedule[s][t] = fwd_idx + 1; fwd_idx += 1; t += 1\n",
    "            if bwd_idx < M:\n",
    "                schedule[s][t] = -(bwd_idx + 1); bwd_idx += 1; t += 1\n",
    "\n",
    "        while bwd_idx < M:  # Phase 3: cooldown\n",
    "            schedule[s][t] = -(bwd_idx + 1); bwd_idx += 1; t += 1\n",
    "\n",
    "    return schedule, total_time"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and analyze the 1F1B schedule\n",
    "P, M = 4, 8\n",
    "f1b_schedule, f1b_time = generate_1f1b_schedule(P, M)\n",
    "\n",
    "total_cells = P * f1b_time\n",
    "active_cells = np.count_nonzero(f1b_schedule)\n",
    "idle_cells = total_cells - active_cells\n",
    "f1b_bubble_pct = idle_cells / total_cells * 100\n",
    "\n",
    "print(f\"1F1B Schedule: P={P} stages, M={M} micro-batches\")\n",
    "print(f\"  Total time steps: {f1b_time}\")\n",
    "print(f\"  Active cells: {active_cells} (expected: {2*P*M})\")\n",
    "print(f\"  Bubble: {f1b_bubble_pct:.1f}%\")\n",
    "print(f\"\\nðŸ’¡ Same bubble fraction as GPipe ({gpipe_bubble_pct:.1f}%)\")\n",
    "print(f\"   But peak memory: 1F1B stores only P={P} activations vs GPipe's M={M}\")"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_14_side_by_side",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_side_by_side.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“Š **Visualization Checkpoint 3**: Side-by-side comparison of GPipe and 1F1B."
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Side-by-side comparison: GPipe vs 1F1B\n",
    "P, M = 4, 8\n",
    "gpipe_sched, gpipe_t = generate_gpipe_schedule(P, M)\n",
    "f1b_sched, f1b_t = generate_1f1b_schedule(P, M)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(18, 7))\n",
    "\n",
    "plot_schedule(gpipe_sched, gpipe_t, P, M,\n",
    "              title=f\"GPipe (P={P}, M={M}) â€” Peak Memory: {M}Ã—A\",\n",
    "              ax=axes[0], show_legend=True)\n",
    "\n",
    "plot_schedule(f1b_sched, f1b_t, P, M,\n",
    "              title=f\"1F1B (P={P}, M={M}) â€” Peak Memory: {P}Ã—A\",\n",
    "              ax=axes[1], show_legend=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observations:\")\n",
    "print(f\"  âœ… Both have the same bubble fraction: {(P-1)/(P-1+M)*100:.1f}%\")\n",
    "print(f\"  âœ… 1F1B peak memory: {P}Ã—A  vs  GPipe: {M}Ã—A\")\n",
    "print(f\"  âœ… Memory savings: {(1 - P/M)*100:.0f}% less with 1F1B!\")"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_15_memory_deep_dive",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Memory Deep Dive\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/15_memory_deep_dive.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Understanding Peak Memory â€” A Deeper Look\n",
    "\n",
    "Why exactly does 1F1B use less memory? Let us trace the activation storage over time for each schedule."
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_memory_profile(schedule, total_time, P, M):\n",
    "    \"\"\"\n",
    "    Track stored activations over time.\n",
    "    Forward pass creates +1 activation; backward pass releases -1.\n",
    "    \"\"\"\n",
    "    memory_timeline = []\n",
    "    stored = [0] * P\n",
    "\n",
    "    for t in range(total_time):\n",
    "        for s in range(P):\n",
    "            val = schedule[s][t]\n",
    "            if val > 0:\n",
    "                stored[s] += 1\n",
    "            elif val < 0:\n",
    "                stored[s] = max(0, stored[s] - 1)\n",
    "        memory_timeline.append(sum(stored))\n",
    "\n",
    "    return memory_timeline"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot memory profiles\n",
    "P, M = 4, 8\n",
    "gpipe_sched, gpipe_t = generate_gpipe_schedule(P, M)\n",
    "f1b_sched, f1b_t = generate_1f1b_schedule(P, M)\n",
    "\n",
    "gpipe_memory = compute_memory_profile(gpipe_sched, gpipe_t, P, M)\n",
    "f1b_memory = compute_memory_profile(f1b_sched, f1b_t, P, M)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "ax.plot(range(gpipe_t), gpipe_memory, 'b-o', markersize=4,\n",
    "        label=f'GPipe (peak={max(gpipe_memory)}Ã—A)', linewidth=2)\n",
    "ax.plot(range(f1b_t), f1b_memory, 'r-s', markersize=4,\n",
    "        label=f'1F1B (peak={max(f1b_memory)}Ã—A)', linewidth=2)\n",
    "\n",
    "ax.axhline(y=M, color='blue', linestyle='--', alpha=0.5,\n",
    "           label=f'M={M} (GPipe theoretical)')\n",
    "ax.axhline(y=P, color='red', linestyle='--', alpha=0.5,\n",
    "           label=f'P={P} (1F1B theoretical)')\n",
    "\n",
    "ax.set_xlabel(\"Time Step\", fontsize=12)\n",
    "ax.set_ylabel(\"Stored Activations (Ã—A)\", fontsize=12)\n",
    "ax.set_title(\"Peak Memory: GPipe vs 1F1B\", fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ðŸ“Š GPipe peak memory:  {max(gpipe_memory)} activations\")\n",
    "print(f\"ðŸ“Š 1F1B peak memory:   {max(f1b_memory)} activations\")\n",
    "savings_pct = (1 - max(f1b_memory)/max(gpipe_memory))*100\n",
    "print(f\"ðŸ“Š Memory savings:     {savings_pct:.0f}%\")\n",
    "print(f\"\\nðŸ’¡ In GPipe, memory ramps up during the forward phase because\")\n",
    "print(f\"   ALL activations are stored before any backward pass runs.\")\n",
    "print(f\"   In 1F1B, backwards start early, releasing memory sooner.\")"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_16_todo1",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/16_todo1.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ðŸ”§ Your Turn\n",
    "\n",
    "Now it is time for you to implement some key components yourself.\n",
    "\n",
    "### ðŸ”§ TODO 1: Implement the Bubble Fraction Calculator\n",
    "\n",
    "Given the number of pipeline stages $P$ and micro-batches $M$, compute the bubble fraction and the pipeline efficiency."
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bubble_fraction(P, M):\n",
    "    \"\"\"\n",
    "    Compute the pipeline bubble fraction and efficiency.\n",
    "\n",
    "    Args:\n",
    "        P: Number of pipeline stages\n",
    "        M: Number of micro-batches\n",
    "\n",
    "    Returns:\n",
    "        tuple: (bubble_fraction, efficiency)\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Implement the bubble fraction formula:\n",
    "    #   bubble = (P - 1) / (P - 1 + M)\n",
    "    #   efficiency = 1 - bubble\n",
    "    #\n",
    "    # Edge cases:\n",
    "    #   - M = 0? Return bubble = 1.0\n",
    "    #   - P = 1? Return bubble = 0.0\n",
    "    # ==============================\n",
    "\n",
    "    bubble_fraction = ???  # YOUR CODE HERE\n",
    "    efficiency = ???       # YOUR CODE HERE\n",
    "\n",
    "    return bubble_fraction, efficiency"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_17_todo1_verify",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/17_todo1_verify.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Verification: Run this cell to check your implementation\n",
    "\n",
    "test_cases = [\n",
    "    (4, 1, 0.75),      (4, 4, 3/7),\n",
    "    (4, 12, 0.20),     (4, 32, 3/35),\n",
    "    (8, 1, 7/8),       (1, 10, 0.0),\n",
    "    (8, 0, 1.0),\n",
    "]\n",
    "\n",
    "all_passed = True\n",
    "for P, M, expected in test_cases:\n",
    "    bubble, eff = compute_bubble_fraction(P, M)\n",
    "    match = abs(bubble - expected) < 1e-6\n",
    "    status = \"âœ…\" if match else \"âŒ\"\n",
    "    if not match:\n",
    "        all_passed = False\n",
    "    print(f\"  {status} P={P}, M={M}: bubble={bubble:.4f} \"\n",
    "          f\"(expected {expected:.4f}), efficiency={eff:.4f}\")\n",
    "\n",
    "if all_passed:\n",
    "    print(\"\\nâœ… All tests passed! Great work!\")\n",
    "else:\n",
    "    print(\"\\nâŒ Some tests failed. Check your formula.\")"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_18_todo2",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/18_todo2.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”§ TODO 2: Implement the GPipe Forward Schedule\n",
    "\n",
    "Given $P$ stages and $M$ micro-batches, generate the **forward phase** of the GPipe schedule. In GPipe, micro-batch $m$ (0-indexed) enters stage $s$ at time step $m + s$."
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpipe_forward_schedule(P, M):\n",
    "    \"\"\"\n",
    "    Generate ONLY the forward phase of the GPipe schedule.\n",
    "\n",
    "    Micro-batch m (0-indexed) enters stage s at time step m + s.\n",
    "\n",
    "    Returns:\n",
    "        schedule: array of shape (P, M+P-1), values are micro-batch\n",
    "                  numbers (1-indexed) or 0 for idle\n",
    "        total_fwd_time: total time steps in forward phase\n",
    "    \"\"\"\n",
    "    total_fwd_time = M + P - 1\n",
    "    schedule = np.zeros((P, total_fwd_time), dtype=int)\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # For each micro-batch m (0 to M-1):\n",
    "    #   For each stage s (0 to P-1):\n",
    "    #     Set schedule[s][m + s] = m + 1\n",
    "    # ==============================\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return schedule, total_fwd_time"
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_19_todo2_verify",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/19_todo2_verify.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Verification: Run this cell to check your implementation\n",
    "\n",
    "P_test, M_test = 4, 6\n",
    "fwd_sched, fwd_time = generate_gpipe_forward_schedule(P_test, M_test)\n",
    "\n",
    "assert fwd_sched.shape == (P_test, fwd_time), \\\n",
    "    f\"âŒ Shape mismatch: {fwd_sched.shape} vs ({P_test}, {fwd_time})\"\n",
    "\n",
    "total_fwd = np.count_nonzero(fwd_sched)\n",
    "assert total_fwd == P_test * M_test, \\\n",
    "    f\"âŒ Total forwards: {total_fwd}, expected {P_test * M_test}\"\n",
    "\n",
    "for s in range(P_test):\n",
    "    for m in range(M_test):\n",
    "        assert fwd_sched[s][m + s] == m + 1, \\\n",
    "            f\"âŒ Stage {s}, time {m+s}: got {fwd_sched[s][m+s]}, expected {m+1}\"\n",
    "\n",
    "print(\"Your GPipe forward schedule:\")\n",
    "for s in range(P_test):\n",
    "    row = \"\"\n",
    "    for t in range(fwd_time):\n",
    "        val = fwd_sched[s][t]\n",
    "        row += f\" F{val}\" if val > 0 else \"  . \"\n",
    "    print(f\"  GPU {s}: {row}\")\n",
    "\n",
    "print(f\"\\nâœ… All checks passed! Your GPipe forward schedule is correct.\")"
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_20_training_setup",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/20_training_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together: Training with Pipeline Parallelism\n",
    "\n",
    "### 6.1 Simulated Pipeline Training\n",
    "\n",
    "Let us actually train a small model using pipeline-style forward and backward passes. We will simulate the pipeline by running stages sequentially (on a single GPU), but tracking the behavior as if they were on separate GPUs."
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh model split into pipeline stages\n",
    "DIM = 32\n",
    "NUM_LAYERS = 8\n",
    "NUM_STAGES = 4\n",
    "LAYERS_PER_STAGE = NUM_LAYERS // NUM_STAGES\n",
    "NUM_MICROBATCHES = 4\n",
    "BATCH_SIZE = 32\n",
    "MICROBATCH_SIZE = BATCH_SIZE // NUM_MICROBATCHES\n",
    "\n",
    "all_layers = [SimpleLayer(DIM, i) for i in range(NUM_LAYERS)]\n",
    "stages = []\n",
    "for s in range(NUM_STAGES):\n",
    "    start = s * LAYERS_PER_STAGE\n",
    "    end = start + LAYERS_PER_STAGE\n",
    "    stage = PipelineStage(all_layers[start:end], stage_id=s)\n",
    "    stages.append(stage)\n",
    "\n",
    "optimizers = [torch.optim.SGD(stage.parameters(), lr=0.01) for stage in stages]\n",
    "\n",
    "print(f\"Model: {NUM_LAYERS} layers, split into {NUM_STAGES} stages\")\n",
    "print(f\"Batch size: {BATCH_SIZE}, Micro-batches: {NUM_MICROBATCHES}\")\n",
    "print(f\"Micro-batch size: {MICROBATCH_SIZE}\")"
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_21_training_phases",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/21_training_phases.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us define the GPipe-style training step. This function runs all forward passes first (storing activations), then all backward passes."
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_forward_phase(stages, input_mbs, M, P):\n",
    "    \"\"\"Phase 1 of GPipe: run ALL forward passes, store activations.\"\"\"\n",
    "    activations = [[None] * (P + 1) for _ in range(M)]\n",
    "\n",
    "    for m in range(M):\n",
    "        activations[m][0] = input_mbs[m].detach().requires_grad_(True)\n",
    "        for s in range(P):\n",
    "            inp = activations[m][s]\n",
    "            if s > 0:\n",
    "                inp = inp.detach().requires_grad_(True)\n",
    "                activations[m][s] = inp\n",
    "            activations[m][s + 1] = stages[s](inp)\n",
    "\n",
    "    return activations"
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_backward_phase(stages, activations, target_mbs, M, P):\n",
    "    \"\"\"Phase 2 of GPipe: run ALL backward passes in reverse order.\"\"\"\n",
    "    losses = []\n",
    "\n",
    "    for m in reversed(range(M)):\n",
    "        output = activations[m][P]\n",
    "        loss = ((output - target_mbs[m]) ** 2).mean()\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "\n",
    "    return losses"
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine the forward and backward phases into one training step, then run training."
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_train_step(stages, optimizers, inputs, targets, num_mb):\n",
    "    \"\"\"Full GPipe training step: forward phase â†’ backward phase â†’ update.\"\"\"\n",
    "    P = len(stages)\n",
    "    M = num_mb\n",
    "    mb_size = inputs.shape[0] // M\n",
    "\n",
    "    input_mbs = inputs.split(mb_size)\n",
    "    target_mbs = targets.split(mb_size)\n",
    "\n",
    "    activations = pipeline_forward_phase(stages, input_mbs, M, P)\n",
    "    losses = pipeline_backward_phase(stages, activations, target_mbs, M, P)\n",
    "\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return np.mean(losses)"
   ],
   "id": "cell_51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_22_training_run",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/22_training_run.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for several steps and track the loss\n",
    "num_steps = 50\n",
    "loss_history = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "    inputs = torch.randn(BATCH_SIZE, DIM)\n",
    "    targets = inputs * 0.5 + 0.1  # Simple regression target\n",
    "\n",
    "    loss = pipeline_train_step(stages, optimizers, inputs, targets,\n",
    "                               NUM_MICROBATCHES)\n",
    "    loss_history.append(loss)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"  Step {step:3d}: Loss = {loss:.4f}\")\n",
    "\n",
    "print(f\"\\nâœ… Training complete! Loss: {loss_history[0]:.4f} â†’ {loss_history[-1]:.4f}\")"
   ],
   "id": "cell_52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Plot the training loss\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(loss_history, 'b-', linewidth=2, alpha=0.8)\n",
    "ax.set_xlabel(\"Training Step\", fontsize=12)\n",
    "ax.set_ylabel(\"Loss (MSE)\", fontsize=12)\n",
    "ax.set_title(\"Pipeline-Parallel Training: Loss Over Time\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax.annotate(f\"Start: {loss_history[0]:.3f}\", xy=(0, loss_history[0]),\n",
    "            fontsize=10, color='red', fontweight='bold')\n",
    "ax.annotate(f\"End: {loss_history[-1]:.3f}\",\n",
    "            xy=(len(loss_history)-1, loss_history[-1]),\n",
    "            fontsize=10, color='green', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ The model successfully trains with pipeline parallelism!\")\n",
    "print(\"   Each step splits the batch into micro-batches and\")\n",
    "print(\"   runs them through the 4-stage pipeline.\")"
   ],
   "id": "cell_53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_23_comprehensive_comparison",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/23_comprehensive_comparison.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Comprehensive Schedule Comparison\n",
    "\n",
    "Let us now do a thorough comparison across different configurations."
   ],
   "id": "cell_54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Big comparison across different P and M values\n",
    "\n",
    "configs = [\n",
    "    (4, 4), (4, 8), (4, 16),\n",
    "    (8, 8), (8, 16), (8, 32),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(len(configs), 2,\n",
    "                         figsize=(20, 3.5 * len(configs)))\n",
    "\n",
    "for idx, (P, M) in enumerate(configs):\n",
    "    gpipe_s, gpipe_t = generate_gpipe_schedule(P, M)\n",
    "    f1b_s, f1b_t = generate_1f1b_schedule(P, M)\n",
    "    bubble = (P - 1) / (P - 1 + M) * 100\n",
    "\n",
    "    plot_schedule(gpipe_s, gpipe_t, P, M,\n",
    "                  title=f\"GPipe (P={P}, M={M}) â€” Bubble: {bubble:.1f}%\",\n",
    "                  ax=axes[idx][0], show_legend=(idx == 0))\n",
    "    plot_schedule(f1b_s, f1b_t, P, M,\n",
    "                  title=f\"1F1B (P={P}, M={M}) â€” Bubble: {bubble:.1f}%\",\n",
    "                  ax=axes[idx][1], show_legend=(idx == 0))\n",
    "\n",
    "plt.suptitle(\"GPipe vs 1F1B Across Configurations\",\n",
    "             fontsize=16, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"{'Config':<12} {'Bubble %':<11} {'GPipe Mem':<13} {'1F1B Mem':<13} {'Savings'}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for P, M in configs:\n",
    "    bubble = (P - 1) / (P - 1 + M) * 100\n",
    "    savings = (1 - P / M) * 100\n",
    "    print(f\"P={P}, M={M:<4} {bubble:<11.1f} {M}Ã—A{'':<9} {P}Ã—A{'':<9} {savings:.0f}%\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nðŸ’¡ Both schedules have the SAME bubble fraction.\")\n",
    "print(\"   1F1B always wins on memory (when M > P).\")"
   ],
   "id": "cell_56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_24_simulator_build",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/24_simulator_build.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ðŸŽ¯ Final Output: Interactive Pipeline Simulator\n",
    "\n",
    "Now let us put everything together into a complete, interactive simulator. The student can change the number of stages ($P$) and micro-batches ($M$) to see how the schedule, bubble fraction, and memory usage change."
   ],
   "id": "cell_57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_polished_grid(ax, schedule, total_time, P, M, title):\n",
    "    \"\"\"Draw a polished schedule grid with matplotlib colormaps.\"\"\"\n",
    "    for s in range(P):\n",
    "        for t in range(total_time):\n",
    "            val = schedule[s][t]\n",
    "            if val > 0:\n",
    "                color = plt.cm.Blues(0.35 + 0.55*(val/M)); lbl = f\"F{val}\"\n",
    "            elif val < 0:\n",
    "                color = plt.cm.Oranges(0.35 + 0.55*(abs(val)/M)); lbl = f\"B{abs(val)}\"\n",
    "            else:\n",
    "                color = '#E8E8E8'; lbl = \"\"\n",
    "            ax.add_patch(plt.Rectangle((t,s), 1, 1, facecolor=color,\n",
    "                                       edgecolor='white', linewidth=1.2))\n",
    "            if lbl and total_time <= 30:\n",
    "                ax.text(t+0.5, s+0.5, lbl, ha='center', va='center',\n",
    "                        fontsize=6 if total_time > 20 else 7,\n",
    "                        fontweight='bold', color='white')\n",
    "\n",
    "    ax.set_xlim(0, total_time); ax.set_ylim(0, P)\n",
    "    ax.set_yticks([i+0.5 for i in range(P)])\n",
    "    ax.set_yticklabels([f\"GPU {i}\" for i in range(P)])\n",
    "    ax.set_xlabel(\"Time Step\", fontsize=10)\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold', pad=10)\n",
    "    ax.invert_yaxis(); ax.set_aspect('equal')"
   ],
   "id": "cell_58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_simulator(P, M):\n",
    "    \"\"\"Full pipeline simulator: schedules + memory + metrics.\"\"\"\n",
    "    gpipe_s, gpipe_t = generate_gpipe_schedule(P, M)\n",
    "    f1b_s, f1b_t = generate_1f1b_schedule(P, M)\n",
    "    bubble = (P-1)/(P-1+M); eff = 1-bubble\n",
    "    mem_sav = (1-P/M)*100 if M > 0 else 0\n",
    "    gm = compute_memory_profile(gpipe_s, gpipe_t, P, M)\n",
    "    fm = compute_memory_profile(f1b_s, f1b_t, P, M)\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 14))\n",
    "    gs = fig.add_gridspec(3, 2, hspace=0.45, wspace=0.3,\n",
    "                          height_ratios=[1, 1, 0.8])\n",
    "    draw_polished_grid(fig.add_subplot(gs[0,:]), gpipe_s, gpipe_t,\n",
    "                       P, M, f\"GPipe Schedule (P={P}, M={M})\")\n",
    "    draw_polished_grid(fig.add_subplot(gs[1,:]), f1b_s, f1b_t,\n",
    "                       P, M, f\"1F1B Schedule (P={P}, M={M})\")\n",
    "    _draw_memory_panel(fig.add_subplot(gs[2,0]),\n",
    "                       gm, fm, gpipe_t, f1b_t, P, M)\n",
    "    _draw_summary_panel(fig.add_subplot(gs[2,1]), bubble, eff, M, P)\n",
    "    _add_legend(fig)\n",
    "    fig.suptitle(f\"Pipeline Simulator â€” P={P}, M={M}\",\n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "    _print_summary(P, M, bubble, eff, mem_sav)"
   ],
   "id": "cell_59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _draw_memory_panel(ax, gpipe_mem, f1b_mem, gpipe_t, f1b_t, P, M):\n",
    "    \"\"\"Draw the memory profile comparison panel.\"\"\"\n",
    "    ax.plot(range(gpipe_t), gpipe_mem, 'b-o', ms=3,\n",
    "            label=f'GPipe (peak={max(gpipe_mem)})', lw=2)\n",
    "    ax.plot(range(f1b_t), f1b_mem, 'r-s', ms=3,\n",
    "            label=f'1F1B (peak={max(f1b_mem)})', lw=2)\n",
    "    ax.axhline(y=M, color='blue', ls='--', alpha=0.3)\n",
    "    ax.axhline(y=P, color='red', ls='--', alpha=0.3)\n",
    "    ax.set_xlabel(\"Time Step\")\n",
    "    ax.set_ylabel(\"Stored Activations\")\n",
    "    ax.set_title(\"Memory Profile\", fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "def _draw_summary_panel(ax, bubble_frac, efficiency, M, P):\n",
    "    \"\"\"Draw the summary metrics bar chart.\"\"\"\n",
    "    labels = ['Bubble\\n(%)', 'Efficiency\\n(%)',\n",
    "              'GPipe Mem\\n(Ã—A)', '1F1B Mem\\n(Ã—A)']\n",
    "    values = [bubble_frac * 100, efficiency * 100, M, P]\n",
    "    colors = ['#FF6B6B', '#4CAF50', '#2196F3', '#FF9800']\n",
    "    bars = ax.bar(labels, values, color=colors,\n",
    "                  edgecolor='white', lw=2)\n",
    "    for bar, val, lbl in zip(bars, values, labels):\n",
    "        txt = f\"{int(val)}Ã—A\" if \"Mem\" in lbl else f\"{val:.1f}%\"\n",
    "        ax.text(bar.get_x() + bar.get_width()/2,\n",
    "                bar.get_height() + 0.5, txt,\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    ax.set_title(\"Summary Metrics\", fontweight='bold')\n",
    "    ax.grid(True, alpha=0.2, axis='y')"
   ],
   "id": "cell_60"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These small helpers handle the figure legend and text summary output."
   ],
   "id": "cell_61"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_legend(fig):\n",
    "    \"\"\"Add a shared legend to the figure.\"\"\"\n",
    "    fwd = mpatches.Patch(facecolor=plt.cm.Blues(0.6), label='Forward')\n",
    "    bwd = mpatches.Patch(facecolor=plt.cm.Oranges(0.6), label='Backward')\n",
    "    idle = mpatches.Patch(facecolor='#E8E8E8', edgecolor='gray',\n",
    "                          label='Idle')\n",
    "    fig.legend(handles=[fwd, bwd, idle], loc='upper right',\n",
    "               fontsize=10, framealpha=0.9)\n",
    "\n",
    "\n",
    "def _print_summary(P, M, bubble_frac, efficiency, mem_savings):\n",
    "    \"\"\"Print text summary of pipeline metrics.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  PIPELINE PARALLELISM SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Stages (P):           {P}\")\n",
    "    print(f\"  Micro-batches (M):    {M}\")\n",
    "    print(f\"  Bubble fraction:      {bubble_frac*100:.1f}%\")\n",
    "    print(f\"  Pipeline efficiency:  {efficiency*100:.1f}%\")\n",
    "    print(f\"  GPipe peak memory:    {M}Ã—A\")\n",
    "    print(f\"  1F1B peak memory:     {P}Ã—A\")\n",
    "    print(f\"  Memory savings (1F1B):{mem_savings:.0f}%\")\n",
    "    print(f\"{'='*60}\")"
   ],
   "id": "cell_62"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is defined. Let us run the simulator!"
   ],
   "id": "cell_63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_25_simulator_run",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/25_simulator_run.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ Run the full simulator with default settings!\n",
    "pipeline_simulator(P=4, M=8)"
   ],
   "id": "cell_64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_26_experiments",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Experiments\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/26_experiments.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try different configurations! Change P and M below to explore how the pipeline schedule changes."
   ],
   "id": "cell_65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ Experiment 1: Few micro-batches (large bubble)\n",
    "print(\"=\" * 60)\n",
    "print(\"  EXPERIMENT 1: Few micro-batches â†’ large bubble\")\n",
    "print(\"=\" * 60)\n",
    "pipeline_simulator(P=4, M=4)"
   ],
   "id": "cell_66"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ Notice how the bubble shrinks as we increase $M$. Let us try doubling it."
   ],
   "id": "cell_67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ Experiment 2: Many micro-batches (small bubble)\n",
    "print(\"=\" * 60)\n",
    "print(\"  EXPERIMENT 2: Many micro-batches â†’ small bubble\")\n",
    "print(\"=\" * 60)\n",
    "pipeline_simulator(P=4, M=16)"
   ],
   "id": "cell_68"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us scale up to 8 stages â€” closer to what production systems use."
   ],
   "id": "cell_69"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ Experiment 3: Many stages (realistic scale)\n",
    "print(\"=\" * 60)\n",
    "print(\"  EXPERIMENT 3: 8 stages with 32 micro-batches\")\n",
    "print(\"=\" * 60)\n",
    "pipeline_simulator(P=8, M=32)"
   ],
   "id": "cell_70"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_27_bubble_explorer",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/27_bubble_explorer.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Bubble Fraction Explorer\n",
    "\n",
    "Let us create one more visualization: how the bubble fraction changes as we increase the number of micro-batches for different numbers of stages."
   ],
   "id": "cell_71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Bubble fraction as a function of M for various P\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "M_values = np.arange(1, 65)\n",
    "\n",
    "for P in [2, 4, 8, 16]:\n",
    "    bubble_fracs = [(P-1) / (P-1+M) * 100 for M in M_values]\n",
    "    ax.plot(M_values, bubble_fracs, linewidth=2.5, label=f'P={P}')\n",
    "\n",
    "ax.axhline(y=10, color='gray', linestyle='--', alpha=0.5, label='10% line')\n",
    "ax.set_xlabel(\"Number of Micro-batches (M)\", fontsize=12)\n",
    "ax.set_ylabel(\"Bubble Fraction (%)\", fontsize=12)\n",
    "ax.set_title(\"Bubble Fraction vs Micro-batches\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 100)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Memory savings of 1F1B over GPipe\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for P in [2, 4, 8, 16]:\n",
    "    M_valid = [m for m in M_values if m >= P]\n",
    "    savings = [(1 - P / m) * 100 for m in M_valid]\n",
    "    ax.plot(M_valid, savings, linewidth=2.5, label=f'P={P}')\n",
    "\n",
    "ax.set_xlabel(\"Number of Micro-batches (M)\", fontsize=12)\n",
    "ax.set_ylabel(\"Memory Savings (%)\", fontsize=12)\n",
    "ax.set_title(\"1F1B Memory Savings over GPipe\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 100)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ’¡ Minimum M needed for <10% bubble at each P\n",
    "print(\"ðŸ’¡ Minimum micro-batches for <10% bubble:\")\n",
    "for P in [2, 4, 8, 16]:\n",
    "    M_min = int(np.ceil(9 * (P - 1)))\n",
    "    actual_bubble = (P - 1) / (P - 1 + M_min) * 100\n",
    "    print(f\"   P={P:<3} â†’ M â‰¥ {M_min:<3} (bubble = {actual_bubble:.1f}%)\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Congratulations! You have built a pipeline parallelism\")\n",
    "print(\"   simulator from scratch!\")\n",
    "print()\n",
    "print(\"   You now understand:\")\n",
    "print(\"   âœ… Why pipeline parallelism exists (split model by depth)\")\n",
    "print(\"   âœ… The bubble problem and its formula\")\n",
    "print(\"   âœ… GPipe schedule (all F then all B)\")\n",
    "print(\"   âœ… 1F1B schedule (interleaved F and B)\")\n",
    "print(\"   âœ… Why 1F1B has the same bubble but lower memory\")\n",
    "print(\"   âœ… How to choose M to minimize the bubble\")"
   ],
   "id": "cell_74"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_28_reflection",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Reflection\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/28_reflection.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reflection and Next Steps\n",
    "\n",
    "### ðŸ¤” Reflection Questions\n",
    "\n",
    "1. **Why does 1F1B have lower peak memory than GPipe despite having the same bubble fraction?**\n",
    "   *Hint: Think about when activations are created and when they are released.*\n",
    "\n",
    "2. **What is the minimum number of micro-batches needed to keep the bubble below 10% with 8 pipeline stages?**\n",
    "   *Hint: Solve $(P - 1) / (P - 1 + M) < 0.10$ for $M$.*\n",
    "\n",
    "3. **In practice, PP is used across nodes while TP is within a node. Why?**\n",
    "   *Hint: Think about communication frequency. PP communicates at stage boundaries (once per micro-batch per layer boundary). TP communicates within every single layer. Which one can tolerate higher latency?*\n",
    "\n",
    "4. **Why do we split a mini-batch into micro-batches instead of just using a larger pipeline depth?**\n",
    "   *Hint: More stages means each stage does less work, which increases the relative cost of communication.*"
   ],
   "id": "cell_75"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ† Optional Challenges\n",
    "\n",
    "For those who want to go deeper:\n",
    "\n",
    "1. **Interleaved 1F1B**: In the interleaved schedule (used by Megatron-LM v3), each GPU handles *non-contiguous* stages. For example, with 8 stages and 4 GPUs, GPU 0 handles stages 0 and 4, GPU 1 handles stages 1 and 5, etc. This reduces the bubble by a factor of $v$ (the number of virtual stages per GPU). Try implementing this!\n",
    "\n",
    "2. **Gradient Accumulation**: Modify the training loop to accumulate gradients across micro-batches before updating weights. This is how real PP systems work â€” the optimizer step only happens once per mini-batch, not once per micro-batch.\n",
    "\n",
    "3. **Zero-Bubble Pipeline**: Recent research (Qi et al., 2024) proposes schedules that achieve near-zero bubble by overlapping the backward pass for weights (B) and the backward pass for activations (W). Read the paper and think about how this works.\n",
    "\n",
    "4. **PipeDream-2BW (Double-Buffered Weights)**: The original PipeDream maintained multiple versions of model weights to avoid stalls. PipeDream-2BW improved this by keeping at most 2 versions. How does this trade off memory for pipeline efficiency?"
   ],
   "id": "cell_76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ¤” Quick exercise: answer Reflection Question 2 computationally\n",
    "P_q2 = 8\n",
    "target_bubble = 0.10\n",
    "# Solve: (P-1)/(P-1+M) < 0.10 => M > 9*(P-1)\n",
    "M_needed = int(np.ceil(9 * (P_q2 - 1)))\n",
    "actual = (P_q2 - 1) / (P_q2 - 1 + M_needed) * 100\n",
    "print(f\"Q2: With P={P_q2}, need M â‰¥ {M_needed} for <10% bubble\")\n",
    "print(f\"    Actual bubble at M={M_needed}: {actual:.1f}%\")"
   ],
   "id": "cell_77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_29_closing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Wrap-Up\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/29_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Next?\n",
    "\n",
    "In this notebook, we built Pipeline Parallelism from scratch. We saw how splitting a model by depth creates an assembly line of GPUs, and how the bubble fraction is the central tradeoff.\n",
    "\n",
    "In the **next notebook**, we will explore **Sequence Parallelism** â€” how to split along the sequence length dimension. This is critical for training models on long contexts (128K+ tokens).\n",
    "\n",
    "**The journey so far:**\n",
    "- Notebook 1: Introduction to 5D Parallelism\n",
    "- Notebook 2: Why Do We Need Parallelism?\n",
    "- Notebook 3: Data Parallelism (Hire More Chefs)\n",
    "- **Notebook 4: Pipeline Parallelism (The Assembly Line)** -- You are here!\n",
    "- Notebook 5: Sequence Parallelism (Split the Sentence)\n",
    "- Notebook 6: Expert Parallelism (Specialist Chefs)"
   ],
   "id": "cell_78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"  ðŸš€ PIPELINE PARALLELISM â€” KEY TAKEAWAYS\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"  1. PP splits the model by DEPTH\")\n",
    "print(\"     GPU 0: layers 1-8, GPU 1: layers 9-16, ...\")\n",
    "print()\n",
    "print(\"  2. The bubble fraction = (P-1) / (P-1+M)\")\n",
    "print(\"     More micro-batches â†’ smaller bubble\")\n",
    "print()\n",
    "print(\"  3. GPipe: all forwards, then all backwards\")\n",
    "print(\"     Simple but high peak memory (M Ã— A)\")\n",
    "print()\n",
    "print(\"  4. 1F1B: interleave forwards and backwards\")\n",
    "print(\"     Same bubble, but peak memory = P Ã— A\")\n",
    "print()\n",
    "print(\"  5. PP is used ACROSS nodes (tolerates latency)\")\n",
    "print(\"     TP is used WITHIN a node (needs fast NVLink)\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"  ðŸŽ‰ Great work! See you in the next notebook.\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "cell_79"
  }
 ]
}