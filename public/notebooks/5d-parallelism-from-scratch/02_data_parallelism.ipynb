{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Data Parallelism & ZeRO from Scratch â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1ventuhdj998YNr_9KusKPNX2VFJg7Av1\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_setup",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parallelism & ZeRO: Splitting Data Across GPUs\n",
    "## Notebook 2 of 6 â€” *5D Parallelism from Scratch*\n",
    "\n",
    "**Vizuara AI â€” Google Colab Notebook**\n",
    "\n",
    "In this notebook, we will implement Data Parallelism from scratch. We will simulate 4 \"virtual GPUs\" on a single device, split mini-batches across them, implement the AllReduce gradient averaging algorithm, and watch all four models converge identically. Then, we will build a ZeRO memory simulator to understand how the Zero Redundancy Optimizer eliminates the massive memory waste of vanilla DP.\n",
    "\n",
    "**Estimated time: ~45 minutes**\n",
    "\n",
    "**What you will learn:**\n",
    "- Why Data Parallelism is the most fundamental distributed training strategy\n",
    "- How AllReduce gradient averaging keeps models synchronized\n",
    "- Why vanilla DP wastes memory and how ZeRO (Stages 1-3) fixes it\n",
    "- The exact memory formulas used in production training systems"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup â€” run this cell first\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"All libraries loaded successfully!\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_why_it_matters",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Data Parallelism is the bread and butter of distributed training. Every single large model trained today â€” GPT-4, Llama 3, DeepSeek-V3, Gemini â€” uses Data Parallelism as one of its core strategies. It is the simplest parallelism technique, and understanding it is the foundation for everything else in this series.\n",
    "\n",
    "Here is the key idea: **if one GPU is too slow, use N GPUs â€” each processing a different chunk of data, then share what they learned.**\n",
    "\n",
    "By the end of this notebook, you will see something beautiful: four separate models, each training on different data, yet converging to *exactly the same weights* â€” as if a single model had trained on all the data at once.\n",
    "\n",
    "Let us start with a teaser. We will run the full Data Parallel training simulation and see all four models converge. If this does not make sense yet, do not worry â€” we will build every component from scratch."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick teaser: What we will build by the end of this notebook\n",
    "# (just run this cell to see the final result â€” we will build it step by step)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  DATA PARALLELISM: 4 Virtual GPUs Training Together\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"By the end of this notebook, you will see:\")\n",
    "print(\"  1. Four model copies, each seeing different data\")\n",
    "print(\"  2. AllReduce averaging their gradients every step\")\n",
    "print(\"  3. All four loss curves perfectly overlapping\")\n",
    "print(\"  4. ZeRO memory savings from 112 GB to 28 GB per GPU\")\n",
    "print()\n",
    "print(\"Let us build it!\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_restaurant_analogy",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Restaurant Analogy\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_restaurant_analogy.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Before we write any equations or code, let us develop a deep intuition for what Data Parallelism actually does.\n",
    "\n",
    "### The Restaurant Analogy\n",
    "\n",
    "Imagine you run a restaurant that has just received 1,000 orders. You have one chef. Each order takes 1 minute. That is 1,000 minutes â€” over 16 hours.\n",
    "\n",
    "What is the simplest fix? **Hire 4 chefs.** Give each chef a **complete copy of the recipe book** and split the orders: Chef 0 gets orders 1â€“250, Chef 1 gets orders 251â€“500, and so on.\n",
    "\n",
    "Each chef cooks independently. But here is the critical part: at the end of each round, all 4 chefs **share notes**. \"I found that adding a pinch more salt works better.\" \"I discovered the oven should be 10 degrees hotter.\" They average their learnings, and everyone updates their recipe book identically.\n",
    "\n",
    "This is Data Parallelism:\n",
    "- **4 chefs** = 4 GPUs\n",
    "- **Complete recipe book** = full copy of the model\n",
    "- **Split orders** = split the mini-batch\n",
    "- **Share notes** = AllReduce (gradient averaging)\n",
    "- **Update recipe identically** = synchronized weight update\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "Notice something important: each chef still needs the **full recipe book**. Hiring more chefs does not make the recipe book smaller. It only makes cooking faster.\n",
    "\n",
    "This is both the strength and the weakness of Data Parallelism:\n",
    "- **Strength**: Linearly scales throughput (4 GPUs â†’ ~4x faster training)\n",
    "- **Weakness**: Does not reduce per-GPU memory at all\n",
    "\n",
    "### Think About This\n",
    "\n",
    "If a 7B parameter model needs 112 GB of memory for training, and you use Data Parallelism across 4 GPUs, how much memory does each GPU need?\n",
    "\n",
    "The answer: still **112 GB each**. Data Parallelism does not help with memory â€” it only helps with speed. Each GPU holds a full copy of everything.\n",
    "\n",
    "What if the recipe book itself is too big for one chef's desk? That is the memory problem DP does not solve. What if, instead of each chef having a full recipe book, we *tore it into sections*? That is ZeRO â€” and we will get there."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_math_allreduce",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Math Allreduce\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_math_allreduce.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "Now let us formalize these intuitions with precise equations. We will derive everything step by step.\n",
    "\n",
    "### 3.1 The AllReduce Equation\n",
    "\n",
    "Each GPU $i$ processes its own mini-batch $B_i$ and computes a gradient $g_i$. To keep all GPUs synchronized, we average these gradients:\n",
    "\n",
    "$$g_{\\text{avg}} = \\frac{1}{N} \\sum_{i=1}^{N} g_i$$\n",
    "\n",
    "Here, $N$ is the number of GPUs (or \"workers\"), and $g_i$ is the gradient computed by GPU $i$.\n",
    "\n",
    "**Why does averaging work?** Because the expected value of the average gradient equals the gradient of the full batch:\n",
    "\n",
    "$$\\mathbb{E}[g_{\\text{avg}}] = \\mathbb{E}\\left[\\frac{1}{N} \\sum_{i=1}^{N} g_i\\right] = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}[g_i] = \\mathbb{E}[g_{\\text{full batch}}]$$\n",
    "\n",
    "Each $g_i$ is an unbiased estimate of the true gradient. Averaging $N$ unbiased estimates gives another unbiased estimate â€” but with $N$ times more data, so it is a better estimate (lower variance).\n",
    "\n",
    "### 3.2 A Concrete Numerical Example\n",
    "\n",
    "Let us plug in some simple numbers. Suppose we have 4 GPUs, and for a single parameter, each GPU computes a gradient:\n",
    "\n",
    "- GPU 0: $g_0 = 0.5$\n",
    "- GPU 1: $g_1 = 0.3$\n",
    "- GPU 2: $g_2 = 0.7$\n",
    "- GPU 3: $g_3 = 0.1$\n",
    "\n",
    "The AllReduce operation computes:\n",
    "\n",
    "$$g_{\\text{avg}} = \\frac{0.5 + 0.3 + 0.7 + 0.1}{4} = \\frac{1.6}{4} = 0.4$$\n",
    "\n",
    "All four GPUs now use $g_{\\text{avg}} = 0.4$ to update their weights. Since they started with identical weights and apply the identical update, they remain perfectly synchronized."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us verify the AllReduce math computationally\n",
    "gradients = [0.5, 0.3, 0.7, 0.1]\n",
    "N = len(gradients)\n",
    "g_avg = sum(gradients) / N\n",
    "\n",
    "print(\"AllReduce â€” Numerical Example\")\n",
    "print(\"-\" * 40)\n",
    "for i, g in enumerate(gradients):\n",
    "    print(f\"  GPU {i}: gradient = {g}\")\n",
    "print(f\"\\n  g_avg = ({' + '.join(str(g) for g in gradients)}) / {N}\")\n",
    "print(f\"        = {sum(gradients)} / {N}\")\n",
    "print(f\"        = {g_avg}\")\n",
    "print(f\"\\nAll 4 GPUs now use g_avg = {g_avg}\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_weight_update",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Weight Update\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_weight_update.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 The Weight Update\n",
    "\n",
    "After AllReduce, every GPU applies the same update rule:\n",
    "\n",
    "$$w_{t+1} = w_t - \\eta \\cdot g_{\\text{avg}}$$\n",
    "\n",
    "where $\\eta$ is the learning rate. Since all GPUs start with the same $w_t$ and apply the same $g_{\\text{avg}}$, they all arrive at the same $w_{t+1}$.\n",
    "\n",
    "This is the invariant that Data Parallelism must maintain: **all model copies have identical weights at every step.**"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the weight update invariant\n",
    "lr = 0.01\n",
    "w_initial = 1.0  # Same starting weight on all GPUs\n",
    "\n",
    "print(\"Weight Update After AllReduce\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Initial weight (all GPUs): w = {w_initial}\")\n",
    "print(f\"  Learning rate: eta = {lr}\")\n",
    "print(f\"  Averaged gradient: g_avg = {g_avg}\")\n",
    "print(f\"\\n  w_new = w - eta x g_avg\")\n",
    "print(f\"        = {w_initial} - {lr} x {g_avg}\")\n",
    "print(f\"        = {w_initial - lr * g_avg}\")\n",
    "print(f\"\\nAll 4 GPUs arrive at w = {w_initial - lr * g_avg}\")\n",
    "print(\"They remain perfectly synchronized!\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_memory_problem",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Memory Problem\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_memory_problem.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 The Memory Problem\n",
    "\n",
    "Now let us understand why vanilla Data Parallelism does not save memory. With mixed-precision training using the Adam optimizer, each parameter requires:\n",
    "\n",
    "| Component | Precision | Bytes per param |\n",
    "|-----------|-----------|-----------------|\n",
    "| Model weights | fp16 | 2 |\n",
    "| Gradients | fp16 | 2 |\n",
    "| Optimizer: fp32 weight copy | fp32 | 4 |\n",
    "| Optimizer: first moment (m) | fp32 | 4 |\n",
    "| Optimizer: second moment (v) | fp32 | 4 |\n",
    "| **Total** | | **16** |\n",
    "\n",
    "For a model with $P$ parameters:\n",
    "\n",
    "$$M_{\\text{per GPU}} = 16P \\text{ bytes}$$\n",
    "\n",
    "With vanilla DP, every GPU stores all of this. For a 7B model:\n",
    "\n",
    "$$M = 7 \\times 10^9 \\times 16 = 112 \\text{ GB per GPU}$$\n",
    "\n",
    "Four GPUs? Still 112 GB each. A thousand GPUs? Still 112 GB each. Vanilla DP does not help with memory at all."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory calculation for vanilla Data Parallelism\n",
    "def vanilla_dp_memory_gb(num_params_billions, num_gpus):\n",
    "    \"\"\"Calculate per-GPU memory for vanilla DP (no savings).\"\"\"\n",
    "    P = num_params_billions * 1e9\n",
    "    bytes_per_param = 16  # 2 (weights) + 2 (grads) + 12 (optimizer)\n",
    "    total_bytes = P * bytes_per_param\n",
    "    per_gpu_gb = total_bytes / (1024**3)\n",
    "    return per_gpu_gb\n",
    "\n",
    "# 7B model with different GPU counts\n",
    "for n_gpus in [1, 4, 8, 64]:\n",
    "    mem = vanilla_dp_memory_gb(7, n_gpus)\n",
    "    print(f\"  7B model, {n_gpus:>2} GPUs -> {mem:.1f} GB per GPU (no savings!)\")\n",
    "\n",
    "print(\"\\nAdding more GPUs does NOT reduce memory with vanilla DP.\")\n",
    "print(\"Each GPU still holds the full model, gradients, and optimizer.\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_07_build_model",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_build_model.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let Us Build It â€” Component by Component\n",
    "\n",
    "Now we will implement Data Parallelism from scratch. We will use a small 2-layer MLP trained on MNIST to keep things fast, but the principles are exactly the same as training a 70B model on 1,000 GPUs.\n",
    "\n",
    "### 4.1 The Model\n",
    "\n",
    "Our model is intentionally simple â€” two linear layers with ReLU activation. The goal is not the model architecture but the parallelism mechanics."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"A 2-layer MLP for MNIST classification.\n",
    "\n",
    "    Small enough to run 4 copies on any hardware,\n",
    "    but complex enough to demonstrate real DP behavior.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=784, hidden_dim=256, output_dim=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten: (B, 1, 28, 28) -> (B, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create and inspect the model\n",
    "model = SimpleMLP().to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model: SimpleMLP\")\n",
    "print(f\"  Parameters: {total_params:,}\")\n",
    "print(f\"  fc1: {784} x {256} + {256} bias = {784*256 + 256:,}\")\n",
    "print(f\"  fc2: {256} x {10} + {10} bias = {256*10 + 10:,}\")\n",
    "print(f\"\\nModel created on {device}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_08_load_mnist",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_load_mnist.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Loading MNIST\n",
    "\n",
    "We will use a subset of MNIST to keep training fast. The key thing to understand is that in real DP, different GPUs would receive different shards of the dataset. Here, we will simulate this by explicitly splitting each mini-batch."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Use a subset for speed (8000 samples is plenty for our demo)\n",
    "train_subset = Subset(train_dataset, range(8000))\n",
    "\n",
    "# Global batch size = 128, which we will split across 4 \"GPUs\"\n",
    "GLOBAL_BATCH_SIZE = 128\n",
    "train_loader = DataLoader(\n",
    "    train_subset, batch_size=GLOBAL_BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_subset):,}\")\n",
    "print(f\"Global batch size: {GLOBAL_BATCH_SIZE}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")\n",
    "print(f\"\\nMNIST loaded successfully\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_09_virtual_gpus",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_virtual_gpus.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Creating Virtual GPUs\n",
    "\n",
    "This is where the simulation begins. In real Data Parallelism, each GPU has its own model copy. We simulate this by creating 4 independent copies of our MLP. The crucial detail: **all copies start with identical weights.**"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GPUS = 4\n",
    "\n",
    "def create_virtual_gpus(base_model, num_gpus):\n",
    "    \"\"\"Create N identical copies of a model to simulate N GPUs.\n",
    "\n",
    "    In real DP, each copy lives on a separate GPU.\n",
    "    Here, they all live on the same device but are independent nn.Modules.\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    for i in range(num_gpus):\n",
    "        model_copy = copy.deepcopy(base_model)\n",
    "        models.append(model_copy)\n",
    "    return models\n",
    "\n",
    "# Create 4 \"virtual GPUs\" â€” each with an identical model copy\n",
    "base_model = SimpleMLP().to(device)\n",
    "virtual_gpus = create_virtual_gpus(base_model, NUM_GPUS)"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a helper to verify that all model copies remain synchronized throughout training. This function checks that every parameter is identical across all copies."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_weight_sync(models):\n",
    "    \"\"\"Check that all model copies have identical weights.\"\"\"\n",
    "    reference = list(models[0].parameters())\n",
    "    for i in range(1, len(models)):\n",
    "        for p_ref, p_i in zip(reference, list(models[i].parameters())):\n",
    "            if not torch.allclose(p_ref, p_i):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "print(f\"Created {NUM_GPUS} virtual GPUs\")\n",
    "print(f\"Weights synchronized: {check_weight_sync(virtual_gpus)}\")\n",
    "print(f\"All {NUM_GPUS} models start with identical weights\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_10_split_batch",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_split_batch.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Splitting the Batch\n",
    "\n",
    "In real DP, a data loader distributes different mini-batches to different GPUs. We simulate this by taking a single batch of 128 samples and splitting it into 4 micro-batches of 32."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_batch(data, targets, num_gpus):\n",
    "    \"\"\"Split a batch evenly across virtual GPUs.\n",
    "\n",
    "    Args:\n",
    "        data: (B, C, H, W) tensor â€” the full mini-batch\n",
    "        targets: (B,) tensor â€” the labels\n",
    "        num_gpus: number of GPUs to split across\n",
    "\n",
    "    Returns:\n",
    "        List of (micro_data, micro_targets) tuples, one per GPU\n",
    "    \"\"\"\n",
    "    micro_batch_size = len(data) // num_gpus\n",
    "    splits = []\n",
    "    for i in range(num_gpus):\n",
    "        start = i * micro_batch_size\n",
    "        end = start + micro_batch_size\n",
    "        splits.append((data[start:end], targets[start:end]))\n",
    "    return splits"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us demonstrate the split on a sample batch."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the split\n",
    "sample_data, sample_targets = next(iter(train_loader))\n",
    "splits = split_batch(sample_data, sample_targets, NUM_GPUS)\n",
    "\n",
    "print(f\"Global batch: {sample_data.shape[0]} samples\")\n",
    "print(f\"Split across {NUM_GPUS} GPUs:\")\n",
    "for i, (d, t) in enumerate(splits):\n",
    "    print(f\"  GPU {i}: {d.shape[0]} samples (indices {i*32}-{(i+1)*32-1})\")\n",
    "print(f\"\\nEach GPU gets {splits[0][0].shape[0]} samples\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_11_forward_backward",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_forward_backward.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Forward and Backward on Each GPU\n",
    "\n",
    "Each virtual GPU independently runs forward and backward passes on its own micro-batch. This is the \"embarrassingly parallel\" part â€” no communication needed here."
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_on_all_gpus(models, splits, criterion):\n",
    "    \"\"\"Run forward + backward on each GPU's micro-batch.\n",
    "\n",
    "    Each model processes its own data independently.\n",
    "    After this, each model has its own gradients stored\n",
    "    in .grad attributes.\n",
    "\n",
    "    Returns:\n",
    "        List of losses (one per GPU)\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    for i, (model, (data, targets)) in enumerate(\n",
    "        zip(models, splits)\n",
    "    ):\n",
    "        model.zero_grad()\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "    return losses"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run the forward and backward passes on all 4 virtual GPUs and inspect the resulting losses."
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run forward + backward on all 4 GPUs\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "losses = forward_backward_on_all_gpus(virtual_gpus, splits, criterion)\n",
    "\n",
    "print(\"Forward + Backward Pass (Independent)\")\n",
    "print(\"-\" * 40)\n",
    "for i, loss in enumerate(losses):\n",
    "    print(f\"  GPU {i}: loss = {loss:.4f}\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us peek at the gradients before AllReduce. Each GPU has different gradients because it saw different data."
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that gradients differ across GPUs BEFORE AllReduce\n",
    "print(\"Gradients BEFORE AllReduce (first 5 values of fc1.weight.grad)\")\n",
    "print(\"-\" * 60)\n",
    "for i, model in enumerate(virtual_gpus):\n",
    "    grads = model.fc1.weight.grad.flatten()[:5]\n",
    "    grad_str = \", \".join(f\"{g:.4f}\" for g in grads)\n",
    "    print(f\"  GPU {i}: [{grad_str}]\")\n",
    "\n",
    "print(\"\\nGradients are DIFFERENT â€” each GPU saw different data!\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_12_allreduce_impl",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_allreduce_impl.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Implementing AllReduce\n",
    "\n",
    "This is the heart of Data Parallelism. AllReduce takes the gradients from all GPUs, averages them, and writes the averaged gradient back to every GPU. After this operation, all models have identical gradients and will take identical weight update steps."
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allreduce_gradients(models):\n",
    "    \"\"\"AllReduce: Average gradients across all model copies.\n",
    "\n",
    "    This is the key operation in Data Parallelism.\n",
    "    After this, all models have identical gradients.\n",
    "\n",
    "    In real distributed training, this is implemented using\n",
    "    efficient ring-allreduce algorithms over NCCL.\n",
    "    Here we simulate it directly.\n",
    "    \"\"\"\n",
    "    num_models = len(models)\n",
    "\n",
    "    # For each parameter, average the gradients across all models\n",
    "    for param_group in zip(*[model.parameters() for model in models]):\n",
    "        # Stack all gradients: shape (num_models, *param_shape)\n",
    "        all_grads = torch.stack([p.grad for p in param_group])\n",
    "        # Compute the mean across models (dimension 0)\n",
    "        avg_grad = all_grads.mean(dim=0)\n",
    "        # Write the averaged gradient back to ALL models\n",
    "        for p in param_group:\n",
    "            p.grad.copy_(avg_grad)\n",
    "\n",
    "# Apply AllReduce\n",
    "allreduce_gradients(virtual_gpus)\n",
    "print(\"AllReduce complete â€” gradients averaged across all GPUs\")"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization Checkpoint 1:** Let us see the effect of AllReduce. We will verify that all GPUs now have identical gradients."
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that gradients are NOW identical across all GPUs\n",
    "print(\"Gradients AFTER AllReduce (first 5 values of fc1.weight.grad)\")\n",
    "print(\"-\" * 60)\n",
    "for i, model in enumerate(virtual_gpus):\n",
    "    grads = model.fc1.weight.grad.flatten()[:5]\n",
    "    grad_str = \", \".join(f\"{g:.4f}\" for g in grads)\n",
    "    print(f\"  GPU {i}: [{grad_str}]\")\n",
    "\n",
    "# Verify they are truly identical\n",
    "all_identical = True\n",
    "ref_grads = list(virtual_gpus[0].parameters())\n",
    "for model in virtual_gpus[1:]:\n",
    "    for p_ref, p in zip(ref_grads, model.parameters()):\n",
    "        if not torch.allclose(p_ref.grad, p.grad):\n",
    "            all_identical = False\n",
    "            break\n",
    "\n",
    "print(f\"\\nAll gradients identical: {all_identical}\")\n",
    "print(\"Now all GPUs will take the exact same weight update step!\")"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_13_allreduce_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_allreduce_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualize the before and after of AllReduce with a side-by-side plot. We re-create fresh models and re-run forward/backward to capture both states."
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-create models and re-run forward/backward\n",
    "demo_models = create_virtual_gpus(base_model, NUM_GPUS)\n",
    "demo_losses = forward_backward_on_all_gpus(demo_models, splits, criterion)\n",
    "\n",
    "# Collect \"before\" gradients (first 20 values of fc1.weight.grad)\n",
    "num_vals = 20\n",
    "before_grads = []\n",
    "for m in demo_models:\n",
    "    before_grads.append(\n",
    "        m.fc1.weight.grad.flatten()[:num_vals].detach().cpu().numpy()\n",
    "    )\n",
    "\n",
    "# Apply AllReduce and collect \"after\" gradients\n",
    "allreduce_gradients(demo_models)\n",
    "after_grads = []\n",
    "for m in demo_models:\n",
    "    after_grads.append(\n",
    "        m.fc1.weight.grad.flatten()[:num_vals].detach().cpu().numpy()\n",
    "    )"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the before vs after comparison."
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "x = np.arange(num_vals)\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "# Plot BEFORE\n",
    "for i in range(NUM_GPUS):\n",
    "    axes[0].plot(x, before_grads[i], 'o-', color=colors[i],\n",
    "                 label=f'GPU {i}', markersize=4, alpha=0.8)\n",
    "axes[0].set_title('BEFORE AllReduce', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Parameter index')\n",
    "axes[0].set_ylabel('Gradient value')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot AFTER\n",
    "for i in range(NUM_GPUS):\n",
    "    axes[1].plot(x, after_grads[i], 'o-', color=colors[i],\n",
    "                 label=f'GPU {i}', markersize=4, alpha=0.8)\n",
    "axes[1].set_title('AFTER AllReduce', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Parameter index')\n",
    "axes[1].set_ylabel('Gradient value')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a title and display the figure."
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualization\n",
    "plt.suptitle('AllReduce: Gradient Averaging Across GPUs',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left: each GPU has different gradients (different data)\")\n",
    "print(\"Right: after AllReduce, all GPUs have the SAME averaged gradient\")"
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_14_training_loop",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_training_loop.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 The Full Data Parallel Training Loop\n",
    "\n",
    "Now we put it all together. The training loop follows this pattern every step:\n",
    "\n",
    "1. Split the mini-batch across GPUs\n",
    "2. Each GPU runs forward + backward independently\n",
    "3. AllReduce averages the gradients\n",
    "4. Each GPU applies the same weight update\n",
    "5. All models remain perfectly synchronized"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_parallel(num_gpus, num_epochs=3, lr=0.01):\n",
    "    \"\"\"Complete Data Parallel training simulation.\n",
    "\n",
    "    Simulates N GPUs by maintaining N model copies on one device.\n",
    "    Returns per-GPU loss histories for visualization.\n",
    "    \"\"\"\n",
    "    # Create identical model copies\n",
    "    base = SimpleMLP().to(device)\n",
    "    models = create_virtual_gpus(base, num_gpus)\n",
    "\n",
    "    # Each GPU has its own optimizer (operating on its local model)\n",
    "    optimizers = [optim.SGD(m.parameters(), lr=lr) for m in models]\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Track per-GPU loss curves\n",
    "    loss_histories = [[] for _ in range(num_gpus)]\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            micro_batches = split_batch(data, targets, num_gpus)\n",
    "            for i, (model, (mb_data, mb_targets)) in enumerate(\n",
    "                zip(models, micro_batches)\n",
    "            ):\n",
    "                optimizers[i].zero_grad()\n",
    "                mb_data = mb_data.to(device)\n",
    "                mb_targets = mb_targets.to(device)\n",
    "                loss = criterion(model(mb_data), mb_targets)\n",
    "                loss.backward()\n",
    "                loss_histories[i].append(loss.item())"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second half of the training loop performs AllReduce and the weight update. We also check synchronization at the end of each epoch."
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # Step 3: AllReduce â€” average gradients\n",
    "            allreduce_gradients(models)\n",
    "\n",
    "            # Step 4: Each GPU applies the same update\n",
    "            for opt in optimizers:\n",
    "                opt.step()\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        # End-of-epoch check\n",
    "        synced = check_weight_sync(models)\n",
    "        avg_loss = np.mean([h[-1] for h in loss_histories])\n",
    "        print(f\"  Epoch {epoch+1}/{num_epochs} â€” \"\n",
    "              f\"Avg loss: {avg_loss:.4f} â€” \"\n",
    "              f\"Synced: {synced}\")\n",
    "\n",
    "    return models, loss_histories"
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now run the complete training simulation."
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training with simulated 4-way Data Parallelism...\")\n",
    "print(\"=\" * 50)\n",
    "trained_models, loss_histories = train_data_parallel(\n",
    "    num_gpus=NUM_GPUS, num_epochs=3, lr=0.01\n",
    ")\n",
    "print(\"=\" * 50)\n",
    "print(\"Training complete!\")"
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_15_loss_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/15_loss_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization Checkpoint 2:** The moment of truth. All four loss curves should overlap perfectly after AllReduce."
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-GPU loss curves (raw and smoothed)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "# Left: Raw per-GPU loss curves\n",
    "for i in range(NUM_GPUS):\n",
    "    axes[0].plot(loss_histories[i], color=colors[i],\n",
    "                 label=f'GPU {i}', alpha=0.7, linewidth=1)\n",
    "axes[0].set_title('Per-GPU Loss Curves', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Training step')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)"
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smoothed view makes the convergence pattern clearer."
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Right: Overlay showing convergence (smoothed)\n",
    "fig2, ax_smooth = plt.subplots(figsize=(8, 5))\n",
    "window = 10\n",
    "for i in range(NUM_GPUS):\n",
    "    smoothed = np.convolve(loss_histories[i],\n",
    "                           np.ones(window)/window, mode='valid')\n",
    "    ax_smooth.plot(smoothed, color=colors[i],\n",
    "                   label=f'GPU {i}', alpha=0.8, linewidth=2)\n",
    "ax_smooth.set_title('Smoothed Loss (window=10)', fontsize=14, fontweight='bold')\n",
    "ax_smooth.set_xlabel('Training step')\n",
    "ax_smooth.set_ylabel('Loss')\n",
    "ax_smooth.legend()\n",
    "ax_smooth.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Data Parallelism: All GPUs Converge Together',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The loss curves differ slightly because each GPU sees different data,\")\n",
    "print(\"but they converge together because AllReduce synchronizes them.\")"
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_16_weight_sync_verify",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Weight Sync Verify\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/16_weight_sync_verify.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also verify that the models have truly identical weights at the end of training."
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify final weight synchronization\n",
    "print(\"Final Weight Synchronization Check\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "max_diff = 0.0\n",
    "for name, p_ref in trained_models[0].named_parameters():\n",
    "    for i in range(1, NUM_GPUS):\n",
    "        p_i = dict(trained_models[i].named_parameters())[name]\n",
    "        diff = (p_ref - p_i).abs().max().item()\n",
    "        max_diff = max(max_diff, diff)\n",
    "\n",
    "print(f\"  Maximum weight difference across all GPUs: {max_diff:.2e}\")\n",
    "\n",
    "if max_diff < 1e-6:\n",
    "    print(f\"\\nAll {NUM_GPUS} models have IDENTICAL weights!\")\n",
    "    print(\"Data Parallelism maintains perfect synchronization.\")\n",
    "else:\n",
    "    print(f\"\\nSmall numerical differences detected (floating point).\")\n",
    "    print(f\"This is normal â€” the key point is they converge together.\")"
   ],
   "id": "cell_51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_17_todo1_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/17_todo1_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn â€” TODO Exercises\n",
    "\n",
    "Now it is your turn to implement the core components. These exercises will cement your understanding of Data Parallelism.\n",
    "\n",
    "### TODO 1: Implement the AllReduce Averaging Step\n",
    "\n",
    "Below is a function signature with a docstring. Your task: **fill in the body** to average gradients across all model copies. This is the most important function in Data Parallelism."
   ],
   "id": "cell_52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_allreduce(models):\n",
    "    \"\"\"Implement AllReduce gradient averaging.\n",
    "\n",
    "    For each parameter across all models:\n",
    "      1. Collect the gradient from each model\n",
    "      2. Compute the mean\n",
    "      3. Write the mean gradient back to ALL models\n",
    "\n",
    "    Hint: Loop over zip(*[model.parameters() for model in models])\n",
    "          to iterate over corresponding parameters simultaneously.\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Stack all gradients into a tensor using torch.stack\n",
    "    # Step 2: Compute the mean across dim=0\n",
    "    # Step 3: Copy the averaged gradient to ALL models using .copy_()\n",
    "    # ==============================\n",
    "\n",
    "    num_models = len(models)\n",
    "\n",
    "    for param_group in zip(*[m.parameters() for m in models]):\n",
    "        all_grads = ???  # YOUR CODE HERE\n",
    "        avg_grad = ???   # YOUR CODE HERE\n",
    "        for p in param_group:\n",
    "            pass          # YOUR CODE HERE â€” use p.grad.copy_(...)"
   ],
   "id": "cell_53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_18_todo1_verify",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo1 Verify\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/18_todo1_verify.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the verification cell below to check your implementation. Do not modify this cell."
   ],
   "id": "cell_54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification for TODO 1\n",
    "print(\"Verifying your AllReduce implementation...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create fresh models and compute gradients\n",
    "test_base = SimpleMLP().to(device)\n",
    "test_models = create_virtual_gpus(test_base, 4)\n",
    "test_data, test_targets = next(iter(train_loader))\n",
    "test_splits = split_batch(test_data, test_targets, 4)\n",
    "_ = forward_backward_on_all_gpus(\n",
    "    test_models, test_splits, nn.CrossEntropyLoss()\n",
    ")"
   ],
   "id": "cell_55"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compare your implementation against the reference."
   ],
   "id": "cell_56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save expected result using reference implementation\n",
    "ref_models = create_virtual_gpus(test_base, 4)\n",
    "_ = forward_backward_on_all_gpus(\n",
    "    ref_models, test_splits, nn.CrossEntropyLoss()\n",
    ")\n",
    "allreduce_gradients(ref_models)\n",
    "\n",
    "# Apply student's implementation\n",
    "my_allreduce(test_models)\n",
    "\n",
    "# Check per-parameter correctness\n",
    "all_correct = True\n",
    "for (name, p_ref), (_, p_stu) in zip(\n",
    "    ref_models[0].named_parameters(),\n",
    "    test_models[0].named_parameters()\n",
    "):\n",
    "    if p_stu.grad is None:\n",
    "        print(f\"  {name}: gradient is None â€” did you forget to set it?\")\n",
    "        all_correct = False\n",
    "    elif not torch.allclose(p_ref.grad, p_stu.grad, atol=1e-6):\n",
    "        print(f\"  {name}: gradient mismatch!\")\n",
    "        all_correct = False"
   ],
   "id": "cell_57"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that all GPUs received the same averaged gradient."
   ],
   "id": "cell_58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cross-GPU gradient consistency\n",
    "if all_correct:\n",
    "    for i in range(1, 4):\n",
    "        for p0, pi in zip(test_models[0].parameters(),\n",
    "                          test_models[i].parameters()):\n",
    "            if not torch.allclose(p0.grad, pi.grad, atol=1e-6):\n",
    "                print(f\"  GPU 0 and GPU {i} have different gradients!\")\n",
    "                all_correct = False\n",
    "                break\n",
    "\n",
    "if all_correct:\n",
    "    print(\"  AllReduce implementation is CORRECT!\")\n",
    "    print(\"  All GPUs have identical averaged gradients!\")\n",
    "else:\n",
    "    print(\"\\n  Hint: Stack gradients -> mean(dim=0) -> copy back to all\")"
   ],
   "id": "cell_59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_19_todo2_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/19_todo2_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement ZeRO Stage 1 Optimizer State Partitioning\n",
    "\n",
    "In ZeRO Stage 1, each GPU only stores 1/N of the optimizer states. After AllReduce of gradients, each GPU updates only its shard of parameters using its local optimizer, then all GPUs broadcast (AllGather) the updated weights so everyone has the full model.\n",
    "\n",
    "Your task: implement the `zero_stage1_step` function that performs one training step with ZeRO-1 style optimizer sharding."
   ],
   "id": "cell_60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_stage1_step(models, optimizers, param_shards, num_gpus):\n",
    "    \"\"\"Perform one ZeRO Stage 1 optimizer step.\n",
    "\n",
    "    In ZeRO-1, each GPU owns a shard of the optimizer states.\n",
    "    After AllReduce of gradients:\n",
    "      1. Each GPU updates ONLY its assigned parameter shard\n",
    "      2. AllGather broadcasts updated weights to all GPUs\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: AllReduce gradients (already implemented â€” call it)\n",
    "    # Step 2: For each GPU, update ONLY its assigned param shard\n",
    "    # Step 3: AllGather â€” copy updated params from owner to all\n",
    "    # ==============================\n",
    "\n",
    "    # Step 1: AllReduce gradients across all models\n",
    "    allreduce_gradients(models)\n",
    "\n",
    "    # Step 2: Each GPU updates only its own shard\n",
    "    for gpu_id in range(num_gpus):\n",
    "        my_indices = param_shards[gpu_id]\n",
    "        opt = optimizers[gpu_id]\n",
    "        opt.zero_grad()\n",
    "        for idx in my_indices:\n",
    "            src_param = list(models[gpu_id].parameters())[idx]\n",
    "            src_param.grad = ???  # YOUR CODE HERE\n",
    "\n",
    "    result = ???  # YOUR CODE HERE â€” perform the optimizer step"
   ],
   "id": "cell_61"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part of `zero_stage1_step`: AllGather broadcasts the updated weights from each owner GPU to all others."
   ],
   "id": "cell_62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Step 3: AllGather â€” broadcast updated weights\n",
    "    for gpu_id in range(num_gpus):\n",
    "        my_indices = param_shards[gpu_id]\n",
    "        for idx in my_indices:\n",
    "            updated = list(models[gpu_id].parameters())[idx]\n",
    "            for other_gpu in range(num_gpus):\n",
    "                if other_gpu != gpu_id:\n",
    "                    target = list(models[other_gpu].parameters())[idx]\n",
    "                    target.data.copy_(???)  # YOUR CODE HERE\n",
    "\n",
    "    return models"
   ],
   "id": "cell_63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_20_todo2_verify",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo2 Verify\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/20_todo2_verify.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the verification cell below to check your ZeRO Stage 1 implementation."
   ],
   "id": "cell_64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification for TODO 2\n",
    "print(\"Verifying your ZeRO Stage 1 implementation...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Setup: create models, assign shards\n",
    "z1_base = SimpleMLP().to(device)\n",
    "z1_models = create_virtual_gpus(z1_base, NUM_GPUS)\n",
    "\n",
    "# Count parameters and assign shards\n",
    "all_param_list = list(z1_models[0].parameters())\n",
    "num_total_params = len(all_param_list)\n",
    "print(f\"  Total parameter groups: {num_total_params}\")"
   ],
   "id": "cell_65"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the shard assignments and run the step."
   ],
   "id": "cell_66"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign parameter indices to GPUs round-robin\n",
    "param_shards = {g: [] for g in range(NUM_GPUS)}\n",
    "for idx in range(num_total_params):\n",
    "    owner = idx % NUM_GPUS\n",
    "    param_shards[owner].append(idx)\n",
    "\n",
    "for g in range(NUM_GPUS):\n",
    "    print(f\"  GPU {g} owns param indices: {param_shards[g]}\")\n",
    "\n",
    "# Create per-GPU optimizers\n",
    "z1_optimizers = [\n",
    "    optim.SGD(m.parameters(), lr=0.01) for m in z1_models\n",
    "]\n",
    "\n",
    "# Run one forward-backward pass\n",
    "z1_data, z1_targets = next(iter(train_loader))\n",
    "z1_splits = split_batch(z1_data, z1_targets, NUM_GPUS)\n",
    "_ = forward_backward_on_all_gpus(\n",
    "    z1_models, z1_splits, nn.CrossEntropyLoss()\n",
    ")"
   ],
   "id": "cell_67"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply your ZeRO-1 step and check if models stay synchronized."
   ],
   "id": "cell_68"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply student's ZeRO-1 step\n",
    "try:\n",
    "    zero_stage1_step(z1_models, z1_optimizers, param_shards, NUM_GPUS)\n",
    "    synced = check_weight_sync(z1_models)\n",
    "    assert synced, \"Models are not synchronized after ZeRO-1 step!\"\n",
    "    print(\"\\n  ZeRO Stage 1 step is CORRECT!\")\n",
    "    print(\"  All GPUs synchronized after shard-based update + AllGather!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n  Error: {e}\")\n",
    "    print(\"  Hint: Each GPU updates its shard, then broadcasts to others.\")"
   ],
   "id": "cell_69"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_21_unified_demo",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/21_unified_demo.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Now let us combine everything â€” Data Parallelism with AllReduce and ZeRO-style memory accounting â€” into a unified demonstration. We will train with simulated 4-way DP, track memory as if each stage of ZeRO were active, and compare the results.\n",
    "\n",
    "### 6.1 Unified DP + ZeRO Training Demo\n",
    "\n",
    "This function wraps our training loop with a ZeRO memory tracker, so you can see both the training convergence and the memory implications side by side."
   ],
   "id": "cell_70"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_memory_per_gpu_bytes(num_params, num_gpus, stage):\n",
    "    \"\"\"Compute per-GPU memory in bytes for a given ZeRO stage.\n",
    "\n",
    "    Args:\n",
    "        num_params: total model parameters (P)\n",
    "        num_gpus: number of GPUs (N)\n",
    "        stage: 0 (vanilla DP), 1, 2, or 3\n",
    "    Returns:\n",
    "        Memory in bytes (float)\n",
    "    \"\"\"\n",
    "    P, N = num_params, num_gpus\n",
    "    if stage == 0:\n",
    "        return 16 * P\n",
    "    elif stage == 1:\n",
    "        return 4 * P + 12 * P / N\n",
    "    elif stage == 2:\n",
    "        return 2 * P + (2 * P + 12 * P) / N\n",
    "    elif stage == 3:\n",
    "        return 16 * P / N\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid stage: {stage}\")"
   ],
   "id": "cell_71"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the unified training function that logs both convergence and ZeRO memory statistics."
   ],
   "id": "cell_72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dp_with_zero_tracking(num_gpus, num_epochs=3, lr=0.01):\n",
    "    \"\"\"Train with DP and report ZeRO memory stats at each epoch.\"\"\"\n",
    "    base = SimpleMLP().to(device)\n",
    "    models = create_virtual_gpus(base, num_gpus)\n",
    "    optimizers = [optim.SGD(m.parameters(), lr=lr) for m in models]\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    total_params = sum(p.numel() for p in models[0].parameters())\n",
    "    loss_histories = [[] for _ in range(num_gpus)]\n",
    "    step = 0\n",
    "\n",
    "    print(f\"Model parameters: {total_params:,}\")\n",
    "    print(f\"Simulating {num_gpus}-way Data Parallelism\")\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for data, targets in train_loader:\n",
    "            micro_batches = split_batch(data, targets, num_gpus)\n",
    "            for i, (model, (d, t)) in enumerate(\n",
    "                zip(models, micro_batches)\n",
    "            ):\n",
    "                optimizers[i].zero_grad()\n",
    "                d, t = d.to(device), t.to(device)\n",
    "                loss = criterion(model(d), t)\n",
    "                loss.backward()\n",
    "                loss_histories[i].append(loss.item())\n",
    "            allreduce_gradients(models)\n",
    "            for opt in optimizers:\n",
    "                opt.step()\n",
    "            step += 1"
   ],
   "id": "cell_73"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of each epoch, we print the ZeRO memory comparison."
   ],
   "id": "cell_74"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        synced = check_weight_sync(models)\n",
    "        avg_loss = np.mean([h[-1] for h in loss_histories])\n",
    "        print(f\"\\n  Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Loss: {avg_loss:.4f} | Synced: {synced}\")\n",
    "\n",
    "        # ZeRO memory report (scaled to 7B for realism)\n",
    "        P_7B = 7e9\n",
    "        print(f\"  Memory per GPU (if 7B model, {num_gpus} GPUs):\")\n",
    "        for stg, name in [(0, \"Vanilla DP\"), (1, \"ZeRO-1\"),\n",
    "                          (2, \"ZeRO-2\"), (3, \"ZeRO-3/FSDP\")]:\n",
    "            mem_gb = zero_memory_per_gpu_bytes(P_7B, num_gpus, stg) / (1024**3)\n",
    "            bar = \"#\" * int(mem_gb / 3)\n",
    "            print(f\"    {name:<14s}: {mem_gb:6.1f} GB  {bar}\")\n",
    "\n",
    "    return models, loss_histories, total_params"
   ],
   "id": "cell_75"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run the unified demo."
   ],
   "id": "cell_76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 55)\n",
    "print(\"  UNIFIED DEMO: Data Parallelism + ZeRO Memory Tracking\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "final_models, final_histories, n_params = train_dp_with_zero_tracking(\n",
    "    num_gpus=NUM_GPUS, num_epochs=3, lr=0.01\n",
    ")\n",
    "print(\"\\n\" + \"=\" * 55)\n",
    "print(\"Demo complete!\")"
   ],
   "id": "cell_77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_22_unified_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/22_unified_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Final Comparison Visualization\n",
    "\n",
    "We combine the training convergence and ZeRO memory savings into a single summary figure."
   ],
   "id": "cell_78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "# Left: Smoothed loss curves from unified training\n",
    "window = 10\n",
    "for i in range(NUM_GPUS):\n",
    "    smoothed = np.convolve(\n",
    "        final_histories[i], np.ones(window)/window, mode='valid'\n",
    "    )\n",
    "    ax1.plot(smoothed, color=colors[i],\n",
    "             label=f'GPU {i}', linewidth=1.5, alpha=0.8)\n",
    "ax1.set_title('DP Training: All GPUs Converge', fontsize=13, fontweight='bold')\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)"
   ],
   "id": "cell_79"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the right panel: ZeRO memory comparison."
   ],
   "id": "cell_80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right: ZeRO memory breakdown for 7B model\n",
    "P_7B = 7e9\n",
    "stage_names = ['Vanilla DP', 'ZeRO-1', 'ZeRO-2', 'ZeRO-3']\n",
    "stage_ids = [0, 1, 2, 3]\n",
    "mem_vals = [\n",
    "    zero_memory_per_gpu_bytes(P_7B, NUM_GPUS, s) / (1024**3)\n",
    "    for s in stage_ids\n",
    "]\n",
    "bar_colors = ['#95a5a6', '#3498db', '#2ecc71', '#e74c3c']\n",
    "\n",
    "ax2.bar(range(4), mem_vals, color=bar_colors, edgecolor='white')\n",
    "ax2.axhline(y=80, color='red', linestyle='--', alpha=0.5, linewidth=1.5)\n",
    "ax2.text(3.4, 81, 'A100 80GB', color='red', fontsize=9)\n",
    "ax2.set_xticks(range(4))\n",
    "ax2.set_xticklabels(stage_names, fontsize=9)\n",
    "ax2.set_title('ZeRO Memory (7B, 4 GPUs)', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel('Per-GPU Memory (GB)')\n",
    "for i, v in enumerate(mem_vals):\n",
    "    ax2.text(i, v + 1.5, f'{v:.0f}', ha='center', fontsize=10,\n",
    "             fontweight='bold')\n",
    "ax2.grid(True, axis='y', alpha=0.3)"
   ],
   "id": "cell_81"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the combined figure and print key takeaways."
   ],
   "id": "cell_82"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualization\n",
    "plt.suptitle('Data Parallelism + ZeRO: Training & Memory',\n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "savings = (1 - mem_vals[3] / mem_vals[0]) * 100\n",
    "print(f\"DP trains correctly: all {NUM_GPUS} GPUs converge together.\")\n",
    "print(f\"ZeRO-3 saves {savings:.0f}% memory vs vanilla DP.\")\n",
    "print(f\"This is how production systems train models that do not fit on one GPU.\")"
   ],
   "id": "cell_83"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_23_zero_deep_dive",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Zero Deep Dive\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/23_zero_deep_dive.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ZeRO: Eliminating Redundancy\n",
    "\n",
    "Now let us go deeper into ZeRO â€” one of the most impactful ideas in distributed training. The core insight is devastatingly simple: **in vanilla DP, every GPU stores the exact same optimizer states, gradients, and weights. That is N-fold redundancy, and it is completely unnecessary.**\n",
    "\n",
    "### 7.1 Understanding Each ZeRO Stage\n",
    "\n",
    "Let us walk through what each GPU stores at each stage.\n",
    "\n",
    "**Vanilla DP (no ZeRO):** Each of 4 GPUs stores:\n",
    "- Full weights (2P bytes)\n",
    "- Full gradients (2P bytes)\n",
    "- Full optimizer states (12P bytes)\n",
    "- Total: 16P bytes per GPU, 64P bytes across the cluster\n",
    "\n",
    "The optimizer states alone are 12 bytes per parameter (fp32 weight copy + Adam first moment + Adam second moment). For 4 GPUs, that is 4 identical copies of 12P bytes = 48P bytes wasted on redundancy.\n",
    "\n",
    "**ZeRO Stage 1 â€” Shard Optimizer States:**\n",
    "Each GPU stores only 1/4 of the optimizer states. After the AllReduce of gradients, each GPU updates only its shard of the optimizer and then broadcasts (AllGather) the updated weights.\n",
    "\n",
    "**ZeRO Stage 2 â€” Shard Optimizer + Gradients:**\n",
    "On top of Stage 1, gradients are also sharded. Each GPU only keeps the gradients for the parameters whose optimizer states it owns.\n",
    "\n",
    "**ZeRO Stage 3 â€” Shard Everything (FSDP):**\n",
    "Even the model weights are sharded. Each GPU only stores 1/N of the weights. When a layer needs weights it does not own, it fetches them via AllGather, uses them for the forward/backward pass, and then discards them."
   ],
   "id": "cell_84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZeRO memory formulas â€” reference implementation\n",
    "def zero_memory_reference(num_params_billions, num_gpus):\n",
    "    \"\"\"Compute per-GPU memory for all ZeRO stages.\n",
    "\n",
    "    Returns a dict of {stage_name: memory_in_GB}.\n",
    "    \"\"\"\n",
    "    P = num_params_billions * 1e9\n",
    "    N = num_gpus\n",
    "\n",
    "    results = {}\n",
    "    results['Vanilla DP'] = 16 * P\n",
    "    results['ZeRO-1'] = 4 * P + 12 * P / N\n",
    "    results['ZeRO-2'] = 2 * P + (2 * P + 12 * P) / N\n",
    "    results['ZeRO-3 (FSDP)'] = 16 * P / N\n",
    "\n",
    "    # Convert bytes to GB\n",
    "    return {k: v / (1024**3) for k, v in results.items()}"
   ],
   "id": "cell_85"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us calculate and display the memory for a 7B model with 4 GPUs."
   ],
   "id": "cell_86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate for 7B model with 4 GPUs\n",
    "mem_4gpu = zero_memory_reference(7, 4)\n",
    "\n",
    "print(\"ZeRO Memory Breakdown â€” 7B Model, 4 GPUs\")\n",
    "print(\"=\" * 50)\n",
    "for stage, mem in mem_4gpu.items():\n",
    "    bar = \"#\" * int(mem / 2)\n",
    "    print(f\"  {stage:<20s}: {mem:6.1f} GB  {bar}\")\n",
    "\n",
    "savings = (1 - mem_4gpu['ZeRO-3 (FSDP)'] / mem_4gpu['Vanilla DP']) * 100\n",
    "print(f\"\\nZeRO-3 reduces memory by {savings:.0f}% compared to vanilla DP!\")\n",
    "print(f\"From {mem_4gpu['Vanilla DP']:.1f} GB -> \"\n",
    "      f\"{mem_4gpu['ZeRO-3 (FSDP)']:.1f} GB per GPU\")"
   ],
   "id": "cell_87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_24_zero_stacked_bar",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/24_zero_stacked_bar.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization Checkpoint 3:** A stacked bar chart showing exactly what each GPU stores under each ZeRO stage."
   ],
   "id": "cell_88"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZeRO Memory Comparison â€” stacked bar chart setup\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "P = 7e9  # 7B parameters\n",
    "N = 4    # 4 GPUs\n",
    "\n",
    "stages = ['Vanilla DP', 'ZeRO-1', 'ZeRO-2', 'ZeRO-3\\n(FSDP)']\n",
    "\n",
    "weights_gb = [\n",
    "    2 * P / 1024**3,\n",
    "    2 * P / 1024**3,\n",
    "    2 * P / 1024**3,\n",
    "    2 * P / N / 1024**3,\n",
    "]\n",
    "gradients_gb = [\n",
    "    2 * P / 1024**3,\n",
    "    2 * P / 1024**3,\n",
    "    2 * P / N / 1024**3,\n",
    "    2 * P / N / 1024**3,\n",
    "]\n",
    "optimizer_gb = [\n",
    "    12 * P / 1024**3,\n",
    "    12 * P / N / 1024**3,\n",
    "    12 * P / N / 1024**3,\n",
    "    12 * P / N / 1024**3,\n",
    "]"
   ],
   "id": "cell_89"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we draw the stacked bar chart."
   ],
   "id": "cell_90"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(stages))\n",
    "width = 0.5\n",
    "\n",
    "bars1 = ax.bar(x, weights_gb, width, label='Weights (fp16)',\n",
    "               color='#3498db', edgecolor='white', linewidth=0.5)\n",
    "bars2 = ax.bar(x, gradients_gb, width, bottom=weights_gb,\n",
    "               label='Gradients (fp16)', color='#2ecc71',\n",
    "               edgecolor='white', linewidth=0.5)\n",
    "bars3 = ax.bar(x, optimizer_gb, width,\n",
    "               bottom=[w+g for w, g in zip(weights_gb, gradients_gb)],\n",
    "               label='Optimizer States (fp32)', color='#e74c3c',\n",
    "               edgecolor='white', linewidth=0.5)\n",
    "\n",
    "# Add total labels on top of each bar\n",
    "for i in range(len(stages)):\n",
    "    total = weights_gb[i] + gradients_gb[i] + optimizer_gb[i]\n",
    "    ax.text(i, total + 1.5, f'{total:.1f} GB',\n",
    "            ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# A100 memory line\n",
    "ax.axhline(y=80, color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
    "ax.text(3.3, 82, 'A100 80GB', color='red', fontsize=11, fontweight='bold')"
   ],
   "id": "cell_91"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format axes and display the chart."
   ],
   "id": "cell_92"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualization\n",
    "ax.set_xlabel('Strategy', fontsize=13)\n",
    "ax.set_ylabel('Per-GPU Memory (GB)', fontsize=13)\n",
    "ax.set_title('ZeRO: Progressive Memory Savings\\n'\n",
    "             '7B Model with 4 GPUs (Mixed Precision + Adam)',\n",
    "             fontsize=15, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(stages, fontsize=12)\n",
    "ax.legend(loc='upper right', fontsize=11)\n",
    "ax.set_ylim(0, 120)\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: Vanilla DP exceeds the A100's 80 GB limit!\")\n",
    "print(\"ZeRO-1 barely fits. ZeRO-2 and ZeRO-3 fit comfortably.\")"
   ],
   "id": "cell_93"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_25_zero_scaling",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/25_zero_scaling.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Scaling ZeRO: More GPUs, More Savings\n",
    "\n",
    "The beauty of ZeRO is that memory savings scale with the number of GPUs. Let us see how per-GPU memory changes as we add more GPUs."
   ],
   "id": "cell_94"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZeRO memory scaling with number of GPUs\n",
    "gpu_counts = [1, 2, 4, 8, 16, 32, 64]\n",
    "P_billions = 7  # 7B model\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "stage_colors = {\n",
    "    'Vanilla DP': '#95a5a6',\n",
    "    'ZeRO-1': '#3498db',\n",
    "    'ZeRO-2': '#2ecc71',\n",
    "    'ZeRO-3 (FSDP)': '#e74c3c',\n",
    "}\n",
    "\n",
    "for stage_name in ['Vanilla DP', 'ZeRO-1', 'ZeRO-2', 'ZeRO-3 (FSDP)']:\n",
    "    memories = []\n",
    "    for n in gpu_counts:\n",
    "        mem = zero_memory_reference(P_billions, n)\n",
    "        memories.append(mem[stage_name])\n",
    "    ax.plot(gpu_counts, memories, 'o-', color=stage_colors[stage_name],\n",
    "            label=stage_name, linewidth=2, markersize=8)"
   ],
   "id": "cell_95"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add reference lines and labels to the scaling plot."
   ],
   "id": "cell_96"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š A100 reference line\n",
    "ax.axhline(y=80, color='red', linestyle='--', alpha=0.5, linewidth=1.5)\n",
    "ax.text(50, 82, 'A100 80GB', color='red', fontsize=10)\n",
    "\n",
    "ax.set_xlabel('Number of GPUs', fontsize=13)\n",
    "ax.set_ylabel('Per-GPU Memory (GB)', fontsize=13)\n",
    "ax.set_title('ZeRO Memory Scaling â€” 7B Model',\n",
    "             fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.set_xticks(gpu_counts)\n",
    "ax.set_xticklabels(gpu_counts)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 120)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Vanilla DP (gray) stays flat â€” no memory savings ever!\")\n",
    "print(\"ZeRO-3 (red) drops to ~1.6 GB with 64 GPUs.\")\n",
    "print(\"This is how we train 70B+ models: shard everything.\")"
   ],
   "id": "cell_97"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_26_communication_tradeoff",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Communication Tradeoff\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/26_communication_tradeoff.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 The Communication Trade-off\n",
    "\n",
    "ZeRO saves memory, but there is a cost: **more communication**. Let us understand this trade-off.\n",
    "\n",
    "- **Vanilla DP**: One AllReduce per step (gradients only). Communication volume: $2P$ bytes.\n",
    "- **ZeRO-1**: Same communication as vanilla DP. The optimizer sharding is \"free\" â€” each GPU only updates its own shard, then broadcasts via AllGather. Roughly $2P$ bytes.\n",
    "- **ZeRO-2**: Same as ZeRO-1 â€” gradient sharding uses ReduceScatter instead of AllReduce, but the total volume is similar. Roughly $2P$ bytes.\n",
    "- **ZeRO-3**: Adds AllGather for weights in both forward and backward passes. Communication roughly $3P$ bytes â€” a 1.5x increase, but memory drops to $16P/N$."
   ],
   "id": "cell_98"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communication volume comparison â€” setup data\n",
    "P_billions = 7\n",
    "P = P_billions * 1e9\n",
    "\n",
    "comm_data = {\n",
    "    'Vanilla DP': 2 * P * 2,\n",
    "    'ZeRO-1':     2 * P * 2,\n",
    "    'ZeRO-2':     2 * P * 2,\n",
    "    'ZeRO-3':     3 * P * 2,\n",
    "}\n",
    "comm_gb = {k: v / 1024**3 for k, v in comm_data.items()}\n",
    "\n",
    "N = 4\n",
    "mem_data = zero_memory_reference(P_billions, N)"
   ],
   "id": "cell_99"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us plot the memory vs communication trade-off side by side."
   ],
   "id": "cell_100"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Memory savings (with N=4)\n",
    "stages_ordered = ['Vanilla DP', 'ZeRO-1', 'ZeRO-2', 'ZeRO-3 (FSDP)']\n",
    "mem_vals = [mem_data[s] for s in stages_ordered]\n",
    "colors_ordered = ['#95a5a6', '#3498db', '#2ecc71', '#e74c3c']\n",
    "\n",
    "ax1.bar(range(4), mem_vals, color=colors_ordered, edgecolor='white')\n",
    "ax1.set_xticks(range(4))\n",
    "ax1.set_xticklabels(\n",
    "    ['Vanilla\\nDP', 'ZeRO-1', 'ZeRO-2', 'ZeRO-3\\n(FSDP)'], fontsize=10\n",
    ")\n",
    "ax1.set_ylabel('Per-GPU Memory (GB)', fontsize=12)\n",
    "ax1.set_title('Memory (lower = better)', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, axis='y', alpha=0.3)\n",
    "for i, v in enumerate(mem_vals):\n",
    "    ax1.text(i, v + 1, f'{v:.0f}', ha='center', fontsize=11,\n",
    "             fontweight='bold')"
   ],
   "id": "cell_101"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the right panel: communication volume comparison."
   ],
   "id": "cell_102"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Communication volume\n",
    "comm_stages = ['Vanilla\\nDP', 'ZeRO-1', 'ZeRO-2', 'ZeRO-3']\n",
    "comm_vals = [comm_gb['Vanilla DP'], comm_gb['ZeRO-1'],\n",
    "             comm_gb['ZeRO-2'], comm_gb['ZeRO-3']]\n",
    "ax2.bar(range(4), comm_vals, color=colors_ordered, edgecolor='white')\n",
    "ax2.set_xticks(range(4))\n",
    "ax2.set_xticklabels(comm_stages, fontsize=10)\n",
    "ax2.set_ylabel('Communication per Step (GB)', fontsize=12)\n",
    "ax2.set_title('Communication (lower = better)',\n",
    "              fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, axis='y', alpha=0.3)\n",
    "for i, v in enumerate(comm_vals):\n",
    "    ax2.text(i, v + 0.3, f'{v:.0f}', ha='center', fontsize=11,\n",
    "             fontweight='bold')\n",
    "\n",
    "plt.suptitle('ZeRO Trade-off: Memory vs Communication\\n'\n",
    "             '7B Model, 4 GPUs', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ZeRO-3 saves 75% memory but increases communication by ~50%.\")\n",
    "print(\"In practice, this trade-off is almost always worth it.\")"
   ],
   "id": "cell_103"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_27_sharding_sim",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/27_sharding_sim.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Simulating ZeRO Sharding\n",
    "\n",
    "Let us simulate what ZeRO actually does under the hood. We will take a small parameter tensor and show how it gets sharded across GPUs at each stage."
   ],
   "id": "cell_104"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_zero_sharding(param_name, param_size, num_gpus):\n",
    "    \"\"\"Simulate ZeRO sharding for a single parameter.\n",
    "\n",
    "    Shows what each GPU stores at each ZeRO stage.\n",
    "    \"\"\"\n",
    "    shard_size = param_size // num_gpus\n",
    "\n",
    "    print(f\"\\nParameter: {param_name} ({param_size} elements)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Vanilla DP\n",
    "    print(\"\\n  Vanilla DP:\")\n",
    "    for g in range(num_gpus):\n",
    "        print(f\"    GPU {g}: weights[0:{param_size}] + \"\n",
    "              f\"grads[0:{param_size}] + opt[0:{param_size}]\")\n",
    "\n",
    "    # ZeRO Stage 1: shard optimizer\n",
    "    print(\"\\n  ZeRO Stage 1 (shard optimizer):\")\n",
    "    for g in range(num_gpus):\n",
    "        os, oe = g * shard_size, (g+1) * shard_size\n",
    "        print(f\"    GPU {g}: weights[0:{param_size}] + \"\n",
    "              f\"grads[0:{param_size}] + opt[{os}:{oe}]\")"
   ],
   "id": "cell_105"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing the simulation for Stages 2 and 3."
   ],
   "id": "cell_106"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ZeRO Stage 2: shard optimizer + gradients\n",
    "    print(\"\\n  ZeRO Stage 2 (shard optimizer + gradients):\")\n",
    "    for g in range(num_gpus):\n",
    "        s, e = g * shard_size, (g+1) * shard_size\n",
    "        print(f\"    GPU {g}: weights[0:{param_size}] + \"\n",
    "              f\"grads[{s}:{e}] + opt[{s}:{e}]\")\n",
    "\n",
    "    # ZeRO Stage 3: shard everything\n",
    "    print(\"\\n  ZeRO Stage 3 / FSDP (shard everything):\")\n",
    "    for g in range(num_gpus):\n",
    "        s, e = g * shard_size, (g+1) * shard_size\n",
    "        print(f\"    GPU {g}: weights[{s}:{e}] + \"\n",
    "              f\"grads[{s}:{e}] + opt[{s}:{e}]\")\n",
    "\n",
    "simulate_zero_sharding(\"fc1.weight\", param_size=1024, num_gpus=4)"
   ],
   "id": "cell_107"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_28_sharding_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/28_sharding_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us build a visual diagram of the sharding. We draw each GPU's stored components at each ZeRO stage."
   ],
   "id": "cell_108"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual simulation of ZeRO sharding â€” setup\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 10))\n",
    "\n",
    "stages = ['Vanilla DP\\n(Full Replication)',\n",
    "          'ZeRO Stage 1\\n(Shard Optimizer)',\n",
    "          'ZeRO Stage 2\\n(Shard Opt + Grad)',\n",
    "          'ZeRO Stage 3 / FSDP\\n(Shard Everything)']\n",
    "\n",
    "N = 4\n",
    "components = [\n",
    "    ('Weights\\n(fp16)', 2, '#3498db'),\n",
    "    ('Gradients\\n(fp16)', 2, '#2ecc71'),\n",
    "    ('Optimizer\\n(fp32)', 12, '#e74c3c'),\n",
    "]"
   ],
   "id": "cell_109"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the sharding diagram for all 4 stages."
   ],
   "id": "cell_110"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ax_idx, (ax, stage_name) in enumerate(zip(axes, stages)):\n",
    "    for gpu_id in range(N):\n",
    "        y = gpu_id * 1.5\n",
    "        x_start = 0\n",
    "        for comp_idx, (comp_name, comp_width, comp_color) in enumerate(\n",
    "            components\n",
    "        ):\n",
    "            if ax_idx == 0:\n",
    "                ew = comp_width\n",
    "            elif ax_idx == 1:\n",
    "                ew = comp_width / N if comp_idx == 2 else comp_width\n",
    "            elif ax_idx == 2:\n",
    "                ew = comp_width / N if comp_idx >= 1 else comp_width\n",
    "            else:\n",
    "                ew = comp_width / N\n",
    "\n",
    "            rect = plt.Rectangle(\n",
    "                (x_start, y), ew, 1.0, facecolor=comp_color,\n",
    "                edgecolor='white', linewidth=1.5, alpha=0.85\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            if ew < comp_width:\n",
    "                ghost = plt.Rectangle(\n",
    "                    (x_start, y), comp_width, 1.0,\n",
    "                    facecolor='none', edgecolor=comp_color,\n",
    "                    linewidth=1, linestyle='--', alpha=0.3\n",
    "                )\n",
    "                ax.add_patch(ghost)\n",
    "            x_start += comp_width"
   ],
   "id": "cell_111"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add labels and formatting to each subplot."
   ],
   "id": "cell_112"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ax.text(-0.5, y + 0.5, f'GPU {gpu_id}', ha='right',\n",
    "                va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "    # Total memory per GPU\n",
    "    if ax_idx == 0:\n",
    "        total = 16\n",
    "    elif ax_idx == 1:\n",
    "        total = 4 + 12/N\n",
    "    elif ax_idx == 2:\n",
    "        total = 2 + 14/N\n",
    "    else:\n",
    "        total = 16/N\n",
    "\n",
    "    ax.text(17, 2.25, f'{total:.1f}x\\nper GPU', ha='center', va='center',\n",
    "            fontsize=11, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "    ax.set_xlim(-3, 19)\n",
    "    ax.set_ylim(-0.3, 6.3)\n",
    "    ax.set_title(stage_name, fontsize=11, fontweight='bold', loc='left')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)"
   ],
   "id": "cell_113"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, add a legend and display the complete figure."
   ],
   "id": "cell_114"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#3498db', edgecolor='white',\n",
    "          label='Weights (2 bytes/param)'),\n",
    "    Patch(facecolor='#2ecc71', edgecolor='white',\n",
    "          label='Gradients (2 bytes/param)'),\n",
    "    Patch(facecolor='#e74c3c', edgecolor='white',\n",
    "          label='Optimizer (12 bytes/param)'),\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='lower center', ncol=3,\n",
    "           fontsize=10, bbox_to_anchor=(0.5, -0.02))\n",
    "\n",
    "plt.suptitle('ZeRO: What Each GPU Stores at Each Stage',\n",
    "             fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.06)\n",
    "plt.show()\n",
    "\n",
    "print(\"Solid bars = what this GPU actually stores\")\n",
    "print(\"Dashed outlines = what it would store in vanilla DP\")\n",
    "print(\"ZeRO-3 reduces each GPU's storage to just 1/N of the total!\")"
   ],
   "id": "cell_115"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_29_final_training",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/29_final_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output\n",
    "\n",
    "Let us put everything together into one complete, clean Data Parallel training run. We will:\n",
    "1. Train a model with simulated 4-way Data Parallelism on MNIST\n",
    "2. Show all 4 GPU loss curves converging identically\n",
    "3. Evaluate final accuracy\n",
    "4. Show the ZeRO memory comparison chart"
   ],
   "id": "cell_116"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"  FINAL: Complete Data Parallel Training Simulation\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# ---- Training Configuration ----\n",
    "NUM_GPUS_FINAL = 4\n",
    "NUM_EPOCHS_FINAL = 3\n",
    "LR_FINAL = 0.01\n",
    "GLOBAL_BS = 128\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Virtual GPUs:     {NUM_GPUS_FINAL}\")\n",
    "print(f\"  Epochs:           {NUM_EPOCHS_FINAL}\")\n",
    "print(f\"  Global batch:     {GLOBAL_BS}\")\n",
    "print(f\"  Micro-batch:      {GLOBAL_BS // NUM_GPUS_FINAL} per GPU\")\n",
    "print(f\"  Learning rate:    {LR_FINAL}\")\n",
    "print(f\"  Model:            SimpleMLP (784 -> 256 -> 10)\")\n",
    "print()"
   ],
   "id": "cell_117"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training run with detailed logging\n",
    "base_model_final = SimpleMLP().to(device)\n",
    "models_final = create_virtual_gpus(base_model_final, NUM_GPUS_FINAL)\n",
    "optimizers_final = [optim.SGD(m.parameters(), lr=LR_FINAL)\n",
    "                    for m in models_final]\n",
    "criterion_final = nn.CrossEntropyLoss()\n",
    "\n",
    "# Track metrics\n",
    "all_loss_histories = [[] for _ in range(NUM_GPUS_FINAL)]\n",
    "sync_checks = []\n",
    "step_count = 0\n",
    "\n",
    "print(\"Training...\")\n",
    "print(\"-\" * 50)\n",
    "for epoch in range(NUM_EPOCHS_FINAL):\n",
    "    epoch_losses = [[] for _ in range(NUM_GPUS_FINAL)]\n",
    "\n",
    "    for data, targets in train_loader:\n",
    "        micro_batches = split_batch(data, targets, NUM_GPUS_FINAL)\n",
    "        for i, (model, (d, t)) in enumerate(\n",
    "            zip(models_final, micro_batches)\n",
    "        ):\n",
    "            optimizers_final[i].zero_grad()\n",
    "            d, t = d.to(device), t.to(device)\n",
    "            out = model(d)\n",
    "            loss = criterion_final(out, t)\n",
    "            loss.backward()\n",
    "            all_loss_histories[i].append(loss.item())\n",
    "            epoch_losses[i].append(loss.item())"
   ],
   "id": "cell_118"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing the training loop: AllReduce, weight update, and epoch logging."
   ],
   "id": "cell_119"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # AllReduce\n",
    "        allreduce_gradients(models_final)\n",
    "\n",
    "        # Update\n",
    "        for opt in optimizers_final:\n",
    "            opt.step()\n",
    "\n",
    "        step_count += 1\n",
    "\n",
    "    # Check synchronization\n",
    "    synced = check_weight_sync(models_final)\n",
    "    sync_checks.append(synced)\n",
    "    avg_epoch_loss = np.mean([np.mean(el) for el in epoch_losses])\n",
    "    print(f\"  Epoch {epoch+1}/{NUM_EPOCHS_FINAL} â€” \"\n",
    "          f\"Loss: {avg_epoch_loss:.4f} â€” Synced: {synced}\")\n",
    "\n",
    "print(f\"\\nTotal steps: {step_count}\")\n",
    "print(f\"All epochs synced: {all(sync_checks)}\")"
   ],
   "id": "cell_120"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_30_final_eval",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Final Eval\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/30_final_eval.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we evaluate the final accuracy on the test set. We use GPU 0's model since all models are identical."
   ],
   "id": "cell_121"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy on test set using GPU 0's model\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "models_final[0].eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        output = models_final[0](data)\n",
    "        _, predicted = output.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "accuracy = 100.0 * correct / total\n",
    "print(f\"\\nTest Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"(Using GPU 0's model â€” all GPUs have identical weights)\")"
   ],
   "id": "cell_122"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_31_final_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/31_final_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us create the complete summary visualization with four panels."
   ],
   "id": "cell_123"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Visualization â€” panel 1: loss curves\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "window = 10\n",
    "for i in range(NUM_GPUS_FINAL):\n",
    "    smoothed = np.convolve(all_loss_histories[i],\n",
    "                           np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(smoothed, color=colors[i], label=f'GPU {i}',\n",
    "             linewidth=1.5, alpha=0.8)\n",
    "ax1.set_title('Loss Curves (All 4 GPUs)', fontsize=13, fontweight='bold')\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)"
   ],
   "id": "cell_124"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Panel 2: weight differences across GPUs."
   ],
   "id": "cell_125"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panel 2: Weight difference\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "weight_diffs = []\n",
    "for i in range(1, NUM_GPUS_FINAL):\n",
    "    diff = 0\n",
    "    for p0, pi in zip(models_final[0].parameters(),\n",
    "                      models_final[i].parameters()):\n",
    "        diff += (p0 - pi).abs().max().item()\n",
    "    weight_diffs.append(diff)\n",
    "ax2.bar([f'GPU 0 vs {i}' for i in range(1, NUM_GPUS_FINAL)],\n",
    "        weight_diffs, color=colors[1:], edgecolor='white')\n",
    "ax2.set_title('Final Weight Differences', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel('Max Absolute Difference')\n",
    "ax2.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "ax2.grid(True, axis='y', alpha=0.3)"
   ],
   "id": "cell_126"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Panels 3 and 4: ZeRO memory and text summary."
   ],
   "id": "cell_127"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panel 3: ZeRO memory comparison\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "mem_7b = zero_memory_reference(7, 4)\n",
    "mem_values = list(mem_7b.values())\n",
    "bar_colors = ['#95a5a6', '#3498db', '#2ecc71', '#e74c3c']\n",
    "ax3.bar(range(4), mem_values, color=bar_colors, edgecolor='white')\n",
    "ax3.axhline(y=80, color='red', linestyle='--', alpha=0.5, linewidth=1.5)\n",
    "ax3.text(3.4, 81, 'A100', color='red', fontsize=9)\n",
    "ax3.set_xticks(range(4))\n",
    "ax3.set_xticklabels(['Vanilla\\nDP', 'ZeRO-1', 'ZeRO-2', 'ZeRO-3'],\n",
    "                     fontsize=9)\n",
    "ax3.set_title('ZeRO Memory (7B, 4 GPUs)', fontsize=13, fontweight='bold')\n",
    "ax3.set_ylabel('Per-GPU Memory (GB)')\n",
    "for i, v in enumerate(mem_values):\n",
    "    ax3.text(i, v + 1.5, f'{v:.0f}', ha='center', fontsize=10,\n",
    "             fontweight='bold')\n",
    "ax3.grid(True, axis='y', alpha=0.3)"
   ],
   "id": "cell_128"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Panel 4: text summary of all results."
   ],
   "id": "cell_129"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualization\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "ax4.axis('off')\n",
    "sav = (1 - mem_7b['ZeRO-3 (FSDP)'] / mem_7b['Vanilla DP']) * 100\n",
    "summary_text = (\n",
    "    f\"Data Parallelism Summary\\n\"\n",
    "    f\"{'=' * 35}\\n\\n\"\n",
    "    f\"Virtual GPUs:        {NUM_GPUS_FINAL}\\n\"\n",
    "    f\"Training epochs:     {NUM_EPOCHS_FINAL}\\n\"\n",
    "    f\"Total steps:         {step_count}\\n\"\n",
    "    f\"Final test accuracy: {accuracy:.2f}%\\n\"\n",
    "    f\"Models synchronized: {'Yes' if all(sync_checks) else 'No'}\\n\\n\"\n",
    "    f\"ZeRO-3 memory savings:\\n\"\n",
    "    f\"  Vanilla DP: {mem_7b['Vanilla DP']:.0f} GB/GPU\\n\"\n",
    "    f\"  ZeRO-3:     {mem_7b['ZeRO-3 (FSDP)']:.0f} GB/GPU\\n\"\n",
    "    f\"  Savings:    {sav:.0f}%\"\n",
    ")\n",
    "ax4.text(0.1, 0.95, summary_text, transform=ax4.transAxes,\n",
    "         fontsize=12, verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.9))\n",
    "\n",
    "plt.suptitle('Data Parallelism & ZeRO â€” Complete Results',\n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_130"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_32_summary_print",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Summary Print\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/32_summary_print.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final summary of everything we built in this notebook."
   ],
   "id": "cell_131"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Congratulations! You have implemented Data Parallelism from scratch!\")\n",
    "print()\n",
    "print(\"Here is what you built:\")\n",
    "print(\"  1. Simulated 4 virtual GPUs with independent model copies\")\n",
    "print(\"  2. Split mini-batches across GPUs (128 -> 4x32)\")\n",
    "print(\"  3. Ran independent forward/backward passes\")\n",
    "print(\"  4. Implemented AllReduce gradient averaging\")\n",
    "print(\"  5. Verified all models converge identically\")\n",
    "print(\"  6. Computed ZeRO memory savings (Stages 1-3)\")\n",
    "print(\"  7. Combined DP + ZeRO in a unified demo\")\n",
    "print()\n",
    "print(f\"Final accuracy: {accuracy:.2f}%\")\n",
    "print()\n",
    "print(\"Key takeaways:\")\n",
    "print(\"  - DP scales throughput linearly â€” 4 GPUs ~ 4x faster\")\n",
    "print(\"  - AllReduce ensures all models stay perfectly synchronized\")\n",
    "print(\"  - Vanilla DP wastes memory â€” every GPU stores everything\")\n",
    "print(\"  - ZeRO-3 (FSDP) shards everything -> 75% memory savings\")\n",
    "print()\n",
    "print(\"Next up: Tensor Parallelism â€” splitting the model itself!\")"
   ],
   "id": "cell_132"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_33_reflection",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Reflection\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/33_reflection.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "\n",
    "Before moving on, think carefully about these questions. They test your understanding of the core concepts.\n",
    "\n",
    "1. **Why does DP require AllReduce after every step?** What would happen if we skipped AllReduce for 10 steps and then did it once? Would the models diverge? Would they produce different predictions? Would they eventually reconverge?\n",
    "\n",
    "2. **ZeRO-3 has the lowest memory but highest communication.** When is this trade-off worth it? Consider: What if the GPUs are connected by slow Ethernet instead of fast NVLink? What if the model is 7B vs 175B?\n",
    "\n",
    "3. **If you have 1024 GPUs, does DP alone give you 1024x speedup?** Think about: the micro-batch size per GPU (global batch 1024 â†’ 1 sample per GPU?), the communication overhead of AllReduce at scale, and the effect of very large batch sizes on convergence.\n",
    "\n",
    "4. **Why is the gradient average mathematically equivalent to the full-batch gradient?** Think about: the linearity of expectation, the definition of the loss function as an average over samples, and when this equivalence might break down."
   ],
   "id": "cell_133"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_34_challenges",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Challenges\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/34_challenges.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Challenges\n",
    "\n",
    "If you want to go further, try these extensions:\n",
    "\n",
    "**Challenge 1: Gradient Compression**\n",
    "\n",
    "Implement top-k gradient compression: only communicate the top 10% largest gradient values. This reduces communication volume by 90% at the cost of some convergence speed."
   ],
   "id": "cell_134"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter code for gradient compression\n",
    "def topk_compress(gradient, k_fraction=0.1):\n",
    "    \"\"\"Keep only the top-k% largest gradient values.\"\"\"\n",
    "    flat = gradient.flatten()\n",
    "    k = max(1, int(len(flat) * k_fraction))\n",
    "    # YOUR CODE: Find top-k values, zero out the rest\n",
    "    # Hint: use torch.topk\n",
    "    pass"
   ],
   "id": "cell_135"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 2: Gradient Accumulation**\n",
    "\n",
    "Implement gradient accumulation: accumulate gradients over 4 micro-batches before doing AllReduce. This simulates a larger effective batch size while using less memory per step.\n",
    "\n",
    "**Challenge 3: Communication Volume Tracker**\n",
    "\n",
    "Add instrumentation to the training loop that measures the exact number of bytes communicated in AllReduce at each step. Compare this to the theoretical minimum."
   ],
   "id": "cell_136"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_35_closing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Wrap-Up\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/35_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook is Part 2 of the Vizuara series on 5D Parallelism from Scratch.*\n",
    "\n",
    "*Next: Notebook 3 â€” Tensor Parallelism: Splitting the Model Itself*"
   ],
   "id": "cell_137"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "chatbot"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸ’¬ AI Teaching Assistant â€” Click â–¶ to start\n",
    "#@markdown This AI chatbot reads your notebook and can answer questions about any concept, code, or exercise.\n",
    "\n",
    "import json as _json\n",
    "import requests as _requests\n",
    "from google.colab import output as _output\n",
    "from IPython.display import display, HTML as _HTML, Markdown as _Markdown\n",
    "\n",
    "# --- Read notebook content for context ---\n",
    "def _get_notebook_context():\n",
    "    try:\n",
    "        from google.colab import _message\n",
    "        nb = _message.blocking_request(\"get_ipynb\", request=\"\", timeout_sec=10)\n",
    "        cells = nb.get(\"ipynb\", {}).get(\"cells\", [])\n",
    "        parts = []\n",
    "        for cell in cells:\n",
    "            src = \"\".join(cell.get(\"source\", []))\n",
    "            tags = cell.get(\"metadata\", {}).get(\"tags\", [])\n",
    "            if \"chatbot\" in tags:\n",
    "                continue\n",
    "            if src.strip():\n",
    "                ct = cell.get(\"cell_type\", \"unknown\")\n",
    "                parts.append(f\"[{ct.upper()}]\\n{src}\")\n",
    "        return \"\\n\\n---\\n\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"Notebook content unavailable.\"\n",
    "\n",
    "_NOTEBOOK_CONTEXT = _get_notebook_context()\n",
    "_CHAT_HISTORY = []\n",
    "_API_URL = \"https://course-creator-brown.vercel.app/api/chat\"\n",
    "\n",
    "def _notebook_chat(question):\n",
    "    global _CHAT_HISTORY\n",
    "    try:\n",
    "        resp = _requests.post(_API_URL, json={\n",
    "            'question': question,\n",
    "            'context': _NOTEBOOK_CONTEXT[:100000],\n",
    "            'history': _CHAT_HISTORY[-10:],\n",
    "        }, timeout=60)\n",
    "        data = resp.json()\n",
    "        answer = data.get('answer', 'Sorry, I could not generate a response.')\n",
    "        _CHAT_HISTORY.append({'role': 'user', 'content': question})\n",
    "        _CHAT_HISTORY.append({'role': 'assistant', 'content': answer})\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f'Error connecting to teaching assistant: {str(e)}'\n",
    "\n",
    "_output.register_callback('notebook_chat', _notebook_chat)\n",
    "\n",
    "def ask(question):\n",
    "    \"\"\"Ask the AI teaching assistant a question about this notebook.\"\"\"\n",
    "    answer = _notebook_chat(question)\n",
    "    display(_Markdown(answer))\n",
    "\n",
    "print(\"\\u2705 AI Teaching Assistant is ready!\")\n",
    "print(\"\\U0001f4a1 Use the chat below, or call ask(\\'your question\\') in any cell.\")\n",
    "\n",
    "# --- Display chat widget ---\n",
    "display(_HTML('''<style>\n  .vc-wrap{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;max-width:100%;border-radius:16px;overflow:hidden;box-shadow:0 4px 24px rgba(0,0,0,.12);background:#fff;border:1px solid #e5e7eb}\n  .vc-hdr{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;padding:16px 20px;display:flex;align-items:center;gap:12px}\n  .vc-avatar{width:42px;height:42px;background:rgba(255,255,255,.2);border-radius:50%;display:flex;align-items:center;justify-content:center;font-size:22px}\n  .vc-hdr h3{font-size:16px;font-weight:600;margin:0}\n  .vc-hdr p{font-size:12px;opacity:.85;margin:2px 0 0}\n  .vc-msgs{height:420px;overflow-y:auto;padding:16px;background:#f8f9fb;display:flex;flex-direction:column;gap:10px}\n  .vc-msg{display:flex;flex-direction:column;animation:vc-fade .25s ease}\n  .vc-msg.user{align-items:flex-end}\n  .vc-msg.bot{align-items:flex-start}\n  .vc-bbl{max-width:85%;padding:10px 14px;border-radius:16px;font-size:14px;line-height:1.55;word-wrap:break-word}\n  .vc-msg.user .vc-bbl{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border-bottom-right-radius:4px}\n  .vc-msg.bot .vc-bbl{background:#fff;color:#1a1a2e;border:1px solid #e8e8e8;border-bottom-left-radius:4px}\n  .vc-bbl code{background:rgba(0,0,0,.07);padding:2px 6px;border-radius:4px;font-size:13px;font-family:'Fira Code',monospace}\n  .vc-bbl pre{background:#1e1e2e;color:#cdd6f4;padding:12px;border-radius:8px;overflow-x:auto;margin:8px 0;font-size:13px}\n  .vc-bbl pre code{background:none;padding:0;color:inherit}\n  .vc-bbl h3,.vc-bbl h4{margin:10px 0 4px;font-size:15px}\n  .vc-bbl ul,.vc-bbl ol{margin:4px 0;padding-left:20px}\n  .vc-bbl li{margin:2px 0}\n  .vc-chips{display:flex;flex-wrap:wrap;gap:8px;padding:0 16px 12px;background:#f8f9fb}\n  .vc-chip{background:#fff;border:1px solid #d1d5db;border-radius:20px;padding:6px 14px;font-size:12px;cursor:pointer;transition:all .15s;color:#4b5563}\n  .vc-chip:hover{border-color:#667eea;color:#667eea;background:#f0f0ff}\n  .vc-input{display:flex;padding:12px 16px;background:#fff;border-top:1px solid #eee;gap:8px}\n  .vc-input input{flex:1;padding:10px 16px;border:2px solid #e8e8e8;border-radius:24px;font-size:14px;outline:none;transition:border-color .2s}\n  .vc-input input:focus{border-color:#667eea}\n  .vc-input button{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border:none;border-radius:50%;width:42px;height:42px;cursor:pointer;display:flex;align-items:center;justify-content:center;font-size:18px;transition:transform .1s}\n  .vc-input button:hover{transform:scale(1.05)}\n  .vc-input button:disabled{opacity:.5;cursor:not-allowed;transform:none}\n  .vc-typing{display:flex;gap:5px;padding:4px 0}\n  .vc-typing span{width:8px;height:8px;background:#667eea;border-radius:50%;animation:vc-bounce 1.4s infinite ease-in-out}\n  .vc-typing span:nth-child(2){animation-delay:.2s}\n  .vc-typing span:nth-child(3){animation-delay:.4s}\n  @keyframes vc-bounce{0%,80%,100%{transform:scale(0)}40%{transform:scale(1)}}\n  @keyframes vc-fade{from{opacity:0;transform:translateY(8px)}to{opacity:1;transform:translateY(0)}}\n  .vc-note{text-align:center;font-size:11px;color:#9ca3af;padding:8px 16px 12px;background:#fff}\n</style>\n<div class=\"vc-wrap\">\n  <div class=\"vc-hdr\">\n    <div class=\"vc-avatar\">&#129302;</div>\n    <div>\n      <h3>Vizuara Teaching Assistant</h3>\n      <p>Ask me anything about this notebook</p>\n    </div>\n  </div>\n  <div class=\"vc-msgs\" id=\"vcMsgs\">\n    <div class=\"vc-msg bot\">\n      <div class=\"vc-bbl\">&#128075; Hi! I've read through this entire notebook. Ask me about any concept, code block, or exercise &mdash; I'm here to help you learn!</div>\n    </div>\n  </div>\n  <div class=\"vc-chips\" id=\"vcChips\">\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Explain the main concept</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Help with the TODO exercise</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Summarize what I learned</span>\n  </div>\n  <div class=\"vc-input\">\n    <input type=\"text\" id=\"vcIn\" placeholder=\"Ask about concepts, code, exercises...\" />\n    <button id=\"vcSend\" onclick=\"vcSendMsg()\">&#10148;</button>\n  </div>\n  <div class=\"vc-note\">AI-generated &middot; Verify important information &middot; <a href=\"#\" onclick=\"vcClear();return false\" style=\"color:#667eea\">Clear chat</a></div>\n</div>\n<script>\n(function(){\n  var msgs=document.getElementById('vcMsgs'),inp=document.getElementById('vcIn'),\n      btn=document.getElementById('vcSend'),chips=document.getElementById('vcChips');\n\n  function esc(s){var d=document.createElement('div');d.textContent=s;return d.innerHTML}\n\n  function md(t){\n    return t\n      .replace(/```(\\w*)\\n([\\s\\S]*?)```/g,function(_,l,c){return '<pre><code>'+esc(c)+'</code></pre>'})\n      .replace(/`([^`]+)`/g,'<code>$1</code>')\n      .replace(/\\*\\*([^*]+)\\*\\*/g,'<strong>$1</strong>')\n      .replace(/\\*([^*]+)\\*/g,'<em>$1</em>')\n      .replace(/^#### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^## (.+)$/gm,'<h3>$1</h3>')\n      .replace(/^\\d+\\. (.+)$/gm,'<li>$1</li>')\n      .replace(/^- (.+)$/gm,'<li>$1</li>')\n      .replace(/\\n\\n/g,'<br><br>')\n      .replace(/\\n/g,'<br>');\n  }\n\n  function addMsg(text,isUser){\n    var m=document.createElement('div');m.className='vc-msg '+(isUser?'user':'bot');\n    var b=document.createElement('div');b.className='vc-bbl';\n    b.innerHTML=isUser?esc(text):md(text);\n    m.appendChild(b);msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function showTyping(){\n    var m=document.createElement('div');m.className='vc-msg bot';m.id='vcTyping';\n    m.innerHTML='<div class=\"vc-bbl\"><div class=\"vc-typing\"><span></span><span></span><span></span></div></div>';\n    msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function hideTyping(){var e=document.getElementById('vcTyping');if(e)e.remove()}\n\n  window.vcSendMsg=function(){\n    var q=inp.value.trim();if(!q)return;\n    inp.value='';chips.style.display='none';\n    addMsg(q,true);showTyping();btn.disabled=true;\n    google.colab.kernel.invokeFunction('notebook_chat',[q],{})\n      .then(function(r){\n        hideTyping();\n        var a=r.data['application/json'];\n        addMsg(typeof a==='string'?a:JSON.stringify(a),false);\n      })\n      .catch(function(){\n        hideTyping();\n        addMsg('Sorry, I encountered an error. Please check your internet connection and try again.',false);\n      })\n      .finally(function(){btn.disabled=false;inp.focus()});\n  };\n\n  window.vcAsk=function(q){inp.value=q;vcSendMsg()};\n  window.vcClear=function(){\n    msgs.innerHTML='<div class=\"vc-msg bot\"><div class=\"vc-bbl\">&#128075; Chat cleared. Ask me anything!</div></div>';\n    chips.style.display='flex';\n  };\n\n  inp.addEventListener('keypress',function(e){if(e.key==='Enter')vcSendMsg()});\n  inp.focus();\n})();\n</script>'''))"
   ],
   "id": "vizuara_chatbot"
  }
 ]
}