{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Double DQN and Game Generalization -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double DQN and Game Generalization -- Vizuara\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebook, we trained a DQN agent that learned to play Pong from raw pixels. The algorithm works remarkably well -- but it has a hidden flaw that we have not addressed yet.\n",
    "\n",
    "Standard DQN systematically **overestimates** Q-values. The $\\max$ operator in the target computation is biased upward because the same network both selects and evaluates the best action. Think of it this way: if you ask someone to pick the best option and also judge how good that option is, they will tend to be overconfident.\n",
    "\n",
    "This overestimation does not always prevent learning, but it can cause instability and suboptimal policies, especially in environments with noisy rewards.\n",
    "\n",
    "In 2015, Hado van Hasselt, Arthur Guez, and David Silver published \"Deep Reinforcement Learning with Double Q-Learning,\" which introduced **Double DQN** -- a simple but elegant fix. Instead of using the same network for selection and evaluation, Double DQN uses the online network to **select** the best action and the target network to **evaluate** its value.\n",
    "\n",
    "In this notebook, we will implement Double DQN, demonstrate the overestimation problem, and examine how DQN generalizes across multiple Atari games.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "### The overestimation problem"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Demo: max of noisy estimates is biased upward\n",
    "# True Q-values for 4 actions\n",
    "true_q = np.array([1.0, 1.0, 1.0, 1.0])  # All actions are equally good\n",
    "\n",
    "# But our estimates have noise\n",
    "n_trials = 10000\n",
    "max_estimates = []\n",
    "double_estimates = []\n",
    "\n",
    "for _ in range(n_trials):\n",
    "    # Noisy Q-value estimates (like a neural network with errors)\n",
    "    noise = np.random.normal(0, 1.0, size=4)\n",
    "    estimated_q = true_q + noise\n",
    "\n",
    "    # Standard DQN: max of noisy estimates\n",
    "    max_estimates.append(np.max(estimated_q))\n",
    "\n",
    "    # Double DQN: select with one noise sample, evaluate with another\n",
    "    noise2 = np.random.normal(0, 1.0, size=4)\n",
    "    estimated_q2 = true_q + noise2\n",
    "    best_action = np.argmax(estimated_q)  # Select with first estimate\n",
    "    double_estimates.append(estimated_q2[best_action])  # Evaluate with second\n",
    "\n",
    "print(f\"True max Q-value:     {np.max(true_q):.3f}\")\n",
    "print(f\"Standard DQN (biased): {np.mean(max_estimates):.3f} (+{np.mean(max_estimates) - np.max(true_q):.3f} overestimation)\")\n",
    "print(f\"Double DQN (unbiased): {np.mean(double_estimates):.3f} (+{np.mean(double_estimates) - np.max(true_q):.3f})\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].hist(max_estimates, bins=50, color='#e74c3c', alpha=0.7, label='Standard DQN')\n",
    "axes[0].axvline(x=np.max(true_q), color='black', linestyle='--', linewidth=2, label='True value')\n",
    "axes[0].axvline(x=np.mean(max_estimates), color='#e74c3c', linestyle='-', linewidth=2, label=f'Mean: {np.mean(max_estimates):.2f}')\n",
    "axes[0].set_title('Standard DQN: Overestimates!', fontweight='bold')\n",
    "axes[0].set_xlabel('Estimated max Q-value')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(double_estimates, bins=50, color='#2ecc71', alpha=0.7, label='Double DQN')\n",
    "axes[1].axvline(x=np.max(true_q), color='black', linestyle='--', linewidth=2, label='True value')\n",
    "axes[1].axvline(x=np.mean(double_estimates), color='#2ecc71', linestyle='-', linewidth=2, label=f'Mean: {np.mean(double_estimates):.2f}')\n",
    "axes[1].set_title('Double DQN: Much closer!', fontweight='bold')\n",
    "axes[1].set_xlabel('Estimated max Q-value')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.suptitle('The Overestimation Problem in Q-Learning', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why overestimation matters"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overestimation compounds across multiple steps\n",
    "gamma = 0.99\n",
    "n_steps = 100\n",
    "\n",
    "# Simulate cumulative overestimation\n",
    "overest_per_step = 0.5  # Average overestimation per step\n",
    "\n",
    "cumulative_standard = []\n",
    "cumulative_double = []\n",
    "\n",
    "for t in range(n_steps):\n",
    "    # Standard DQN: overestimation accumulates with discount\n",
    "    cum_standard = sum(overest_per_step * gamma**i for i in range(t+1))\n",
    "    cumulative_standard.append(cum_standard)\n",
    "\n",
    "    # Double DQN: much smaller overestimation\n",
    "    cum_double = sum(0.05 * gamma**i for i in range(t+1))\n",
    "    cumulative_double.append(cum_double)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(cumulative_standard, color='#e74c3c', linewidth=2, label='Standard DQN')\n",
    "plt.plot(cumulative_double, color='#2ecc71', linewidth=2, label='Double DQN')\n",
    "plt.xlabel('Planning Horizon (steps)', fontsize=12)\n",
    "plt.ylabel('Cumulative Overestimation', fontsize=12)\n",
    "plt.title('Overestimation Compounds Over Time', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"After {n_steps} steps:\")\n",
    "print(f\"  Standard DQN overestimation: {cumulative_standard[-1]:.2f}\")\n",
    "print(f\"  Double DQN overestimation:   {cumulative_double[-1]:.2f}\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. The Mathematics\n",
    "\n",
    "### Standard DQN target\n",
    "\n",
    "$$y_{\\text{DQN}} = r + \\gamma \\max_{a'} Q(s', a'; \\theta^-)$$\n",
    "\n",
    "The problem: $\\max_{a'}$ both **selects** and **evaluates** using the target network. If the target network overestimates any action's value, $\\max$ will always pick that overestimation.\n",
    "\n",
    "### Double DQN target\n",
    "\n",
    "$$y_{\\text{DDQN}} = r + \\gamma Q\\left(s', \\arg\\max_{a'} Q(s', a'; \\theta),\\; \\theta^-\\right)$$\n",
    "\n",
    "The fix: use the **online network** $\\theta$ to select the best action, but the **target network** $\\theta^-$ to evaluate it. This decouples selection from evaluation."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison\n",
    "print(\"=\" * 60)\n",
    "print(\"Standard DQN vs Double DQN -- The Key Difference\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Standard DQN target:\")\n",
    "print(\"  1. Ask TARGET network: what is max Q(s', a')?\")\n",
    "print(\"     -> Target both SELECTS and EVALUATES the best action\")\n",
    "print(\"     -> Biased upward because max of noisy estimates > true max\")\n",
    "print()\n",
    "print(\"Double DQN target:\")\n",
    "print(\"  1. Ask ONLINE network: which action a* is best?\")\n",
    "print(\"     -> a* = argmax Q(s', a'; theta)\")\n",
    "print(\"  2. Ask TARGET network: what is Q(s', a*)?\")\n",
    "print(\"     -> Evaluate a* using target network theta^-\")\n",
    "print(\"     -> Selection and evaluation are independent\")\n",
    "print()\n",
    "print(\"The change is literally ONE line of code!\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worked example\n",
    "print(\"\\n--- Worked Example ---\")\n",
    "print()\n",
    "\n",
    "# Online network predictions for next state\n",
    "q_online_next = torch.tensor([2.5, 3.1, 1.8, 2.9])\n",
    "# Target network predictions for next state\n",
    "q_target_next = torch.tensor([2.3, 2.8, 1.9, 3.2])\n",
    "r = 1.0\n",
    "gamma = 0.99\n",
    "\n",
    "# Standard DQN\n",
    "max_target_q = q_target_next.max().item()\n",
    "y_dqn = r + gamma * max_target_q\n",
    "print(f\"Standard DQN:\")\n",
    "print(f\"  Target net Q-values: {q_target_next.numpy()}\")\n",
    "print(f\"  max Q(s', a'; theta^-) = {max_target_q:.2f} (action {q_target_next.argmax().item()})\")\n",
    "print(f\"  y_DQN = {r} + {gamma} * {max_target_q:.2f} = {y_dqn:.4f}\")\n",
    "\n",
    "# Double DQN\n",
    "best_action_online = q_online_next.argmax().item()\n",
    "eval_target = q_target_next[best_action_online].item()\n",
    "y_ddqn = r + gamma * eval_target\n",
    "print(f\"\\nDouble DQN:\")\n",
    "print(f\"  Online net Q-values:  {q_online_next.numpy()}\")\n",
    "print(f\"  Best action (online): {best_action_online} (Q = {q_online_next[best_action_online]:.2f})\")\n",
    "print(f\"  Target net evaluates action {best_action_online}: Q = {eval_target:.2f}\")\n",
    "print(f\"  y_DDQN = {r} + {gamma} * {eval_target:.2f} = {y_ddqn:.4f}\")\n",
    "\n",
    "print(f\"\\nDifference: {y_dqn - y_ddqn:.4f} (DQN target is higher)\")\n",
    "print(\"This small difference per step accumulates over training!\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### Step 1: Standard vs Double DQN loss"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512), nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.fc(x.view(x.size(0), -1))\n",
    "\n",
    "\n",
    "def compute_loss_standard(online_net, target_net, states, actions, rewards, next_states, dones, gamma=0.99):\n",
    "    \"\"\"Standard DQN loss.\"\"\"\n",
    "    q_values = online_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Standard: max from target network\n",
    "        max_next_q = target_net(next_states).max(1)[0]\n",
    "        target = rewards + gamma * max_next_q * (~dones).float()\n",
    "\n",
    "    return F.mse_loss(q_values, target)\n",
    "\n",
    "\n",
    "def compute_loss_double(online_net, target_net, states, actions, rewards, next_states, dones, gamma=0.99):\n",
    "    \"\"\"Double DQN loss -- the only change is in target computation.\"\"\"\n",
    "    q_values = online_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Double DQN: SELECT with online, EVALUATE with target\n",
    "        best_actions = online_net(next_states).argmax(1)  # <-- online selects\n",
    "        next_q = target_net(next_states).gather(1, best_actions.unsqueeze(1)).squeeze(1)  # <-- target evaluates\n",
    "        target = rewards + gamma * next_q * (~dones).float()\n",
    "\n",
    "    return F.mse_loss(q_values, target)\n",
    "\n",
    "\n",
    "# Compare the two on identical data\n",
    "n_actions = 4\n",
    "online = DQN(n_actions).to(device)\n",
    "target = DQN(n_actions).to(device)\n",
    "target.load_state_dict(online.state_dict())\n",
    "\n",
    "# Fake batch\n",
    "batch_size = 32\n",
    "s = torch.randn(batch_size, 4, 84, 84).to(device)\n",
    "a = torch.randint(0, n_actions, (batch_size,)).to(device)\n",
    "r = torch.randn(batch_size).to(device)\n",
    "ns = torch.randn(batch_size, 4, 84, 84).to(device)\n",
    "d = torch.zeros(batch_size, dtype=torch.bool).to(device)\n",
    "\n",
    "loss_std = compute_loss_standard(online, target, s, a, r, ns, d)\n",
    "loss_dbl = compute_loss_double(online, target, s, a, r, ns, d)\n",
    "\n",
    "print(f\"Standard DQN loss: {loss_std.item():.4f}\")\n",
    "print(f\"Double DQN loss:   {loss_dbl.item():.4f}\")\n",
    "print(f\"\\nWith identical weights, both losses are similar.\")\n",
    "print(\"The difference emerges during training as networks diverge.\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Tracking Q-value estimates during training"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class QValueTracker:\n",
    "    \"\"\"Track Q-value estimates over training to detect overestimation.\"\"\"\n",
    "    def __init__(self, n_eval_states=100):\n",
    "        self.eval_states = None\n",
    "        self.n_eval_states = n_eval_states\n",
    "        self.q_history = []\n",
    "\n",
    "    def set_eval_states(self, replay_buffer):\n",
    "        \"\"\"Sample fixed evaluation states from the buffer.\"\"\"\n",
    "        if len(replay_buffer) >= self.n_eval_states:\n",
    "            batch = replay_buffer.sample(self.n_eval_states)\n",
    "            self.eval_states = batch[0]\n",
    "\n",
    "    def record(self, online_net):\n",
    "        \"\"\"Record average Q-value on evaluation states.\"\"\"\n",
    "        if self.eval_states is None:\n",
    "            return\n",
    "        with torch.no_grad():\n",
    "            q = online_net(self.eval_states.to(device))\n",
    "            avg_q = q.max(1)[0].mean().item()\n",
    "            self.q_history.append(avg_q)\n",
    "\n",
    "tracker_standard = QValueTracker()\n",
    "tracker_double = QValueTracker()\n",
    "print(\"Q-value tracker ready. Will use during training comparison.\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Implement Dueling DQN architecture\n",
    "\n",
    "Double DQN changes the loss computation. **Dueling DQN** changes the architecture. Instead of outputting Q-values directly, it separates the value function $V(s)$ and advantage function $A(s,a)$:\n",
    "\n",
    "$$Q(s, a) = V(s) + A(s, a) - \\frac{1}{|A|}\\sum_{a'} A(s, a')$$"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Implement the Dueling DQN architecture\n# The scaffolding is provided -- fill in the stream definitions and forward pass.\n\nclass DuelingDQN(nn.Module):\n    def __init__(self, n_actions):\n        super().__init__()\n        # Same convolutional backbone\n        self.conv = nn.Sequential(\n            nn.Conv2d(4, 32, kernel_size=8, stride=4), nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU()\n        )\n\n        # Value stream: 3136 -> 512 -> 1 (single state value)\n        self.value_stream = nn.Sequential(\n            nn.Linear(64 * 7 * 7, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1)\n        )\n\n        # Advantage stream: 3136 -> 512 -> n_actions (per-action advantage)\n        self.advantage_stream = nn.Sequential(\n            nn.Linear(64 * 7 * 7, 512),\n            nn.ReLU(),\n            nn.Linear(512, n_actions)\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.view(x.size(0), -1)\n\n        value = self.value_stream(x)            # (batch, 1)\n        advantage = self.advantage_stream(x)    # (batch, n_actions)\n        # Combine: Q = V + A - mean(A)\n        q = value + advantage - advantage.mean(dim=1, keepdim=True)\n        return q\n\n# Test: DuelingDQN should have the same input/output shape as DQN\ndueling = DuelingDQN(4).to(device)\ntest = torch.randn(1, 4, 84, 84).to(device)\nprint(f\"Dueling DQN output: {dueling(test).shape}\")  # Should be [1, 4]\n\n# Compare parameter counts\ndqn_params = sum(p.numel() for p in DQN(4).parameters())\ndueling_params = sum(p.numel() for p in dueling.parameters())\nprint(f\"Standard DQN params: {dqn_params:,}\")\nprint(f\"Dueling DQN params:  {dueling_params:,}\")\nprint(f\"Dueling has two separate FC streams -> more parameters\")",
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Compare overestimation empirically"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train both standard and double DQN on the substitute environment\n",
    "# and compare their Q-value estimates over time.\n",
    "#\n",
    "# 1. Train standard DQN for 200 episodes, recording avg Q-values every 10 episodes\n",
    "# 2. Train Double DQN for 200 episodes, recording avg Q-values every 10 episodes\n",
    "# 3. Plot both Q-value curves on the same graph\n",
    "# 4. The standard DQN curve should be consistently ABOVE the Double DQN curve\n",
    "#    (overestimation)\n",
    "#\n",
    "# YOUR CODE HERE"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Putting It All Together\n",
    "\n",
    "### DQN performance across Atari games"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce the key results from the DQN Nature paper\n",
    "# Data from Mnih et al. (2015), Table 1\n",
    "\n",
    "games_data = {\n",
    "    'Breakout':           {'human': 31.8,  'dqn': 401.2, 'category': 'superhuman'},\n",
    "    'Pong':               {'human': 9.3,   'dqn': 18.9,  'category': 'superhuman'},\n",
    "    'Space Invaders':     {'human': 1652,  'dqn': 1976,  'category': 'superhuman'},\n",
    "    'Boxing':             {'human': 4.3,   'dqn': 71.8,  'category': 'superhuman'},\n",
    "    'Enduro':             {'human': 309.6, 'dqn': 475.6, 'category': 'superhuman'},\n",
    "    'Beam Rider':         {'human': 7456,  'dqn': 8627,  'category': 'superhuman'},\n",
    "    'Seaquest':           {'human': 20182, 'dqn': 5286,  'category': 'below'},\n",
    "    \"Montezuma's Rev.\":   {'human': 4367,  'dqn': 0,     'category': 'fails'},\n",
    "    'Private Eye':        {'human': 69571, 'dqn': 146,   'category': 'fails'},\n",
    "}\n",
    "\n",
    "# Compute DQN as % of human\n",
    "games = list(games_data.keys())\n",
    "pct_human = []\n",
    "colors = []\n",
    "for g in games:\n",
    "    d = games_data[g]\n",
    "    if d['human'] > 0:\n",
    "        pct = (d['dqn'] / d['human']) * 100\n",
    "    else:\n",
    "        pct = 100\n",
    "    pct_human.append(min(pct, 1500))  # Cap for visualization\n",
    "    if d['category'] == 'superhuman':\n",
    "        colors.append('#2ecc71')\n",
    "    elif d['category'] == 'below':\n",
    "        colors.append('#f39c12')\n",
    "    else:\n",
    "        colors.append('#e74c3c')\n",
    "\n",
    "# Sort by performance\n",
    "sorted_indices = np.argsort(pct_human)[::-1]\n",
    "games_sorted = [games[i] for i in sorted_indices]\n",
    "pct_sorted = [pct_human[i] for i in sorted_indices]\n",
    "colors_sorted = [colors[i] for i in sorted_indices]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.barh(range(len(games_sorted)), pct_sorted, color=colors_sorted, edgecolor='white', height=0.7)\n",
    "ax.axvline(x=100, color='black', linestyle='--', linewidth=2, label='Human level (100%)')\n",
    "ax.set_yticks(range(len(games_sorted)))\n",
    "ax.set_yticklabels(games_sorted, fontsize=11)\n",
    "ax.set_xlabel('DQN Score as % of Human Performance', fontsize=12)\n",
    "ax.set_title('DQN Performance Across Atari Games (Mnih et al., 2015)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (bar, pct) in enumerate(zip(bars, pct_sorted)):\n",
    "    ax.text(bar.get_width() + 10, bar.get_y() + bar.get_height()/2,\n",
    "            f'{pct:.0f}%', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"  Superhuman (green): Games requiring pattern recognition and reactive play\")\n",
    "print(\"  Below human (orange): Games with complex reward structures\")\n",
    "print(\"  Fails (red): Games requiring long-term memory and sparse rewards\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why DQN fails on some games\n",
    "print(\"=\" * 60)\n",
    "print(\"WHY DQN FAILS ON MONTEZUMA'S REVENGE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "Montezuma's Revenge requires:\n",
    "  1. Long sequences of actions before any reward (sparse rewards)\n",
    "  2. Remembering which rooms were visited (long-term memory)\n",
    "  3. Picking up specific keys to open specific doors (planning)\n",
    "\n",
    "DQN's epsilon-greedy exploration is purely random.\n",
    "The probability of randomly discovering the correct sequence\n",
    "of 100+ actions to reach the first reward is astronomically low.\n",
    "\n",
    "This led to research on:\n",
    "  - Curiosity-driven exploration (intrinsic motivation)\n",
    "  - Hierarchical RL (options, sub-goals)\n",
    "  - Go-Explore (archive-based exploration)\n",
    "  - Return-conditioned decision transformers\n",
    "\"\"\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Training and Results\n",
    "\n",
    "### The DQN family -- improvements over original"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the evolution of DQN improvements\n",
    "improvements = {\n",
    "    'DQN (2013)':                   {'score_pct': 121, 'year': 2013},\n",
    "    'Double DQN (2015)':            {'score_pct': 149, 'year': 2015},\n",
    "    'Prioritized Replay (2015)':    {'score_pct': 192, 'year': 2015},\n",
    "    'Dueling DQN (2016)':           {'score_pct': 175, 'year': 2016},\n",
    "    'A3C (2016)':                   {'score_pct': 116, 'year': 2016},\n",
    "    'NoisyNet (2017)':              {'score_pct': 218, 'year': 2017},\n",
    "    'Distributional (2017)':        {'score_pct': 315, 'year': 2017},\n",
    "    'Rainbow (2017)':               {'score_pct': 427, 'year': 2017},\n",
    "}\n",
    "\n",
    "names = list(improvements.keys())\n",
    "scores = [improvements[n]['score_pct'] for n in names]\n",
    "years = [improvements[n]['year'] for n in names]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.9, len(names)))\n",
    "bars = ax.bar(range(len(names)), scores, color=colors, edgecolor='white', width=0.7)\n",
    "ax.axhline(y=100, color='red', linestyle='--', linewidth=2, label='Human level')\n",
    "ax.set_xticks(range(len(names)))\n",
    "ax.set_xticklabels(names, rotation=45, ha='right', fontsize=10)\n",
    "ax.set_ylabel('Median Score (% of Human)', fontsize=12)\n",
    "ax.set_title('Evolution of DQN: Each Improvement Builds on the Last', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, score in zip(bars, scores):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 5,\n",
    "            f'{score}%', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRainbow (2017) combines ALL improvements:\")\n",
    "print(\"  1. Double DQN (reduces overestimation)\")\n",
    "print(\"  2. Prioritized replay (focus on important transitions)\")\n",
    "print(\"  3. Dueling architecture (separate value and advantage)\")\n",
    "print(\"  4. Multi-step returns (n-step bootstrapping)\")\n",
    "print(\"  5. Distributional RL (predict return distribution)\")\n",
    "print(\"  6. NoisyNets (learned exploration)\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Final Output"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"COURSE SUMMARY: Building DQN Atari Agents\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Across these 4 notebooks, we built DQN from scratch:\n",
    "\n",
    "  Notebook 1: The DQN Architecture\n",
    "    - Function approximation: Q-table -> neural network\n",
    "    - CNN processes 4 stacked 84x84 grayscale frames\n",
    "    - Outputs Q-values for every possible action\n",
    "\n",
    "  Notebook 2: Experience Replay and Target Networks\n",
    "    - Replay buffer breaks temporal correlations\n",
    "    - Target network provides stable training targets\n",
    "    - Both are essential for training stability\n",
    "\n",
    "  Notebook 3: Complete Training Pipeline\n",
    "    - Epsilon-greedy with annealing\n",
    "    - Full training loop on Pong\n",
    "    - Agent learns from raw pixels to win games\n",
    "\n",
    "  Notebook 4: Double DQN and Generalization\n",
    "    - Standard DQN overestimates Q-values\n",
    "    - Double DQN: separate selection and evaluation\n",
    "    - DQN excels at reactive play, struggles with planning\n",
    "    - Rainbow combines all improvements\n",
    "\n",
    "Historical Significance:\n",
    "  - DQN (2013) was the first successful deep RL agent\n",
    "  - Same architecture worked on 49 different games\n",
    "  - Led to Google's acquisition of DeepMind for ~\\\\$500M\n",
    "  - Opened the door to AlphaGo, robotics, and modern RLHF\n",
    "\n",
    "These same principles -- function approximation, replay buffers,\n",
    "target networks -- are used in virtually every modern deep RL system.\n",
    "\"\"\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**What we built:**\n",
    "- Double DQN loss computation (one line change from standard DQN)\n",
    "- Visualization of the overestimation problem\n",
    "- Analysis of DQN performance across Atari games\n",
    "- Survey of the DQN improvement family (Rainbow)\n",
    "\n",
    "**Key insights:**\n",
    "1. The $\\max$ operator causes systematic overestimation of Q-values\n",
    "2. Double DQN decouples action selection from value evaluation\n",
    "3. DQN excels at reactive pattern recognition but fails at long-horizon planning\n",
    "4. Rainbow combines 6 improvements for state-of-the-art performance\n",
    "\n",
    "**The bigger picture:**\n",
    "- DQN showed that a single neural network can learn directly from pixels\n",
    "- The three core ideas (CNNs for function approximation, experience replay, target networks) remain foundational in modern deep RL\n",
    "- The limitations of DQN (sparse rewards, planning) motivated entire subfields: curiosity-driven exploration, hierarchical RL, and model-based RL\n",
    "\n",
    "**Where to go next:**\n",
    "- Policy gradient methods (REINFORCE, PPO) -- a fundamentally different approach to RL\n",
    "- Actor-critic methods that combine value-based and policy-based learning\n",
    "- Model-based RL where the agent learns a world model and plans inside it"
   ],
   "id": "cell_23"
  }
 ]
}