{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Building DQN Atari Agents -- Notebook Index"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building DQN Atari Agents -- Notebook Index\n",
    "\n",
    "## Vizuara AI\n",
    "\n",
    "Welcome to the **Building DQN Atari Agents** notebook series. This set of 4 interactive notebooks takes you from the fundamental limitation of tabular Q-learning all the way to training a Deep Q-Network that plays Atari games from raw pixels.\n",
    "\n",
    "---\n",
    "\n",
    "## Notebooks\n",
    "\n",
    "### 01 -- From Tables to Neural Networks: The DQN Architecture\n",
    "**What you will build:** The DQN convolutional neural network that maps raw pixel frames to Q-values.\n",
    "\n",
    "Key topics:\n",
    "- Why Q-tables fail for large state spaces\n",
    "- Function approximation with neural networks\n",
    "- The DQN CNN architecture (Conv layers, dimensions, forward pass)\n",
    "- Atari preprocessing pipeline (grayscale, resize, frame stacking)\n",
    "\n",
    "### 02 -- Experience Replay and Target Networks\n",
    "**What you will build:** The two innovations that make training deep Q-networks stable.\n",
    "\n",
    "Key topics:\n",
    "- Why correlated data causes catastrophic forgetting\n",
    "- Experience replay buffer implementation\n",
    "- The moving target problem and target networks\n",
    "- DQN loss function with TD error computation\n",
    "\n",
    "### 03 -- Training a DQN Agent to Play Pong\n",
    "**What you will build:** A complete end-to-end DQN training pipeline.\n",
    "\n",
    "Key topics:\n",
    "- Epsilon-greedy exploration with annealing\n",
    "- Complete training loop with all components\n",
    "- Training on Pong from raw pixels\n",
    "- Evaluation and visualization of learned behavior\n",
    "\n",
    "### 04 -- Double DQN and Game Generalization\n",
    "**What you will build:** Double DQN to fix overestimation bias, plus analysis across games.\n",
    "\n",
    "Key topics:\n",
    "- The Q-value overestimation problem\n",
    "- Double DQN: decoupling selection and evaluation\n",
    "- DQN performance across 49 Atari games\n",
    "- The DQN family: Dueling DQN, Prioritized Replay, Rainbow\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "- Basic understanding of reinforcement learning (states, actions, rewards)\n",
    "- Familiarity with Q-learning (Q-values, Bellman equation)\n",
    "- Basic PyTorch (tensors, nn.Module, gradient descent)\n",
    "\n",
    "## Environment\n",
    "- Google Colab with T4 GPU\n",
    "- All notebooks are self-contained with pip installs included\n",
    "- Each notebook runs in under 10 minutes\n",
    "\n",
    "## References\n",
    "1. Mnih, V., et al. (2013). \"Playing Atari with Deep Reinforcement Learning.\" arXiv:1312.5602.\n",
    "2. Mnih, V., et al. (2015). \"Human-level control through deep reinforcement learning.\" Nature, 518(7540), 529-533.\n",
    "3. van Hasselt, H., Guez, A., Silver, D. (2015). \"Deep Reinforcement Learning with Double Q-Learning.\" arXiv:1509.06461."
   ],
   "id": "cell_1"
  }
 ]
}