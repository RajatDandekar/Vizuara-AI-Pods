{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Training a DQN Agent to Play Pong -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a DQN Agent to Play Pong -- Vizuara\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the first two notebooks, we built every component of the DQN algorithm: the convolutional neural network (Notebook 1), the replay buffer, and the target network (Notebook 2). Now it is time to put them all together and train an agent to play an actual Atari game.\n",
    "\n",
    "We will train a DQN agent on **Pong** -- the classic game where two paddles hit a ball back and forth. The agent controls one paddle, and the built-in game AI controls the other. The score ranges from -21 (complete loss) to +21 (complete win). A random agent scores about -21 because it almost never hits the ball.\n",
    "\n",
    "By the end of this notebook, you will have a fully trained DQN agent that learns to play Pong from raw pixels -- no hand-crafted features, no game-specific knowledge. The same code you write here is essentially what DeepMind used in their groundbreaking 2013 paper.\n",
    "\n",
    "We will use a lightweight environment wrapper to keep training under 10 minutes on a T4 GPU.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "### The epsilon-greedy exploration schedule\n",
    "\n",
    "Before we start training, let us understand how the agent explores. DQN uses epsilon-greedy exploration with **annealing**: epsilon starts high (fully random) and decreases over time (mostly greedy)."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "# Epsilon schedule\n",
    "def get_epsilon(step, eps_start=1.0, eps_end=0.1, decay_steps=100000):\n",
    "    \"\"\"Linear epsilon decay.\"\"\"\n",
    "    return max(eps_end, eps_start - step * (eps_start - eps_end) / decay_steps)\n",
    "\n",
    "# Visualize the schedule\n",
    "steps = np.arange(0, 200000)\n",
    "epsilons = [get_epsilon(s) for s in steps]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(steps, epsilons, color='#3498db', linewidth=2)\n",
    "plt.axvline(x=100000, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.text(50000, 0.7, 'Exploration phase\\n(mostly random)', ha='center', fontsize=12, color='#e74c3c')\n",
    "plt.text(150000, 0.2, 'Exploitation phase\\n(mostly greedy)', ha='center', fontsize=12, color='#2ecc71')\n",
    "plt.xlabel('Training Steps', fontsize=12)\n",
    "plt.ylabel('Epsilon', fontsize=12)\n",
    "plt.title('Epsilon-Greedy Annealing Schedule', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Concrete examples\n",
    "for t in [0, 25000, 50000, 75000, 100000, 150000]:\n",
    "    eps = get_epsilon(t)\n",
    "    print(f\"  Step {t:>7,}: epsilon = {eps:.3f} ({eps*100:.1f}% random, {(1-eps)*100:.1f}% greedy)\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Pong works for the agent"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us understand the Pong action space\n",
    "print(\"Pong Action Space:\")\n",
    "print(\"  0: NOOP (do nothing)\")\n",
    "print(\"  1: FIRE (start the game / not used during play)\")\n",
    "print(\"  2: UP   (move paddle up)\")\n",
    "print(\"  3: DOWN (move paddle down)\")\n",
    "print(\"  4: UP + FIRE\")\n",
    "print(\"  5: DOWN + FIRE\")\n",
    "print()\n",
    "print(\"Effective actions: UP, DOWN, NOOP\")\n",
    "print(\"The agent learns which action maximizes long-term score.\")\n",
    "print()\n",
    "print(\"Reward signal:\")\n",
    "print(\"  +1 when the agent scores a point\")\n",
    "print(\"  -1 when the opponent scores a point\")\n",
    "print(\"   0 at all other time steps\")\n",
    "print()\n",
    "print(\"A random agent scores about -21 (loses every rally).\")\n",
    "print(\"A trained DQN agent typically reaches +18 to +21 (wins almost every rally).\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. The Mathematics\n",
    "\n",
    "The complete DQN algorithm combines everything we have built:\n",
    "\n",
    "**Epsilon-greedy policy:**\n",
    "\n",
    "$$a = \\begin{cases} \\text{random action} & \\text{with probability } \\epsilon \\\\ \\arg\\max_a Q(s, a; \\theta) & \\text{with probability } 1 - \\epsilon \\end{cases}$$\n",
    "\n",
    "**Loss function:**\n",
    "\n",
    "$$L(\\theta) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\left[ \\left( r + \\gamma \\max_{a'} Q(s', a'; \\theta^-) - Q(s, a; \\theta) \\right)^2 \\right]$$\n",
    "\n",
    "**Epsilon annealing:**\n",
    "\n",
    "$$\\epsilon(t) = \\max\\left(\\epsilon_{\\text{end}},\\; \\epsilon_{\\text{start}} - t \\cdot \\frac{\\epsilon_{\\text{start}} - \\epsilon_{\\text{end}}}{\\text{decay\\_steps}}\\right)$$"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the hyperparameter table from the DQN paper\n",
    "print(\"=\" * 55)\n",
    "print(\"DQN Hyperparameters (Mnih et al., 2015)\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "hyperparams = {\n",
    "    \"Replay buffer size\":      \"1,000,000 (we use 50,000)\",\n",
    "    \"Mini-batch size\":         \"32\",\n",
    "    \"Discount factor (gamma)\": \"0.99\",\n",
    "    \"Learning rate\":           \"0.00025 (RMSProp) / 1e-4 (Adam)\",\n",
    "    \"Epsilon start\":           \"1.0\",\n",
    "    \"Epsilon end\":             \"0.1\",\n",
    "    \"Epsilon decay frames\":    \"1,000,000 (we use 50,000)\",\n",
    "    \"Target update freq (C)\":  \"10,000 (we use 1,000)\",\n",
    "    \"Training start\":          \"50,000 frames (we use 1,000)\",\n",
    "    \"Frame skip\":              \"4\",\n",
    "    \"Frame stack\":             \"4\",\n",
    "}\n",
    "\n",
    "for k, v in hyperparams.items():\n",
    "    print(f\"  {k:30s} {v}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### Step 1: All components"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512), nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        s, a, r, ns, d = zip(*batch)\n",
    "        return (torch.stack(s), torch.tensor(a, dtype=torch.long),\n",
    "                torch.tensor(r, dtype=torch.float32),\n",
    "                torch.stack(ns), torch.tensor(d, dtype=torch.bool))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "print(\"DQN, ReplayBuffer defined. Ready to train.\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Environment setup"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install gymnasium with Atari support\n",
    "import subprocess\n",
    "subprocess.run(['pip', 'install', '-q', 'gymnasium[atari]', 'gymnasium[accept-rom-license]'],\n",
    "               capture_output=True)\n",
    "\n",
    "import gymnasium as gym\n",
    "import cv2\n",
    "\n",
    "class AtariWrapper:\n",
    "    \"\"\"\n",
    "    Wraps an Atari environment with DQN preprocessing.\n",
    "    - Frame skip (repeat action N times)\n",
    "    - Grayscale + resize to 84x84\n",
    "    - Stack 4 frames\n",
    "    - Clip rewards to {-1, 0, +1}\n",
    "    \"\"\"\n",
    "    def __init__(self, env_name=\"PongNoFrameskip-v4\", frame_skip=4, stack_size=4):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.frame_skip = frame_skip\n",
    "        self.stack_size = stack_size\n",
    "        self.frames = deque(maxlen=stack_size)\n",
    "        self.n_actions = self.env.action_space.n\n",
    "\n",
    "    def _preprocess(self, frame):\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        resized = cv2.resize(gray, (84, 84))\n",
    "        return resized.astype(np.float32) / 255.0\n",
    "\n",
    "    def reset(self):\n",
    "        obs, info = self.env.reset()\n",
    "        frame = self._preprocess(obs)\n",
    "        for _ in range(self.stack_size):\n",
    "            self.frames.append(frame)\n",
    "        return torch.tensor(np.array(list(self.frames)))\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for _ in range(self.frame_skip):\n",
    "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "            if done:\n",
    "                break\n",
    "        frame = self._preprocess(obs)\n",
    "        self.frames.append(frame)\n",
    "        state = torch.tensor(np.array(list(self.frames)))\n",
    "        # Clip reward\n",
    "        clipped_reward = np.clip(total_reward, -1.0, 1.0)\n",
    "        return state, clipped_reward, done\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "# Create the environment\n",
    "try:\n",
    "    env = AtariWrapper(\"PongNoFrameskip-v4\")\n",
    "    state = env.reset()\n",
    "    print(f\"Environment: PongNoFrameskip-v4\")\n",
    "    print(f\"State shape: {state.shape}\")\n",
    "    print(f\"Number of actions: {env.n_actions}\")\n",
    "\n",
    "    # Show what the agent sees\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    for i in range(4):\n",
    "        axes[i].imshow(state[i].numpy(), cmap='gray')\n",
    "        axes[i].set_title(f'Frame {i+1}')\n",
    "        axes[i].axis('off')\n",
    "    plt.suptitle('What the DQN agent sees: 4 stacked frames', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    env.close()\n",
    "    ATARI_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(f\"Atari environment not available: {e}\")\n",
    "    print(\"We will use a substitute environment for training.\")\n",
    "    ATARI_AVAILABLE = False"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Substitute environment (fallback)"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PongSubstitute:\n",
    "    \"\"\"\n",
    "    A simplified Pong-like environment for when Atari ROMs are unavailable.\n",
    "    Ball bounces, paddle moves up/down. Similar reward structure.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.n_actions = 3  # 0=stay, 1=up, 2=down\n",
    "        self.size = 84\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.paddle_y = 42\n",
    "        self.ball_x = 42\n",
    "        self.ball_y = 42\n",
    "        self.ball_dx = 2\n",
    "        self.ball_dy = random.choice([-1, 0, 1])\n",
    "        self.opponent_y = 42\n",
    "        self.score = 0\n",
    "        self.steps = 0\n",
    "        self.frames = deque(maxlen=4)\n",
    "        frame = self._render()\n",
    "        for _ in range(4):\n",
    "            self.frames.append(frame)\n",
    "        return torch.tensor(np.array(list(self.frames)))\n",
    "\n",
    "    def _render(self):\n",
    "        img = np.zeros((84, 84), dtype=np.float32)\n",
    "        # Agent paddle (right side)\n",
    "        py = max(0, min(self.size-12, self.paddle_y - 6))\n",
    "        img[py:py+12, 78:80] = 1.0\n",
    "        # Opponent paddle (left side)\n",
    "        oy = max(0, min(self.size-12, self.opponent_y - 6))\n",
    "        img[oy:oy+12, 4:6] = 1.0\n",
    "        # Ball\n",
    "        bx = max(0, min(self.size-3, int(self.ball_x)))\n",
    "        by = max(0, min(self.size-3, int(self.ball_y)))\n",
    "        img[by:by+3, bx:bx+3] = 1.0\n",
    "        # Center line\n",
    "        img[::4, 42] = 0.3\n",
    "        return img\n",
    "\n",
    "    def step(self, action):\n",
    "        # Move paddle\n",
    "        if action == 1:\n",
    "            self.paddle_y = max(6, self.paddle_y - 3)\n",
    "        elif action == 2:\n",
    "            self.paddle_y = min(78, self.paddle_y + 3)\n",
    "\n",
    "        # Move ball\n",
    "        self.ball_x += self.ball_dx\n",
    "        self.ball_y += self.ball_dy\n",
    "\n",
    "        # Bounce off top/bottom\n",
    "        if self.ball_y <= 1 or self.ball_y >= 82:\n",
    "            self.ball_dy = -self.ball_dy\n",
    "\n",
    "        # Check paddle hit (right side - agent)\n",
    "        reward = 0.0\n",
    "        done = False\n",
    "        if self.ball_x >= 76:\n",
    "            if abs(self.ball_y - self.paddle_y) < 8:\n",
    "                self.ball_dx = -abs(self.ball_dx)\n",
    "                self.ball_dy = (self.ball_y - self.paddle_y) / 4\n",
    "            else:\n",
    "                reward = -1.0\n",
    "                self.ball_x = 42\n",
    "                self.ball_y = 42\n",
    "                self.ball_dx = 2\n",
    "                self.ball_dy = random.choice([-1, 0, 1])\n",
    "\n",
    "        # Opponent paddle (simple AI)\n",
    "        if self.ball_x < 10:\n",
    "            if abs(self.ball_y - self.opponent_y) < 8:\n",
    "                self.ball_dx = abs(self.ball_dx)\n",
    "                self.ball_dy = (self.ball_y - self.opponent_y) / 4\n",
    "            else:\n",
    "                reward = 1.0\n",
    "                self.ball_x = 42\n",
    "                self.ball_y = 42\n",
    "                self.ball_dx = 2\n",
    "                self.ball_dy = random.choice([-1, 0, 1])\n",
    "\n",
    "        self.opponent_y += np.clip(self.ball_y - self.opponent_y, -2, 2)\n",
    "\n",
    "        self.steps += 1\n",
    "        if self.steps >= 2000:\n",
    "            done = True\n",
    "\n",
    "        frame = self._render()\n",
    "        self.frames.append(frame)\n",
    "        state = torch.tensor(np.array(list(self.frames)))\n",
    "        return state, np.clip(reward, -1, 1), done\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "# Test the substitute environment\n",
    "sub_env = PongSubstitute()\n",
    "state = sub_env.reset()\n",
    "print(f\"Substitute env -- State shape: {state.shape}, Actions: {sub_env.n_actions}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i in range(4):\n",
    "    axes[i].imshow(state[i].numpy(), cmap='gray')\n",
    "    axes[i].set_title(f'Frame {i+1}')\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle('Substitute Pong Environment', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "sub_env.close()"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: The complete training loop"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(env_class, n_episodes=500, max_steps=2000,\n",
    "              buffer_size=50000, batch_size=32, gamma=0.99,\n",
    "              lr=1e-4, target_update=1000,\n",
    "              eps_start=1.0, eps_end=0.1, eps_decay=50000,\n",
    "              min_buffer=1000):\n",
    "    \"\"\"\n",
    "    Complete DQN training loop.\n",
    "\n",
    "    Returns: episode_rewards, losses, trained online_net\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Create environment\n",
    "    if env_class == \"atari\":\n",
    "        env = AtariWrapper(\"PongNoFrameskip-v4\")\n",
    "        n_actions = env.n_actions\n",
    "    else:\n",
    "        env = PongSubstitute()\n",
    "        n_actions = env.n_actions\n",
    "\n",
    "    # Networks\n",
    "    online_net = DQN(n_actions).to(device)\n",
    "    target_net = DQN(n_actions).to(device)\n",
    "    target_net.load_state_dict(online_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(online_net.parameters(), lr=lr)\n",
    "    replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "    # Tracking\n",
    "    episode_rewards = []\n",
    "    losses = []\n",
    "    step_count = 0\n",
    "    best_reward = -float('inf')\n",
    "    start_time = time.time()\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            # Epsilon-greedy action selection\n",
    "            epsilon = get_epsilon(step_count, eps_start, eps_end, eps_decay)\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randint(0, n_actions - 1)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_vals = online_net(state.unsqueeze(0).to(device))\n",
    "                    action = q_vals.argmax(dim=1).item()\n",
    "\n",
    "            # Environment step\n",
    "            next_state, reward, done = env.step(action)\n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "            # Training step\n",
    "            if len(replay_buffer) >= min_buffer:\n",
    "                s, a, r, ns, d = replay_buffer.sample(batch_size)\n",
    "                s, a, r, ns, d = (s.to(device), a.to(device), r.to(device),\n",
    "                                   ns.to(device), d.to(device))\n",
    "\n",
    "                # Q-values for taken actions\n",
    "                q_values = online_net(s).gather(1, a.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                # Target Q-values\n",
    "                with torch.no_grad():\n",
    "                    next_q = target_net(ns).max(1)[0]\n",
    "                    target = r + gamma * next_q * (~d).float()\n",
    "\n",
    "                loss = F.mse_loss(q_values, target)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # Gradient clipping (stabilizes training)\n",
    "                torch.nn.utils.clip_grad_norm_(online_net.parameters(), 10.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            # Update target network\n",
    "            if step_count % target_update == 0:\n",
    "                target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "        if episode_reward > best_reward:\n",
    "            best_reward = episode_reward\n",
    "\n",
    "        # Progress logging\n",
    "        if (episode + 1) % 25 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-25:])\n",
    "            avg_loss = np.mean(losses[-100:]) if losses else 0\n",
    "            elapsed = time.time() - start_time\n",
    "            eps = get_epsilon(step_count, eps_start, eps_end, eps_decay)\n",
    "            print(f\"Episode {episode+1:4d} | \"\n",
    "                  f\"Avg Reward: {avg_reward:7.2f} | \"\n",
    "                  f\"Best: {best_reward:6.1f} | \"\n",
    "                  f\"Epsilon: {eps:.3f} | \"\n",
    "                  f\"Buffer: {len(replay_buffer):6d} | \"\n",
    "                  f\"Loss: {avg_loss:.4f} | \"\n",
    "                  f\"Time: {elapsed:.0f}s\")\n",
    "\n",
    "    env.close()\n",
    "    return episode_rewards, losses, online_net\n",
    "\n",
    "# Choose environment\n",
    "if ATARI_AVAILABLE:\n",
    "    print(\"Training on Atari Pong...\")\n",
    "    env_type = \"atari\"\n",
    "else:\n",
    "    print(\"Training on substitute Pong environment...\")\n",
    "    env_type = \"substitute\"\n",
    "\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "rewards, losses, trained_net = train_dqn(\n",
    "    env_class=env_type,\n",
    "    n_episodes=300,\n",
    "    eps_decay=30000,\n",
    "    target_update=500,\n",
    "    min_buffer=500\n",
    ")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Implement reward clipping analysis"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze how reward clipping affects the Q-value distribution\n",
    "#\n",
    "# In the DQN paper, rewards are clipped to {-1, 0, +1}.\n",
    "# This means the agent treats a +10 reward the same as a +1 reward.\n",
    "#\n",
    "# YOUR TASK:\n",
    "# 1. Generate 1000 random transitions with varying rewards (e.g., -5 to +5)\n",
    "# 2. Compute TD targets with and without reward clipping\n",
    "# 3. Plot the distribution of TD targets for both cases\n",
    "# 4. Explain why clipping helps training stability\n",
    "#\n",
    "# YOUR CODE HERE"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Visualize learned Q-values"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: After training, visualize what the agent has learned\n",
    "#\n",
    "# 1. Create 10 different game states (ball in different positions)\n",
    "# 2. Run each through the trained network\n",
    "# 3. Create a heatmap: rows = states, columns = actions, values = Q-values\n",
    "# 4. Interpret: does the agent learn to move the paddle toward the ball?\n",
    "#\n",
    "# YOUR CODE HERE\n",
    "# Hint: use plt.imshow() for the heatmap with plt.colorbar()"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Putting It All Together"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Episode rewards\n",
    "ax = axes[0, 0]\n",
    "ax.plot(rewards, alpha=0.3, color='gray', linewidth=0.5)\n",
    "window = 20\n",
    "if len(rewards) >= window:\n",
    "    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(rewards)), smoothed, color='#2ecc71', linewidth=2, label=f'{window}-episode moving average')\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5, label='Break even')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Episode Reward')\n",
    "ax.set_title('Training Reward Over Time')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Training loss\n",
    "ax = axes[0, 1]\n",
    "if losses:\n",
    "    ax.plot(losses, alpha=0.1, color='gray')\n",
    "    loss_window = 100\n",
    "    if len(losses) >= loss_window:\n",
    "        smoothed_loss = np.convolve(losses, np.ones(loss_window)/loss_window, mode='valid')\n",
    "        ax.plot(range(loss_window-1, len(losses)), smoothed_loss, color='#e74c3c', linewidth=2)\n",
    "ax.set_xlabel('Training Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('DQN Training Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Reward distribution\n",
    "ax = axes[1, 0]\n",
    "ax.hist(rewards, bins=30, color='#3498db', edgecolor='white', alpha=0.7)\n",
    "ax.axvline(x=np.mean(rewards), color='red', linestyle='--', label=f'Mean: {np.mean(rewards):.1f}')\n",
    "ax.set_xlabel('Episode Reward')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Reward Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Learning progress (first half vs second half)\n",
    "ax = axes[1, 1]\n",
    "mid = len(rewards) // 2\n",
    "first_half = rewards[:mid]\n",
    "second_half = rewards[mid:]\n",
    "ax.boxplot([first_half, second_half], labels=['First Half', 'Second Half'])\n",
    "ax.set_ylabel('Episode Reward')\n",
    "ax.set_title('Learning Progress: First vs Second Half')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('DQN Training Results on Pong', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Total episodes: {len(rewards)}\")\n",
    "print(f\"  Final avg reward (last 25): {np.mean(rewards[-25:]):.2f}\")\n",
    "print(f\"  Best episode reward: {max(rewards):.1f}\")\n",
    "print(f\"  Improvement: {np.mean(rewards[:25]):.2f} -> {np.mean(rewards[-25:]):.2f}\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Training and Results"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch the trained agent play\n",
    "def evaluate_agent(trained_net, env_class, n_episodes=5):\n",
    "    \"\"\"Evaluate the trained agent without exploration.\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if env_class == \"atari\":\n",
    "        env = AtariWrapper(\"PongNoFrameskip-v4\")\n",
    "    else:\n",
    "        env = PongSubstitute()\n",
    "\n",
    "    eval_rewards = []\n",
    "    for ep in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        frames_to_show = []\n",
    "\n",
    "        for t in range(2000):\n",
    "            with torch.no_grad():\n",
    "                q_vals = trained_net(state.unsqueeze(0).to(device))\n",
    "                action = q_vals.argmax(dim=1).item()\n",
    "\n",
    "            if t % 50 == 0:\n",
    "                frames_to_show.append(state[0].numpy().copy())\n",
    "\n",
    "            state, reward, done = env.step(action)\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        eval_rewards.append(episode_reward)\n",
    "        print(f\"  Episode {ep+1}: reward = {episode_reward:.1f} ({t+1} steps)\")\n",
    "\n",
    "        # Show frames from the first episode\n",
    "        if ep == 0 and frames_to_show:\n",
    "            n_show = min(8, len(frames_to_show))\n",
    "            fig, axes = plt.subplots(1, n_show, figsize=(2*n_show, 2))\n",
    "            for i in range(n_show):\n",
    "                axes[i].imshow(frames_to_show[i], cmap='gray')\n",
    "                axes[i].axis('off')\n",
    "                axes[i].set_title(f't={i*50}', fontsize=8)\n",
    "            plt.suptitle('Trained Agent Playing', fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    env.close()\n",
    "    print(f\"\\nEvaluation: mean reward = {np.mean(eval_rewards):.2f} +/- {np.std(eval_rewards):.2f}\")\n",
    "    return eval_rewards\n",
    "\n",
    "print(\"Evaluating trained agent (no exploration)...\")\n",
    "eval_rewards = evaluate_agent(trained_net, env_type)"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Final Output"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"COMPLETE DQN TRAINING PIPELINE -- SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Components:\n",
    "  1. DQN Network:     4x84x84 -> Conv -> Conv -> Conv -> FC -> Q-values\n",
    "  2. Replay Buffer:   Stores transitions, samples random mini-batches\n",
    "  3. Target Network:  Frozen copy, updated every C steps\n",
    "  4. Epsilon-Greedy:  Anneals from 1.0 (random) to 0.1 (greedy)\n",
    "  5. Preprocessing:   RGB -> Grayscale -> 84x84 -> Stack 4 frames\n",
    "\n",
    "Training Loop:\n",
    "  For each step:\n",
    "    1. Select action (epsilon-greedy)\n",
    "    2. Execute in environment\n",
    "    3. Store transition in replay buffer\n",
    "    4. Sample mini-batch and compute TD loss\n",
    "    5. Gradient descent on online network\n",
    "    6. Periodically copy to target network\n",
    "\n",
    "Results:\n",
    "  Starting reward:  {np.mean(rewards[:25]):.2f}\n",
    "  Final reward:     {np.mean(rewards[-25:]):.2f}\n",
    "  Best reward:      {max(rewards):.1f}\n",
    "  Total episodes:   {len(rewards)}\n",
    "\n",
    "The agent learned to play Pong from raw pixels alone!\n",
    "\"\"\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**What we built:**\n",
    "- A complete, end-to-end DQN training pipeline\n",
    "- Epsilon-greedy exploration with linear annealing\n",
    "- Full training loop with logging and evaluation\n",
    "- Trained an agent to play Pong from raw pixels\n",
    "\n",
    "**Key takeaways:**\n",
    "1. The DQN algorithm is surprisingly simple once you have all the components\n",
    "2. Epsilon annealing is critical -- the agent needs to explore early and exploit later\n",
    "3. Gradient clipping helps stabilize training\n",
    "4. Reward clipping normalizes the learning signal across different games\n",
    "\n",
    "**Think about:**\n",
    "1. We used the same architecture for Pong. Could you use it for Breakout without changing anything? (Yes -- that is the power of DQN.)\n",
    "2. Why does the agent sometimes plateau before improving? What is happening during those flat periods?\n",
    "3. The DQN paper used 50 million training frames. We used far fewer. How would more training change the results?\n",
    "\n",
    "**Next notebook:** We will explore DQN extensions -- Double DQN, which fixes the overestimation bias we have been ignoring, and examine how DQN performs across different Atari games."
   ],
   "id": "cell_25"
  }
 ]
}