{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "From Tables to Neural Networks: The DQN Architecture -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Tables to Neural Networks: The DQN Architecture -- Vizuara\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "In our previous exploration of Q-learning, we built agents that maintained a table of Q-values -- one entry for every state-action pair. This worked beautifully for small grid worlds with a handful of states.\n",
    "\n",
    "But what happens when we want an agent to play Atari games directly from raw pixel input?\n",
    "\n",
    "A single Atari frame is 210 x 160 pixels with 128 possible color values per pixel. The number of possible states is $128^{33{,}600}$ -- a number so incomprehensibly large that no table could ever store it. Even a simplified 10x10 grid with 4 colors gives $4^{100} \\approx 10^{60}$ states, more than the atoms in the observable universe.\n",
    "\n",
    "This is the problem that DeepMind solved in 2013 with Deep Q-Networks (DQN). Instead of memorizing Q-values, they trained a convolutional neural network to **approximate** them. The same architecture, the same hyperparameters, applied to 49 different Atari games -- and it achieved superhuman performance on 29 of them.\n",
    "\n",
    "In this notebook, we will build the DQN neural network from scratch. By the end, you will have a working CNN that takes raw pixel frames as input and outputs Q-values for every possible action.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "### Why tables fail\n",
    "\n",
    "Let us start with something concrete. We will build a small Q-table and see exactly where it breaks down."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# A tiny 3x3 grid world -- Q-table works fine\n",
    "n_states_small = 9\n",
    "n_actions = 4  # up, down, left, right\n",
    "\n",
    "# Initialize a Q-table\n",
    "q_table = np.zeros((n_states_small, n_actions))\n",
    "print(f\"Q-table for 3x3 grid: {q_table.shape}\")\n",
    "print(f\"Total entries to store: {q_table.size}\")\n",
    "print(f\"Memory: {q_table.nbytes} bytes\\n\")\n",
    "\n",
    "# Now try Atari-scale\n",
    "# Even a tiny grayscale 84x84 image with 256 values per pixel\n",
    "pixels = 84 * 84  # 7056 pixels\n",
    "values_per_pixel = 256\n",
    "print(f\"For an 84x84 grayscale image:\")\n",
    "print(f\"  Pixels: {pixels}\")\n",
    "print(f\"  Possible states: 256^{pixels}\")\n",
    "print(f\"  That is approximately 10^{int(pixels * np.log10(256))}\")\n",
    "print(f\"  Atoms in the universe: ~10^80\")\n",
    "print(f\"  Ratio: 10^{int(pixels * np.log10(256)) - 80} times more states than atoms!\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function approximation idea\n",
    "\n",
    "Instead of memorizing every face you have ever seen, your brain learns general features -- the shape of eyes, the curve of a nose -- and combines them to recognize new faces instantly. Function approximation does the same thing for Q-values."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of a lookup table, we want a FUNCTION:\n",
    "# Q(state, action; theta) -> value\n",
    "#\n",
    "# Let us see this with a simple example.\n",
    "# True Q-values for a tiny problem:\n",
    "true_q = {\n",
    "    'state_A': [2.0, 5.0],  # action 0 -> 2.0, action 1 -> 5.0\n",
    "    'state_B': [3.0, 1.0],\n",
    "    'state_C': [4.0, 4.5],\n",
    "}\n",
    "\n",
    "# A table stores all 6 values explicitly.\n",
    "# A neural network LEARNS the patterns and can generalize.\n",
    "\n",
    "# Simple demo: approximate with a linear function\n",
    "# State features: [x_position, y_position]\n",
    "states = torch.tensor([\n",
    "    [0.0, 0.0],  # state_A\n",
    "    [1.0, 0.0],  # state_B\n",
    "    [0.5, 1.0],  # state_C\n",
    "])\n",
    "targets = torch.tensor([\n",
    "    [2.0, 5.0],\n",
    "    [3.0, 1.0],\n",
    "    [4.0, 4.5],\n",
    "])\n",
    "\n",
    "# A simple linear approximator\n",
    "linear_q = nn.Linear(2, 2)\n",
    "optimizer = torch.optim.Adam(linear_q.parameters(), lr=0.01)\n",
    "\n",
    "losses = []\n",
    "for step in range(500):\n",
    "    pred = linear_q(states)\n",
    "    loss = F.mse_loss(pred, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Linear function learning to approximate Q-values')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTrue Q-values:        {targets.numpy()}\")\n",
    "print(f\"Approximated Q-values: {linear_q(states).detach().numpy().round(2)}\")\n",
    "print(f\"\\nThe function learned to approximate the Q-values from features!\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why CNNs for pixel input?\n",
    "\n",
    "When states are raw images, we need a function that understands spatial patterns -- edges, shapes, objects. Convolutional neural networks are purpose-built for this."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what convolution does to a game-like image\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# Create a simple \"game screen\" with a paddle and ball\n",
    "game_screen = np.zeros((84, 84))\n",
    "# Paddle at bottom\n",
    "game_screen[75:80, 30:50] = 1.0\n",
    "# Ball\n",
    "game_screen[40:44, 42:46] = 1.0\n",
    "# Bricks at top\n",
    "for row in range(3):\n",
    "    for col in range(8):\n",
    "        game_screen[5+row*5:5+row*5+3, 5+col*10:5+col*10+8] = 0.7\n",
    "\n",
    "axes[0].imshow(game_screen, cmap='gray')\n",
    "axes[0].set_title('Input: 84x84 game frame')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Apply a simple edge-detection kernel (like Conv1 might learn)\n",
    "from scipy.ndimage import convolve\n",
    "edge_kernel = np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]])\n",
    "edges = convolve(game_screen, edge_kernel)\n",
    "axes[1].imshow(np.abs(edges), cmap='hot')\n",
    "axes[1].set_title('After edge detection (Conv layer)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Show that the CNN extracts meaningful features\n",
    "axes[2].imshow(game_screen, cmap='gray')\n",
    "axes[2].add_patch(Rectangle((30, 75), 20, 5, linewidth=2, edgecolor='cyan', facecolor='none'))\n",
    "axes[2].add_patch(Rectangle((42, 40), 4, 4, linewidth=2, edgecolor='red', facecolor='none'))\n",
    "axes[2].set_title('CNN learns to detect: paddle, ball, bricks')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle('Why CNNs? They extract spatial features from raw pixels', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. The Mathematics\n",
    "\n",
    "The core idea is to replace the Q-table with a parameterized function:\n",
    "\n",
    "$$Q(s, a; \\theta) \\approx Q^*(s, a)$$\n",
    "\n",
    "where $\\theta$ represents the neural network weights.\n",
    "\n",
    "For a convolutional layer, the output spatial dimension is:\n",
    "\n",
    "$$o = \\frac{i - k + 2p}{s} + 1$$\n",
    "\n",
    "where $i$ is input size, $k$ is kernel size, $p$ is padding, and $s$ is stride.\n",
    "\n",
    "Let us trace through the DQN architecture dimensions step by step."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us compute the DQN dimensions mathematically\n",
    "def conv_output_size(input_size, kernel_size, stride, padding=0):\n",
    "    \"\"\"Calculate output size of a conv layer.\"\"\"\n",
    "    return (input_size - kernel_size + 2 * padding) // stride + 1\n",
    "\n",
    "# DQN architecture dimensions\n",
    "input_size = 84\n",
    "\n",
    "# Conv1: 32 filters, 8x8 kernel, stride 4\n",
    "after_conv1 = conv_output_size(input_size, kernel_size=8, stride=4)\n",
    "print(f\"Input: {input_size}x{input_size}\")\n",
    "print(f\"After Conv1 (8x8, stride 4): {after_conv1}x{after_conv1} with 32 channels\")\n",
    "\n",
    "# Conv2: 64 filters, 4x4 kernel, stride 2\n",
    "after_conv2 = conv_output_size(after_conv1, kernel_size=4, stride=2)\n",
    "print(f\"After Conv2 (4x4, stride 2): {after_conv2}x{after_conv2} with 64 channels\")\n",
    "\n",
    "# Conv3: 64 filters, 3x3 kernel, stride 1\n",
    "after_conv3 = conv_output_size(after_conv2, kernel_size=3, stride=1)\n",
    "print(f\"After Conv3 (3x3, stride 1): {after_conv3}x{after_conv3} with 64 channels\")\n",
    "\n",
    "# Flatten\n",
    "flat_size = 64 * after_conv3 * after_conv3\n",
    "print(f\"\\nFlattened size: 64 x {after_conv3} x {after_conv3} = {flat_size}\")\n",
    "print(f\"FC layer: {flat_size} -> 512 -> n_actions\")\n",
    "\n",
    "# Visualize the dimension reduction\n",
    "sizes = [84, after_conv1, after_conv2, after_conv3]\n",
    "channels = [4, 32, 64, 64]\n",
    "total_values = [s*s*c for s, c in zip(sizes, channels)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "labels = ['Input\\n4x84x84', 'Conv1\\n32x20x20', 'Conv2\\n64x9x9', 'Conv3\\n64x7x7']\n",
    "colors = ['#3498db', '#2ecc71', '#e67e22', '#e74c3c']\n",
    "bars = ax.bar(labels, total_values, color=colors, edgecolor='white', linewidth=2)\n",
    "for bar, val in zip(bars, total_values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 200,\n",
    "            f'{val:,}', ha='center', va='bottom', fontweight='bold')\n",
    "ax.set_ylabel('Total Values (height x width x channels)')\n",
    "ax.set_title('DQN progressively compresses spatial information')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### Step 1: The DQN network"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network as described in Mnih et al. (2013/2015).\n",
    "\n",
    "    Input:  (batch, 4, 84, 84) -- 4 stacked grayscale frames\n",
    "    Output: (batch, n_actions)  -- Q-value for each action\n",
    "    \"\"\"\n",
    "    def __init__(self, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # Convolutional backbone -- extracts spatial features\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),  # -> 32x20x20\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # -> 64x9x9\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), # -> 64x7x7\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Fully connected head -- maps features to Q-values\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, 4, 84, 84), pixel values in [0, 1]\n",
    "        conv_out = self.conv(x)\n",
    "        flat = conv_out.view(conv_out.size(0), -1)  # Flatten to (batch, 3136)\n",
    "        return self.fc(flat)\n",
    "\n",
    "# Create the network\n",
    "n_actions = 4  # Typical Atari game (e.g., Breakout: noop, fire, left, right)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dqn = DQN(n_actions).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in dqn.parameters())\n",
    "print(f\"DQN Network Summary:\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  That is {total_params * 4 / 1024 / 1024:.1f} MB of weights (float32)\\n\")\n",
    "\n",
    "# Print layer details\n",
    "for name, param in dqn.named_parameters():\n",
    "    print(f\"  {name:30s} {str(list(param.shape)):20s} ({param.numel():>8,} params)\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Verify with a forward pass"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fake batch of 4-frame stacks\n",
    "batch_size = 8\n",
    "fake_input = torch.randn(batch_size, 4, 84, 84).to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    q_values = dqn(fake_input)\n",
    "\n",
    "print(f\"Input shape:  {fake_input.shape}\")\n",
    "print(f\"Output shape: {q_values.shape}\")\n",
    "print(f\"\\nQ-values for first sample:\")\n",
    "for i, q in enumerate(q_values[0]):\n",
    "    print(f\"  Action {i}: Q = {q.item():.4f}\")\n",
    "\n",
    "# The action we would select (greedy policy)\n",
    "best_action = q_values[0].argmax().item()\n",
    "print(f\"\\nGreedy action: {best_action} (highest Q-value)\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Trace through the conv layers"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us manually trace through each layer to verify our math\n",
    "x = torch.randn(1, 4, 84, 84).to(device)\n",
    "\n",
    "print(\"Tracing through DQN layers:\")\n",
    "print(f\"  Input:         {x.shape}\")\n",
    "\n",
    "# Conv1\n",
    "x1 = dqn.conv[0](x)  # Conv2d\n",
    "print(f\"  After Conv1:   {x1.shape}  (expected: [1, 32, 20, 20])\")\n",
    "x1 = dqn.conv[1](x1)  # ReLU\n",
    "\n",
    "# Conv2\n",
    "x2 = dqn.conv[2](x1)  # Conv2d\n",
    "print(f\"  After Conv2:   {x2.shape}  (expected: [1, 64, 9, 9])\")\n",
    "x2 = dqn.conv[3](x2)  # ReLU\n",
    "\n",
    "# Conv3\n",
    "x3 = dqn.conv[4](x2)  # Conv2d\n",
    "print(f\"  After Conv3:   {x3.shape}  (expected: [1, 64, 7, 7])\")\n",
    "x3 = dqn.conv[5](x3)  # ReLU\n",
    "\n",
    "# Flatten\n",
    "flat = x3.view(1, -1)\n",
    "print(f\"  After flatten: {flat.shape} (expected: [1, 3136])\")\n",
    "\n",
    "# FC\n",
    "out = dqn.fc(flat)\n",
    "print(f\"  Output:        {out.shape}  (expected: [1, {n_actions}])\")\n",
    "print(f\"\\nAll dimensions match the DQN paper exactly.\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Modify the architecture\n",
    "\n",
    "The original DQN uses ReLU activations. What happens if you use a different activation function?"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Create a modified DQN that uses LeakyReLU instead of ReLU\n# Compare the output distributions of both networks on the same input\n\nclass DQN_LeakyReLU(nn.Module):\n    def __init__(self, n_actions):\n        super().__init__()\n        # YOUR CODE HERE\n        # Hint: Replace nn.ReLU() with nn.LeakyReLU(0.01)\n        self.conv = nn.Sequential(\n            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n            nn.LeakyReLU(0.01),  # TODO: already filled in as example\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n            nn.LeakyReLU(0.01),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n            nn.LeakyReLU(0.01),\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(64 * 7 * 7, 512),\n            nn.LeakyReLU(0.01),\n            nn.Linear(512, n_actions),\n        )\n\n    def forward(self, x):\n        # YOUR CODE HERE\n        conv_out = self.conv(x)\n        flat = conv_out.view(conv_out.size(0), -1)\n        return self.fc(flat)\n\n# After implementing, run this comparison:\ndqn_leaky = DQN_LeakyReLU(n_actions).to(device)\ntest_input = torch.randn(100, 4, 84, 84).to(device)\nwith torch.no_grad():\n    q_relu = dqn(test_input)\n    q_leaky = dqn_leaky(test_input)\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\naxes[0].hist(q_relu.cpu().numpy().flatten(), bins=50, alpha=0.7, color='blue')\naxes[0].set_title('ReLU Q-value distribution')\naxes[1].hist(q_leaky.cpu().numpy().flatten(), bins=50, alpha=0.7, color='green')\naxes[1].set_title('LeakyReLU Q-value distribution')\nplt.tight_layout()\nplt.show()",
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Compute the receptive field\n",
    "\n",
    "Each output neuron \"sees\" a specific region of the input image. This is called the receptive field."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the receptive field of the final conv layer\n",
    "#\n",
    "# The receptive field formula for layer L is:\n",
    "#   RF_L = RF_{L-1} + (kernel_size_L - 1) * stride_product_{1..L-1}\n",
    "#\n",
    "# Starting with RF_0 = 1:\n",
    "#   Conv1: kernel=8, stride=4\n",
    "#   Conv2: kernel=4, stride=2\n",
    "#   Conv3: kernel=3, stride=1\n",
    "#\n",
    "# YOUR CODE HERE:\n",
    "# rf_0 = 1\n",
    "# rf_1 = rf_0 + (8 - 1) * 1      # = ?\n",
    "# rf_2 = rf_1 + (4 - 1) * 4      # = ? (stride product so far: 4)\n",
    "# rf_3 = rf_2 + (3 - 1) * (4*2)  # = ? (stride product so far: 4*2=8)\n",
    "#\n",
    "# print(f\"Receptive field of Conv3 output: {rf_3}x{rf_3} pixels\")\n",
    "# print(f\"Input image: 84x84 pixels\")\n",
    "# print(f\"Each output neuron 'sees' {rf_3/84*100:.1f}% of the input\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Putting It All Together\n",
    "\n",
    "Now let us build the complete frame preprocessing pipeline and connect it to our DQN."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from collections import deque\n",
    "\n",
    "class AtariPreprocessor:\n",
    "    \"\"\"\n",
    "    Preprocesses raw Atari frames for the DQN.\n",
    "\n",
    "    Steps:\n",
    "    1. Convert RGB to grayscale\n",
    "    2. Resize to 84x84\n",
    "    3. Normalize to [0, 1]\n",
    "    4. Stack 4 consecutive frames\n",
    "    \"\"\"\n",
    "    def __init__(self, frame_stack=4, resize_shape=(84, 84)):\n",
    "        self.frame_stack = frame_stack\n",
    "        self.resize_shape = resize_shape\n",
    "        self.frames = deque(maxlen=frame_stack)\n",
    "\n",
    "    def preprocess_frame(self, frame):\n",
    "        \"\"\"Convert a single raw frame to grayscale 84x84.\"\"\"\n",
    "        if len(frame.shape) == 3:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        else:\n",
    "            gray = frame\n",
    "        resized = cv2.resize(gray, self.resize_shape)\n",
    "        return resized.astype(np.float32) / 255.0\n",
    "\n",
    "    def reset(self, frame):\n",
    "        \"\"\"Initialize the frame stack with copies of the first frame.\"\"\"\n",
    "        processed = self.preprocess_frame(frame)\n",
    "        for _ in range(self.frame_stack):\n",
    "            self.frames.append(processed)\n",
    "        return torch.tensor(np.array(self.frames))\n",
    "\n",
    "    def step(self, frame):\n",
    "        \"\"\"Add a new frame to the stack.\"\"\"\n",
    "        processed = self.preprocess_frame(frame)\n",
    "        self.frames.append(processed)\n",
    "        return torch.tensor(np.array(self.frames))\n",
    "\n",
    "# Demo: preprocess a synthetic game frame\n",
    "preprocessor = AtariPreprocessor()\n",
    "\n",
    "# Create a fake RGB Atari frame (210x160x3)\n",
    "fake_frame = np.random.randint(0, 256, (210, 160, 3), dtype=np.uint8)\n",
    "# Add a \"paddle\" and \"ball\" for visual clarity\n",
    "fake_frame[180:190, 60:100, :] = [255, 255, 255]  # paddle\n",
    "fake_frame[100:106, 78:84, :] = [255, 0, 0]       # ball\n",
    "\n",
    "# Process it\n",
    "stacked = preprocessor.reset(fake_frame)\n",
    "print(f\"Raw frame:    {fake_frame.shape} (H x W x C, uint8)\")\n",
    "print(f\"Preprocessed: {stacked.shape}   (C x H x W, float32)\")\n",
    "\n",
    "# Visualize the preprocessing pipeline\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow(fake_frame)\n",
    "axes[0].set_title(f'Raw RGB ({fake_frame.shape[0]}x{fake_frame.shape[1]}x3)')\n",
    "axes[0].axis('off')\n",
    "\n",
    "single_processed = preprocessor.preprocess_frame(fake_frame)\n",
    "axes[1].imshow(single_processed, cmap='gray')\n",
    "axes[1].set_title(f'Grayscale + Resized (84x84)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Show the 4-frame stack as a grid\n",
    "axes[2].imshow(stacked[0].numpy(), cmap='gray')\n",
    "axes[2].set_title(f'Frame stack: {stacked.shape}')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle('Atari Preprocessing Pipeline', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect the preprocessor to the DQN\n",
    "stacked_input = stacked.unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "with torch.no_grad():\n",
    "    q_values = dqn(stacked_input)\n",
    "\n",
    "print(f\"Input to DQN:  {stacked_input.shape}\")\n",
    "print(f\"Q-values out:  {q_values.shape}\")\n",
    "print(f\"\\nQ-values for each action:\")\n",
    "action_names = ['NOOP', 'FIRE', 'LEFT', 'RIGHT']\n",
    "for i, (name, q) in enumerate(zip(action_names, q_values[0])):\n",
    "    marker = ' <-- BEST' if i == q_values[0].argmax().item() else ''\n",
    "    print(f\"  {name:6s}: Q = {q.item():+.4f}{marker}\")\n",
    "\n",
    "print(f\"\\nThe DQN takes 4 stacked 84x84 frames and outputs one Q-value per action.\")\n",
    "print(f\"The agent selects the action with the highest Q-value.\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Training and Results\n",
    "\n",
    "Let us verify our network can actually learn something meaningful. We will train the DQN on a simplified pattern recognition task to confirm the architecture works correctly."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified test: can the DQN learn to distinguish different \"game states\"?\n",
    "# We create synthetic frames with different patterns and train it to output\n",
    "# the correct Q-values.\n",
    "\n",
    "# Generate training data: 4 classes of game states\n",
    "def generate_synthetic_state(class_id):\n",
    "    \"\"\"Generate a synthetic 4x84x84 game state.\"\"\"\n",
    "    frames = np.zeros((4, 84, 84), dtype=np.float32)\n",
    "    for f in range(4):\n",
    "        if class_id == 0:  # Ball top-left\n",
    "            frames[f, 10:16, 10:16] = 1.0\n",
    "            frames[f, 70:76, 30+f*3:50+f*3] = 1.0  # Moving paddle\n",
    "        elif class_id == 1:  # Ball top-right\n",
    "            frames[f, 10:16, 68:74] = 1.0\n",
    "            frames[f, 70:76, 30+f*3:50+f*3] = 1.0\n",
    "        elif class_id == 2:  # Ball bottom-left\n",
    "            frames[f, 55:61, 10:16] = 1.0\n",
    "            frames[f, 70:76, 30+f*3:50+f*3] = 1.0\n",
    "        else:  # Ball bottom-right\n",
    "            frames[f, 55:61, 68:74] = 1.0\n",
    "            frames[f, 70:76, 30+f*3:50+f*3] = 1.0\n",
    "    return torch.tensor(frames)\n",
    "\n",
    "# Training\n",
    "dqn_train = DQN(4).to(device)\n",
    "optimizer = torch.optim.Adam(dqn_train.parameters(), lr=1e-3)\n",
    "\n",
    "# Target Q-values: each class should prefer a different action\n",
    "target_q = torch.tensor([\n",
    "    [5.0, 0.0, 0.0, 0.0],  # Class 0 -> action 0\n",
    "    [0.0, 5.0, 0.0, 0.0],  # Class 1 -> action 1\n",
    "    [0.0, 0.0, 5.0, 0.0],  # Class 2 -> action 2\n",
    "    [0.0, 0.0, 0.0, 5.0],  # Class 3 -> action 3\n",
    "]).to(device)\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    batch_states = []\n",
    "    batch_targets = []\n",
    "\n",
    "    for class_id in range(4):\n",
    "        for _ in range(8):  # 8 samples per class\n",
    "            state = generate_synthetic_state(class_id)\n",
    "            batch_states.append(state)\n",
    "            batch_targets.append(target_q[class_id])\n",
    "\n",
    "    states = torch.stack(batch_states).to(device)\n",
    "    targets = torch.stack(batch_targets)\n",
    "\n",
    "    pred_q = dqn_train(states)\n",
    "    loss = F.mse_loss(pred_q, targets)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Check accuracy\n",
    "    pred_actions = pred_q.argmax(dim=1)\n",
    "    true_actions = targets.argmax(dim=1)\n",
    "    accuracy = (pred_actions == true_actions).float().mean().item()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.plot(losses, color='#e74c3c')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('MSE Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(accuracies, color='#2ecc71')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Action Selection Accuracy')\n",
    "ax2.set_ylim(-0.05, 1.05)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('DQN learns to map different visual patterns to correct actions', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal loss: {losses[-1]:.4f}\")\n",
    "print(f\"Final accuracy: {accuracies[-1]:.1%}\")\n",
    "print(f\"\\nThe DQN architecture can learn visual pattern -> action mappings!\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Final Output"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration: the trained DQN correctly classifies game states\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL VERIFICATION: DQN Architecture\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for class_id in range(4):\n",
    "    state = generate_synthetic_state(class_id).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        q_vals = dqn_train(state)\n",
    "\n",
    "    axes[class_id].imshow(state[0, 0].cpu().numpy(), cmap='gray')\n",
    "    axes[class_id].set_title(f'State class {class_id}')\n",
    "    axes[class_id].axis('off')\n",
    "\n",
    "    action = q_vals.argmax().item()\n",
    "    q_str = ', '.join([f'{q:.1f}' for q in q_vals[0].cpu().numpy()])\n",
    "    axes[class_id].set_xlabel(f'Q: [{q_str}]\\nAction: {action}', fontsize=9)\n",
    "\n",
    "plt.suptitle('DQN correctly maps visual states to actions', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nNetwork architecture: 4x84x84 -> Conv(32) -> Conv(64) -> Conv(64) -> FC(512) -> {n_actions} actions\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in dqn_train.parameters()):,}\")\n",
    "print(f\"The same architecture is used for ALL 49 Atari games in the DQN paper.\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**What we built:**\n",
    "- A complete DQN convolutional neural network that maps raw pixel frames to Q-values\n",
    "- An Atari preprocessing pipeline (RGB -> grayscale -> resize -> frame stack)\n",
    "- Verified the architecture can learn visual pattern-to-action mappings\n",
    "\n",
    "**Key dimensions to remember:**\n",
    "- Input: 4 x 84 x 84 (4 stacked grayscale frames)\n",
    "- Conv1: 32 filters, 8x8, stride 4 -> 32 x 20 x 20\n",
    "- Conv2: 64 filters, 4x4, stride 2 -> 64 x 9 x 9\n",
    "- Conv3: 64 filters, 3x3, stride 1 -> 64 x 7 x 7\n",
    "- Flatten: 3136 -> FC: 512 -> Output: n_actions\n",
    "\n",
    "**Think about:**\n",
    "1. Why did DeepMind choose these specific kernel sizes and strides? What happens if you use smaller kernels with stride 1 everywhere?\n",
    "2. The network has no pooling layers. Why might that be important for game playing?\n",
    "3. Why 4 stacked frames instead of 2 or 8?\n",
    "\n",
    "**Next notebook:** We will tackle the two innovations that made training this network stable -- experience replay and target networks. Without them, the DQN architecture we just built would fail to learn anything useful."
   ],
   "id": "cell_26"
  }
 ]
}