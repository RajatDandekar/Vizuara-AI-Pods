{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Experience Replay and Target Networks -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay and Target Networks -- Vizuara\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebook, we built a DQN that can take raw pixel frames and output Q-values. The architecture is ready. But if we try to train it naively -- updating the network on each transition as it happens -- the training will be wildly unstable and the agent will learn nothing useful.\n",
    "\n",
    "Two fundamental problems sabotage naive training:\n",
    "\n",
    "**Correlated data.** Consecutive game frames are nearly identical. Frame 100 looks almost the same as frame 101. Training a neural network on such correlated data causes it to forget earlier experiences -- a phenomenon called catastrophic forgetting.\n",
    "\n",
    "**Moving targets.** In Q-learning, the target value depends on the same network we are updating. Every gradient step shifts the target, so we are chasing a moving goal. This is like trying to hit an archery target that moves every time you release the arrow.\n",
    "\n",
    "DeepMind's 2013 paper introduced two brilliantly simple solutions: **experience replay** (store transitions and sample randomly) and **target networks** (freeze the target for stability). These two ideas -- not the CNN architecture -- are what made DQN actually work.\n",
    "\n",
    "In this notebook, we will build both components from scratch, understand exactly why they are necessary, and visualize their impact on training stability.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "### Why correlated data breaks training"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Demo: training on correlated vs random data\n",
    "# We have 4 \"patterns\" the network should learn\n",
    "# Let us see what happens with sequential vs random ordering\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simple network for demonstration\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(10, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 4)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Create 4 distinct patterns\n",
    "patterns = {\n",
    "    0: torch.randn(10) + torch.tensor([2.0]*5 + [0.0]*5),\n",
    "    1: torch.randn(10) + torch.tensor([0.0]*5 + [2.0]*5),\n",
    "    2: torch.randn(10) + torch.tensor([-2.0]*5 + [0.0]*5),\n",
    "    3: torch.randn(10) + torch.tensor([0.0]*5 + [-2.0]*5),\n",
    "}\n",
    "targets = {\n",
    "    0: torch.tensor([5.0, 0.0, 0.0, 0.0]),\n",
    "    1: torch.tensor([0.0, 5.0, 0.0, 0.0]),\n",
    "    2: torch.tensor([0.0, 0.0, 5.0, 0.0]),\n",
    "    3: torch.tensor([0.0, 0.0, 0.0, 5.0]),\n",
    "}\n",
    "\n",
    "# Training with SEQUENTIAL data (correlated - bad!)\n",
    "net_seq = SimpleNet()\n",
    "opt_seq = torch.optim.SGD(net_seq.parameters(), lr=0.01)\n",
    "losses_seq = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    # Sequential: train on pattern 0 for 50 steps, then 1, then 2, then 3\n",
    "    pattern_id = (epoch * 4 // 200) % 4\n",
    "    x = patterns[pattern_id].unsqueeze(0)\n",
    "    t = targets[pattern_id].unsqueeze(0)\n",
    "    pred = net_seq(x)\n",
    "    loss = F.mse_loss(pred, t)\n",
    "    opt_seq.zero_grad()\n",
    "    loss.backward()\n",
    "    opt_seq.step()\n",
    "    # Check ALL patterns\n",
    "    total_loss = sum(F.mse_loss(net_seq(patterns[i].unsqueeze(0)), targets[i].unsqueeze(0)).item() for i in range(4))\n",
    "    losses_seq.append(total_loss / 4)\n",
    "\n",
    "# Training with RANDOM data (uncorrelated - good!)\n",
    "net_rand = SimpleNet()\n",
    "# Use same initial weights\n",
    "net_rand.load_state_dict(SimpleNet().state_dict())\n",
    "opt_rand = torch.optim.SGD(net_rand.parameters(), lr=0.01)\n",
    "losses_rand = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    # Random: pick a random pattern each step\n",
    "    pattern_id = random.randint(0, 3)\n",
    "    x = patterns[pattern_id].unsqueeze(0)\n",
    "    t = targets[pattern_id].unsqueeze(0)\n",
    "    pred = net_rand(x)\n",
    "    loss = F.mse_loss(pred, t)\n",
    "    opt_rand.zero_grad()\n",
    "    loss.backward()\n",
    "    opt_rand.step()\n",
    "    total_loss = sum(F.mse_loss(net_rand(patterns[i].unsqueeze(0)), targets[i].unsqueeze(0)).item() for i in range(4))\n",
    "    losses_rand.append(total_loss / 4)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses_seq, label='Sequential (correlated)', color='#e74c3c', linewidth=2)\n",
    "plt.plot(losses_rand, label='Random (uncorrelated)', color='#2ecc71', linewidth=2)\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Average Loss Across All Patterns')\n",
    "plt.title('Correlated data causes catastrophic forgetting')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Sequential training: the network forgets earlier patterns!\")\n",
    "print(\"Random sampling: the network learns all patterns simultaneously.\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The moving target problem"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: trying to hit a moving target vs a fixed target\n",
    "\n",
    "# Fixed target: easy to converge\n",
    "value = 0.0\n",
    "fixed_target = 10.0\n",
    "lr = 0.1\n",
    "values_fixed = [value]\n",
    "\n",
    "for _ in range(50):\n",
    "    value = value + lr * (fixed_target - value)\n",
    "    values_fixed.append(value)\n",
    "\n",
    "# Moving target: oscillates or diverges\n",
    "value = 0.0\n",
    "moving_target = 10.0\n",
    "values_moving = [value]\n",
    "\n",
    "for _ in range(50):\n",
    "    # Target depends on current value (like Q-learning without target network)\n",
    "    moving_target = 3.0 + 0.99 * (value + np.random.normal(0, 0.5))\n",
    "    value = value + lr * (moving_target - value)\n",
    "    values_moving.append(value)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(values_fixed, label='Fixed target (converges)', color='#2ecc71', linewidth=2)\n",
    "plt.axhline(y=10.0, color='#2ecc71', linestyle='--', alpha=0.5)\n",
    "plt.plot(values_moving, label='Moving target (unstable)', color='#e74c3c', linewidth=2)\n",
    "plt.xlabel('Update Step')\n",
    "plt.ylabel('Estimated Value')\n",
    "plt.title('Fixed vs Moving Targets in Value Estimation')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. The Mathematics\n",
    "\n",
    "### Experience replay\n",
    "\n",
    "The agent stores transitions $(s_t, a_t, r_t, s_{t+1}, \\text{done}_t)$ in a circular buffer $\\mathcal{D}$ of fixed capacity $N$. During training, a random mini-batch is sampled uniformly:\n",
    "\n",
    "$$(s_j, a_j, r_j, s_{j+1}) \\sim \\text{Uniform}(\\mathcal{D})$$\n",
    "\n",
    "This breaks temporal correlations because transitions from different episodes and time steps are mixed together in each batch.\n",
    "\n",
    "### The DQN loss function\n",
    "\n",
    "DQN maintains two networks: the **online network** (parameters $\\theta$) that we train, and the **target network** (parameters $\\theta^-$) that provides stable targets.\n",
    "\n",
    "The loss function is:\n",
    "\n",
    "$$L(\\theta) = \\mathbb{E}_{(s, a, r, s') \\sim \\mathcal{D}} \\left[ \\left( r + \\gamma \\max_{a'} Q(s', a'; \\theta^-) - Q(s, a; \\theta) \\right)^2 \\right]$$\n",
    "\n",
    "The term inside the square is the **temporal difference (TD) error**. The target network $\\theta^-$ is updated by copying from $\\theta$ every $C$ steps:\n",
    "\n",
    "$$\\theta^- \\leftarrow \\theta \\quad \\text{every } C \\text{ steps}$$"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us compute the TD error for a concrete example\n",
    "print(\"=\" * 50)\n",
    "print(\"Computing TD Error -- Worked Example\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Given:\n",
    "r = 3.0           # reward\n",
    "gamma = 0.99       # discount factor\n",
    "q_online = 7.0     # Q(s, a=1; theta) -- online network's prediction\n",
    "# Target network predicts for next state:\n",
    "q_target_a0 = 5.0  # Q(s', a'=0; theta^-)\n",
    "q_target_a1 = 8.0  # Q(s', a'=1; theta^-)\n",
    "\n",
    "# Step 1: max Q from target network\n",
    "max_q_target = max(q_target_a0, q_target_a1)\n",
    "print(f\"\\n1. max Q(s', a'; theta^-) = max({q_target_a0}, {q_target_a1}) = {max_q_target}\")\n",
    "\n",
    "# Step 2: compute target value\n",
    "target = r + gamma * max_q_target\n",
    "print(f\"2. Target = r + gamma * max_Q = {r} + {gamma} * {max_q_target} = {target:.2f}\")\n",
    "\n",
    "# Step 3: TD error\n",
    "td_error = target - q_online\n",
    "print(f\"3. TD error = target - Q(s,a;theta) = {target:.2f} - {q_online} = {td_error:.2f}\")\n",
    "\n",
    "# Step 4: loss\n",
    "loss = td_error ** 2\n",
    "print(f\"4. Loss = (TD error)^2 = ({td_error:.2f})^2 = {loss:.2f}\")\n",
    "\n",
    "print(f\"\\nInterpretation: The online network predicted {q_online},\")\n",
    "print(f\"but the target says it should be {target:.2f}.\")\n",
    "print(f\"Gradient descent will push the prediction upward by {td_error:.2f}.\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### Step 1: The Replay Buffer"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Circular buffer for storing and sampling experience transitions.\n",
    "\n",
    "    Stores (state, action, reward, next_state, done) tuples.\n",
    "    When full, the oldest transitions are overwritten.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store a single transition.\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a random mini-batch of transitions.\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.stack(states),\n",
    "            torch.tensor(actions, dtype=torch.long),\n",
    "            torch.tensor(rewards, dtype=torch.float32),\n",
    "            torch.stack(next_states),\n",
    "            torch.tensor(dones, dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Test it\n",
    "buffer = ReplayBuffer(capacity=10000)\n",
    "print(f\"Empty buffer: {len(buffer)} transitions\")\n",
    "\n",
    "# Add some fake transitions\n",
    "for i in range(100):\n",
    "    state = torch.randn(4, 84, 84)\n",
    "    action = random.randint(0, 3)\n",
    "    reward = random.choice([-1.0, 0.0, 1.0])\n",
    "    next_state = torch.randn(4, 84, 84)\n",
    "    done = random.random() < 0.05  # 5% chance of episode end\n",
    "    buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "print(f\"After 100 transitions: {len(buffer)}\")\n",
    "\n",
    "# Sample a mini-batch\n",
    "states, actions, rewards, next_states, dones = buffer.sample(32)\n",
    "print(f\"\\nSampled batch:\")\n",
    "print(f\"  States:      {states.shape}\")\n",
    "print(f\"  Actions:     {actions.shape} -- values: {actions[:5].tolist()}\")\n",
    "print(f\"  Rewards:     {rewards.shape} -- values: {rewards[:5].tolist()}\")\n",
    "print(f\"  Next states: {next_states.shape}\")\n",
    "print(f\"  Dones:       {dones.shape} -- values: {dones[:5].tolist()}\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Verify random sampling breaks correlations"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store transitions from a \"sequential episode\"\n",
    "buffer_demo = ReplayBuffer(capacity=1000)\n",
    "\n",
    "# Simulate an episode with sequential states\n",
    "for t in range(100):\n",
    "    # State is just the time step (sequential!)\n",
    "    state = torch.full((4, 84, 84), float(t) / 100)\n",
    "    action = t % 4\n",
    "    reward = 1.0 if t > 80 else 0.0  # Reward only near the end\n",
    "    next_state = torch.full((4, 84, 84), float(t+1) / 100)\n",
    "    done = (t == 99)\n",
    "    buffer_demo.push(state, action, reward, next_state, done)\n",
    "\n",
    "# Sample and check -- are the indices random?\n",
    "states_batch, _, rewards_batch, _, _ = buffer_demo.sample(16)\n",
    "time_steps = (states_batch[:, 0, 0, 0] * 100).int().tolist()\n",
    "\n",
    "print(\"Sequential storage: t = 0, 1, 2, ..., 99\")\n",
    "print(f\"Random sample of 16: t = {sorted(time_steps)}\")\n",
    "print(f\"\\nThe sample contains transitions from different time steps!\")\n",
    "print(\"This is exactly what breaks temporal correlations.\")\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Sequential\n",
    "ax1.bar(range(100), range(100), width=1, color='#e74c3c', alpha=0.3)\n",
    "ax1.set_title('Sequential Training')\n",
    "ax1.set_xlabel('Training step')\n",
    "ax1.set_ylabel('Transition time step')\n",
    "ax1.text(50, 80, 'Correlated!', ha='center', fontsize=14, color='red', fontweight='bold')\n",
    "\n",
    "# Random sampling\n",
    "for _ in range(100):\n",
    "    sample = random.sample(range(100), 1)[0]\n",
    "    ax2.scatter(_, sample, c='#2ecc71', alpha=0.5, s=10)\n",
    "ax2.set_title('Random Sampling from Buffer')\n",
    "ax2.set_xlabel('Training step')\n",
    "ax2.set_ylabel('Transition time step')\n",
    "ax2.text(50, 80, 'Uncorrelated!', ha='center', fontsize=14, color='green', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Experience Replay Breaks Temporal Correlations', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: The Target Network"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"DQN from the previous notebook.\"\"\"\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x)\n",
    "        flat = conv_out.view(conv_out.size(0), -1)\n",
    "        return self.fc(flat)\n",
    "\n",
    "# Create online and target networks\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_actions = 4\n",
    "\n",
    "online_net = DQN(n_actions).to(device)\n",
    "target_net = DQN(n_actions).to(device)\n",
    "\n",
    "# Initialize target network with same weights as online network\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "target_net.eval()  # Target network is never trained directly\n",
    "\n",
    "print(\"Online network and target network initialized with identical weights.\")\n",
    "\n",
    "# Verify they produce the same output\n",
    "test_input = torch.randn(1, 4, 84, 84).to(device)\n",
    "with torch.no_grad():\n",
    "    q_online = online_net(test_input)\n",
    "    q_target = target_net(test_input)\n",
    "\n",
    "print(f\"\\nSame input -> same output (before any training):\")\n",
    "print(f\"  Online:  {q_online[0].cpu().numpy().round(4)}\")\n",
    "print(f\"  Target:  {q_target[0].cpu().numpy().round(4)}\")\n",
    "print(f\"  Match:   {torch.allclose(q_online, q_target)}\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Computing the DQN loss"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dqn_loss(online_net, target_net, batch, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Compute the DQN loss for a batch of transitions.\n",
    "\n",
    "    L = E[(r + gamma * max_a' Q(s', a'; theta^-) - Q(s, a; theta))^2]\n",
    "    \"\"\"\n",
    "    states, actions, rewards, next_states, dones = batch\n",
    "\n",
    "    states = states.to(device)\n",
    "    actions = actions.to(device)\n",
    "    rewards = rewards.to(device)\n",
    "    next_states = next_states.to(device)\n",
    "    dones = dones.to(device)\n",
    "\n",
    "    # Current Q-values: Q(s, a; theta) for the actions that were taken\n",
    "    q_values = online_net(states)\n",
    "    q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # Target Q-values: r + gamma * max_a' Q(s', a'; theta^-)\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_net(next_states)\n",
    "        max_next_q = next_q_values.max(1)[0]\n",
    "        # If done, there is no next state -- target is just r\n",
    "        target = rewards + gamma * max_next_q * (~dones).float()\n",
    "\n",
    "    # MSE loss\n",
    "    loss = F.mse_loss(q_value, target)\n",
    "\n",
    "    return loss, (q_value - target).detach()  # Also return TD errors\n",
    "\n",
    "# Test with a small batch from our buffer\n",
    "batch = buffer.sample(32)\n",
    "loss, td_errors = compute_dqn_loss(online_net, target_net, batch)\n",
    "print(f\"DQN Loss: {loss.item():.4f}\")\n",
    "print(f\"TD errors -- mean: {td_errors.mean().item():.4f}, std: {td_errors.std().item():.4f}\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Implement target network soft update\n",
    "\n",
    "The original DQN copies weights every $C$ steps (hard update). An alternative is **soft update** (also called Polyak averaging): $\\theta^- \\leftarrow \\tau \\theta + (1 - \\tau) \\theta^-$, where $\\tau$ is a small value like 0.005."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement soft update and compare with hard update\n",
    "\n",
    "def hard_update(online_net, target_net):\n",
    "    \"\"\"Copy all weights from online to target.\"\"\"\n",
    "    target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "def soft_update(online_net, target_net, tau=0.005):\n",
    "    \"\"\"Polyak averaging: slowly blend online weights into target.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: iterate over zip(target_net.parameters(), online_net.parameters())\n",
    "    # For each pair, update: target_param.data = tau * online_param.data + (1-tau) * target_param.data\n",
    "    pass\n",
    "\n",
    "# Test your implementation:\n",
    "# 1. Create two networks with different random weights\n",
    "# 2. Apply soft_update 200 times\n",
    "# 3. Check that target weights gradually approach online weights\n",
    "# 4. Plot the distance between weights over time"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Analyze replay buffer statistics"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill the replay buffer with transitions and analyze the statistics\n",
    "#\n",
    "# 1. Create a buffer of size 10000\n",
    "# 2. Fill it with 20000 transitions (so it wraps around)\n",
    "# 3. Sample 1000 batches of size 32\n",
    "# 4. For each batch, compute the average \"time gap\" between transitions\n",
    "#    (how far apart in time are the sampled transitions?)\n",
    "# 5. Plot the distribution of time gaps\n",
    "#\n",
    "# This shows that random sampling produces batches with diverse time gaps,\n",
    "# which is exactly what we want for breaking correlations.\n",
    "#\n",
    "# YOUR CODE HERE"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Putting It All Together\n",
    "\n",
    "Let us now train a DQN with both experience replay and target networks on a simple environment."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use a simplified environment to demonstrate the training loop\n",
    "# This avoids needing a full Atari setup while showing the same principles\n",
    "\n",
    "class SimpleGridEnv:\n",
    "    \"\"\"\n",
    "    A 5x5 grid world where the agent must reach a goal.\n",
    "    State: flattened into a pseudo-image (4, 84, 84)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.size = 5\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_pos = [0, 0]\n",
    "        self.goal_pos = [4, 4]\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"Create a 4x84x84 pseudo-image from the grid state.\"\"\"\n",
    "        frame = np.zeros((84, 84), dtype=np.float32)\n",
    "        # Draw agent\n",
    "        ax, ay = self.agent_pos\n",
    "        px, py = ax * 16 + 2, ay * 16 + 2\n",
    "        frame[py:py+12, px:px+12] = 1.0\n",
    "        # Draw goal\n",
    "        gx, gy = self.goal_pos\n",
    "        gpx, gpy = gx * 16 + 2, gy * 16 + 2\n",
    "        frame[gpy:gpy+12, gpx:gpx+12] = 0.5\n",
    "        # Stack 4 copies (no motion in this simple env)\n",
    "        return torch.tensor(np.stack([frame]*4))\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Execute action: 0=up, 1=down, 2=left, 3=right\"\"\"\n",
    "        dx, dy = [(0,-1), (0,1), (-1,0), (1,0)][action]\n",
    "        new_x = max(0, min(self.size-1, self.agent_pos[0] + dx))\n",
    "        new_y = max(0, min(self.size-1, self.agent_pos[1] + dy))\n",
    "        self.agent_pos = [new_x, new_y]\n",
    "\n",
    "        done = (self.agent_pos == self.goal_pos)\n",
    "        reward = 1.0 if done else -0.01\n",
    "        return self._get_state(), reward, done\n",
    "\n",
    "# Train with experience replay + target network\n",
    "env = SimpleGridEnv()\n",
    "online_net = DQN(n_actions=4).to(device)\n",
    "target_net = DQN(n_actions=4).to(device)\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "optimizer = torch.optim.Adam(online_net.parameters(), lr=1e-4)\n",
    "replay_buffer = ReplayBuffer(capacity=10000)\n",
    "\n",
    "# Hyperparameters\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "TARGET_UPDATE = 100\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.05\n",
    "EPSILON_DECAY = 500\n",
    "\n",
    "episode_rewards = []\n",
    "losses_log = []\n",
    "step_count = 0\n",
    "\n",
    "for episode in range(300):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for t in range(50):  # Max 50 steps per episode\n",
    "        # Epsilon-greedy\n",
    "        epsilon = EPSILON_END + (EPSILON_START - EPSILON_END) * \\\n",
    "                  np.exp(-step_count / EPSILON_DECAY)\n",
    "\n",
    "        if random.random() < epsilon:\n",
    "            action = random.randint(0, 3)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q = online_net(state.unsqueeze(0).to(device))\n",
    "                action = q.argmax(dim=1).item()\n",
    "\n",
    "        next_state, reward, done = env.step(action)\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        step_count += 1\n",
    "\n",
    "        # Training\n",
    "        if len(replay_buffer) >= BATCH_SIZE:\n",
    "            batch = replay_buffer.sample(BATCH_SIZE)\n",
    "            loss, _ = compute_dqn_loss(online_net, target_net, batch, GAMMA)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses_log.append(loss.item())\n",
    "\n",
    "        # Target network update\n",
    "        if step_count % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Smooth rewards\n",
    "window = 20\n",
    "smoothed = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "ax1.plot(episode_rewards, alpha=0.3, color='gray')\n",
    "ax1.plot(range(window-1, len(episode_rewards)), smoothed, color='#2ecc71', linewidth=2)\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Episode Reward')\n",
    "ax1.set_title('Training Reward (with replay + target network)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "if losses_log:\n",
    "    smoothed_loss = np.convolve(losses_log, np.ones(100)/100, mode='valid')\n",
    "    ax2.plot(losses_log, alpha=0.1, color='gray')\n",
    "    ax2.plot(range(99, len(losses_log)), smoothed_loss, color='#e74c3c', linewidth=2)\n",
    "ax2.set_xlabel('Training Step')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('DQN Training Loss')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('DQN Training with Experience Replay + Target Network', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal avg reward (last 20 episodes): {np.mean(episode_rewards[-20:]):.3f}\")\n",
    "print(f\"Buffer size: {len(replay_buffer)}\")\n",
    "print(f\"Total training steps: {step_count}\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Training and Results\n",
    "\n",
    "Let us now compare training with and without our two innovations."
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn_variant(use_replay, use_target_net, n_episodes=200, label=\"\"):\n",
    "    \"\"\"Train a DQN variant and return rewards.\"\"\"\n",
    "    env = SimpleGridEnv()\n",
    "    online = DQN(4).to(device)\n",
    "    target = DQN(4).to(device)\n",
    "    target.load_state_dict(online.state_dict())\n",
    "    opt = torch.optim.Adam(online.parameters(), lr=1e-4)\n",
    "    buf = ReplayBuffer(10000)\n",
    "    rewards = []\n",
    "    step = 0\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        ep_reward = 0\n",
    "        for t in range(50):\n",
    "            eps = max(0.05, 1.0 - step / 500)\n",
    "            if random.random() < eps:\n",
    "                action = random.randint(0, 3)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    action = online(state.unsqueeze(0).to(device)).argmax(1).item()\n",
    "\n",
    "            ns, r, done = env.step(action)\n",
    "            buf.push(state, action, r, ns, done)\n",
    "            state = ns\n",
    "            ep_reward += r\n",
    "            step += 1\n",
    "\n",
    "            if len(buf) >= 32:\n",
    "                if use_replay:\n",
    "                    batch = buf.sample(32)\n",
    "                else:\n",
    "                    # No replay: use last 32 transitions (correlated!)\n",
    "                    recent = list(buf.buffer)[-32:]\n",
    "                    s, a, rr, ns2, d = zip(*recent)\n",
    "                    batch = (torch.stack(s), torch.tensor(a, dtype=torch.long),\n",
    "                             torch.tensor(rr, dtype=torch.float32),\n",
    "                             torch.stack(ns2), torch.tensor(d, dtype=torch.bool))\n",
    "\n",
    "                tgt = target if use_target_net else online\n",
    "                loss, _ = compute_dqn_loss(online, tgt, batch)\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            if use_target_net and step % 100 == 0:\n",
    "                target.load_state_dict(online.state_dict())\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "# Compare all variants\n",
    "print(\"Training 4 variants (this may take a minute)...\")\n",
    "r1 = train_dqn_variant(use_replay=False, use_target_net=False, label=\"No replay, no target\")\n",
    "r2 = train_dqn_variant(use_replay=True, use_target_net=False, label=\"Replay only\")\n",
    "r3 = train_dqn_variant(use_replay=False, use_target_net=True, label=\"Target net only\")\n",
    "r4 = train_dqn_variant(use_replay=True, use_target_net=True, label=\"Both (full DQN)\")\n",
    "\n",
    "window = 15\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for rewards, label, color in [\n",
    "    (r1, 'No replay, no target net', '#e74c3c'),\n",
    "    (r2, 'Replay only', '#f39c12'),\n",
    "    (r3, 'Target net only', '#9b59b6'),\n",
    "    (r4, 'Both (full DQN)', '#2ecc71'),\n",
    "]:\n",
    "    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(rewards)), smoothed, label=label, linewidth=2, color=color)\n",
    "\n",
    "ax.set_xlabel('Episode', fontsize=12)\n",
    "ax.set_ylabel('Episode Reward', fontsize=12)\n",
    "ax.set_title('Impact of Experience Replay and Target Networks', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBoth innovations together give the most stable, highest-performing training.\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Final Output"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY: Experience Replay and Target Networks\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "EXPERIENCE REPLAY:\n",
    "  - Stores transitions (s, a, r, s', done) in a circular buffer\n",
    "  - Samples random mini-batches for training\n",
    "  - Breaks temporal correlations between consecutive frames\n",
    "  - Allows reuse of each transition in multiple updates\n",
    "\n",
    "TARGET NETWORK:\n",
    "  - Separate copy of the network, updated every C steps\n",
    "  - Provides stable targets: r + gamma * max Q(s', a'; theta^-)\n",
    "  - Prevents the \"moving target\" problem\n",
    "  - Original DQN: hard copy every 10,000 steps\n",
    "\n",
    "TOGETHER:\n",
    "  - These two ideas made deep Q-learning practical\n",
    "  - Without them, the CNN architecture from Notebook 1 fails to learn\n",
    "  - They are now standard components in ALL deep RL algorithms\n",
    "\"\"\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**What we built:**\n",
    "- A replay buffer that stores and randomly samples transitions\n",
    "- The target network mechanism for stable Q-value targets\n",
    "- The complete DQN loss computation\n",
    "- Demonstrated the impact of both innovations on training stability\n",
    "\n",
    "**Key insights:**\n",
    "1. Correlated sequential data causes catastrophic forgetting -- random sampling fixes this\n",
    "2. Moving targets cause instability -- freezing the target network fixes this\n",
    "3. Both innovations are necessary; either alone is insufficient\n",
    "\n",
    "**Think about:**\n",
    "1. The replay buffer stores transitions uniformly. What if some transitions are more \"important\" than others? (This leads to Prioritized Experience Replay.)\n",
    "2. Why every C steps for the target update? What happens if C is too small or too large?\n",
    "3. The buffer has fixed capacity. What happens when it fills up -- do we lose important early experiences?\n",
    "\n",
    "**Next notebook:** We will put everything together into a complete DQN training loop and train an agent to play Pong from raw pixels."
   ],
   "id": "cell_25"
  }
 ]
}