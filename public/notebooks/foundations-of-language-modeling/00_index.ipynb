{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Foundations of Language Modeling \u2014 Index \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Language Modeling -- Notebook Series\n",
    "\n",
    "## From N-grams to Neural LMs to Transformers\n",
    "\n",
    "Welcome to the **Foundations of Language Modeling** notebook series by Vizuara. This series traces the complete evolution of how machines learned to predict the next word -- from simple counting to the Transformer architecture that powers every modern LLM.\n",
    "\n",
    "---\n",
    "\n",
    "## Course Structure\n",
    "\n",
    "| # | Notebook | Key Concepts | Time |\n",
    "|---|----------|-------------|------|\n",
    "| 1 | **N-gram Language Models** | Bigram/trigram counting, Markov assumption, sparsity problem, perplexity | ~45 min |\n",
    "| 2 | **Neural Language Models & Embeddings** | Bengio's model, word embeddings, Word2Vec, RNNs, vanishing gradients | ~60 min |\n",
    "| 3 | **Self-Attention & the Transformer** | Q/K/V, scaled dot-product attention, multi-head attention, positional encoding, Transformer blocks | ~60 min |\n",
    "| 4 | **Building a Tiny Language Model** | Complete Mini-GPT, training loop, text generation, temperature sampling | ~45 min |\n",
    "\n",
    "**Total estimated time: ~3.5 hours**\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic Python and PyTorch familiarity\n",
    "- Understanding of neural network fundamentals (layers, backpropagation, loss functions)\n",
    "- No prior NLP experience required -- we build everything from first principles\n",
    "\n",
    "---\n",
    "\n",
    "## How to Use These Notebooks\n",
    "\n",
    "1. **Run in Google Colab** with a T4 GPU (free tier works fine)\n",
    "2. **Go in order** -- each notebook builds on the previous one\n",
    "3. **Complete the TODOs** -- hands-on exercises are where the real learning happens\n",
    "4. **Read the article first** for conceptual context, then use notebooks for implementation\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Links"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Foundations of Language Modeling \u2014 Vizuara Notebook Series\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Notebook 1: N-gram Language Models\")\n",
    "print(\"  \u2192 Build bigram/trigram models from scratch\")\n",
    "print(\"  \u2192 Generate text by sampling from count-based distributions\")\n",
    "print(\"  \u2192 Discover the sparsity problem that limits N-grams\")\n",
    "print()\n",
    "print(\"Notebook 2: Neural Language Models & Word Embeddings\")\n",
    "print(\"  \u2192 Implement Bengio's 2003 neural language model\")\n",
    "print(\"  \u2192 Train word embeddings that capture semantic similarity\")\n",
    "print(\"  \u2192 Build an RNN and see the vanishing gradient problem\")\n",
    "print()\n",
    "print(\"Notebook 3: Self-Attention & the Transformer\")\n",
    "print(\"  \u2192 Implement scaled dot-product attention step by step\")\n",
    "print(\"  \u2192 Build multi-head attention and positional encoding\")\n",
    "print(\"  \u2192 Assemble a complete Transformer block\")\n",
    "print()\n",
    "print(\"Notebook 4: Building a Tiny Language Model (Mini-GPT)\")\n",
    "print(\"  \u2192 Build a complete GPT-style model (~1M parameters)\")\n",
    "print(\"  \u2192 Train on Shakespeare with next-token prediction\")\n",
    "print(\"  \u2192 Generate text with temperature-controlled sampling\")\n",
    "print()\n",
    "print(\"Let's begin! Open Notebook 1 to start.\")"
   ],
   "id": "cell_2"
  }
 ]
}