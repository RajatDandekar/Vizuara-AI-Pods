{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "N-gram Language Models: Predicting by Counting \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram Language Models: Predicting the Next Word by Counting -- Vizuara\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "Every time your phone suggests the next word, every time a chatbot completes your sentence, every time a search engine guesses what you are about to type -- a **language model** is at work. And it all started with the simplest idea imaginable: **counting.**\n",
    "\n",
    "Before neural networks, before transformers, before GPT -- the dominant approach to language modeling was the **N-gram model.** The idea is disarmingly simple: look at the last few words, and predict what comes next based on how often you have seen that pattern before.\n",
    "\n",
    "In this notebook, you will build an N-gram language model **completely from scratch.** No deep learning libraries, no pre-trained weights -- just Python, counting, and probability. By the end, you will have a working model that can generate (surprisingly coherent) text, and you will understand exactly why this approach eventually hit a wall that only neural networks could overcome.\n",
    "\n",
    "**What you will build:**\n",
    "- A bigram (2-gram) and trigram (3-gram) language model\n",
    "- Probability estimation from raw counts\n",
    "- Text generation using sampling\n",
    "- Perplexity evaluation to measure model quality\n",
    "- Visualizations of the sparsity problem that killed N-grams\n",
    "\n",
    "Let us begin.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "Think about how you predict the next word yourself. If I say \"I went to the grocery ___\", you instantly think \"store.\" Why? Because in your lifetime of reading and listening, you have encountered \"grocery store\" thousands of times -- far more often than \"grocery elephant\" or \"grocery democracy.\"\n",
    "\n",
    "You are doing something remarkably similar to counting: your brain has observed word patterns so frequently that the prediction feels automatic.\n",
    "\n",
    "An N-gram model does exactly this, but explicitly. It counts every pair (or triple, or quadruple) of consecutive words in a large corpus and uses those counts to estimate probabilities.\n",
    "\n",
    "Let us start with the simplest case: what is the probability that a given word appears at all?"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The foundation: word frequencies in a corpus\n",
    "corpus_text = \"\"\"\n",
    "the cat sat on the mat\n",
    "the cat ate the fish\n",
    "the dog sat on the mat\n",
    "the bird flew over the house\n",
    "the cat sat on the rug\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize\n",
    "words = corpus_text.lower().split()\n",
    "total_words = len(words)\n",
    "\n",
    "# Count each word\n",
    "from collections import Counter\n",
    "word_counts = Counter(words)\n",
    "\n",
    "print(\"Word frequencies:\")\n",
    "print(\"-\" * 30)\n",
    "for word, count in word_counts.most_common():\n",
    "    prob = count / total_words\n",
    "    print(f\"  '{word}': count={count}, P('{word}') = {count}/{total_words} = {prob:.3f}\")\n",
    "\n",
    "print(f\"\\nTotal words: {total_words}\")\n",
    "print(f\"Vocabulary size: {len(word_counts)}\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualize these frequencies. This is our first look at the **distributional structure** of language."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sort by frequency\n",
    "words_sorted = word_counts.most_common()\n",
    "labels = [w for w, c in words_sorted]\n",
    "counts = [c for w, c in words_sorted]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart of word frequencies\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(labels)))\n",
    "axes[0].barh(labels[::-1], counts[::-1], color=colors[::-1])\n",
    "axes[0].set_xlabel('Count', fontsize=12)\n",
    "axes[0].set_title('Word Frequencies in Our Tiny Corpus', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Zipf's law preview: rank vs frequency\n",
    "ranks = np.arange(1, len(counts) + 1)\n",
    "axes[1].plot(ranks, sorted(counts, reverse=True), 'o-', color='#2196F3', markersize=8, linewidth=2)\n",
    "axes[1].set_xlabel('Rank', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title(\"Rank vs. Frequency (Zipf's Law Preview)\", fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice something: \"the\" appears far more often than any other word. This is a universal property of natural language called **Zipf's Law** -- a handful of words account for most of the text, while most words are rare. This will matter a lot when we hit the sparsity problem later.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. The Mathematics\n",
    "\n",
    "A **language model** assigns a probability to a sequence of words $w_1, w_2, \\ldots, w_n$. Using the **chain rule of probability**, we decompose:\n",
    "\n",
    "$$P(w_1, w_2, \\ldots, w_n) = \\prod_{i=1}^{n} P(w_i \\mid w_1, \\ldots, w_{i-1})$$\n",
    "\n",
    "The problem: estimating $P(w_i \\mid w_1, \\ldots, w_{i-1})$ requires seeing the exact sequence $w_1, \\ldots, w_{i-1}$ many times. For long sequences, this is impossible.\n",
    "\n",
    "The **Markov assumption** fixes this by truncating the history:\n",
    "\n",
    "- **Bigram** (n=2): $P(w_i \\mid w_1, \\ldots, w_{i-1}) \\approx P(w_i \\mid w_{i-1})$\n",
    "- **Trigram** (n=3): $P(w_i \\mid w_1, \\ldots, w_{i-1}) \\approx P(w_i \\mid w_{i-2}, w_{i-1})$\n",
    "\n",
    "And we estimate these conditional probabilities by counting:\n",
    "\n",
    "$$P(w_i \\mid w_{i-1}) = \\frac{\\text{Count}(w_{i-1}, w_i)}{\\text{Count}(w_{i-1})}$$\n",
    "\n",
    "Let us verify this computationally. If our corpus has \"cat sat\" appearing 2 times and \"cat\" appearing 3 times, then $P(\\text{sat} \\mid \\text{cat}) = 2/3$."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute this by hand and verify\n",
    "# Chain rule: P(\"the cat sat\") = P(\"the\") * P(\"cat\"|\"the\") * P(\"sat\"|\"cat\")\n",
    "\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the cat ate the fish\",\n",
    "    \"the dog sat on the mat\",\n",
    "    \"the bird flew over the house\",\n",
    "    \"the cat sat on the rug\",\n",
    "]\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Count unigrams and bigrams\n",
    "unigram_counts = defaultdict(int)\n",
    "bigram_counts = defaultdict(int)\n",
    "\n",
    "for sentence in corpus:\n",
    "    words = sentence.lower().split()\n",
    "    for i in range(len(words)):\n",
    "        unigram_counts[words[i]] += 1\n",
    "        if i < len(words) - 1:\n",
    "            bigram_counts[(words[i], words[i+1])] += 1\n",
    "\n",
    "# Bigram probability\n",
    "def bigram_prob(w1, w2):\n",
    "    \"\"\"P(w2 | w1) = Count(w1, w2) / Count(w1)\"\"\"\n",
    "    if unigram_counts[w1] == 0:\n",
    "        return 0.0\n",
    "    return bigram_counts[(w1, w2)] / unigram_counts[w1]\n",
    "\n",
    "# Compute P(\"the cat sat\")\n",
    "p_the = unigram_counts[\"the\"] / sum(unigram_counts.values())\n",
    "p_cat_given_the = bigram_prob(\"the\", \"cat\")\n",
    "p_sat_given_cat = bigram_prob(\"cat\", \"sat\")\n",
    "\n",
    "print(\"Computing P('the cat sat') using the chain rule + bigram assumption:\")\n",
    "print(f\"  P('the')         = {unigram_counts['the']}/{sum(unigram_counts.values())} = {p_the:.4f}\")\n",
    "print(f\"  P('cat'|'the')   = {bigram_counts[('the','cat')]}/{unigram_counts['the']} = {p_cat_given_the:.4f}\")\n",
    "print(f\"  P('sat'|'cat')   = {bigram_counts[('cat','sat')]}/{unigram_counts['cat']} = {p_sat_given_cat:.4f}\")\n",
    "print(f\"\\n  P('the cat sat') = {p_the:.4f} \u00d7 {p_cat_given_the:.4f} \u00d7 {p_sat_given_cat:.4f} = {p_the * p_cat_given_the * p_sat_given_cat:.6f}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "Now let us build a proper N-gram language model class that can handle any value of n."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class NgramLanguageModel:\n",
    "    \"\"\"\n",
    "    An N-gram language model built from scratch.\n",
    "\n",
    "    This model estimates P(next_word | previous n-1 words)\n",
    "    by counting occurrences in the training corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n=2, smoothing=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n: The N in N-gram (2 = bigram, 3 = trigram, etc.)\n",
    "            smoothing: Laplace smoothing parameter (0 = no smoothing)\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.smoothing = smoothing\n",
    "        self.ngram_counts = defaultdict(int)   # Count of full n-grams\n",
    "        self.context_counts = defaultdict(int)  # Count of (n-1)-gram contexts\n",
    "        self.vocabulary = set()\n",
    "        self.total_tokens = 0\n",
    "\n",
    "    def _get_ngrams(self, tokens):\n",
    "        \"\"\"Extract all n-grams from a list of tokens.\"\"\"\n",
    "        # Add start/end tokens for proper sentence boundaries\n",
    "        padded = [\"<s>\"] * (self.n - 1) + tokens + [\"</s>\"]\n",
    "        ngrams = []\n",
    "        for i in range(len(padded) - self.n + 1):\n",
    "            ngram = tuple(padded[i:i + self.n])\n",
    "            ngrams.append(ngram)\n",
    "        return ngrams\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Train the model by counting n-grams in the corpus.\n",
    "\n",
    "        Args:\n",
    "            corpus: List of sentences (strings)\n",
    "        \"\"\"\n",
    "        for sentence in corpus:\n",
    "            tokens = sentence.lower().split()\n",
    "            self.vocabulary.update(tokens)\n",
    "            self.total_tokens += len(tokens)\n",
    "\n",
    "            ngrams = self._get_ngrams(tokens)\n",
    "            for ngram in ngrams:\n",
    "                context = ngram[:-1]  # First n-1 words\n",
    "                self.ngram_counts[ngram] += 1\n",
    "                self.context_counts[context] += 1\n",
    "\n",
    "        self.vocabulary.add(\"<s>\")\n",
    "        self.vocabulary.add(\"</s>\")\n",
    "        print(f\"Trained {self.n}-gram model:\")\n",
    "        print(f\"  Vocabulary size: {len(self.vocabulary)}\")\n",
    "        print(f\"  Total tokens: {self.total_tokens}\")\n",
    "        print(f\"  Unique {self.n}-grams: {len(self.ngram_counts)}\")\n",
    "        print(f\"  Unique contexts: {len(self.context_counts)}\")\n",
    "\n",
    "    def probability(self, word, context):\n",
    "        \"\"\"\n",
    "        Compute P(word | context) using counts + optional smoothing.\n",
    "\n",
    "        Args:\n",
    "            word: The word to predict\n",
    "            context: Tuple of previous (n-1) words\n",
    "        \"\"\"\n",
    "        ngram = context + (word,)\n",
    "\n",
    "        numerator = self.ngram_counts[ngram] + self.smoothing\n",
    "        denominator = self.context_counts[context] + self.smoothing * len(self.vocabulary)\n",
    "\n",
    "        if denominator == 0:\n",
    "            return 1 / len(self.vocabulary)  # Uniform fallback\n",
    "\n",
    "        return numerator / denominator\n",
    "\n",
    "    def get_distribution(self, context):\n",
    "        \"\"\"Get the full probability distribution over vocabulary given a context.\"\"\"\n",
    "        dist = {}\n",
    "        for word in self.vocabulary:\n",
    "            dist[word] = self.probability(word, context)\n",
    "        return dist\n",
    "\n",
    "# Train a bigram model\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the cat ate the fish\",\n",
    "    \"the dog sat on the mat\",\n",
    "    \"the bird flew over the house\",\n",
    "    \"the cat sat on the rug\",\n",
    "]\n",
    "\n",
    "bigram_model = NgramLanguageModel(n=2)\n",
    "bigram_model.train(corpus)"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us examine what the model learned:"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the model predict after \"the\"?\n",
    "context = (\"the\",)\n",
    "dist = bigram_model.get_distribution(context)\n",
    "\n",
    "# Sort by probability\n",
    "sorted_dist = sorted(dist.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"P(word | 'the') -- top predictions:\")\n",
    "print(\"-\" * 40)\n",
    "for word, prob in sorted_dist[:10]:\n",
    "    bar = \"\u2588\" * int(prob * 50)\n",
    "    print(f\"  {word:12s}  {prob:.3f}  {bar}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the bigram transition matrix\n",
    "# This is the heart of the N-gram model -- a lookup table of probabilities\n",
    "\n",
    "# Get the most common words for visualization\n",
    "common_words = [w for w, _ in Counter(\n",
    "    [w for s in corpus for w in s.lower().split()]\n",
    ").most_common(8)]\n",
    "common_words = [\"<s>\"] + common_words + [\"</s>\"]\n",
    "\n",
    "# Build transition matrix\n",
    "matrix = np.zeros((len(common_words), len(common_words)))\n",
    "for i, w1 in enumerate(common_words):\n",
    "    for j, w2 in enumerate(common_words):\n",
    "        matrix[i, j] = bigram_model.probability(w2, (w1,))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(matrix, cmap='YlOrRd', aspect='auto')\n",
    "\n",
    "ax.set_xticks(range(len(common_words)))\n",
    "ax.set_yticks(range(len(common_words)))\n",
    "ax.set_xticklabels(common_words, rotation=45, ha='right', fontsize=11)\n",
    "ax.set_yticklabels(common_words, fontsize=11)\n",
    "\n",
    "# Annotate cells with probabilities\n",
    "for i in range(len(common_words)):\n",
    "    for j in range(len(common_words)):\n",
    "        if matrix[i, j] > 0.01:\n",
    "            color = 'white' if matrix[i, j] > 0.3 else 'black'\n",
    "            ax.text(j, i, f'{matrix[i,j]:.2f}', ha='center', va='center',\n",
    "                    fontsize=9, color=color, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Next Word', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Current Word', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Bigram Transition Probabilities\\nP(next word | current word)',\n",
    "             fontsize=15, fontweight='bold')\n",
    "plt.colorbar(im, label='Probability')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Your Turn\n",
    "\n",
    "**TODO 1: Build a Trigram Model**\n",
    "\n",
    "The bigram model only looks at the previous word. A trigram model looks at the previous TWO words, which should give better predictions."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a trigram model on the same corpus and compare\n",
    "#\n",
    "# Instructions:\n",
    "# 1. Create a trigram model using NgramLanguageModel(n=3)\n",
    "# 2. Train it on the corpus\n",
    "# 3. Compare P(\"sat\" | \"cat\") from bigram vs P(\"sat\" | \"the\", \"cat\") from trigram\n",
    "# 4. Which gives a higher probability? Why?\n",
    "\n",
    "# YOUR CODE HERE\n",
    "trigram_model = NgramLanguageModel(n=3)\n",
    "# trigram_model.train(???)\n",
    "#\n",
    "# bigram_p = bigram_model.probability(\"sat\", (\"cat\",))\n",
    "# trigram_p = trigram_model.probability(\"sat\", (\"the\", \"cat\"))\n",
    "#\n",
    "# print(f\"Bigram  P('sat' | 'cat')        = {bigram_p:.4f}\")\n",
    "# print(f\"Trigram P('sat' | 'the', 'cat') = {trigram_p:.4f}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2: Implement Laplace Smoothing**\n",
    "\n",
    "Without smoothing, unseen n-grams get probability zero. Laplace smoothing adds a small count to every possible n-gram.\n",
    "\n",
    "The smoothed probability is:\n",
    "\n",
    "$$P_{\\text{smooth}}(w_i \\mid w_{i-1}) = \\frac{\\text{Count}(w_{i-1}, w_i) + \\alpha}{\\text{Count}(w_{i-1}) + \\alpha \\cdot |V|}$$\n",
    "\n",
    "where $|V|$ is the vocabulary size and $\\alpha$ is usually 1."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare predictions with and without smoothing\n",
    "#\n",
    "# Instructions:\n",
    "# 1. Create a smoothed bigram model with smoothing=1.0\n",
    "# 2. Train it on the same corpus\n",
    "# 3. Compare P(\"ran\" | \"cat\") with and without smoothing\n",
    "# 4. What happens to zero-probability events?\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# smoothed_model = NgramLanguageModel(n=2, smoothing=1.0)\n",
    "# smoothed_model.train(corpus)\n",
    "#\n",
    "# p_no_smooth = bigram_model.probability(\"ran\", (\"cat\",))\n",
    "# p_smooth = smoothed_model.probability(\"ran\", (\"cat\",))\n",
    "#\n",
    "# print(f\"Without smoothing: P('ran' | 'cat') = {p_no_smooth:.6f}\")\n",
    "# print(f\"With smoothing:    P('ran' | 'cat') = {p_smooth:.6f}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Putting It All Together\n",
    "\n",
    "Now let us add text generation. The model samples the next word from its probability distribution, then uses that word as context for the next prediction."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramGenerator(NgramLanguageModel):\n",
    "    \"\"\"Extends the N-gram model with text generation capabilities.\"\"\"\n",
    "\n",
    "    def generate(self, max_length=20, temperature=1.0, seed=None):\n",
    "        \"\"\"\n",
    "        Generate text by sampling from the model.\n",
    "\n",
    "        Args:\n",
    "            max_length: Maximum number of words to generate\n",
    "            temperature: Controls randomness (lower = more deterministic)\n",
    "            seed: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        # Start with the beginning-of-sentence context\n",
    "        context = tuple([\"<s>\"] * (self.n - 1))\n",
    "        generated = []\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            # Get probability distribution\n",
    "            dist = self.get_distribution(context)\n",
    "\n",
    "            # Apply temperature\n",
    "            words = list(dist.keys())\n",
    "            probs = np.array([dist[w] for w in words])\n",
    "\n",
    "            # Temperature scaling\n",
    "            if temperature != 1.0:\n",
    "                log_probs = np.log(probs + 1e-10)\n",
    "                scaled = log_probs / temperature\n",
    "                probs = np.exp(scaled) / np.sum(np.exp(scaled))\n",
    "\n",
    "            # Normalize\n",
    "            probs = probs / probs.sum()\n",
    "\n",
    "            # Sample\n",
    "            idx = np.random.choice(len(words), p=probs)\n",
    "            next_word = words[idx]\n",
    "\n",
    "            if next_word == \"</s>\":\n",
    "                break\n",
    "\n",
    "            generated.append(next_word)\n",
    "\n",
    "            # Update context: slide the window\n",
    "            context = tuple(list(context[1:]) + [next_word])\n",
    "\n",
    "        return \" \".join(generated)\n",
    "\n",
    "# Build and train the generator\n",
    "gen_model = NgramGenerator(n=2, smoothing=0.1)\n",
    "gen_model.train(corpus)\n",
    "\n",
    "# Generate several sentences\n",
    "print(\"Generated sentences (bigram, temperature=1.0):\")\n",
    "print(\"=\" * 50)\n",
    "for i in range(10):\n",
    "    text = gen_model.generate(max_length=15, temperature=1.0, seed=i)\n",
    "    print(f\"  {i+1}. {text}\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how temperature affects generation:"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "temperatures = [0.3, 1.0, 2.0]\n",
    "labels = [\"Low (0.3)\\nMore deterministic\", \"Normal (1.0)\\nBalanced\", \"High (2.0)\\nMore random\"]\n",
    "\n",
    "for ax, temp, label in zip(axes, temperatures, labels):\n",
    "    sentences = []\n",
    "    for seed in range(8):\n",
    "        text = gen_model.generate(max_length=10, temperature=temp, seed=seed*10)\n",
    "        sentences.append(text)\n",
    "\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(-0.5, len(sentences) - 0.5)\n",
    "    for i, s in enumerate(sentences):\n",
    "        ax.text(0.05, len(sentences) - 1 - i, s, fontsize=9,\n",
    "                fontfamily='monospace', verticalalignment='center')\n",
    "    ax.set_title(f'Temperature = {temp}\\n{label}', fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Effect of Temperature on Text Generation', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us build on a real corpus to see the model in action:"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a larger corpus -- nursery rhymes and simple text\n",
    "large_corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"the cat ate the fish\",\n",
    "    \"the dog ate the bone\",\n",
    "    \"the bird flew over the house\",\n",
    "    \"the bird sat on the tree\",\n",
    "    \"the cat ran after the dog\",\n",
    "    \"the dog ran after the cat\",\n",
    "    \"the fish swam in the pond\",\n",
    "    \"the bird flew over the pond\",\n",
    "    \"i like the cat\",\n",
    "    \"i like the dog\",\n",
    "    \"i saw the bird fly\",\n",
    "    \"the cat is on the mat\",\n",
    "    \"the dog is on the rug\",\n",
    "    \"the rug is on the floor\",\n",
    "    \"i want to eat the fish\",\n",
    "    \"the cat wants to eat the fish\",\n",
    "    \"the dog wants to play with the cat\",\n",
    "    \"i saw the cat on the mat\",\n",
    "    \"the bird is in the tree\",\n",
    "    \"the fish is in the water\",\n",
    "    \"i like to see the cat sit on the mat\",\n",
    "    \"the cat and the dog are friends\",\n",
    "    \"i have a cat and a dog\",\n",
    "]\n",
    "\n",
    "# Train bigram and trigram generators\n",
    "bigram_gen = NgramGenerator(n=2, smoothing=0.1)\n",
    "bigram_gen.train(large_corpus)\n",
    "\n",
    "trigram_gen = NgramGenerator(n=3, smoothing=0.1)\n",
    "trigram_gen.train(large_corpus)\n",
    "\n",
    "print(\"\\n--- Bigram Generated Sentences ---\")\n",
    "for i in range(5):\n",
    "    print(f\"  {bigram_gen.generate(max_length=12, seed=i+42)}\")\n",
    "\n",
    "print(\"\\n--- Trigram Generated Sentences ---\")\n",
    "for i in range(5):\n",
    "    print(f\"  {trigram_gen.generate(max_length=12, seed=i+42)}\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Training and Results\n",
    "\n",
    "Let us measure our model's quality using **perplexity** -- the standard evaluation metric for language models. Perplexity measures how \"surprised\" the model is by test data.\n",
    "\n",
    "$$\\text{Perplexity} = \\exp\\!\\left(-\\frac{1}{N}\\sum_{i=1}^{N}\\log P(w_i \\mid \\text{context})\\right)$$\n",
    "\n",
    "Lower perplexity = better model. A perplexity of $k$ means the model is as uncertain as if it were choosing uniformly among $k$ words at each step."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, test_sentences):\n",
    "    \"\"\"\n",
    "    Compute perplexity of the model on test sentences.\n",
    "\n",
    "    Perplexity = exp(-1/N * sum(log P(w_i | context)))\n",
    "    \"\"\"\n",
    "    total_log_prob = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for sentence in test_sentences:\n",
    "        tokens = sentence.lower().split()\n",
    "        padded = [\"<s>\"] * (model.n - 1) + tokens + [\"</s>\"]\n",
    "\n",
    "        for i in range(model.n - 1, len(padded)):\n",
    "            context = tuple(padded[i - model.n + 1:i])\n",
    "            word = padded[i]\n",
    "\n",
    "            prob = model.probability(word, context)\n",
    "            if prob > 0:\n",
    "                total_log_prob += np.log(prob)\n",
    "            else:\n",
    "                total_log_prob += np.log(1e-10)  # Avoid -inf\n",
    "\n",
    "            total_tokens += 1\n",
    "\n",
    "    avg_log_prob = total_log_prob / total_tokens\n",
    "    perplexity = np.exp(-avg_log_prob)\n",
    "    return perplexity\n",
    "\n",
    "# Test on held-out sentences\n",
    "test_sentences = [\n",
    "    \"the cat sat on the rug\",\n",
    "    \"the dog ate the fish\",\n",
    "    \"the bird flew over the tree\",\n",
    "]\n",
    "\n",
    "# Compare models\n",
    "models = {\n",
    "    \"Bigram (no smooth)\": NgramLanguageModel(n=2, smoothing=0.0),\n",
    "    \"Bigram (smooth=0.1)\": NgramLanguageModel(n=2, smoothing=0.1),\n",
    "    \"Bigram (smooth=1.0)\": NgramLanguageModel(n=2, smoothing=1.0),\n",
    "    \"Trigram (smooth=0.1)\": NgramLanguageModel(n=3, smoothing=0.1),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.train(large_corpus)\n",
    "    ppl = compute_perplexity(model, test_sentences)\n",
    "    results[name] = ppl\n",
    "    print(f\"  {name:25s}  Perplexity = {ppl:.2f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "names = list(results.keys())\n",
    "ppls = list(results.values())\n",
    "colors = ['#E53935', '#FB8C00', '#43A047', '#1E88E5']\n",
    "bars = ax.bar(names, ppls, color=colors, edgecolor='white', linewidth=2)\n",
    "\n",
    "for bar, ppl in zip(bars, ppls):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "            f'{ppl:.1f}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Perplexity (lower is better)', fontsize=12)\n",
    "ax.set_title('Model Comparison: Perplexity on Test Set', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, max(ppls) * 1.2)\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Final Output\n",
    "\n",
    "Now let us demonstrate the fundamental limitation of N-grams: the **sparsity problem.**"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sparsity problem: visualizing how much of the count table is empty\n",
    "\n",
    "# With just 12 unique words, how many possible bigrams are there?\n",
    "vocab = list(bigram_gen.vocabulary)\n",
    "vocab_size = len(vocab)\n",
    "possible_bigrams = vocab_size ** 2\n",
    "observed_bigrams = len(bigram_gen.ngram_counts)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Possible bigrams: {vocab_size}^2 = {possible_bigrams}\")\n",
    "print(f\"Observed bigrams: {observed_bigrams}\")\n",
    "print(f\"Coverage: {observed_bigrams/possible_bigrams*100:.1f}%\")\n",
    "print(f\"Zero entries: {possible_bigrams - observed_bigrams} ({(1 - observed_bigrams/possible_bigrams)*100:.1f}%)\")\n",
    "\n",
    "# Scale this up to realistic vocabulary sizes\n",
    "vocab_sizes = [100, 1000, 10000, 50000, 100000]\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: possible vs observed (log scale)\n",
    "for v in vocab_sizes:\n",
    "    possible = v ** 2\n",
    "    # Estimate observed: in practice, roughly O(V * avg_context_diversity)\n",
    "    # For real corpora, typically ~ 10*V observed bigrams\n",
    "    estimated_observed = min(10 * v, possible)\n",
    "    axes[0].scatter(v, possible, color='#E53935', s=80, zorder=5)\n",
    "    axes[0].scatter(v, estimated_observed, color='#1E88E5', s=80, zorder=5)\n",
    "\n",
    "axes[0].plot(vocab_sizes, [v**2 for v in vocab_sizes], 'r--', label='Possible bigrams (V\u00b2)', linewidth=2)\n",
    "axes[0].plot(vocab_sizes, [min(10*v, v**2) for v in vocab_sizes], 'b--', label='Observed (\u224810V)', linewidth=2)\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].set_xlabel('Vocabulary Size', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Bigrams', fontsize=12)\n",
    "axes[0].set_title('The Sparsity Explosion', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Right: coverage percentage\n",
    "coverage = [min(10*v, v**2) / (v**2) * 100 for v in vocab_sizes]\n",
    "axes[1].bar(range(len(vocab_sizes)), coverage,\n",
    "            color=['#4CAF50', '#FF9800', '#F44336', '#F44336', '#F44336'],\n",
    "            edgecolor='white', linewidth=2)\n",
    "axes[1].set_xticks(range(len(vocab_sizes)))\n",
    "axes[1].set_xticklabels([f'V={v:,}' for v in vocab_sizes], rotation=15)\n",
    "axes[1].set_ylabel('Coverage (%)', fontsize=12)\n",
    "axes[1].set_title('% of Bigrams Observed in Training', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylim(0, 105)\n",
    "\n",
    "for i, c in enumerate(coverage):\n",
    "    axes[1].text(i, c + 2, f'{c:.1f}%', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe verdict: with a realistic vocabulary of 50,000 words,\")\n",
    "print(\"a bigram model has 2.5 BILLION possible entries, but observes only ~0.02%.\")\n",
    "print(\"Most word pairs get probability ZERO \u2014 not because they are impossible,\")\n",
    "print(\"but because we simply never saw them. This is the sparsity problem.\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The similarity problem: N-grams see no relationship between similar words\n",
    "\n",
    "print(\"The Similarity Blind Spot\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"To an N-gram model, 'cat' and 'dog' are COMPLETELY unrelated.\")\n",
    "print(\"Learning 'the cat sat' tells it NOTHING about 'the dog sat'.\")\n",
    "print()\n",
    "\n",
    "# Demonstrate\n",
    "pairs = [(\"cat\", \"sat\"), (\"dog\", \"sat\"), (\"cat\", \"ran\"), (\"dog\", \"ran\")]\n",
    "print(\"Bigram probabilities:\")\n",
    "for w1, w2 in pairs:\n",
    "    p = bigram_gen.probability(w2, (w1,))\n",
    "    status = \"\u2713 seen\" if bigram_gen.ngram_counts[(w1, w2)] > 0 else \"\u2717 unseen\"\n",
    "    print(f\"  P('{w2}' | '{w1}') = {p:.4f}  [{status}]\")\n",
    "\n",
    "print()\n",
    "print(\"A human knows that cats and dogs can both 'sit' and 'run.'\")\n",
    "print(\"But to the N-gram model, each word is just an arbitrary symbol.\")\n",
    "print()\n",
    "print(\"\u2192 This is the fundamental limitation that neural language models solve.\")\n",
    "print(\"  By learning EMBEDDINGS \u2014 dense vectors where similar words are nearby \u2014\")\n",
    "print(\"  knowledge about 'cat' automatically transfers to 'dog'.\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "1. **Language models assign probabilities to word sequences.** The chain rule lets us decompose this into a product of conditional probabilities.\n",
    "\n",
    "2. **N-gram models estimate these probabilities by counting.** Simple, interpretable, and fast \u2014 but brittle.\n",
    "\n",
    "3. **The Markov assumption trades accuracy for tractability.** Bigrams look at 1 word of history, trigrams look at 2, etc.\n",
    "\n",
    "4. **Smoothing helps but does not solve the fundamental problem.** Adding small counts to unseen n-grams prevents zero probabilities, but does not capture word similarity.\n",
    "\n",
    "5. **The sparsity problem grows exponentially with vocabulary size.** With 50K words, most bigram entries are zero.\n",
    "\n",
    "6. **N-grams have no notion of semantic similarity.** \"Cat\" and \"dog\" are as different as \"cat\" and \"democracy.\"\n",
    "\n",
    "**What comes next:**\n",
    "\n",
    "In the next notebook, we will see how **neural language models** solve both of these problems at once. By representing words as dense vectors (embeddings), a neural network learns that similar words should have similar predictions \u2014 and knowledge transfers automatically.\n",
    "\n",
    "This is the leap from **counting** to **learning.**"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"  NOTEBOOK COMPLETE: N-gram Language Models\")\n",
    "print(\"  You built a bigram/trigram model from scratch,\")\n",
    "print(\"  generated text, measured perplexity, and saw\")\n",
    "print(\"  why counting alone cannot capture language.\")\n",
    "print()\n",
    "print(\"  Next: Neural Language Models & Word Embeddings\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "cell_28"
  }
 ]
}