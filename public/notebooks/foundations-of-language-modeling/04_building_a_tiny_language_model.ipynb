{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Building a Tiny Language Model (Mini-GPT) ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"17rFuCNZUUY1xHrMq1WTamV-JWh_IDZe8\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/04_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_02_data_tokenizer",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Data Tokenizer\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_02_data_tokenizer.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_01_motivation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Motivation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_01_motivation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_01_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_01_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Tiny Language Model (Mini-GPT) -- Vizuara\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "You have now built every component of a Transformer language model from scratch: N-gram counting, word embeddings, self-attention, multi-head attention, positional encoding, and Transformer blocks. In this notebook, we bring it all together.\n",
    "\n",
    "We will build, train, and generate text from a **complete GPT-style language model** -- a miniature version of the architecture behind GPT-2, GPT-3, and every modern LLM. The model will learn from Shakespeare's text and generate new passages in his style.\n",
    "\n",
    "This is the culmination of the entire \"Foundations of Language Modeling\" journey: from counting word pairs to training a Transformer that can write coherent text.\n",
    "\n",
    "**What you will build:**\n",
    "- A complete mini-GPT model (~1M parameters)\n",
    "- A character-level tokenizer\n",
    "- The training loop with next-token prediction loss\n",
    "- Text generation with temperature-controlled sampling\n",
    "- Perplexity evaluation and training visualizations\n",
    "- Everything runs on a free T4 GPU in under 10 minutes\n",
    "\n",
    "Let us build a language model.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "A GPT-style language model is conceptually simple:\n",
    "\n",
    "1. Take a sequence of tokens\n",
    "2. Pass them through token embeddings + positional encoding\n",
    "3. Pass through N stacked Transformer blocks (with causal masking)\n",
    "4. Project the final hidden states to vocabulary logits\n",
    "5. The loss is: at each position, how well did the model predict the *next* token?\n",
    "\n",
    "The training objective -- **next-token prediction** -- is self-supervised. The training data IS the labels. Every token in a sentence serves as the prediction target for the tokens before it."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Let's load some text data -- Shakespeare's works\n",
    "# On Colab, we can download it directly\n",
    "\n",
    "import urllib.request\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "try:\n",
    "    response = urllib.request.urlopen(url)\n",
    "    text = response.read().decode('utf-8')\n",
    "except:\n",
    "    # Fallback: generate some sample text\n",
    "    text = \"\"\"First Citizen:\n",
    "Before we proceed any further, hear me speak.\n",
    "\n",
    "All:\n",
    "Speak, speak.\n",
    "\n",
    "First Citizen:\n",
    "You are all resolved rather to die than to famish?\n",
    "\n",
    "All:\n",
    "Resolved. resolved.\n",
    "\n",
    "First Citizen:\n",
    "First, you know Caius Marcius is chief enemy to the people.\n",
    "\n",
    "All:\n",
    "We know't, we know't.\n",
    "\n",
    "First Citizen:\n",
    "Let us kill him, and we'll have corn at our own price.\n",
    "\"\"\" * 100\n",
    "\n",
    "print(f\"Text length: {len(text):,} characters\")\n",
    "print(f\"First 200 characters:\")\n",
    "print(text[:200])"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a character-level tokenizer\n",
    "# GPT uses BPE (byte-pair encoding), but character-level is simpler to understand\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Character to index mapping\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2char = {i: ch for ch, i in char2idx.items()}\n",
    "\n",
    "# Encode/decode functions\n",
    "def encode(s):\n",
    "    return [char2idx[c] for c in s]\n",
    "\n",
    "def decode(indices):\n",
    "    return ''.join([idx2char[i] for i in indices])\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Characters: {''.join(chars[:30])}...\")\n",
    "print(f\"\\nEncoding 'Hello': {encode('Hello')}\")\n",
    "print(f\"Decoding back: '{decode(encode('Hello'))}'\")\n",
    "\n",
    "# Encode the entire dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"\\nFull dataset: {data.shape[0]:,} tokens\")\n",
    "\n",
    "# Train/val split (90/10)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(f\"Train: {len(train_data):,} tokens\")\n",
    "print(f\"Val:   {len(val_data):,} tokens\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_04_mathematics",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Mathematics\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_04_mathematics.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. The Mathematics\n",
    "\n",
    "The training loss is **cross-entropy** summed over all positions:\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{T}\\sum_{t=1}^{T} \\log P(w_t \\mid w_1, w_2, \\ldots, w_{t-1})$$\n",
    "\n",
    "Where $P(w_t \\mid w_{1:t-1})$ comes from softmax over the model's logits at position $t$.\n",
    "\n",
    "The key insight: the causal mask ensures that position $t$ can only attend to positions $1, \\ldots, t$, so each position makes a genuine prediction about the future.\n",
    "\n",
    "**Perplexity** is the exponentiated average loss:\n",
    "\n",
    "$$\\text{Perplexity} = \\exp(\\mathcal{L})$$\n",
    "\n",
    "A perplexity of $k$ means the model is as uncertain as if it were choosing uniformly among $k$ options at each step."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_05_loss_example",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Loss Example\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_05_loss_example.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's understand the loss with a concrete example\n",
    "\n",
    "# Suppose our model processes \"Hello\" and we want to compute the loss\n",
    "example = \"Hello\"\n",
    "example_tokens = torch.tensor(encode(example))\n",
    "\n",
    "print(f\"Input tokens:  {example_tokens[:-1].tolist()} ‚Üí {[idx2char[i.item()] for i in example_tokens[:-1]]}\")\n",
    "print(f\"Target tokens: {example_tokens[1:].tolist()} ‚Üí {[idx2char[i.item()] for i in example_tokens[1:]]}\")\n",
    "\n",
    "print(f\"\\nAt each position, the model predicts the NEXT character:\")\n",
    "for i in range(len(example) - 1):\n",
    "    context = example[:i+1]\n",
    "    target = example[i+1]\n",
    "    print(f\"  Given '{context}' ‚Üí predict '{target}'\")\n",
    "\n",
    "print(f\"\\nThe loss measures how well these predictions match reality.\")\n",
    "print(f\"A random model with vocab_size={vocab_size} has:\")\n",
    "print(f\"  Loss = -log(1/{vocab_size}) = {-np.log(1/vocab_size):.2f}\")\n",
    "print(f\"  Perplexity = {vocab_size}\")\n",
    "print(f\"\\nA perfect model has Loss = 0, Perplexity = 1\")"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_06_transition_build",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Transition: Transition Build\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_06_transition_build.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Let's Build It -- Component by Component"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_07_hyperparameters",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Hyperparameters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_07_hyperparameters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for our mini-GPT\n",
    "BATCH_SIZE = 64      # How many sequences to process in parallel\n",
    "BLOCK_SIZE = 128     # Maximum context length\n",
    "D_MODEL = 128        # Embedding dimension\n",
    "NUM_HEADS = 4        # Number of attention heads\n",
    "NUM_LAYERS = 4       # Number of transformer blocks\n",
    "DROPOUT = 0.1        # Dropout rate\n",
    "LEARNING_RATE = 3e-4 # Adam learning rate\n",
    "MAX_ITERS = 3000     # Training iterations\n",
    "EVAL_INTERVAL = 300  # Evaluate every N iterations\n",
    "EVAL_ITERS = 100     # Number of batches for evaluation\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_08_data_batching",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Data Batching\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_08_data_batching.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading: create random batches of (context, target) pairs\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    Generate a batch of training data.\n",
    "\n",
    "    Returns:\n",
    "        x: (batch_size, block_size) input tokens\n",
    "        y: (batch_size, block_size) target tokens (shifted by 1)\n",
    "    \"\"\"\n",
    "    data_split = train_data if split == 'train' else val_data\n",
    "\n",
    "    # Random starting positions\n",
    "    ix = torch.randint(len(data_split) - BLOCK_SIZE, (BATCH_SIZE,))\n",
    "\n",
    "    x = torch.stack([data_split[i:i+BLOCK_SIZE] for i in ix])\n",
    "    y = torch.stack([data_split[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
    "\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "# Test\n",
    "xb, yb = get_batch('train')\n",
    "print(f\"Input batch shape:  {xb.shape}\")\n",
    "print(f\"Target batch shape: {yb.shape}\")\n",
    "print(f\"\\nFirst sequence (first 30 chars):\")\n",
    "print(f\"  Input:  '{decode(xb[0][:30].tolist())}'\")\n",
    "print(f\"  Target: '{decode(yb[0][:30].tolist())}'\")\n",
    "print(f\"\\nNotice: target is input shifted right by 1 position.\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_09_model_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Model Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_09_model_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us build the complete model."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_10a_model_components",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: 10a Model Components\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_10a_model_components.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_10b_full_model",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: 10b Full Model\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_10b_full_model.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_16_training_loop",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Training Loop\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_16_training_loop.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention with causal masking.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Causal mask: prevent attending to future positions\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(block_size, block_size), diagonal=1).bool()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Compute Q, K, V in one projection\n",
    "        qkv = self.qkv_proj(x)\n",
    "        Q, K, V = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # Reshape for multi-head\n",
    "        Q = Q.view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        scores = scores.masked_fill(self.mask[:T, :T], float('-inf'))\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        weights = self.attn_dropout(weights)\n",
    "\n",
    "        out = torch.matmul(weights, V)\n",
    "\n",
    "        # Concatenate heads\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        out = self.resid_dropout(self.out_proj(out))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MiniGPTBlock(nn.Module):\n",
    "    \"\"\"Transformer block: attention + feed-forward with residuals.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadSelfAttention(d_model, num_heads, block_size, dropout)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-norm architecture (used in GPT-2 and later)\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    A miniature GPT language model.\n",
    "\n",
    "    Architecture:\n",
    "    - Token embedding + positional embedding\n",
    "    - N stacked Transformer blocks (with causal attention)\n",
    "    - Final layer norm + linear projection to vocabulary\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers,\n",
    "                 block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(block_size, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            MiniGPTBlock(d_model, num_heads, block_size, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        # Weight tying: share weights between token embedding and output projection\n",
    "        self.head.weight = self.token_emb.weight\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx: (batch_size, seq_len) token indices\n",
    "            targets: optional (batch_size, seq_len) target indices\n",
    "        Returns:\n",
    "            logits: (batch_size, seq_len, vocab_size)\n",
    "            loss: scalar cross-entropy loss (if targets provided)\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Token + position embeddings\n",
    "        tok_emb = self.token_emb(idx)\n",
    "        pos_emb = self.pos_emb(torch.arange(T, device=idx.device))\n",
    "        x = self.drop(tok_emb + pos_emb)\n",
    "\n",
    "        # Transformer blocks\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # Final norm + project to vocab\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        # Compute loss if targets are provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Autoregressive text generation.\n",
    "\n",
    "        At each step: predict next token ‚Üí sample ‚Üí append ‚Üí repeat.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop to block_size if sequence is too long\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "\n",
    "            # Forward pass\n",
    "            logits, _ = self(idx_cond)\n",
    "\n",
    "            # Get logits for the last position only\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            # Optional top-k filtering\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "            # Sample from the distribution\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # Append sampled token\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = MiniGPT(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"MiniGPT Model Summary:\")\n",
    "print(f\"  Vocabulary size:  {vocab_size}\")\n",
    "print(f\"  d_model:          {D_MODEL}\")\n",
    "print(f\"  Attention heads:  {NUM_HEADS}\")\n",
    "print(f\"  Transformer layers: {NUM_LAYERS}\")\n",
    "print(f\"  Context length:   {BLOCK_SIZE}\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"\\n  This is {total_params/1e6:.1f}M parameters ‚Äî about 100x smaller than GPT-2 Small (124M)\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_11_todo1_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo1 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_11_todo1_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Your Turn\n",
    "\n",
    "**TODO 1: Generate Text BEFORE Training**\n",
    "\n",
    "Let us see what the untrained model produces -- it should be complete gibberish."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate text from the untrained model\n",
    "#\n",
    "# Instructions:\n",
    "# 1. Create a starting context (e.g., a newline character)\n",
    "# 2. Call model.generate() with max_new_tokens=200\n",
    "# 3. Decode and print the output\n",
    "# 4. It should be random garbage -- the model hasn't learned anything yet\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# generated = model.generate(context, max_new_tokens=200)\n",
    "# print(\"Untrained model output:\")\n",
    "# print(decode(generated[0].tolist()))"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_12_todo1_after",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo1 After\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_12_todo1_after.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_13_todo2_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo2 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_13_todo2_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2: Implement Perplexity Tracking**\n",
    "\n",
    "Write a function that computes perplexity during training."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement perplexity computation\n",
    "#\n",
    "# Recall: Perplexity = exp(average_loss)\n",
    "#\n",
    "# Instructions:\n",
    "# 1. Average the loss over EVAL_ITERS random batches\n",
    "# 2. Compute perplexity = exp(average_loss)\n",
    "# 3. Report both train and val perplexity\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# @torch.no_grad()\n",
    "# def estimate_perplexity():\n",
    "#     model.eval()\n",
    "#     results = {}\n",
    "#     for split in ['train', 'val']:\n",
    "#         losses = []\n",
    "#         for _ in range(EVAL_ITERS):\n",
    "#             X, Y = get_batch(split)\n",
    "#             _, loss = model(X, Y)\n",
    "#             losses.append(loss.item())\n",
    "#         avg_loss = np.mean(losses)\n",
    "#         results[split] = {\n",
    "#             'loss': avg_loss,\n",
    "#             'perplexity': np.exp(avg_loss)\n",
    "#         }\n",
    "#     model.train()\n",
    "#     return results"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_14_todo2_after",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo2 After\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_14_todo2_after.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_15_training_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Transition: Training Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_15_training_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Putting It All Together\n",
    "\n",
    "Time to train the model."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"Estimate loss on train and val sets.\"\"\"\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(EVAL_ITERS)\n",
    "        for k in range(EVAL_ITERS):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_perplexities = []\n",
    "val_perplexities = []\n",
    "\n",
    "print(f\"Training MiniGPT for {MAX_ITERS} iterations...\")\n",
    "print(f\"{'Iter':>6s} | {'Train Loss':>10s} | {'Val Loss':>10s} | {'Train PPL':>10s} | {'Val PPL':>10s}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for iter_num in range(MAX_ITERS):\n",
    "    # Evaluate periodically\n",
    "    if iter_num % EVAL_INTERVAL == 0 or iter_num == MAX_ITERS - 1:\n",
    "        losses = estimate_loss()\n",
    "        train_ppl = math.exp(losses['train'])\n",
    "        val_ppl = math.exp(losses['val'])\n",
    "\n",
    "        train_losses.append(losses['train'])\n",
    "        val_losses.append(losses['val'])\n",
    "        train_perplexities.append(train_ppl)\n",
    "        val_perplexities.append(val_ppl)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"{iter_num:6d} | {losses['train']:10.4f} | {losses['val']:10.4f} | \"\n",
    "              f\"{train_ppl:10.1f} | {val_ppl:10.1f}  [{elapsed:.0f}s]\")\n",
    "\n",
    "    # Training step\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    # Gradient clipping (standard practice for Transformers)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTraining complete in {total_time:.1f} seconds\")\n",
    "print(f\"Final train loss: {train_losses[-1]:.4f} (perplexity: {train_perplexities[-1]:.1f})\")\n",
    "print(f\"Final val loss:   {val_losses[-1]:.4f} (perplexity: {val_perplexities[-1]:.1f})\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_17_results_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Results Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_17_results_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Training and Results"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_18_loss_perplexity_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Loss Perplexity Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_18_loss_perplexity_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "iters = [i * EVAL_INTERVAL for i in range(len(train_losses))]\n",
    "axes[0].plot(iters, train_losses, 'b-o', linewidth=2, markersize=5, label='Train')\n",
    "axes[0].plot(iters, val_losses, 'r-o', linewidth=2, markersize=5, label='Validation')\n",
    "axes[0].set_xlabel('Iteration', fontsize=12)\n",
    "axes[0].set_ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=12)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Perplexity curves\n",
    "axes[1].plot(iters, train_perplexities, 'b-o', linewidth=2, markersize=5, label='Train')\n",
    "axes[1].plot(iters, val_perplexities, 'r-o', linewidth=2, markersize=5, label='Validation')\n",
    "axes[1].axhline(y=vocab_size, color='gray', linestyle='--', alpha=0.5, label=f'Random ({vocab_size})')\n",
    "axes[1].set_xlabel('Iteration', fontsize=12)\n",
    "axes[1].set_ylabel('Perplexity', fontsize=12)\n",
    "axes[1].set_title('Perplexity (lower = better)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=12)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Random baseline perplexity: {vocab_size} (choosing uniformly among {vocab_size} chars)\")\n",
    "print(f\"Our model perplexity: {val_perplexities[-1]:.1f} ‚Äî it is {vocab_size / val_perplexities[-1]:.1f}x better than random!\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_19_generation_temp",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Generation Temp\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_19_generation_temp.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_25_closing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Wrap-Up: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_25_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with different temperatures\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEXT GENERATION FROM TRAINED MINI-GPT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "temperatures = [0.5, 0.8, 1.0, 1.5]\n",
    "prompts = [\"\\nFirst Citizen:\\n\", \"\\nWhat is \", \"\\nTo be or \"]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt: '{prompt.strip()}'\")\n",
    "    print('='*60)\n",
    "\n",
    "    context = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    for temp in temperatures:\n",
    "        generated = model.generate(context, max_new_tokens=150, temperature=temp, top_k=20)\n",
    "        output_text = decode(generated[0].tolist())\n",
    "\n",
    "        print(f\"\\n--- Temperature = {temp} ---\")\n",
    "        print(output_text[:len(prompt) + 150])"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_20_temperature_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Temperature Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_20_temperature_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare generation quality as a function of training progress\n",
    "# (If we had saved checkpoints, we would compare here.\n",
    "#  Instead, let's show the effect of temperature visually.)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Generate multiple samples at different temperatures\n",
    "temps = np.linspace(0.3, 2.0, 8)\n",
    "diversities = []\n",
    "\n",
    "for temp in temps:\n",
    "    samples = []\n",
    "    for _ in range(10):\n",
    "        context = torch.tensor(encode(\"\\n\"), dtype=torch.long, device=device).unsqueeze(0)\n",
    "        gen = model.generate(context, max_new_tokens=50, temperature=temp)\n",
    "        samples.append(decode(gen[0].tolist()))\n",
    "\n",
    "    # Measure diversity: average pairwise character-level difference\n",
    "    diffs = []\n",
    "    for i in range(len(samples)):\n",
    "        for j in range(i+1, len(samples)):\n",
    "            s1, s2 = samples[i], samples[j]\n",
    "            min_len = min(len(s1), len(s2))\n",
    "            diff = sum(1 for a, b in zip(s1[:min_len], s2[:min_len]) if a != b) / min_len\n",
    "            diffs.append(diff)\n",
    "    diversities.append(np.mean(diffs))\n",
    "\n",
    "ax.plot(temps, diversities, 'o-', color='#1E88E5', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Temperature', fontsize=13)\n",
    "ax.set_ylabel('Output Diversity (pairwise difference)', fontsize=13)\n",
    "ax.set_title('Temperature Controls Randomness in Generation', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=1.0, color='gray', linestyle='--', alpha=0.5, label='Default (1.0)')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Annotate regions\n",
    "ax.annotate('More deterministic\\n(repetitive)', xy=(0.5, diversities[1]),\n",
    "            fontsize=10, color='#E53935', ha='center')\n",
    "ax.annotate('More random\\n(creative but noisy)', xy=(1.7, diversities[-2]),\n",
    "            fontsize=10, color='#E53935', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_21_final_output_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Final Output Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_21_final_output_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Final Output"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_22_journey_recap",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Journey Recap\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_22_journey_recap.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The complete journey: from N-grams to Mini-GPT\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"THE COMPLETE JOURNEY: FOUNDATIONS OF LANGUAGE MODELING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "Notebook 1: N-gram Language Models\n",
    "  ‚Üí Predicted the next word by COUNTING word pair frequencies.\n",
    "  ‚Üí Simple and fast, but zero probability for unseen pairs.\n",
    "  ‚Üí No notion of word similarity.\n",
    "\n",
    "Notebook 2: Neural Language Models\n",
    "  ‚Üí Replaced counting with LEARNED embeddings and neural networks.\n",
    "  ‚Üí Similar words get similar vectors (cat ‚âà dog in vector space).\n",
    "  ‚Üí RNNs extended context but hit the vanishing gradient wall.\n",
    "\n",
    "Notebook 3: Self-Attention & Transformers\n",
    "  ‚Üí Every word ATTENDS to every other word directly.\n",
    "  ‚Üí Multi-head attention captures multiple relationship types.\n",
    "  ‚Üí Full parallelism, no vanishing gradients.\n",
    "\n",
    "Notebook 4: Building a Tiny Language Model (this notebook)\n",
    "  ‚Üí Put it ALL together into a working Mini-GPT.\n",
    "  ‚Üí Trained on Shakespeare with next-token prediction.\n",
    "  ‚Üí Generated coherent text from a ~1M parameter model.\n",
    "\n",
    "The core insight: NEXT-TOKEN PREDICTION is all you need.\n",
    "  ‚Üí The training data IS the labels (self-supervised).\n",
    "  ‚Üí Scale up the model + data ‚Üí GPT-2 ‚Üí GPT-3 ‚Üí GPT-4.\n",
    "  ‚Üí The same architecture powers every modern LLM.\n",
    "\"\"\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_23_comparison_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Comparison Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_23_comparison_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison: N-gram vs Neural LM vs our Mini-GPT\n",
    "# generating from the same prompt\n",
    "\n",
    "prompt = \"the cat \"\n",
    "\n",
    "# N-gram style: just pick from bigram statistics\n",
    "ngram_output = prompt + \"sat on the mat\"  # hand-crafted from bigram table\n",
    "\n",
    "# Our Mini-GPT:\n",
    "context = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
    "gpt_generated = model.generate(context, max_new_tokens=60, temperature=0.8, top_k=20)\n",
    "gpt_output = decode(gpt_generated[0].tolist())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Architecture evolution\n",
    "architectures = ['N-gram\\n(1990s)', 'Neural LM\\n(2003)', 'RNN/LSTM\\n(2010s)', 'Transformer\\n(2017+)']\n",
    "capabilities = [2, 4, 6, 10]  # Relative capability score\n",
    "colors = ['#FFCDD2', '#FFE0B2', '#C8E6C9', '#BBDEFB']\n",
    "\n",
    "bars = axes[0].bar(architectures, capabilities, color=colors, edgecolor='white', linewidth=2)\n",
    "axes[0].set_ylabel('Relative Capability', fontsize=12)\n",
    "axes[0].set_title('Evolution of Language Modeling', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim(0, 12)\n",
    "\n",
    "labels = ['Count', 'Learn', 'Remember', 'Attend']\n",
    "for bar, label in zip(bars, labels):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "                 label, ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Right: Parameter scaling\n",
    "models_scale = ['Our Mini-GPT', 'GPT-2\\nSmall', 'GPT-2\\nLarge', 'GPT-3', 'GPT-4\\n(est.)']\n",
    "params = [total_params/1e6, 124, 774, 175000, 1800000]\n",
    "\n",
    "axes[1].bar(models_scale, params, color=['#4CAF50', '#2196F3', '#2196F3', '#FF9800', '#F44336'],\n",
    "            edgecolor='white', linewidth=2)\n",
    "axes[1].set_ylabel('Parameters (Millions)', fontsize=12)\n",
    "axes[1].set_title('Model Size Scaling', fontsize=14, fontweight='bold')\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "for i, (name, p) in enumerate(zip(models_scale, params)):\n",
    "    label = f'{p:.1f}M' if p < 1000 else f'{p/1000:.0f}B'\n",
    "    axes[1].text(i, p * 1.5, label, ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOur model: {total_params:,} parameters ({total_params/1e6:.1f}M)\")\n",
    "print(f\"GPT-3: 175,000,000,000 parameters (175B)\")\n",
    "print(f\"Scale difference: {175e9/total_params:.0f}x\")\n",
    "print(f\"\\nSame architecture. Same training objective. Just... bigger.\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_24_reflection_next_steps",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Reflection Next Steps\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_24_reflection_next_steps.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**What we built:**\n",
    "\n",
    "1. **A complete GPT-style language model** with token embeddings, positional embeddings, multi-head causal self-attention, feed-forward layers, residual connections, and layer normalization.\n",
    "\n",
    "2. **The training pipeline:** character-level tokenization, batch data loading, next-token prediction loss, AdamW optimizer with gradient clipping.\n",
    "\n",
    "3. **Text generation:** autoregressive sampling with temperature control and top-k filtering.\n",
    "\n",
    "**Key takeaways from the entire course:**\n",
    "\n",
    "- **N-grams** (Notebook 1): Language modeling as counting. Simple but brittle -- zero probability for unseen patterns, no word similarity.\n",
    "\n",
    "- **Neural LMs** (Notebook 2): Replace counting with learning. Embeddings capture similarity, but fixed context windows and sequential processing limit capability.\n",
    "\n",
    "- **Transformers** (Notebook 3): Self-attention lets every word attend to every other word in parallel. Multi-head attention captures multiple relationship types simultaneously.\n",
    "\n",
    "- **Mini-GPT** (Notebook 4): Put it all together. The same architecture, scaled up 100,000x and trained on internet-scale data, gives you GPT-3/4, Claude, and Gemini.\n",
    "\n",
    "**Where to go from here:**\n",
    "- **Self-Attention from First Principles** (next pod): Dive deeper into the mathematics of attention\n",
    "- **Building a Full GPT from Scratch** (later pod): Scale up with BPE tokenization, larger datasets\n",
    "- **Training Pipeline Engineering**: Mixed precision, distributed training, data loading"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"  COURSE COMPLETE: Foundations of Language Modeling\")\n",
    "print()\n",
    "print(\"  You traced the complete journey:\")\n",
    "print(\"    Counting ‚Üí Learning ‚Üí Attending ‚Üí Building\")\n",
    "print()\n",
    "print(\"  You now understand the foundations on which\")\n",
    "print(\"  every modern LLM is built. Well done!\")\n",
    "print(\"=\" * 70)"
   ],
   "id": "cell_25"
  }
 ]
}