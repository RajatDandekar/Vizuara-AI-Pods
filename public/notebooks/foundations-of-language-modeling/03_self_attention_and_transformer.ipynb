{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Self-Attention and the Transformer ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1_yOuaRupWcvvBB5tNnjVrtDllXqg6x4Q\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/03_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_01_setup_run_cell",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Setup Run Cell\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_01_setup_run_cell.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_04_visualization_it_example",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Visualization It Example\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_04_visualization_it_example.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_02_why_it_matters",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_02_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_03_building_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Building Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_03_building_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention and the Transformer -- Vizuara\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "In 2017, a team at Google published a paper with one of the most confident titles in AI history: \"Attention Is All You Need.\" They were right.\n",
    "\n",
    "The Transformer architecture they introduced solved every limitation of RNNs in a single stroke: it processes all words **simultaneously** (no sequential bottleneck), it can attend to **any position** directly (no vanishing gradients), and it scales beautifully with modern GPU hardware.\n",
    "\n",
    "Every modern LLM -- GPT, BERT, LLaMA, Claude, Gemini -- is built on this architecture. Understanding self-attention is not optional if you want to understand modern AI.\n",
    "\n",
    "In this notebook, you will:\n",
    "- Build the self-attention mechanism from scratch, step by step\n",
    "- Implement scaled dot-product attention with actual matrix operations\n",
    "- Build multi-head attention to capture multiple types of relationships\n",
    "- Implement positional encodings so the model knows word order\n",
    "- Assemble a complete Transformer block\n",
    "- Visualize attention patterns to see what the model is \"looking at\"\n",
    "\n",
    "Let us attend.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "Consider: \"The cat sat on the mat because **it** was comfortable.\"\n",
    "\n",
    "What does \"it\" refer to? The mat. How did you figure it out? You **looked back** at all the previous words and decided \"mat\" was most relevant.\n",
    "\n",
    "Now: \"The cat sat on the mat because **it** was hungry.\"\n",
    "\n",
    "Now \"it\" refers to the cat. Same structure, different meaning -- and you figured it out by attending to a **different** word.\n",
    "\n",
    "This selective, context-dependent backward glance is self-attention. Instead of processing words one at a time and hoping a hidden state remembers what matters, self-attention lets every word **directly look at every other word** and decide how much to pay attention.\n",
    "\n",
    "Let us see this in action."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The library analogy for Q, K, V:\n",
    "# You walk into a library with a QUERY (\"I want to learn about cats\").\n",
    "# Each book has a KEY on its spine (title, keywords).\n",
    "# You compare your query to every key, find matches, and read the VALUES (contents).\n",
    "\n",
    "# In self-attention, every word plays ALL THREE roles simultaneously.\n",
    "\n",
    "# Let's demonstrate with a simple example\n",
    "words = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "\n",
    "# Imagine the \"it\" is trying to figure out what it refers to\n",
    "query_word = \"it\"\n",
    "print(f\"Query word: '{query_word}'\")\n",
    "print(f\"The word '{query_word}' asks: 'Who should I pay attention to?'\\n\")\n",
    "\n",
    "# Simulated attention scores (what self-attention learns)\n",
    "attention_case1 = {\"The\": 0.05, \"cat\": 0.08, \"sat\": 0.05,\n",
    "                   \"on\": 0.02, \"the\": 0.05, \"mat\": 0.75}\n",
    "attention_case2 = {\"The\": 0.05, \"cat\": 0.72, \"sat\": 0.08,\n",
    "                   \"on\": 0.02, \"the\": 0.05, \"mat\": 0.08}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Case 1: \"it was comfortable\" ‚Üí mat\n",
    "bars1 = axes[0].bar(words, [attention_case1[w] for w in words],\n",
    "                     color=['#90CAF9' if v < 0.3 else '#E53935'\n",
    "                            for v in [attention_case1[w] for w in words]],\n",
    "                     edgecolor='white', linewidth=2)\n",
    "axes[0].set_ylabel('Attention Weight', fontsize=12)\n",
    "axes[0].set_title('\"...because it was comfortable\"\\n‚Üí \"it\" attends to \"mat\"',\n",
    "                  fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylim(0, 1.0)\n",
    "\n",
    "# Case 2: \"it was hungry\" ‚Üí cat\n",
    "bars2 = axes[1].bar(words, [attention_case2[w] for w in words],\n",
    "                     color=['#90CAF9' if v < 0.3 else '#E53935'\n",
    "                            for v in [attention_case2[w] for w in words]],\n",
    "                     edgecolor='white', linewidth=2)\n",
    "axes[1].set_ylabel('Attention Weight', fontsize=12)\n",
    "axes[1].set_title('\"...because it was hungry\"\\n‚Üí \"it\" attends to \"cat\"',\n",
    "                  fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylim(0, 1.0)\n",
    "\n",
    "plt.suptitle('Self-Attention: Same Structure, Different Attention Pattern',\n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: The attention pattern changes based on CONTEXT.\")\n",
    "print(\"This is impossible for N-grams or fixed-window models.\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_05_the_mathematics",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: The Mathematics\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_05_the_mathematics.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. The Mathematics\n",
    "\n",
    "The attention formula is:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "Where:\n",
    "- $Q = X W_Q$ -- Queries: \"what am I looking for?\"\n",
    "- $K = X W_K$ -- Keys: \"what do I have to offer?\"\n",
    "- $V = X W_V$ -- Values: \"what information do I carry?\"\n",
    "- $d_k$ -- dimension of keys (for scaling)\n",
    "\n",
    "The $\\sqrt{d_k}$ scaling prevents the dot products from growing too large, which would push softmax into regions with near-zero gradients.\n",
    "\n",
    "Let us compute this step by step with actual numbers."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_06_step_by_step_computation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Step By Step Computation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_06_step_by_step_computation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_30_closing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Wrap-Up: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_30_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-step attention computation with actual numbers\n",
    "\n",
    "# 3 words: \"the\", \"cat\", \"sat\"\n",
    "# Each has a d_k = 2 dimensional representation after projection\n",
    "\n",
    "# After multiplying by W_Q, W_K, W_V, suppose we get:\n",
    "Q = torch.tensor([\n",
    "    [1.0, 0.0],   # \"the\" query\n",
    "    [0.0, 1.0],   # \"cat\" query\n",
    "    [1.0, 1.0],   # \"sat\" query\n",
    "])\n",
    "\n",
    "K = torch.tensor([\n",
    "    [0.0, 1.0],   # \"the\" key\n",
    "    [1.0, 0.0],   # \"cat\" key\n",
    "    [1.0, 1.0],   # \"sat\" key\n",
    "])\n",
    "\n",
    "V = torch.tensor([\n",
    "    [1.0, 0.0],   # \"the\" value\n",
    "    [0.0, 1.0],   # \"cat\" value\n",
    "    [0.5, 0.5],   # \"sat\" value\n",
    "])\n",
    "\n",
    "d_k = Q.shape[-1]  # 2\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP-BY-STEP ATTENTION COMPUTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Q @ K^T\n",
    "print(\"\\nStep 1: Compute QK^T (dot product between queries and keys)\")\n",
    "scores = Q @ K.T\n",
    "print(f\"  Q @ K^T = \")\n",
    "print(f\"  {scores.numpy()}\")\n",
    "print(f\"\\n  Each entry (i,j) = how relevant word j's KEY is to word i's QUERY\")\n",
    "print(f\"  e.g., scores[2,2] = {scores[2,2].item():.1f} ‚Üí 'sat' is most relevant to itself\")\n",
    "\n",
    "# Step 2: Scale\n",
    "print(f\"\\nStep 2: Scale by sqrt(d_k) = sqrt({d_k}) = {d_k**0.5:.3f}\")\n",
    "scaled = scores / (d_k ** 0.5)\n",
    "print(f\"  Scaled scores = \")\n",
    "print(f\"  {scaled.numpy()}\")\n",
    "print(f\"\\n  Why scale? Without it, large d_k ‚Üí large scores ‚Üí near-one-hot softmax ‚Üí tiny gradients\")\n",
    "\n",
    "# Step 3: Softmax\n",
    "print(f\"\\nStep 3: Apply softmax row-wise (each row sums to 1)\")\n",
    "weights = F.softmax(scaled, dim=-1)\n",
    "print(f\"  Attention weights = \")\n",
    "print(f\"  {weights.numpy().round(3)}\")\n",
    "print(f\"\\n  Row sums: {weights.sum(dim=-1).numpy()}\")\n",
    "\n",
    "# Step 4: Multiply by V\n",
    "print(f\"\\nStep 4: Multiply weights by V to get output\")\n",
    "output = weights @ V\n",
    "print(f\"  Output = Weights @ V = \")\n",
    "print(f\"  {output.numpy().round(3)}\")\n",
    "print(f\"\\n  Each word's output is a WEIGHTED COMBINATION of all V vectors\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_07_visualization_flow_diagram",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Visualization Flow Diagram\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_07_visualization_flow_diagram.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the attention computation as a flow diagram\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 5))\n",
    "word_labels = [\"the\", \"cat\", \"sat\"]\n",
    "\n",
    "# Panel 1: Raw scores Q@K^T\n",
    "im1 = axes[0].imshow(scores.numpy(), cmap='Blues', vmin=-0.5, vmax=2.5)\n",
    "axes[0].set_title('Step 1: QK^T\\n(raw scores)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xticks(range(3)); axes[0].set_xticklabels(word_labels, fontsize=11)\n",
    "axes[0].set_yticks(range(3)); axes[0].set_yticklabels(word_labels, fontsize=11)\n",
    "axes[0].set_xlabel('Key (K)', fontsize=11); axes[0].set_ylabel('Query (Q)', fontsize=11)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[0].text(j, i, f'{scores[i,j]:.1f}', ha='center', va='center',\n",
    "                     fontsize=13, fontweight='bold')\n",
    "\n",
    "# Panel 2: Scaled scores\n",
    "im2 = axes[1].imshow(scaled.numpy(), cmap='Blues', vmin=-0.5, vmax=2.0)\n",
    "axes[1].set_title('Step 2: Scale\\n√∑ sqrt(d_k)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xticks(range(3)); axes[1].set_xticklabels(word_labels, fontsize=11)\n",
    "axes[1].set_yticks(range(3)); axes[1].set_yticklabels(word_labels, fontsize=11)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[1].text(j, i, f'{scaled[i,j]:.2f}', ha='center', va='center',\n",
    "                     fontsize=12, fontweight='bold')\n",
    "\n",
    "# Panel 3: Attention weights (after softmax)\n",
    "im3 = axes[2].imshow(weights.numpy(), cmap='Reds', vmin=0, vmax=0.6)\n",
    "axes[2].set_title('Step 3: Softmax\\n(attention weights)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xticks(range(3)); axes[2].set_xticklabels(word_labels, fontsize=11)\n",
    "axes[2].set_yticks(range(3)); axes[2].set_yticklabels(word_labels, fontsize=11)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[2].text(j, i, f'{weights[i,j]:.3f}', ha='center', va='center',\n",
    "                     fontsize=12, fontweight='bold',\n",
    "                     color='white' if weights[i,j] > 0.4 else 'black')\n",
    "\n",
    "# Panel 4: Output\n",
    "im4 = axes[3].imshow(output.numpy(), cmap='Greens', vmin=0, vmax=0.7)\n",
    "axes[3].set_title('Step 4: Output\\n= Weights √ó V', fontsize=12, fontweight='bold')\n",
    "axes[3].set_xticks(range(2)); axes[3].set_xticklabels(['d‚ÇÅ', 'd‚ÇÇ'], fontsize=11)\n",
    "axes[3].set_yticks(range(3)); axes[3].set_yticklabels(word_labels, fontsize=11)\n",
    "for i in range(3):\n",
    "    for j in range(2):\n",
    "        axes[3].text(j, i, f'{output[i,j]:.3f}', ha='center', va='center',\n",
    "                     fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Scaled Dot-Product Attention: Complete Computation',\n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_08_building_components_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Transition: Building Components Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_08_building_components_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "Now let us implement self-attention as a proper PyTorch module."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_09_self_attention_class",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Self Attention Class\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_09_self_attention_class.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Single-head self-attention.\n",
    "\n",
    "    Each word creates a Query, Key, and Value vector.\n",
    "    Attention weights are computed from Q and K, then used to\n",
    "    create a weighted combination of V vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_k):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Dimension of input embeddings\n",
    "            d_k: Dimension of Q, K, V projections\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "\n",
    "        # Learned projection matrices\n",
    "        self.W_q = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_k, bias=False)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model) input embeddings\n",
    "            mask: optional (seq_len, seq_len) boolean mask (True = block)\n",
    "        Returns:\n",
    "            output: (batch_size, seq_len, d_k) attention output\n",
    "            weights: (batch_size, seq_len, seq_len) attention weights\n",
    "        \"\"\"\n",
    "        Q = self.W_q(x)  # (batch, seq_len, d_k)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "\n",
    "        # Apply mask (for causal/decoder attention)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(weights, V)\n",
    "\n",
    "        return output, weights\n",
    "\n",
    "# Test it\n",
    "d_model = 8\n",
    "d_k = 4\n",
    "seq_len = 5\n",
    "\n",
    "attn = SelfAttention(d_model, d_k)\n",
    "\n",
    "# Random input (batch=1, seq_len=5, d_model=8)\n",
    "x = torch.randn(1, seq_len, d_model)\n",
    "output, weights = attn(x)\n",
    "\n",
    "print(f\"Input shape:    {x.shape}\")\n",
    "print(f\"Output shape:   {output.shape}\")\n",
    "print(f\"Weights shape:  {weights.shape}\")\n",
    "print(f\"Weight row sum: {weights[0].sum(dim=-1).detach().numpy()}\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_10_multi_head_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Multi Head Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_10_multi_head_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us build **multi-head attention** -- multiple attention heads running in parallel, each learning different types of relationships."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_11_multi_head_attention_class",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Multi Head Attention Class\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_11_multi_head_attention_class.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention.\n",
    "\n",
    "    Runs h parallel attention heads, each learning different\n",
    "    types of relationships (syntax, semantics, coreference, etc.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # Linear projections for all heads at once\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "        # Project Q, K, V\n",
    "        Q = self.W_q(x)  # (batch, seq_len, d_model)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        # Reshape for multi-head: split d_model into num_heads * d_k\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        # Now: (batch, num_heads, seq_len, d_k)\n",
    "\n",
    "        # Scaled dot-product attention for each head\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(weights, V)\n",
    "        # (batch, num_heads, seq_len, d_k)\n",
    "\n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(batch_size, seq_len, d_model)\n",
    "\n",
    "        # Final projection\n",
    "        output = self.W_o(attn_output)\n",
    "\n",
    "        return output, weights\n",
    "\n",
    "# Test multi-head attention\n",
    "d_model = 16\n",
    "num_heads = 4\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "x = torch.randn(1, 6, d_model)\n",
    "output, weights = mha(x)\n",
    "\n",
    "print(f\"Input shape:   {x.shape}\")\n",
    "print(f\"Output shape:  {output.shape}\")\n",
    "print(f\"Weights shape: {weights.shape}  (batch, heads, seq, seq)\")\n",
    "print(f\"\\nEach of {num_heads} heads learns a DIFFERENT attention pattern.\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_12_positional_encoding_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Positional Encoding Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_12_positional_encoding_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us implement **positional encoding** -- without it, the Transformer cannot distinguish word order."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_13_positional_encoding_class_vis",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Positional Encoding Class Vis\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_13_positional_encoding_class_vis.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encoding from \"Attention Is All You Need.\"\n",
    "\n",
    "    Adds position information to word embeddings so the model\n",
    "    can distinguish \"cat sat\" from \"sat cat.\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even dimensions\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd dimensions\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "\n",
    "# Visualize positional encodings\n",
    "pe = PositionalEncoding(d_model=64, max_len=100)\n",
    "pe_matrix = pe.pe[0].numpy()  # (100, 64)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Heatmap\n",
    "im = axes[0].imshow(pe_matrix[:50, :].T, aspect='auto', cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "axes[0].set_xlabel('Position in Sequence', fontsize=12)\n",
    "axes[0].set_ylabel('Encoding Dimension', fontsize=12)\n",
    "axes[0].set_title('Positional Encoding Heatmap\\nEach position has a unique \"fingerprint\"',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Individual dimensions\n",
    "for dim in [0, 1, 4, 5, 20, 21]:\n",
    "    style = '-' if dim % 2 == 0 else '--'\n",
    "    label = f'dim {dim} ({\"sin\" if dim % 2 == 0 else \"cos\"})'\n",
    "    axes[1].plot(range(50), pe_matrix[:50, dim], style, linewidth=2, label=label, alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel('Position', fontsize=12)\n",
    "axes[1].set_ylabel('Encoding Value', fontsize=12)\n",
    "axes[1].set_title('Individual Encoding Dimensions\\nLow dims = high frequency, High dims = low frequency',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].legend(ncol=3, fontsize=9, loc='upper right')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each position gets a unique combination of sin/cos values.\")\n",
    "print(\"The key property: PE[pos+k] - PE[pos] is the same for any pos,\")\n",
    "print(\"so the model can learn relative position patterns.\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_14_todo_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_14_todo_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Your Turn\n",
    "\n",
    "**TODO 1: Implement Causal (Masked) Attention**\n",
    "\n",
    "In a decoder language model (like GPT), each word can only attend to previous words -- it cannot peek at the future. Implement the causal mask."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_15_todo_1_causal_mask",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo Causal Mask\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_15_todo_1_causal_mask.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and apply a causal attention mask\n",
    "#\n",
    "# Instructions:\n",
    "# 1. Create a mask where mask[i, j] = True means position i CANNOT attend to position j\n",
    "# 2. For causal attention: i cannot attend to j if j > i (no peeking at future)\n",
    "# 3. Apply the mask in SelfAttention and verify the attention weights\n",
    "# 4. Check: attention_weights[i, j] should be 0 for all j > i\n",
    "\n",
    "# YOUR CODE HERE\n",
    "seq_len = 5\n",
    "\n",
    "# Create causal mask: upper triangle = True (blocked)\n",
    "# causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "# print(\"Causal mask (True = blocked):\")\n",
    "# print(causal_mask.int())\n",
    "\n",
    "# Test with SelfAttention\n",
    "# attn_causal = SelfAttention(d_model=8, d_k=4)\n",
    "# x_test = torch.randn(1, seq_len, 8)\n",
    "# output_causal, weights_causal = attn_causal(x_test, mask=causal_mask)\n",
    "#\n",
    "# print(\"\\nAttention weights with causal mask:\")\n",
    "# print(weights_causal[0].detach().numpy().round(3))\n",
    "# print(\"\\nVerify: all upper-triangle weights should be 0 ‚úì\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_16_todo_2_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_16_todo_2_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2: Build and Visualize 4-Head Attention Patterns**\n",
    "\n",
    "Each attention head specializes in different relationships. Visualize what 4 different heads learn."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_17_todo_2_multi_head_vis",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo Multi Head Vis\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_17_todo_2_multi_head_vis.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize attention patterns from all 4 heads\n",
    "#\n",
    "# Instructions:\n",
    "# 1. Create a MultiHeadAttention with d_model=16, num_heads=4\n",
    "# 2. Feed a sentence through it (use random embeddings for simplicity)\n",
    "# 3. Extract the attention weights for each head\n",
    "# 4. Create a 2x2 grid of heatmaps, one per head\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# sentence = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "# mha_vis = MultiHeadAttention(d_model=16, num_heads=4)\n",
    "# x_vis = torch.randn(1, len(sentence), 16)\n",
    "# _, head_weights = mha_vis(x_vis)  # (1, 4, 6, 6)\n",
    "#\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "# for h in range(4):\n",
    "#     ax = axes[h // 2][h % 2]\n",
    "#     w = head_weights[0, h].detach().numpy()\n",
    "#     ax.imshow(w, cmap='Blues')\n",
    "#     ax.set_xticks(range(len(sentence)))\n",
    "#     ax.set_xticklabels(sentence, rotation=45)\n",
    "#     ax.set_yticks(range(len(sentence)))\n",
    "#     ax.set_yticklabels(sentence)\n",
    "#     ax.set_title(f'Head {h+1}')\n",
    "# plt.suptitle('Multi-Head Attention Patterns')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_18_todo_3_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_18_todo_3_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 3: Implement the Feed-Forward Network**\n",
    "\n",
    "Each Transformer block has a position-wise feed-forward network after attention. It applies the same two-layer MLP to each position independently.\n",
    "\n",
    "$$\\text{FFN}(x) = \\max(0, x W_1 + b_1) W_2 + b_2$$"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_19_todo_3_feed_forward",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo Feed Forward\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_19_todo_3_feed_forward.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_21_transformer_block_class",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Transformer Block Class\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_21_transformer_block_class.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the feed-forward network\n",
    "#\n",
    "# Instructions:\n",
    "# 1. Two linear layers with a GELU activation between them\n",
    "# 2. Typical expansion: inner dimension = 4 * d_model\n",
    "# 3. This processes each position independently\n",
    "\n",
    "# class FeedForward(nn.Module):\n",
    "#     def __init__(self, d_model, d_ff=None):\n",
    "#         super().__init__()\n",
    "#         if d_ff is None:\n",
    "#             d_ff = 4 * d_model\n",
    "#         # TODO: Define two linear layers and activation\n",
    "#         pass\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # TODO: x ‚Üí Linear ‚Üí GELU ‚Üí Linear\n",
    "#         pass"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_20_putting_it_together_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Transition: Putting It Together Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_20_putting_it_together_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Putting It All Together\n",
    "\n",
    "Now let us assemble a complete Transformer block from our components."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff=None):\n",
    "        super().__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.activation(self.linear1(x)))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer decoder block.\n",
    "\n",
    "    Components:\n",
    "    1. Multi-Head Self-Attention (with causal mask)\n",
    "    2. Add & Layer Normalize\n",
    "    3. Feed-Forward Network\n",
    "    4. Add & Layer Normalize\n",
    "\n",
    "    Residual connections around both sub-layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Sub-layer 1: Multi-Head Attention + Residual + Norm\n",
    "        attn_output, attn_weights = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "\n",
    "        # Sub-layer 2: Feed-Forward + Residual + Norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_output))\n",
    "\n",
    "        return x, attn_weights\n",
    "\n",
    "# Test the complete Transformer block\n",
    "d_model = 32\n",
    "num_heads = 4\n",
    "seq_len = 6\n",
    "\n",
    "block = TransformerBlock(d_model, num_heads)\n",
    "x = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "# Create causal mask\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "\n",
    "output, weights = block(x, mask=causal_mask)\n",
    "\n",
    "print(f\"Transformer Block:\")\n",
    "print(f\"  Input shape:  {x.shape}\")\n",
    "print(f\"  Output shape: {output.shape}\")\n",
    "print(f\"  Attention:    {weights.shape}\")\n",
    "print(f\"\\n  Components:\")\n",
    "print(f\"    Multi-Head Attention: {num_heads} heads √ó {d_model//num_heads} dim each\")\n",
    "print(f\"    Feed-Forward: {d_model} ‚Üí {4*d_model} ‚Üí {d_model}\")\n",
    "print(f\"    Layer Norm: 2 √ó\")\n",
    "print(f\"    Residual connections: 2 √ó\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in block.parameters())\n",
    "print(f\"\\n  Total parameters: {total_params:,}\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_22_stacking_blocks_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Stacking Blocks Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_22_stacking_blocks_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us stack multiple blocks and see how representations evolve through layers."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_23_transformer_decoder_class",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Transformer Decoder Class\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_23_transformer_decoder_class.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stack of Transformer blocks for language modeling.\n",
    "\n",
    "    Includes token embedding, positional encoding, and output projection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, dropout=0.1)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        # Causal mask\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(x.device)\n",
    "\n",
    "        # Embed + positional encoding\n",
    "        h = self.token_embedding(x) * (self.d_model ** 0.5)\n",
    "        h = self.pos_encoding(h)\n",
    "\n",
    "        # Pass through transformer blocks\n",
    "        all_weights = []\n",
    "        for block in self.blocks:\n",
    "            h, weights = block(h, mask)\n",
    "            all_weights.append(weights)\n",
    "\n",
    "        # Final layer norm + projection\n",
    "        h = self.norm(h)\n",
    "        logits = self.output_proj(h)\n",
    "\n",
    "        return logits, all_weights\n",
    "\n",
    "\n",
    "# Create a small Transformer\n",
    "vocab_size = 20\n",
    "d_model = 32\n",
    "num_heads = 4\n",
    "num_layers = 3\n",
    "\n",
    "transformer = TransformerDecoder(vocab_size, d_model, num_heads, num_layers)\n",
    "\n",
    "# Test\n",
    "test_input = torch.randint(0, vocab_size, (1, 8))  # 8 tokens\n",
    "logits, all_weights = transformer(test_input)\n",
    "\n",
    "print(f\"Transformer Decoder (GPT-style):\")\n",
    "print(f\"  Vocab size:  {vocab_size}\")\n",
    "print(f\"  d_model:     {d_model}\")\n",
    "print(f\"  Heads:       {num_heads}\")\n",
    "print(f\"  Layers:      {num_layers}\")\n",
    "print(f\"\\n  Input:  {test_input.shape}\")\n",
    "print(f\"  Output: {logits.shape}\")\n",
    "print(f\"  ‚Üí At each position, outputs logits over {vocab_size} vocabulary words\")\n",
    "\n",
    "total_params = sum(p.numel() for p in transformer.parameters())\n",
    "print(f\"\\n  Total parameters: {total_params:,}\")\n",
    "print(f\"\\n  For reference:\")\n",
    "print(f\"    GPT-2 Small:  124M parameters\")\n",
    "print(f\"    GPT-3:        175B parameters\")\n",
    "print(f\"    Our mini-GPT: {total_params:,} parameters\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_24_training_results_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Transition: Training Results Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_24_training_results_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Training and Results\n",
    "\n",
    "Let us visualize the attention patterns across all layers and heads to see what different parts of the model focus on."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_25_visualization_across_layers_heads",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Visualization Across Layers Heads\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_25_visualization_across_layers_heads.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention across all layers\n",
    "\n",
    "fig, axes = plt.subplots(num_layers, num_heads, figsize=(16, 10))\n",
    "\n",
    "words_vis = [f\"w{i}\" for i in range(8)]  # placeholder labels\n",
    "\n",
    "for layer_idx in range(num_layers):\n",
    "    for head_idx in range(num_heads):\n",
    "        ax = axes[layer_idx][head_idx]\n",
    "        w = all_weights[layer_idx][0, head_idx].detach().numpy()\n",
    "\n",
    "        im = ax.imshow(w, cmap='Blues', vmin=0, vmax=w.max())\n",
    "        ax.set_xticks(range(8))\n",
    "        ax.set_xticklabels(words_vis, fontsize=8, rotation=45)\n",
    "        ax.set_yticks(range(8))\n",
    "        ax.set_yticklabels(words_vis, fontsize=8)\n",
    "\n",
    "        if layer_idx == 0:\n",
    "            ax.set_title(f'Head {head_idx+1}', fontsize=11, fontweight='bold')\n",
    "        if head_idx == 0:\n",
    "            ax.set_ylabel(f'Layer {layer_idx+1}', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Attention Patterns Across Layers and Heads\\n'\n",
    "             '(each head in each layer learns a different pattern)',\n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: the causal mask ensures each position only attends to earlier positions.\")\n",
    "print(\"Different heads specialize ‚Äî some focus on nearby words, others on distant ones.\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_26_visualization_architecture_comparison",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Visualization Architecture Comparison\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_26_visualization_architecture_comparison.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare architectures: N-gram vs Neural LM vs RNN vs Transformer\n",
    "\n",
    "comparison = {\n",
    "    'Feature': ['Context Window', 'Word Similarity', 'Long-Range Dependencies',\n",
    "                'Parallel Training', 'Typical Parameters'],\n",
    "    'N-gram': ['n-1 words (fixed)', 'None', 'None (n < 5)',\n",
    "               'N/A (counting)', '~10M entries'],\n",
    "    'Neural LM': ['n-1 words (fixed)', 'Learned embeddings', 'Limited by window',\n",
    "                  'Yes (batch)', '~1-10M'],\n",
    "    'RNN/LSTM': ['Unlimited (theory)', 'Learned embeddings', '~10-200 tokens',\n",
    "                 'No (sequential)', '~10-100M'],\n",
    "    'Transformer': ['Full sequence', 'Learned embeddings', 'Full sequence',\n",
    "                    'Yes (fully parallel)', '100M - 1T'],\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(\n",
    "    cellText=[comparison[k] for k in comparison],\n",
    "    rowLabels=list(comparison.keys()),\n",
    "    loc='center',\n",
    "    cellLoc='center'\n",
    ")\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 1.8)\n",
    "\n",
    "# Color the header row\n",
    "for j in range(5):\n",
    "    table[0, j].set_facecolor('#E3F2FD')\n",
    "    table[0, j].set_text_props(fontweight='bold')\n",
    "\n",
    "# Color the Transformer column\n",
    "for i in range(len(comparison)):\n",
    "    table[i, 4].set_facecolor('#E8F5E9')\n",
    "\n",
    "ax.set_title('Architecture Comparison: The Journey from N-grams to Transformers',\n",
    "             fontsize=15, fontweight='bold', y=0.95)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_27_final_output_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Transition: Final Output Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_27_final_output_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Final Output"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_28_final_visualization",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Final Visualization\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_28_final_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration: attention weight visualization on a real-looking sentence\n",
    "\n",
    "# Let's create a more meaningful visualization\n",
    "sentence = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \"because\", \"it\"]\n",
    "n = len(sentence)\n",
    "\n",
    "# Simulate realistic attention patterns for visualization\n",
    "# Head 1: local attention (nearby words)\n",
    "local_attn = np.zeros((n, n))\n",
    "for i in range(n):\n",
    "    for j in range(i+1):\n",
    "        local_attn[i, j] = np.exp(-abs(i - j) * 0.5)\n",
    "    local_attn[i, :i+1] /= local_attn[i, :i+1].sum()\n",
    "\n",
    "# Head 2: syntactic attention (subject-verb, article-noun)\n",
    "syntactic_attn = np.zeros((n, n))\n",
    "syntactic_pairs = {1: 0, 2: 1, 5: 4, 7: 5}  # each word attends to its syntactic partner\n",
    "for i in range(n):\n",
    "    for j in range(i+1):\n",
    "        if i in syntactic_pairs and syntactic_pairs[i] == j:\n",
    "            syntactic_attn[i, j] = 0.7\n",
    "        else:\n",
    "            syntactic_attn[i, j] = 0.3 / max(i, 1)\n",
    "    syntactic_attn[i, :i+1] /= syntactic_attn[i, :i+1].sum()\n",
    "\n",
    "# Head 3: coreference attention (\"it\" ‚Üí \"mat\")\n",
    "coref_attn = np.zeros((n, n))\n",
    "for i in range(n):\n",
    "    for j in range(i+1):\n",
    "        coref_attn[i, j] = 0.1\n",
    "    if i == 7:  # \"it\"\n",
    "        coref_attn[7, 5] = 0.6  # attends to \"mat\"\n",
    "    coref_attn[i, :i+1] /= coref_attn[i, :i+1].sum()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "titles = ['Head 1: Local Proximity', 'Head 2: Syntactic Structure', 'Head 3: Coreference']\n",
    "matrices = [local_attn, syntactic_attn, coref_attn]\n",
    "\n",
    "for ax, title, matrix in zip(axes, titles, matrices):\n",
    "    im = ax.imshow(matrix, cmap='Blues', vmin=0, vmax=0.8)\n",
    "    ax.set_xticks(range(n))\n",
    "    ax.set_xticklabels(sentence, rotation=45, ha='right', fontsize=11)\n",
    "    ax.set_yticks(range(n))\n",
    "    ax.set_yticklabels(sentence, fontsize=11)\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if matrix[i, j] > 0.01:\n",
    "                color = 'white' if matrix[i, j] > 0.4 else 'black'\n",
    "                ax.text(j, i, f'{matrix[i,j]:.2f}', ha='center', va='center',\n",
    "                       fontsize=8, color=color)\n",
    "\n",
    "plt.suptitle('Multi-Head Attention: Each Head Learns Different Relationships',\n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each attention head specializes:\")\n",
    "print(\"  Head 1 ‚Äî focuses on nearby words (local context)\")\n",
    "print(\"  Head 2 ‚Äî connects syntactically related words (subject‚Üîverb, article‚Üînoun)\")\n",
    "print(\"  Head 3 ‚Äî resolves coreferences ('it' ‚Üí 'mat')\")\n",
    "print(\"\\nThis is why multi-head attention is so powerful:\")\n",
    "print(\"it captures MULTIPLE types of relationships simultaneously.\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_29_reflection",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Reflection\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_29_reflection.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "1. **Self-attention lets every word look at every other word directly.** No more vanishing gradients, no sequential bottleneck.\n",
    "\n",
    "2. **Q, K, V are learned projections** that determine what each word is looking for (Q), what it advertises (K), and what information it carries (V).\n",
    "\n",
    "3. **Scaling by sqrt(d_k) is essential** to keep softmax in a useful gradient range.\n",
    "\n",
    "4. **Multi-head attention captures multiple relationship types simultaneously** -- syntax, semantics, coreference, and more.\n",
    "\n",
    "5. **Positional encoding gives the model word-order information** that is otherwise lost in the permutation-invariant attention operation.\n",
    "\n",
    "6. **A Transformer block = Attention + Feed-Forward + Residuals + LayerNorm** -- and you stack N of these.\n",
    "\n",
    "**What comes next:**\n",
    "\n",
    "In the next notebook, we will put everything together: build a complete mini-GPT model, train it on real text, and generate coherent language. We will implement the training loop with next-token prediction, measure perplexity, and see the model learn to write."
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"  NOTEBOOK COMPLETE: Self-Attention & the Transformer\")\n",
    "print(\"  You built attention from scratch, implemented\")\n",
    "print(\"  multi-head attention, positional encoding, and\")\n",
    "print(\"  assembled a complete Transformer block.\")\n",
    "print()\n",
    "print(\"  Next: Building a Tiny Language Model (Mini-GPT)\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "cell_28"
  }
 ]
}