{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Neural Language Models and Word Embeddings ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1RJjttCvltRK-j5XaI_Tp752cibGKRYMf\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/02_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_01_setup_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Setup Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_01_setup_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_02_why_it_matters",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_02_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_03_building_intuition_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Building Intuition Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_03_building_intuition_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Language Models and Word Embeddings -- Vizuara\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebook, we built an N-gram language model that predicts the next word by counting. It works, but it has two fatal flaws: it assigns zero probability to unseen word combinations, and it has absolutely no notion that similar words should behave similarly.\n",
    "\n",
    "In 2003, Yoshua Bengio asked a simple question: **what if, instead of counting words, we could learn to represent them?** His answer ‚Äî the **Neural Probabilistic Language Model** ‚Äî is one of the most important papers in the history of NLP.\n",
    "\n",
    "The core idea: represent each word as a **dense vector of real numbers** (an embedding), then use a neural network to predict the next word from these embeddings. Words that appear in similar contexts end up with similar vectors, so knowledge transfers automatically from one word to another.\n",
    "\n",
    "In this notebook, you will:\n",
    "- Build Bengio's neural language model from scratch in PyTorch\n",
    "- Train word embeddings and visualize semantic relationships\n",
    "- Implement Word2Vec (Skip-gram) and see the famous King - Man + Woman = Queen\n",
    "- Build a simple RNN language model and see its limitations\n",
    "- Understand why fixed-context and sequential processing are fundamental bottlenecks\n",
    "\n",
    "Let us move from counting to learning.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "Imagine you are in a foreign city looking for a good restaurant. The N-gram approach is like having a massive phone book: you look up the exact address. If the restaurant is not in the book, you are stuck.\n",
    "\n",
    "The neural approach is like having a **map.** Even if the specific restaurant is not marked, you can see that there is a cluster of restaurants in a particular neighborhood and walk there. The \"map\" is the **embedding space**, and similar words live in the same neighborhood.\n",
    "\n",
    "Let us see what \"word as a number\" means in practice."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_04_embedding_visualization",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Embedding Visualization\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_04_embedding_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_12_train_bengiolm_results",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Train Bengiolm Results\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_12_train_bengiolm_results.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_15_visualize_embeddings_results",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Visualize Embeddings Results\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_15_visualize_embeddings_results.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_26_train_rnn_results",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Train Rnn Results\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_26_train_rnn_results.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_29_rnn_hidden_state_results",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Rnn Hidden State Results\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_29_rnn_hidden_state_results.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_35_summary_visualization_results",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Summary Visualization Results\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_35_summary_visualization_results.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# In an N-gram model, each word is just an INDEX ‚Äî a meaningless integer.\n",
    "# \"cat\" = 0, \"dog\" = 1, \"fish\" = 2\n",
    "# There is NO relationship between these numbers.\n",
    "\n",
    "# In a neural model, each word is a VECTOR ‚Äî a point in continuous space.\n",
    "# \"cat\" = [0.2, 0.8, -0.1, 0.5, ...]\n",
    "# \"dog\" = [0.3, 0.7, -0.2, 0.4, ...]  <-- close to cat!\n",
    "# \"fish\" = [-0.5, 0.1, 0.9, -0.3, ...]  <-- farther away\n",
    "\n",
    "# Let's visualize this difference\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: N-gram view (discrete indices)\n",
    "words = [\"cat\", \"dog\", \"fish\", \"bird\", \"mat\", \"rug\"]\n",
    "indices = list(range(len(words)))\n",
    "\n",
    "axes[0].scatter(indices, [0]*len(indices), s=200, c='#E53935', zorder=5)\n",
    "for i, w in enumerate(words):\n",
    "    axes[0].annotate(w, (i, 0), textcoords=\"offset points\",\n",
    "                     xytext=(0, 15), ha='center', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlim(-0.5, len(words)-0.5)\n",
    "axes[0].set_ylim(-0.5, 0.5)\n",
    "axes[0].set_title('N-gram View: Words as Discrete Indices\\n(no relationships)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Word Index', fontsize=11)\n",
    "axes[0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].set_yticks([])\n",
    "\n",
    "# Right: Embedding view (continuous vectors)\n",
    "np.random.seed(42)\n",
    "embeddings_2d = {\n",
    "    \"cat\": [1.2, 2.1], \"dog\": [1.5, 1.8], \"bird\": [1.8, 2.5], \"fish\": [2.2, 2.0],\n",
    "    \"mat\": [-1.0, -0.5], \"rug\": [-0.7, -0.3],\n",
    "}\n",
    "colors = ['#E53935', '#E53935', '#E53935', '#E53935', '#1E88E5', '#1E88E5']\n",
    "\n",
    "for (word, (x, y)), c in zip(embeddings_2d.items(), colors):\n",
    "    axes[1].scatter(x, y, s=200, c=c, zorder=5)\n",
    "    axes[1].annotate(word, (x, y), textcoords=\"offset points\",\n",
    "                     xytext=(8, 8), fontsize=13, fontweight='bold')\n",
    "\n",
    "# Draw similarity circles\n",
    "from matplotlib.patches import Ellipse\n",
    "axes[1].add_patch(Ellipse((1.6, 2.1), 1.8, 1.2, fill=False,\n",
    "                           linestyle='--', color='#E53935', alpha=0.5, linewidth=2))\n",
    "axes[1].add_patch(Ellipse((-0.85, -0.4), 1.0, 0.6, fill=False,\n",
    "                           linestyle='--', color='#1E88E5', alpha=0.5, linewidth=2))\n",
    "axes[1].text(1.6, 2.9, 'animals', ha='center', fontsize=11, color='#E53935', fontstyle='italic')\n",
    "axes[1].text(-0.85, -0.9, 'surfaces', ha='center', fontsize=11, color='#1E88E5', fontstyle='italic')\n",
    "\n",
    "axes[1].set_title('Embedding View: Words as Vectors\\n(similar words are nearby)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Dimension 1', fontsize=11)\n",
    "axes[1].set_ylabel('Dimension 2', fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: In embedding space, 'cat' and 'dog' are CLOSE together.\")\n",
    "print(\"Anything the model learns about 'cat' automatically helps it predict 'dog'.\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_05_math_explanation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Math Explanation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_05_math_explanation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. The Mathematics\n",
    "\n",
    "Bengio's model computes:\n",
    "\n",
    "$$P(w_t \\mid w_{t-n+1}, \\ldots, w_{t-1}) = \\text{softmax}(W \\cdot h + b)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$h = \\tanh(H \\cdot x + d)$$\n",
    "\n",
    "$$x = [C(w_{t-n+1}); \\ldots; C(w_{t-1})]$$\n",
    "\n",
    "Here, $C$ is the **embedding matrix** ‚Äî each row is a word's vector representation. The key parameters:\n",
    "\n",
    "- $C \\in \\mathbb{R}^{|V| \\times d}$ : embedding matrix (|V| words, d dimensions each)\n",
    "- $H \\in \\mathbb{R}^{h \\times (n-1)d}$ : hidden layer weights\n",
    "- $W \\in \\mathbb{R}^{|V| \\times h}$ : output layer weights\n",
    "\n",
    "The loss function is **cross-entropy** (negative log-likelihood):\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{N}\\sum_{i=1}^{N} \\log P(w_i \\mid \\text{context}_i)$$\n",
    "\n",
    "Let us trace through the forward pass with actual numbers."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_06_manual_forward_pass_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Manual Forward Pass Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_06_manual_forward_pass_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_07_manual_forward_pass_results",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Manual Forward Pass Results\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_07_manual_forward_pass_results.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual forward pass through Bengio's model\n",
    "# Vocabulary: cat=0, sat=1, mat=2 (3 words)\n",
    "# Context window: n=2 (bigram ‚Äî predict from 1 previous word)\n",
    "# Embedding dimension: d=3\n",
    "# Hidden dimension: h=4\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "vocab = {\"cat\": 0, \"sat\": 1, \"mat\": 2}\n",
    "V = len(vocab)\n",
    "d = 3   # embedding dimension\n",
    "h = 4   # hidden dimension\n",
    "\n",
    "# Embedding matrix C: each row is a word's vector\n",
    "C = torch.tensor([\n",
    "    [0.2, 0.8, -0.1],   # cat\n",
    "    [0.5, 0.1, 0.7],    # sat\n",
    "    [0.9, 0.3, -0.5],   # mat\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(\"Embedding matrix C:\")\n",
    "for word, idx in vocab.items():\n",
    "    print(f\"  '{word}' (idx={idx}) ‚Üí {C[idx].tolist()}\")\n",
    "\n",
    "# Step 1: Look up embedding for context word \"cat\"\n",
    "context_word = \"cat\"\n",
    "x = C[vocab[context_word]]  # shape: (d,) = (3,)\n",
    "print(f\"\\nStep 1: Embedding lookup for '{context_word}'\")\n",
    "print(f\"  x = C[{vocab[context_word]}] = {x.tolist()}\")\n",
    "\n",
    "# Step 2: Hidden layer\n",
    "H = torch.tensor([\n",
    "    [0.3, -0.1, 0.2],\n",
    "    [0.4, 0.2, -0.3],\n",
    "    [-0.1, 0.5, 0.1],\n",
    "    [0.2, -0.2, 0.4],\n",
    "], dtype=torch.float32)  # shape: (h, d) = (4, 3)\n",
    "d_bias = torch.zeros(h)\n",
    "\n",
    "hidden = torch.tanh(H @ x + d_bias)\n",
    "print(f\"\\nStep 2: Hidden layer h = tanh(H @ x)\")\n",
    "print(f\"  H @ x = {(H @ x).tolist()}\")\n",
    "print(f\"  h = tanh(H @ x) = {hidden.tolist()}\")\n",
    "\n",
    "# Step 3: Output layer + softmax\n",
    "W = torch.tensor([\n",
    "    [0.5, -0.3, 0.2, 0.1],\n",
    "    [-0.1, 0.4, 0.3, -0.2],\n",
    "    [0.2, 0.1, -0.4, 0.5],\n",
    "], dtype=torch.float32)  # shape: (V, h) = (3, 4)\n",
    "b = torch.zeros(V)\n",
    "\n",
    "logits = W @ hidden + b\n",
    "probs = F.softmax(logits, dim=0)\n",
    "\n",
    "print(f\"\\nStep 3: Output logits = W @ h = {logits.tolist()}\")\n",
    "print(f\"  Softmax probabilities:\")\n",
    "for word, idx in vocab.items():\n",
    "    print(f\"    P('{word}' | '{context_word}') = {probs[idx].item():.4f}\")\n",
    "\n",
    "print(f\"\\n  Most likely next word: '{list(vocab.keys())[probs.argmax().item()]}'\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_08_transition_to_pytorch",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Transition: Transition To Pytorch\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_08_transition_to_pytorch.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "Now let us build Bengio's neural language model as a proper PyTorch module and train it."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_09_bengiolm_class",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Bengiolm Class\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_09_bengiolm_class.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengioLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Bengio's Neural Probabilistic Language Model (2003).\n",
    "\n",
    "    Given (n-1) context words, predicts the probability of the next word.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, context_size, hidden_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Number of words in vocabulary\n",
    "            embed_dim: Dimension of word embeddings\n",
    "            context_size: Number of context words (n-1 for n-gram)\n",
    "            hidden_dim: Hidden layer dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # The embedding matrix C\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # Hidden layer: takes concatenated context embeddings\n",
    "        self.hidden = nn.Linear(context_size * embed_dim, hidden_dim)\n",
    "\n",
    "        # Output layer: produces logits over vocabulary\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context_indices):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            context_indices: (batch_size, context_size) tensor of word indices\n",
    "        Returns:\n",
    "            logits: (batch_size, vocab_size) unnormalized log-probabilities\n",
    "        \"\"\"\n",
    "        # Step 1: Look up embeddings for each context word\n",
    "        embeds = self.embeddings(context_indices)  # (batch, context_size, embed_dim)\n",
    "\n",
    "        # Step 2: Concatenate embeddings\n",
    "        x = embeds.view(embeds.size(0), -1)  # (batch, context_size * embed_dim)\n",
    "\n",
    "        # Step 3: Hidden layer with tanh\n",
    "        h = torch.tanh(self.hidden(x))  # (batch, hidden_dim)\n",
    "\n",
    "        # Step 4: Output logits\n",
    "        logits = self.output(h)  # (batch, vocab_size)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Prepare a corpus and vocabulary\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the rug\",\n",
    "    \"the cat ate the fish\",\n",
    "    \"the dog ate the bone\",\n",
    "    \"the bird flew over the house\",\n",
    "    \"the bird sat on the tree\",\n",
    "    \"the cat ran after the dog\",\n",
    "    \"the dog ran after the cat\",\n",
    "    \"the fish swam in the pond\",\n",
    "    \"the bird flew over the pond\",\n",
    "    \"a cat sat on a mat\",\n",
    "    \"a dog sat on a rug\",\n",
    "    \"the cat is a nice pet\",\n",
    "    \"the dog is a good pet\",\n",
    "    \"a bird sang in the tree\",\n",
    "]\n",
    "\n",
    "# Build vocabulary\n",
    "all_words = []\n",
    "for sentence in corpus:\n",
    "    all_words.extend(sentence.lower().split())\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "vocab_list = [\"<unk>\"] + [w for w, c in word_counts.most_common()]\n",
    "word2idx = {w: i for i, w in enumerate(vocab_list)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "print(f\"Vocabulary: {len(word2idx)} words\")\n",
    "print(f\"Words: {vocab_list}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_10_data_prep",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Data Prep\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_10_data_prep.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data: (context, target) pairs for bigram model\n",
    "CONTEXT_SIZE = 2  # Use 2 previous words (trigram-style)\n",
    "\n",
    "def make_training_data(corpus, context_size, word2idx):\n",
    "    \"\"\"Create (context, target) pairs from corpus.\"\"\"\n",
    "    data = []\n",
    "    for sentence in corpus:\n",
    "        tokens = sentence.lower().split()\n",
    "        for i in range(context_size, len(tokens)):\n",
    "            context = [word2idx.get(tokens[j], 0) for j in range(i - context_size, i)]\n",
    "            target = word2idx.get(tokens[i], 0)\n",
    "            data.append((context, target))\n",
    "    return data\n",
    "\n",
    "training_data = make_training_data(corpus, CONTEXT_SIZE, word2idx)\n",
    "\n",
    "# Convert to tensors\n",
    "contexts = torch.tensor([d[0] for d in training_data])\n",
    "targets = torch.tensor([d[1] for d in training_data])\n",
    "\n",
    "print(f\"Training examples: {len(training_data)}\")\n",
    "print(f\"\\nFirst 5 examples:\")\n",
    "for i in range(5):\n",
    "    ctx_words = [idx2word[c.item()] for c in contexts[i]]\n",
    "    tgt_word = idx2word[targets[i].item()]\n",
    "    print(f\"  Context: {ctx_words} ‚Üí Target: '{tgt_word}'\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_11_train_bengiolm_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Train Bengiolm Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_11_train_bengiolm_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "EMBED_DIM = 16\n",
    "HIDDEN_DIM = 64\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 200\n",
    "\n",
    "model = BengioLM(\n",
    "    vocab_size=len(word2idx),\n",
    "    embed_dim=EMBED_DIM,\n",
    "    context_size=CONTEXT_SIZE,\n",
    "    hidden_dim=HIDDEN_DIM\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Forward pass\n",
    "    logits = model(contexts)\n",
    "    loss = criterion(logits, targets)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, color='#1E88E5', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "plt.title('Training Loss: Bengio Neural Language Model', fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_13_visualize_embeddings_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Visualize Embeddings Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_13_visualize_embeddings_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualize the learned embeddings -- the model should have placed similar words near each other."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_14_visualize_embeddings_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Visualize Embeddings Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_14_visualize_embeddings_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract learned embeddings and visualize with PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Get embedding vectors\n",
    "with torch.no_grad():\n",
    "    embedding_matrix = model.embeddings.weight.numpy()\n",
    "\n",
    "# Reduce to 2D with PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embedding_matrix)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Color code by category\n",
    "animals = {\"cat\", \"dog\", \"bird\", \"fish\", \"pet\"}\n",
    "surfaces = {\"mat\", \"rug\"}\n",
    "actions = {\"sat\", \"ate\", \"ran\", \"flew\", \"swam\", \"sang\"}\n",
    "locations = {\"tree\", \"house\", \"pond\"}\n",
    "\n",
    "for i, word in enumerate(vocab_list):\n",
    "    x, y = embeddings_2d[i]\n",
    "\n",
    "    if word in animals:\n",
    "        color = '#E53935'\n",
    "        category = 'animals'\n",
    "    elif word in surfaces:\n",
    "        color = '#1E88E5'\n",
    "        category = 'surfaces'\n",
    "    elif word in actions:\n",
    "        color = '#43A047'\n",
    "        category = 'actions'\n",
    "    elif word in locations:\n",
    "        color = '#FF9800'\n",
    "        category = 'locations'\n",
    "    else:\n",
    "        color = '#9E9E9E'\n",
    "        category = 'other'\n",
    "\n",
    "    ax.scatter(x, y, c=color, s=100, zorder=5)\n",
    "    ax.annotate(word, (x, y), textcoords=\"offset points\",\n",
    "                xytext=(5, 5), fontsize=10)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='#E53935', markersize=10, label='Animals'),\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='#1E88E5', markersize=10, label='Surfaces'),\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='#43A047', markersize=10, label='Actions'),\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='#FF9800', markersize=10, label='Locations'),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='best', fontsize=11)\n",
    "ax.set_title('Learned Word Embeddings (PCA Projection)\\nSimilar words cluster together!',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_16_todo1_word2vec_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo1 Word2vec Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_16_todo1_word2vec_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Your Turn\n",
    "\n",
    "**TODO 1: Implement Word2Vec (Skip-gram)**\n",
    "\n",
    "Word2Vec flips the prediction: instead of predicting the next word from context, it predicts **context words from the center word.** This is more efficient for learning embeddings.\n",
    "\n",
    "Given center word $w_c$, predict each context word $w_o$ within a window:\n",
    "\n",
    "$$P(w_o \\mid w_c) = \\frac{\\exp(v'_{w_o} \\cdot v_{w_c})}{\\sum_{w=1}^{V} \\exp(v'_w \\cdot v_{w_c})}$$"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_17_todo1_word2vec_task",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo1 Word2vec Task\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_17_todo1_word2vec_task.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the Word2Vec Skip-gram model\n",
    "#\n",
    "# Instructions:\n",
    "# 1. Fill in the forward() method\n",
    "# 2. The model should have TWO embedding layers:\n",
    "#    - center_embeddings: for center words\n",
    "#    - context_embeddings: for context words\n",
    "# 3. Forward pass: dot product between center and context embeddings\n",
    "\n",
    "class Word2VecSkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        # TODO: Define two embedding layers\n",
    "        self.center_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, center_word, context_word):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            center_word: (batch_size,) indices of center words\n",
    "            context_word: (batch_size,) indices of context words\n",
    "        Returns:\n",
    "            scores: (batch_size,) dot product scores\n",
    "        \"\"\"\n",
    "        # TODO: Compute dot product between center and context embeddings\n",
    "        # center_embed = self.center_embeddings(center_word)  # (batch, embed_dim)\n",
    "        # context_embed = ???\n",
    "        # score = ???  # dot product\n",
    "        # return score\n",
    "        pass\n",
    "\n",
    "# TODO: Create training pairs (center_word, context_word) with window_size=2\n",
    "# TODO: Train the model for 100 epochs\n",
    "# TODO: Visualize the learned embeddings"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_19_todo2_cosine_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo2 Cosine Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_19_todo2_cosine_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2: Measure Cosine Similarity Between Word Pairs**\n",
    "\n",
    "After training, compute the cosine similarity between word pairs to verify that similar words have similar embeddings.\n",
    "\n",
    "$$\\text{cosine}(u, v) = \\frac{u \\cdot v}{\\|u\\| \\|v\\|}$$"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_20_todo2_cosine_task",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo2 Cosine Task\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_20_todo2_cosine_task.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute cosine similarities between word pairs\n",
    "#\n",
    "# Instructions:\n",
    "# 1. Extract the embedding vectors from the trained Bengio model\n",
    "# 2. Compute cosine similarity for these pairs:\n",
    "#    - (\"cat\", \"dog\") ‚Äî should be high\n",
    "#    - (\"cat\", \"fish\") ‚Äî should be moderate\n",
    "#    - (\"cat\", \"mat\") ‚Äî should be low\n",
    "#    - (\"mat\", \"rug\") ‚Äî should be high\n",
    "# 3. Create a bar chart comparing the similarities\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# def cosine_similarity(word1, word2, model, word2idx):\n",
    "#     with torch.no_grad():\n",
    "#         v1 = model.embeddings.weight[word2idx[word1]]\n",
    "#         v2 = model.embeddings.weight[word2idx[word2]]\n",
    "#         return F.cosine_similarity(v1.unsqueeze(0), v2.unsqueeze(0)).item()\n",
    "#\n",
    "# pairs = [(\"cat\", \"dog\"), (\"cat\", \"fish\"), (\"cat\", \"mat\"), (\"mat\", \"rug\")]\n",
    "# for w1, w2 in pairs:\n",
    "#     sim = cosine_similarity(w1, w2, model, word2idx)\n",
    "#     print(f\"  cosine('{w1}', '{w2}') = {sim:.4f}\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_22_transition_to_rnns",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Transition: Transition To Rnns\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_22_transition_to_rnns.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Putting It All Together\n",
    "\n",
    "Now let us build a simple **RNN language model** to see how recurrent processing extends the context window beyond a fixed number of words."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_23_rnn_model_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Rnn Model Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_23_rnn_model_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_24_rnn_data_prep",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Rnn Data Prep\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_24_rnn_data_prep.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple RNN language model.\n",
    "\n",
    "    Unlike Bengio's model which uses a fixed context window,\n",
    "    the RNN carries a hidden state that (in theory) summarizes\n",
    "    the entire history of the sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # RNN cell: h_t = tanh(W_hh * h_{t-1} + W_xh * x_t + b)\n",
    "        self.W_xh = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.W_hh = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "        # Output: project hidden state to vocabulary\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_seq, hidden=None):\n",
    "        \"\"\"\n",
    "        Process a sequence one token at a time.\n",
    "\n",
    "        Args:\n",
    "            input_seq: (batch_size, seq_len) token indices\n",
    "            hidden: initial hidden state, or None for zeros\n",
    "        Returns:\n",
    "            logits: (batch_size, seq_len, vocab_size)\n",
    "            hidden: final hidden state\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_seq.shape\n",
    "\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(batch_size, self.hidden_dim)\n",
    "\n",
    "        embeds = self.embeddings(input_seq)  # (batch, seq_len, embed_dim)\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = embeds[:, t, :]  # (batch, embed_dim)\n",
    "\n",
    "            # RNN step: h_t = tanh(W_xh * x_t + W_hh * h_{t-1})\n",
    "            hidden = torch.tanh(self.W_xh(x_t) + self.W_hh(hidden))\n",
    "\n",
    "            # Predict next word from hidden state\n",
    "            logit = self.output(hidden)  # (batch, vocab_size)\n",
    "            outputs.append(logit.unsqueeze(1))\n",
    "\n",
    "        logits = torch.cat(outputs, dim=1)  # (batch, seq_len, vocab_size)\n",
    "        return logits, hidden\n",
    "\n",
    "\n",
    "# Prepare sequence data for RNN\n",
    "def prepare_rnn_data(corpus, word2idx, max_len=None):\n",
    "    \"\"\"Convert sentences to padded index tensors.\"\"\"\n",
    "    sequences = []\n",
    "    for sentence in corpus:\n",
    "        tokens = sentence.lower().split()\n",
    "        indices = [word2idx.get(t, 0) for t in tokens]\n",
    "        sequences.append(indices)\n",
    "\n",
    "    if max_len is None:\n",
    "        max_len = max(len(s) for s in sequences)\n",
    "\n",
    "    # Pad sequences\n",
    "    padded = torch.zeros(len(sequences), max_len, dtype=torch.long)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = min(len(seq), max_len)\n",
    "        padded[i, :length] = torch.tensor(seq[:length])\n",
    "\n",
    "    return padded\n",
    "\n",
    "# Prepare data: input is all but last token, target is all but first\n",
    "sequences = prepare_rnn_data(corpus, word2idx)\n",
    "inputs = sequences[:, :-1]    # everything except last word\n",
    "targets_rnn = sequences[:, 1:]  # everything except first word\n",
    "\n",
    "print(f\"Input shape: {inputs.shape}\")\n",
    "print(f\"Target shape: {targets_rnn.shape}\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Input:  {[idx2word[i.item()] for i in inputs[0] if i.item() in idx2word]}\")\n",
    "print(f\"  Target: {[idx2word[i.item()] for i in targets_rnn[0] if i.item() in idx2word]}\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_25_train_rnn_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Train Rnn Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_25_train_rnn_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the RNN\n",
    "rnn_model = RNNLanguageModel(\n",
    "    vocab_size=len(word2idx),\n",
    "    embed_dim=16,\n",
    "    hidden_dim=32\n",
    ")\n",
    "\n",
    "optimizer_rnn = torch.optim.Adam(rnn_model.parameters(), lr=0.01)\n",
    "criterion_rnn = nn.CrossEntropyLoss(ignore_index=0)  # ignore padding\n",
    "\n",
    "rnn_losses = []\n",
    "\n",
    "for epoch in range(300):\n",
    "    logits_rnn, _ = rnn_model(inputs)\n",
    "\n",
    "    # Reshape for cross-entropy: (batch*seq_len, vocab_size)\n",
    "    loss_rnn = criterion_rnn(\n",
    "        logits_rnn.reshape(-1, len(word2idx)),\n",
    "        targets_rnn.reshape(-1)\n",
    "    )\n",
    "\n",
    "    optimizer_rnn.zero_grad()\n",
    "    loss_rnn.backward()\n",
    "    optimizer_rnn.step()\n",
    "\n",
    "    rnn_losses.append(loss_rnn.item())\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/300, Loss: {loss_rnn.item():.4f}\")\n",
    "\n",
    "# Plot both training curves\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(losses, label='Bengio Feed-Forward LM', color='#1E88E5', linewidth=2, alpha=0.8)\n",
    "ax.plot(rnn_losses, label='RNN Language Model', color='#E53935', linewidth=2, alpha=0.8)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "ax.set_title('Training Comparison: Feed-Forward vs RNN', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_27_rnn_advantage_limitation_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Rnn Advantage Limitation Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_27_rnn_advantage_limitation_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Training and Results\n",
    "\n",
    "Let us demonstrate the RNN's key advantage ‚Äî and its key limitation."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_28_rnn_hidden_state_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Rnn Hidden State Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_28_rnn_hidden_state_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN advantage: it uses the ENTIRE sequence history, not just a fixed window\n",
    "\n",
    "# Let's trace the hidden state through a sentence to see how\n",
    "# information accumulates\n",
    "rnn_model.eval()\n",
    "test_sentence = \"the cat sat on the mat\"\n",
    "test_tokens = test_sentence.split()\n",
    "test_indices = torch.tensor([[word2idx.get(w, 0) for w in test_tokens]])\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeds = rnn_model.embeddings(test_indices)\n",
    "    hidden = torch.zeros(1, rnn_model.hidden_dim)\n",
    "\n",
    "    hidden_states = [hidden.squeeze().numpy().copy()]\n",
    "    predictions = []\n",
    "\n",
    "    for t in range(len(test_tokens)):\n",
    "        x_t = embeds[:, t, :]\n",
    "        hidden = torch.tanh(rnn_model.W_xh(x_t) + rnn_model.W_hh(hidden))\n",
    "        hidden_states.append(hidden.squeeze().numpy().copy())\n",
    "\n",
    "        logit = rnn_model.output(hidden)\n",
    "        pred_idx = logit.argmax(dim=-1).item()\n",
    "        predictions.append(idx2word.get(pred_idx, \"<unk>\"))\n",
    "\n",
    "# Visualize hidden states\n",
    "hidden_matrix = np.array(hidden_states)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Hidden state heatmap\n",
    "im = axes[0].imshow(hidden_matrix.T, aspect='auto', cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "axes[0].set_yticks(range(0, rnn_model.hidden_dim, 4))\n",
    "step_labels = [\"h‚ÇÄ\"] + [f\"'{w}'\" for w in test_tokens]\n",
    "axes[0].set_xticks(range(len(step_labels)))\n",
    "axes[0].set_xticklabels(step_labels, fontsize=11)\n",
    "axes[0].set_ylabel('Hidden Dimension', fontsize=12)\n",
    "axes[0].set_title('RNN Hidden State Evolution\\nEach column accumulates more information',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im, ax=axes[0], label='Activation')\n",
    "\n",
    "# Predictions at each step\n",
    "axes[1].axis('off')\n",
    "header = \"Step  |  Input Word  |  Hidden State Info  |  Predicted Next Word\"\n",
    "axes[1].text(0.05, 0.92, header, fontsize=11, fontfamily='monospace',\n",
    "             fontweight='bold', transform=axes[1].transAxes)\n",
    "axes[1].axhline(y=0.88, xmin=0.03, xmax=0.97, color='gray', linewidth=1,\n",
    "                transform=axes[1].transAxes)\n",
    "\n",
    "for t, (word, pred) in enumerate(zip(test_tokens, predictions)):\n",
    "    y_pos = 0.82 - t * 0.12\n",
    "    info = f\"Encodes words 1..{t+1}\"\n",
    "    line = f\"  {t+1}   |  {word:10s}  |  {info:20s}  |  '{pred}'\"\n",
    "    axes[1].text(0.05, y_pos, line, fontsize=10, fontfamily='monospace',\n",
    "                 transform=axes[1].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_30_vanishing_gradient_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Vanishing Gradient Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_30_vanishing_gradient_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see the vanishing gradient problem in action."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_31_vanishing_gradient_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Vanishing Gradient Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_31_vanishing_gradient_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_32_vanishing_gradient_results",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Vanishing Gradient Results\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_32_vanishing_gradient_results.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vanishing gradient problem: gradients shrink exponentially\n",
    "\n",
    "# Simulate gradient flow through an RNN\n",
    "seq_lengths = [5, 10, 20, 50, 100]\n",
    "weight_scale = 0.8  # Typical weight magnitude\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: gradient magnitude over time steps\n",
    "for length in seq_lengths:\n",
    "    gradients = [1.0]  # Start with gradient = 1 at the last step\n",
    "    for t in range(1, length):\n",
    "        # Gradient shrinks by factor of ~weight_scale at each step\n",
    "        gradients.append(gradients[-1] * weight_scale)\n",
    "    gradients.reverse()\n",
    "\n",
    "    steps = list(range(length))\n",
    "    axes[0].plot(steps, gradients, linewidth=2, label=f'T={length}')\n",
    "\n",
    "axes[0].set_xlabel('Time Step (from start)', fontsize=12)\n",
    "axes[0].set_ylabel('Relative Gradient Magnitude', fontsize=12)\n",
    "axes[0].set_title('Vanishing Gradients in RNNs\\nGradient signal fades for early words',\n",
    "                  fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].axhline(y=0.01, color='red', linestyle='--', alpha=0.5, label='Negligible')\n",
    "\n",
    "# Right: effective memory window\n",
    "effective_memory = []\n",
    "thresholds = [0.1, 0.01, 0.001]\n",
    "for thresh in thresholds:\n",
    "    # How many steps back can gradient reach with magnitude > threshold?\n",
    "    steps_back = int(np.log(thresh) / np.log(weight_scale))\n",
    "    effective_memory.append(steps_back)\n",
    "\n",
    "bars = axes[1].bar(\n",
    "    [f'>{t}' for t in thresholds],\n",
    "    effective_memory,\n",
    "    color=['#4CAF50', '#FF9800', '#F44336'],\n",
    "    edgecolor='white', linewidth=2\n",
    ")\n",
    "for bar, mem in zip(bars, effective_memory):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                 f'{mem} steps', ha='center', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Gradient Threshold', fontsize=12)\n",
    "axes[1].set_ylabel('Effective Memory (steps)', fontsize=12)\n",
    "axes[1].set_title('How Far Back Can an RNN \"Remember\"?\\n(with weight scale = 0.8)',\n",
    "                  fontsize=13, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key takeaway: With typical weight scales, RNN gradients fade within 10-20 steps.\")\n",
    "print(\"This means the model CANNOT effectively use long-range context.\")\n",
    "print(\"LSTMs/GRUs extend this to ~100-200 tokens, but the fundamental\")\n",
    "print(\"problem remains: sequential processing prevents parallelism.\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_33_summary_visualization_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Transition: Summary Visualization Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_33_summary_visualization_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Final Output"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_34_summary_visualization_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Summary Visualization Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_34_summary_visualization_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary visualization: the complete picture so far\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Panel 1: N-grams (count table)\n",
    "axes[0].set_title('Era 1: N-grams\\n\"Count\"', fontsize=14, fontweight='bold')\n",
    "table_data = [\n",
    "    ['the ‚Üí cat', '3/10', '‚úì'],\n",
    "    ['the ‚Üí dog', '2/10', '‚úì'],\n",
    "    ['cat ‚Üí sat', '2/3', '‚úì'],\n",
    "    ['cat ‚Üí ran', '0/3', '‚úó'],\n",
    "    ['dog ‚Üí flew', '0/2', '‚úó'],\n",
    "]\n",
    "table = axes[0].table(cellText=table_data,\n",
    "                       colLabels=['Bigram', 'P', 'Seen?'],\n",
    "                       loc='center', cellLoc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 1.5)\n",
    "axes[0].axis('off')\n",
    "axes[0].text(0.5, -0.05, 'Limitation: Zero probability\\nfor unseen combinations',\n",
    "             ha='center', fontsize=10, color='#E53935', transform=axes[0].transAxes)\n",
    "\n",
    "# Panel 2: Neural LM (embeddings)\n",
    "axes[1].set_title('Era 2: Neural LMs\\n\"Learn\"', fontsize=14, fontweight='bold')\n",
    "words_demo = {\"cat\": [0.8, 1.5], \"dog\": [1.0, 1.3], \"bird\": [0.6, 1.8],\n",
    "              \"mat\": [-0.5, -0.8], \"rug\": [-0.3, -0.6]}\n",
    "for w, (x, y) in words_demo.items():\n",
    "    color = '#E53935' if w in ['cat', 'dog', 'bird'] else '#1E88E5'\n",
    "    axes[1].scatter(x, y, s=120, c=color, zorder=5)\n",
    "    axes[1].annotate(w, (x, y), textcoords=\"offset points\", xytext=(5, 5), fontsize=11)\n",
    "axes[1].set_xlabel('Dim 1', fontsize=10)\n",
    "axes[1].set_ylabel('Dim 2', fontsize=10)\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].text(0.5, -0.05, 'Strength: Similarity transfers\\nLimit: Fixed context window',\n",
    "             ha='center', fontsize=10, color='#FF9800', transform=axes[1].transAxes)\n",
    "\n",
    "# Panel 3: RNN (sequential, vanishing gradients)\n",
    "axes[2].set_title('Era 2.5: RNNs\\n\"Remember\"', fontsize=14, fontweight='bold')\n",
    "words_seq = ['the', 'cat', 'sat', 'on', 'the', 'mat']\n",
    "for i, w in enumerate(words_seq):\n",
    "    axes[2].add_patch(plt.Rectangle((i*1.3, 0.8), 1.0, 0.6, fill=True,\n",
    "                                      facecolor='#E3F2FD', edgecolor='#1E88E5', linewidth=2))\n",
    "    axes[2].text(i*1.3 + 0.5, 1.1, w, ha='center', fontsize=10)\n",
    "    if i < len(words_seq) - 1:\n",
    "        axes[2].annotate('', xy=((i+1)*1.3, 1.1), xytext=(i*1.3 + 1.0, 1.1),\n",
    "                         arrowprops=dict(arrowstyle='->', color='#1E88E5',\n",
    "                                         alpha=max(0.2, 1.0 - i*0.15), linewidth=2))\n",
    "axes[2].set_xlim(-0.3, 8.5)\n",
    "axes[2].set_ylim(0.3, 1.8)\n",
    "axes[2].axis('off')\n",
    "axes[2].text(0.5, -0.05, 'Strength: Unlimited context (theory)\\nLimit: Vanishing gradients + sequential',\n",
    "             ha='center', fontsize=10, color='#FF9800', transform=axes[2].transAxes)\n",
    "\n",
    "plt.suptitle('The Journey So Far: From Counting to Learning',\n",
    "             fontsize=16, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_36_reflection_next_steps",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Reflection Next Steps\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_36_reflection_next_steps.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "1. **Neural language models replace count tables with learned parameters.** Every weight -- embeddings, hidden layers, output projections -- is trained end-to-end.\n",
    "\n",
    "2. **Word embeddings solve the sparsity problem.** Similar words get similar vectors, so knowledge transfers automatically.\n",
    "\n",
    "3. **Bengio's 2003 model was the breakthrough.** Embedding lookup, concatenation, hidden layer, softmax -- a simple recipe with profound consequences.\n",
    "\n",
    "4. **RNNs extend context but hit walls.** The vanishing gradient problem limits effective memory to ~10-20 steps (LSTMs extend to ~100-200).\n",
    "\n",
    "5. **Sequential processing is a bottleneck.** RNNs process one word at a time, making them slow on modern parallel hardware.\n",
    "\n",
    "**What comes next:**\n",
    "\n",
    "Both Bengio's model and RNNs share a fundamental limitation: they process context **sequentially** or through a **fixed window.** What if every word could look at every other word simultaneously and decide what is important?\n",
    "\n",
    "That is exactly what the **Transformer** does with **self-attention.** In the next notebook, we will build the attention mechanism from scratch."
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_37_closing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Wrap-Up: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_37_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"  NOTEBOOK COMPLETE: Neural Language Models\")\n",
    "print(\"  You built Bengio's model, trained embeddings,\")\n",
    "print(\"  and saw why RNNs hit the vanishing gradient wall.\")\n",
    "print()\n",
    "print(\"  Next: Self-Attention and the Transformer\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "cell_25"
  }
 ]
}