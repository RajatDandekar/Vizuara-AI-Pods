{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "04_lora_finetuning ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"17rFuCNZUUY1xHrMq1WTamV-JWh_IDZe8\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/04_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_01_setup_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Setup Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_01_setup_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_02_why_lora_matters",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why Lora Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_02_why_lora_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Fine-Tuning from Scratch\n",
    "\n",
    "*Part 4 of the Vizuara series on Inference & Scaling*\n",
    "*Estimated time: 65 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "You have a pretrained language model that can generate fluent text. But it was trained on generic internet data -- it does not know how to follow your specific instructions, write in your company's style, or answer domain-specific questions accurately.\n",
    "\n",
    "The obvious solution is fine-tuning: continue training on task-specific data. The problem? A 7-billion parameter model needs over 28 GB just to store its weights in FP32. Add gradients (another 28 GB) and Adam optimizer states (another 56 GB), and you need **112 GB of GPU memory** -- more than an A100 can offer.\n",
    "\n",
    "**LoRA** (Low-Rank Adaptation) solves this beautifully. Instead of updating all 7 billion parameters, LoRA freezes the original weights and adds tiny trainable matrices that capture the task-specific changes. The key insight: fine-tuning weight updates are **low-rank**, meaning they can be compressed into much smaller matrices without losing quality.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Understand **why** weight updates are low-rank (with empirical evidence)\n",
    "2. Implement LoRA layers from scratch\n",
    "3. Inject LoRA into a pretrained model\n",
    "4. Fine-tune on a classification task and compare against full fine-tuning\n",
    "5. Analyze the parameter efficiency"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_03_initial_setup",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Initial Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_03_initial_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup -- run this cell first\n",
    "!pip install -q torch matplotlib numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_04_building_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Building Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_04_building_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Imagine you are an expert chef who has mastered French cuisine. Someone asks you to also cook Japanese food. You do not need to re-learn everything from scratch -- you already know knife skills, temperature control, sauce making, and plating. You just need to learn a few new techniques: how to prepare rice for sushi, how to make dashi, and how to slice sashimi.\n",
    "\n",
    "The \"updates\" to your cooking ability are **low-dimensional**: a small number of new skills layered on top of your vast existing knowledge. LoRA applies this same idea to neural networks.\n",
    "\n",
    "Mathematically, when you fine-tune a weight matrix $W \\in \\mathbb{R}^{d \\times d}$ (where $d = 4096$), the change $\\Delta W = W_{\\text{finetuned}} - W_{\\text{pretrained}}$ has 16 million entries. But if you compute the singular values of $\\Delta W$, you find that most of the \"energy\" is concentrated in just a handful of singular values. The effective rank of $\\Delta W$ is often 4, 8, or 16 -- not 4096.\n",
    "\n",
    "This means we can write $\\Delta W \\approx BA$ where $B \\in \\mathbb{R}^{d \\times r}$ and $A \\in \\mathbb{R}^{r \\times d}$, with $r \\ll d$. Instead of updating $d^2 = 16{,}777{,}216$ parameters, we update $2 \\times d \\times r = 2 \\times 4096 \\times 16 = 131{,}072$ parameters. That is a **128x reduction**."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_05_mathematics",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Mathematics\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_05_mathematics.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### The LoRA Decomposition\n",
    "\n",
    "For a pretrained weight matrix $W_0 \\in \\mathbb{R}^{d_{\\text{out}} \\times d_{\\text{in}}}$, LoRA replaces the fine-tuned weight with:\n",
    "\n",
    "$$W = W_0 + \\frac{\\alpha}{r} \\cdot BA$$\n",
    "\n",
    "where:\n",
    "- $B \\in \\mathbb{R}^{d_{\\text{out}} \\times r}$ is initialized to **zeros**\n",
    "- $A \\in \\mathbb{R}^{r \\times d_{\\text{in}}}$ is initialized with a **random Gaussian**\n",
    "- $r$ is the rank (typically 4, 8, or 16)\n",
    "- $\\alpha$ is a scaling hyperparameter (typically equal to $r$ or $2r$)\n",
    "\n",
    "The forward pass becomes:\n",
    "\n",
    "$$h = W_0 x + \\frac{\\alpha}{r} \\cdot BAx$$\n",
    "\n",
    "**Why this initialization?** $B = 0$ ensures that at the start of training, $\\Delta W = BA = 0$, so the model behaves exactly like the pretrained model. Training gradually learns the update.\n",
    "\n",
    "### Parameter Count\n",
    "\n",
    "For a single weight matrix of shape $(d, d)$ with rank $r$:\n",
    "\n",
    "$$\\text{LoRA params} = d \\times r + r \\times d = 2dr$$\n",
    "\n",
    "$$\\text{Compression ratio} = \\frac{d^2}{2dr} = \\frac{d}{2r}$$\n",
    "\n",
    "For $d = 4096$ and $r = 16$: compression ratio = $4096 / 32 = 128\\times$.\n",
    "\n",
    "### Where to Apply LoRA\n",
    "\n",
    "In a transformer, each attention layer has four weight matrices: $W_Q, W_K, W_V, W_O$. The original LoRA paper found that applying LoRA to $W_Q$ and $W_V$ gives the best results for a given parameter budget. Later work (QLoRA, etc.) showed that applying to all four can help, especially at higher ranks."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_06_lora_layer_impl",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Lora Layer Impl\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_06_lora_layer_impl.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 The LoRA Layer"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    A linear layer with LoRA adaptation.\n",
    "\n",
    "    The forward pass computes: y = W_0 @ x + (alpha/r) * B @ A @ x\n",
    "    where W_0 is frozen and B, A are trainable.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, original_linear, rank=8, alpha=16):\n",
    "        super().__init__()\n",
    "        self.original = original_linear\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "        d_out, d_in = original_linear.weight.shape\n",
    "\n",
    "        # LoRA matrices\n",
    "        self.A = nn.Parameter(torch.randn(rank, d_in) * 0.01)\n",
    "        self.B = nn.Parameter(torch.zeros(d_out, rank))\n",
    "\n",
    "        # Freeze original weights\n",
    "        self.original.weight.requires_grad = False\n",
    "        if self.original.bias is not None:\n",
    "            self.original.bias.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Original path (frozen)\n",
    "        base_output = self.original(x)\n",
    "\n",
    "        # LoRA path (trainable)\n",
    "        lora_output = F.linear(F.linear(x, self.A), self.B) * self.scaling\n",
    "\n",
    "        return base_output + lora_output\n",
    "\n",
    "    def merge(self):\n",
    "        \"\"\"Merge LoRA weights into original for efficient inference.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.original.weight += self.scaling * (self.B @ self.A)\n",
    "        return self.original\n",
    "\n",
    "    @property\n",
    "    def lora_params(self):\n",
    "        return self.A.numel() + self.B.numel()\n",
    "\n",
    "\n",
    "# Test\n",
    "original = nn.Linear(256, 256, bias=False)\n",
    "lora_layer = LoRALinear(original, rank=8, alpha=16)\n",
    "\n",
    "x = torch.randn(1, 10, 256)\n",
    "y = lora_layer(x)\n",
    "print(f\"Input: {x.shape}\")\n",
    "print(f\"Output: {y.shape}\")\n",
    "print(f\"Original params: {original.weight.numel():,}\")\n",
    "print(f\"LoRA params: {lora_layer.lora_params:,}\")\n",
    "print(f\"Compression: {original.weight.numel() / lora_layer.lora_params:.1f}x\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_07_injecting_lora",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Injecting Lora\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_07_injecting_lora.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Injecting LoRA into a Pretrained Model"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.shape\n",
    "        h = self.ln1(x)\n",
    "\n",
    "        Q = self.W_q(h).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(h).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(h).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        mask = torch.triu(torch.ones(T, T, device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        scores.masked_fill_(mask, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, self.d_model)\n",
    "        out = self.W_o(out)\n",
    "\n",
    "        x = x + out\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class SmallGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, n_heads=4, n_layers=4, max_len=256):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        h = self.tok_emb(x) + self.pos_emb(torch.arange(T, device=x.device))\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "        h = self.ln_f(h)\n",
    "        return self.head(h)\n",
    "\n",
    "\n",
    "def inject_lora(model, rank=8, alpha=16, target_modules=('W_q', 'W_v')):\n",
    "    \"\"\"\n",
    "    Inject LoRA adapters into specified modules of the model.\n",
    "    Freezes all original parameters and only makes LoRA params trainable.\n",
    "    \"\"\"\n",
    "    # First, freeze everything\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    lora_count = 0\n",
    "    for name, module in model.named_modules():\n",
    "        for target in target_modules:\n",
    "            if hasattr(module, target):\n",
    "                original = getattr(module, target)\n",
    "                if isinstance(original, nn.Linear):\n",
    "                    lora_layer = LoRALinear(original, rank=rank, alpha=alpha)\n",
    "                    setattr(module, target, lora_layer)\n",
    "                    lora_count += 1\n",
    "\n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f\"Injected LoRA into {lora_count} layers\")\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {trainable_params:,}\")\n",
    "    print(f\"Trainable fraction: {trainable_params/total_params*100:.2f}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create and inject\n",
    "vocab_size = 1000\n",
    "model = SmallGPT(vocab_size, d_model=256, n_heads=4, n_layers=4).to(device)\n",
    "print(\"Before LoRA:\")\n",
    "print(f\"  Total params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print()\n",
    "\n",
    "model = inject_lora(model, rank=8, alpha=16)"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_08_visualization_params",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Visualization Params\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_08_visualization_params.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Checkpoint: Parameter Comparison"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the parameter efficiency of LoRA\n",
    "\n",
    "d_values = [256, 512, 1024, 2048, 4096]\n",
    "ranks = [4, 8, 16, 32]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Full vs LoRA params for different model dimensions\n",
    "for r in ranks:\n",
    "    lora_params = [2 * d * r for d in d_values]\n",
    "    full_params = [d * d for d in d_values]\n",
    "    ax1.plot(d_values, [l/f*100 for l, f in zip(lora_params, full_params)],\n",
    "             'o-', label=f'rank={r}', linewidth=2, markersize=6)\n",
    "\n",
    "ax1.set_xlabel('Model Dimension (d)')\n",
    "ax1.set_ylabel('LoRA params as % of Full')\n",
    "ax1.set_title('LoRA Parameter Efficiency')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_ylim(0.01, 100)\n",
    "\n",
    "# Right: Memory comparison for a 7B model\n",
    "categories = ['Model\\nWeights', 'Full FT\\nGradients', 'Full FT\\nOptimizer', 'LoRA\\nParams', 'LoRA\\nGradients', 'LoRA\\nOptimizer']\n",
    "# 7B model, FP16 weights, FP32 gradients/optimizer\n",
    "model_mem = 14  # 7B * 2 bytes (FP16)\n",
    "full_grad = 28  # 7B * 4 bytes (FP32)\n",
    "full_opt = 56   # 7B * 2 states * 4 bytes (FP32)\n",
    "# LoRA: 0.1% of params = 7M params\n",
    "lora_params_mem = 0.028  # 7M * 4 bytes\n",
    "lora_grad = 0.028\n",
    "lora_opt = 0.056\n",
    "\n",
    "mem_full = [model_mem, full_grad, full_opt, 0, 0, 0]\n",
    "mem_lora = [model_mem, 0, 0, lora_params_mem, lora_grad, lora_opt]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.35\n",
    "ax2.bar(x - width/2, mem_full, width, label='Full Fine-Tuning', color='#e74c3c', alpha=0.8)\n",
    "ax2.bar(x + width/2, mem_lora, width, label='LoRA', color='#2ecc71', alpha=0.8)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(categories, fontsize=9)\n",
    "ax2.set_ylabel('GPU Memory (GB)')\n",
    "ax2.set_title('Memory: Full Fine-Tuning vs LoRA (7B Model)')\n",
    "ax2.legend()\n",
    "\n",
    "# Add total annotations\n",
    "total_full = model_mem + full_grad + full_opt\n",
    "total_lora = model_mem + lora_params_mem + lora_grad + lora_opt\n",
    "ax2.text(1, full_grad + 1, f'Total: {total_full:.0f} GB', ha='center', fontweight='bold', color='#e74c3c')\n",
    "ax2.text(4, model_mem + 1, f'Total: {total_lora:.1f} GB', ha='center', fontweight='bold', color='#2ecc71')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Full fine-tuning: {total_full:.0f} GB\")\n",
    "print(f\"LoRA fine-tuning: {total_lora:.1f} GB\")\n",
    "print(f\"Memory saving: {total_full/total_lora:.0f}x\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_09_todo1_ranks",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo1 Ranks\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_09_todo1_ranks.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Experiment with Different Ranks\n",
    "\n",
    "Fine-tune the model with ranks 1, 4, 8, 16, and 32. Plot the final loss vs. rank to understand the rank-performance tradeoff."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the model with different LoRA ranks and compare\n",
    "# final training loss.\n",
    "#\n",
    "# For each rank:\n",
    "# 1. Create a fresh model\n",
    "# 2. Inject LoRA with the given rank\n",
    "# 3. Train for a fixed number of steps\n",
    "# 4. Record the final loss\n",
    "#\n",
    "# YOUR CODE HERE\n",
    "# ranks_to_test = [1, 4, 8, 16, 32]\n",
    "# final_losses = []\n",
    "#\n",
    "# for rank in ranks_to_test:\n",
    "#     # Create fresh model\n",
    "#     test_model = SmallGPT(vocab_size, d_model=256, n_heads=4, n_layers=4).to(device)\n",
    "#     test_model = inject_lora(test_model, rank=rank, alpha=rank*2)\n",
    "#\n",
    "#     # Create optimizer (only trainable params)\n",
    "#     optimizer = torch.optim.Adam(\n",
    "#         [p for p in test_model.parameters() if p.requires_grad],\n",
    "#         lr=1e-3\n",
    "#     )\n",
    "#\n",
    "#     # Train for N steps on synthetic data\n",
    "#     ...\n",
    "#\n",
    "#     final_losses.append(loss)\n",
    "#     print(f\"Rank {rank}: loss = {loss:.4f}, params = {trainable:,}\")\n",
    "#\n",
    "# plt.plot(ranks_to_test, final_losses, 'bo-')\n",
    "# plt.xlabel('LoRA Rank')\n",
    "# plt.ylabel('Final Loss')\n",
    "# plt.title('Loss vs LoRA Rank')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.show()"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_10_todo2_merge",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo2 Merge\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_10_todo2_merge.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement LoRA Merging and Verify Equivalence\n",
    "\n",
    "After training, merge the LoRA weights back into the original model and verify that the merged model produces identical outputs."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Merge LoRA weights and verify output equivalence.\n",
    "#\n",
    "# Steps:\n",
    "# 1. Get output from the LoRA model on a test input\n",
    "# 2. Merge LoRA weights into the original linear layers\n",
    "# 3. Get output from the merged model on the same test input\n",
    "# 4. Verify they are identical (or very close, within FP precision)\n",
    "#\n",
    "# YOUR CODE HERE\n",
    "# test_input = torch.randint(0, vocab_size, (1, 32), device=device)\n",
    "#\n",
    "# # Get LoRA output\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     lora_output = model(test_input)\n",
    "#\n",
    "# # Merge LoRA into base weights\n",
    "# for module in model.modules():\n",
    "#     if isinstance(module, LoRALinear):\n",
    "#         merged = module.merge()\n",
    "#         # Replace the LoRA module with the merged linear\n",
    "#\n",
    "# # Get merged output\n",
    "# with torch.no_grad():\n",
    "#     merged_output = model(test_input)\n",
    "#\n",
    "# # Compare\n",
    "# diff = (lora_output - merged_output).abs().max().item()\n",
    "# print(f\"Max difference: {diff:.2e}\")\n",
    "# print(f\"Outputs are equivalent: {diff < 1e-5}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_11_putting_it_together_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Transition: Putting It Together Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_11_putting_it_together_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us run a complete fine-tuning experiment: pretrain a model on one task, then use LoRA to adapt it to a different task."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_12_pretrain_task",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Pretrain Task\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_12_pretrain_task.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Pretrain on character-level next-token prediction\n",
    "pretrain_text = (\"the quick brown fox jumps over the lazy dog \" * 50 +\n",
    "                 \"a stitch in time saves nine \" * 50 +\n",
    "                 \"all that glitters is not gold \" * 50)\n",
    "\n",
    "chars = sorted(set(pretrain_text))\n",
    "c2i = {c: i for i, c in enumerate(chars)}\n",
    "i2c = {i: c for c, i in c2i.items()}\n",
    "v = len(chars)\n",
    "\n",
    "pretrain_data = torch.tensor([c2i[c] for c in pretrain_text], device=device)\n",
    "\n",
    "# Pretrain\n",
    "base_model = SmallGPT(v, d_model=128, n_heads=4, n_layers=3, max_len=128).to(device)\n",
    "optimizer = torch.optim.Adam(base_model.parameters(), lr=3e-3)\n",
    "\n",
    "base_model.train()\n",
    "seq_len = 48\n",
    "pretrain_losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    total_loss, n = 0, 0\n",
    "    for i in range(0, len(pretrain_data) - seq_len - 1, seq_len):\n",
    "        x = pretrain_data[i:i+seq_len].unsqueeze(0)\n",
    "        y = pretrain_data[i+1:i+seq_len+1].unsqueeze(0)\n",
    "        logits = base_model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, v), y.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        n += 1\n",
    "    avg_loss = total_loss / n\n",
    "    pretrain_losses.append(avg_loss)\n",
    "    if (epoch + 1) % 25 == 0:\n",
    "        print(f\"Pretrain epoch {epoch+1}: loss = {avg_loss:.4f}\")\n",
    "\n",
    "# Save pretrained weights\n",
    "pretrained_state = {k: v.clone() for k, v in base_model.state_dict().items()}\n",
    "print(f\"\\nPretraining complete. Final loss: {pretrain_losses[-1]:.4f}\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_13_finetune_task",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Finetune Task\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_13_finetune_task.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning on a New Task"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Fine-tune on different text (Shakespeare-style)\n",
    "finetune_text = (\"to be or not to be that is the question \" * 50 +\n",
    "                 \"whether tis nobler in the mind to suffer \" * 50 +\n",
    "                 \"the slings and arrows of outrageous fortune \" * 50)\n",
    "\n",
    "# Only use characters that exist in our vocabulary\n",
    "finetune_text = ''.join(c for c in finetune_text if c in c2i)\n",
    "finetune_data = torch.tensor([c2i[c] for c in finetune_text], device=device)\n",
    "\n",
    "# Method 1: Full fine-tuning\n",
    "full_ft_model = SmallGPT(v, d_model=128, n_heads=4, n_layers=3, max_len=128).to(device)\n",
    "full_ft_model.load_state_dict(pretrained_state)\n",
    "full_ft_optimizer = torch.optim.Adam(full_ft_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Method 2: LoRA fine-tuning\n",
    "lora_model = SmallGPT(v, d_model=128, n_heads=4, n_layers=3, max_len=128).to(device)\n",
    "lora_model.load_state_dict(pretrained_state)\n",
    "lora_model = inject_lora(lora_model, rank=8, alpha=16)\n",
    "lora_optimizer = torch.optim.Adam(\n",
    "    [p for p in lora_model.parameters() if p.requires_grad], lr=1e-3\n",
    ")\n",
    "\n",
    "# Train both\n",
    "n_epochs = 80\n",
    "full_ft_losses = []\n",
    "lora_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Full fine-tuning\n",
    "    full_ft_model.train()\n",
    "    ft_loss_sum, n = 0, 0\n",
    "    for i in range(0, len(finetune_data) - seq_len - 1, seq_len):\n",
    "        x = finetune_data[i:i+seq_len].unsqueeze(0)\n",
    "        y = finetune_data[i+1:i+seq_len+1].unsqueeze(0)\n",
    "        logits = full_ft_model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, v), y.view(-1))\n",
    "        full_ft_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        full_ft_optimizer.step()\n",
    "        ft_loss_sum += loss.item()\n",
    "        n += 1\n",
    "    full_ft_losses.append(ft_loss_sum / n)\n",
    "\n",
    "    # LoRA fine-tuning\n",
    "    lora_model.train()\n",
    "    lora_loss_sum, n = 0, 0\n",
    "    for i in range(0, len(finetune_data) - seq_len - 1, seq_len):\n",
    "        x = finetune_data[i:i+seq_len].unsqueeze(0)\n",
    "        y = finetune_data[i+1:i+seq_len+1].unsqueeze(0)\n",
    "        logits = lora_model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, v), y.view(-1))\n",
    "        lora_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        lora_optimizer.step()\n",
    "        lora_loss_sum += loss.item()\n",
    "        n += 1\n",
    "    lora_losses.append(lora_loss_sum / n)\n",
    "\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Full FT loss = {full_ft_losses[-1]:.4f}, \"\n",
    "              f\"LoRA loss = {lora_losses[-1]:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_14_results_visualization",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Results Visualization\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_14_results_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two approaches\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(full_ft_losses, label='Full Fine-Tuning', linewidth=2, color='#e74c3c')\n",
    "ax1.plot(lora_losses, label='LoRA (rank=8)', linewidth=2, color='#2ecc71')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Fine-Tuning Loss: Full vs LoRA')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter comparison\n",
    "full_params = sum(p.numel() for p in full_ft_model.parameters())\n",
    "lora_trainable = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "lora_total = sum(p.numel() for p in lora_model.parameters())\n",
    "\n",
    "categories = ['Full FT\\nTrainable', 'LoRA\\nTrainable', 'LoRA\\nFrozen']\n",
    "values = [full_params, lora_trainable, lora_total - lora_trainable]\n",
    "colors = ['#e74c3c', '#2ecc71', '#95a5a6']\n",
    "\n",
    "ax2.bar(categories, values, color=colors, alpha=0.8, edgecolor='white', linewidth=2)\n",
    "ax2.set_ylabel('Number of Parameters')\n",
    "ax2.set_title('Trainable Parameters Comparison')\n",
    "\n",
    "for i, val in enumerate(values):\n",
    "    ax2.text(i, val + max(values)*0.02, f'{val:,}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFull FT: {full_params:,} trainable params, final loss = {full_ft_losses[-1]:.4f}\")\n",
    "print(f\"LoRA:    {lora_trainable:,} trainable params, final loss = {lora_losses[-1]:.4f}\")\n",
    "print(f\"LoRA trains {lora_trainable/full_params*100:.2f}% of parameters\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_15_final_output_generation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Narration: Final Output Generation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_15_final_output_generation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text from both models to see the qualitative difference\n",
    "full_ft_model.eval()\n",
    "lora_model.eval()\n",
    "\n",
    "prompt_text = \"to be or \"\n",
    "prompt_ids = torch.tensor([[c2i[c] for c in prompt_text]], device=device)\n",
    "\n",
    "def generate_simple(model, prompt, n_tokens=80):\n",
    "    tokens = prompt.clone()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_tokens):\n",
    "            logits = model(tokens)\n",
    "            next_logits = logits[:, -1, :] / 0.8  # temperature\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            next_tok = torch.multinomial(probs, 1)\n",
    "            tokens = torch.cat([tokens, next_tok], dim=1)\n",
    "    return ''.join([i2c[t.item()] for t in tokens[0]])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Full Fine-Tuning:\")\n",
    "print(f\"  {generate_simple(full_ft_model, prompt_ids)}\")\n",
    "print()\n",
    "print(\"LoRA Fine-Tuning (rank=8):\")\n",
    "print(f\"  {generate_simple(lora_model, prompt_ids)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n--- Summary ---\")\n",
    "print(f\"Full FT:  {full_params:>10,} trainable params | loss = {full_ft_losses[-1]:.4f}\")\n",
    "print(f\"LoRA:     {lora_trainable:>10,} trainable params | loss = {lora_losses[-1]:.4f}\")\n",
    "print(f\"Reduction: {full_params/lora_trainable:.0f}x fewer trainable parameters\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_16_reflection_and_next_steps",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Narration: Reflection And Next Steps\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_16_reflection_and_next_steps.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Fine-tuning updates are low-rank**: When we fine-tune a large model, the weight changes $\\Delta W$ can be well-approximated by a product of two small matrices $BA$. This is not a theoretical assumption -- it is an empirical observation that holds across many tasks.\n",
    "\n",
    "2. **LoRA implementation is simple**: Replace `nn.Linear` with a wrapper that adds $BA$ to the output. Freeze the original weights. Train only $A$ and $B$.\n",
    "\n",
    "3. **The parameter savings are dramatic**: For typical configurations (rank 8-16), LoRA uses 0.1-1% of the parameters of full fine-tuning, with comparable performance.\n",
    "\n",
    "4. **Merging is free**: After training, the LoRA weights can be merged back into the original model ($W \\leftarrow W + \\frac{\\alpha}{r} BA$), resulting in zero inference overhead.\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "| Hyperparameter | Typical Range | Effect |\n",
    "|---------------|---------------|--------|\n",
    "| Rank ($r$) | 4-32 | Higher = more capacity, more params |\n",
    "| Alpha ($\\alpha$) | $r$ to $2r$ | Scales the LoRA update magnitude |\n",
    "| Target modules | Q, V (or all) | Which weight matrices to adapt |\n",
    "| Learning rate | 1e-4 to 3e-4 | Usually higher than full FT |\n",
    "\n",
    "### What is Next\n",
    "\n",
    "With efficient fine-tuning covered, the final notebook addresses the most fundamental question: **how do we make models not just capable, but aligned with human values?** We will implement DPO alignment and explore scaling laws."
   ],
   "id": "cell_24"
  }
 ]
}