{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "02_sampling_strategies ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1RJjttCvltRK-j5XaI_Tp752cibGKRYMf\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/02_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_01_setup_check",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Setup Check\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_01_setup_check.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_02_why_this_matters_concept",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why This Matters Concept\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_02_why_this_matters_concept.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Strategies for Text Generation\n",
    "\n",
    "*Part 2 of the Vizuara series on Inference & Scaling*\n",
    "*Estimated time: 50 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "You have a language model that can generate the next token efficiently (thanks to the KV cache). But the model does not output a single token -- it outputs a **probability distribution** over the entire vocabulary. Fifty thousand numbers, each telling you how likely that token is to come next.\n",
    "\n",
    "How do you pick one?\n",
    "\n",
    "This is not a trivial question. Pick wrong, and your model either produces boring, repetitive text (always choosing the most likely token) or incoherent gibberish (sampling too randomly). The difference between a useful chatbot and a useless one often comes down to the sampling strategy.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Implement **greedy decoding** and see why it fails\n",
    "2. Build **temperature scaling** and understand exactly how it reshapes distributions\n",
    "3. Implement **top-k sampling** and see its limitations\n",
    "4. Implement **top-p (nucleus) sampling** and understand why it dominates in practice\n",
    "5. Compare all strategies head-to-head on real generated text"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_03_setup_imports",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Setup Imports\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_03_setup_imports.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup -- run this cell first\n",
    "!pip install -q torch matplotlib numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_04_building_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Building Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_04_building_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Imagine you are completing this sentence: \"The cat sat on the ___\"\n",
    "\n",
    "Your brain considers many possibilities: \"mat\", \"couch\", \"floor\", \"table\", \"roof\", \"windowsill\"... Some are much more likely than others. \"Mat\" and \"couch\" feel natural. \"Roof\" is unusual but creative. \"Refrigerator\" is possible but weird. \"Quantum\" makes no sense at all.\n",
    "\n",
    "A language model faces this exact choice at every single token. The model's probability distribution might look like:\n",
    "\n",
    "| Token | Probability |\n",
    "|-------|------------|\n",
    "| mat | 0.25 |\n",
    "| couch | 0.18 |\n",
    "| floor | 0.15 |\n",
    "| table | 0.12 |\n",
    "| bed | 0.08 |\n",
    "| roof | 0.04 |\n",
    "| ... | ... |\n",
    "| quantum | 0.000001 |\n",
    "\n",
    "**Greedy decoding** always picks \"mat.\" Always. Every single time. This makes the model predictable and repetitive.\n",
    "\n",
    "**Pure random sampling** would occasionally pick \"quantum\" -- which is almost certainly wrong.\n",
    "\n",
    "The art of sampling is finding the sweet spot: enough randomness for creativity, enough constraint for coherence."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_05_the_mathematics",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: The Mathematics\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_05_the_mathematics.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Temperature Scaling\n",
    "\n",
    "Given raw logits $z_v$ for each token $v$ in the vocabulary, temperature scaling modifies the softmax:\n",
    "\n",
    "$$P(v) = \\frac{\\exp(z_v / \\tau)}{\\sum_{v' \\in V} \\exp(z_{v'} / \\tau)}$$\n",
    "\n",
    "where $\\tau$ is the temperature parameter.\n",
    "\n",
    "**Why does this work?** Dividing by $\\tau < 1$ amplifies the differences between logits (making the distribution sharper). Dividing by $\\tau > 1$ dampens the differences (making it flatter).\n",
    "\n",
    "Mathematically, as $\\tau \\to 0$:\n",
    "$$P(v) \\to \\begin{cases} 1 & \\text{if } v = \\arg\\max(z) \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "This is greedy decoding. As $\\tau \\to \\infty$, all probabilities become equal: $P(v) \\to 1/|V|$. This is uniform random sampling.\n",
    "\n",
    "### Top-k Sampling\n",
    "\n",
    "Let $V_k$ be the set of $k$ tokens with the highest probabilities. Top-k sampling sets:\n",
    "\n",
    "$$P_{\\text{top-k}}(v) = \\begin{cases} \\frac{P(v)}{\\sum_{v' \\in V_k} P(v')} & \\text{if } v \\in V_k \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "### Top-p (Nucleus) Sampling\n",
    "\n",
    "Let $V_p$ be the smallest set of tokens such that $\\sum_{v \\in V_p} P(v) \\geq p$. Top-p sampling sets:\n",
    "\n",
    "$$P_{\\text{top-p}}(v) = \\begin{cases} \\frac{P(v)}{\\sum_{v' \\in V_p} P(v')} & \\text{if } v \\in V_p \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "The key difference: $|V_k| = k$ is always the same, but $|V_p|$ varies. When the model is confident, $|V_p|$ is small. When uncertain, $|V_p|$ is large. This adaptivity is why top-p is preferred in practice."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_06_lets_build_it",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Transition: Lets Build It\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_06_lets_build_it.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 Implementing Each Sampling Strategy"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_07_implementing_strategies",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Implementing Strategies\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_07_implementing_strategies.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(logits):\n",
    "    \"\"\"Always pick the most probable token.\"\"\"\n",
    "    return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "def temperature_sample(logits, temperature=1.0):\n",
    "    \"\"\"Sample with temperature scaling.\"\"\"\n",
    "    scaled_logits = logits / temperature\n",
    "    probs = F.softmax(scaled_logits, dim=-1)\n",
    "    return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "\n",
    "def top_k_sample(logits, k=10, temperature=1.0):\n",
    "    \"\"\"Sample from the top-k most probable tokens.\"\"\"\n",
    "    scaled_logits = logits / temperature\n",
    "\n",
    "    # Find the k-th largest value\n",
    "    top_k_values, _ = torch.topk(scaled_logits, k, dim=-1)\n",
    "    min_top_k = top_k_values[:, -1:]  # The k-th largest value\n",
    "\n",
    "    # Mask everything below the threshold\n",
    "    filtered_logits = scaled_logits.clone()\n",
    "    filtered_logits[filtered_logits < min_top_k] = float('-inf')\n",
    "\n",
    "    probs = F.softmax(filtered_logits, dim=-1)\n",
    "    return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "\n",
    "def top_p_sample(logits, p=0.9, temperature=1.0):\n",
    "    \"\"\"Sample from the nucleus (smallest set of tokens with cumulative prob >= p).\"\"\"\n",
    "    scaled_logits = logits / temperature\n",
    "\n",
    "    # Sort in descending order\n",
    "    sorted_logits, sorted_indices = torch.sort(scaled_logits, descending=True, dim=-1)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "    # Remove tokens with cumulative probability above the threshold\n",
    "    # Shift right so we keep the first token that crosses the threshold\n",
    "    sorted_mask = cumulative_probs - F.softmax(sorted_logits, dim=-1) >= p\n",
    "    sorted_logits[sorted_mask] = float('-inf')\n",
    "\n",
    "    # Sample from filtered distribution\n",
    "    probs = F.softmax(sorted_logits, dim=-1)\n",
    "    sampled_index = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "    # Map back to original indices\n",
    "    original_index = sorted_indices.gather(-1, sampled_index)\n",
    "    return original_index\n",
    "\n",
    "\n",
    "# Quick test with synthetic logits\n",
    "test_logits = torch.tensor([[2.0, 1.5, 1.0, 0.5, 0.1, -0.5, -1.0, -2.0, -3.0, -5.0]])\n",
    "\n",
    "print(\"Greedy:\", greedy_decode(test_logits).item())\n",
    "print(\"Temperature=0.5:\", temperature_sample(test_logits, temperature=0.5).item())\n",
    "print(\"Temperature=1.0:\", temperature_sample(test_logits, temperature=1.0).item())\n",
    "print(\"Temperature=2.0:\", temperature_sample(test_logits, temperature=2.0).item())\n",
    "print(\"Top-k (k=3):\", top_k_sample(test_logits, k=3).item())\n",
    "print(\"Top-p (p=0.9):\", top_p_sample(test_logits, p=0.9).item())"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_08_viz_temp_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Viz Temp Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_08_viz_temp_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Checkpoint 1: How Temperature Reshapes the Distribution"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_09_viz_temp_explain",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Viz Temp Explain\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_09_viz_temp_explain.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature effects on a real distribution\n",
    "logits = torch.tensor([3.0, 2.5, 2.0, 1.5, 1.0, 0.5, 0.0, -0.5, -1.0, -2.0])\n",
    "tokens = [f\"tok_{i}\" for i in range(len(logits))]\n",
    "temperatures = [0.3, 0.5, 1.0, 2.0, 5.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(temperatures), figsize=(20, 4), sharey=True)\n",
    "\n",
    "for ax, temp in zip(axes, temperatures):\n",
    "    probs = F.softmax(logits / temp, dim=-1).numpy()\n",
    "    colors = plt.cm.RdYlBu_r(np.linspace(0.2, 0.8, len(probs)))\n",
    "    ax.bar(range(len(probs)), probs, color=colors, edgecolor='white', linewidth=0.5)\n",
    "    ax.set_title(f'T = {temp}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Token index')\n",
    "    if ax == axes[0]:\n",
    "        ax.set_ylabel('Probability')\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    # Show entropy\n",
    "    entropy = -np.sum(probs * np.log(probs + 1e-10))\n",
    "    ax.text(0.95, 0.95, f'H={entropy:.2f}', transform=ax.transAxes,\n",
    "            ha='right', va='top', fontsize=9, style='italic')\n",
    "\n",
    "plt.suptitle('Temperature Scaling: From Greedy (low T) to Uniform (high T)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_10_topk_vs_topp_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Topk Vs Topp Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_10_topk_vs_topp_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Top-k vs Top-p: The Adaptivity Advantage"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_11_topk_vs_topp_explain",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Topk Vs Topp Explain\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_11_topk_vs_topp_explain.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the key difference between top-k and top-p\n",
    "\n",
    "# Case 1: Model is CONFIDENT (one token dominates)\n",
    "confident_logits = torch.tensor([[5.0, 1.0, 0.5, 0.1, -0.5, -1.0, -2.0, -3.0, -4.0, -5.0]])\n",
    "confident_probs = F.softmax(confident_logits, dim=-1).squeeze()\n",
    "\n",
    "# Case 2: Model is UNCERTAIN (many tokens are likely)\n",
    "uncertain_logits = torch.tensor([[1.0, 0.9, 0.85, 0.8, 0.7, 0.6, 0.5, 0.3, 0.1, -0.1]])\n",
    "uncertain_probs = F.softmax(uncertain_logits, dim=-1).squeeze()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 9))\n",
    "\n",
    "for row, (probs, title) in enumerate([(confident_probs, 'Confident'), (uncertain_probs, 'Uncertain')]):\n",
    "    # Original distribution\n",
    "    axes[row, 0].bar(range(10), probs.numpy(), color='steelblue', alpha=0.8)\n",
    "    axes[row, 0].set_title(f'{title}: Original Distribution')\n",
    "    axes[row, 0].set_ylim(0, max(probs.numpy()) * 1.2)\n",
    "\n",
    "    # Top-k (k=3) -- always selects exactly 3\n",
    "    top_k_mask = torch.zeros_like(probs, dtype=torch.bool)\n",
    "    _, top_k_idx = torch.topk(probs, 3)\n",
    "    top_k_mask[top_k_idx] = True\n",
    "    top_k_probs = probs.clone()\n",
    "    top_k_probs[~top_k_mask] = 0\n",
    "    top_k_probs = top_k_probs / top_k_probs.sum()\n",
    "\n",
    "    colors_k = ['#e74c3c' if m else '#ddd' for m in top_k_mask]\n",
    "    axes[row, 1].bar(range(10), top_k_probs.numpy(), color=colors_k, edgecolor='white')\n",
    "    axes[row, 1].set_title(f'{title}: Top-k (k=3) -- {top_k_mask.sum().item()} tokens')\n",
    "    axes[row, 1].set_ylim(0, max(top_k_probs.numpy()) * 1.2)\n",
    "\n",
    "    # Top-p (p=0.9) -- adapts!\n",
    "    sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "    cumsum = torch.cumsum(sorted_probs, dim=0)\n",
    "    # Find cutoff: first index where cumsum >= 0.9\n",
    "    n_included = (cumsum < 0.9).sum().item() + 1\n",
    "    top_p_mask = torch.zeros_like(probs, dtype=torch.bool)\n",
    "    top_p_mask[sorted_idx[:n_included]] = True\n",
    "    top_p_probs = probs.clone()\n",
    "    top_p_probs[~top_p_mask] = 0\n",
    "    top_p_probs = top_p_probs / top_p_probs.sum()\n",
    "\n",
    "    colors_p = ['#2ecc71' if m else '#ddd' for m in top_p_mask]\n",
    "    axes[row, 2].bar(range(10), top_p_probs.numpy(), color=colors_p, edgecolor='white')\n",
    "    axes[row, 2].set_title(f'{title}: Top-p (p=0.9) -- {top_p_mask.sum().item()} tokens')\n",
    "    axes[row, 2].set_ylim(0, max(top_p_probs.numpy()) * 1.2)\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel('Token index')\n",
    "    ax.set_ylabel('Probability')\n",
    "\n",
    "plt.suptitle('Top-k uses fixed count; Top-p adapts to model confidence', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Confident model: top-k selects 3 tokens, top-p selects {(cumsum < 0.9).sum().item() + 1} tokens\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_12_todo1_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo1 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_12_todo1_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Implement Combined Sampling\n",
    "\n",
    "In practice, top-p and temperature are used **together**. Implement a function that applies temperature first, then top-p filtering, then samples."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_13_todo1_impl",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo1 Impl\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_13_todo1_impl.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement combined temperature + top-p sampling\n",
    "def sample_combined(logits, temperature=0.8, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Apply temperature scaling, then top-p filtering, then sample.\n",
    "\n",
    "    Steps:\n",
    "    1. Divide logits by temperature\n",
    "    2. Sort by probability (descending)\n",
    "    3. Compute cumulative probabilities\n",
    "    4. Mask tokens beyond the top-p threshold\n",
    "    5. Sample from the filtered distribution\n",
    "\n",
    "    Args:\n",
    "        logits: (batch_size, vocab_size) raw logits\n",
    "        temperature: temperature parameter (lower = more deterministic)\n",
    "        top_p: nucleus probability threshold\n",
    "    Returns:\n",
    "        next_token: (batch_size, 1) sampled token indices\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "# test_logits = torch.tensor([[3.0, 2.0, 1.5, 1.0, 0.5, 0.0, -1.0, -2.0]])\n",
    "# for _ in range(10):\n",
    "#     token = sample_combined(test_logits, temperature=0.8, top_p=0.9)\n",
    "#     print(f\"Sampled token: {token.item()}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_14_todo2_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo2 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_14_todo2_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Measure Diversity vs. Quality\n",
    "\n",
    "Sample 50 completions from the same prompt using different strategies. Measure the **diversity** (number of unique completions) and visualize the distribution of generated tokens."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_15_todo2_impl",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo2 Impl\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_15_todo2_impl.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate multiple completions with different strategies\n",
    "# and compare their diversity.\n",
    "#\n",
    "# Strategies to compare:\n",
    "# 1. Greedy (baseline -- 0 diversity)\n",
    "# 2. Temperature = 0.3 (low diversity)\n",
    "# 3. Temperature = 1.0 (medium diversity)\n",
    "# 4. Top-p = 0.9, Temperature = 0.8 (production setting)\n",
    "#\n",
    "# For each strategy:\n",
    "# - Generate 50 single-token samples from the same logits\n",
    "# - Count the number of unique tokens sampled\n",
    "# - Plot a histogram of which tokens were selected\n",
    "#\n",
    "# YOUR CODE HERE\n",
    "# test_logits = torch.tensor([[3.0, 2.5, 2.0, 1.5, 1.0, 0.5, 0.0, -0.5, -1.0, -2.0]])\n",
    "# strategies = {\n",
    "#     \"Greedy\": lambda l: greedy_decode(l),\n",
    "#     \"Temp=0.3\": lambda l: temperature_sample(l, 0.3),\n",
    "#     \"Temp=1.0\": lambda l: temperature_sample(l, 1.0),\n",
    "#     \"Top-p=0.9\": lambda l: top_p_sample(l, 0.9, 0.8),\n",
    "# }\n",
    "#\n",
    "# n_samples = 50\n",
    "# fig, axes = plt.subplots(1, 4, figsize=(20, 4), sharey=True)\n",
    "# ..."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_16_viz_repetition_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Viz Repetition Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_16_viz_repetition_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Checkpoint 2: Repetition Penalty Effect"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_17_viz_repetition_explain",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Viz Repetition Explain\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_17_viz_repetition_explain.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how different strategies affect repetition\n",
    "# We simulate a sequence of token probabilities and show how\n",
    "# greedy always picks the same token while others explore\n",
    "\n",
    "logits_sequence = [\n",
    "    torch.tensor([[3.0, 2.5, 2.0, 1.0, 0.5]]),\n",
    "    torch.tensor([[2.8, 2.6, 2.1, 1.2, 0.3]]),\n",
    "    torch.tensor([[3.2, 2.4, 1.9, 1.1, 0.4]]),\n",
    "    torch.tensor([[2.9, 2.7, 2.0, 0.9, 0.5]]),\n",
    "    torch.tensor([[3.1, 2.3, 2.2, 1.3, 0.1]]),\n",
    "] * 4  # Repeat to get 20 steps\n",
    "\n",
    "strategies = {\n",
    "    \"Greedy\": lambda l: greedy_decode(l).item(),\n",
    "    \"Temp=0.5\": lambda l: temperature_sample(l, 0.5).item(),\n",
    "    \"Temp=1.0\": lambda l: temperature_sample(l, 1.0).item(),\n",
    "    \"Top-p=0.9\": lambda l: top_p_sample(l, 0.9, 0.8).item(),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4), sharey=True)\n",
    "\n",
    "for ax, (name, strategy) in zip(axes, strategies.items()):\n",
    "    tokens = [strategy(l) for l in logits_sequence]\n",
    "    ax.plot(tokens, 'o-', markersize=6, linewidth=1.5)\n",
    "    ax.set_title(f'{name}\\n{len(set(tokens))} unique out of {len(tokens)}')\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Token ID')\n",
    "    ax.set_ylim(-0.5, 4.5)\n",
    "    ax.set_yticks(range(5))\n",
    "\n",
    "plt.suptitle('Greedy is Repetitive; Sampling Explores', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_18_putting_it_together_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Transition: Putting It Together Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_18_putting_it_together_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us train a tiny character-level model and see how each sampling strategy produces different text."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_19_model_training",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Model Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_19_model_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a small model (reuse the architecture from Notebook 1 or define inline)\n",
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, n_heads=4, n_layers=2, max_len=256):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=4*d_model,\n",
    "            dropout=0.1, activation='gelu', batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        pos = torch.arange(T, device=x.device)\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(T, device=x.device)\n",
    "        h = self.tok_emb(x) + self.pos_emb(pos)\n",
    "        h = self.transformer(h, mask=mask, is_causal=True)\n",
    "        return self.head(h)\n",
    "\n",
    "\n",
    "# Prepare a small text dataset\n",
    "text = (\"To be or not to be that is the question. \"\n",
    "        \"Whether tis nobler in the mind to suffer the slings and arrows \"\n",
    "        \"of outrageous fortune or to take arms against a sea of troubles \"\n",
    "        \"and by opposing end them. To die to sleep no more and by a sleep \"\n",
    "        \"to say we end the heartache. \") * 10\n",
    "\n",
    "chars = sorted(set(text))\n",
    "c2i = {c: i for i, c in enumerate(chars)}\n",
    "i2c = {i: c for c, i in c2i.items()}\n",
    "data = torch.tensor([c2i[c] for c in text], device=device)\n",
    "\n",
    "# Train\n",
    "model = TinyTransformer(len(chars)).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "seq_len = 48\n",
    "\n",
    "model.train()\n",
    "for epoch in range(150):\n",
    "    total_loss, n = 0, 0\n",
    "    for i in range(0, len(data) - seq_len - 1, seq_len):\n",
    "        x = data[i:i+seq_len].unsqueeze(0)\n",
    "        y = data[i+1:i+seq_len+1].unsqueeze(0)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, len(chars)), y.view(-1))\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item()\n",
    "        n += 1\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}: loss = {total_loss/n:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_20_training_results_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Transition: Training Results Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_20_training_results_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_21_generate_text",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Generate Text\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_21_generate_text.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with different sampling strategies\n",
    "model.eval()\n",
    "\n",
    "prompt_text = \"To be or \"\n",
    "prompt_ids = torch.tensor([[c2i[c] for c in prompt_text]], device=device)\n",
    "\n",
    "def generate_text(model, prompt, n_tokens, sample_fn):\n",
    "    \"\"\"Generate text using a given sampling function.\"\"\"\n",
    "    tokens = prompt.clone()\n",
    "    for _ in range(n_tokens):\n",
    "        logits = model(tokens)\n",
    "        next_logits = logits[:, -1, :]\n",
    "        next_token = sample_fn(next_logits)\n",
    "        tokens = torch.cat([tokens, next_token], dim=1)\n",
    "    return ''.join([i2c[t.item()] for t in tokens[0]])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "strategies = [\n",
    "    (\"Greedy\", lambda l: greedy_decode(l)),\n",
    "    (\"Temp=0.3\", lambda l: temperature_sample(l, 0.3)),\n",
    "    (\"Temp=0.8\", lambda l: temperature_sample(l, 0.8)),\n",
    "    (\"Temp=1.5\", lambda l: temperature_sample(l, 1.5)),\n",
    "    (\"Top-k=5\", lambda l: top_k_sample(l, k=5, temperature=0.8)),\n",
    "    (\"Top-p=0.9\", lambda l: top_p_sample(l, p=0.9, temperature=0.8)),\n",
    "]\n",
    "\n",
    "for name, fn in strategies:\n",
    "    text_out = generate_text(model, prompt_ids, 80, fn)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  {text_out}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_22_final_output_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Final Output Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_22_final_output_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_23_final_output_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For: Final Output Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_23_final_output_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary visualization: token diversity across strategies\n",
    "model.eval()\n",
    "\n",
    "n_runs = 30\n",
    "gen_len = 40\n",
    "\n",
    "diversity_data = {}\n",
    "for name, fn in strategies:\n",
    "    all_tokens = []\n",
    "    for _ in range(n_runs):\n",
    "        tokens = prompt_ids.clone()\n",
    "        for _ in range(gen_len):\n",
    "            logits = model(tokens)\n",
    "            next_logits = logits[:, -1, :]\n",
    "            next_token = fn(next_logits)\n",
    "            tokens = torch.cat([tokens, next_token], dim=1)\n",
    "        generated = tokens[0, prompt_ids.shape[1]:].tolist()\n",
    "        all_tokens.append(tuple(generated))\n",
    "\n",
    "    unique_sequences = len(set(all_tokens))\n",
    "    avg_unique_tokens = np.mean([len(set(seq)) for seq in all_tokens])\n",
    "    diversity_data[name] = {\n",
    "        'unique_seqs': unique_sequences,\n",
    "        'avg_unique_tokens': avg_unique_tokens,\n",
    "    }\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "names = list(diversity_data.keys())\n",
    "unique_seqs = [diversity_data[n]['unique_seqs'] for n in names]\n",
    "unique_toks = [diversity_data[n]['avg_unique_tokens'] for n in names]\n",
    "\n",
    "ax1.barh(names, unique_seqs, color='steelblue', alpha=0.8)\n",
    "ax1.set_xlabel(f'Unique Sequences (out of {n_runs} runs)')\n",
    "ax1.set_title('Sequence Diversity')\n",
    "\n",
    "ax2.barh(names, unique_toks, color='coral', alpha=0.8)\n",
    "ax2.set_xlabel(f'Avg Unique Tokens per Sequence (out of {gen_len})')\n",
    "ax2.set_title('Token Diversity')\n",
    "\n",
    "plt.suptitle('Sampling Strategy Comparison', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_24_reflection_and_next_steps",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Wrap-Up: Reflection And Next Steps\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_24_reflection_and_next_steps.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Greedy decoding** is deterministic and repetitive. It always picks the most probable token, leading to degenerate loops in longer sequences.\n",
    "\n",
    "2. **Temperature** controls the sharpness of the distribution. Low temperature approaches greedy decoding; high temperature approaches uniform random sampling. It is a single knob for the diversity-quality tradeoff.\n",
    "\n",
    "3. **Top-k sampling** restricts sampling to a fixed number of tokens. This works but is inflexible -- the optimal $k$ depends on how confident the model is at each step.\n",
    "\n",
    "4. **Top-p (nucleus) sampling** adapts the candidate set dynamically based on the model's confidence. When the model is sure, few tokens are considered. When uncertain, more options are available. This adaptivity makes it the standard choice in production.\n",
    "\n",
    "5. In practice, **temperature + top-p** are used together: temperature controls overall randomness, and top-p prevents sampling from the dangerous tail.\n",
    "\n",
    "### Production Defaults\n",
    "\n",
    "| System | Temperature | Top-p | Notes |\n",
    "|--------|------------|-------|-------|\n",
    "| OpenAI API | 1.0 | 1.0 | Adjustable per request |\n",
    "| Claude | ~0.7 | ~0.9 | Tuned internally |\n",
    "| LLaMA | 0.6 | 0.9 | Recommended defaults |\n",
    "\n",
    "### What is Next\n",
    "\n",
    "In the next notebook, we will combine the KV cache (Notebook 1) with sampling strategies (this notebook) into a **complete inference engine** and benchmark it end-to-end."
   ],
   "id": "cell_24"
  }
 ]
}