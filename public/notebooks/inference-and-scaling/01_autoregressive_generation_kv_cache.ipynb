{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "01_autoregressive_generation_kv_cache \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive Generation and the KV Cache\n",
    "\n",
    "*Part 1 of the Vizuara series on Inference & Scaling*\n",
    "*Estimated time: 55 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "You have trained a language model. The weights are learned, the loss has converged. Now you need to actually *use* it -- generate text, one token at a time.\n",
    "\n",
    "But here is the problem: naive autoregressive generation is shockingly wasteful. Every time you generate a new token, the model re-processes the *entire* sequence from scratch. For a 1000-token prompt generating 100 new tokens, you end up processing over 100,000 tokens total -- for just 100 outputs.\n",
    "\n",
    "The **KV cache** is the single most important optimization in LLM inference. It eliminates this redundancy entirely, and every production system (ChatGPT, Claude, Gemini -- all of them) relies on it.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Build naive autoregressive generation and **measure** the waste\n",
    "2. Derive **why** the KV cache works from the attention equations\n",
    "3. Implement a KV cache from scratch and see the speedup firsthand\n",
    "4. Analyze the **memory tradeoff** -- because nothing is free\n",
    "\n",
    "By the end, you will have a working KV cache implementation and a deep intuition for why inference is memory-bound, not compute-bound."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup -- run this cell first\n",
    "!pip install -q torch matplotlib numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Let us start with a concrete analogy. Imagine you are writing a book review by hand. You write the first sentence. Then, to write the second sentence, you re-read the entire first sentence from the beginning. To write the third sentence, you re-read sentences one and two from the beginning. By paragraph ten, you are re-reading nine paragraphs just to write one new sentence.\n",
    "\n",
    "That is insane. No human does this. You *remember* what you wrote and just continue from where you left off.\n",
    "\n",
    "Autoregressive generation without a KV cache is exactly this insane re-reading process. The KV cache is the model's \"memory\" of what it has already processed.\n",
    "\n",
    "Let us see this concretely. Here is what autoregressive generation looks like step by step:\n",
    "\n",
    "**Step 1:** Input = \"The capital of\" --> Model processes 3 tokens --> Predicts \"France\"\n",
    "\n",
    "**Step 2:** Input = \"The capital of France\" --> Model processes 4 tokens --> Predicts \"is\"\n",
    "\n",
    "**Step 3:** Input = \"The capital of France is\" --> Model processes 5 tokens --> Predicts \"Paris\"\n",
    "\n",
    "At step 3, the model is re-computing the representations for \"The\", \"capital\", and \"of\" -- even though those computations are *identical* to what it did at step 1. Under causal attention, a token's Key and Value depend only on itself and earlier tokens, never on future tokens. So these values cannot change.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "If we generate $T$ tokens from a prompt of length $n$, how many total tokens get processed without caching? We process $n+1$, then $n+2$, ..., up to $n+T$. The total is:\n",
    "\n",
    "$$\\text{Total} = \\sum_{t=1}^{T}(n+t) = nT + \\frac{T(T+1)}{2}$$\n",
    "\n",
    "For $n=512$ and $T=256$: that is $512 \\times 256 + \\frac{256 \\times 257}{2} = 131{,}072 + 32{,}896 = 163{,}968$ tokens processed for just 256 outputs. Over 640 tokens processed per token generated."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Self-Attention Recap\n",
    "\n",
    "In a single attention head, each token $i$ produces three vectors from its hidden state $h_i$:\n",
    "\n",
    "$$Q_i = W_Q h_i, \\quad K_i = W_K h_i, \\quad V_i = W_V h_i$$\n",
    "\n",
    "The attention output for token $i$ is:\n",
    "\n",
    "$$\\text{Attn}(i) = \\sum_{j \\leq i} \\frac{\\exp(Q_i \\cdot K_j / \\sqrt{d_k})}{\\sum_{m \\leq i} \\exp(Q_i \\cdot K_m / \\sqrt{d_k})} \\cdot V_j$$\n",
    "\n",
    "The critical observation: under causal attention ($j \\leq i$), the Key $K_j$ and Value $V_j$ for token $j$ depend only on $h_j$ and the weight matrices $W_K, W_V$. They do not depend on any future token. So once computed, they never change.\n",
    "\n",
    "### What the KV Cache Stores\n",
    "\n",
    "At generation step $t$, we have already computed $K_1, K_2, \\ldots, K_{t-1}$ and $V_1, V_2, \\ldots, V_{t-1}$ from all previous steps. The KV cache stores these vectors.\n",
    "\n",
    "For the new token at position $t$, we compute:\n",
    "1. $Q_t, K_t, V_t$ from the new token's hidden state\n",
    "2. Append $K_t$ to the cached keys: $[K_1, \\ldots, K_{t-1}, K_t]$\n",
    "3. Append $V_t$ to the cached values: $[V_1, \\ldots, V_{t-1}, V_t]$\n",
    "4. Compute attention: $Q_t$ attends to all cached keys\n",
    "\n",
    "The computation per step goes from $O(t \\cdot d^2)$ (recomputing all projections) to $O(d^2 + t \\cdot d_\\text{head})$ (one projection + attention dot products).\n",
    "\n",
    "### Memory Cost\n",
    "\n",
    "For a model with $L$ layers, $h$ heads, head dimension $d_k$, and sequence length $t$:\n",
    "\n",
    "$$\\text{KV cache memory} = 2 \\times L \\times t \\times h \\times d_k \\times \\text{bytes}$$\n",
    "\n",
    "The factor of 2 accounts for both K and V. This is the tradeoff: we save enormous compute but consume GPU memory proportional to the sequence length."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 A Minimal Transformer Layer\n",
    "\n",
    "We will build a single-layer transformer with multi-head attention, then add the KV cache."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention with optional KV cache support.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x, kv_cache=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model) -- full sequence or single new token\n",
    "            kv_cache: tuple of (cached_K, cached_V) or None\n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "            new_kv_cache: tuple of (K, V) including new tokens\n",
    "        \"\"\"\n",
    "        B, T, _ = x.shape\n",
    "\n",
    "        # Project Q, K, V for new tokens\n",
    "        Q = self.W_q(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        # Q, K, V shape: (B, n_heads, T, d_k)\n",
    "\n",
    "        # If we have a cache, concatenate previous K, V\n",
    "        if kv_cache is not None:\n",
    "            K_prev, V_prev = kv_cache\n",
    "            K = torch.cat([K_prev, K], dim=2)  # (B, n_heads, T_prev + T, d_k)\n",
    "            V = torch.cat([V_prev, V], dim=2)\n",
    "\n",
    "        # Store the updated cache\n",
    "        new_kv_cache = (K, V)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "\n",
    "        # Causal mask: only attend to current and previous positions\n",
    "        # Q has T positions, K has T_total positions\n",
    "        T_total = K.shape[2]\n",
    "        # Each query position can attend to keys at positions <= its absolute position\n",
    "        # Absolute positions of queries: T_total - T, ..., T_total - 1\n",
    "        # A query at absolute position p can attend to keys at positions 0..p\n",
    "        causal_mask = torch.triu(\n",
    "            torch.ones(T, T_total, device=x.device, dtype=torch.bool),\n",
    "            diagonal=T_total - T + 1\n",
    "        )\n",
    "        scores = scores.masked_fill(causal_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        # Reshape and project\n",
    "        output = output.transpose(1, 2).contiguous().view(B, T, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "\n",
    "        return output, new_kv_cache\n",
    "\n",
    "\n",
    "# Quick test\n",
    "d_model, n_heads = 64, 4\n",
    "attn = MultiHeadAttention(d_model, n_heads).to(device)\n",
    "\n",
    "x = torch.randn(1, 5, d_model, device=device)\n",
    "out, cache = attn(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Cache K shape: {cache[0].shape}\")\n",
    "print(f\"Cache V shape: {cache[1].shape}\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Building the Full Transformer Block"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A single transformer block: attention + FFN + layer norms.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, n_heads, d_ff=None):\n",
    "        super().__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, kv_cache=None):\n",
    "        # Pre-norm architecture (used by GPT-2, LLaMA, etc.)\n",
    "        normed = self.ln1(x)\n",
    "        attn_out, new_cache = self.attn(normed, kv_cache=kv_cache)\n",
    "        x = x + attn_out\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x, new_cache\n",
    "\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    \"\"\"A minimal GPT-style model for demonstrating KV cache.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, max_seq_len=512):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, tokens, kv_caches=None, start_pos=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tokens: (B, T) token indices\n",
    "            kv_caches: list of KV caches, one per layer, or None\n",
    "            start_pos: position offset for positional embeddings\n",
    "        Returns:\n",
    "            logits: (B, T, vocab_size)\n",
    "            new_kv_caches: list of updated KV caches\n",
    "        \"\"\"\n",
    "        B, T = tokens.shape\n",
    "        positions = torch.arange(start_pos, start_pos + T, device=tokens.device)\n",
    "\n",
    "        x = self.token_emb(tokens) + self.pos_emb(positions)\n",
    "\n",
    "        new_caches = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            cache = kv_caches[i] if kv_caches is not None else None\n",
    "            x, new_cache = layer(x, kv_cache=cache)\n",
    "            new_caches.append(new_cache)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        return logits, new_caches\n",
    "\n",
    "\n",
    "# Create a small model\n",
    "vocab_size = 256\n",
    "d_model = 128\n",
    "n_heads = 4\n",
    "n_layers = 4\n",
    "\n",
    "model = MiniGPT(vocab_size, d_model, n_heads, n_layers).to(device)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params:,}\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Naive Generation (No KV Cache)"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_naive(model, prompt_tokens, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Generate tokens WITHOUT KV cache.\n",
    "    At each step, re-process the entire sequence from scratch.\n",
    "    \"\"\"\n",
    "    tokens = prompt_tokens.clone()\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, _ = model(tokens, kv_caches=None, start_pos=0)\n",
    "        next_logit = logits[:, -1, :]  # Only care about the last position\n",
    "        next_token = torch.argmax(next_logit, dim=-1, keepdim=True)\n",
    "        tokens = torch.cat([tokens, next_token], dim=1)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Time the naive approach\n",
    "prompt = torch.randint(0, vocab_size, (1, 32), device=device)\n",
    "\n",
    "start = time.time()\n",
    "output_naive = generate_naive(model, prompt, max_new_tokens=100)\n",
    "naive_time = time.time() - start\n",
    "\n",
    "print(f\"Naive generation: {naive_time:.3f}s for 100 tokens\")\n",
    "print(f\"Output length: {output_naive.shape[1]} tokens\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 KV-Cached Generation"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_with_cache(model, prompt_tokens, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Generate tokens WITH KV cache.\n",
    "    First pass: process full prompt, populate cache.\n",
    "    Subsequent passes: process only the new token.\n",
    "    \"\"\"\n",
    "    # Prefill: process the entire prompt\n",
    "    logits, kv_caches = model(prompt_tokens, kv_caches=None, start_pos=0)\n",
    "    next_logit = logits[:, -1, :]\n",
    "    next_token = torch.argmax(next_logit, dim=-1, keepdim=True)\n",
    "\n",
    "    all_tokens = [prompt_tokens, next_token]\n",
    "    cur_pos = prompt_tokens.shape[1]\n",
    "\n",
    "    for _ in range(max_new_tokens - 1):\n",
    "        # Only feed the single new token\n",
    "        logits, kv_caches = model(next_token, kv_caches=kv_caches, start_pos=cur_pos)\n",
    "        cur_pos += 1\n",
    "        next_logit = logits[:, -1, :]\n",
    "        next_token = torch.argmax(next_logit, dim=-1, keepdim=True)\n",
    "        all_tokens.append(next_token)\n",
    "\n",
    "    return torch.cat(all_tokens, dim=1)\n",
    "\n",
    "\n",
    "# Time the cached approach\n",
    "start = time.time()\n",
    "output_cached = generate_with_cache(model, prompt, max_new_tokens=100)\n",
    "cached_time = time.time() - start\n",
    "\n",
    "print(f\"Cached generation: {cached_time:.3f}s for 100 tokens\")\n",
    "print(f\"Speedup: {naive_time / cached_time:.1f}x\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Verify Output Equivalence\n",
    "\n",
    "The KV cache is an *optimization*, not an approximation. The outputs should be **exactly identical** to naive generation. Your task: verify this."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare the outputs from naive and cached generation.\n",
    "# They should be identical (same tokens at every position).\n",
    "# Hint: use torch.equal() or compare element-wise.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# are_equal = ...\n",
    "# print(f\"Outputs are identical: {are_equal}\")\n",
    "\n",
    "# If they are NOT identical, there is a bug in the implementation.\n",
    "# Check the causal masking and position offset logic."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Measure the Speedup Curve\n",
    "\n",
    "How does the speedup change as we increase the number of generated tokens? Generate 10, 50, 100, 200, and 500 tokens with and without cache, and plot the speedup factor."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a plot showing speedup vs. number of generated tokens.\n",
    "# Expected behavior: speedup increases with more tokens because\n",
    "# the naive approach wastes more and more computation.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# generation_lengths = [10, 50, 100, 200]\n",
    "# naive_times = []\n",
    "# cached_times = []\n",
    "#\n",
    "# for n_tokens in generation_lengths:\n",
    "#     # Time naive generation\n",
    "#     ...\n",
    "#     # Time cached generation\n",
    "#     ...\n",
    "#\n",
    "# speedups = [n / c for n, c in zip(naive_times, cached_times)]\n",
    "#\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.plot(generation_lengths, speedups, 'bo-', linewidth=2, markersize=8)\n",
    "# plt.xlabel('Number of generated tokens')\n",
    "# plt.ylabel('Speedup (naive / cached)')\n",
    "# plt.title('KV Cache Speedup vs. Generation Length')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us now visualize the compute and memory tradeoffs to build deeper intuition."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute analysis: count FLOPs for naive vs cached\n",
    "def count_operations(n_prompt, n_generate, d_model, n_layers):\n",
    "    \"\"\"Estimate relative computation for naive vs cached generation.\"\"\"\n",
    "    # Naive: at step t, process (n_prompt + t) tokens through all layers\n",
    "    naive_ops = 0\n",
    "    for t in range(1, n_generate + 1):\n",
    "        seq_len = n_prompt + t\n",
    "        # Projection cost: 3 * seq_len * d_model^2 per layer\n",
    "        # Attention cost: seq_len^2 * d_model per layer\n",
    "        naive_ops += n_layers * (3 * seq_len * d_model**2 + seq_len**2 * d_model)\n",
    "\n",
    "    # Cached: prefill + per-token decode\n",
    "    # Prefill: process n_prompt tokens once\n",
    "    prefill_ops = n_layers * (3 * n_prompt * d_model**2 + n_prompt**2 * d_model)\n",
    "    # Decode: process 1 token at each step\n",
    "    decode_ops = 0\n",
    "    for t in range(1, n_generate + 1):\n",
    "        seq_len = n_prompt + t\n",
    "        # Projection: 3 * 1 * d_model^2 per layer\n",
    "        # Attention: 1 * seq_len * d_model per layer (Q dot all K's)\n",
    "        decode_ops += n_layers * (3 * d_model**2 + seq_len * d_model)\n",
    "    cached_ops = prefill_ops + decode_ops\n",
    "\n",
    "    return naive_ops, cached_ops\n",
    "\n",
    "# Calculate for different prompt lengths\n",
    "prompt_lengths = [32, 64, 128, 256, 512]\n",
    "n_generate = 100\n",
    "\n",
    "naive_ops_list = []\n",
    "cached_ops_list = []\n",
    "\n",
    "for n_prompt in prompt_lengths:\n",
    "    naive, cached = count_operations(n_prompt, n_generate, d_model=128, n_layers=4)\n",
    "    naive_ops_list.append(naive)\n",
    "    cached_ops_list.append(cached)\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart: compute comparison\n",
    "x = np.arange(len(prompt_lengths))\n",
    "width = 0.35\n",
    "bars1 = ax1.bar(x - width/2, [n/1e9 for n in naive_ops_list], width, label='Naive', color='#e74c3c', alpha=0.8)\n",
    "bars2 = ax1.bar(x + width/2, [c/1e9 for c in cached_ops_list], width, label='KV Cache', color='#2ecc71', alpha=0.8)\n",
    "ax1.set_xlabel('Prompt Length')\n",
    "ax1.set_ylabel('Operations (billions)')\n",
    "ax1.set_title('Compute: Naive vs KV Cache')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(prompt_lengths)\n",
    "ax1.legend()\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Memory analysis\n",
    "# KV cache memory = 2 * n_layers * seq_len * d_model * bytes_per_element\n",
    "d_model_real = 4096  # Realistic model\n",
    "n_layers_real = 32\n",
    "bytes_per_element = 2  # FP16\n",
    "\n",
    "seq_lengths = np.arange(1, 8193)\n",
    "kv_memory_gb = (2 * n_layers_real * seq_lengths * d_model_real * bytes_per_element) / (1024**3)\n",
    "\n",
    "ax2.plot(seq_lengths, kv_memory_gb, 'b-', linewidth=2)\n",
    "ax2.axhline(y=16, color='r', linestyle='--', alpha=0.7, label='16 GB (T4 GPU)')\n",
    "ax2.axhline(y=80, color='orange', linestyle='--', alpha=0.7, label='80 GB (A100 GPU)')\n",
    "ax2.set_xlabel('Sequence Length (tokens)')\n",
    "ax2.set_ylabel('KV Cache Memory (GB)')\n",
    "ax2.set_title('KV Cache Memory vs Sequence Length\\n(32-layer, d=4096 model, FP16)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nKV cache at 2048 tokens: {kv_memory_gb[2047]:.2f} GB\")\n",
    "print(f\"KV cache at 4096 tokens: {kv_memory_gb[4095]:.2f} GB\")\n",
    "print(f\"KV cache at 8192 tokens: {kv_memory_gb[8191]:.2f} GB\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "Since we are focused on *inference* rather than training in this notebook, let us quantify the empirical performance of our KV cache implementation."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: measure tokens per second for different configurations\n",
    "configs = [\n",
    "    {\"prompt_len\": 32, \"gen_len\": 50},\n",
    "    {\"prompt_len\": 128, \"gen_len\": 100},\n",
    "    {\"prompt_len\": 256, \"gen_len\": 200},\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Config':<25} {'Naive (tok/s)':<15} {'Cached (tok/s)':<15} {'Speedup':<10}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for cfg in configs:\n",
    "    prompt = torch.randint(0, vocab_size, (1, cfg[\"prompt_len\"]), device=device)\n",
    "\n",
    "    # Warm up\n",
    "    _ = generate_naive(model, prompt, max_new_tokens=5)\n",
    "    _ = generate_with_cache(model, prompt, max_new_tokens=5)\n",
    "\n",
    "    # Benchmark naive\n",
    "    start = time.time()\n",
    "    for _ in range(3):\n",
    "        _ = generate_naive(model, prompt, max_new_tokens=cfg[\"gen_len\"])\n",
    "    naive_t = (time.time() - start) / 3\n",
    "    naive_tps = cfg[\"gen_len\"] / naive_t\n",
    "\n",
    "    # Benchmark cached\n",
    "    start = time.time()\n",
    "    for _ in range(3):\n",
    "        _ = generate_with_cache(model, prompt, max_new_tokens=cfg[\"gen_len\"])\n",
    "    cached_t = (time.time() - start) / 3\n",
    "    cached_tps = cfg[\"gen_len\"] / cached_t\n",
    "\n",
    "    label = f\"prompt={cfg['prompt_len']}, gen={cfg['gen_len']}\"\n",
    "    print(f\"{label:<25} {naive_tps:<15.1f} {cached_tps:<15.1f} {cached_tps/naive_tps:<10.1f}x\")\n",
    "\n",
    "print(\"=\" * 70)"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output\n",
    "\n",
    "Let us generate some actual text to see our complete system in action. We will use a character-level model on a tiny corpus so we can train it quickly."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a tiny character-level model for demonstration\n",
    "text = \"\"\"The quick brown fox jumps over the lazy dog. The dog barked at the fox.\n",
    "The fox ran away quickly. The lazy dog went back to sleep. The brown fox was clever.\n",
    "The dog woke up and chased the fox again. The fox jumped over the fence.\"\"\"\n",
    "\n",
    "# Character-level tokenization\n",
    "chars = sorted(set(text))\n",
    "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
    "idx_to_char = {i: c for c, i in char_to_idx.items()}\n",
    "v_size = len(chars)\n",
    "\n",
    "# Create training data\n",
    "data = torch.tensor([char_to_idx[c] for c in text], dtype=torch.long, device=device)\n",
    "\n",
    "# Train the model\n",
    "small_model = MiniGPT(v_size, d_model=64, n_heads=4, n_layers=2, max_seq_len=256).to(device)\n",
    "optimizer = torch.optim.Adam(small_model.parameters(), lr=3e-3)\n",
    "\n",
    "# Simple training loop\n",
    "small_model.train()\n",
    "seq_len = 32\n",
    "\n",
    "for epoch in range(200):\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    for i in range(0, len(data) - seq_len - 1, seq_len):\n",
    "        x = data[i:i+seq_len].unsqueeze(0)\n",
    "        y = data[i+1:i+seq_len+1].unsqueeze(0)\n",
    "        logits, _ = small_model(x, start_pos=0)\n",
    "        loss = F.cross_entropy(logits.view(-1, v_size), y.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}: loss = {total_loss/n_batches:.4f}\")\n",
    "\n",
    "# Generate text with KV cache\n",
    "small_model.eval()\n",
    "prompt_text = \"The quick\"\n",
    "prompt_ids = torch.tensor([[char_to_idx[c] for c in prompt_text]], device=device)\n",
    "\n",
    "generated = generate_with_cache(small_model, prompt_ids, max_new_tokens=100)\n",
    "generated_text = ''.join([idx_to_char[t.item()] for t in generated[0]])\n",
    "print(f\"\\nGenerated text (KV cached):\\n{generated_text}\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Autoregressive generation** is inherently sequential: each token depends on all previous tokens. Without optimization, this leads to massive redundant computation.\n",
    "\n",
    "2. **The KV cache** exploits the fact that under causal attention, past tokens' Keys and Values never change. By caching them, we reduce per-step computation from processing the entire sequence to processing a single token.\n",
    "\n",
    "3. **The tradeoff is memory**: the KV cache grows linearly with sequence length and model depth. For large models (70B+ parameters) serving many concurrent users, KV cache memory management becomes the primary bottleneck.\n",
    "\n",
    "4. **The speedup is substantial**: for our small model, we saw significant speedups that increase with longer generation.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Metric | Without KV Cache | With KV Cache |\n",
    "|--------|-----------------|---------------|\n",
    "| Tokens processed per step | Entire sequence | 1 token |\n",
    "| Computation scaling | $O(T^2)$ total | $O(T)$ total (for projections) |\n",
    "| Memory overhead | Minimal | $O(L \\times T \\times d)$ |\n",
    "\n",
    "### What is Next\n",
    "\n",
    "In the next notebook, we will tackle the other half of the generation problem: **sampling strategies**. The KV cache tells us *how* to generate efficiently. Sampling strategies tell us *what* to generate -- how to choose the next token from the probability distribution in a way that produces coherent, diverse, and high-quality text.\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- Vaswani et al., \"Attention Is All You Need\" (2017) -- the original transformer and attention mechanism\n",
    "- Pope et al., \"Efficiently Scaling Transformer Inference\" (2022) -- advanced KV cache management\n",
    "- Kwon et al., \"Efficient Memory Management for Large Language Model Serving with PagedAttention\" (2023) -- vLLM and PagedAttention"
   ],
   "id": "cell_24"
  }
 ]
}