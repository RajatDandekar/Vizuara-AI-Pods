{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Inference and Scaling \u2014 Notebook Index"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Scaling -- Notebook Index\n",
    "\n",
    "*Build LLM from Scratch, Pod 5*\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the hands-on notebooks for **Inference and Scaling**. This pod covers everything that happens after training: how to generate text efficiently, how to control output quality, how to build a production inference engine, and how to adapt pretrained models to new tasks.\n",
    "\n",
    "These four notebooks follow a natural progression. Each one builds on the previous, and by the end you will have a complete, working inference system with LoRA fine-tuning."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 1: Autoregressive Generation and the KV Cache\n",
    "\n",
    "**File:** `01_autoregressive_generation_kv_cache.ipynb`\n",
    "**Estimated time:** 55 minutes\n",
    "\n",
    "You start with the most fundamental question: how does a language model actually generate text? You will build naive autoregressive generation from scratch, measure the computational waste, derive why the KV cache works from the attention equations, and implement a working KV cache that delivers dramatic speedups. By the end, you will understand why inference is memory-bound, not compute-bound.\n",
    "\n",
    "**Key concepts:**\n",
    "- Autoregressive token-by-token generation\n",
    "- Redundant computation in naive generation\n",
    "- KV cache derivation from causal attention\n",
    "- Memory-compute tradeoff analysis"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 2: Sampling Strategies\n",
    "\n",
    "**File:** `02_sampling_strategies.ipynb`\n",
    "**Estimated time:** 50 minutes\n",
    "\n",
    "With efficient generation in hand, you tackle the next question: how do you pick the right token? You will implement greedy decoding and see it degenerate into repetition. Then you will build temperature scaling, top-k sampling, and top-p (nucleus) sampling from scratch, visualize how each reshapes the probability distribution, and develop intuition for when to use each strategy.\n",
    "\n",
    "**Key concepts:**\n",
    "- Greedy decoding and its failure modes\n",
    "- Temperature scaling and its effect on distributions\n",
    "- Top-k sampling with fixed vocabulary cutoff\n",
    "- Top-p (nucleus) sampling with adaptive cutoff\n",
    "- Comparing sampling strategies on real generation tasks"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 3: Complete Inference Engine\n",
    "\n",
    "**File:** `03_complete_inference_engine.ipynb`\n",
    "**Estimated time:** 60 minutes\n",
    "\n",
    "Now you combine everything into a production-grade inference pipeline. You will build a complete generate function with KV cache, configurable sampling, and stopping criteria. You will benchmark throughput, measure latency at different sequence lengths, and understand the system-level considerations that matter when serving LLMs in production.\n",
    "\n",
    "**Key concepts:**\n",
    "- End-to-end generation loop with KV cache and sampling\n",
    "- Batched inference for throughput\n",
    "- Latency profiling and bottleneck analysis\n",
    "- Stopping criteria and special token handling"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 4: LoRA Fine-Tuning\n",
    "\n",
    "**File:** `04_lora_finetuning.ipynb`\n",
    "**Estimated time:** 60 minutes\n",
    "\n",
    "Finally, you learn how to adapt a pretrained model to new tasks without retraining all the parameters. You will implement LoRA (Low-Rank Adaptation) from scratch, understand why weight updates are low-rank, apply LoRA adapters to a transformer, and fine-tune on a downstream task. You will see how training 0.1% of the parameters can match full fine-tuning performance.\n",
    "\n",
    "**Key concepts:**\n",
    "- The low-rank structure of weight updates\n",
    "- LoRA decomposition: freezing W, training B and A\n",
    "- Applying LoRA to attention projections\n",
    "- Parameter-efficient fine-tuning in practice\n",
    "- Comparing LoRA to full fine-tuning"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested Approach\n",
    "\n",
    "1. Work through the notebooks in order. Each one assumes familiarity with the previous material.\n",
    "2. Run every code cell. The computations are designed to run on a free Colab T4 GPU.\n",
    "3. Complete the TODO exercises before looking at the solutions. The exercises are where the real learning happens.\n",
    "4. Pay attention to the numerical examples. They build intuition that the theory alone cannot provide."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Familiarity with the Transformer architecture (self-attention, multi-head attention, layer normalization)\n",
    "- Basic PyTorch fluency (tensors, modules, autograd)\n",
    "- Completion of Pods 1-4 in this course (or equivalent knowledge)"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick environment check\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"\\nYou are ready to begin. Open Notebook 01 to start.\")"
   ],
   "id": "cell_8"
  }
 ]
}