{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "03_complete_inference_engine \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Complete Inference Engine\n",
    "\n",
    "*Part 3 of the Vizuara series on Inference & Scaling*\n",
    "*Estimated time: 60 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In Notebook 1, we built the KV cache to make generation fast. In Notebook 2, we built sampling strategies to make generation good. Now we put them together into a single, complete inference engine -- the kind of system that powers every LLM chatbot in the world.\n",
    "\n",
    "This is not just engineering glue. There are important design decisions at the intersection:\n",
    "- How do we handle the **prefill phase** (processing the prompt) separately from the **decode phase** (generating tokens)?\n",
    "- How do we manage **stopping conditions** (end-of-sequence token, max length)?\n",
    "- What are the **real bottlenecks** in practice?\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Build a **complete generation pipeline** with KV cache + top-p sampling\n",
    "2. Implement proper **prefill/decode separation**\n",
    "3. Add **streaming output** (token-by-token delivery)\n",
    "4. **Benchmark** the full system: throughput, latency, and memory\n",
    "5. Profile where time is actually spent"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup -- run this cell first\n",
    "!pip install -q torch matplotlib numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Think of an LLM inference request as a restaurant order. There are two distinct phases:\n",
    "\n",
    "**Prefill** (the kitchen prepares your main dish): The model processes the entire prompt at once, computing attention over all prompt tokens in parallel. This is compute-bound -- the GPU does a lot of math on many tokens simultaneously.\n",
    "\n",
    "**Decode** (the waiter brings courses one at a time): The model generates tokens sequentially, one at a time, using the KV cache. This is memory-bound -- the GPU spends most of its time loading cached KV vectors from memory rather than doing math.\n",
    "\n",
    "Understanding this distinction is critical for optimizing inference:\n",
    "- **Prefill optimization**: batch prompts, use tensor parallelism, maximize GPU compute utilization\n",
    "- **Decode optimization**: minimize memory access, use quantized KV caches, maximize memory bandwidth utilization\n",
    "\n",
    "Most production systems separate these two phases entirely, running them on different hardware configurations."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Prefill Phase\n",
    "\n",
    "Given a prompt of $n$ tokens, the prefill phase processes all tokens in parallel through the transformer. For each layer $l$:\n",
    "\n",
    "$$Q^{(l)}, K^{(l)}, V^{(l)} = W_Q^{(l)} H^{(l)}, \\; W_K^{(l)} H^{(l)}, \\; W_V^{(l)} H^{(l)}$$\n",
    "\n",
    "where $H^{(l)} \\in \\mathbb{R}^{n \\times d}$ is the hidden state matrix for all $n$ tokens.\n",
    "\n",
    "Attention is computed in parallel over all $n$ query positions with causal masking:\n",
    "\n",
    "$$\\text{Attn}^{(l)} = \\text{softmax}\\left(\\frac{Q^{(l)} {K^{(l)}}^T}{\\sqrt{d_k}} + M_{\\text{causal}}\\right) V^{(l)}$$\n",
    "\n",
    "**Compute cost**: $O(n^2 \\cdot d)$ per layer for attention, plus $O(n \\cdot d^2)$ for projections and FFN.\n",
    "\n",
    "**Output**: The KV cache is populated with $K^{(l)}$ and $V^{(l)}$ for all layers. We also get the logits for the last token to begin generation.\n",
    "\n",
    "### Decode Phase\n",
    "\n",
    "At each decode step $t$, we process a single new token:\n",
    "\n",
    "1. Compute $q_t, k_t, v_t$ from the new token's hidden state (cost: $O(d^2)$ per layer)\n",
    "2. Append $k_t, v_t$ to the cache\n",
    "3. Compute attention: $q_t$ against all $(n + t)$ cached keys (cost: $O((n+t) \\cdot d_k)$ per layer per head)\n",
    "\n",
    "**Total cost per decode step**: $O(d^2 + (n+t) \\cdot d)$ per layer\n",
    "\n",
    "The key insight: for large models ($d = 4096+$), the projection cost $O(d^2)$ dominates when the sequence is short, but the attention cost $O((n+t) \\cdot d)$ dominates for long sequences.\n",
    "\n",
    "### Arithmetic Intensity\n",
    "\n",
    "The **arithmetic intensity** (FLOPs per byte of memory accessed) determines whether a workload is compute-bound or memory-bound:\n",
    "\n",
    "$$\\text{AI}_{\\text{prefill}} = O(n) \\quad \\text{(high -- compute-bound)}$$\n",
    "$$\\text{AI}_{\\text{decode}} = O(1) \\quad \\text{(low -- memory-bound)}$$\n",
    "\n",
    "Prefill processes $n$ tokens and reuses the weight matrices across all tokens -- high reuse, high arithmetic intensity. Decode processes 1 token -- low reuse, low arithmetic intensity. This is why decode is almost always memory-bandwidth-bound."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 The Model (with KV Cache Support)"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x, kv_cache=None, start_pos=0):\n",
    "        B, T, _ = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.split(self.d_model, dim=-1)\n",
    "\n",
    "        q = q.view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            k_prev, v_prev = kv_cache\n",
    "            k = torch.cat([k_prev, k], dim=2)\n",
    "            v = torch.cat([v_prev, v], dim=2)\n",
    "\n",
    "        new_cache = (k, v)\n",
    "        T_total = k.shape[2]\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "\n",
    "        # Causal mask\n",
    "        causal = torch.triu(\n",
    "            torch.ones(T, T_total, device=x.device, dtype=torch.bool),\n",
    "            diagonal=T_total - T + 1\n",
    "        )\n",
    "        scores.masked_fill_(causal.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, self.d_model)\n",
    "        return self.out_proj(out), new_cache\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.attn = CausalSelfAttention(d_model, n_heads)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, kv_cache=None, start_pos=0):\n",
    "        h = self.ln1(x)\n",
    "        attn_out, new_cache = self.attn(h, kv_cache=kv_cache, start_pos=start_pos)\n",
    "        x = x + attn_out\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x, new_cache\n",
    "\n",
    "\n",
    "class InferenceGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, n_heads=4, n_layers=4, max_len=512):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(d_model, n_heads) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.eos_token_id = None  # Set after tokenizer is defined\n",
    "\n",
    "    def forward(self, tokens, kv_caches=None, start_pos=0):\n",
    "        B, T = tokens.shape\n",
    "        pos = torch.arange(start_pos, start_pos + T, device=tokens.device)\n",
    "        x = self.tok_emb(tokens) + self.pos_emb(pos)\n",
    "\n",
    "        new_caches = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            cache = kv_caches[i] if kv_caches is not None else None\n",
    "            x, new_cache = layer(x, kv_cache=cache, start_pos=start_pos)\n",
    "            new_caches.append(new_cache)\n",
    "\n",
    "        logits = self.head(self.ln_f(x))\n",
    "        return logits, new_caches\n",
    "\n",
    "\n",
    "# Create model\n",
    "vocab_size = 128  # ASCII characters\n",
    "model = InferenceGPT(vocab_size).to(device)\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Complete Generation Engine"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationEngine:\n",
    "    \"\"\"\n",
    "    A complete LLM inference engine with KV cache and configurable sampling.\n",
    "    Separates prefill and decode phases explicitly.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, eos_token_id=None):\n",
    "        self.model = model\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.model.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, prompt_tokens, max_new_tokens=100,\n",
    "                 temperature=0.8, top_p=0.9, stream=False):\n",
    "        \"\"\"\n",
    "        Generate tokens with KV cache and top-p sampling.\n",
    "\n",
    "        Args:\n",
    "            prompt_tokens: (1, seq_len) input token ids\n",
    "            max_new_tokens: maximum tokens to generate\n",
    "            temperature: sampling temperature\n",
    "            top_p: nucleus sampling threshold\n",
    "            stream: if True, yield tokens one at a time\n",
    "        \"\"\"\n",
    "        # Phase 1: Prefill\n",
    "        prefill_start = time.time()\n",
    "        logits, kv_caches = self.model(prompt_tokens, kv_caches=None, start_pos=0)\n",
    "        prefill_time = time.time() - prefill_start\n",
    "\n",
    "        # Sample first token\n",
    "        next_logits = logits[:, -1, :]\n",
    "        next_token = self._sample(next_logits, temperature, top_p)\n",
    "        generated = [next_token]\n",
    "\n",
    "        if stream:\n",
    "            yield next_token.item(), {'phase': 'decode', 'step': 0}\n",
    "\n",
    "        cur_pos = prompt_tokens.shape[1]\n",
    "\n",
    "        # Phase 2: Decode\n",
    "        decode_start = time.time()\n",
    "        for step in range(1, max_new_tokens):\n",
    "            logits, kv_caches = self.model(\n",
    "                next_token, kv_caches=kv_caches, start_pos=cur_pos\n",
    "            )\n",
    "            cur_pos += 1\n",
    "            next_logits = logits[:, -1, :]\n",
    "            next_token = self._sample(next_logits, temperature, top_p)\n",
    "            generated.append(next_token)\n",
    "\n",
    "            if stream:\n",
    "                yield next_token.item(), {\n",
    "                    'phase': 'decode',\n",
    "                    'step': step,\n",
    "                }\n",
    "\n",
    "            if self.eos_token_id is not None and next_token.item() == self.eos_token_id:\n",
    "                break\n",
    "\n",
    "        decode_time = time.time() - decode_start\n",
    "\n",
    "        if not stream:\n",
    "            all_tokens = torch.cat([prompt_tokens] + generated, dim=1)\n",
    "            stats = {\n",
    "                'prefill_time': prefill_time,\n",
    "                'decode_time': decode_time,\n",
    "                'prefill_tokens': prompt_tokens.shape[1],\n",
    "                'decode_tokens': len(generated),\n",
    "                'prefill_tps': prompt_tokens.shape[1] / prefill_time if prefill_time > 0 else 0,\n",
    "                'decode_tps': len(generated) / decode_time if decode_time > 0 else 0,\n",
    "            }\n",
    "            return all_tokens, stats\n",
    "\n",
    "    def _sample(self, logits, temperature=1.0, top_p=1.0):\n",
    "        \"\"\"Top-p sampling with temperature.\"\"\"\n",
    "        if temperature <= 0:\n",
    "            return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        logits = logits / temperature\n",
    "\n",
    "        if top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
    "            cumprobs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            mask = cumprobs - F.softmax(sorted_logits, dim=-1) >= top_p\n",
    "            sorted_logits[mask] = float('-inf')\n",
    "            probs = F.softmax(sorted_logits, dim=-1)\n",
    "            idx = torch.multinomial(probs, num_samples=1)\n",
    "            return sorted_indices.gather(-1, idx)\n",
    "        else:\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "\n",
    "# Create engine\n",
    "engine = GenerationEngine(model)\n",
    "\n",
    "# Quick test\n",
    "prompt = torch.randint(0, vocab_size, (1, 32), device=device)\n",
    "output, stats = engine.generate(prompt, max_new_tokens=50, temperature=0.8, top_p=0.9)\n",
    "\n",
    "print(f\"Generated {stats['decode_tokens']} tokens\")\n",
    "print(f\"Prefill: {stats['prefill_time']*1000:.1f}ms ({stats['prefill_tps']:.0f} tok/s)\")\n",
    "print(f\"Decode:  {stats['decode_time']*1000:.1f}ms ({stats['decode_tps']:.0f} tok/s)\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Checkpoint: Prefill vs Decode Timing"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile prefill vs decode across different prompt lengths\n",
    "prompt_lengths = [16, 32, 64, 128, 256]\n",
    "gen_length = 100\n",
    "n_trials = 3\n",
    "\n",
    "prefill_times = []\n",
    "decode_times = []\n",
    "prefill_tps_list = []\n",
    "decode_tps_list = []\n",
    "\n",
    "for plen in prompt_lengths:\n",
    "    p_times = []\n",
    "    d_times = []\n",
    "    for _ in range(n_trials):\n",
    "        prompt = torch.randint(0, vocab_size, (1, plen), device=device)\n",
    "        _, stats = engine.generate(prompt, max_new_tokens=gen_length)\n",
    "        p_times.append(stats['prefill_time'])\n",
    "        d_times.append(stats['decode_time'])\n",
    "\n",
    "    prefill_times.append(np.mean(p_times))\n",
    "    decode_times.append(np.mean(d_times))\n",
    "    prefill_tps_list.append(np.mean([plen / t for t in p_times]))\n",
    "    decode_tps_list.append(np.mean([gen_length / t for t in d_times]))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Stacked bar: prefill vs decode time\n",
    "ax = axes[0]\n",
    "x = np.arange(len(prompt_lengths))\n",
    "ax.bar(x, [t*1000 for t in prefill_times], label='Prefill', color='#3498db', alpha=0.8)\n",
    "ax.bar(x, [t*1000 for t in decode_times], bottom=[t*1000 for t in prefill_times],\n",
    "       label='Decode', color='#e74c3c', alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(prompt_lengths)\n",
    "ax.set_xlabel('Prompt Length')\n",
    "ax.set_ylabel('Time (ms)')\n",
    "ax.set_title('Total Latency Breakdown')\n",
    "ax.legend()\n",
    "\n",
    "# Throughput comparison\n",
    "ax = axes[1]\n",
    "ax.plot(prompt_lengths, prefill_tps_list, 'bo-', linewidth=2, label='Prefill tok/s')\n",
    "ax.plot(prompt_lengths, decode_tps_list, 'ro-', linewidth=2, label='Decode tok/s')\n",
    "ax.set_xlabel('Prompt Length')\n",
    "ax.set_ylabel('Tokens per Second')\n",
    "ax.set_title('Throughput: Prefill vs Decode')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Fraction of time in decode\n",
    "ax = axes[2]\n",
    "decode_fracs = [d / (p + d) * 100 for p, d in zip(prefill_times, decode_times)]\n",
    "ax.bar(x, decode_fracs, color='#e74c3c', alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(prompt_lengths)\n",
    "ax.set_xlabel('Prompt Length')\n",
    "ax.set_ylabel('% of Total Time')\n",
    "ax.set_title('Fraction of Time in Decode Phase')\n",
    "ax.axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Implement Streaming Generation\n",
    "\n",
    "The engine supports a `stream=True` mode that yields tokens one at a time. Use it to build a function that prints tokens as they are generated, simulating the \"typing\" effect you see in ChatGPT."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the streaming mode to print tokens as they arrive.\n",
    "# The engine.generate() function is a generator when stream=True.\n",
    "#\n",
    "# YOUR CODE HERE\n",
    "# def stream_generate(engine, prompt_tokens, max_new_tokens=50,\n",
    "#                     temperature=0.8, top_p=0.9):\n",
    "#     \"\"\"Print tokens as they are generated, with timing info.\"\"\"\n",
    "#     print(\"Generating: \", end=\"\", flush=True)\n",
    "#     token_times = []\n",
    "#\n",
    "#     for token_id, info in engine.generate(\n",
    "#         prompt_tokens, max_new_tokens=max_new_tokens,\n",
    "#         temperature=temperature, top_p=top_p, stream=True\n",
    "#     ):\n",
    "#         # Convert token_id to character and print\n",
    "#         char = chr(token_id) if 32 <= token_id < 127 else '?'\n",
    "#         print(char, end=\"\", flush=True)\n",
    "#         token_times.append(time.time())\n",
    "#\n",
    "#     print(\"\\n\")\n",
    "#\n",
    "#     # Calculate inter-token latencies\n",
    "#     if len(token_times) > 1:\n",
    "#         latencies = [token_times[i+1] - token_times[i]\n",
    "#                      for i in range(len(token_times)-1)]\n",
    "#         print(f\"Mean inter-token latency: {np.mean(latencies)*1000:.1f}ms\")\n",
    "#         print(f\"P50 latency: {np.percentile(latencies, 50)*1000:.1f}ms\")\n",
    "#         print(f\"P99 latency: {np.percentile(latencies, 99)*1000:.1f}ms\")\n",
    "#\n",
    "# # Test it\n",
    "# prompt = torch.randint(0, vocab_size, (1, 16), device=device)\n",
    "# stream_generate(engine, prompt)"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: KV Cache Memory Calculator\n",
    "\n",
    "Build a function that calculates the exact KV cache memory for any model configuration and prints a formatted report."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a KV cache memory calculator.\n",
    "#\n",
    "# def kv_cache_memory(d_model, n_heads, n_layers, seq_len,\n",
    "#                     dtype_bytes=2, batch_size=1):\n",
    "#     \"\"\"\n",
    "#     Calculate KV cache memory in bytes.\n",
    "#\n",
    "#     Memory = 2 (K and V) * n_layers * seq_len * d_model * dtype_bytes * batch_size\n",
    "#\n",
    "#     Returns: dict with memory in bytes, MB, and GB\n",
    "#     \"\"\"\n",
    "#     # YOUR CODE HERE\n",
    "#     pass\n",
    "#\n",
    "# # Test with common model sizes\n",
    "# models = {\n",
    "#     \"GPT-2 (124M)\": {\"d_model\": 768, \"n_heads\": 12, \"n_layers\": 12},\n",
    "#     \"LLaMA-7B\": {\"d_model\": 4096, \"n_heads\": 32, \"n_layers\": 32},\n",
    "#     \"LLaMA-70B\": {\"d_model\": 8192, \"n_heads\": 64, \"n_layers\": 80},\n",
    "# }\n",
    "#\n",
    "# print(f\"{'Model':<20} {'Seq=2K':<12} {'Seq=8K':<12} {'Seq=32K':<12}\")\n",
    "# print(\"-\" * 56)\n",
    "# for name, cfg in models.items():\n",
    "#     mem_2k = kv_cache_memory(**cfg, seq_len=2048)\n",
    "#     mem_8k = kv_cache_memory(**cfg, seq_len=8192)\n",
    "#     mem_32k = kv_cache_memory(**cfg, seq_len=32768)\n",
    "#     print(f\"{name:<20} {mem_2k['gb']:<12.2f} {mem_8k['gb']:<12.2f} {mem_32k['gb']:<12.2f}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us train the model on real text and run the complete engine."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on Shakespeare-like text\n",
    "corpus = \"\"\"\n",
    "To be or not to be that is the question\n",
    "Whether tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune\n",
    "Or to take arms against a sea of troubles\n",
    "And by opposing end them To die to sleep\n",
    "No more and by a sleep to say we end\n",
    "The heartache and the thousand natural shocks\n",
    "That flesh is heir to Tis a consummation\n",
    "Devoutly to be wished To die to sleep\n",
    "To sleep perchance to dream aye there is the rub\n",
    "\"\"\".strip() * 20\n",
    "\n",
    "chars = sorted(set(corpus))\n",
    "c2i = {c: i for i, c in enumerate(chars)}\n",
    "i2c = {i: c for c, i in c2i.items()}\n",
    "v = len(chars)\n",
    "\n",
    "# Rebuild model with correct vocab\n",
    "model = InferenceGPT(v, d_model=128, n_heads=4, n_layers=4).to(device)\n",
    "model.eos_token_id = None\n",
    "\n",
    "data = torch.tensor([c2i[c] for c in corpus], device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "seq_len = 64\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    total_loss, n = 0, 0\n",
    "    for i in range(0, len(data) - seq_len - 1, seq_len):\n",
    "        x = data[i:i+seq_len].unsqueeze(0)\n",
    "        y = data[i+1:i+seq_len+1].unsqueeze(0)\n",
    "        logits, _ = model(x, start_pos=0)\n",
    "        loss = F.cross_entropy(logits.view(-1, v), y.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        n += 1\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}: loss = {total_loss/n:.4f}\")\n",
    "\n",
    "engine = GenerationEngine(model)"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with the trained model using different settings\n",
    "model.eval()\n",
    "prompt_text = \"To be or not\"\n",
    "prompt_ids = torch.tensor([[c2i[c] for c in prompt_text]], device=device)\n",
    "\n",
    "settings = [\n",
    "    {\"temperature\": 0.0, \"top_p\": 1.0, \"label\": \"Greedy (T=0)\"},\n",
    "    {\"temperature\": 0.5, \"top_p\": 0.9, \"label\": \"Conservative (T=0.5, p=0.9)\"},\n",
    "    {\"temperature\": 0.8, \"top_p\": 0.9, \"label\": \"Balanced (T=0.8, p=0.9)\"},\n",
    "    {\"temperature\": 1.2, \"top_p\": 0.95, \"label\": \"Creative (T=1.2, p=0.95)\"},\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Prompt: '{prompt_text}'\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for s in settings:\n",
    "    output, stats = engine.generate(\n",
    "        prompt_ids, max_new_tokens=80,\n",
    "        temperature=s['temperature'], top_p=s['top_p']\n",
    "    )\n",
    "    text = ''.join([i2c[t.item()] for t in output[0]])\n",
    "    print(f\"\\n[{s['label']}]\")\n",
    "    print(f\"  {text}\")\n",
    "    print(f\"  Prefill: {stats['prefill_time']*1000:.1f}ms | \"\n",
    "          f\"Decode: {stats['decode_time']*1000:.1f}ms | \"\n",
    "          f\"Throughput: {stats['decode_tps']:.0f} tok/s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive performance dashboard\n",
    "gen_lengths = [25, 50, 100, 200]\n",
    "prompt_len = 32\n",
    "\n",
    "results = []\n",
    "for gl in gen_lengths:\n",
    "    prompt = torch.tensor([[c2i[c] for c in \"To be or not to be that is\"[:prompt_len]]], device=device)\n",
    "    # Pad if needed\n",
    "    if prompt.shape[1] < prompt_len:\n",
    "        pad = torch.randint(0, v, (1, prompt_len - prompt.shape[1]), device=device)\n",
    "        prompt = torch.cat([prompt, pad], dim=1)\n",
    "\n",
    "    times = []\n",
    "    for _ in range(5):\n",
    "        _, stats = engine.generate(prompt, max_new_tokens=gl)\n",
    "        times.append(stats)\n",
    "\n",
    "    avg_prefill = np.mean([t['prefill_time'] for t in times])\n",
    "    avg_decode = np.mean([t['decode_time'] for t in times])\n",
    "    avg_tps = np.mean([t['decode_tps'] for t in times])\n",
    "\n",
    "    results.append({\n",
    "        'gen_len': gl,\n",
    "        'prefill_ms': avg_prefill * 1000,\n",
    "        'decode_ms': avg_decode * 1000,\n",
    "        'total_ms': (avg_prefill + avg_decode) * 1000,\n",
    "        'tokens_per_sec': avg_tps,\n",
    "    })\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n{'Gen Length':<12} {'Prefill (ms)':<14} {'Decode (ms)':<14} {'Total (ms)':<14} {'Tok/s':<10}\")\n",
    "print(\"-\" * 64)\n",
    "for r in results:\n",
    "    print(f\"{r['gen_len']:<12} {r['prefill_ms']:<14.1f} {r['decode_ms']:<14.1f} \"\n",
    "          f\"{r['total_ms']:<14.1f} {r['tokens_per_sec']:<10.0f}\")\n",
    "\n",
    "# Performance plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "gls = [r['gen_len'] for r in results]\n",
    "ax1.plot(gls, [r['total_ms'] for r in results], 'bo-', linewidth=2, markersize=8, label='Total')\n",
    "ax1.plot(gls, [r['prefill_ms'] for r in results], 'g^--', linewidth=1.5, label='Prefill')\n",
    "ax1.plot(gls, [r['decode_ms'] for r in results], 'rs--', linewidth=1.5, label='Decode')\n",
    "ax1.set_xlabel('Generation Length (tokens)')\n",
    "ax1.set_ylabel('Latency (ms)')\n",
    "ax1.set_title('End-to-End Latency')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(gls, [r['tokens_per_sec'] for r in results], 'ko-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Generation Length (tokens)')\n",
    "ax2.set_ylabel('Tokens per Second')\n",
    "ax2.set_title('Decode Throughput')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Prefill and decode are fundamentally different**: Prefill processes many tokens in parallel (compute-bound), while decode processes one token at a time (memory-bound). In production, they are often handled separately.\n",
    "\n",
    "2. **The complete generation loop** combines KV cache management, positional encoding offsets, sampling strategy, and stopping conditions into a single pipeline.\n",
    "\n",
    "3. **Streaming generation** delivers tokens to the user as they are produced, hiding the per-token latency behind the reading speed of the human user.\n",
    "\n",
    "4. **Performance characteristics**: Decode throughput is relatively constant regardless of generation length (until KV cache memory becomes a bottleneck). Prefill latency scales roughly quadratically with prompt length due to attention.\n",
    "\n",
    "### Key Metrics for Production Systems\n",
    "\n",
    "| Metric | What It Measures | Typical Values |\n",
    "|--------|-----------------|----------------|\n",
    "| Time to First Token (TTFT) | Prefill latency | 50-500ms |\n",
    "| Inter-Token Latency (ITL) | Per-token decode time | 10-50ms |\n",
    "| Throughput (tok/s) | Decode speed | 30-150 tok/s per user |\n",
    "| KV Cache Memory | Memory per user | 0.5-4 GB |\n",
    "\n",
    "### What is Next\n",
    "\n",
    "With efficient, high-quality generation working, the next notebook tackles a different problem entirely: **how to make the model better at specific tasks without retraining it from scratch**. We will implement LoRA fine-tuning from scratch."
   ],
   "id": "cell_22"
  }
 ]
}