{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Reward Modeling \u2014 Teaching a Neural Network Human Preferences \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Modeling \u2014 Teaching a Neural Network Human Preferences\n",
    "\n",
    "**Vizuara AI**\n",
    "\n",
    "In this notebook, we will build a reward model from scratch. A reward model is the component of RLHF that learns to predict which response a human would prefer. We will implement the Bradley-Terry preference model, train it on pairwise comparisons, and visualize how it learns to rank responses.\n",
    "\n",
    "By the end of this notebook, you will have a working reward model that can score any text completion.\n",
    "\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "Let us start with a simple question: How do you teach a language model what a \"good\" response looks like?\n",
    "\n",
    "For math problems, this is easy \u2014 the answer is either right or wrong. But for open-ended tasks like \"explain gravity to a child,\" there is no single correct answer. Different humans might prefer different explanations.\n",
    "\n",
    "The key insight behind RLHF is that **humans are much better at comparing two things than scoring one thing absolutely**. Show a person two explanations and ask \"which is better?\" \u2014 they can answer immediately and consistently.\n",
    "\n",
    "A reward model takes this insight and turns it into a neural network. It learns to assign a scalar score to any response, such that preferred responses get higher scores than rejected ones. This is the foundation of the entire RLHF pipeline."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "!pip install torch transformers datasets -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Before we write any math, let us build intuition for what a reward model does.\n",
    "\n",
    "Imagine you are a teacher grading essays. You have a stack of essays and you need to rank them from best to worst. Grading each essay on a 1-10 scale is hard \u2014 is this essay a 7 or an 8? But comparing two essays and saying \"this one is better\" is much easier.\n",
    "\n",
    "A reward model works the same way. It does not directly learn absolute scores. Instead, it learns from **pairwise comparisons**: \"For this prompt, Response A is better than Response B.\"\n",
    "\n",
    "Let us create some synthetic data to see this in action."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us create synthetic preference data to build intuition\n",
    "# Imagine a simple scenario: we have text of varying \"quality\" (length + keyword presence)\n",
    "\n",
    "prompts = [\n",
    "    \"Explain machine learning simply.\",\n",
    "    \"What is reinforcement learning?\",\n",
    "    \"How do neural networks work?\",\n",
    "]\n",
    "\n",
    "# Each prompt has a preferred (w) and rejected (l) response\n",
    "preference_data = [\n",
    "    {\n",
    "        \"prompt\": \"Explain machine learning simply.\",\n",
    "        \"preferred\": \"Machine learning is when computers learn patterns from data, like how you learn to recognize faces.\",\n",
    "        \"rejected\": \"ML utilizes gradient-based optimization of parameterized function approximators.\",\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is reinforcement learning?\",\n",
    "        \"preferred\": \"RL is like training a dog \u2014 good behavior gets treats (rewards), bad behavior gets nothing.\",\n",
    "        \"rejected\": \"Reinforcement learning optimizes a policy via the Bellman optimality equation.\",\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"How do neural networks work?\",\n",
    "        \"preferred\": \"A neural network passes information through layers, like a game of telephone, refining the message at each step.\",\n",
    "        \"rejected\": \"Neural networks are differentiable computational graphs with backpropagation.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Sample preference pair:\")\n",
    "print(f\"Prompt: {preference_data[0]['prompt']}\")\n",
    "print(f\"Preferred: {preference_data[0]['preferred']}\")\n",
    "print(f\"Rejected: {preference_data[0]['rejected']}\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "Now let us formalize this with the **Bradley-Terry model**. Given two responses $y_w$ (preferred) and $y_l$ (rejected) to a prompt $x$, the probability that a human prefers $y_w$ is:\n",
    "\n",
    "$$P(y_w \\succ y_l | x) = \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l))$$\n",
    "\n",
    "Here, $r_\\theta$ is our reward model (a neural network with parameters $\\theta$) and $\\sigma$ is the sigmoid function.\n",
    "\n",
    "**Numerical example:** Suppose $r_\\theta(x, y_w) = 3.0$ and $r_\\theta(x, y_l) = 1.0$:\n",
    "\n",
    "$$P = \\sigma(3.0 - 1.0) = \\sigma(2.0) = \\frac{1}{1 + e^{-2.0}} = \\frac{1}{1.135} = 0.881$$\n",
    "\n",
    "This says the model believes there is an 88.1% chance the preferred response is indeed better. This is exactly what we want.\n",
    "\n",
    "The **loss function** for training the reward model is:\n",
    "\n",
    "$$\\mathcal{L} = -\\log \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l))$$\n",
    "\n",
    "This loss is small when the reward model ranks correctly (preferred gets a higher score) and large when it ranks incorrectly."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us visualize the Bradley-Terry loss to build intuition\n",
    "\n",
    "reward_diff = np.linspace(-5, 5, 200)\n",
    "sigmoid_vals = 1 / (1 + np.exp(-reward_diff))\n",
    "loss_vals = -np.log(sigmoid_vals + 1e-8)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sigmoid\n",
    "axes[0].plot(reward_diff, sigmoid_vals, 'b-', linewidth=2)\n",
    "axes[0].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].scatter([2], [0.881], color='green', s=100, zorder=5, label='Case 1: correct (r_w > r_l)')\n",
    "axes[0].scatter([-2], [0.119], color='red', s=100, zorder=5, label='Case 2: wrong (r_w < r_l)')\n",
    "axes[0].set_xlabel('r(y_w) - r(y_l)', fontsize=12)\n",
    "axes[0].set_ylabel('P(y_w preferred)', fontsize=12)\n",
    "axes[0].set_title('Sigmoid: Preference Probability', fontsize=13)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(reward_diff, loss_vals, 'r-', linewidth=2)\n",
    "axes[1].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].scatter([2], [-np.log(0.881)], color='green', s=100, zorder=5, label='Case 1: low loss (0.13)')\n",
    "axes[1].scatter([-2], [-np.log(0.119)], color='red', s=100, zorder=5, label='Case 2: high loss (2.13)')\n",
    "axes[1].set_xlabel('r(y_w) - r(y_l)', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].set_title('Bradley-Terry Loss', fontsize=13)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: The loss heavily penalizes incorrect rankings!\")\n",
    "print(f\"Correct ranking (diff=+2): Loss = {-np.log(0.881):.3f}\")\n",
    "print(f\"Wrong ranking (diff=-2):   Loss = {-np.log(0.119):.3f}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It \u2014 Component by Component\n",
    "\n",
    "Now let us build a reward model step by step. In real RLHF, the reward model starts from a pretrained LLM and replaces the output head with a scalar projection. We will build a simplified version that captures all the essential mechanics.\n",
    "\n",
    "### Step 1: Text Encoder\n",
    "\n",
    "First, we need a way to convert text into a fixed-size vector. We will use a simple embedding + average pooling approach."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTextEncoder(nn.Module):\n",
    "    \"\"\"A simple text encoder that converts token IDs to a fixed-size vector.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        # token_ids: (batch, seq_len)\n",
    "        embedded = self.embedding(token_ids)    # (batch, seq_len, embed_dim)\n",
    "        pooled = embedded.mean(dim=1)           # (batch, embed_dim) \u2014 average pooling\n",
    "        return self.layers(pooled)              # (batch, hidden_dim)\n",
    "\n",
    "# Test it\n",
    "encoder = SimpleTextEncoder(vocab_size=1000, embed_dim=64, hidden_dim=128).to(device)\n",
    "dummy_input = torch.randint(0, 1000, (2, 20)).to(device)\n",
    "output = encoder(dummy_input)\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Encoded shape: {output.shape}\")\n",
    "print(\"Text encoder works!\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Reward Head\n",
    "\n",
    "The reward head takes the encoded representation and projects it to a single scalar value."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Reward model: text encoder + scalar reward head.\n",
    "\n",
    "    Architecture mirrors real RLHF reward models:\n",
    "    - Shared encoder (analogous to LLM backbone)\n",
    "    - Linear projection to scalar reward\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = SimpleTextEncoder(vocab_size, embed_dim, hidden_dim)\n",
    "        self.reward_head = nn.Linear(hidden_dim, 1)  # Project to scalar\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        hidden = self.encoder(token_ids)      # (batch, hidden_dim)\n",
    "        reward = self.reward_head(hidden)      # (batch, 1)\n",
    "        return reward.squeeze(-1)              # (batch,) \u2014 scalar per example\n",
    "\n",
    "# Test it\n",
    "reward_model = RewardModel(vocab_size=1000, embed_dim=64, hidden_dim=128).to(device)\n",
    "dummy_input = torch.randint(0, 1000, (4, 20)).to(device)\n",
    "rewards = reward_model(dummy_input)\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Rewards shape: {rewards.shape}\")\n",
    "print(f\"Reward values: {rewards.detach().cpu().numpy()}\")\n",
    "print(\"\\nReward model assigns a scalar score to each input!\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Bradley-Terry Loss\n",
    "\n",
    "Now let us implement the loss function that trains the reward model on preference pairs."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bradley_terry_loss(reward_preferred, reward_rejected):\n",
    "    \"\"\"\n",
    "    Bradley-Terry preference loss.\n",
    "\n",
    "    L = -log(sigmoid(r_preferred - r_rejected))\n",
    "\n",
    "    Args:\n",
    "        reward_preferred: (batch,) rewards for preferred responses\n",
    "        reward_rejected: (batch,) rewards for rejected responses\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    return -F.logsigmoid(reward_preferred - reward_rejected).mean()\n",
    "\n",
    "# Test with our intuition from earlier\n",
    "r_pref = torch.tensor([3.0, 1.5, 4.0])\n",
    "r_rej = torch.tensor([1.0, 2.0, 0.5])\n",
    "\n",
    "loss = bradley_terry_loss(r_pref, r_rej)\n",
    "print(f\"Preferred rewards: {r_pref.numpy()}\")\n",
    "print(f\"Rejected rewards:  {r_rej.numpy()}\")\n",
    "print(f\"Differences:       {(r_pref - r_rej).numpy()}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print(\"\\nNote: Pair 2 has r_pref < r_rej (wrong ranking), increasing the loss!\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Implement a Preference Accuracy Metric\n",
    "\n",
    "The accuracy metric tells us what fraction of preference pairs the reward model ranks correctly."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preference_accuracy(reward_preferred, reward_rejected):\n",
    "    \"\"\"\n",
    "    Calculate the fraction of pairs where the reward model\n",
    "    correctly assigns a higher reward to the preferred response.\n",
    "\n",
    "    Args:\n",
    "        reward_preferred: (batch,) tensor of rewards for preferred responses\n",
    "        reward_rejected: (batch,) tensor of rewards for rejected responses\n",
    "\n",
    "    Returns:\n",
    "        Float: accuracy between 0.0 and 1.0\n",
    "\n",
    "    Hint: A pair is correct if reward_preferred > reward_rejected\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # Step 1: Compare reward_preferred to reward_rejected element-wise\n",
    "    # Step 2: Calculate the fraction that are correct\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "r_pref = torch.tensor([3.0, 1.5, 4.0, 2.0])\n",
    "r_rej = torch.tensor([1.0, 2.0, 0.5, 1.8])\n",
    "# Expected: 3 out of 4 correct = 0.75 (pair 2 is wrong)\n",
    "\n",
    "acc = preference_accuracy(r_pref, r_rej)\n",
    "print(f\"Accuracy: {acc}\")\n",
    "# Should print: Accuracy: 0.75"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Create a Synthetic Preference Dataset\n",
    "\n",
    "Create a dataset where \"quality\" is determined by a known function, so we can verify our reward model learns the right thing."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticPreferenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Synthetic dataset for testing reward model training.\n",
    "\n",
    "    Each \"text\" is a random token sequence. Quality is determined by\n",
    "    a hidden scoring function (average token value). The preferred\n",
    "    response always has higher true quality.\n",
    "\n",
    "    TODO: Implement the __getitem__ method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_pairs, seq_len, vocab_size):\n",
    "        self.num_pairs = num_pairs\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_pairs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generate a preference pair.\n",
    "\n",
    "        Returns:\n",
    "            dict with keys:\n",
    "                'preferred': tensor of token IDs (seq_len,) \u2014 higher quality\n",
    "                'rejected': tensor of token IDs (seq_len,) \u2014 lower quality\n",
    "\n",
    "        Hint:\n",
    "            1. Generate two random token sequences\n",
    "            2. Calculate a \"quality score\" for each (e.g., mean token value)\n",
    "            3. The one with higher quality becomes 'preferred'\n",
    "        \"\"\"\n",
    "        # TODO: Implement this method\n",
    "        # Step 1: Generate two random token sequences using torch.randint\n",
    "        # Step 2: Calculate quality scores (try: mean token value / vocab_size)\n",
    "        # Step 3: Assign preferred/rejected based on quality\n",
    "        pass\n",
    "\n",
    "# Test your implementation\n",
    "# dataset = SyntheticPreferenceDataset(num_pairs=1000, seq_len=20, vocab_size=1000)\n",
    "# sample = dataset[0]\n",
    "# print(f\"Preferred shape: {sample['preferred'].shape}\")\n",
    "# print(f\"Rejected shape: {sample['rejected'].shape}\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Now let us train our reward model on synthetic preference data and see if it learns to rank correctly."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic dataset with a known quality function\n",
    "class WorkingSyntheticDataset(Dataset):\n",
    "    \"\"\"Each sequence has quality = mean(token_ids) / vocab_size.\"\"\"\n",
    "\n",
    "    def __init__(self, num_pairs, seq_len, vocab_size):\n",
    "        self.data = []\n",
    "        for _ in range(num_pairs):\n",
    "            seq_a = torch.randint(0, vocab_size, (seq_len,))\n",
    "            seq_b = torch.randint(0, vocab_size, (seq_len,))\n",
    "            quality_a = seq_a.float().mean().item()\n",
    "            quality_b = seq_b.float().mean().item()\n",
    "\n",
    "            if quality_a > quality_b:\n",
    "                self.data.append({'preferred': seq_a, 'rejected': seq_b})\n",
    "            else:\n",
    "                self.data.append({'preferred': seq_b, 'rejected': seq_a})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = WorkingSyntheticDataset(num_pairs=2000, seq_len=20, vocab_size=1000)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "reward_model = RewardModel(vocab_size=1000, embed_dim=64, hidden_dim=128).to(device)\n",
    "optimizer = torch.optim.Adam(reward_model.parameters(), lr=1e-3)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} preference pairs\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in reward_model.parameters()):,}\")\n",
    "print(\"Ready to train!\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "losses = []\n",
    "accuracies = []\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    epoch_total = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        preferred = batch['preferred'].to(device)\n",
    "        rejected = batch['rejected'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        r_pref = reward_model(preferred)\n",
    "        r_rej = reward_model(rejected)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = bradley_terry_loss(r_pref, r_rej)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track metrics\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_correct += (r_pref > r_rej).sum().item()\n",
    "        epoch_total += len(preferred)\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    accuracy = epoch_correct / epoch_total\n",
    "    losses.append(avg_loss)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} \u2014 Loss: {avg_loss:.4f}, Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "print(f\"\\nFinal accuracy: {accuracies[-1]:.3f}\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(range(1, num_epochs+1), losses, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Reward Model Training Loss', fontsize=13)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(range(1, num_epochs+1), accuracies, 'g-', linewidth=2)\n",
    "axes[1].axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Random baseline')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Preference Accuracy', fontsize=12)\n",
    "axes[1].set_title('Reward Model Preference Accuracy', fontsize=13)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(0.4, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe reward model learned to rank preferred responses higher!\")\n",
    "print(\"Starting from random (50%), it achieves significantly better accuracy.\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output\n",
    "\n",
    "Let us verify that our trained reward model assigns higher scores to higher-quality sequences."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sequences of varying quality and check if reward model scores them correctly\n",
    "reward_model.eval()\n",
    "\n",
    "qualities = []\n",
    "rewards = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for target_quality in np.linspace(100, 900, 20):\n",
    "        # Generate sequence with tokens centered around target_quality\n",
    "        seq = torch.randint(\n",
    "            max(0, int(target_quality - 100)),\n",
    "            min(1000, int(target_quality + 100)),\n",
    "            (1, 20)\n",
    "        ).to(device)\n",
    "\n",
    "        reward = reward_model(seq).item()\n",
    "        true_quality = seq.float().mean().item()\n",
    "\n",
    "        qualities.append(true_quality)\n",
    "        rewards.append(reward)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(qualities, rewards, c='blue', s=50, alpha=0.8)\n",
    "plt.xlabel('True Quality (mean token value)', fontsize=12)\n",
    "plt.ylabel('Predicted Reward', fontsize=12)\n",
    "plt.title('Reward Model: True Quality vs Predicted Reward', fontsize=13)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add correlation\n",
    "correlation = np.corrcoef(qualities, rewards)[0, 1]\n",
    "plt.text(0.05, 0.95, f'Correlation: {correlation:.3f}',\n",
    "         transform=plt.gca().transAxes, fontsize=12,\n",
    "         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCorrelation between true quality and predicted reward: {correlation:.3f}\")\n",
    "print(\"A high positive correlation means the reward model learned the quality function!\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**What we built:**\n",
    "- A reward model that learns from pairwise preference comparisons\n",
    "- The Bradley-Terry loss function for training on ranked pairs\n",
    "- Visualization of how the model learns to rank correctly\n",
    "\n",
    "**Key takeaways:**\n",
    "1. Reward models learn from **comparisons**, not absolute scores\n",
    "2. The Bradley-Terry loss pushes preferred responses to get higher rewards\n",
    "3. Even a simple model can learn a latent quality function from comparisons\n",
    "\n",
    "**Think about:**\n",
    "- What happens if the preference data is noisy (humans disagree)?\n",
    "- How would you handle ties (both responses are equally good)?\n",
    "- In real RLHF, the reward model starts from a pretrained LLM \u2014 why is this important?\n",
    "\n",
    "**Next notebook:** We will use this reward model to actually improve a language model using policy gradients and PPO."
   ],
   "id": "cell_24"
  }
 ]
}