{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "End-to-End RLHF Pipeline \u2014 Sentiment Alignment with GPT-2 \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End RLHF Pipeline \u2014 Sentiment Alignment with GPT-2\n",
    "\n",
    "**Vizuara AI**\n",
    "\n",
    "In this notebook, we bring everything together. We will take a pretrained GPT-2 model and align it to generate positive-sentiment text using RLHF. This combines the reward model from Notebook 1 and the PPO algorithm from Notebook 2 into a complete, working pipeline.\n",
    "\n",
    "By the end, you will have a GPT-2 model that has been steered towards generating positive-sentiment completions through reinforcement learning from a reward signal.\n",
    "\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "This notebook is the capstone \u2014 it shows how all the pieces of RLHF fit together in practice. We will use a real GPT-2 model (not a toy), a real reward function (sentiment analysis), and a real optimization algorithm (PPO with KL penalty).\n",
    "\n",
    "The task is deliberately simple \u2014 steer text towards positive sentiment \u2014 so we can focus on understanding the mechanics rather than fighting with scale. But the exact same pipeline powers the alignment of ChatGPT, Claude, and other frontier language models. The only differences are model size, reward model complexity, and compute budget."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup \u2014 this requires a GPU for reasonable speed\n",
    "!pip install torch transformers datasets -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "The RLHF pipeline has three stages that build on each other:\n",
    "\n",
    "1. **Start with a pretrained LLM** (GPT-2) \u2014 it can generate coherent text but has no notion of \"good\" vs \"bad\"\n",
    "2. **Define a reward signal** \u2014 we use sentiment analysis as a proxy for human preference (positive = good)\n",
    "3. **Optimize with PPO + KL penalty** \u2014 nudge the model towards higher rewards while staying close to the original\n",
    "\n",
    "Think of it like training a musician. Stage 1 is learning to play notes (language ability). Stage 2 is having a music critic who scores performances (reward model). Stage 3 is the musician practicing and improving based on the critic's feedback, while keeping their own musical style (KL penalty).\n",
    "\n",
    "Let us start by loading our base model and seeing what it generates without any alignment."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 (small, 124M parameters)\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Active model (will be optimized)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Reference model (frozen \u2014 for KL penalty)\n",
    "ref_model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "ref_model.eval()\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Reference model frozen: {not any(p.requires_grad for p in ref_model.parameters())}\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some baseline completions (before RLHF)\n",
    "def generate_text(model, tokenizer, prompts, max_new_tokens=30):\n",
    "    \"\"\"Generate completions for a list of prompts.\"\"\"\n",
    "    model.eval()\n",
    "    completions = []\n",
    "    with torch.no_grad():\n",
    "        for prompt in prompts:\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_k=50,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "            completion = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            completions.append(completion)\n",
    "    return completions\n",
    "\n",
    "test_prompts = [\n",
    "    \"Today I feel\",\n",
    "    \"The best thing about\",\n",
    "    \"My favorite memory is\",\n",
    "    \"I am excited because\",\n",
    "]\n",
    "\n",
    "print(\"=== Baseline GPT-2 Completions (Before RLHF) ===\\n\")\n",
    "baseline_completions = generate_text(model, tokenizer, test_prompts)\n",
    "for prompt, completion in zip(test_prompts, baseline_completions):\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Completion: {completion}\\n\")"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "The complete RLHF objective combines three components:\n",
    "\n",
    "**1. Reward from the reward model:**\n",
    "$$r_{\\text{RM}}(x, y) = \\text{SentimentScore}(x + y)$$\n",
    "\n",
    "**2. KL penalty against the reference model:**\n",
    "$$\\text{KL}_t = \\log \\pi_\\theta(y_t | s_t) - \\log \\pi_{\\text{ref}}(y_t | s_t)$$\n",
    "\n",
    "**3. Total per-token reward:**\n",
    "$$r_t = \\begin{cases} -\\beta \\cdot \\text{KL}_t & \\text{for } t < T \\\\ r_{\\text{RM}}(x, y) - \\beta \\cdot \\text{KL}_t & \\text{for } t = T \\end{cases}$$\n",
    "\n",
    "**Numerical example:** Suppose $r_{\\text{RM}} = 2.5$, $\\beta = 0.1$, and per-token KL values are $[0.1, 0.3, 0.2, 0.5]$:\n",
    "- Total KL = $0.1 + 0.3 + 0.2 + 0.5 = 1.1$\n",
    "- KL penalty = $0.1 \\times 1.1 = 0.11$\n",
    "- Effective reward = $2.5 - 0.11 = 2.39$\n",
    "\n",
    "The KL penalty is small because the model has not diverged much from the reference. If the model started generating very different outputs (KL = 15.0), the penalty would be $0.1 \\times 15.0 = 1.5$, significantly reducing the effective reward.\n",
    "\n",
    "The PPO objective then maximizes this reward:\n",
    "$$L^{\\text{CLIP}} = \\mathbb{E}\\left[\\min\\left(r_t A_t, \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon) A_t\\right)\\right]$$\n",
    "\n",
    "\n",
    "## 4. Let's Build It \u2014 Component by Component\n",
    "\n",
    "### Step 1: Reward Function (Sentiment Scoring)\n",
    "\n",
    "We use a simple keyword-based sentiment scorer. In production RLHF, this would be a trained reward model."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple sentiment reward function\n",
    "# In real RLHF, this would be a trained reward model (see Notebook 1)\n",
    "\n",
    "POSITIVE_WORDS = {\n",
    "    'happy', 'great', 'wonderful', 'amazing', 'love', 'beautiful',\n",
    "    'excellent', 'fantastic', 'joy', 'excited', 'good', 'best',\n",
    "    'brilliant', 'awesome', 'delightful', 'cheerful', 'grateful',\n",
    "    'blessed', 'fortunate', 'incredible', 'superb', 'perfect',\n",
    "    'smile', 'laugh', 'fun', 'kind', 'warm', 'bright', 'hope',\n",
    "}\n",
    "\n",
    "NEGATIVE_WORDS = {\n",
    "    'sad', 'terrible', 'awful', 'hate', 'ugly', 'worst',\n",
    "    'horrible', 'disaster', 'pain', 'angry', 'bad', 'dead',\n",
    "    'kill', 'die', 'fear', 'sick', 'miserable', 'depressed',\n",
    "    'lonely', 'failure', 'broken', 'dark', 'cry', 'suffer',\n",
    "}\n",
    "\n",
    "def sentiment_reward(text):\n",
    "    \"\"\"\n",
    "    Simple keyword-based sentiment reward.\n",
    "    Returns a score between -2.0 and 2.0.\n",
    "    \"\"\"\n",
    "    words = text.lower().split()\n",
    "    pos_count = sum(1 for w in words if w.strip('.,!?') in POSITIVE_WORDS)\n",
    "    neg_count = sum(1 for w in words if w.strip('.,!?') in NEGATIVE_WORDS)\n",
    "\n",
    "    # Normalize by text length\n",
    "    total = max(len(words), 1)\n",
    "    score = (pos_count - neg_count) / total * 10\n",
    "    return max(-2.0, min(2.0, score))  # Clip to [-2, 2]\n",
    "\n",
    "# Test it\n",
    "test_texts = [\n",
    "    \"I am so happy and grateful for this wonderful day\",\n",
    "    \"The weather is okay nothing special\",\n",
    "    \"This is terrible and I hate everything about it\",\n",
    "]\n",
    "\n",
    "print(\"Sentiment Reward Examples:\")\n",
    "for text in test_texts:\n",
    "    score = sentiment_reward(text)\n",
    "    print(f\"  '{text}' -> Reward: {score:.2f}\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: KL Divergence Computation"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl_penalty(model, ref_model, input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    Compute per-token KL divergence between model and reference model.\n",
    "\n",
    "    KL_t = log pi_theta(y_t | s_t) - log pi_ref(y_t | s_t)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        ref_outputs = ref_model(input_ids, attention_mask=attention_mask)\n",
    "        ref_logprobs = F.log_softmax(ref_outputs.logits, dim=-1)\n",
    "\n",
    "    model_outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    model_logprobs = F.log_softmax(model_outputs.logits, dim=-1)\n",
    "\n",
    "    # Get log probs for actual tokens (shift by 1 for autoregressive)\n",
    "    # For token at position t, the prediction is from position t-1\n",
    "    token_ids = input_ids[:, 1:]  # Target tokens\n",
    "    model_token_logprobs = model_logprobs[:, :-1].gather(\n",
    "        2, token_ids.unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "    ref_token_logprobs = ref_logprobs[:, :-1].gather(\n",
    "        2, token_ids.unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "\n",
    "    # Per-token KL divergence\n",
    "    kl_per_token = model_token_logprobs - ref_token_logprobs\n",
    "\n",
    "    return kl_per_token, model_token_logprobs\n",
    "\n",
    "# Quick test\n",
    "test_input = tokenizer(\"Hello world\", return_tensors=\"pt\").to(device)\n",
    "kl, logprobs = compute_kl_penalty(model, ref_model,\n",
    "                                   test_input['input_ids'],\n",
    "                                   test_input['attention_mask'])\n",
    "print(f\"KL per token: {kl.detach().cpu().numpy()}\")\n",
    "print(\"KL is approximately 0 because model and ref are identical (no training yet)\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: RLHF Training Step\n",
    "\n",
    "Now we build the core training function that combines generation, reward computation, and PPO optimization."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rlhf_training_step(model, ref_model, tokenizer, optimizer,\n",
    "                       prompts, beta=0.1, epsilon=0.2,\n",
    "                       max_new_tokens=20, ppo_epochs=2):\n",
    "    \"\"\"\n",
    "    One step of RLHF training:\n",
    "    1. Generate completions from current policy\n",
    "    2. Score with reward function\n",
    "    3. Compute KL penalty\n",
    "    4. Optimize with PPO\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    # --- Step 1: Generate completions ---\n",
    "    model.eval()\n",
    "    all_input_ids = []\n",
    "    all_completions = []\n",
    "    prompt_lengths = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for prompt in prompts:\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            prompt_len = inputs['input_ids'].shape[1]\n",
    "\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.8,\n",
    "                top_k=50,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "            all_input_ids.append(outputs[0])\n",
    "            completion = tokenizer.decode(outputs[0][prompt_len:],\n",
    "                                         skip_special_tokens=True)\n",
    "            all_completions.append(completion)\n",
    "            prompt_lengths.append(prompt_len)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # --- Step 2: Compute rewards ---\n",
    "    rewards = []\n",
    "    for prompt, completion in zip(prompts, all_completions):\n",
    "        r = sentiment_reward(prompt + \" \" + completion)\n",
    "        rewards.append(r)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "\n",
    "    # --- Step 3: Compute old log probs and KL penalty ---\n",
    "    total_loss = torch.tensor(0.0, device=device)\n",
    "    total_reward = rewards.mean().item()\n",
    "    total_kl = 0.0\n",
    "\n",
    "    for i, (input_ids, prompt_len) in enumerate(zip(all_input_ids, prompt_lengths)):\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        kl_per_token, model_logprobs = compute_kl_penalty(\n",
    "            model, ref_model, input_ids, attention_mask\n",
    "        )\n",
    "\n",
    "        # Per-token rewards: KL penalty on all tokens, sentiment reward on last\n",
    "        completion_kl = kl_per_token[:, prompt_len-1:]\n",
    "        token_rewards = -beta * completion_kl\n",
    "        if completion_kl.shape[1] > 0:\n",
    "            token_rewards[:, -1] += rewards[i]\n",
    "\n",
    "        # Simple advantage: reward - mean\n",
    "        advantages = token_rewards - token_rewards.mean()\n",
    "\n",
    "        # Policy gradient loss (simplified REINFORCE with KL-adjusted rewards)\n",
    "        completion_logprobs = model_logprobs[:, prompt_len-1:]\n",
    "        pg_loss = -(completion_logprobs * advantages.detach()).mean()\n",
    "\n",
    "        total_loss = total_loss + pg_loss\n",
    "        total_kl += completion_kl.abs().mean().item()\n",
    "\n",
    "    total_loss = total_loss / len(prompts)\n",
    "\n",
    "    # --- Step 4: Optimize ---\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss.item(),\n",
    "        'mean_reward': total_reward,\n",
    "        'mean_kl': total_kl / len(prompts),\n",
    "        'completions': list(zip(prompts, all_completions)),\n",
    "    }"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Implement Reward Logging and Visualization\n",
    "\n",
    "Track rewards over training and create a live-updating plot."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingLogger:\n",
    "    \"\"\"\n",
    "    Log training metrics and create visualizations.\n",
    "\n",
    "    TODO: Implement the plot method.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.rewards = []\n",
    "        self.kl_values = []\n",
    "        self.losses = []\n",
    "\n",
    "    def log(self, metrics):\n",
    "        \"\"\"Log a training step's metrics.\"\"\"\n",
    "        self.rewards.append(metrics['mean_reward'])\n",
    "        self.kl_values.append(metrics['mean_kl'])\n",
    "        self.losses.append(metrics['loss'])\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Create a 3-panel figure showing:\n",
    "        1. Mean reward over training steps\n",
    "        2. KL divergence over training steps\n",
    "        3. Loss over training steps\n",
    "\n",
    "        TODO: Implement this method\n",
    "        Hint:\n",
    "            - Use plt.subplots(1, 3, figsize=(18, 4))\n",
    "            - Plot self.rewards, self.kl_values, self.losses\n",
    "            - Add labels, titles, and grid\n",
    "        \"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "\n",
    "# logger = TrainingLogger()"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement Adaptive KL Penalty\n",
    "\n",
    "In production RLHF, beta is adjusted dynamically to keep KL within a target range."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_kl_controller(beta, kl_value, kl_target=0.5):\n",
    "    \"\"\"\n",
    "    Adjust beta to keep KL divergence near the target.\n",
    "\n",
    "    If KL > 1.5 * target: increase beta (more penalty)\n",
    "    If KL < target / 1.5: decrease beta (less penalty)\n",
    "\n",
    "    Args:\n",
    "        beta: current KL penalty coefficient\n",
    "        kl_value: observed KL divergence\n",
    "        kl_target: desired KL divergence\n",
    "\n",
    "    Returns:\n",
    "        Updated beta value\n",
    "\n",
    "    TODO: Implement this function\n",
    "    Hint:\n",
    "        - If kl_value > 1.5 * kl_target, multiply beta by 1.5\n",
    "        - If kl_value < kl_target / 1.5, multiply beta by (1 / 1.5)\n",
    "        - Otherwise keep beta the same\n",
    "        - Clip beta to [0.01, 10.0] range\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    pass\n",
    "\n",
    "# Test (uncomment after implementing):\n",
    "# print(adaptive_kl_controller(0.1, 1.0, 0.5))  # Should increase beta\n",
    "# print(adaptive_kl_controller(0.1, 0.2, 0.5))  # Should decrease beta\n",
    "# print(adaptive_kl_controller(0.1, 0.5, 0.5))  # Should stay same"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us run the full RLHF training loop."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training prompts \u2014 we cycle through these during training\n",
    "training_prompts = [\n",
    "    \"Today I feel\", \"The best part of\", \"I really enjoy\",\n",
    "    \"My favorite thing is\", \"I am grateful for\", \"What makes me happy is\",\n",
    "    \"The world is\", \"People are\", \"Life is\",\n",
    "    \"I believe that\", \"The future looks\", \"Every day I\",\n",
    "    \"My friends always\", \"I love when\", \"The sun makes me\",\n",
    "    \"Music makes me feel\", \"Nature is so\", \"Kindness is\",\n",
    "]\n",
    "\n",
    "# Setup\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "beta = 0.1\n",
    "num_steps = 40\n",
    "batch_size = 4\n",
    "\n",
    "all_metrics = {'rewards': [], 'kl': [], 'losses': []}\n",
    "\n",
    "print(\"Starting RLHF training...\")\n",
    "print(f\"Steps: {num_steps}, Batch size: {batch_size}, Beta: {beta}\\n\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(num_steps):\n",
    "    # Sample random prompts for this batch\n",
    "    batch_prompts = [training_prompts[np.random.randint(len(training_prompts))]\n",
    "                     for _ in range(batch_size)]\n",
    "\n",
    "    # Run RLHF training step\n",
    "    metrics = rlhf_training_step(\n",
    "        model, ref_model, tokenizer, optimizer,\n",
    "        batch_prompts, beta=beta, max_new_tokens=20,\n",
    "    )\n",
    "\n",
    "    all_metrics['rewards'].append(metrics['mean_reward'])\n",
    "    all_metrics['kl'].append(metrics['mean_kl'])\n",
    "    all_metrics['losses'].append(metrics['loss'])\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(f\"Step {step+1}/{num_steps} \u2014 \"\n",
    "              f\"Reward: {metrics['mean_reward']:.3f}, \"\n",
    "              f\"KL: {metrics['mean_kl']:.4f}, \"\n",
    "              f\"Loss: {metrics['loss']:.4f}\")\n",
    "        # Show a sample completion\n",
    "        if metrics['completions']:\n",
    "            p, c = metrics['completions'][0]\n",
    "            print(f\"  Sample: '{p}' -> '{c}'\\n\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "# Rewards\n",
    "window = 5\n",
    "smoothed_rewards = [np.mean(all_metrics['rewards'][max(0,i-window):i+1])\n",
    "                    for i in range(len(all_metrics['rewards']))]\n",
    "axes[0].plot(smoothed_rewards, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Step', fontsize=12)\n",
    "axes[0].set_ylabel('Mean Reward', fontsize=12)\n",
    "axes[0].set_title('Sentiment Reward During Training', fontsize=13)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# KL divergence\n",
    "axes[1].plot(all_metrics['kl'], 'r-', linewidth=2)\n",
    "axes[1].set_xlabel('Step', fontsize=12)\n",
    "axes[1].set_ylabel('KL Divergence', fontsize=12)\n",
    "axes[1].set_title('KL Divergence from Reference', fontsize=13)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[2].plot(all_metrics['losses'], 'g-', linewidth=2)\n",
    "axes[2].set_xlabel('Step', fontsize=12)\n",
    "axes[2].set_ylabel('Loss', fontsize=12)\n",
    "axes[2].set_title('Policy Gradient Loss', fontsize=13)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output\n",
    "\n",
    "Let us compare the model's outputs before and after RLHF training."
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate aligned completions (after RLHF)\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARISON: Before vs After RLHF\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_prompts = [\n",
    "    \"Today I feel\",\n",
    "    \"The best thing about\",\n",
    "    \"My favorite memory is\",\n",
    "    \"I am excited because\",\n",
    "]\n",
    "\n",
    "# Generate with aligned model\n",
    "aligned_completions = generate_text(model, tokenizer, test_prompts)\n",
    "\n",
    "# Generate with reference (original) model\n",
    "ref_completions = generate_text(ref_model, tokenizer, test_prompts)\n",
    "\n",
    "for prompt, ref_comp, aligned_comp in zip(test_prompts, ref_completions, aligned_completions):\n",
    "    ref_reward = sentiment_reward(ref_comp)\n",
    "    aligned_reward = sentiment_reward(aligned_comp)\n",
    "\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(f\"  Before RLHF (reward={ref_reward:.2f}): {ref_comp}\")\n",
    "    print(f\"  After RLHF  (reward={aligned_reward:.2f}): {aligned_comp}\")\n",
    "    print()\n",
    "\n",
    "# Overall statistics\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "n_samples = 50\n",
    "ref_rewards = []\n",
    "aligned_rewards = []\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    prompt = training_prompts[np.random.randint(len(training_prompts))]\n",
    "    ref_comp = generate_text(ref_model, tokenizer, [prompt])[0]\n",
    "    aligned_comp = generate_text(model, tokenizer, [prompt])[0]\n",
    "    ref_rewards.append(sentiment_reward(ref_comp))\n",
    "    aligned_rewards.append(sentiment_reward(aligned_comp))\n",
    "\n",
    "print(f\"\\nStatistics over {n_samples} samples:\")\n",
    "print(f\"  Reference model mean reward: {np.mean(ref_rewards):.3f}\")\n",
    "print(f\"  Aligned model mean reward:   {np.mean(aligned_rewards):.3f}\")\n",
    "print(f\"  Improvement: {np.mean(aligned_rewards) - np.mean(ref_rewards):.3f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(ref_rewards, bins=15, alpha=0.5, label='Before RLHF', color='red')\n",
    "plt.hist(aligned_rewards, bins=15, alpha=0.5, label='After RLHF', color='blue')\n",
    "plt.xlabel('Sentiment Reward', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.title('Reward Distribution: Before vs After RLHF', fontsize=13)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe aligned model generates consistently more positive text!\")\n",
    "print(\"This is exactly what we want.\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**What we built:**\n",
    "- A complete RLHF pipeline: generation, reward scoring, KL penalty, policy gradient optimization\n",
    "- Applied it to a real GPT-2 model with a sentiment reward signal\n",
    "- Demonstrated measurable improvement in alignment\n",
    "\n",
    "**Key takeaways:**\n",
    "1. RLHF combines reward signals with KL-penalized policy gradients\n",
    "2. The KL penalty is critical \u2014 without it, the model would collapse to reward-hacking\n",
    "3. Even simple reward functions can meaningfully steer model behavior\n",
    "4. The same pipeline scales to frontier models \u2014 only the scale changes\n",
    "\n",
    "**Think about:**\n",
    "- What happens if you increase beta too much? (Hint: the model barely changes)\n",
    "- What happens if you set beta to 0? (Hint: reward hacking)\n",
    "- How would you replace the keyword-based reward with a trained reward model?\n",
    "- How does this scale to models with billions of parameters?\n",
    "\n",
    "**Congratulations!** You have built a complete RLHF pipeline from scratch. The same principles \u2014 reward modeling, policy optimization, KL regularization \u2014 power the alignment of every major language model today."
   ],
   "id": "cell_23"
  }
 ]
}