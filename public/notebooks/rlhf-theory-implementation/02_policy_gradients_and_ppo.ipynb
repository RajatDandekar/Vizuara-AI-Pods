{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Policy Gradients and PPO for Language Models \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients and PPO for Language Models\n",
    "\n",
    "**Vizuara AI**\n",
    "\n",
    "In this notebook, we will build the optimization engine of RLHF \u2014 the algorithm that uses reward signals to improve a language model. We will implement vanilla policy gradients from scratch, understand why they are unstable, and then build PPO (Proximal Policy Optimization) step by step.\n",
    "\n",
    "By the end, you will have a working PPO implementation that can optimize any policy given a reward signal.\n",
    "\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "In Notebook 1, we built a reward model that can score any text completion. But scoring alone does not improve the model \u2014 we need an algorithm that takes these scores and adjusts the model's behavior accordingly.\n",
    "\n",
    "This is where policy gradient methods come in. The idea is elegant: if an action (token) led to a high reward, increase its probability. If it led to a low reward, decrease its probability. Do this over many iterations, and the model gradually learns to produce better outputs.\n",
    "\n",
    "But there is a catch \u2014 vanilla policy gradients are noisy and can cause catastrophic policy updates. PPO solves this with a brilliantly simple trick: **clip the updates so the policy cannot change too much in one step**. This single idea made RLHF practical for training models like ChatGPT."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "!pip install torch numpy matplotlib -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Let us build intuition with a simple analogy. Imagine you are learning to play darts.\n",
    "\n",
    "- **Policy** = your throwing technique (angle, force, release point)\n",
    "- **Action** = each individual throw\n",
    "- **Reward** = points scored based on where the dart lands\n",
    "\n",
    "After each round, you reflect: \"When I threw with more arc, I hit closer to the bullseye. When I threw flat, I missed.\" You then adjust your technique to favor the higher-arc throws.\n",
    "\n",
    "Policy gradients work exactly the same way. The model generates text (throws darts), receives a reward score, and adjusts its parameters to favor the sequences that scored well.\n",
    "\n",
    "Let us build a simple environment to see this in action."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple \"bandit\" environment to build intuition\n",
    "# The agent must learn to pick the action with highest reward\n",
    "\n",
    "class SimpleBandit:\n",
    "    \"\"\"\n",
    "    Multi-armed bandit: 5 actions with different mean rewards.\n",
    "    The agent must learn which action gives the best reward.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # True mean rewards (agent does not know these)\n",
    "        self.true_rewards = torch.tensor([0.2, 0.5, 0.8, 0.3, 0.1])\n",
    "\n",
    "    def step(self, action):\n",
    "        # Reward = true mean + noise\n",
    "        reward = self.true_rewards[action] + 0.1 * torch.randn(1).item()\n",
    "        return reward\n",
    "\n",
    "# The policy is a simple softmax over action logits\n",
    "class BanditPolicy(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "        self.logits = nn.Parameter(torch.zeros(n_actions))\n",
    "\n",
    "    def forward(self):\n",
    "        return F.softmax(self.logits, dim=0)\n",
    "\n",
    "    def sample_action(self):\n",
    "        probs = self.forward()\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        return action, probs[action]\n",
    "\n",
    "env = SimpleBandit()\n",
    "policy = BanditPolicy(5)\n",
    "\n",
    "print(\"True reward means:\", env.true_rewards.numpy())\n",
    "print(\"Initial action probabilities:\", policy().detach().numpy())\n",
    "print(\"\\nThe best action is action 2 (reward = 0.8)\")\n",
    "print(\"The policy starts uniform \u2014 it does not know which action is best.\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "The policy gradient theorem gives us the gradient of the expected reward:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}\\left[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot A(s, a)\\right]$$\n",
    "\n",
    "Where:\n",
    "- $\\pi_\\theta(a|s)$ is the policy probability of action $a$ in state $s$\n",
    "- $A(s, a)$ is the **advantage** \u2014 how much better this action is compared to average\n",
    "- $\\nabla_\\theta \\log \\pi_\\theta(a|s)$ points in the direction that increases $\\pi_\\theta(a|s)$\n",
    "\n",
    "**Numerical example:** Suppose $\\pi_\\theta(\\text{action}_2) = 0.3$ and the advantage $A = 2.0$:\n",
    "\n",
    "$$\\nabla_\\theta \\log(0.3) \\cdot 2.0$$\n",
    "\n",
    "The gradient $\\nabla_\\theta \\log(0.3)$ points in the direction that increases the probability of action 2. Multiplying by $A = 2.0$ (positive \u2014 good action) means we step in that direction, increasing the probability of this action.\n",
    "\n",
    "If instead $A = -1.0$ (bad action), we step in the opposite direction, decreasing its probability. This is exactly what we want.\n",
    "\n",
    "**The PPO clipping trick:**\n",
    "\n",
    "Vanilla policy gradients can cause wild updates. PPO fixes this:\n",
    "\n",
    "$$L^{\\text{CLIP}} = \\mathbb{E}\\left[\\min\\left(r_t A_t, \\; \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon) A_t\\right)\\right]$$\n",
    "\n",
    "Where $r_t = \\pi_\\theta(a_t|s_t) / \\pi_{\\theta_\\text{old}}(a_t|s_t)$ is the ratio between new and old policy.\n",
    "\n",
    "**Numerical example:** With $\\epsilon = 0.2$, $r_t = 1.5$, $A_t = 2.0$:\n",
    "- Unclipped: $1.5 \\times 2.0 = 3.0$\n",
    "- Clipped: $\\text{clip}(1.5, 0.8, 1.2) \\times 2.0 = 1.2 \\times 2.0 = 2.4$\n",
    "- PPO objective: $\\min(3.0, 2.4) = 2.4$\n",
    "\n",
    "The clipping reduces the gradient, preventing an overly aggressive update."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PPO clipping\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "epsilon = 0.2\n",
    "ratio = np.linspace(0.0, 2.5, 500)\n",
    "\n",
    "# Positive advantage\n",
    "A_pos = 1.0\n",
    "unclipped_pos = ratio * A_pos\n",
    "clipped_pos = np.clip(ratio, 1 - epsilon, 1 + epsilon) * A_pos\n",
    "ppo_pos = np.minimum(unclipped_pos, clipped_pos)\n",
    "\n",
    "axes[0].plot(ratio, unclipped_pos, 'b--', alpha=0.5, label='Unclipped')\n",
    "axes[0].plot(ratio, ppo_pos, 'b-', linewidth=2.5, label='PPO (clipped)')\n",
    "axes[0].axvline(x=1.0, color='gray', linestyle=':', alpha=0.5)\n",
    "axes[0].fill_between(ratio, ppo_pos, unclipped_pos,\n",
    "                     where=(unclipped_pos > ppo_pos), alpha=0.15, color='red',\n",
    "                     label='Clipped away')\n",
    "axes[0].set_xlabel('Policy Ratio r(\u03b8)', fontsize=12)\n",
    "axes[0].set_ylabel('Objective', fontsize=12)\n",
    "axes[0].set_title('Positive Advantage (A > 0)', fontsize=13)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Negative advantage\n",
    "A_neg = -1.0\n",
    "unclipped_neg = ratio * A_neg\n",
    "clipped_neg = np.clip(ratio, 1 - epsilon, 1 + epsilon) * A_neg\n",
    "ppo_neg = np.minimum(unclipped_neg, clipped_neg)\n",
    "\n",
    "axes[1].plot(ratio, unclipped_neg, 'r--', alpha=0.5, label='Unclipped')\n",
    "axes[1].plot(ratio, ppo_neg, 'r-', linewidth=2.5, label='PPO (clipped)')\n",
    "axes[1].axvline(x=1.0, color='gray', linestyle=':', alpha=0.5)\n",
    "axes[1].fill_between(ratio, ppo_neg, unclipped_neg,\n",
    "                     where=(unclipped_neg > ppo_neg), alpha=0.15, color='blue',\n",
    "                     label='Clipped away')\n",
    "axes[1].set_xlabel('Policy Ratio r(\u03b8)', fontsize=12)\n",
    "axes[1].set_ylabel('Objective', fontsize=12)\n",
    "axes[1].set_title('Negative Advantage (A < 0)', fontsize=13)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"PPO prevents the policy from changing too much in either direction!\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It \u2014 Component by Component\n",
    "\n",
    "### Step 1: REINFORCE (Vanilla Policy Gradient)\n",
    "\n",
    "Let us first implement the simplest policy gradient algorithm \u2014 REINFORCE \u2014 on our bandit problem."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REINFORCE on the bandit problem\n",
    "policy = BanditPolicy(5).to(device)\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=0.05)\n",
    "env = SimpleBandit()\n",
    "\n",
    "rewards_history = []\n",
    "probs_history = []\n",
    "\n",
    "for episode in range(500):\n",
    "    # Sample action from policy\n",
    "    action, prob = policy.sample_action()\n",
    "\n",
    "    # Get reward from environment\n",
    "    reward = env.step(action)\n",
    "\n",
    "    # REINFORCE loss: -log(pi(a)) * R\n",
    "    loss = -torch.log(prob) * reward\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    rewards_history.append(reward)\n",
    "    probs_history.append(policy().detach().cpu().numpy().copy())\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Smoothed rewards\n",
    "window = 50\n",
    "smoothed = [np.mean(rewards_history[max(0,i-window):i+1]) for i in range(len(rewards_history))]\n",
    "axes[0].plot(smoothed, 'b-', linewidth=1.5)\n",
    "axes[0].axhline(y=0.8, color='green', linestyle='--', label='Optimal reward (0.8)')\n",
    "axes[0].set_xlabel('Episode', fontsize=12)\n",
    "axes[0].set_ylabel('Reward (smoothed)', fontsize=12)\n",
    "axes[0].set_title('REINFORCE Training Rewards', fontsize=13)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Action probabilities over time\n",
    "probs_array = np.array(probs_history)\n",
    "for i in range(5):\n",
    "    axes[1].plot(probs_array[:, i], label=f'Action {i} (r={env.true_rewards[i]:.1f})')\n",
    "axes[1].set_xlabel('Episode', fontsize=12)\n",
    "axes[1].set_ylabel('Probability', fontsize=12)\n",
    "axes[1].set_title('Action Probabilities During Training', fontsize=13)\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal action probabilities: {policy().detach().cpu().numpy()}\")\n",
    "print(f\"Best action (action 2) probability: {policy().detach().cpu()[2]:.3f}\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: PPO Implementation\n",
    "\n",
    "Now let us implement PPO. The key difference is that we collect a batch of experiences, then do **multiple optimization steps** on the same batch, using the clipped objective to prevent wild updates."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_loss(log_probs_new, log_probs_old, advantages, epsilon=0.2):\n",
    "    \"\"\"\n",
    "    PPO clipped surrogate loss.\n",
    "\n",
    "    Args:\n",
    "        log_probs_new: log pi_theta(a|s) under current policy\n",
    "        log_probs_old: log pi_theta_old(a|s) under old policy (detached)\n",
    "        advantages: A(s, a) advantage estimates\n",
    "        epsilon: clipping parameter (default 0.2)\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss (negated for gradient ascent)\n",
    "    \"\"\"\n",
    "    # Compute importance sampling ratio\n",
    "    ratio = torch.exp(log_probs_new - log_probs_old)\n",
    "\n",
    "    # Unclipped objective\n",
    "    unclipped = ratio * advantages\n",
    "\n",
    "    # Clipped objective\n",
    "    clipped = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages\n",
    "\n",
    "    # PPO takes the minimum (pessimistic bound)\n",
    "    loss = -torch.min(unclipped, clipped).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Test it\n",
    "log_new = torch.tensor([-0.5, -1.0, -0.3])\n",
    "log_old = torch.tensor([-0.7, -0.8, -0.5])\n",
    "adv = torch.tensor([2.0, -1.0, 0.5])\n",
    "\n",
    "loss = ppo_loss(log_new, log_old, adv)\n",
    "ratios = torch.exp(log_new - log_old)\n",
    "print(f\"Ratios: {ratios.numpy()}\")\n",
    "print(f\"Advantages: {adv.numpy()}\")\n",
    "print(f\"PPO Loss: {loss.item():.4f}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Implement GAE (Generalized Advantage Estimation)\n",
    "\n",
    "In real RLHF, advantages are computed using GAE, which balances bias and variance."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, gamma=0.99, lam=0.95):\n",
    "    \"\"\"\n",
    "    Compute Generalized Advantage Estimation.\n",
    "\n",
    "    GAE computes advantages as an exponentially weighted average of\n",
    "    TD residuals: delta_t = r_t + gamma * V(s_{t+1}) - V(s_t)\n",
    "\n",
    "    A_t = sum_{l=0}^{inf} (gamma * lambda)^l * delta_{t+l}\n",
    "\n",
    "    Args:\n",
    "        rewards: list of rewards [r_0, r_1, ..., r_T]\n",
    "        values: list of value estimates [V(s_0), V(s_1), ..., V(s_T), V(s_{T+1})]\n",
    "        gamma: discount factor\n",
    "        lam: GAE lambda parameter\n",
    "\n",
    "    Returns:\n",
    "        advantages: list of advantage estimates [A_0, A_1, ..., A_T]\n",
    "\n",
    "    Hint:\n",
    "        1. Compute TD residuals: delta_t = r_t + gamma * V(s_{t+1}) - V(s_t)\n",
    "        2. Work backwards from t=T to t=0\n",
    "        3. A_t = delta_t + gamma * lambda * A_{t+1}\n",
    "    \"\"\"\n",
    "    # TODO: Implement GAE\n",
    "    # Step 1: Initialize advantages list and set A_{T+1} = 0\n",
    "    # Step 2: Loop backwards from T to 0\n",
    "    # Step 3: Compute delta_t = rewards[t] + gamma * values[t+1] - values[t]\n",
    "    # Step 4: Compute A_t = delta_t + gamma * lam * A_{t+1}\n",
    "    pass\n",
    "\n",
    "# Test (uncomment after implementing):\n",
    "# rewards = [1.0, 0.5, 2.0, 0.0, 1.0]\n",
    "# values = [0.5, 0.6, 0.4, 0.8, 0.3, 0.0]  # includes V(s_{T+1}) = 0\n",
    "# advantages = compute_gae(rewards, values)\n",
    "# print(f\"Advantages: {advantages}\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Add Value Function Loss to PPO\n",
    "\n",
    "PPO typically also trains a value function alongside the policy. Implement the combined loss."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_combined_loss(log_probs_new, log_probs_old, advantages,\n",
    "                      values, returns, epsilon=0.2, vf_coeff=0.5):\n",
    "    \"\"\"\n",
    "    Combined PPO loss = Policy loss + Value function loss.\n",
    "\n",
    "    Args:\n",
    "        log_probs_new: current policy log probs\n",
    "        log_probs_old: old policy log probs\n",
    "        advantages: GAE advantages\n",
    "        values: value function predictions V(s)\n",
    "        returns: discounted returns (targets for value function)\n",
    "        epsilon: clipping parameter\n",
    "        vf_coeff: coefficient for value function loss\n",
    "\n",
    "    Returns:\n",
    "        total_loss, policy_loss, value_loss\n",
    "\n",
    "    TODO: Implement this function\n",
    "    Hint:\n",
    "        1. Compute PPO clipped policy loss (reuse ppo_loss function)\n",
    "        2. Compute value loss as MSE: (values - returns)^2\n",
    "        3. Combine: total = policy_loss + vf_coeff * value_loss\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    pass\n",
    "\n",
    "# Test (uncomment after implementing):\n",
    "# total, pol, val = ppo_combined_loss(log_new, log_old, adv,\n",
    "#                                      torch.tensor([1.0, 2.0, 0.5]),\n",
    "#                                      torch.tensor([1.5, 1.8, 0.7]))\n",
    "# print(f\"Total: {total:.4f}, Policy: {pol:.4f}, Value: {val:.4f}\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us now train PPO on a slightly more complex problem \u2014 a contextual bandit where the optimal action depends on the state."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualBandit:\n",
    "    \"\"\"\n",
    "    Contextual bandit: the best action depends on the state (context).\n",
    "    State is a 4-dim vector. Optimal action = argmax(state * weights).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_actions=5, state_dim=4):\n",
    "        self.n_actions = n_actions\n",
    "        self.state_dim = state_dim\n",
    "        self.weights = torch.randn(state_dim, n_actions) * 0.5\n",
    "\n",
    "    def get_state(self):\n",
    "        return torch.randn(self.state_dim)\n",
    "\n",
    "    def step(self, state, action):\n",
    "        true_values = state @ self.weights\n",
    "        reward = true_values[action].item() + 0.1 * np.random.randn()\n",
    "        return reward\n",
    "\n",
    "class PPOPolicy(nn.Module):\n",
    "    def __init__(self, state_dim, n_actions, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        # Actor (policy)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_actions),\n",
    "        )\n",
    "        # Critic (value function)\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        logits = self.actor(state)\n",
    "        value = self.critic(state).squeeze(-1)\n",
    "        return logits, value\n",
    "\n",
    "    def get_action(self, state):\n",
    "        logits, value = self.forward(state)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        log_prob = F.log_softmax(logits, dim=-1)[action]\n",
    "        return action, log_prob, value\n",
    "\n",
    "# Initialize\n",
    "env = ContextualBandit(n_actions=5, state_dim=4)\n",
    "policy = PPOPolicy(state_dim=4, n_actions=5).to(device)\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=3e-4)\n",
    "\n",
    "print(f\"Policy parameters: {sum(p.numel() for p in policy.parameters()):,}\")\n",
    "print(\"Ready for PPO training!\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Training Loop\n",
    "GAMMA = 0.99\n",
    "EPSILON = 0.2\n",
    "PPO_EPOCHS = 4\n",
    "BATCH_SIZE = 64\n",
    "NUM_ITERATIONS = 200\n",
    "\n",
    "all_rewards = []\n",
    "\n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "    # --- Collect batch of experiences ---\n",
    "    states, actions, rewards, log_probs_old, values = [], [], [], [], []\n",
    "\n",
    "    for _ in range(BATCH_SIZE):\n",
    "        state = env.get_state().to(device)\n",
    "        action, log_prob, value = policy.get_action(state)\n",
    "        reward = env.step(state.cpu(), action)\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        log_probs_old.append(log_prob.detach())\n",
    "        values.append(value.detach())\n",
    "\n",
    "    # Convert to tensors\n",
    "    states_t = torch.stack(states)\n",
    "    actions_t = torch.tensor(actions, device=device)\n",
    "    rewards_t = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "    log_probs_old_t = torch.stack(log_probs_old)\n",
    "    values_t = torch.stack(values)\n",
    "\n",
    "    # Compute advantages (simple: reward - value baseline)\n",
    "    advantages = rewards_t - values_t\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    # --- PPO update (multiple epochs on same batch) ---\n",
    "    for ppo_epoch in range(PPO_EPOCHS):\n",
    "        logits, new_values = policy(states_t)\n",
    "        log_probs_all = F.log_softmax(logits, dim=-1)\n",
    "        log_probs_new = log_probs_all.gather(1, actions_t.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # PPO clipped policy loss\n",
    "        ratio = torch.exp(log_probs_new - log_probs_old_t)\n",
    "        unclipped = ratio * advantages\n",
    "        clipped = torch.clamp(ratio, 1 - EPSILON, 1 + EPSILON) * advantages\n",
    "        policy_loss = -torch.min(unclipped, clipped).mean()\n",
    "\n",
    "        # Value function loss\n",
    "        value_loss = F.mse_loss(new_values, rewards_t)\n",
    "\n",
    "        # Combined loss\n",
    "        loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    all_rewards.append(np.mean(rewards))\n",
    "\n",
    "    if (iteration + 1) % 50 == 0:\n",
    "        recent = np.mean(all_rewards[-50:])\n",
    "        print(f\"Iteration {iteration+1}/{NUM_ITERATIONS} \u2014 \"\n",
    "              f\"Mean Reward: {recent:.3f}\")\n",
    "\n",
    "# Plot training curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "window = 20\n",
    "smoothed = [np.mean(all_rewards[max(0,i-window):i+1]) for i in range(len(all_rewards))]\n",
    "plt.plot(smoothed, 'b-', linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Mean Reward (smoothed)', fontsize=12)\n",
    "plt.title('PPO Training on Contextual Bandit', fontsize=13)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal mean reward: {np.mean(all_rewards[-20:]):.3f}\")\n",
    "print(\"PPO successfully learns to pick the best action for each context!\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output\n",
    "\n",
    "Let us compare the behavior of the trained policy with a random baseline to see how much PPO improved."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare trained policy vs random policy\n",
    "policy.eval()\n",
    "n_test = 200\n",
    "\n",
    "trained_rewards = []\n",
    "random_rewards = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_test):\n",
    "        state = env.get_state().to(device)\n",
    "\n",
    "        # Trained policy\n",
    "        logits, _ = policy(state)\n",
    "        action = logits.argmax().item()\n",
    "        trained_reward = env.step(state.cpu(), action)\n",
    "        trained_rewards.append(trained_reward)\n",
    "\n",
    "        # Random policy\n",
    "        random_action = np.random.randint(0, 5)\n",
    "        random_reward = env.step(state.cpu(), random_action)\n",
    "        random_rewards.append(random_reward)\n",
    "\n",
    "# Optimal (cheating \u2014 picking best action with known weights)\n",
    "optimal_rewards = []\n",
    "for _ in range(n_test):\n",
    "    state = env.get_state()\n",
    "    true_values = state @ env.weights\n",
    "    optimal_action = true_values.argmax().item()\n",
    "    optimal_reward = env.step(state, optimal_action)\n",
    "    optimal_rewards.append(optimal_reward)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "data = [random_rewards, trained_rewards, optimal_rewards]\n",
    "bp = plt.boxplot(data, labels=['Random', 'PPO-Trained', 'Optimal'], patch_artist=True)\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "plt.ylabel('Reward', fontsize=12)\n",
    "plt.title('Policy Comparison: Random vs PPO vs Optimal', fontsize=13)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Random mean reward:  {np.mean(random_rewards):.3f}\")\n",
    "print(f\"PPO mean reward:     {np.mean(trained_rewards):.3f}\")\n",
    "print(f\"Optimal mean reward: {np.mean(optimal_rewards):.3f}\")\n",
    "improvement = (np.mean(trained_rewards) - np.mean(random_rewards)) / (np.mean(optimal_rewards) - np.mean(random_rewards)) * 100\n",
    "print(f\"\\nPPO closes {improvement:.0f}% of the gap between random and optimal!\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**What we built:**\n",
    "- Vanilla REINFORCE algorithm from scratch\n",
    "- PPO with clipped surrogate objective\n",
    "- Actor-Critic architecture with separate policy and value heads\n",
    "- Training on a contextual bandit environment\n",
    "\n",
    "**Key takeaways:**\n",
    "1. Policy gradients increase the probability of actions that led to high rewards\n",
    "2. Vanilla REINFORCE is simple but high-variance\n",
    "3. PPO clips the policy ratio to prevent catastrophic updates\n",
    "4. Multiple epochs on the same batch make PPO sample-efficient\n",
    "\n",
    "**Think about:**\n",
    "- Why does PPO use multiple epochs on the same batch instead of collecting fresh data each time?\n",
    "- What would happen if epsilon were set to 0 (no clipping at all)?\n",
    "- How does PPO scale to language models where the \"action space\" is the entire vocabulary?\n",
    "\n",
    "**Next notebook:** We will combine the reward model from Notebook 1 with the PPO algorithm from this notebook to build a complete RLHF pipeline that aligns a language model."
   ],
   "id": "cell_21"
  }
 ]
}