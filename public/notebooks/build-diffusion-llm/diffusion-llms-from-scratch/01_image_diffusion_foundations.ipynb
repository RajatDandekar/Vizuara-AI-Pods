{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Image Diffusion Foundations ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1VimSva8YFMP_2-RyAjHL8KfiPhTmlA-o\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/01_01_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Image Diffusion Foundations: From Noise to Pictures\n",
    "\n",
    "*Part 1 of the Vizuara series on Diffusion LLMs from Scratch*\n",
    "*Estimated time: 30 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ü§ñ AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** ‚Äî it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[üëâ Open AI Teaching Assistant](https://course-creator-brown.vercel.app/courses/diffusion-llms-from-scratch/practice/1/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_02_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_02_why_it_matters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Imagine you are writing an essay on a typewriter. You type one letter at a time, left to right. Once a letter is pressed onto the page, it is permanent ‚Äî you cannot go back and change it. If you realize halfway through a sentence that the beginning was wrong, too bad.\n",
    "\n",
    "This is exactly how modern language models like GPT-4 and LLaMA work. They generate text **one token at a time**, from left to right. Each token depends on all the previous tokens, but has no knowledge of what will come after it.\n",
    "\n",
    "Now think of how an **artist** works. An artist does not paint the top-left pixel first, then the next pixel. Instead, they start with a rough sketch of the whole canvas, then progressively refine the details ‚Äî adding color here, sharpening edges there, going back to fix proportions. The whole image comes into focus *at the same time.*\n",
    "\n",
    "This is how **diffusion models** generate images. And in this notebook series, we will see how this same idea can be applied to **text generation** ‚Äî leading to a fundamentally new paradigm for language models.\n",
    "\n",
    "**By the end of this notebook, you will:**\n",
    "- Understand how image diffusion works from first principles\n",
    "- Build a working diffusion model that generates MNIST digits from pure noise\n",
    "- See why this approach breaks for text (setting up Notebook 2)"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Building Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_03_building_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_03_building_intuition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### The Two Phases of Diffusion\n",
    "\n",
    "The core idea behind diffusion models is beautifully simple:\n",
    "\n",
    "**Phase 1 ‚Äî The Forward Process (Destroying):** Take a clean image. Gradually add random noise to it, step by step. After enough steps, the image becomes pure random static ‚Äî no trace of the original remains.\n",
    "\n",
    "**Phase 2 ‚Äî The Reverse Process (Creating):** Train a neural network that learns to reverse each noise step. Given a noisy image, predict what it looked like one step earlier (slightly less noisy). Chain these predictions together, and you can go from pure noise all the way back to a clean image.\n",
    "\n",
    "The magic: once the network learns to denoise, you can start from **pure random noise** and generate entirely new images that never existed in the training set.\n",
    "\n",
    "### ü§î Think About This\n",
    "\n",
    "If I gave you a completely noisy image and asked you to denoise it, what information would you need?\n",
    "\n",
    "You would need to know:\n",
    "1. **How noisy** the image is (a little noisy? a lot?)\n",
    "2. **What kind of images** are possible (faces? digits? landscapes?)\n",
    "\n",
    "The first is provided by the **timestep**. The second is learned from the **training data**. Keep these two ideas in mind ‚Äî they drive every design decision."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: The Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_04_the_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_04_the_math"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### The Forward Process\n",
    "\n",
    "At any timestep $t$, we can jump directly from the clean image $x_0$ to the noisy version $x_t$ using:\n",
    "\n",
    "$$x_t = \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\n",
    "\n",
    "**What this says computationally:** Mix the original image with random noise. The parameter $\\bar{\\alpha}_t$ controls the ratio:\n",
    "- When $\\bar{\\alpha}_t = 1$: all signal, no noise (clean image)\n",
    "- When $\\bar{\\alpha}_t = 0$: no signal, all noise (pure static)\n",
    "- In between: a blend of image and noise"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worked Example\n",
    "\n",
    "Let us plug in real numbers. Suppose we have a single pixel with value $x_0 = 0.8$, and at timestep $t$, the noise schedule gives $\\bar{\\alpha}_t = 0.5$. A random noise sample gives $\\epsilon = 0.3$.\n",
    "\n",
    "$$x_t = \\sqrt{0.5} \\times 0.8 + \\sqrt{0.5} \\times 0.3 = 0.707 \\times 0.8 + 0.707 \\times 0.3 = 0.566 + 0.212 = 0.778$$\n",
    "\n",
    "The pixel shifted from 0.8 towards the noise. As $\\bar{\\alpha}_t$ decreases towards 0, the first term shrinks and noise dominates. At $\\bar{\\alpha}_t = 0$, the original image is completely gone.\n",
    "\n",
    "### The Training Objective\n",
    "\n",
    "The neural network learns to predict the noise $\\epsilon$ that was added. The simplified loss is:\n",
    "\n",
    "$$\\mathcal{L} = \\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2$$\n",
    "\n",
    "**What this says computationally:** Take the actual noise that was added, subtract the model's prediction of that noise, and square the difference. This is just mean squared error between the true noise and predicted noise."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Noise Schedule\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_05_noise_schedule.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_05_noise_schedule"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It ‚Äî Component by Component\n",
    "\n",
    "### 4.1 The Noise Schedule\n",
    "\n",
    "The noise schedule defines how quickly we destroy the image. We start with $\\beta_t$ values (noise rates) and compute $\\bar{\\alpha}_t$ from them."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "TIMESTEPS = 1000\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    \"\"\"Linear schedule from Ho et al. (2020).\n",
    "    Beta increases linearly from 0.0001 to 0.02.\n",
    "    \"\"\"\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "# Compute all schedule quantities\n",
    "betas = linear_beta_schedule(TIMESTEPS)\n",
    "alphas = 1.0 - betas\n",
    "alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "sqrt_alpha_bars = torch.sqrt(alpha_bars)\n",
    "sqrt_one_minus_alpha_bars = torch.sqrt(1.0 - alpha_bars)\n",
    "\n",
    "print(f\"alpha_bar at t=0:   {alpha_bars[0]:.4f} (almost no noise)\")\n",
    "print(f\"alpha_bar at t=500: {alpha_bars[500]:.4f} (half-noised)\")\n",
    "print(f\"alpha_bar at t=999: {alpha_bars[999]:.4f} (nearly pure noise)\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize the noise schedule\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].plot(betas.numpy(), color='#e53935', linewidth=2)\n",
    "axes[0].set_xlabel('Timestep t', fontsize=11)\n",
    "axes[0].set_ylabel(r'$\\beta_t$', fontsize=13)\n",
    "axes[0].set_title('Noise Rate (Beta)', fontsize=13)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(alpha_bars.numpy(), color='#1565c0', linewidth=2)\n",
    "axes[1].set_xlabel('Timestep t', fontsize=11)\n",
    "axes[1].set_ylabel(r'$\\bar{\\alpha}_t$', fontsize=13)\n",
    "axes[1].set_title('Signal Retention (Alpha Bar)', fontsize=13)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Left: noise rate increases over time.\")\n",
    "print(\"Right: signal retention decreases ‚Äî by t=1000, almost no original signal remains.\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Forward Process Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_06_forward_process_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_06_forward_process_code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Forward Process"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion(x_0, t, sqrt_alpha_bar, sqrt_one_minus_alpha_bar):\n",
    "    \"\"\"Noise an image to timestep t in one step.\n",
    "\n",
    "    Args:\n",
    "        x_0: Clean images, shape (B, C, H, W)\n",
    "        t: Timestep indices, shape (B,)\n",
    "        sqrt_alpha_bar: Precomputed sqrt(alpha_bar), shape (T,)\n",
    "        sqrt_one_minus_alpha_bar: Precomputed sqrt(1-alpha_bar), shape (T,)\n",
    "\n",
    "    Returns:\n",
    "        x_t: Noised images\n",
    "        noise: The noise that was added (needed for training)\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x_0)\n",
    "\n",
    "    # Gather the schedule values for each sample's timestep\n",
    "    s_ab = sqrt_alpha_bar[t].view(-1, 1, 1, 1)\n",
    "    s_omab = sqrt_one_minus_alpha_bar[t].view(-1, 1, 1, 1)\n",
    "\n",
    "    x_t = s_ab * x_0 + s_omab * noise\n",
    "    return x_t, noise"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Forward Visualization\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_07_forward_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_07_forward_visualization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize: a single MNIST digit being progressively noised\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Scale to [-1, 1]\n",
    "])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True,\n",
    "                                transform=transform)\n",
    "\n",
    "# Pick a digit\n",
    "sample_img = train_dataset[0][0].unsqueeze(0)  # (1, 1, 28, 28)\n",
    "\n",
    "timesteps_to_show = [0, 50, 150, 300, 500, 750, 999]\n",
    "fig, axes = plt.subplots(1, len(timesteps_to_show), figsize=(18, 3))\n",
    "\n",
    "for ax, t_val in zip(axes, timesteps_to_show):\n",
    "    t = torch.tensor([t_val])\n",
    "    noised, _ = forward_diffusion(sample_img, t,\n",
    "                                   sqrt_alpha_bars, sqrt_one_minus_alpha_bars)\n",
    "    ax.imshow(noised[0, 0].numpy(), cmap='gray')\n",
    "    ax.set_title(f't = {t_val}', fontsize=11)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Forward Process: Gradually Adding Noise to a Digit', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"The digit is progressively destroyed until only noise remains.\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Unet Model\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_08_unet_model.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_08_unet_model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The Denoising Model (Simple U-Net)\n",
    "\n",
    "We need a neural network that takes a noisy image and timestep, and predicts the noise. We use a minimal U-Net with time embedding."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    \"\"\"Sinusoidal timestep embedding, like positional encoding.\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t.float().unsqueeze(1) * emb.unsqueeze(0)\n",
    "        return torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Conv -> GroupNorm -> SiLU, with time embedding injection.\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, time_dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.GroupNorm(8, out_ch),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.time_proj = nn.Linear(time_dim, out_ch)\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        h = self.conv(x)\n",
    "        # Add time embedding (broadcast over spatial dims)\n",
    "        h = h + self.time_proj(t_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "        return h"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleUNet(nn.Module):\n",
    "    \"\"\"Minimal U-Net for 28x28 MNIST images.\"\"\"\n",
    "    def __init__(self, in_ch=1, base_ch=32, time_dim=64):\n",
    "        super().__init__()\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalTimeEmbedding(time_dim),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch, time_dim)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch * 2, time_dim)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ConvBlock(base_ch * 2, base_ch * 2, time_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.dec2 = ConvBlock(base_ch * 4, base_ch, time_dim)  # skip connection\n",
    "        self.dec1 = ConvBlock(base_ch * 2, base_ch, time_dim)\n",
    "\n",
    "        self.final = nn.Conv2d(base_ch, in_ch, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_embed(t)\n",
    "\n",
    "        # Encoder\n",
    "        h1 = self.enc1(x, t_emb)               # (B, 32, 28, 28)\n",
    "        h2 = self.enc2(self.pool(h1), t_emb)    # (B, 64, 14, 14)\n",
    "\n",
    "        # Bottleneck\n",
    "        h = self.bottleneck(self.pool(h2), t_emb)  # (B, 64, 7, 7)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        h = self.up(h)                           # (B, 64, 14, 14)\n",
    "        h = self.dec2(torch.cat([h, h2], dim=1), t_emb)  # (B, 32, 14, 14)\n",
    "        h = self.up(h)                           # (B, 32, 28, 28)\n",
    "        h = self.dec1(torch.cat([h, h1], dim=1), t_emb)  # (B, 32, 28, 28)\n",
    "\n",
    "        return self.final(h)                     # (B, 1, 28, 28)\n",
    "\n",
    "\n",
    "model = SimpleUNet().to(device)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"SimpleUNet parameters: {n_params:,}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_09_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_09_training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The Training Loop"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "losses = []\n",
    "\n",
    "print(\"Training the diffusion model on MNIST...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_losses = []\n",
    "    for batch_idx, (images, _) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Sample random timesteps for each image\n",
    "        t = torch.randint(0, TIMESTEPS, (images.shape[0],), device=device)\n",
    "\n",
    "        # Forward process: add noise\n",
    "        x_t, noise = forward_diffusion(images, t,\n",
    "                                        sqrt_alpha_bars.to(device),\n",
    "                                        sqrt_one_minus_alpha_bars.to(device))\n",
    "\n",
    "        # Model predicts the noise\n",
    "        predicted_noise = model(x_t, t)\n",
    "\n",
    "        # MSE loss between true noise and predicted noise\n",
    "        loss = F.mse_loss(predicted_noise, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Training loss curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, marker='o', color='#1565c0', linewidth=2, markersize=6)\n",
    "plt.xlabel('Epoch', fontsize=11)\n",
    "plt.ylabel('Mean MSE Loss', fontsize=11)\n",
    "plt.title('Diffusion Model Training Loss', fontsize=13)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"The loss should decrease steadily ‚Äî the model is learning to predict noise.\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo1 Reverse\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_10_todo1_reverse.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_10_todo1_reverse"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîß Your Turn\n",
    "\n",
    "### TODO 1: Implement the Sampling / Generation Function\n",
    "\n",
    "This is the reverse process ‚Äî going from pure noise back to a clean image. At each timestep, the model predicts the noise, and we subtract a carefully scaled version of it.\n",
    "\n",
    "The formula for each reverse step from $t$ to $t-1$:\n",
    "\n",
    "$$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\cdot \\epsilon_\\theta(x_t, t) \\right) + \\sigma_t \\cdot z$$\n",
    "\n",
    "where $z \\sim \\mathcal{N}(0, I)$ for $t > 1$ and $z = 0$ for $t = 1$, and $\\sigma_t = \\sqrt{\\beta_t}$.\n",
    "\n",
    "**What this says computationally:** Take the current noisy image, subtract the model's noise prediction (scaled appropriately), and add a small amount of fresh noise. At the very last step, skip the fresh noise to get a clean output."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, n_samples=16, img_size=28):\n",
    "    \"\"\"Generate images via the reverse diffusion process.\n",
    "\n",
    "    Args:\n",
    "        model: Trained SimpleUNet\n",
    "        n_samples: Number of images to generate\n",
    "        img_size: Spatial size (28 for MNIST)\n",
    "\n",
    "    Returns:\n",
    "        Generated images, shape (n_samples, 1, img_size, img_size)\n",
    "        history: List of intermediate snapshots\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    betas_d = betas.to(device)\n",
    "    alphas_d = alphas.to(device)\n",
    "    alpha_bars_d = alpha_bars.to(device)\n",
    "\n",
    "    # Start from pure Gaussian noise\n",
    "    x = torch.randn(n_samples, 1, img_size, img_size, device=device)\n",
    "    history = [x.cpu().clone()]\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Reverse loop: t = TIMESTEPS-1 down to 0\n",
    "    for t_val in reversed(range(TIMESTEPS)):\n",
    "        t = torch.full((n_samples,), t_val, device=device, dtype=torch.long)\n",
    "\n",
    "        # Step 1: Get the model's noise prediction\n",
    "        predicted_noise = ???  # YOUR CODE HERE\n",
    "\n",
    "        # Step 2: Compute scaling coefficients\n",
    "        beta_t = betas_d[t_val]\n",
    "        alpha_t = alphas_d[t_val]\n",
    "        alpha_bar_t = alpha_bars_d[t_val]\n",
    "        noise_coeff = beta_t / torch.sqrt(1.0 - alpha_bar_t)\n",
    "\n",
    "        # Step 3: Compute x_{t-1}\n",
    "        x = ???  # YOUR CODE HERE: (1/sqrt(alpha_t)) * (x - noise_coeff * predicted_noise)\n",
    "\n",
    "        # Step 4: Add stochastic noise (except at final step t=0)\n",
    "        if t_val > 0:\n",
    "            z = torch.randn_like(x)\n",
    "            sigma_t = torch.sqrt(beta_t)\n",
    "            x = ???  # YOUR CODE HERE: x + sigma_t * z\n",
    "\n",
    "        if t_val % 100 == 0 or t_val == 0:\n",
    "            history.append(x.cpu().clone())\n",
    "    # ==============================\n",
    "\n",
    "    model.train()\n",
    "    return x, history"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification\n",
    "try:\n",
    "    test_samples, test_history = sample(model, n_samples=4)\n",
    "    assert test_samples.shape == (4, 1, 28, 28), f\"Wrong shape: {test_samples.shape}\"\n",
    "    print(\"‚úÖ Sampling works! Generated 4 test images.\")\n",
    "    print(f\"   Pixel range: [{test_samples.min():.2f}, {test_samples.max():.2f}]\")\n",
    "    print(f\"   Snapshots: {len(test_history)}\")\n",
    "except NameError:\n",
    "    print(\"‚ùå Replace the ??? placeholders with your code.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Stop And Think Solution\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_11_stop_and_think_solution.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_11_stop_and_think_solution"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ‚úã Stop and Think\n",
    "Before looking at the solution:\n",
    "1. Why do we add fresh noise $z$ at every step except the last?\n",
    "2. What would happen if we used a larger $\\sigma_t$?\n",
    "3. Why do we go from $t = T-1$ all the way to $t = 0$?\n",
    "\n",
    "*Take a minute. Then scroll down.*\n",
    "\n",
    "---"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, n_samples=16, img_size=28):\n",
    "    \"\"\"Generate images via the reverse diffusion process.\"\"\"\n",
    "    model.eval()\n",
    "    betas_d = betas.to(device)\n",
    "    alphas_d = alphas.to(device)\n",
    "    alpha_bars_d = alpha_bars.to(device)\n",
    "\n",
    "    x = torch.randn(n_samples, 1, img_size, img_size, device=device)\n",
    "    history = [x.cpu().clone()]\n",
    "\n",
    "    for t_val in reversed(range(TIMESTEPS)):\n",
    "        t = torch.full((n_samples,), t_val, device=device, dtype=torch.long)\n",
    "\n",
    "        predicted_noise = model(x, t)\n",
    "\n",
    "        beta_t = betas_d[t_val]\n",
    "        alpha_t = alphas_d[t_val]\n",
    "        alpha_bar_t = alpha_bars_d[t_val]\n",
    "        noise_coeff = beta_t / torch.sqrt(1.0 - alpha_bar_t)\n",
    "\n",
    "        x = (1.0 / torch.sqrt(alpha_t)) * (x - noise_coeff * predicted_noise)\n",
    "\n",
    "        if t_val > 0:\n",
    "            z = torch.randn_like(x)\n",
    "            sigma_t = torch.sqrt(beta_t)\n",
    "            x = x + sigma_t * z\n",
    "\n",
    "        if t_val % 100 == 0 or t_val == 0:\n",
    "            history.append(x.cpu().clone())\n",
    "\n",
    "    model.train()\n",
    "    return x, history\n",
    "\n",
    "\n",
    "generated_images, generation_history = sample(model, n_samples=16)\n",
    "print(f\"Generated {generated_images.shape[0]} images!\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo2 Cosine\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_12_todo2_cosine.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_12_todo2_cosine"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement a Cosine Noise Schedule\n",
    "\n",
    "The linear schedule works, but a **cosine schedule** (Nichol & Dhariwal, 2021) spends more time at intermediate noise levels:\n",
    "\n",
    "$$\\bar{\\alpha}_t = \\frac{f(t)}{f(0)}, \\quad f(t) = \\cos\\!\\left(\\frac{t/T + s}{1 + s} \\cdot \\frac{\\pi}{2}\\right)^2, \\quad s = 0.008$$"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"Cosine noise schedule.\n",
    "\n",
    "    Hints:\n",
    "        1. Compute f(t) for t = 0, 1, ..., T\n",
    "        2. alpha_bars = f(t) / f(0)\n",
    "        3. betas = 1 - alpha_bar[t] / alpha_bar[t-1]\n",
    "        4. Clamp betas to [0, 0.999]\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    t = torch.linspace(0, timesteps, steps)\n",
    "\n",
    "    # ============ TODO ============\n",
    "    f_t = ???  # YOUR CODE: cos((t/T + s)/(1+s) * pi/2)^2\n",
    "    alpha_bars_cos = ???  # YOUR CODE: f(t) / f(0)\n",
    "    betas_cos = ???  # YOUR CODE: 1 - alpha_bar[t] / alpha_bar[t-1]\n",
    "    betas_cos = ???  # YOUR CODE: clamp to [0, 0.999]\n",
    "    # ==============================\n",
    "\n",
    "    return betas_cos"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification ‚Äî compare schedules\n",
    "try:\n",
    "    cosine_betas = cosine_beta_schedule(TIMESTEPS)\n",
    "    cosine_alphas = 1.0 - cosine_betas\n",
    "    cosine_alpha_bars = torch.cumprod(cosine_alphas, dim=0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(alpha_bars.numpy(), label='Linear', color='#1565c0', linewidth=2.5)\n",
    "    ax.plot(cosine_alpha_bars.numpy(), label='Cosine', color='#e53935',\n",
    "            linewidth=2.5, linestyle='--')\n",
    "    ax.set_xlabel('Timestep t', fontsize=12)\n",
    "    ax.set_ylabel(r'$\\bar{\\alpha}_t$', fontsize=14)\n",
    "    ax.set_title('Linear vs Cosine Noise Schedule', fontsize=14)\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"‚úÖ Cosine schedule implemented!\")\n",
    "    print(\"Notice: cosine spends more time at intermediate noise levels.\")\n",
    "except NameError:\n",
    "    print(\"‚ùå Replace the ??? placeholders.\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Results\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_13_results.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_13_results"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Progressive denoising: watch a digit emerge from noise\n",
    "torch.manual_seed(123)\n",
    "_, full_history = sample(model, n_samples=1)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(full_history), figsize=(20, 3))\n",
    "for idx, (ax, snapshot) in enumerate(zip(axes, full_history)):\n",
    "    img = snapshot[0, 0].numpy()\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    if idx == 0:\n",
    "        ax.set_title('t=999\\n(noise)', fontsize=9)\n",
    "    elif idx == len(full_history) - 1:\n",
    "        ax.set_title('t=0\\n(clean)', fontsize=9)\n",
    "    else:\n",
    "        t_approx = 999 - idx * 100\n",
    "        ax.set_title(f't‚âà{max(t_approx, 0)}', fontsize=9)\n",
    "\n",
    "plt.suptitle('Reverse Process: From Pure Noise to a Digit', fontsize=14, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Watch the digit emerge ‚Äî first the rough shape, then fine details.\")\n",
    "print(\"This is exactly like an artist refining a sketch.\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üéØ Final Output"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a grid of MNIST digits from pure noise\n",
    "torch.manual_seed(42)\n",
    "final_images, _ = sample(model, n_samples=64)\n",
    "final_images = torch.clamp(final_images, -1, 1)\n",
    "\n",
    "fig, axes = plt.subplots(8, 8, figsize=(12, 12))\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        idx = i * 8 + j\n",
    "        axes[i, j].imshow(final_images[idx, 0].cpu().numpy(), cmap='gray')\n",
    "        axes[i, j].axis('off')\n",
    "\n",
    "plt.suptitle('Generated MNIST Digits ‚Äî From Pure Noise', fontsize=16, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Side-by-side: Real vs Generated\n",
    "fig, axes = plt.subplots(2, 10, figsize=(16, 4))\n",
    "\n",
    "for i in range(10):\n",
    "    real_img = train_dataset[i * 600][0][0].numpy()\n",
    "    axes[0, i].imshow(real_img, cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 5:\n",
    "        axes[0, i].set_title('Real MNIST', fontsize=12, pad=10)\n",
    "\n",
    "torch.manual_seed(7)\n",
    "comp_images, _ = sample(model, n_samples=10)\n",
    "comp_images = torch.clamp(comp_images, -1, 1)\n",
    "\n",
    "for i in range(10):\n",
    "    axes[1, i].imshow(comp_images[i, 0].cpu().numpy(), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 5:\n",
    "        axes[1, i].set_title('Generated', fontsize=12, pad=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Top: Real MNIST digits. Bottom: Generated from pure noise.\")"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Breaks For Text\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_14_breaks_for_text.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_14_breaks_for_text"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The Problem ‚Äî Why This Breaks for Text\n",
    "\n",
    "We have a working image diffusion model. Now let us confront the fundamental question that motivates this entire series: **why can't we apply this directly to text?**\n",
    "\n",
    "### Continuous vs Discrete\n",
    "\n",
    "Images live in a **continuous** space. Each pixel is a floating-point number. Adding Gaussian noise to pixel 0.8 gives you 0.73 or 0.85 ‚Äî still valid pixels.\n",
    "\n",
    "Text lives in a **discrete** space. Each token is an integer ‚Äî an index into a vocabulary. The word \"cat\" might be token 3421, \"dog\" might be token 7856. What does token 5638.5 mean? Nothing. It is not a real word."
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate why Gaussian noise fails for text\n",
    "print(\"=\" * 55)\n",
    "print(\"EXPERIMENT: Gaussian Noise on Token IDs\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "vocab = {42: \"the\", 3421: \"cat\", 891: \"sat\", 156: \"on\", 7856: \"dog\", 2001: \"mat\"}\n",
    "\n",
    "sentence = \"the cat sat on the mat\"\n",
    "tokens = [42, 3421, 891, 156, 42, 2001]\n",
    "print(f\"\\nOriginal: '{sentence}'\")\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "\n",
    "print(f\"\\n--- Adding Gaussian noise (sigma=500) ---\")\n",
    "np.random.seed(42)\n",
    "for token_id, word in zip(tokens, sentence.split()):\n",
    "    noise = np.random.normal(0, 500)\n",
    "    noised_id = token_id + noise\n",
    "    rounded_id = int(round(noised_id))\n",
    "    noised_word = vocab.get(rounded_id, \"???\")\n",
    "    print(f\"  '{word}' ({token_id:5d}) + noise {noise:+8.1f} \"\n",
    "          f\"= {noised_id:8.1f} ‚Üí '{noised_word}'\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Most noised tokens map to NOTHING in the vocabulary!\")\n",
    "print(f\"   Gaussian noise is MEANINGLESS for discrete data.\")"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three Approaches to Fix This\n",
    "\n",
    "Researchers have proposed three solutions:\n",
    "\n",
    "1. **Continuous embedding diffusion** (Diffusion-LM, 2022) ‚Äî embed tokens into continuous vectors, run Gaussian diffusion on embeddings, round back. Problem: rounding introduces errors.\n",
    "\n",
    "2. **Masked diffusion** (MDLM, LLaDA) ‚Äî replace tokens with [MASK] instead of adding noise. Masking is a natural \"noise\" for discrete data. Simplest and most successful.\n",
    "\n",
    "3. **Score-based discrete diffusion** (SEDD) ‚Äî define transition probabilities directly in discrete space. Elegant but complex."
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview: what masked \"noise\" looks like\n",
    "print(\"=\" * 55)\n",
    "print(\"PREVIEW: Masked Diffusion (Next Notebook)\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "words = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "print(f\"\\nOriginal:  {' '.join(words)}\\n\")\n",
    "\n",
    "np.random.seed(0)\n",
    "for ratio in [0.0, 0.17, 0.33, 0.50, 0.67, 0.83, 1.0]:\n",
    "    masked = [\"[M]\" if np.random.random() < ratio else w for w in words]\n",
    "    pct = masked.count(\"[M]\") / len(masked) * 100\n",
    "    print(f\"  t={ratio:.2f} ({pct:3.0f}% masked):  {' '.join(masked)}\")\n",
    "\n",
    "print(\"\\nEvery intermediate state is interpretable!\")\n",
    "print(\"No 'half-cat' nonsense. Tokens are present or masked.\")\n",
    "print(\"\\nüí° Masking IS the noise process for text.\")"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_15_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_15_closing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "\n",
    "1. **Why does the model need to know the timestep $t$?** Without it, the model cannot distinguish between slightly noisy images (needing gentle correction) and heavily noisy images (needing aggressive reconstruction).\n",
    "\n",
    "2. **What would happen with a very aggressive noise schedule?** If the image is destroyed too quickly, most of the trajectory is spent going from pure noise to slightly-less-pure noise ‚Äî the model has few steps to learn fine details.\n",
    "\n",
    "3. **Could we use a Transformer instead of a U-Net?** Yes! The Diffusion Transformer (DiT) does exactly this and powers DALL-E 3 and Stable Diffusion 3. The diffusion math stays the same ‚Äî only the backbone changes.\n",
    "\n",
    "### üèÜ Optional Challenges\n",
    "\n",
    "1. Train on Fashion-MNIST and compare generation quality\n",
    "2. Implement classifier-free guidance for conditional generation\n",
    "3. Implement DDIM sampling (deterministic, fewer steps needed)\n",
    "\n",
    "---\n",
    "\n",
    "**Up Next ‚Äî Notebook 2:** *Masked Diffusion for Text.* We will see how replacing Gaussian noise with token masking gives us a diffusion process that works for discrete text ‚Äî and it turns out to be just BERT training, generalized to all masking ratios."
   ],
   "id": "cell_37"
  }
 ]
}