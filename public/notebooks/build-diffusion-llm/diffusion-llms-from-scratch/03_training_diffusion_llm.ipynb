{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Training a Diffusion LLM ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1LnfxlgM23VTaqwxDE_vcyPrGp_tSwhZr\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/03_00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Training a Diffusion LLM: The ELBO and Beyond\n",
    "\n",
    "*Part 3 of the Vizuara series on Diffusion LLMs from Scratch*\n",
    "*Estimated time: 40 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ü§ñ AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** ‚Äî it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[üëâ Open AI Teaching Assistant](https://course-creator-brown.vercel.app/courses/diffusion-llms-from-scratch/practice/3/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In Notebook 2, we built a bidirectional Transformer that predicts masked tokens. We trained it on synthetic patterns. But two big questions remain:\n",
    "\n",
    "1. **Is this mathematically rigorous?** Is \"train BERT at all masking ratios\" really a valid diffusion model?\n",
    "2. **Does it work on real text?** Can we train on actual language and get a model that learns the structure?\n",
    "\n",
    "In this notebook, we answer both. We will derive the **Evidence Lower Bound (ELBO)**, show it simplifies to masked language modeling, and train a complete diffusion LLM on real text.\n",
    "\n",
    "**By the end of this notebook, you will:**\n",
    "- Understand why the ELBO guarantees our approach is a valid diffusion model\n",
    "- Train a diffusion LLM on TinyShakespeare (character-level)\n",
    "- See the model learn to predict masked characters with increasing accuracy\n",
    "- Analyze how performance varies across masking ratios"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_01_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_01_intuition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### The Difficulty Curriculum\n",
    "\n",
    "Think of training across all masking ratios as giving the model a **curriculum**:\n",
    "\n",
    "- **Low $t$ (few masks):** Most tokens are visible. The masked ones can be inferred from abundant context. This is the \"easy homework.\"\n",
    "- **High $t$ (many masks):** Very few tokens are visible. The model must rely on global patterns and priors. This is the \"hard exam.\"\n",
    "\n",
    "Training uniformly over $t \\in [0, 1]$ exposes the model to both easy and hard examples. The easy examples teach it local patterns (bigrams, common phrases). The hard examples teach it global structure (sentence templates, long-range dependencies).\n",
    "\n",
    "### Why Bidirectional Attention Is the Secret Weapon\n",
    "\n",
    "Consider a sentence where 90% of tokens are masked. An autoregressive model would see almost nothing ‚Äî just the first few tokens. Our model sees the **scattered 10% of tokens across the entire sequence**, giving it signal from both the beginning and the end.\n",
    "\n",
    "### ü§î Think About This\n",
    "\n",
    "If you were given a sentence where 90% of characters were hidden, but you could see random characters scattered throughout, could you guess the original? Probably yes ‚Äî because English has so much redundancy. The model exploits this same redundancy."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Elbo\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_02_elbo.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_02_elbo"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### The Evidence Lower Bound (ELBO)\n",
    "\n",
    "We want to maximize the log-likelihood of our data:\n",
    "\n",
    "$$\\log p_\\theta(x_0)$$\n",
    "\n",
    "Directly computing this is intractable. So we optimize a lower bound called the **ELBO**:\n",
    "\n",
    "$$\\log p_\\theta(x_0) \\geq \\text{ELBO} = \\mathbb{E}_{q} \\left[ \\log p_\\theta(x_0 \\mid x_1) + \\sum_{t=2}^{T} \\log \\frac{p_\\theta(x_{t-1} \\mid x_t)}{q(x_t \\mid x_{t-1}, x_0)} \\right]$$\n",
    "\n",
    "**What this says computationally:** The ELBO measures how well our reverse process (unmasking) matches the true reverse of the forward process (masking). If our model perfectly predicts masked tokens, the ELBO equals the true log-likelihood."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worked Example\n",
    "\n",
    "Suppose we have a 2-token vocabulary {A, B}, a 3-token sequence, and $T = 2$ steps. At each step, the model assigns probability 0.9 to the correct token:\n",
    "\n",
    "$$\\text{ELBO} \\approx 3 \\times \\log(0.9) = 3 \\times (-0.105) = -0.315$$\n",
    "\n",
    "A perfect model gives ELBO = 0. As the model improves, ELBO increases towards 0.\n",
    "\n",
    "### The Beautiful Simplification\n",
    "\n",
    "Here is the key result from the MDLM paper (NeurIPS 2024): **when you work through the math for masked diffusion, the ELBO simplifies to a weighted mixture of masked language modeling losses at different masking ratios.**\n",
    "\n",
    "$$\\text{ELBO} \\propto -\\mathbb{E}_{t \\sim U(0,1)} \\left[ \\frac{1}{t} \\sum_{i : x_t^{(i)} = [\\text{MASK}]} \\log p_\\theta(x_0^{(i)} \\mid x_t) \\right]$$\n",
    "\n",
    "**What this means:** The theoretically rigorous diffusion training objective is essentially \"train BERT at all masking ratios\" with a $1/t$ importance weight. The theory confirms what the intuition suggested.\n",
    "\n",
    "### üí° Key Insight\n",
    "\n",
    "The $1/t$ weighting means low-masking-ratio examples (where few tokens are masked) get *more weight* per masked token. This makes sense ‚Äî when almost nothing is masked, each prediction is more informative about the model's understanding.\n",
    "\n",
    "In practice, many implementations use uniform weighting (no $1/t$ factor) and it still works well."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Data Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_03_data_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_03_data_setup"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It ‚Äî Component by Component\n",
    "\n",
    "### 4.1 The TinyShakespeare Dataset"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import urllib.request\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "%matplotlib inline\n",
    "\n",
    "# Download TinyShakespeare\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = \"shakespeare.txt\"\n",
    "try:\n",
    "    with open(filepath, 'r') as f:\n",
    "        text = f.read()\n",
    "except FileNotFoundError:\n",
    "    urllib.request.urlretrieve(url, filepath)\n",
    "    with open(filepath, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "print(f\"Dataset size: {len(text):,} characters\")\n",
    "print(f\"First 200 chars:\\n{text[:200]}\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character-level tokenizer\n",
    "chars = sorted(list(set(text)))\n",
    "VOCAB_SIZE = len(chars) + 1  # +1 for [MASK] token\n",
    "MASK_TOKEN = 0\n",
    "\n",
    "# Map characters to IDs (starting from 1; 0 is reserved for MASK)\n",
    "char_to_id = {ch: i + 1 for i, ch in enumerate(chars)}\n",
    "id_to_char = {i + 1: ch for i, ch in enumerate(chars)}\n",
    "id_to_char[MASK_TOKEN] = '[M]'\n",
    "\n",
    "def encode(s):\n",
    "    return [char_to_id[c] for c in s]\n",
    "\n",
    "def decode(ids):\n",
    "    return ''.join(id_to_char.get(i, '?') for i in ids)\n",
    "\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE} (including [MASK])\")\n",
    "print(f\"Characters: {''.join(chars[:30])}...\")\n",
    "print(f\"\\nEncoding test: 'Hello' ‚Üí {encode('Hello')}\")\n",
    "print(f\"Decoding test: {encode('Hello')} ‚Üí '{decode(encode('Hello'))}'\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training sequences\n",
    "SEQ_LEN = 64\n",
    "BATCH_SIZE = 64\n",
    "D_MODEL = 128\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 4\n",
    "\n",
    "# Encode entire text\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"Total tokens: {len(data):,}\")\n",
    "\n",
    "# Split into training sequences\n",
    "n_sequences = len(data) // SEQ_LEN\n",
    "sequences = data[:n_sequences * SEQ_LEN].reshape(n_sequences, SEQ_LEN)\n",
    "print(f\"Training sequences: {sequences.shape[0]:,} of length {SEQ_LEN}\")\n",
    "\n",
    "# Train/val split (90/10)\n",
    "n_train = int(0.9 * len(sequences))\n",
    "train_data = sequences[:n_train].to(device)\n",
    "val_data = sequences[n_train:].to(device)\n",
    "print(f\"Train: {len(train_data):,} | Val: {len(val_data):,}\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Masking And Model\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_04_masking_and_model.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_04_masking_and_model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Forward Process\n",
    "\n",
    "Same masking function as Notebook 2, but now on real text."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_tokens(x_0, t):\n",
    "    \"\"\"Mask tokens with probability t.\"\"\"\n",
    "    random_vals = torch.rand_like(x_0.float())\n",
    "    mask = random_vals < t\n",
    "    x_t = x_0.clone()\n",
    "    x_t[mask] = MASK_TOKEN\n",
    "    return x_t, mask"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize masking on real Shakespeare text\n",
    "sample = train_data[0:1]  # One sequence\n",
    "print(f\"Original: '{decode(sample[0].tolist())}'\")\n",
    "print()\n",
    "\n",
    "for t_val in [0.2, 0.5, 0.8]:\n",
    "    torch.manual_seed(0)\n",
    "    t = torch.tensor([[t_val]], device=device)\n",
    "    masked, _ = mask_tokens(sample, t)\n",
    "    masked_str = decode(masked[0].tolist())\n",
    "    print(f\"t={t_val}: '{masked_str}'\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The Model\n",
    "\n",
    "Same architecture as Notebook 2 but scaled up for real text."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class DiffusionLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, max_len=512):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, d_model), nn.SiLU(), nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1, batch_first=True, norm_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)\n",
    "        self.output_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x_t, t):\n",
    "        h = self.token_embed(x_t)\n",
    "        h = self.pos_enc(h)\n",
    "        h = h + self.time_mlp(t).unsqueeze(1)\n",
    "        h = self.transformer(h)\n",
    "        return self.output_head(h)\n",
    "\n",
    "\n",
    "model = DiffusionLM(VOCAB_SIZE, D_MODEL, N_HEADS, N_LAYERS).to(device)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params:,}\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_05_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_05_training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The Full Training Loop"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, NUM_EPOCHS * (n_train // BATCH_SIZE)\n",
    ")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Training on TinyShakespeare...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Shuffle training data\n",
    "    perm = torch.randperm(len(train_data))\n",
    "    epoch_losses = []\n",
    "\n",
    "    for i in range(0, len(train_data) - BATCH_SIZE, BATCH_SIZE):\n",
    "        batch = train_data[perm[i:i + BATCH_SIZE]]\n",
    "\n",
    "        # Random masking ratio per sample\n",
    "        t = torch.rand(BATCH_SIZE, 1, device=device) * 0.98 + 0.02\n",
    "\n",
    "        # Mask tokens\n",
    "        x_t, mask = mask_tokens(batch, t)\n",
    "\n",
    "        # Predict and compute loss on masked positions\n",
    "        logits = model(x_t, t)\n",
    "        logits_masked = logits[mask]\n",
    "        targets_masked = batch[mask]\n",
    "\n",
    "        if logits_masked.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        loss = F.cross_entropy(logits_masked, targets_masked)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    # Validation loss\n",
    "    with torch.no_grad():\n",
    "        val_t = torch.full((len(val_data), 1), 0.5, device=device)\n",
    "        val_masked, val_mask = mask_tokens(val_data, val_t)\n",
    "        val_logits = model(val_masked, val_t)\n",
    "        v_loss = F.cross_entropy(val_logits[val_mask], val_data[val_mask]).item()\n",
    "\n",
    "    avg_train = np.mean(epoch_losses)\n",
    "    train_losses.append(avg_train)\n",
    "    val_losses.append(v_loss)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train: {avg_train:.3f} | Val: {v_loss:.3f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Training and validation loss curves\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(train_losses, label='Train', color='#1565c0', linewidth=2)\n",
    "ax.plot(val_losses, label='Validation', color='#e53935', linewidth=2, linestyle='--')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "ax.set_title('Diffusion LM Training on TinyShakespeare', fontsize=14)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Accuracy\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_06_accuracy.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_06_accuracy"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Evaluating Across Masking Ratios"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä How does accuracy change with masking ratio?\n",
    "@torch.no_grad()\n",
    "def evaluate_at_t(model, data, t_val, n_samples=200):\n",
    "    \"\"\"Compute accuracy at a specific masking ratio.\"\"\"\n",
    "    indices = torch.randperm(len(data))[:n_samples]\n",
    "    batch = data[indices]\n",
    "    t = torch.full((len(batch), 1), t_val, device=device)\n",
    "    x_t, mask = mask_tokens(batch, t)\n",
    "    logits = model(x_t, t)\n",
    "    preds = logits[mask].argmax(dim=-1)\n",
    "    targets = batch[mask]\n",
    "    accuracy = (preds == targets).float().mean().item()\n",
    "    return accuracy\n",
    "\n",
    "t_vals = np.linspace(0.1, 0.95, 15)\n",
    "accuracies = [evaluate_at_t(model, val_data, t) * 100 for t in t_vals]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(t_vals, accuracies, 'o-', color='#1565c0', linewidth=2, markersize=6)\n",
    "plt.xlabel('Masking ratio t', fontsize=12)\n",
    "plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "plt.title('Prediction Accuracy vs Masking Ratio (Shakespeare)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Low masking ‚Üí high accuracy (lots of context).\")\n",
    "print(\"High masking ‚Üí lower accuracy (very little context).\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Predictions\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_07_predictions.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_07_predictions"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Visualizing Predictions on Real Text"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show predictions on actual Shakespeare text\n",
    "@torch.no_grad()\n",
    "def show_predictions(model, text_str, t_val=0.5):\n",
    "    \"\"\"Mask a text string and show model predictions.\"\"\"\n",
    "    ids = encode(text_str[:SEQ_LEN])\n",
    "    if len(ids) < SEQ_LEN:\n",
    "        ids = ids + [char_to_id[' ']] * (SEQ_LEN - len(ids))\n",
    "    x = torch.tensor([ids], device=device)\n",
    "\n",
    "    t = torch.tensor([[t_val]], device=device)\n",
    "    x_t, mask = mask_tokens(x, t)\n",
    "    logits = model(x_t, t)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    preds = logits[0].argmax(dim=-1)\n",
    "\n",
    "    print(f\"Original:  '{decode(ids)}'\")\n",
    "    masked_str = decode(x_t[0].tolist())\n",
    "    print(f\"Masked:    '{masked_str}'\")\n",
    "\n",
    "    # Show predictions at masked positions\n",
    "    pred_ids = x_t[0].clone()\n",
    "    for i in range(SEQ_LEN):\n",
    "        if mask[0, i]:\n",
    "            pred_ids[i] = preds[i]\n",
    "    print(f\"Predicted: '{decode(pred_ids.tolist())}'\")\n",
    "\n",
    "    # Count correct\n",
    "    n_correct = sum(1 for i in range(SEQ_LEN) if mask[0, i] and preds[i] == x[0, i])\n",
    "    n_total = mask[0].sum().item()\n",
    "    print(f\"Accuracy:  {n_correct}/{n_total} = {n_correct/max(n_total,1)*100:.0f}%\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "show_predictions(model, \"First Citizen:\\nBefore we proceed any further, hear me speak.\")\n",
    "print()\n",
    "show_predictions(model, \"To be, or not to be, that is the question:\")\n",
    "print()\n",
    "show_predictions(model, \"All that glitters is not gold; often have you heard that told\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo1\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_08_todo1.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_08_todo1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîß Your Turn\n",
    "\n",
    "### TODO 1: Implement a Learning Rate Scheduler Comparison"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_schedule(schedule_type='cosine', n_steps=500, lr=3e-4):\n",
    "    \"\"\"Train a small model with different LR schedules and compare.\n",
    "\n",
    "    Args:\n",
    "        schedule_type: 'cosine', 'constant', or 'linear'\n",
    "        n_steps: Number of training steps\n",
    "        lr: Base learning rate\n",
    "\n",
    "    Returns:\n",
    "        List of losses\n",
    "    \"\"\"\n",
    "    small_model = DiffusionLM(VOCAB_SIZE, 64, 2, 2).to(device)\n",
    "    optimizer = torch.optim.AdamW(small_model.parameters(), lr=lr)\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Create the appropriate scheduler based on schedule_type\n",
    "    if schedule_type == 'cosine':\n",
    "        scheduler = ???  # YOUR CODE: CosineAnnealingLR\n",
    "    elif schedule_type == 'linear':\n",
    "        scheduler = ???  # YOUR CODE: LinearLR with end_factor=0.01\n",
    "    else:  # constant\n",
    "        scheduler = ???  # YOUR CODE: ConstantLR or no scheduler\n",
    "    # ==============================\n",
    "\n",
    "    losses = []\n",
    "    for step in range(n_steps):\n",
    "        batch_idx = torch.randint(0, len(train_data), (32,))\n",
    "        batch = train_data[batch_idx]\n",
    "        t = torch.rand(32, 1, device=device) * 0.98 + 0.02\n",
    "        x_t, mask = mask_tokens(batch, t)\n",
    "        logits = small_model(x_t, t)\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        loss = F.cross_entropy(logits[mask], batch[mask])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return losses"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification: compare schedules\n",
    "try:\n",
    "    losses_cos = train_with_schedule('cosine')\n",
    "    losses_const = train_with_schedule('constant')\n",
    "    losses_lin = train_with_schedule('linear')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    w = 20\n",
    "    for losses, label, color in [\n",
    "        (losses_cos, 'Cosine', '#1565c0'),\n",
    "        (losses_const, 'Constant', '#e53935'),\n",
    "        (losses_lin, 'Linear', '#2e7d32'),\n",
    "    ]:\n",
    "        smoothed = np.convolve(losses, np.ones(w)/w, mode='valid')\n",
    "        ax.plot(smoothed, label=label, color=color, linewidth=2)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('LR Schedule Comparison')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"‚úÖ Compare which schedule converges fastest!\")\n",
    "except NameError:\n",
    "    print(\"‚ùå Replace the ??? placeholders.\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo2\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_09_todo2.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_09_todo2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Per-Position Confidence Analysis"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def confidence_analysis(model, data, t_val=0.5, n_samples=100):\n",
    "    \"\"\"Compute the model's confidence at each position.\n",
    "\n",
    "    Args:\n",
    "        model: Trained DiffusionLM\n",
    "        data: Validation data\n",
    "        t_val: Masking ratio to evaluate at\n",
    "        n_samples: Number of samples\n",
    "\n",
    "    Returns:\n",
    "        position_confidences: Average confidence per position, shape (SEQ_LEN,)\n",
    "    \"\"\"\n",
    "    indices = torch.randperm(len(data))[:n_samples]\n",
    "    batch = data[indices]\n",
    "    t = torch.full((n_samples, 1), t_val, device=device)\n",
    "    x_t, mask = mask_tokens(batch, t)\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Get model predictions\n",
    "    logits = ???  # YOUR CODE\n",
    "\n",
    "    # Step 2: Convert to probabilities\n",
    "    probs = ???  # YOUR CODE: softmax over vocab dim\n",
    "\n",
    "    # Step 3: Get max probability at each position (confidence)\n",
    "    confidence = ???  # YOUR CODE: max prob at each position, shape (n_samples, SEQ_LEN)\n",
    "\n",
    "    # Step 4: Average confidence per position (only at masked positions)\n",
    "    #         Set unmasked positions to NaN so they don't affect the mean\n",
    "    confidence[~mask] = float('nan')\n",
    "    position_confidences = torch.nanmean(confidence, dim=0)\n",
    "    # ==============================\n",
    "\n",
    "    return position_confidences.cpu().numpy()"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification\n",
    "try:\n",
    "    conf = confidence_analysis(model, val_data, t_val=0.5)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.bar(range(SEQ_LEN), conf, color='#1565c0', alpha=0.7)\n",
    "    plt.xlabel('Position', fontsize=11)\n",
    "    plt.ylabel('Avg Confidence', fontsize=11)\n",
    "    plt.title('Model Confidence by Position (t=0.5)', fontsize=13)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"‚úÖ Do you see any positional patterns in confidence?\")\n",
    "except NameError:\n",
    "    print(\"‚ùå Replace the ??? placeholders.\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Results\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_10_results.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_10_results"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline demo: take Shakespeare, mask it, predict, show results\n",
    "@torch.no_grad()\n",
    "def full_demo(model, text_str, t_val=0.5):\n",
    "    \"\"\"Complete pipeline demonstration.\"\"\"\n",
    "    ids = encode(text_str[:SEQ_LEN])\n",
    "    if len(ids) < SEQ_LEN:\n",
    "        ids = ids + [char_to_id[' ']] * (SEQ_LEN - len(ids))\n",
    "    x = torch.tensor([ids], device=device)\n",
    "    t = torch.tensor([[t_val]], device=device)\n",
    "    x_t, mask = mask_tokens(x, t)\n",
    "    logits = model(x_t, t)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    confidences = probs.max(dim=-1).values[0]\n",
    "\n",
    "    return x[0], x_t[0], logits[0].argmax(dim=-1), confidences, mask[0]\n",
    "\n",
    "\n",
    "sample_text = \"ROMEO:\\nBut, soft! what light through yonder window breaks?\"\n",
    "orig, masked, predicted, conf, mask = full_demo(model, sample_text)\n",
    "\n",
    "print(\"Original: \", decode(orig.tolist()))\n",
    "print(\"Masked:   \", decode(masked.tolist()))\n",
    "print(\"Predicted:\", decode(predicted.tolist()))\n",
    "print()\n",
    "\n",
    "n_correct = ((predicted == orig) & mask).sum().item()\n",
    "n_total = mask.sum().item()\n",
    "print(f\"Accuracy on masked tokens: {n_correct}/{n_total} = {n_correct/max(n_total,1)*100:.0f}%\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üéØ Final Output"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Beautiful visualization: predictions with confidence coloring\n",
    "fig, axes = plt.subplots(3, 1, figsize=(18, 6))\n",
    "\n",
    "sample_texts = [\n",
    "    \"First Citizen:\\nBefore we proceed any further, hear me speak.\",\n",
    "    \"ROMEO:\\nBut, soft! what light through yonder window breaks?\",\n",
    "    \"To be, or not to be, that is the question: whether 'tis\",\n",
    "]\n",
    "\n",
    "for ax, sample_text in zip(axes, sample_texts):\n",
    "    orig, masked, predicted, conf, mask_bool = full_demo(model, sample_text, t_val=0.4)\n",
    "\n",
    "    for pos in range(SEQ_LEN):\n",
    "        if mask_bool[pos]:\n",
    "            correct = predicted[pos] == orig[pos]\n",
    "            color = '#2e7d32' if correct else '#e53935'\n",
    "            alpha = min(1.0, conf[pos].item() + 0.3)\n",
    "            ax.add_patch(plt.Rectangle((pos, 0), 1, 1, color=color, alpha=alpha))\n",
    "            char = id_to_char.get(predicted[pos].item(), '?')\n",
    "            ax.text(pos + 0.5, 0.5, char, ha='center', va='center',\n",
    "                    fontsize=7, fontweight='bold', color='white')\n",
    "        else:\n",
    "            ax.add_patch(plt.Rectangle((pos, 0), 1, 1, color='#e3f2fd'))\n",
    "            char = id_to_char.get(orig[pos].item(), '?')\n",
    "            ax.text(pos + 0.5, 0.5, char, ha='center', va='center',\n",
    "                    fontsize=7, color='#333333')\n",
    "\n",
    "    ax.set_xlim(0, SEQ_LEN)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.suptitle('Predictions at t=0.4 (green=correct, red=wrong, blue=unmasked)',\n",
    "             fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Landscape\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_11_landscape.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_11_landscape"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The Landscape of Diffusion LLMs\n",
    "\n",
    "Our tiny character-level model is conceptually identical to state-of-the-art diffusion LLMs ‚Äî just scaled up.\n",
    "\n",
    "| Model | Year | Approach | Key Result |\n",
    "|---|---|---|---|\n",
    "| MDLM | 2024 | Masked diffusion | Within 14% of GPT-2 perplexity |\n",
    "| SEDD | 2024 | Score-based discrete | ICML Best Paper |\n",
    "| LLaDA | 2025 | Masked diffusion (8B) | Competitive with LLaMA 3 |\n",
    "| Mercury | 2025 | Diffusion (commercial) | 1,000+ tok/s |\n",
    "| Gemini Diffusion | 2025 | Diffusion (commercial) | 1,479 tok/s |\n",
    "\n",
    "The standout is **LLaDA** ‚Äî an 8B-parameter diffusion LLM that matches LLaMA 3. It even solves the **reversal curse**: when trained that \"A is B,\" it can infer \"B is A\" ‚Äî something GPT-4o cannot do, because it only sees left-to-right context."
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_12_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_12_closing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "\n",
    "1. **Why is the ELBO a lower bound?** It is tight when the reverse process perfectly matches the forward process. Any imperfection in the model's predictions loosens the bound.\n",
    "\n",
    "2. **Should we weight masking ratios non-uniformly?** The theory says $1/t$ weighting is optimal. In practice, uniform works well too. Some papers use cosine schedules over $t$.\n",
    "\n",
    "3. **How would you handle very long sequences?** The Transformer's attention is $O(L^2)$. For long sequences, you would need efficient attention (sparse, linear, or local+global patterns).\n",
    "\n",
    "### üèÜ Optional Challenges\n",
    "\n",
    "1. Try a larger model (more layers, bigger d_model) and see if accuracy improves\n",
    "2. Implement weight tying (embedding and output head share parameters)\n",
    "3. Try word-level tokenization instead of character-level\n",
    "4. Compute the actual ELBO on the validation set and compare to cross-entropy loss\n",
    "\n",
    "---\n",
    "\n",
    "**Up Next ‚Äî Notebook 4:** *Generation ‚Äî Iterative Unmasking.* We will use our trained model to generate text from scratch, starting with all [MASK] tokens and progressively revealing them in order of confidence ‚Äî the grand finale!"
   ],
   "id": "cell_33"
  }
 ]
}