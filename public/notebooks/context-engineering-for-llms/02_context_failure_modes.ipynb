{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Context Failure Modes â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1HXCxUPrYtInnadQfEbZ0gts10ZmgZl9x\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Title Overview\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_title_overview.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_title_overview"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Context Failure Modes: When Good Models Get Bad Context\n",
    "\n",
    "**Part 2 of the Vizuara Context Engineering Series**\n",
    "\n",
    "In Part 1, we learned that context engineering is the art of filling an LLM's context window with exactly the right information. But what happens when we get it *wrong*?\n",
    "\n",
    "In this notebook, we will get our hands dirty with Drew Breunig's taxonomy of four context failure modes. We will not just read about them â€” we will **simulate each one**, measure the damage, and build a diagnostic tool that can detect these failures in any LLM application.\n",
    "\n",
    "**What you will learn:**\n",
    "- How a single hallucination can poison an entire conversation (Context Poisoning)\n",
    "- Why more context often makes models *worse*, not better (Context Distraction)\n",
    "- How irrelevant information degrades model performance (Context Confusion)\n",
    "- Why contradictory instructions cause catastrophic failures (Context Clash)\n",
    "- How to build a Context Health Checker that diagnoses all four failure modes\n",
    "\n",
    "**Time:** ~25 minutes\n",
    "**Prerequisites:** Basic Python, familiarity with text similarity concepts\n",
    "**API keys needed:** None! Everything runs locally with classical NLP."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Surgeon Analogy\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_surgeon_analogy.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_surgeon_analogy"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Imagine you are a brilliant surgeon about to operate. Your skills are world-class. But someone has:\n",
    "- Swapped one of your X-rays with another patient's (poisoning)\n",
    "- Buried the critical lab result on page 47 of a 60-page report (distraction)\n",
    "- Included the patient's dental records, tax returns, and vacation photos in the file (confusion)\n",
    "- Given you two conflicting surgical plans from two different doctors (clash)\n",
    "\n",
    "Would you expect a good outcome? Of course not. **The surgeon's skill is irrelevant if the information environment is broken.**\n",
    "\n",
    "This is exactly what happens to LLMs every day. The model might be GPT-4, Claude, or Gemini â€” it does not matter. Bad context produces bad outputs. And unlike a surgeon, an LLM cannot raise its hand and say, \"Wait, something seems off here.\"\n",
    "\n",
    "Let us build the tools to detect these failures before they cause damage."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Imports Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_imports_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_imports_setup"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition â€” Setup and Imports\n",
    "\n",
    "Before we dive into the failure modes, let us set up our environment. We will use only standard scientific Python libraries â€” no API keys needed."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install any missing dependencies (all standard libraries)\n",
    "!pip install -q numpy matplotlib scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set consistent styling for all our plots\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (10, 6),\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'lines.linewidth': 2,\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white',\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "})\n",
    "\n",
    "# Color palette for our failure modes\n",
    "COLORS = {\n",
    "    'poisoning': '#e74c3c',    # Red\n",
    "    'distraction': '#f39c12',  # Orange\n",
    "    'confusion': '#9b59b6',    # Purple\n",
    "    'clash': '#3498db',        # Blue\n",
    "    'healthy': '#2ecc71',      # Green\n",
    "}\n",
    "\n",
    "print(\"Setup complete! All libraries loaded.\")\n",
    "print(\"No API keys needed â€” everything runs locally.\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Math Foundations\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_math_foundations.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_math_foundations"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics â€” Measuring Context Quality\n",
    "\n",
    "Before we simulate failures, we need a way to **measure** context quality. We will use two key metrics from information retrieval:\n",
    "\n",
    "**TF-IDF (Term Frequency - Inverse Document Frequency)** converts text into numerical vectors based on word importance. Common words like \"the\" get low scores; distinctive words like \"transformer\" get high scores.\n",
    "\n",
    "**Cosine Similarity** measures how similar two text vectors are, on a scale from 0 (completely different) to 1 (identical).\n",
    "\n",
    "Think of it this way: if your query is \"How does attention work in transformers?\" and a context chunk talks about \"self-attention mechanisms in transformer architectures,\" the cosine similarity will be high. If the chunk talks about \"recipes for chocolate cake,\" the similarity will be near zero.\n",
    "\n",
    "These two tools are our stethoscope â€” they let us diagnose context health without needing an actual LLM."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Measurement Functions\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_measurement_functions.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_measurement_functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_relevance(query, documents):\n",
    "    \"\"\"\n",
    "    Compute the relevance of each document to a query using TF-IDF + cosine similarity.\n",
    "\n",
    "    This is the foundation of our context health analysis. In a real RAG system,\n",
    "    you'd use neural embeddings â€” but TF-IDF captures the same intuition:\n",
    "    relevant documents share important words with the query.\n",
    "\n",
    "    Args:\n",
    "        query: The question or task (string)\n",
    "        documents: List of context chunks (list of strings)\n",
    "\n",
    "    Returns:\n",
    "        similarities: Array of relevance scores (0 to 1)\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    # Fit on all text, transform query and documents\n",
    "    all_text = [query] + documents\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_text)\n",
    "\n",
    "    # Cosine similarity between query (index 0) and each document\n",
    "    similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
    "    return similarities\n",
    "\n",
    "\n",
    "def signal_to_noise_ratio(query, documents):\n",
    "    \"\"\"\n",
    "    Compute the signal-to-noise ratio of a context.\n",
    "\n",
    "    Signal = average relevance of the top-K most relevant chunks\n",
    "    Noise = average relevance of everything else\n",
    "\n",
    "    A healthy context has high SNR (lots of signal, little noise).\n",
    "    A sick context has low SNR (noise drowns the signal).\n",
    "    \"\"\"\n",
    "    similarities = compute_relevance(query, documents)\n",
    "\n",
    "    if len(similarities) == 0:\n",
    "        return 0.0, similarities\n",
    "\n",
    "    # Top 30% are \"signal,\" rest is \"noise\"\n",
    "    k = max(1, int(len(similarities) * 0.3))\n",
    "    sorted_sims = np.sort(similarities)[::-1]\n",
    "\n",
    "    signal = np.mean(sorted_sims[:k])\n",
    "    noise = np.mean(sorted_sims[k:]) if len(sorted_sims) > k else 0.001\n",
    "\n",
    "    snr = signal / max(noise, 0.001)  # Avoid division by zero\n",
    "    return snr, similarities\n",
    "\n",
    "\n",
    "# Quick demo\n",
    "demo_query = \"How does the attention mechanism work in transformer models?\"\n",
    "demo_docs = [\n",
    "    \"The self-attention mechanism allows transformers to weigh the importance of different input tokens.\",\n",
    "    \"Batch normalization normalizes layer inputs to stabilize training in deep networks.\",\n",
    "    \"Attention computes query, key, and value matrices to determine token relationships.\",\n",
    "    \"The recipe for chocolate cake requires flour, sugar, eggs, and cocoa powder.\",\n",
    "    \"Multi-head attention runs several attention operations in parallel for richer representations.\",\n",
    "]\n",
    "\n",
    "snr, sims = signal_to_noise_ratio(demo_query, demo_docs)\n",
    "print(f\"Query: '{demo_query[:60]}...'\")\n",
    "print(f\"\\nRelevance scores:\")\n",
    "for doc, sim in zip(demo_docs, sims):\n",
    "    relevance = \"RELEVANT\" if sim > 0.1 else \"NOISE\"\n",
    "    bar = \"â–ˆ\" * int(sim * 40)\n",
    "    print(f\"  [{relevance:>8}] {sim:.3f} {bar} | {doc[:65]}...\")\n",
    "print(f\"\\nSignal-to-Noise Ratio: {snr:.2f}\")\n",
    "print(f\"Interpretation: {'Healthy context' if snr > 3 else 'Context needs improvement' if snr > 1.5 else 'Noisy context!'}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Poisoning Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_poisoning_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_poisoning_intro"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It â€” Simulating the Four Failure Modes\n",
    "\n",
    "Now for the main event. We will simulate each of Drew Breunig's four failure modes, measure the damage quantitatively, and visualize the results.\n",
    "\n",
    "### Failure Mode 1: Context Poisoning\n",
    "\n",
    "Context poisoning happens when a hallucination or error enters the context and gets referenced repeatedly. It is like a student writing the wrong formula on their reference sheet â€” every subsequent answer built on that formula will be wrong, and they will never know why.\n",
    "\n",
    "**The real-world case:** Google built a Gemini-based agent to play Pokemon. The agent hallucinated about game state â€” it believed it had items it did not have, and objectives that did not exist. This false information poisoned the \"goals\" section, and the agent developed completely nonsensical strategies.\n",
    "\n",
    "Let us simulate this. We will start with a clean, accurate context and inject a false fact. Then we will watch how the error propagates as the context grows over multiple turns."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Poisoning Simulation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_poisoning_simulation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_poisoning_simulation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_poisoning(num_turns=10, poison_turn=2):\n",
    "    \"\"\"\n",
    "    Simulate context poisoning over multiple conversation turns.\n",
    "\n",
    "    The setup: We have a knowledge base of true facts about neural networks.\n",
    "    At turn `poison_turn`, a false fact enters the context (like a hallucination).\n",
    "    We then measure how much the context degrades over subsequent turns as\n",
    "    \"responses\" build on the poisoned information.\n",
    "\n",
    "    This mirrors the Gemini Pokemon case: one hallucinated game state\n",
    "    entry corrupted all downstream reasoning.\n",
    "    \"\"\"\n",
    "    # Ground truth knowledge base\n",
    "    true_facts = [\n",
    "        \"Transformers use self-attention to process sequences in parallel.\",\n",
    "        \"The learning rate controls the step size during gradient descent.\",\n",
    "        \"Batch normalization normalizes layer inputs for stable training.\",\n",
    "        \"Dropout randomly deactivates neurons to prevent overfitting.\",\n",
    "        \"Residual connections allow gradients to flow through deep networks.\",\n",
    "        \"Adam optimizer combines momentum with adaptive learning rates.\",\n",
    "        \"Layer normalization normalizes across features within a single example.\",\n",
    "        \"Weight decay adds L2 regularization to prevent large weight values.\",\n",
    "        \"Gradient clipping prevents exploding gradients in deep networks.\",\n",
    "        \"Cross-entropy loss measures divergence between predicted and true distributions.\",\n",
    "    ]\n",
    "\n",
    "    # The poison: a plausible-sounding but WRONG fact\n",
    "    poison_fact = \"Transformers process tokens sequentially from left to right like RNNs.\"\n",
    "\n",
    "    # Downstream \"responses\" that build on the poison (error propagation)\n",
    "    poison_derivatives = [\n",
    "        \"Since transformers are sequential, they cannot parallelize training across tokens.\",\n",
    "        \"The left-to-right processing in transformers limits context to previous tokens only.\",\n",
    "        \"Transformer training is slow because each token must wait for the previous one.\",\n",
    "        \"Bidirectional context is impossible in transformers due to sequential processing.\",\n",
    "        \"Scaling transformers requires reducing sequence length because of sequential bottleneck.\",\n",
    "    ]\n",
    "\n",
    "    context_accuracy = []\n",
    "    context_contents = []\n",
    "\n",
    "    for turn in range(num_turns):\n",
    "        if turn < poison_turn:\n",
    "            # Clean context: only true facts\n",
    "            context = true_facts[:turn + 2]\n",
    "        elif turn == poison_turn:\n",
    "            # Poison injected!\n",
    "            context = true_facts[:turn + 1] + [poison_fact]\n",
    "        else:\n",
    "            # Post-poison: derivatives of the wrong fact accumulate\n",
    "            derivative_idx = min(turn - poison_turn - 1, len(poison_derivatives) - 1)\n",
    "            context = (true_facts[:poison_turn + 1] +\n",
    "                      [poison_fact] +\n",
    "                      poison_derivatives[:derivative_idx + 1] +\n",
    "                      true_facts[poison_turn + 1:turn])\n",
    "\n",
    "        context_contents.append(context)\n",
    "\n",
    "        # Measure accuracy: what fraction of context is true/accurate?\n",
    "        accurate_count = sum(1 for item in context if item in true_facts)\n",
    "        accuracy = accurate_count / len(context)\n",
    "        context_accuracy.append(accuracy)\n",
    "\n",
    "    return context_accuracy, context_contents, poison_turn\n",
    "\n",
    "\n",
    "# Run the simulation\n",
    "accuracy_over_turns, contexts, poison_turn = simulate_poisoning(num_turns=12, poison_turn=3)\n",
    "\n",
    "# Visualize the degradation\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "turns = range(len(accuracy_over_turns))\n",
    "colors_per_turn = [COLORS['healthy'] if t < poison_turn else COLORS['poisoning'] for t in turns]\n",
    "\n",
    "bars = ax.bar(turns, accuracy_over_turns, color=colors_per_turn, edgecolor='white', linewidth=0.5)\n",
    "ax.axvline(x=poison_turn - 0.5, color=COLORS['poisoning'], linestyle='--', linewidth=2,\n",
    "           label=f'Poison injected (turn {poison_turn})')\n",
    "\n",
    "ax.set_xlabel('Conversation Turn')\n",
    "ax.set_ylabel('Context Accuracy (fraction of true facts)')\n",
    "ax.set_title('Context Poisoning: How One Hallucination Degrades Everything')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend(fontsize=11)\n",
    "\n",
    "# Annotate the key insight\n",
    "ax.annotate('Error compounds\\nover time!',\n",
    "            xy=(poison_turn + 4, accuracy_over_turns[poison_turn + 4]),\n",
    "            xytext=(poison_turn + 5, 0.85),\n",
    "            fontsize=11, color=COLORS['poisoning'], fontweight='bold',\n",
    "            arrowprops=dict(arrowstyle='->', color=COLORS['poisoning'], lw=2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nContext accuracy BEFORE poisoning: {accuracy_over_turns[poison_turn-1]:.0%}\")\n",
    "print(f\"Context accuracy AFTER poisoning (final turn): {accuracy_over_turns[-1]:.0%}\")\n",
    "print(f\"Degradation: {accuracy_over_turns[poison_turn-1] - accuracy_over_turns[-1]:.0%} drop\")\n",
    "print(f\"\\nðŸ’¡ Key insight: The poison doesn't just add one wrong fact â€”\")\n",
    "print(f\"   it generates DERIVATIVE errors that compound over turns.\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Poisoning Reflection\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_poisoning_reflection.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_poisoning_reflection"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Distraction Reflection\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_distraction_reflection.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_12_distraction_reflection"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Confusion Reflection\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/15_confusion_reflection.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_15_confusion_reflection"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤” Think About It\n",
    "\n",
    "Notice how the accuracy does not just drop by one fact â€” it **cascades**. Each turn after the poison, new \"responses\" are generated that build on the wrong fact, adding more and more incorrect information to the context.\n",
    "\n",
    "This is exactly what happened with Google's Pokemon agent. One hallucinated game state entry led to a chain of increasingly absurd strategies. The agent was not \"stupid\" â€” it was reasoning perfectly logically from a poisoned premise.\n",
    "\n",
    "**The lesson:** In any multi-turn LLM application, you need mechanisms to validate context entries against ground truth. We will build exactly that in our TODO section."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Distraction Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_distraction_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_distraction_intro"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure Mode 2: Context Distraction â€” \"Lost in the Middle\"\n",
    "\n",
    "This failure mode is backed by one of the most cited LLM research papers of 2023: \"Lost in the Middle\" by Liu et al. They discovered that LLMs attend strongly to information at the **beginning** and **end** of their context, but perform poorly on information buried in the **middle**.\n",
    "\n",
    "The implication is counterintuitive: **more context can make models worse**, not better. Beyond ~100K tokens, agents start repeating actions from their history rather than synthesizing new plans.\n",
    "\n",
    "Let us recreate this famous U-shaped curve."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Distraction Simulation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_distraction_simulation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_11_distraction_simulation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_lost_in_the_middle(num_positions=20, num_trials=50):\n",
    "    \"\"\"\n",
    "    Simulate the 'Lost in the Middle' effect (Liu et al. 2023).\n",
    "\n",
    "    We create a long context with many 'distractor' paragraphs and one\n",
    "    'target' paragraph containing the answer. We vary WHERE the target\n",
    "    appears (beginning, middle, end) and measure retrieval accuracy.\n",
    "\n",
    "    Instead of using an actual LLM, we simulate the attention pattern\n",
    "    that Liu et al. discovered: a U-shaped curve where the model strongly\n",
    "    attends to the beginning and end, but poorly to the middle.\n",
    "    \"\"\"\n",
    "    # The target fact we want the model to find\n",
    "    target = \"The critical breakthrough in transformer efficiency was the introduction of flash attention, which reduces memory from quadratic to linear by computing attention in blocks.\"\n",
    "\n",
    "    # Distractor paragraphs (plausible ML content that is NOT the answer)\n",
    "    distractors = [\n",
    "        \"Neural networks use backpropagation to compute gradients through the chain rule.\",\n",
    "        \"Convolutional neural networks apply learned filters to detect spatial features.\",\n",
    "        \"Recurrent networks maintain hidden state across sequential time steps.\",\n",
    "        \"Generative adversarial networks train a generator and discriminator in opposition.\",\n",
    "        \"Variational autoencoders learn latent representations through probabilistic encoding.\",\n",
    "        \"Graph neural networks propagate information along edges between nodes.\",\n",
    "        \"Reinforcement learning agents maximize cumulative reward through trial and error.\",\n",
    "        \"Transfer learning reuses pretrained features for downstream tasks with less data.\",\n",
    "        \"Data augmentation artificially expands training sets through random transformations.\",\n",
    "        \"Gradient descent iteratively minimizes the loss function by following the negative gradient.\",\n",
    "        \"Batch processing groups multiple samples together for efficient parallel computation.\",\n",
    "        \"Regularization techniques like weight decay prevent models from memorizing training data.\",\n",
    "        \"Hyperparameter tuning searches for optimal model configuration across a defined space.\",\n",
    "        \"Ensemble methods combine predictions from multiple models to reduce variance.\",\n",
    "        \"Knowledge distillation transfers learned representations from large to small models.\",\n",
    "        \"Curriculum learning presents training examples in order of increasing difficulty.\",\n",
    "        \"Few-shot learning enables models to generalize from very limited labeled examples.\",\n",
    "        \"Continual learning addresses catastrophic forgetting when training on new tasks.\",\n",
    "        \"Self-supervised learning creates labels from the data itself without human annotation.\",\n",
    "        \"Neural architecture search automates the design of network topologies.\",\n",
    "    ]\n",
    "\n",
    "    query = \"What was the critical breakthrough in transformer efficiency?\"\n",
    "\n",
    "    # Simulate retrieval accuracy at each position\n",
    "    # We model the U-shaped attention pattern from the paper\n",
    "    positions = np.arange(num_positions)\n",
    "    mid = num_positions / 2\n",
    "\n",
    "    retrieval_accuracies = []\n",
    "    retrieval_std = []\n",
    "\n",
    "    for pos in positions:\n",
    "        trial_scores = []\n",
    "        for _ in range(num_trials):\n",
    "            # Build context with target at position `pos`\n",
    "            context_docs = list(distractors[:num_positions - 1])\n",
    "            context_docs.insert(pos, target)\n",
    "\n",
    "            # Simulate the attention bias:\n",
    "            # - Strong attention to beginning (positions 0-3)\n",
    "            # - Strong attention to end (positions 16-19)\n",
    "            # - Weak attention to middle (positions 7-12)\n",
    "            normalized_pos = pos / (num_positions - 1)  # 0 to 1\n",
    "\n",
    "            # U-shaped attention curve: high at edges, low in middle\n",
    "            # Based on Liu et al. findings\n",
    "            attention_score = (\n",
    "                0.9 * np.exp(-8 * (normalized_pos - 0)**2) +   # Beginning peak\n",
    "                0.85 * np.exp(-8 * (normalized_pos - 1)**2) +  # End peak\n",
    "                0.15                                              # Baseline\n",
    "            )\n",
    "            attention_score = min(attention_score, 1.0)\n",
    "\n",
    "            # Add noise to simulate variability\n",
    "            noisy_score = attention_score + np.random.normal(0, 0.08)\n",
    "            noisy_score = np.clip(noisy_score, 0, 1)\n",
    "\n",
    "            # Retrieval succeeds if attention score exceeds threshold\n",
    "            trial_scores.append(noisy_score)\n",
    "\n",
    "        retrieval_accuracies.append(np.mean(trial_scores))\n",
    "        retrieval_std.append(np.std(trial_scores))\n",
    "\n",
    "    return positions, retrieval_accuracies, retrieval_std\n",
    "\n",
    "\n",
    "# Run the simulation\n",
    "positions, accuracies, stds = simulate_lost_in_the_middle(num_positions=20, num_trials=100)\n",
    "\n",
    "# ðŸ“Š Visualization: The famous U-shaped curve\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.fill_between(positions,\n",
    "                np.array(accuracies) - np.array(stds),\n",
    "                np.array(accuracies) + np.array(stds),\n",
    "                alpha=0.2, color=COLORS['distraction'])\n",
    "ax.plot(positions, accuracies, 'o-', color=COLORS['distraction'], markersize=6, linewidth=2)\n",
    "\n",
    "# Annotate the U-shape\n",
    "ax.annotate('Strong attention\\nat beginning',\n",
    "            xy=(1, accuracies[1]), xytext=(4, 0.95),\n",
    "            fontsize=10, fontweight='bold', color='#27ae60',\n",
    "            arrowprops=dict(arrowstyle='->', color='#27ae60', lw=1.5))\n",
    "\n",
    "ax.annotate('\"Lost in\\nthe Middle\"',\n",
    "            xy=(10, accuracies[10]), xytext=(12, 0.35),\n",
    "            fontsize=11, fontweight='bold', color=COLORS['poisoning'],\n",
    "            arrowprops=dict(arrowstyle='->', color=COLORS['poisoning'], lw=2))\n",
    "\n",
    "ax.annotate('Strong attention\\nat end',\n",
    "            xy=(18, accuracies[18]), xytext=(14, 0.92),\n",
    "            fontsize=10, fontweight='bold', color='#27ae60',\n",
    "            arrowprops=dict(arrowstyle='->', color='#27ae60', lw=1.5))\n",
    "\n",
    "ax.set_xlabel('Position of Key Information in Context')\n",
    "ax.set_ylabel('Retrieval Accuracy (simulated)')\n",
    "ax.set_title('Context Distraction: The \"Lost in the Middle\" Effect (Liu et al. 2023)')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.set_xticks(range(0, 20, 2))\n",
    "ax.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5, label='50% accuracy baseline')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the key finding\n",
    "best_pos = positions[np.argmax(accuracies)]\n",
    "worst_pos = positions[np.argmin(accuracies)]\n",
    "print(f\"\\nðŸ“Š Results:\")\n",
    "print(f\"  Best retrieval position:  {best_pos} (accuracy: {max(accuracies):.2f})\")\n",
    "print(f\"  Worst retrieval position: {worst_pos} (accuracy: {min(accuracies):.2f})\")\n",
    "print(f\"  Drop from best to worst:  {max(accuracies) - min(accuracies):.2f}\")\n",
    "print(f\"\\nðŸ’¡ Key insight: Information placed in the middle of a long context\")\n",
    "print(f\"   is up to {((max(accuracies) - min(accuracies)) / max(accuracies) * 100):.0f}% less likely to be retrieved correctly.\")\n",
    "print(f\"   More context is NOT always better!\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤” Think About It\n",
    "\n",
    "The U-shaped curve tells us something profound: LLMs are not uniform information processors. They have attention biases â€” strong at the edges, weak in the middle. This means:\n",
    "\n",
    "1. **Put your most important information first** (beginning of context)\n",
    "2. **Recency also helps** (end of context)\n",
    "3. **Never bury critical facts in the middle** of a long context\n",
    "\n",
    "This is why reranking matters in RAG pipelines â€” you want to push the most relevant chunks to the top of the context, not let them get lost in the middle of 50 retrieved documents."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Confusion Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_confusion_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_13_confusion_intro"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure Mode 3: Context Confusion â€” When More Tools Means More Failure\n",
    "\n",
    "This is perhaps the most counterintuitive failure mode: giving the model **more capabilities** can make it **worse**. Researchers tested a quantized Llama 3.1 8B and found it failed when given 46 tools but succeeded perfectly with only 19. The extra 27 tools â€” none of which were needed â€” confused the model enough to cause complete failure.\n",
    "\n",
    "The analogy: imagine trying to cook dinner, but instead of just having the recipe and ingredients you need, someone has dumped every kitchen gadget from a professional restaurant onto your counter. The food processor, the sous vide machine, the commercial mixer, the pasta extruder â€” none of which you need for a simple stir-fry. The clutter itself causes mistakes.\n",
    "\n",
    "Let us simulate this."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Confusion Simulation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_confusion_simulation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_14_confusion_simulation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_context_confusion(max_tools=50, step=2, num_trials=30):\n",
    "    \"\"\"\n",
    "    Simulate the context confusion effect: adding irrelevant tool descriptions\n",
    "    degrades a model's ability to select the correct tool.\n",
    "\n",
    "    We simulate a tool-selection task where the model must pick the right tool\n",
    "    for a query from a growing list. As irrelevant tools are added, the\n",
    "    'matching accuracy' drops â€” recreating the 46-vs-19 tools finding.\n",
    "    \"\"\"\n",
    "    # The query and the correct tool\n",
    "    query = \"Calculate the total revenue from Q3 sales data and generate a bar chart.\"\n",
    "\n",
    "    # Relevant tools (would actually help with this task)\n",
    "    relevant_tools = [\n",
    "        {\"name\": \"calculate_sum\", \"desc\": \"Computes the sum of numerical values in a dataset column.\"},\n",
    "        {\"name\": \"filter_by_date\", \"desc\": \"Filters records to a specific date range like quarterly periods.\"},\n",
    "        {\"name\": \"create_bar_chart\", \"desc\": \"Generates a bar chart visualization from categorical data.\"},\n",
    "        {\"name\": \"load_csv\", \"desc\": \"Loads data from a CSV file into a structured table format.\"},\n",
    "        {\"name\": \"export_report\", \"desc\": \"Exports analysis results as a formatted PDF report.\"},\n",
    "    ]\n",
    "\n",
    "    # Irrelevant tools (noise â€” these have nothing to do with the task)\n",
    "    irrelevant_tools = [\n",
    "        {\"name\": \"send_email\", \"desc\": \"Sends an email message to specified recipients.\"},\n",
    "        {\"name\": \"resize_image\", \"desc\": \"Resizes an image to specified pixel dimensions.\"},\n",
    "        {\"name\": \"translate_text\", \"desc\": \"Translates text from one language to another.\"},\n",
    "        {\"name\": \"compress_file\", \"desc\": \"Compresses files into a zip archive format.\"},\n",
    "        {\"name\": \"play_audio\", \"desc\": \"Plays an audio file through the system speakers.\"},\n",
    "        {\"name\": \"set_alarm\", \"desc\": \"Sets a timer or alarm for a specified time.\"},\n",
    "        {\"name\": \"check_weather\", \"desc\": \"Retrieves current weather conditions for a location.\"},\n",
    "        {\"name\": \"convert_currency\", \"desc\": \"Converts amounts between different currencies.\"},\n",
    "        {\"name\": \"merge_pdfs\", \"desc\": \"Combines multiple PDF documents into a single file.\"},\n",
    "        {\"name\": \"crop_video\", \"desc\": \"Trims a video file to a specified time range.\"},\n",
    "        {\"name\": \"spell_check\", \"desc\": \"Checks text for spelling errors and suggests corrections.\"},\n",
    "        {\"name\": \"generate_qr\", \"desc\": \"Creates a QR code from a URL or text string.\"},\n",
    "        {\"name\": \"scrape_webpage\", \"desc\": \"Extracts text content from a webpage URL.\"},\n",
    "        {\"name\": \"encrypt_data\", \"desc\": \"Encrypts data using AES-256 encryption algorithm.\"},\n",
    "        {\"name\": \"schedule_meeting\", \"desc\": \"Creates a calendar event with specified attendees.\"},\n",
    "        {\"name\": \"parse_xml\", \"desc\": \"Parses XML formatted data into a tree structure.\"},\n",
    "        {\"name\": \"bluetooth_scan\", \"desc\": \"Scans for nearby Bluetooth devices and lists them.\"},\n",
    "        {\"name\": \"manage_contacts\", \"desc\": \"Adds, updates, or deletes entries in a contact list.\"},\n",
    "        {\"name\": \"run_diagnostics\", \"desc\": \"Runs system hardware diagnostics and reports status.\"},\n",
    "        {\"name\": \"stream_video\", \"desc\": \"Streams video content from a URL to the display.\"},\n",
    "        {\"name\": \"backup_database\", \"desc\": \"Creates a backup copy of the entire database.\"},\n",
    "        {\"name\": \"calibrate_sensor\", \"desc\": \"Calibrates a connected sensor to baseline readings.\"},\n",
    "        {\"name\": \"print_document\", \"desc\": \"Sends a document to the default printer.\"},\n",
    "        {\"name\": \"defrag_disk\", \"desc\": \"Defragments the hard drive to improve read performance.\"},\n",
    "        {\"name\": \"monitor_network\", \"desc\": \"Monitors network traffic and bandwidth usage.\"},\n",
    "        {\"name\": \"update_firmware\", \"desc\": \"Flashes new firmware to a connected hardware device.\"},\n",
    "        {\"name\": \"record_screen\", \"desc\": \"Captures a video recording of the screen display.\"},\n",
    "        {\"name\": \"sync_cloud\", \"desc\": \"Synchronizes local files with cloud storage service.\"},\n",
    "        {\"name\": \"clean_cache\", \"desc\": \"Clears temporary cached files to free disk space.\"},\n",
    "        {\"name\": \"manage_users\", \"desc\": \"Administers user accounts and access permissions.\"},\n",
    "        {\"name\": \"analyze_logs\", \"desc\": \"Parses and summarizes system log files for errors.\"},\n",
    "        {\"name\": \"test_api\", \"desc\": \"Sends test requests to an API endpoint and validates responses.\"},\n",
    "        {\"name\": \"optimize_images\", \"desc\": \"Reduces image file sizes while preserving quality.\"},\n",
    "        {\"name\": \"migrate_database\", \"desc\": \"Transfers data between different database systems.\"},\n",
    "        {\"name\": \"configure_firewall\", \"desc\": \"Sets up network firewall rules for traffic filtering.\"},\n",
    "        {\"name\": \"convert_format\", \"desc\": \"Converts files between different document formats.\"},\n",
    "        {\"name\": \"track_inventory\", \"desc\": \"Manages and tracks physical inventory stock levels.\"},\n",
    "        {\"name\": \"process_payments\", \"desc\": \"Handles credit card and payment processing transactions.\"},\n",
    "        {\"name\": \"manage_dns\", \"desc\": \"Configures domain name system records for a domain.\"},\n",
    "        {\"name\": \"profile_code\", \"desc\": \"Analyzes code execution to find performance bottlenecks.\"},\n",
    "        {\"name\": \"scan_malware\", \"desc\": \"Scans files and directories for malicious software.\"},\n",
    "        {\"name\": \"generate_invoice\", \"desc\": \"Creates a formatted invoice document with line items.\"},\n",
    "        {\"name\": \"compress_video\", \"desc\": \"Reduces video file size using codec compression.\"},\n",
    "        {\"name\": \"manage_cron\", \"desc\": \"Schedules and manages recurring system tasks.\"},\n",
    "        {\"name\": \"audit_security\", \"desc\": \"Performs a security audit of system configurations.\"},\n",
    "    ]\n",
    "\n",
    "    tool_counts = []\n",
    "    accuracies_mean = []\n",
    "    accuracies_std = []\n",
    "\n",
    "    # Test with increasing numbers of total tools\n",
    "    for num_irrelevant in range(0, max_tools - len(relevant_tools) + 1, step):\n",
    "        total_tools = relevant_tools + irrelevant_tools[:num_irrelevant]\n",
    "        total_count = len(total_tools)\n",
    "\n",
    "        trial_accuracies = []\n",
    "        for _ in range(num_trials):\n",
    "            # Simulate tool selection using TF-IDF similarity\n",
    "            # The model \"picks\" the tool whose description is most similar to the query\n",
    "            tool_descs = [t['desc'] for t in total_tools]\n",
    "            tool_names = [t['name'] for t in total_tools]\n",
    "\n",
    "            vectorizer = TfidfVectorizer(stop_words='english')\n",
    "            tfidf_matrix = vectorizer.fit_transform([query] + tool_descs)\n",
    "            similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
    "\n",
    "            # Add noise to simulate model uncertainty (more tools = more uncertainty)\n",
    "            noise_scale = 0.02 + 0.003 * num_irrelevant  # Noise grows with tool count\n",
    "            noisy_sims = similarities + np.random.normal(0, noise_scale, len(similarities))\n",
    "\n",
    "            # Model selects top-3 tools\n",
    "            selected_indices = np.argsort(noisy_sims)[::-1][:3]\n",
    "            selected_tools = [tool_names[i] for i in selected_indices]\n",
    "\n",
    "            # Accuracy: fraction of selected tools that are actually relevant\n",
    "            relevant_names = {t['name'] for t in relevant_tools}\n",
    "            correct = sum(1 for t in selected_tools if t in relevant_names)\n",
    "            trial_accuracies.append(correct / 3)\n",
    "\n",
    "        tool_counts.append(total_count)\n",
    "        accuracies_mean.append(np.mean(trial_accuracies))\n",
    "        accuracies_std.append(np.std(trial_accuracies))\n",
    "\n",
    "    return tool_counts, accuracies_mean, accuracies_std\n",
    "\n",
    "\n",
    "# Run the simulation\n",
    "tool_counts, acc_mean, acc_std = simulate_context_confusion(max_tools=50, step=2, num_trials=50)\n",
    "\n",
    "# ðŸ“Š Visualization: Accuracy vs. Number of Tools\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "acc_mean = np.array(acc_mean)\n",
    "acc_std = np.array(acc_std)\n",
    "\n",
    "ax.fill_between(tool_counts, acc_mean - acc_std, acc_mean + acc_std,\n",
    "                alpha=0.2, color=COLORS['confusion'])\n",
    "ax.plot(tool_counts, acc_mean, 'o-', color=COLORS['confusion'], markersize=5, linewidth=2)\n",
    "\n",
    "# Mark the 19-tool and 46-tool points (from the research)\n",
    "# Find closest indices\n",
    "idx_19 = min(range(len(tool_counts)), key=lambda i: abs(tool_counts[i] - 19))\n",
    "idx_46 = min(range(len(tool_counts)), key=lambda i: abs(tool_counts[i] - 46))\n",
    "\n",
    "ax.axvline(x=19, color=COLORS['healthy'], linestyle='--', alpha=0.7, linewidth=2, label='19 tools (succeeds)')\n",
    "ax.axvline(x=46, color=COLORS['poisoning'], linestyle='--', alpha=0.7, linewidth=2, label='46 tools (fails)')\n",
    "\n",
    "ax.set_xlabel('Number of Tools in Context')\n",
    "ax.set_ylabel('Tool Selection Accuracy')\n",
    "ax.set_title('Context Confusion: More Tools = Worse Performance')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.legend(fontsize=11)\n",
    "\n",
    "# Annotate\n",
    "ax.annotate('Llama 3.1 8B succeeded\\nwith 19 tools',\n",
    "            xy=(19, acc_mean[idx_19] if idx_19 < len(acc_mean) else 0.8),\n",
    "            xytext=(22, 0.95),\n",
    "            fontsize=10, fontweight='bold', color=COLORS['healthy'],\n",
    "            arrowprops=dict(arrowstyle='->', color=COLORS['healthy'], lw=1.5))\n",
    "\n",
    "ax.annotate('...but FAILED\\nwith 46 tools',\n",
    "            xy=(46, acc_mean[idx_46] if idx_46 < len(acc_mean) else 0.5),\n",
    "            xytext=(35, 0.3),\n",
    "            fontsize=10, fontweight='bold', color=COLORS['poisoning'],\n",
    "            arrowprops=dict(arrowstyle='->', color=COLORS['poisoning'], lw=1.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Results:\")\n",
    "print(f\"  Accuracy with ~19 tools: {acc_mean[idx_19]:.2f}\")\n",
    "print(f\"  Accuracy with ~46 tools: {acc_mean[idx_46]:.2f}\")\n",
    "print(f\"  Degradation: {(acc_mean[idx_19] - acc_mean[idx_46]) / acc_mean[idx_19] * 100:.0f}%\")\n",
    "print(f\"\\nðŸ’¡ Key insight: The 27 extra tools weren't just useless â€” they actively\")\n",
    "print(f\"   HARMED performance. Less is more in context engineering.\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤” Think About It\n",
    "\n",
    "This result has profound implications for anyone building AI agents. Every tool you add to your agent's toolkit has a cost â€” not just in tokens, but in **decision quality**. Each irrelevant tool is noise that the model must process alongside the signal.\n",
    "\n",
    "The practical takeaway: curate your tool sets aggressively. Use **progressive disclosure** â€” only show the model tools that are relevant to the current step. An agent that dynamically loads 5 relevant tools will outperform one that permanently carries 50."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Clash Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/16_clash_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_16_clash_intro"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure Mode 4: Context Clash â€” Contradictory Instructions\n",
    "\n",
    "The final failure mode occurs when different parts of the context contain contradictory instructions. Microsoft and Salesforce found that \"sharding\" prompts â€” splitting instructions across multiple sections â€” dropped performance by 39% on average. Model o3 plummeted from 98.1% to 64.1% accuracy simply because instructions were fragmented.\n",
    "\n",
    "The analogy: imagine your boss sends you two emails. The first says, \"Respond to all customer complaints within 2 hours with a detailed technical explanation.\" The second says, \"Keep all responses under 50 words and avoid technical jargon.\" You cannot satisfy both. The conflict itself is the failure â€” even before you start working."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Clash Simulation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/17_clash_simulation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_17_clash_simulation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_context_clash():\n",
    "    \"\"\"\n",
    "    Simulate context clash by creating pairs of contradictory instructions\n",
    "    and measuring the contradiction level using cosine similarity on\n",
    "    opposing instruction templates.\n",
    "\n",
    "    When instructions contradict each other, the model receives conflicting\n",
    "    signals about what to do â€” leading to degraded, inconsistent outputs.\n",
    "    \"\"\"\n",
    "    # Pairs of instructions: first is the \"base\", second is the \"clash\"\n",
    "    instruction_pairs = [\n",
    "        # (Instruction A, Instruction B, clash category)\n",
    "        (\"Always respond with detailed technical explanations including code examples.\",\n",
    "         \"Keep all responses under 50 words. Never include code.\",\n",
    "         \"Verbosity\"),\n",
    "\n",
    "        (\"Use formal academic language with citations and references.\",\n",
    "         \"Write casually like you're texting a friend. Use slang and emojis.\",\n",
    "         \"Tone\"),\n",
    "\n",
    "        (\"Never make claims without citing peer-reviewed sources.\",\n",
    "         \"Be creative and speculative. Explore bold hypotheses freely.\",\n",
    "         \"Rigor\"),\n",
    "\n",
    "        (\"Process all data locally. Never send user data to external services.\",\n",
    "         \"Use the cloud API to analyze user data for personalized recommendations.\",\n",
    "         \"Privacy\"),\n",
    "\n",
    "        (\"Always present multiple perspectives and let the user decide.\",\n",
    "         \"Give direct, definitive answers. Users want confidence, not hedging.\",\n",
    "         \"Decisiveness\"),\n",
    "\n",
    "        (\"Prioritize speed. Give the fastest possible response.\",\n",
    "         \"Take your time to think through every angle before responding.\",\n",
    "         \"Speed\"),\n",
    "\n",
    "        (\"Focus exclusively on the user's specific question. Stay on topic.\",\n",
    "         \"Proactively provide related context, background, and tangential insights.\",\n",
    "         \"Scope\"),\n",
    "\n",
    "        (\"Always ask clarifying questions before proceeding.\",\n",
    "         \"Never ask questions. Infer the user's intent and act immediately.\",\n",
    "         \"Interaction\"),\n",
    "    ]\n",
    "\n",
    "    # Build the full instruction set (as if all were in one system prompt)\n",
    "    all_instructions_a = [pair[0] for pair in instruction_pairs]\n",
    "    all_instructions_b = [pair[1] for pair in instruction_pairs]\n",
    "    categories = [pair[2] for pair in instruction_pairs]\n",
    "\n",
    "    # Compute pairwise contradiction scores\n",
    "    # We measure similarity between instruction A and instruction B in each pair\n",
    "    # High similarity + opposite meaning = clash\n",
    "    # We also check cross-pair interactions\n",
    "    all_instructions = all_instructions_a + all_instructions_b\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_instructions)\n",
    "    sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    n = len(instruction_pairs)\n",
    "\n",
    "    # Extract the clash scores (A_i vs B_i for each pair)\n",
    "    clash_scores = []\n",
    "    for i in range(n):\n",
    "        # Similarity between instruction A_i and instruction B_i\n",
    "        score = sim_matrix[i, n + i]\n",
    "        clash_scores.append(score)\n",
    "\n",
    "    # Simulate performance degradation with increasing clashes\n",
    "    num_clash_levels = range(0, n + 1)\n",
    "    performance_scores = []\n",
    "\n",
    "    base_performance = 0.95  # Performance with clean, consistent instructions\n",
    "\n",
    "    for num_clashes in num_clash_levels:\n",
    "        if num_clashes == 0:\n",
    "            performance_scores.append(base_performance)\n",
    "        else:\n",
    "            # Each clash degrades performance\n",
    "            # The degradation compounds (not just additive)\n",
    "            degradation = 1.0\n",
    "            for i in range(num_clashes):\n",
    "                # Higher similarity between contradicting pairs = worse confusion\n",
    "                # The model \"sees\" related words but opposite meanings\n",
    "                clash_penalty = 0.85 + 0.10 * clash_scores[i]  # 0.85 to 0.95 multiplier\n",
    "                degradation *= clash_penalty\n",
    "\n",
    "            performance_scores.append(base_performance * degradation)\n",
    "\n",
    "    return instruction_pairs, clash_scores, categories, num_clash_levels, performance_scores, sim_matrix, n\n",
    "\n",
    "\n",
    "# Run the simulation\n",
    "pairs, clash_scores, categories, clash_levels, perf_scores, sim_matrix, n = simulate_context_clash()\n",
    "\n",
    "# ðŸ“Š Visualization: Contradiction heatmap\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left panel: Clash scores by category\n",
    "ax1 = axes[0]\n",
    "bars = ax1.barh(categories, clash_scores, color=COLORS['clash'], edgecolor='white')\n",
    "ax1.set_xlabel('Contradiction Score (TF-IDF Similarity)')\n",
    "ax1.set_title('Contradiction Strength by Category')\n",
    "ax1.set_xlim(0, max(clash_scores) * 1.3)\n",
    "\n",
    "# Color bars by severity\n",
    "for bar, score in zip(bars, clash_scores):\n",
    "    if score > 0.15:\n",
    "        bar.set_color(COLORS['poisoning'])\n",
    "    elif score > 0.08:\n",
    "        bar.set_color(COLORS['distraction'])\n",
    "    else:\n",
    "        bar.set_color(COLORS['clash'])\n",
    "\n",
    "# Right panel: Performance vs number of clashing instruction pairs\n",
    "ax2 = axes[1]\n",
    "ax2.plot(list(clash_levels), perf_scores, 'o-', color=COLORS['clash'], markersize=8, linewidth=2)\n",
    "ax2.fill_between(list(clash_levels), perf_scores, alpha=0.15, color=COLORS['clash'])\n",
    "\n",
    "# Mark the Microsoft/Salesforce finding\n",
    "ax2.axhline(y=0.641, color=COLORS['poisoning'], linestyle=':', alpha=0.7,\n",
    "            label='o3 with fragmented instructions (64.1%)')\n",
    "ax2.axhline(y=0.981, color=COLORS['healthy'], linestyle=':', alpha=0.7,\n",
    "            label='o3 with unified instructions (98.1%)')\n",
    "\n",
    "ax2.set_xlabel('Number of Contradictory Instruction Pairs')\n",
    "ax2.set_ylabel('Simulated Task Performance')\n",
    "ax2.set_title('Performance Degrades with More Clashes')\n",
    "ax2.set_ylim(0, 1.05)\n",
    "ax2.legend(fontsize=9, loc='lower left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Instruction Clash Analysis:\")\n",
    "for cat, score, (a, b, _) in zip(categories, clash_scores, pairs):\n",
    "    severity = \"HIGH\" if score > 0.15 else \"MEDIUM\" if score > 0.08 else \"LOW\"\n",
    "    print(f\"\\n  [{severity:>6}] {cat} (score: {score:.3f})\")\n",
    "    print(f\"    A: \\\"{a[:70]}...\\\"\")\n",
    "    print(f\"    B: \\\"{b[:70]}...\\\"\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Key insight: o3 dropped from 98.1% to 64.1% accuracy â€” a 34-point\")\n",
    "print(f\"   collapse â€” just from fragmenting instructions. Unified > sharded.\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Your Turn Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/18_your_turn_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_18_your_turn_intro"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn â€” Build the Detection Tools ðŸ”§\n",
    "\n",
    "Now it is your turn. You have seen the four failure modes in action. Let us build the tools to **detect** them automatically. These functions will form the core of our Context Health Checker."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo1\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/19_todo1.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_19_todo1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1: Detect Context Poisoning\n",
    "\n",
    "Your task: implement `detect_poisoning(context_statements, known_facts)` that checks if any statements in the context contradict known facts.\n",
    "\n",
    "**Approach:**\n",
    "1. Use TF-IDF to vectorize all statements and known facts together\n",
    "2. For each context statement, find its most similar known fact\n",
    "3. If a statement is *topically related* (similarity > 0.1) but *not a match* (similarity < 0.5), flag it as potentially poisoned â€” it is talking about the same topic but saying something different\n",
    "4. Return a list of flagged statements with their confidence scores"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_poisoning(context_statements, known_facts, topic_threshold=0.1, match_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Detect potential context poisoning by comparing context statements\n",
    "    against a knowledge base of known facts.\n",
    "\n",
    "    A statement is flagged as potentially poisoned if:\n",
    "    - It is topically related to a known fact (similarity > topic_threshold)\n",
    "    - But does NOT closely match it (similarity < match_threshold)\n",
    "    This suggests it's discussing the same topic but with different (possibly wrong) claims.\n",
    "\n",
    "    Args:\n",
    "        context_statements: List of statements currently in the context\n",
    "        known_facts: List of verified true facts\n",
    "        topic_threshold: Minimum similarity to be considered same-topic (default: 0.1)\n",
    "        match_threshold: Minimum similarity to be considered a match (default: 0.5)\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with 'statement', 'closest_fact', 'similarity', 'flagged' keys\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Combine all text and fit a TF-IDF vectorizer\n",
    "    all_text = context_statements + known_facts\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_text)\n",
    "\n",
    "    # Step 2: Split the matrix into context vectors and fact vectors\n",
    "    n_context = len(context_statements)\n",
    "    context_vectors = tfidf_matrix[:n_context]\n",
    "    fact_vectors = tfidf_matrix[n_context:]\n",
    "\n",
    "    # Step 3: Compute similarity between each context statement and all known facts\n",
    "    sim_matrix = cosine_similarity(context_vectors, fact_vectors)\n",
    "\n",
    "    # Step 4: For each context statement, find the most similar known fact\n",
    "    results = []\n",
    "    for i, statement in enumerate(context_statements):\n",
    "        best_fact_idx = np.argmax(sim_matrix[i])\n",
    "        best_similarity = sim_matrix[i, best_fact_idx]\n",
    "        closest_fact = known_facts[best_fact_idx]\n",
    "\n",
    "        # Step 5: Flag if topically related but not a close match\n",
    "        flagged = (best_similarity > topic_threshold) and (best_similarity < match_threshold)\n",
    "\n",
    "        results.append({\n",
    "            'statement': statement,\n",
    "            'closest_fact': closest_fact,\n",
    "            'similarity': best_similarity,\n",
    "            'flagged': flagged\n",
    "        })\n",
    "\n",
    "    return results\n",
    "    # ============ END TODO ============\n",
    "\n",
    "\n",
    "# âœ… Test your implementation\n",
    "known_facts = [\n",
    "    \"Transformers use self-attention to process sequences in parallel.\",\n",
    "    \"The learning rate controls the step size during gradient descent.\",\n",
    "    \"Dropout randomly deactivates neurons to prevent overfitting.\",\n",
    "    \"Adam optimizer combines momentum with adaptive learning rates.\",\n",
    "    \"Batch normalization normalizes layer inputs to stabilize training.\",\n",
    "]\n",
    "\n",
    "context = [\n",
    "    \"Transformers use self-attention to process sequences in parallel.\",          # True - should NOT be flagged\n",
    "    \"Transformers process tokens sequentially from left to right like RNNs.\",     # FALSE - SHOULD be flagged\n",
    "    \"The learning rate controls the step size during gradient descent.\",          # True - should NOT be flagged\n",
    "    \"Dropout makes all neurons always active to maximize network capacity.\",      # FALSE - SHOULD be flagged\n",
    "    \"Convolutional neural networks detect spatial features using learned filters.\", # Unrelated - should NOT be flagged\n",
    "]\n",
    "\n",
    "results = detect_poisoning(context, known_facts)\n",
    "\n",
    "print(\"ðŸ” Poisoning Detection Results:\")\n",
    "print(\"=\" * 80)\n",
    "for r in results:\n",
    "    status = \"âš ï¸  POISONED?\" if r['flagged'] else \"âœ… OK\"\n",
    "    print(f\"\\n{status} (similarity: {r['similarity']:.3f})\")\n",
    "    print(f\"  Statement: \\\"{r['statement'][:75]}\\\"\")\n",
    "    if r['flagged']:\n",
    "        print(f\"  Closest fact: \\\"{r['closest_fact'][:75]}\\\"\")\n",
    "        print(f\"  â†’ Talks about same topic but says something DIFFERENT!\")\n",
    "\n",
    "flagged_count = sum(1 for r in results if r['flagged'])\n",
    "print(f\"\\nðŸ“Š Summary: {flagged_count}/{len(context)} statements flagged as potentially poisoned\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo2\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/20_todo2.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_20_todo2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Measure Context Confusion Score\n",
    "\n",
    "Your task: implement `measure_confusion_score(context_chunks, query)` that measures what fraction of the context is irrelevant to the query.\n",
    "\n",
    "**Approach:**\n",
    "1. Compute TF-IDF relevance of each chunk to the query\n",
    "2. A chunk is \"irrelevant\" if its similarity to the query is below a threshold\n",
    "3. The confusion score = (number of irrelevant chunks) / (total chunks)\n",
    "4. A score of 0.0 means perfect context; 1.0 means entirely irrelevant noise"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_confusion_score(context_chunks, query, relevance_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Measure the confusion score of a context: what fraction is irrelevant noise?\n",
    "\n",
    "    A high confusion score means the model is being forced to process lots of\n",
    "    irrelevant information alongside the signal â€” exactly the condition that\n",
    "    caused Llama 3.1 8B to fail with 46 tools.\n",
    "\n",
    "    Args:\n",
    "        context_chunks: List of text chunks in the current context\n",
    "        query: The current query/task\n",
    "        relevance_threshold: Minimum similarity to be considered \"relevant\" (default: 0.05)\n",
    "\n",
    "    Returns:\n",
    "        confusion_score: Float from 0.0 (all relevant) to 1.0 (all noise)\n",
    "        chunk_relevances: List of (chunk, relevance_score, is_relevant) tuples\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Compute relevance of each chunk to the query using TF-IDF\n",
    "    if not context_chunks:\n",
    "        return 0.0, []\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    all_text = [query] + context_chunks\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_text)\n",
    "\n",
    "    # Step 2: Cosine similarity between query and each chunk\n",
    "    similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
    "\n",
    "    # Step 3: Classify each chunk as relevant or irrelevant\n",
    "    chunk_relevances = []\n",
    "    irrelevant_count = 0\n",
    "    for chunk, sim in zip(context_chunks, similarities):\n",
    "        is_relevant = sim >= relevance_threshold\n",
    "        if not is_relevant:\n",
    "            irrelevant_count += 1\n",
    "        chunk_relevances.append((chunk, float(sim), is_relevant))\n",
    "\n",
    "    # Step 4: Confusion score = fraction of irrelevant chunks\n",
    "    confusion_score = irrelevant_count / len(context_chunks)\n",
    "\n",
    "    return confusion_score, chunk_relevances\n",
    "    # ============ END TODO ============\n",
    "\n",
    "\n",
    "# âœ… Test your implementation\n",
    "query = \"How does the attention mechanism work in transformer models?\"\n",
    "\n",
    "# Mix of relevant and irrelevant chunks\n",
    "chunks = [\n",
    "    \"Self-attention computes query, key, and value matrices to determine token relationships.\",\n",
    "    \"Multi-head attention runs multiple attention operations in parallel.\",\n",
    "    \"The recipe for chocolate cake requires flour, sugar, eggs, and cocoa powder.\",\n",
    "    \"Attention scores are computed as softmax of the dot product of queries and keys.\",\n",
    "    \"The 2024 Olympic Games were held in Paris, France.\",\n",
    "    \"Stock market indices showed mixed results in the third quarter.\",\n",
    "    \"Scaled dot-product attention divides by the square root of the key dimension.\",\n",
    "    \"The best Italian restaurants in New York are located in Little Italy.\",\n",
    "    \"Weather forecasts predict rain for the upcoming weekend.\",\n",
    "    \"Positional encoding adds position information since attention is permutation-invariant.\",\n",
    "]\n",
    "\n",
    "confusion, details = measure_confusion_score(chunks, query)\n",
    "\n",
    "print(f\"ðŸ” Context Confusion Analysis for query:\")\n",
    "print(f\"   \\\"{query}\\\"\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for chunk, relevance, is_relevant in details:\n",
    "    status = \"âœ… RELEVANT\" if is_relevant else \"âŒ NOISE\"\n",
    "    bar = \"â–ˆ\" * int(relevance * 60)\n",
    "    print(f\"\\n  [{status}] relevance: {relevance:.3f} {bar}\")\n",
    "    print(f\"    \\\"{chunk[:75]}\\\"\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Confusion Score: {confusion:.1%}\")\n",
    "print(f\"   {sum(1 for _, _, r in details if not r)}/{len(chunks)} chunks are irrelevant noise\")\n",
    "if confusion > 0.5:\n",
    "    print(f\"   âš ï¸  WARNING: More than half the context is noise! This will degrade performance.\")\n",
    "elif confusion > 0.3:\n",
    "    print(f\"   âš ï¸  CAUTION: Significant noise in context. Consider pruning irrelevant chunks.\")\n",
    "else:\n",
    "    print(f\"   âœ… Context is reasonably focused. Good signal-to-noise ratio.\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo3\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/21_todo3.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_21_todo3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 3: Detect Context Clashes\n",
    "\n",
    "Your task: implement `detect_clashes(instructions)` that finds contradictory instruction pairs.\n",
    "\n",
    "**Approach:**\n",
    "1. Vectorize all instructions using TF-IDF\n",
    "2. Compute pairwise cosine similarity between all instructions\n",
    "3. Two instructions \"clash\" if they are topically related (moderate similarity) â€” this means they address the same concern but likely with different directives\n",
    "4. Return the most likely clashing pairs\n",
    "\n",
    "The key insight: truly contradictory instructions often share many of the same keywords (same topic) but use them in opposing ways. Pure keyword overlap (high similarity) is fine â€” that just means instructions are consistent. It is the moderate-similarity pairs that are suspicious."
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_clashes(instructions, clash_range=(0.08, 0.45)):\n",
    "    \"\"\"\n",
    "    Detect potentially clashing instruction pairs.\n",
    "\n",
    "    Instructions that are moderately similar (same topic, different directive)\n",
    "    are the most likely to clash. Very high similarity = consistent/redundant.\n",
    "    Very low similarity = different topics entirely (no clash risk).\n",
    "\n",
    "    Args:\n",
    "        instructions: List of instruction strings\n",
    "        clash_range: (min, max) similarity range that suggests a clash\n",
    "\n",
    "    Returns:\n",
    "        clashes: List of dicts with 'instruction_a', 'instruction_b', 'similarity', 'risk'\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    if len(instructions) < 2:\n",
    "        return []\n",
    "\n",
    "    # Step 1: Vectorize all instructions\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(instructions)\n",
    "\n",
    "    # Step 2: Compute pairwise similarities\n",
    "    sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Step 3: Find pairs in the clash range\n",
    "    clashes = []\n",
    "    n = len(instructions)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            sim = sim_matrix[i, j]\n",
    "            if clash_range[0] <= sim <= clash_range[1]:\n",
    "                # Compute risk level based on how deep in the clash range\n",
    "                mid = (clash_range[0] + clash_range[1]) / 2\n",
    "                risk = 1.0 - abs(sim - mid) / (mid - clash_range[0])\n",
    "                risk = max(0.0, min(1.0, risk))\n",
    "\n",
    "                clashes.append({\n",
    "                    'instruction_a': instructions[i],\n",
    "                    'instruction_b': instructions[j],\n",
    "                    'similarity': float(sim),\n",
    "                    'risk': risk,\n",
    "                    'index_a': i,\n",
    "                    'index_b': j,\n",
    "                })\n",
    "\n",
    "    # Sort by risk (highest first)\n",
    "    clashes.sort(key=lambda x: x['risk'], reverse=True)\n",
    "    return clashes\n",
    "    # ============ END TODO ============\n",
    "\n",
    "\n",
    "# âœ… Test your implementation\n",
    "instructions = [\n",
    "    \"Always respond with detailed technical explanations including code examples.\",\n",
    "    \"Keep all responses under 50 words. Never include code.\",\n",
    "    \"Use formal academic language with citations.\",\n",
    "    \"Write casually like you're texting a friend.\",\n",
    "    \"Never make claims without peer-reviewed sources.\",\n",
    "    \"Be creative and speculative. Explore bold hypotheses.\",\n",
    "    \"Focus exclusively on the user's question. Stay on topic.\",\n",
    "    \"Proactively provide related context and tangential insights.\",\n",
    "    \"Always ask clarifying questions before proceeding.\",\n",
    "    \"Never ask questions. Infer intent and act immediately.\",\n",
    "]\n",
    "\n",
    "clashes = detect_clashes(instructions)\n",
    "\n",
    "print(\"ðŸ” Context Clash Detection Results:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not clashes:\n",
    "    print(\"  No clashes detected!\")\n",
    "else:\n",
    "    for i, clash in enumerate(clashes[:8]):  # Show top 8\n",
    "        risk_label = \"HIGH\" if clash['risk'] > 0.7 else \"MEDIUM\" if clash['risk'] > 0.4 else \"LOW\"\n",
    "        risk_color = \"ðŸ”´\" if clash['risk'] > 0.7 else \"ðŸŸ¡\" if clash['risk'] > 0.4 else \"ðŸŸ¢\"\n",
    "        print(f\"\\n{risk_color} Clash #{i+1} [{risk_label}] (similarity: {clash['similarity']:.3f}, risk: {clash['risk']:.2f})\")\n",
    "        print(f\"  A: \\\"{clash['instruction_a'][:75]}\\\"\")\n",
    "        print(f\"  B: \\\"{clash['instruction_b'][:75]}\\\"\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Summary: {len(clashes)} potential clashes detected in {len(instructions)} instructions\")\n",
    "if len(clashes) > 5:\n",
    "    print(f\"   âš ï¸  WARNING: Many conflicting instructions! Consider consolidating your prompt.\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Health Checker Class\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/22_health_checker_class.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_22_health_checker_class"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together â€” The Context Health Checker\n",
    "\n",
    "Now let us combine all four detection tools into a single diagnostic class. This is the tool you will take with you â€” a context health checker that scores any LLM context on all four failure modes and gives actionable recommendations."
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextHealthChecker:\n",
    "    \"\"\"\n",
    "    A diagnostic tool that analyzes LLM context for all four failure modes:\n",
    "    1. Poisoning: false facts that compound over turns\n",
    "    2. Distraction: important info buried in long context\n",
    "    3. Confusion: irrelevant noise drowning the signal\n",
    "    4. Clash: contradictory instructions\n",
    "\n",
    "    Usage:\n",
    "        checker = ContextHealthChecker()\n",
    "        report = checker.diagnose(context_chunks, query, known_facts, instructions)\n",
    "        checker.print_report(report)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, poisoning_thresholds=(0.1, 0.5),\n",
    "                 relevance_threshold=0.05,\n",
    "                 clash_range=(0.08, 0.45)):\n",
    "        self.topic_threshold, self.match_threshold = poisoning_thresholds\n",
    "        self.relevance_threshold = relevance_threshold\n",
    "        self.clash_range = clash_range\n",
    "\n",
    "    def _check_poisoning(self, context_statements, known_facts):\n",
    "        \"\"\"Check for potential context poisoning.\"\"\"\n",
    "        if not known_facts or not context_statements:\n",
    "            return {'score': 0.0, 'flagged': [], 'status': 'NO_DATA'}\n",
    "\n",
    "        results = detect_poisoning(\n",
    "            context_statements, known_facts,\n",
    "            self.topic_threshold, self.match_threshold\n",
    "        )\n",
    "        flagged = [r for r in results if r['flagged']]\n",
    "        score = len(flagged) / len(context_statements) if context_statements else 0\n",
    "        return {\n",
    "            'score': score,\n",
    "            'flagged': flagged,\n",
    "            'total_checked': len(context_statements),\n",
    "            'status': 'CRITICAL' if score > 0.3 else 'WARNING' if score > 0.1 else 'HEALTHY'\n",
    "        }\n",
    "\n",
    "    def _check_distraction(self, context_chunks, query):\n",
    "        \"\"\"Check for the 'Lost in the Middle' risk based on context length.\"\"\"\n",
    "        if not context_chunks:\n",
    "            return {'score': 0.0, 'status': 'NO_DATA'}\n",
    "\n",
    "        num_chunks = len(context_chunks)\n",
    "\n",
    "        # Risk increases with context length\n",
    "        # Based on Liu et al.: problems start around 10+ chunks\n",
    "        if num_chunks <= 5:\n",
    "            risk = 0.1\n",
    "        elif num_chunks <= 10:\n",
    "            risk = 0.3\n",
    "        elif num_chunks <= 20:\n",
    "            risk = 0.6\n",
    "        else:\n",
    "            risk = min(0.95, 0.6 + 0.01 * (num_chunks - 20))\n",
    "\n",
    "        # Check if key information is in the danger zone (middle 60%)\n",
    "        relevances = compute_relevance(query, context_chunks)\n",
    "        top_idx = np.argmax(relevances)\n",
    "        position_ratio = top_idx / max(len(context_chunks) - 1, 1)\n",
    "\n",
    "        # Is the most relevant chunk in the middle (danger zone)?\n",
    "        in_danger_zone = 0.2 < position_ratio < 0.8\n",
    "        if in_danger_zone:\n",
    "            risk = min(1.0, risk * 1.5)\n",
    "\n",
    "        return {\n",
    "            'score': risk,\n",
    "            'num_chunks': num_chunks,\n",
    "            'most_relevant_position': top_idx,\n",
    "            'most_relevant_position_ratio': position_ratio,\n",
    "            'in_danger_zone': in_danger_zone,\n",
    "            'status': 'CRITICAL' if risk > 0.7 else 'WARNING' if risk > 0.4 else 'HEALTHY'\n",
    "        }\n",
    "\n",
    "    def _check_confusion(self, context_chunks, query):\n",
    "        \"\"\"Check for context confusion (irrelevant noise).\"\"\"\n",
    "        if not context_chunks:\n",
    "            return {'score': 0.0, 'status': 'NO_DATA'}\n",
    "\n",
    "        confusion_score, chunk_details = measure_confusion_score(\n",
    "            context_chunks, query, self.relevance_threshold\n",
    "        )\n",
    "\n",
    "        irrelevant = [(c, s) for c, s, r in chunk_details if not r]\n",
    "        return {\n",
    "            'score': confusion_score,\n",
    "            'irrelevant_count': len(irrelevant),\n",
    "            'total_chunks': len(context_chunks),\n",
    "            'worst_offenders': irrelevant[:3],  # Top 3 most irrelevant\n",
    "            'status': 'CRITICAL' if confusion_score > 0.5 else 'WARNING' if confusion_score > 0.3 else 'HEALTHY'\n",
    "        }\n",
    "\n",
    "    def _check_clash(self, instructions):\n",
    "        \"\"\"Check for contradictory instructions.\"\"\"\n",
    "        if not instructions or len(instructions) < 2:\n",
    "            return {'score': 0.0, 'clashes': [], 'status': 'NO_DATA'}\n",
    "\n",
    "        clashes = detect_clashes(instructions, self.clash_range)\n",
    "\n",
    "        # Score based on number and severity of clashes\n",
    "        if not clashes:\n",
    "            score = 0.0\n",
    "        else:\n",
    "            max_possible = len(instructions) * (len(instructions) - 1) / 2\n",
    "            score = min(1.0, len(clashes) / max_possible * 2)  # Scale up\n",
    "\n",
    "        return {\n",
    "            'score': score,\n",
    "            'clashes': clashes,\n",
    "            'num_clashes': len(clashes),\n",
    "            'num_instructions': len(instructions),\n",
    "            'status': 'CRITICAL' if score > 0.5 else 'WARNING' if score > 0.2 else 'HEALTHY'\n",
    "        }\n",
    "\n",
    "    def diagnose(self, context_chunks, query, known_facts=None, instructions=None):\n",
    "        \"\"\"\n",
    "        Run a full diagnostic on the context.\n",
    "\n",
    "        Args:\n",
    "            context_chunks: List of text chunks in the current context\n",
    "            query: The current query/task\n",
    "            known_facts: Optional list of known-true facts for poisoning check\n",
    "            instructions: Optional list of instructions for clash check\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with results for each failure mode + overall health score\n",
    "        \"\"\"\n",
    "        poisoning = self._check_poisoning(context_chunks, known_facts or [])\n",
    "        distraction = self._check_distraction(context_chunks, query)\n",
    "        confusion = self._check_confusion(context_chunks, query)\n",
    "        clash = self._check_clash(instructions or [])\n",
    "\n",
    "        # Overall health: weighted average (inverse of risk scores)\n",
    "        scores = []\n",
    "        weights = []\n",
    "        for check, weight in [(poisoning, 3), (distraction, 2), (confusion, 2), (clash, 3)]:\n",
    "            if check['status'] != 'NO_DATA':\n",
    "                scores.append(1.0 - check['score'])\n",
    "                weights.append(weight)\n",
    "\n",
    "        overall_health = np.average(scores, weights=weights) if scores else 1.0\n",
    "\n",
    "        return {\n",
    "            'poisoning': poisoning,\n",
    "            'distraction': distraction,\n",
    "            'confusion': confusion,\n",
    "            'clash': clash,\n",
    "            'overall_health': overall_health,\n",
    "            'overall_status': ('CRITICAL' if overall_health < 0.5 else\n",
    "                             'WARNING' if overall_health < 0.7 else 'HEALTHY'),\n",
    "        }\n",
    "\n",
    "    def print_report(self, report):\n",
    "        \"\"\"Print a formatted health report.\"\"\"\n",
    "        status_emoji = {\n",
    "            'HEALTHY': 'âœ…',\n",
    "            'WARNING': 'âš ï¸ ',\n",
    "            'CRITICAL': 'ðŸ”´',\n",
    "            'NO_DATA': 'â¬œ'\n",
    "        }\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"       CONTEXT HEALTH REPORT\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Overall\n",
    "        overall = report['overall_status']\n",
    "        print(f\"\\n{status_emoji[overall]} Overall Health: {report['overall_health']:.0%} ({overall})\")\n",
    "\n",
    "        # Poisoning\n",
    "        p = report['poisoning']\n",
    "        print(f\"\\n{status_emoji[p['status']]} Poisoning Risk: {p['score']:.0%}\")\n",
    "        if p['status'] != 'NO_DATA' and p['flagged']:\n",
    "            for f in p['flagged'][:2]:\n",
    "                print(f\"    â†’ Suspicious: \\\"{f['statement'][:60]}...\\\"\")\n",
    "\n",
    "        # Distraction\n",
    "        d = report['distraction']\n",
    "        print(f\"\\n{status_emoji[d['status']]} Distraction Risk: {d['score']:.0%}\")\n",
    "        if d['status'] != 'NO_DATA':\n",
    "            print(f\"    Context length: {d['num_chunks']} chunks\")\n",
    "            if d.get('in_danger_zone'):\n",
    "                print(f\"    â†’ Key info at position {d['most_relevant_position']} (DANGER ZONE!)\")\n",
    "\n",
    "        # Confusion\n",
    "        c = report['confusion']\n",
    "        print(f\"\\n{status_emoji[c['status']]} Confusion Score: {c['score']:.0%}\")\n",
    "        if c['status'] != 'NO_DATA':\n",
    "            print(f\"    {c['irrelevant_count']}/{c['total_chunks']} chunks are noise\")\n",
    "\n",
    "        # Clash\n",
    "        cl = report['clash']\n",
    "        print(f\"\\n{status_emoji[cl['status']]} Clash Risk: {cl['score']:.0%}\")\n",
    "        if cl['status'] != 'NO_DATA' and cl['clashes']:\n",
    "            print(f\"    {cl['num_clashes']} contradictory pairs found\")\n",
    "\n",
    "        # Recommendations\n",
    "        print(f\"\\n{'â”€' * 60}\")\n",
    "        print(\"RECOMMENDATIONS:\")\n",
    "        if p['status'] in ('WARNING', 'CRITICAL'):\n",
    "            print(\"  1. Add fact-checking against a knowledge base\")\n",
    "            print(\"     before including LLM outputs in context.\")\n",
    "        if d['status'] in ('WARNING', 'CRITICAL'):\n",
    "            print(\"  2. Shorten context or move key info to the\")\n",
    "            print(\"     beginning. Consider context compression.\")\n",
    "        if c['status'] in ('WARNING', 'CRITICAL'):\n",
    "            print(\"  3. Prune irrelevant chunks. Use progressive\")\n",
    "            print(\"     disclosure â€” only load what's needed now.\")\n",
    "        if cl['status'] in ('WARNING', 'CRITICAL'):\n",
    "            print(\"  4. Consolidate contradictory instructions into\")\n",
    "            print(\"     a single, unified directive.\")\n",
    "        if report['overall_status'] == 'HEALTHY':\n",
    "            print(\"  Your context looks healthy! No major issues detected.\")\n",
    "\n",
    "        print(\"=\" * 60)"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Sick Context Demo\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/23_sick_context_demo.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_23_sick_context_demo"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us run the health checker on a realistic scenario â€” a context that has all four problems:"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a realistic \"sick\" context that has all four problems\n",
    "query = \"How do transformer models handle long-range dependencies?\"\n",
    "\n",
    "# Context chunks: mix of relevant, irrelevant, and poisoned content\n",
    "context_chunks = [\n",
    "    # Relevant\n",
    "    \"Self-attention allows each token to attend to every other token regardless of distance.\",\n",
    "    \"Positional encoding adds position information since attention is permutation-invariant.\",\n",
    "    \"Multi-head attention enables the model to jointly attend to information from different subspaces.\",\n",
    "\n",
    "    # Poisoned (wrong facts)\n",
    "    \"Transformers process tokens sequentially from left to right, limiting long-range dependency handling.\",\n",
    "    \"Attention complexity is linear in sequence length, making transformers efficient for any length.\",\n",
    "\n",
    "    # Irrelevant noise\n",
    "    \"The best pizza restaurants in New York include Di Fara and Lucali.\",\n",
    "    \"The 2024 Paris Olympics featured 329 events across 32 sports.\",\n",
    "    \"Python was created by Guido van Rossum and released in 1991.\",\n",
    "    \"Mount Everest stands at 8,849 meters above sea level.\",\n",
    "\n",
    "    # More relevant (but buried in the middle!)\n",
    "    \"Flash attention reduces memory from quadratic to linear by computing attention in blocks.\",\n",
    "    \"Sparse attention mechanisms attend to a subset of tokens to handle longer sequences.\",\n",
    "]\n",
    "\n",
    "# Known facts for poisoning detection\n",
    "known_facts = [\n",
    "    \"Transformers use self-attention to process ALL tokens in parallel, not sequentially.\",\n",
    "    \"Standard attention has quadratic complexity O(n^2) in sequence length.\",\n",
    "    \"Self-attention allows each token to attend to every other token regardless of distance.\",\n",
    "    \"Positional encoding adds position information since attention is permutation-invariant.\",\n",
    "]\n",
    "\n",
    "# Instructions with clashes\n",
    "instructions = [\n",
    "    \"Provide detailed technical explanations with mathematical formulas.\",\n",
    "    \"Keep responses brief and avoid technical jargon.\",\n",
    "    \"Always cite specific research papers when making claims.\",\n",
    "    \"Speak conversationally without academic references.\",\n",
    "    \"Focus only on the user's specific question.\",\n",
    "    \"Proactively explain related concepts and background.\",\n",
    "]\n",
    "\n",
    "# Run the diagnostic\n",
    "checker = ContextHealthChecker()\n",
    "report = checker.diagnose(context_chunks, query, known_facts, instructions)\n",
    "checker.print_report(report)"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Dashboard\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/24_dashboard.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_24_dashboard"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Output â€” The 2x2 Failure Mode Dashboard ðŸŽ¯\n",
    "\n",
    "Let us bring everything together into a single, comprehensive visualization. This dashboard shows all four failure modes side by side, making it easy to compare their effects and diagnose context problems at a glance."
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_failure_mode_dashboard():\n",
    "    \"\"\"\n",
    "    Create the final 2x2 dashboard showing all four context failure modes.\n",
    "\n",
    "    Top-left: Context Poisoning â€” degradation curve over turns\n",
    "    Top-right: Context Distraction â€” U-shaped \"Lost in the Middle\" curve\n",
    "    Bottom-left: Context Confusion â€” accuracy vs. number of irrelevant tools\n",
    "    Bottom-right: Context Clash â€” heatmap of instruction contradictions\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('The Four Context Failure Modes â€” A Complete Diagnostic Dashboard',\n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Panel 1 (Top-Left): Context Poisoning\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    ax1 = axes[0, 0]\n",
    "    accuracy, _, poison_turn = simulate_poisoning(num_turns=12, poison_turn=3)\n",
    "    turns = range(len(accuracy))\n",
    "    colors_bar = [COLORS['healthy'] if t < poison_turn else COLORS['poisoning'] for t in turns]\n",
    "    ax1.bar(turns, accuracy, color=colors_bar, edgecolor='white', linewidth=0.5)\n",
    "    ax1.axvline(x=poison_turn - 0.5, color='black', linestyle='--', linewidth=1.5, alpha=0.5)\n",
    "    ax1.set_xlabel('Conversation Turn', fontsize=10)\n",
    "    ax1.set_ylabel('Context Accuracy', fontsize=10)\n",
    "    ax1.set_title('1. Context Poisoning', fontsize=13, fontweight='bold', color=COLORS['poisoning'])\n",
    "    ax1.set_ylim(0, 1.05)\n",
    "    ax1.text(poison_turn + 0.3, 0.95, 'Poison\\ninjected',\n",
    "             fontsize=9, color=COLORS['poisoning'], fontweight='bold')\n",
    "    ax1.text(0.5, 0.02, 'Errors compound over turns',\n",
    "             fontsize=8, fontstyle='italic', color='gray',\n",
    "             transform=ax1.transAxes, ha='center')\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Panel 2 (Top-Right): Context Distraction / Lost in the Middle\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    ax2 = axes[0, 1]\n",
    "    positions, acc_litm, stds_litm = simulate_lost_in_the_middle(num_positions=20, num_trials=80)\n",
    "    ax2.fill_between(positions, np.array(acc_litm) - np.array(stds_litm),\n",
    "                     np.array(acc_litm) + np.array(stds_litm),\n",
    "                     alpha=0.15, color=COLORS['distraction'])\n",
    "    ax2.plot(positions, acc_litm, 'o-', color=COLORS['distraction'], markersize=4, linewidth=2)\n",
    "    ax2.set_xlabel('Position of Key Info in Context', fontsize=10)\n",
    "    ax2.set_ylabel('Retrieval Accuracy', fontsize=10)\n",
    "    ax2.set_title('2. Context Distraction', fontsize=13, fontweight='bold', color=COLORS['distraction'])\n",
    "    ax2.set_ylim(0, 1.1)\n",
    "\n",
    "    # Shade the middle danger zone\n",
    "    ax2.axvspan(4, 15, alpha=0.08, color=COLORS['poisoning'])\n",
    "    ax2.text(10, 0.15, '\"Lost in\\nthe Middle\"',\n",
    "             fontsize=10, fontweight='bold', color=COLORS['poisoning'],\n",
    "             ha='center', fontstyle='italic')\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Panel 3 (Bottom-Left): Context Confusion\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    ax3 = axes[1, 0]\n",
    "    tool_cts, acc_conf, std_conf = simulate_context_confusion(max_tools=50, step=2, num_trials=40)\n",
    "    acc_conf = np.array(acc_conf)\n",
    "    std_conf = np.array(std_conf)\n",
    "    ax3.fill_between(tool_cts, acc_conf - std_conf, acc_conf + std_conf,\n",
    "                     alpha=0.15, color=COLORS['confusion'])\n",
    "    ax3.plot(tool_cts, acc_conf, 'o-', color=COLORS['confusion'], markersize=4, linewidth=2)\n",
    "    ax3.axvline(x=19, color=COLORS['healthy'], linestyle='--', alpha=0.6, linewidth=1.5)\n",
    "    ax3.axvline(x=46, color=COLORS['poisoning'], linestyle='--', alpha=0.6, linewidth=1.5)\n",
    "    ax3.set_xlabel('Number of Tools in Context', fontsize=10)\n",
    "    ax3.set_ylabel('Tool Selection Accuracy', fontsize=10)\n",
    "    ax3.set_title('3. Context Confusion', fontsize=13, fontweight='bold', color=COLORS['confusion'])\n",
    "    ax3.set_ylim(0, 1.1)\n",
    "    ax3.text(17, 0.15, '19', fontsize=9, color=COLORS['healthy'], fontweight='bold', ha='center')\n",
    "    ax3.text(44, 0.15, '46', fontsize=9, color=COLORS['poisoning'], fontweight='bold', ha='center')\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Panel 4 (Bottom-Right): Context Clash â€” Heatmap\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    ax4 = axes[1, 1]\n",
    "\n",
    "    # Create instructions and compute contradiction heatmap\n",
    "    short_labels = ['Detail', 'Brief', 'Formal', 'Casual', 'Cite', 'Speculate', 'Focus', 'Expand']\n",
    "    instruction_texts = [\n",
    "        \"Always respond with detailed technical explanations including code examples.\",\n",
    "        \"Keep all responses under 50 words. Never include code.\",\n",
    "        \"Use formal academic language with citations and references.\",\n",
    "        \"Write casually like you're texting a friend. Use slang and emojis.\",\n",
    "        \"Never make claims without citing peer-reviewed sources.\",\n",
    "        \"Be creative and speculative. Explore bold hypotheses freely.\",\n",
    "        \"Focus exclusively on the user's specific question. Stay on topic.\",\n",
    "        \"Proactively provide related context, background, and tangential insights.\",\n",
    "    ]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_mat = vectorizer.fit_transform(instruction_texts)\n",
    "    clash_matrix = cosine_similarity(tfidf_mat)\n",
    "\n",
    "    # Mask the diagonal (self-similarity is always 1, not interesting)\n",
    "    np.fill_diagonal(clash_matrix, np.nan)\n",
    "\n",
    "    im = ax4.imshow(clash_matrix, cmap='YlOrRd', vmin=0, vmax=0.4, aspect='auto')\n",
    "    ax4.set_xticks(range(len(short_labels)))\n",
    "    ax4.set_yticks(range(len(short_labels)))\n",
    "    ax4.set_xticklabels(short_labels, rotation=45, ha='right', fontsize=9)\n",
    "    ax4.set_yticklabels(short_labels, fontsize=9)\n",
    "    ax4.set_title('4. Context Clash', fontsize=13, fontweight='bold', color=COLORS['clash'])\n",
    "\n",
    "    # Add text annotations for high-clash pairs\n",
    "    for i in range(len(short_labels)):\n",
    "        for j in range(len(short_labels)):\n",
    "            if i != j and not np.isnan(clash_matrix[i, j]):\n",
    "                val = clash_matrix[i, j]\n",
    "                if val > 0.1:\n",
    "                    ax4.text(j, i, f'{val:.2f}', ha='center', va='center',\n",
    "                            fontsize=7, fontweight='bold' if val > 0.2 else 'normal',\n",
    "                            color='white' if val > 0.25 else 'black')\n",
    "\n",
    "    plt.colorbar(im, ax=ax4, label='Contradiction Score', shrink=0.8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('context_failure_modes_dashboard.png', dpi=150, bbox_inches='tight',\n",
    "                facecolor='white', edgecolor='none')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nðŸŽ¯ Dashboard saved as 'context_failure_modes_dashboard.png'\")\n",
    "    print(\"\\nWhat each panel tells you:\")\n",
    "    print(\"  1. POISONING  â†’ One wrong fact cascades into many wrong answers\")\n",
    "    print(\"  2. DISTRACTION â†’ Information in the middle of long context gets lost\")\n",
    "    print(\"  3. CONFUSION  â†’ More tools/info beyond what's needed HURTS performance\")\n",
    "    print(\"  4. CLASH      â†’ Contradictory instructions cause unpredictable behavior\")\n",
    "\n",
    "\n",
    "# Generate the final dashboard\n",
    "create_failure_mode_dashboard()"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Reflection\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/25_reflection.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_25_reflection"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reflection ðŸ’¡\n",
    "\n",
    "Let us step back and think about what we have built and learned.\n",
    "\n",
    "### What We Built\n",
    "1. **Context quality metrics** â€” TF-IDF similarity and signal-to-noise ratio to measure context health\n",
    "2. **Four failure mode simulators** â€” each one demonstrates a real, documented failure pattern\n",
    "3. **Three detection functions** â€” poisoning detection, confusion scoring, and clash detection\n",
    "4. **A Context Health Checker** â€” a reusable diagnostic tool that combines all four checks\n",
    "\n",
    "### The Four Failure Modes â€” Summary\n",
    "\n",
    "| Failure Mode | What Goes Wrong | Real-World Example | Our Detection |\n",
    "|---|---|---|---|\n",
    "| **Poisoning** | False facts compound over turns | Gemini Pokemon agent | TF-IDF fact verification |\n",
    "| **Distraction** | Key info lost in long context | \"Lost in the Middle\" (Liu 2023) | Position-based risk scoring |\n",
    "| **Confusion** | Irrelevant noise drowns signal | Llama 3.1 fails at 46 tools | Relevance fraction scoring |\n",
    "| **Clash** | Contradictory instructions | o3 drops from 98% to 64% | Pairwise similarity detection |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **More is not always better.** More context, more tools, more instructions â€” all can make models *worse*, not better. Context engineering is as much about what you *exclude* as what you include.\n",
    "\n",
    "2. **Position matters.** Where you place information in the context affects whether the model can find it. Put the most important information first.\n",
    "\n",
    "3. **Errors compound.** A single hallucination does not just add one wrong answer â€” it generates derivative errors that accumulate over turns. Validate context entries against ground truth.\n",
    "\n",
    "4. **Contradictions are catastrophic.** Fragmenting or contradicting instructions causes disproportionate damage. Unified, consistent instructions dramatically outperform scattered ones.\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In Part 3 of this series, we will learn the **four core strategies** for engineering context well â€” Write, Select, Compress, and Isolate â€” and build tools that implement each one. If this notebook taught you what can go wrong, the next one teaches you how to get it right."
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/26_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_26_closing"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final verification: let's make sure everything ran successfully\n",
    "print(\"=\" * 60)\n",
    "print(\"  âœ… NOTEBOOK COMPLETE â€” Context Failure Modes\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"  What we built:\")\n",
    "print(\"    â€¢ compute_relevance()        â€” TF-IDF context scoring\")\n",
    "print(\"    â€¢ signal_to_noise_ratio()    â€” SNR measurement\")\n",
    "print(\"    â€¢ simulate_poisoning()       â€” Failure mode 1 simulator\")\n",
    "print(\"    â€¢ simulate_lost_in_middle()  â€” Failure mode 2 simulator\")\n",
    "print(\"    â€¢ simulate_context_confusion() â€” Failure mode 3 simulator\")\n",
    "print(\"    â€¢ simulate_context_clash()   â€” Failure mode 4 simulator\")\n",
    "print(\"    â€¢ detect_poisoning()         â€” TODO 1 âœ“\")\n",
    "print(\"    â€¢ measure_confusion_score()  â€” TODO 2 âœ“\")\n",
    "print(\"    â€¢ detect_clashes()           â€” TODO 3 âœ“\")\n",
    "print(\"    â€¢ ContextHealthChecker       â€” Full diagnostic class\")\n",
    "print()\n",
    "print(\"  Visualizations generated:\")\n",
    "print(\"    â€¢ Poisoning degradation curve\")\n",
    "print(\"    â€¢ Lost in the Middle U-curve\")\n",
    "print(\"    â€¢ Tool confusion accuracy plot\")\n",
    "print(\"    â€¢ Instruction clash heatmap\")\n",
    "print(\"    â€¢ 2x2 failure mode dashboard\")\n",
    "print()\n",
    "print(\"  No API keys were needed â€” everything ran locally!\")\n",
    "print(\"  Time estimate: ~25 minutes\")\n",
    "print()\n",
    "print(\"  â†’ Next: Part 3 â€” Four Core Strategies (Write, Select,\")\n",
    "print(\"    Compress, Isolate)\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "chatbot"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸ’¬ AI Teaching Assistant â€” Click â–¶ to start\n",
    "#@markdown This AI chatbot reads your notebook and can answer questions about any concept, code, or exercise.\n",
    "\n",
    "import json as _json\n",
    "import requests as _requests\n",
    "from google.colab import output as _output\n",
    "from IPython.display import display, HTML as _HTML, Markdown as _Markdown\n",
    "\n",
    "# --- Read notebook content for context ---\n",
    "def _get_notebook_context():\n",
    "    try:\n",
    "        from google.colab import _message\n",
    "        nb = _message.blocking_request(\"get_ipynb\", request=\"\", timeout_sec=10)\n",
    "        cells = nb.get(\"ipynb\", {}).get(\"cells\", [])\n",
    "        parts = []\n",
    "        for cell in cells:\n",
    "            src = \"\".join(cell.get(\"source\", []))\n",
    "            tags = cell.get(\"metadata\", {}).get(\"tags\", [])\n",
    "            if \"chatbot\" in tags:\n",
    "                continue\n",
    "            if src.strip():\n",
    "                ct = cell.get(\"cell_type\", \"unknown\")\n",
    "                parts.append(f\"[{ct.upper()}]\\n{src}\")\n",
    "        return \"\\n\\n---\\n\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"Notebook content unavailable.\"\n",
    "\n",
    "_NOTEBOOK_CONTEXT = _get_notebook_context()\n",
    "_CHAT_HISTORY = []\n",
    "_API_URL = \"https://course-creator-brown.vercel.app/api/chat\"\n",
    "\n",
    "def _notebook_chat(question):\n",
    "    global _CHAT_HISTORY\n",
    "    try:\n",
    "        resp = _requests.post(_API_URL, json={\n",
    "            'question': question,\n",
    "            'context': _NOTEBOOK_CONTEXT[:100000],\n",
    "            'history': _CHAT_HISTORY[-10:],\n",
    "        }, timeout=60)\n",
    "        data = resp.json()\n",
    "        answer = data.get('answer', 'Sorry, I could not generate a response.')\n",
    "        _CHAT_HISTORY.append({'role': 'user', 'content': question})\n",
    "        _CHAT_HISTORY.append({'role': 'assistant', 'content': answer})\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f'Error connecting to teaching assistant: {str(e)}'\n",
    "\n",
    "_output.register_callback('notebook_chat', _notebook_chat)\n",
    "\n",
    "def ask(question):\n",
    "    \"\"\"Ask the AI teaching assistant a question about this notebook.\"\"\"\n",
    "    answer = _notebook_chat(question)\n",
    "    display(_Markdown(answer))\n",
    "\n",
    "print(\"\\u2705 AI Teaching Assistant is ready!\")\n",
    "print(\"\\U0001f4a1 Use the chat below, or call ask(\\'your question\\') in any cell.\")\n",
    "\n",
    "# --- Display chat widget ---\n",
    "display(_HTML('''<style>\n  .vc-wrap{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;max-width:100%;border-radius:16px;overflow:hidden;box-shadow:0 4px 24px rgba(0,0,0,.12);background:#fff;border:1px solid #e5e7eb}\n  .vc-hdr{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;padding:16px 20px;display:flex;align-items:center;gap:12px}\n  .vc-avatar{width:42px;height:42px;background:rgba(255,255,255,.2);border-radius:50%;display:flex;align-items:center;justify-content:center;font-size:22px}\n  .vc-hdr h3{font-size:16px;font-weight:600;margin:0}\n  .vc-hdr p{font-size:12px;opacity:.85;margin:2px 0 0}\n  .vc-msgs{height:420px;overflow-y:auto;padding:16px;background:#f8f9fb;display:flex;flex-direction:column;gap:10px}\n  .vc-msg{display:flex;flex-direction:column;animation:vc-fade .25s ease}\n  .vc-msg.user{align-items:flex-end}\n  .vc-msg.bot{align-items:flex-start}\n  .vc-bbl{max-width:85%;padding:10px 14px;border-radius:16px;font-size:14px;line-height:1.55;word-wrap:break-word}\n  .vc-msg.user .vc-bbl{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border-bottom-right-radius:4px}\n  .vc-msg.bot .vc-bbl{background:#fff;color:#1a1a2e;border:1px solid #e8e8e8;border-bottom-left-radius:4px}\n  .vc-bbl code{background:rgba(0,0,0,.07);padding:2px 6px;border-radius:4px;font-size:13px;font-family:'Fira Code',monospace}\n  .vc-bbl pre{background:#1e1e2e;color:#cdd6f4;padding:12px;border-radius:8px;overflow-x:auto;margin:8px 0;font-size:13px}\n  .vc-bbl pre code{background:none;padding:0;color:inherit}\n  .vc-bbl h3,.vc-bbl h4{margin:10px 0 4px;font-size:15px}\n  .vc-bbl ul,.vc-bbl ol{margin:4px 0;padding-left:20px}\n  .vc-bbl li{margin:2px 0}\n  .vc-chips{display:flex;flex-wrap:wrap;gap:8px;padding:0 16px 12px;background:#f8f9fb}\n  .vc-chip{background:#fff;border:1px solid #d1d5db;border-radius:20px;padding:6px 14px;font-size:12px;cursor:pointer;transition:all .15s;color:#4b5563}\n  .vc-chip:hover{border-color:#667eea;color:#667eea;background:#f0f0ff}\n  .vc-input{display:flex;padding:12px 16px;background:#fff;border-top:1px solid #eee;gap:8px}\n  .vc-input input{flex:1;padding:10px 16px;border:2px solid #e8e8e8;border-radius:24px;font-size:14px;outline:none;transition:border-color .2s}\n  .vc-input input:focus{border-color:#667eea}\n  .vc-input button{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border:none;border-radius:50%;width:42px;height:42px;cursor:pointer;display:flex;align-items:center;justify-content:center;font-size:18px;transition:transform .1s}\n  .vc-input button:hover{transform:scale(1.05)}\n  .vc-input button:disabled{opacity:.5;cursor:not-allowed;transform:none}\n  .vc-typing{display:flex;gap:5px;padding:4px 0}\n  .vc-typing span{width:8px;height:8px;background:#667eea;border-radius:50%;animation:vc-bounce 1.4s infinite ease-in-out}\n  .vc-typing span:nth-child(2){animation-delay:.2s}\n  .vc-typing span:nth-child(3){animation-delay:.4s}\n  @keyframes vc-bounce{0%,80%,100%{transform:scale(0)}40%{transform:scale(1)}}\n  @keyframes vc-fade{from{opacity:0;transform:translateY(8px)}to{opacity:1;transform:translateY(0)}}\n  .vc-note{text-align:center;font-size:11px;color:#9ca3af;padding:8px 16px 12px;background:#fff}\n</style>\n<div class=\"vc-wrap\">\n  <div class=\"vc-hdr\">\n    <div class=\"vc-avatar\">&#129302;</div>\n    <div>\n      <h3>Vizuara Teaching Assistant</h3>\n      <p>Ask me anything about this notebook</p>\n    </div>\n  </div>\n  <div class=\"vc-msgs\" id=\"vcMsgs\">\n    <div class=\"vc-msg bot\">\n      <div class=\"vc-bbl\">&#128075; Hi! I've read through this entire notebook. Ask me about any concept, code block, or exercise &mdash; I'm here to help you learn!</div>\n    </div>\n  </div>\n  <div class=\"vc-chips\" id=\"vcChips\">\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Explain the main concept</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Help with the TODO exercise</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Summarize what I learned</span>\n  </div>\n  <div class=\"vc-input\">\n    <input type=\"text\" id=\"vcIn\" placeholder=\"Ask about concepts, code, exercises...\" />\n    <button id=\"vcSend\" onclick=\"vcSendMsg()\">&#10148;</button>\n  </div>\n  <div class=\"vc-note\">AI-generated &middot; Verify important information &middot; <a href=\"#\" onclick=\"vcClear();return false\" style=\"color:#667eea\">Clear chat</a></div>\n</div>\n<script>\n(function(){\n  var msgs=document.getElementById('vcMsgs'),inp=document.getElementById('vcIn'),\n      btn=document.getElementById('vcSend'),chips=document.getElementById('vcChips');\n\n  function esc(s){var d=document.createElement('div');d.textContent=s;return d.innerHTML}\n\n  function md(t){\n    return t\n      .replace(/```(\\w*)\\n([\\s\\S]*?)```/g,function(_,l,c){return '<pre><code>'+esc(c)+'</code></pre>'})\n      .replace(/`([^`]+)`/g,'<code>$1</code>')\n      .replace(/\\*\\*([^*]+)\\*\\*/g,'<strong>$1</strong>')\n      .replace(/\\*([^*]+)\\*/g,'<em>$1</em>')\n      .replace(/^#### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^## (.+)$/gm,'<h3>$1</h3>')\n      .replace(/^\\d+\\. (.+)$/gm,'<li>$1</li>')\n      .replace(/^- (.+)$/gm,'<li>$1</li>')\n      .replace(/\\n\\n/g,'<br><br>')\n      .replace(/\\n/g,'<br>');\n  }\n\n  function addMsg(text,isUser){\n    var m=document.createElement('div');m.className='vc-msg '+(isUser?'user':'bot');\n    var b=document.createElement('div');b.className='vc-bbl';\n    b.innerHTML=isUser?esc(text):md(text);\n    m.appendChild(b);msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function showTyping(){\n    var m=document.createElement('div');m.className='vc-msg bot';m.id='vcTyping';\n    m.innerHTML='<div class=\"vc-bbl\"><div class=\"vc-typing\"><span></span><span></span><span></span></div></div>';\n    msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function hideTyping(){var e=document.getElementById('vcTyping');if(e)e.remove()}\n\n  window.vcSendMsg=function(){\n    var q=inp.value.trim();if(!q)return;\n    inp.value='';chips.style.display='none';\n    addMsg(q,true);showTyping();btn.disabled=true;\n    google.colab.kernel.invokeFunction('notebook_chat',[q],{})\n      .then(function(r){\n        hideTyping();\n        var a=r.data['application/json'];\n        addMsg(typeof a==='string'?a:JSON.stringify(a),false);\n      })\n      .catch(function(){\n        hideTyping();\n        addMsg('Sorry, I encountered an error. Please check your internet connection and try again.',false);\n      })\n      .finally(function(){btn.disabled=false;inp.focus()});\n  };\n\n  window.vcAsk=function(q){inp.value=q;vcSendMsg()};\n  window.vcClear=function(){\n    msgs.innerHTML='<div class=\"vc-msg bot\"><div class=\"vc-bbl\">&#128075; Chat cleared. Ask me anything!</div></div>';\n    chips.style.display='flex';\n  };\n\n  inp.addEventListener('keypress',function(e){if(e.key==='Enter')vcSendMsg()});\n  inp.focus();\n})();\n</script>'''))"
   ],
   "id": "vizuara_chatbot"
  }
 ]
}