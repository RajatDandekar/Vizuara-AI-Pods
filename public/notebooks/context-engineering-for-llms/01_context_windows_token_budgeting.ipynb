{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Context Windows & Token Budgeting ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1tEkJg2sHo0VEj67JJ5QGDqcePeldjAlx\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Desk Analogy\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_desk_analogy.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_desk_analogy"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Context Windows & Token Budgeting: Understanding the LLM's Working Desk\n",
    "\n",
    "*Part 1 of the Vizuara series on Context Engineering for LLMs*\n",
    "*Estimated time: 30 minutes*\n",
    "\n",
    "Let us start with an analogy. Imagine you are sitting in an open-book exam. You have a desk of fixed size ‚Äî say, enough room for exactly 10 pages of notes. You brought your textbook, your formula sheet, some practice problems, and a few blank pages to write your answers on.\n",
    "\n",
    "Here is the catch: **if you bring too many pages, they fall off the desk.** The desk does not grow. You have to choose what goes on it ‚Äî and what you leave behind.\n",
    "\n",
    "This is *exactly* the situation a Large Language Model faces every single time it processes a request. The desk is the **context window**. The pages are **tokens**. And your job, as the developer, is to decide what goes on that desk.\n",
    "\n",
    "Andrej Karpathy put it best:\n",
    "\n",
    "> *\"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\"*\n",
    "\n",
    "By the end of this notebook, you will understand what fills that desk, how to measure it, and how to budget it wisely ‚Äî for any model, any application."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_why_it_matters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ü§î Why Does This Matter?\n",
    "\n",
    "Every LLM has a **context window** ‚Äî a hard limit on how many tokens it can process in a single call. Go over that limit and your request simply fails. Stay well under it and you are leaving capability on the table.\n",
    "\n",
    "But here is the subtle part: the context window is not just \"your prompt.\" It is shared across **six distinct components**, each competing for the same finite space:\n",
    "\n",
    "| Component | Typical Budget | What It Contains |\n",
    "|-----------|---------------|-----------------|\n",
    "| System Prompt | ~2K tokens | Personality, rules, format instructions |\n",
    "| User Message | ~1K tokens | The current question or request |\n",
    "| Conversation History | ~20K tokens | Previous turns in the chat |\n",
    "| Retrieved Context (RAG) | ~60K tokens | Documents fetched from a knowledge base |\n",
    "| Tool Results | ~10K tokens | Outputs from function calls, APIs |\n",
    "| Reserved for Output | ~35K tokens | Space the model needs to generate its answer |\n",
    "\n",
    "Think of it this way: if you stuff 100K tokens of retrieved documents into a 128K window, you have left almost no room for the model to actually *think* and *respond*.\n",
    "\n",
    "Let us make this concrete with code."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Install And Imports\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_install_and_imports.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_install_and_imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's install tiktoken ‚Äî OpenAI's fast tokenizer library\n",
    "# This works for GPT-style tokenizers and gives us ground truth token counts\n",
    "!pip install tiktoken matplotlib numpy -q"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "from typing import Dict, Optional, Tuple, List\n",
    "\n",
    "# Use a clean style for all our visualizations\n",
    "plt.rcParams.update({\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': '#fafafa',\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "    'font.size': 11,\n",
    "    'figure.dpi': 100,\n",
    "})\n",
    "\n",
    "print(\"‚úÖ All imports ready. Let's explore context windows!\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: What Is A Token\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_what_is_a_token.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_what_is_a_token"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üí° Building Intuition: What Is a Token, Really?\n",
    "\n",
    "Before we can budget tokens, we need to understand what they are. A **token** is not a word. It is not a character. It is a *subword unit* ‚Äî a chunk of text that the model's tokenizer has learned to treat as a single piece.\n",
    "\n",
    "A rough rule of thumb: **1 token ‚âà 4 characters** of English text, or about **¬æ of a word**.\n",
    "\n",
    "But this is only an approximation. The actual count depends on the specific tokenizer and the text itself. Let us see the difference."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rough_token_estimate(text: str) -> int:\n",
    "    \"\"\"Estimate tokens using the ~4 characters per token rule of thumb.\"\"\"\n",
    "    return len(text) // 4\n",
    "\n",
    "def exact_token_count(text: str, model: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"Count exact tokens using tiktoken (OpenAI's tokenizer).\n",
    "\n",
    "    cl100k_base is used by GPT-4, GPT-3.5-turbo, and text-embedding-ada-002.\n",
    "    \"\"\"\n",
    "    encoder = tiktoken.get_encoding(model)\n",
    "    return len(encoder.encode(text))\n",
    "\n",
    "# Let's test with different types of text\n",
    "samples = {\n",
    "    \"Simple English\": \"The cat sat on the mat and looked out the window.\",\n",
    "    \"Technical\": \"The transformer architecture uses multi-head self-attention mechanisms.\",\n",
    "    \"Code snippet\": \"def forward(self, x): return self.linear(self.relu(self.norm(x)))\",\n",
    "    \"JSON data\": '{\"name\": \"Alice\", \"age\": 30, \"scores\": [95, 87, 92]}',\n",
    "    \"Repeated text\": \"buffalo \" * 20,\n",
    "}\n",
    "\n",
    "print(f\"{'Text Type':<20} {'Chars':>6} {'Rough Est.':>10} {'Exact (tiktoken)':>16} {'Ratio':>8}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for label, text in samples.items():\n",
    "    chars = len(text)\n",
    "    rough = rough_token_estimate(text)\n",
    "    exact = exact_token_count(text)\n",
    "    ratio = chars / exact if exact > 0 else 0\n",
    "    print(f\"{label:<20} {chars:>6} {rough:>10} {exact:>16} {ratio:>7.1f}:1\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î **Think about it:** Notice how the ratio of characters-to-tokens varies by text type. Code and JSON tend to use *more* tokens per character than plain English. This matters when you are budgeting ‚Äî a 10K-character JSON blob from a tool call uses more of your context window than 10K characters of natural language."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Visualize Tokens\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_visualize_tokens.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_visualize_tokens"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üìä Visualization Checkpoint 1: Seeing Tokenization in Action\n",
    "\n",
    "Let us actually *see* how a tokenizer breaks text into pieces. This builds intuition for why token counts vary so much."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tokens(text: str, max_display: int = 40):\n",
    "    \"\"\"Show how tiktoken breaks a string into individual tokens.\"\"\"\n",
    "    encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    token_ids = encoder.encode(text)\n",
    "    tokens = [encoder.decode([tid]) for tid in token_ids]\n",
    "\n",
    "    # Color each token differently for visibility\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, min(len(tokens), max_display)))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 2.5))\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Tokenization: {len(tokens)} tokens from {len(text)} characters',\n",
    "                 fontsize=13, fontweight='bold', pad=10)\n",
    "\n",
    "    x_pos = 0.02\n",
    "    y_pos = 0.7\n",
    "    for i, (token, color) in enumerate(zip(tokens[:max_display], colors)):\n",
    "        display = repr(token)[1:-1]  # Show whitespace clearly\n",
    "        text_width = max(len(display) * 0.015, 0.03)\n",
    "\n",
    "        rect = mpatches.FancyBboxPatch(\n",
    "            (x_pos, y_pos - 0.15), text_width + 0.01, 0.3,\n",
    "            boxstyle=\"round,pad=0.005\", facecolor=color, edgecolor='gray', alpha=0.8\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x_pos + (text_width + 0.01) / 2, y_pos, display,\n",
    "                ha='center', va='center', fontsize=8, fontfamily='monospace')\n",
    "\n",
    "        x_pos += text_width + 0.015\n",
    "        if x_pos > 0.95:\n",
    "            x_pos = 0.02\n",
    "            y_pos -= 0.4\n",
    "            if y_pos < 0:\n",
    "                break\n",
    "\n",
    "    if len(tokens) > max_display:\n",
    "        ax.text(0.5, 0.05, f'... and {len(tokens) - max_display} more tokens',\n",
    "                ha='center', va='center', fontsize=10, style='italic', color='gray')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize tokenization of a technical sentence\n",
    "visualize_tokens(\"Context engineering is the delicate art of filling the context window with just the right information.\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compare: how does the tokenizer handle code vs prose?\n",
    "visualize_tokens('def calculate_budget(system=2000, user=1000, rag=60000): return sum([system, user, rag])')"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the tokenizer handles code differently ‚Äî variable names get split into subwords, punctuation often gets its own token, and numbers may be chunked in unexpected ways. This is why **exact token counting matters** when you are working close to the limits of a context window."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Math Budget Equation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_math_budget_equation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_math_budget_equation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Mathematics: Token Budget Equation\n",
    "\n",
    "Now that we have intuition, let us formalize. The total context window of an LLM is divided as:\n",
    "\n",
    "$$T_{\\text{total}} = T_{\\text{system}} + T_{\\text{history}} + T_{\\text{RAG}} + T_{\\text{tools}} + T_{\\text{user}} + T_{\\text{reserved}}$$\n",
    "\n",
    "For a 128K-token model (like GPT-4-Turbo), a typical allocation looks like:\n",
    "\n",
    "$$128{,}000 = 2{,}000 + 20{,}000 + 60{,}000 + 10{,}000 + 1{,}000 + 35{,}000$$\n",
    "\n",
    "What does each term mean computationally?\n",
    "\n",
    "- **$T_{\\text{system}}$**: The instructions you give the model. These are present in *every* API call, so keeping them concise directly saves budget.\n",
    "- **$T_{\\text{history}}$**: Past conversation turns. In a chatbot, this grows with each exchange ‚Äî you must decide when to summarize or truncate.\n",
    "- **$T_{\\text{RAG}}$**: Retrieved documents. This is typically the *largest* consumer. If your vector search returns too much, you blow the budget here.\n",
    "- **$T_{\\text{tools}}$**: Results from function calls (e.g., a database query returning JSON). Often overlooked in budgeting.\n",
    "- **$T_{\\text{user}}$**: The current user message. Usually small, but can be large if the user pastes a document.\n",
    "- **$T_{\\text{reserved}}$**: Space for the model's response. **If you don't reserve enough, the output gets truncated mid-sentence.**\n",
    "\n",
    "The key insight: **these components compete for a fixed resource.** Increasing one means decreasing another."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our context window components as a clean data structure\n",
    "\n",
    "# Standard budget for a 128K model (in tokens)\n",
    "DEFAULT_BUDGET = {\n",
    "    \"System Prompt\":       2_000,\n",
    "    \"Conversation History\": 20_000,\n",
    "    \"Retrieved Context (RAG)\": 60_000,\n",
    "    \"Tool Results\":        10_000,\n",
    "    \"User Message\":        1_000,\n",
    "    \"Reserved for Output\": 35_000,\n",
    "}\n",
    "\n",
    "COMPONENT_COLORS = {\n",
    "    \"System Prompt\":       \"#4C72B0\",\n",
    "    \"Conversation History\": \"#55A868\",\n",
    "    \"Retrieved Context (RAG)\": \"#C44E52\",\n",
    "    \"Tool Results\":        \"#8172B2\",\n",
    "    \"User Message\":        \"#CCB974\",\n",
    "    \"Reserved for Output\": \"#64B5CD\",\n",
    "}\n",
    "\n",
    "MODEL_LIMITS = {\n",
    "    \"GPT-3.5 (4K)\":       4_096,\n",
    "    \"GPT-4 (32K)\":        32_768,\n",
    "    \"GPT-4 Turbo (128K)\": 128_000,\n",
    "    \"Gemini 1.5 Pro (1M)\": 1_000_000,\n",
    "}\n",
    "\n",
    "def total_usage(budget: Dict[str, int]) -> int:\n",
    "    \"\"\"Sum all token allocations in a budget.\"\"\"\n",
    "    return sum(budget.values())\n",
    "\n",
    "budget_total = total_usage(DEFAULT_BUDGET)\n",
    "print(f\"Total budget: {budget_total:,} tokens\")\n",
    "print(f\"Target model: 128K\")\n",
    "print(f\"Match: {'‚úÖ Exact fit!' if budget_total == 128_000 else '‚ùå Mismatch!'}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo 1\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_todo_1.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_todo_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîß Let's Build It: Your Turn (TODO #1)\n",
    "\n",
    "Time to get your hands dirty. Implement the `estimate_tokens` function below. It should:\n",
    "\n",
    "1. Accept a text string and an optional `method` parameter\n",
    "2. When `method=\"rough\"`, return the ~4 chars/token estimate\n",
    "3. When `method=\"exact\"`, use tiktoken\n",
    "4. When `method=\"both\"`, return a dictionary with both estimates\n",
    "\n",
    "This is the kind of utility function you will use in every context engineering project."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO ============\n",
    "# Implement the estimate_tokens function below.\n",
    "#\n",
    "# Requirements:\n",
    "#   - method=\"rough\" ‚Üí return int (len(text) // 4)\n",
    "#   - method=\"exact\" ‚Üí return int (tiktoken count using cl100k_base)\n",
    "#   - method=\"both\"  ‚Üí return dict {\"rough\": ..., \"exact\": ...}\n",
    "#   - Handle empty strings gracefully (return 0 or {\"rough\": 0, \"exact\": 0})\n",
    "#\n",
    "# Hint: You already saw rough_token_estimate() and exact_token_count() above.\n",
    "# ============ TODO ============\n",
    "\n",
    "def estimate_tokens(text: str, method: str = \"both\") -> int | Dict[str, int]:\n",
    "    \"\"\"Estimate the token count of a text string.\n",
    "\n",
    "    Args:\n",
    "        text: The input text to tokenize.\n",
    "        method: One of \"rough\", \"exact\", or \"both\".\n",
    "\n",
    "    Returns:\n",
    "        Token count (int) for \"rough\"/\"exact\", or dict for \"both\".\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE ‚Äî replace the pass statements\n",
    "    if not text:\n",
    "        if method == \"both\":\n",
    "            return {\"rough\": 0, \"exact\": 0}\n",
    "        return 0\n",
    "\n",
    "    rough = len(text) // 4\n",
    "\n",
    "    if method == \"rough\":\n",
    "        return rough\n",
    "\n",
    "    encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    exact = len(encoder.encode(text))\n",
    "\n",
    "    if method == \"exact\":\n",
    "        return exact\n",
    "\n",
    "    # method == \"both\"\n",
    "    return {\"rough\": rough, \"exact\": exact}\n",
    "\n",
    "# ============ VERIFICATION ============\n",
    "# Run this cell to check your implementation\n",
    "\n",
    "test_text = \"The transformer architecture revolutionized natural language processing in 2017.\"\n",
    "\n",
    "rough_result = estimate_tokens(test_text, method=\"rough\")\n",
    "exact_result = estimate_tokens(test_text, method=\"exact\")\n",
    "both_result = estimate_tokens(test_text, method=\"both\")\n",
    "\n",
    "assert isinstance(rough_result, int), \"rough should return an int\"\n",
    "assert isinstance(exact_result, int), \"exact should return an int\"\n",
    "assert isinstance(both_result, dict), \"both should return a dict\"\n",
    "assert \"rough\" in both_result and \"exact\" in both_result, \"both dict must have 'rough' and 'exact' keys\"\n",
    "assert estimate_tokens(\"\", method=\"rough\") == 0, \"empty string should return 0\"\n",
    "assert estimate_tokens(\"\", method=\"both\") == {\"rough\": 0, \"exact\": 0}, \"empty string both should return zeros\"\n",
    "\n",
    "print(\"‚úÖ All assertions passed!\")\n",
    "print(f\"   Test text: '{test_text}'\")\n",
    "print(f\"   Rough estimate: {rough_result} tokens\")\n",
    "print(f\"   Exact count:    {exact_result} tokens\")\n",
    "print(f\"   Both:           {both_result}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Viz Budget Breakdown\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_viz_budget_breakdown.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_viz_budget_breakdown"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üìä Visualization Checkpoint 2: The Context Window Budget\n",
    "\n",
    "Now let us visualize *where* your tokens go. This is the single most important diagram in context engineering ‚Äî a stacked bar chart showing how each component fills (or overfills) the context window."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_context_budget(budget: Dict[str, int], model_limit: int,\n",
    "                         title: str = \"Context Window Budget Breakdown\"):\n",
    "    \"\"\"Visualize a token budget as a stacked horizontal bar chart.\n",
    "\n",
    "    Shows each component's allocation and whether the total exceeds the model limit.\n",
    "    \"\"\"\n",
    "    components = list(budget.keys())\n",
    "    values = list(budget.values())\n",
    "    colors = [COMPONENT_COLORS.get(c, '#999999') for c in components]\n",
    "    total = sum(values)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "    # Draw stacked horizontal bars\n",
    "    left = 0\n",
    "    bars = []\n",
    "    for comp, val, color in zip(components, values, colors):\n",
    "        bar = ax.barh(0, val, left=left, color=color, edgecolor='white',\n",
    "                      linewidth=1.5, height=0.5)\n",
    "        bars.append(bar)\n",
    "\n",
    "        # Label inside the bar if there's room\n",
    "        if val / model_limit > 0.05:\n",
    "            pct = val / total * 100\n",
    "            ax.text(left + val / 2, 0, f\"{val // 1000}K\\n({pct:.0f}%)\",\n",
    "                    ha='center', va='center', fontsize=9, fontweight='bold', color='white')\n",
    "        left += val\n",
    "\n",
    "    # Model limit line\n",
    "    ax.axvline(x=model_limit, color='red', linewidth=2, linestyle='--', label=f'Model Limit ({model_limit // 1000}K)')\n",
    "\n",
    "    # Overflow zone\n",
    "    if total > model_limit:\n",
    "        overflow = total - model_limit\n",
    "        ax.axvspan(model_limit, total, alpha=0.15, color='red')\n",
    "        ax.text(model_limit + overflow / 2, 0.35, f'‚ö†Ô∏è OVERFLOW\\n{overflow // 1000}K tokens',\n",
    "                ha='center', va='center', fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "    ax.set_xlim(0, max(total, model_limit) * 1.05)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('Tokens', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "    # Legend\n",
    "    legend_patches = [mpatches.Patch(color=color, label=comp)\n",
    "                      for comp, color in zip(components, colors)]\n",
    "    legend_patches.append(mpatches.Patch(facecolor='white', edgecolor='red',\n",
    "                                          linestyle='--', label=f'Model Limit'))\n",
    "    ax.legend(handles=legend_patches, loc='upper center', bbox_to_anchor=(0.5, -0.15),\n",
    "              ncol=3, fontsize=9, frameon=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary\n",
    "    status = \"‚úÖ Within budget\" if total <= model_limit else f\"‚ùå Over budget by {total - model_limit:,} tokens\"\n",
    "    print(f\"\\nTotal: {total:,} / {model_limit:,} tokens ‚Äî {status}\")\n",
    "\n",
    "# Visualize the default 128K budget\n",
    "plot_context_budget(DEFAULT_BUDGET, model_limit=128_000,\n",
    "                    title=\"Standard Budget: GPT-4 Turbo (128K)\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Insight:** Look at how RAG dominates the budget ‚Äî nearly half of the entire context window. This is why retrieval quality matters so much. If your retriever returns irrelevant documents, you are wasting the most valuable real estate in the window.\n",
    "\n",
    "Also notice that the output reservation (35K) is substantial. A shorter expected response means more room for input context."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Overflow\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_overflow.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_overflow"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. What Happens When Things Go Wrong?\n",
    "\n",
    "Let us simulate a realistic scenario: your RAG pipeline returns **80K tokens** instead of the budgeted 60K. What happens?"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: RAG returns more than expected\n",
    "overflow_budget = DEFAULT_BUDGET.copy()\n",
    "overflow_budget[\"Retrieved Context (RAG)\"] = 80_000  # 20K more than planned\n",
    "\n",
    "plot_context_budget(overflow_budget, model_limit=128_000,\n",
    "                    title=\"‚ö†Ô∏è Overflow Scenario: RAG Returns 80K Tokens\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also see: what if we try to cram this into a smaller model?\n",
    "small_budget = {\n",
    "    \"System Prompt\":       1_000,\n",
    "    \"Conversation History\": 1_500,\n",
    "    \"Retrieved Context (RAG)\": 500,\n",
    "    \"Tool Results\":        200,\n",
    "    \"User Message\":        500,\n",
    "    \"Reserved for Output\": 396,\n",
    "}\n",
    "\n",
    "plot_context_budget(small_budget, model_limit=4_096,\n",
    "                    title=\"Tight Budget: GPT-3.5 (4K) ‚Äî Every Token Counts\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î **Think about it:** With only 4K tokens, you can barely fit a system prompt and a short conversation. This is why the jump from 4K to 128K models was so transformative ‚Äî it unlocked RAG, multi-turn conversations, and tool use all at once.\n",
    "\n",
    "But even with 1M tokens (Gemini 1.5 Pro), budgeting still matters. More context means more latency, more cost, and more chances for the model to get confused by irrelevant information. **Bigger is not always better.**"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Multi Model\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_multi_model.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_11_multi_model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üìä Visualization Checkpoint 3: Multi-Model Comparison\n",
    "\n",
    "Let us see how the same application's needs map onto different model sizes. This is a question every developer faces: \"Which model do I need for my use case?\""
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_budget_to_model(base_budget: Dict[str, int], base_limit: int,\n",
    "                           target_limit: int) -> Dict[str, int]:\n",
    "    \"\"\"Scale a budget proportionally to fit a different model size.\n",
    "\n",
    "    Keeps the same relative proportions, but scales to the new limit.\n",
    "    The system prompt and user message have minimums to stay functional.\n",
    "    \"\"\"\n",
    "    ratio = target_limit / base_limit\n",
    "    scaled = {}\n",
    "    for component, tokens in base_budget.items():\n",
    "        scaled_val = int(tokens * ratio)\n",
    "        # Enforce minimums for critical components\n",
    "        if component == \"System Prompt\":\n",
    "            scaled_val = max(scaled_val, 200)\n",
    "        elif component == \"User Message\":\n",
    "            scaled_val = max(scaled_val, 100)\n",
    "        elif component == \"Reserved for Output\":\n",
    "            scaled_val = max(scaled_val, 200)\n",
    "        scaled[component] = scaled_val\n",
    "    return scaled\n",
    "\n",
    "\n",
    "def plot_multi_model_comparison(base_budget: Dict[str, int],\n",
    "                                 model_limits: Dict[str, int]):\n",
    "    \"\"\"Compare token budgets across multiple model sizes.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    model_names = list(model_limits.keys())\n",
    "    y_positions = range(len(model_names))\n",
    "    bar_height = 0.5\n",
    "\n",
    "    for i, (model_name, limit) in enumerate(model_limits.items()):\n",
    "        budget = scale_budget_to_model(base_budget, 128_000, limit)\n",
    "\n",
    "        left = 0\n",
    "        for comp, val in budget.items():\n",
    "            color = COMPONENT_COLORS.get(comp, '#999999')\n",
    "            ax.barh(i, val, left=left, color=color, edgecolor='white',\n",
    "                    linewidth=1, height=bar_height)\n",
    "            left += val\n",
    "\n",
    "        # Model limit marker\n",
    "        ax.plot(limit, i, 'r|', markersize=20, markeredgewidth=2)\n",
    "\n",
    "        # Label total\n",
    "        total = sum(budget.values())\n",
    "        ax.text(total + limit * 0.02, i, f'{total:,.0f} tokens',\n",
    "                va='center', fontsize=9, color='#333')\n",
    "\n",
    "    ax.set_yticks(y_positions)\n",
    "    ax.set_yticklabels(model_names, fontsize=11)\n",
    "    ax.set_xlabel('Tokens', fontsize=12)\n",
    "    ax.set_title('Context Budget Across Model Sizes', fontsize=14, fontweight='bold')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlim(100, 2_000_000)\n",
    "\n",
    "    # Legend\n",
    "    legend_patches = [mpatches.Patch(color=color, label=comp)\n",
    "                      for comp, color in COMPONENT_COLORS.items()]\n",
    "    ax.legend(handles=legend_patches, loc='upper center', bbox_to_anchor=(0.5, -0.12),\n",
    "              ncol=3, fontsize=9, frameon=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_multi_model_comparison(DEFAULT_BUDGET, MODEL_LIMITS)"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Key takeaway from this visualization:** On a log scale, you can see that the 4K model barely registers. The jump from 32K to 128K is where RAG-heavy applications become feasible. And 1M tokens? That is enough to fit an entire codebase or book ‚Äî but you pay for it in latency and cost."
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Budget Calculator\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_budget_calculator.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_12_budget_calculator"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Let's Build It: The Budget Calculator\n",
    "\n",
    "Now let us build a proper budget calculator ‚Äî a function that takes your desired allocations and tells you exactly where you stand."
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def budget_calculator(budget: Dict[str, int], model_limit: int) -> Dict:\n",
    "    \"\"\"Analyze a token budget against a model's context limit.\n",
    "\n",
    "    Returns a detailed analysis including:\n",
    "    - Total usage and remaining capacity\n",
    "    - Per-component percentages\n",
    "    - Warnings for over-budget or tight allocations\n",
    "    - Suggestions for rebalancing\n",
    "    \"\"\"\n",
    "    total = sum(budget.values())\n",
    "    remaining = model_limit - total\n",
    "    utilization = total / model_limit * 100\n",
    "\n",
    "    analysis = {\n",
    "        \"model_limit\": model_limit,\n",
    "        \"total_used\": total,\n",
    "        \"remaining\": remaining,\n",
    "        \"utilization_pct\": round(utilization, 1),\n",
    "        \"is_over_budget\": total > model_limit,\n",
    "        \"components\": {},\n",
    "        \"warnings\": [],\n",
    "        \"suggestions\": [],\n",
    "    }\n",
    "\n",
    "    # Analyze each component\n",
    "    for comp, tokens in budget.items():\n",
    "        pct_of_total = tokens / total * 100 if total > 0 else 0\n",
    "        pct_of_limit = tokens / model_limit * 100\n",
    "        analysis[\"components\"][comp] = {\n",
    "            \"tokens\": tokens,\n",
    "            \"pct_of_total\": round(pct_of_total, 1),\n",
    "            \"pct_of_limit\": round(pct_of_limit, 1),\n",
    "        }\n",
    "\n",
    "    # Generate warnings\n",
    "    if total > model_limit:\n",
    "        analysis[\"warnings\"].append(\n",
    "            f\"‚ö†Ô∏è OVER BUDGET by {total - model_limit:,} tokens! \"\n",
    "            f\"Reduce allocations by at least {(total - model_limit) / model_limit * 100:.1f}%.\"\n",
    "        )\n",
    "\n",
    "    output_budget = budget.get(\"Reserved for Output\", 0)\n",
    "    if output_budget < model_limit * 0.1:\n",
    "        analysis[\"warnings\"].append(\n",
    "            f\"‚ö†Ô∏è Output reservation ({output_budget:,}) is less than 10% of the window. \"\n",
    "            f\"Responses may be truncated.\"\n",
    "        )\n",
    "\n",
    "    rag_budget = budget.get(\"Retrieved Context (RAG)\", 0)\n",
    "    if rag_budget > model_limit * 0.6:\n",
    "        analysis[\"warnings\"].append(\n",
    "            f\"‚ö†Ô∏è RAG allocation ({rag_budget:,}) exceeds 60% of the window. \"\n",
    "            f\"Consider more aggressive retrieval filtering.\"\n",
    "        )\n",
    "\n",
    "    if remaining > model_limit * 0.2 and not analysis[\"is_over_budget\"]:\n",
    "        analysis[\"suggestions\"].append(\n",
    "            f\"üí° You have {remaining:,} tokens unused ({remaining / model_limit * 100:.0f}%). \"\n",
    "            f\"Consider expanding RAG or history allocation.\"\n",
    "        )\n",
    "\n",
    "    if utilization > 90 and not analysis[\"is_over_budget\"]:\n",
    "        analysis[\"suggestions\"].append(\n",
    "            f\"üí° Running at {utilization:.0f}% utilization. Leave a small buffer for \"\n",
    "            f\"variable-length inputs.\"\n",
    "        )\n",
    "\n",
    "    return analysis\n",
    "\n",
    "\n",
    "def print_budget_report(analysis: Dict):\n",
    "    \"\"\"Pretty-print a budget analysis report.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  CONTEXT WINDOW BUDGET REPORT\")\n",
    "    print(f\"  Model limit: {analysis['model_limit']:,} tokens\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(f\"\\n{'Component':<30} {'Tokens':>8} {'% Total':>8} {'% Limit':>8}\")\n",
    "    print(\"-\" * 56)\n",
    "    for comp, info in analysis[\"components\"].items():\n",
    "        print(f\"{comp:<30} {info['tokens']:>8,} {info['pct_of_total']:>7.1f}% {info['pct_of_limit']:>7.1f}%\")\n",
    "\n",
    "    print(\"-\" * 56)\n",
    "    status = \"üî¥ OVER\" if analysis[\"is_over_budget\"] else \"üü¢ OK\"\n",
    "    print(f\"{'TOTAL':<30} {analysis['total_used']:>8,} {'100.0':>7}% {analysis['utilization_pct']:>7.1f}%  {status}\")\n",
    "    print(f\"{'Remaining':<30} {analysis['remaining']:>8,}\")\n",
    "\n",
    "    if analysis[\"warnings\"]:\n",
    "        print(f\"\\n{'WARNINGS':}\")\n",
    "        for w in analysis[\"warnings\"]:\n",
    "            print(f\"  {w}\")\n",
    "\n",
    "    if analysis[\"suggestions\"]:\n",
    "        print(f\"\\n{'SUGGESTIONS':}\")\n",
    "        for s in analysis[\"suggestions\"]:\n",
    "            print(f\"  {s}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "# Test with our default budget\n",
    "report = budget_calculator(DEFAULT_BUDGET, 128_000)\n",
    "print_budget_report(report)"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now test with the overflow scenario\n",
    "overflow_budget = DEFAULT_BUDGET.copy()\n",
    "overflow_budget[\"Retrieved Context (RAG)\"] = 80_000\n",
    "\n",
    "report_overflow = budget_calculator(overflow_budget, 128_000)\n",
    "print_budget_report(report_overflow)"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo 2\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_todo_2.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_13_todo_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. üîß Your Turn (TODO #2): The Budget Optimizer\n",
    "\n",
    "Here is the real challenge. Given a set of **constraints** (minimums and maximums for each component) and a total token budget, find the **optimal allocation**.\n",
    "\n",
    "For example: \"I need at least 40K for RAG and at least 20K for output. How should I distribute the rest?\"\n",
    "\n",
    "This is a constrained optimization problem. The simplest approach: satisfy all minimums first, then distribute the remaining tokens proportionally among components that have room to grow."
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO ============\n",
    "# Implement the budget_optimizer function.\n",
    "#\n",
    "# Input:\n",
    "#   constraints: Dict[str, Dict] where each key is a component name and value is:\n",
    "#       {\"min\": int, \"max\": int, \"priority\": float}\n",
    "#       - min: minimum tokens needed (hard constraint)\n",
    "#       - max: maximum tokens useful (soft cap)\n",
    "#       - priority: how much this component benefits from extra tokens (0.0 to 1.0)\n",
    "#   max_tokens: int ‚Äî the total context window size\n",
    "#\n",
    "# Algorithm:\n",
    "#   1. Start by giving each component its minimum\n",
    "#   2. Calculate remaining tokens after all minimums\n",
    "#   3. If remaining < 0, raise ValueError (constraints are infeasible)\n",
    "#   4. Distribute remaining tokens proportionally by priority,\n",
    "#      but never exceed a component's max\n",
    "#   5. If there are still tokens left after all components hit max,\n",
    "#      add them to \"Reserved for Output\"\n",
    "#\n",
    "# Return: Dict[str, int] ‚Äî the optimized allocation\n",
    "# ============ TODO ============\n",
    "\n",
    "def budget_optimizer(\n",
    "    constraints: Dict[str, Dict],\n",
    "    max_tokens: int\n",
    ") -> Dict[str, int]:\n",
    "    \"\"\"Find the optimal token allocation given constraints and a total budget.\n",
    "\n",
    "    Args:\n",
    "        constraints: Per-component constraints with min, max, and priority.\n",
    "        max_tokens: Total context window size.\n",
    "\n",
    "    Returns:\n",
    "        Optimized allocation as {component_name: token_count}.\n",
    "    \"\"\"\n",
    "    # Step 1: Assign minimums\n",
    "    allocation = {comp: info[\"min\"] for comp, info in constraints.items()}\n",
    "\n",
    "    # Step 2: Calculate remaining\n",
    "    remaining = max_tokens - sum(allocation.values())\n",
    "\n",
    "    # Step 3: Check feasibility\n",
    "    if remaining < 0:\n",
    "        raise ValueError(\n",
    "            f\"Infeasible! Minimums sum to {sum(allocation.values()):,} \"\n",
    "            f\"but model limit is {max_tokens:,}. \"\n",
    "            f\"Over by {-remaining:,} tokens.\"\n",
    "        )\n",
    "\n",
    "    # Step 4: Distribute remaining by priority, respecting maxes\n",
    "    # We may need multiple passes because when one component hits its max,\n",
    "    # leftover tokens redistribute to others.\n",
    "    components_with_room = {\n",
    "        comp: info for comp, info in constraints.items()\n",
    "        if allocation[comp] < info[\"max\"]\n",
    "    }\n",
    "\n",
    "    while remaining > 0 and components_with_room:\n",
    "        total_priority = sum(info[\"priority\"] for info in components_with_room.values())\n",
    "        if total_priority == 0:\n",
    "            break\n",
    "\n",
    "        distributed_this_round = 0\n",
    "        newly_maxed = []\n",
    "\n",
    "        for comp, info in components_with_room.items():\n",
    "            share = int(remaining * (info[\"priority\"] / total_priority))\n",
    "            room = info[\"max\"] - allocation[comp]\n",
    "            addition = min(share, room)\n",
    "            allocation[comp] += addition\n",
    "            distributed_this_round += addition\n",
    "\n",
    "            if allocation[comp] >= info[\"max\"]:\n",
    "                newly_maxed.append(comp)\n",
    "\n",
    "        remaining -= distributed_this_round\n",
    "\n",
    "        # Remove maxed-out components\n",
    "        for comp in newly_maxed:\n",
    "            del components_with_room[comp]\n",
    "\n",
    "        # Safety: if no progress was made, break to avoid infinite loop\n",
    "        if distributed_this_round == 0:\n",
    "            break\n",
    "\n",
    "    # Step 5: Any leftover goes to output reservation\n",
    "    if remaining > 0:\n",
    "        if \"Reserved for Output\" in allocation:\n",
    "            allocation[\"Reserved for Output\"] += remaining\n",
    "        else:\n",
    "            allocation[\"Reserved for Output\"] = remaining\n",
    "\n",
    "    return allocation\n",
    "\n",
    "\n",
    "# ============ VERIFICATION ============\n",
    "\n",
    "test_constraints = {\n",
    "    \"System Prompt\":       {\"min\": 1_000, \"max\": 3_000,  \"priority\": 0.1},\n",
    "    \"Conversation History\": {\"min\": 5_000, \"max\": 30_000, \"priority\": 0.2},\n",
    "    \"Retrieved Context (RAG)\": {\"min\": 40_000, \"max\": 70_000, \"priority\": 0.4},\n",
    "    \"Tool Results\":        {\"min\": 2_000, \"max\": 15_000, \"priority\": 0.1},\n",
    "    \"User Message\":        {\"min\": 500,   \"max\": 2_000,  \"priority\": 0.05},\n",
    "    \"Reserved for Output\": {\"min\": 20_000,\"max\": 50_000, \"priority\": 0.15},\n",
    "}\n",
    "\n",
    "optimized = budget_optimizer(test_constraints, max_tokens=128_000)\n",
    "\n",
    "print(\"‚úÖ Optimized Budget Allocation:\")\n",
    "print(f\"{'Component':<30} {'Tokens':>10} {'Min':>8} {'Max':>8}\")\n",
    "print(\"-\" * 58)\n",
    "for comp, tokens in optimized.items():\n",
    "    info = test_constraints[comp]\n",
    "    status = \"‚úì\" if info[\"min\"] <= tokens <= info[\"max\"] else \"‚úó\"\n",
    "    print(f\"{comp:<30} {tokens:>10,} {info['min']:>8,} {info['max']:>8,}  {status}\")\n",
    "\n",
    "total = sum(optimized.values())\n",
    "print(\"-\" * 58)\n",
    "print(f\"{'Total':<30} {total:>10,}\")\n",
    "print(f\"\\n{'Budget used:':<30} {total:,} / 128,000 ({total/128_000*100:.1f}%)\")\n",
    "assert total <= 128_000, f\"Over budget! {total:,} > 128,000\"\n",
    "assert all(optimized[c] >= test_constraints[c][\"min\"] for c in test_constraints), \"Minimums violated!\"\n",
    "print(\"‚úÖ All constraints satisfied!\")"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Comparison Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_comparison_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_14_comparison_viz"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Let us visualize the optimized allocation alongside the default one to see the difference."
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_budget_comparison(budgets: Dict[str, Dict[str, int]], model_limit: int):\n",
    "    \"\"\"Compare multiple budget allocations side by side.\"\"\"\n",
    "    fig, axes = plt.subplots(len(budgets), 1, figsize=(13, 3 * len(budgets)),\n",
    "                             sharex=True)\n",
    "    if len(budgets) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (budget_name, budget) in zip(axes, budgets.items()):\n",
    "        total = sum(budget.values())\n",
    "        left = 0\n",
    "\n",
    "        for comp, val in budget.items():\n",
    "            color = COMPONENT_COLORS.get(comp, '#999999')\n",
    "            ax.barh(0, val, left=left, color=color, edgecolor='white',\n",
    "                    linewidth=1.5, height=0.5)\n",
    "            if val / model_limit > 0.04:\n",
    "                ax.text(left + val / 2, 0, f\"{val // 1000}K\",\n",
    "                        ha='center', va='center', fontsize=9,\n",
    "                        fontweight='bold', color='white')\n",
    "            left += val\n",
    "\n",
    "        ax.axvline(x=model_limit, color='red', linewidth=2, linestyle='--')\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(f\"{budget_name} ‚Äî Total: {total:,} tokens ({total/model_limit*100:.0f}%)\",\n",
    "                     fontsize=12, fontweight='bold')\n",
    "        ax.set_xlim(0, model_limit * 1.05)\n",
    "\n",
    "    axes[-1].set_xlabel('Tokens', fontsize=12)\n",
    "\n",
    "    legend_patches = [mpatches.Patch(color=color, label=comp)\n",
    "                      for comp, color in COMPONENT_COLORS.items()]\n",
    "    fig.legend(handles=legend_patches, loc='upper center',\n",
    "               bbox_to_anchor=(0.5, -0.02), ncol=3, fontsize=9, frameon=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_budget_comparison({\n",
    "    \"Default (Hand-Tuned)\": DEFAULT_BUDGET,\n",
    "    \"Optimizer (RAG-Heavy Constraints)\": optimized,\n",
    "}, model_limit=128_000)"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Notice the difference?** The optimizer respected our constraint that RAG needs at least 40K tokens, and distributed the remaining budget according to priorities. The hand-tuned budget assumed fixed allocations. In practice, you would run the optimizer for each use case ‚Äî a chatbot needs more history, a RAG system needs more retrieval space, a code generator needs more output reservation."
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Dashboard\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/15_dashboard.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_15_dashboard"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. üìä Putting It All Together: The Final Dashboard\n",
    "\n",
    "Let us build the capstone visualization ‚Äî a multi-panel dashboard that shows everything we have learned in one view."
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dashboard(base_budget: Dict[str, int], model_limits: Dict[str, int]):\n",
    "    \"\"\"Create a comprehensive context engineering dashboard.\n",
    "\n",
    "    Panel 1: Pie chart of budget components (proportions)\n",
    "    Panel 2: What happens when RAG overflows (bar chart progression)\n",
    "    Panel 3: Optimal vs suboptimal allocation comparison\n",
    "    Panel 4: Multi-model scaling (how budget maps to different models)\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(16, 14))\n",
    "    fig.suptitle('Context Engineering Dashboard', fontsize=18, fontweight='bold', y=0.98)\n",
    "\n",
    "    # ‚îÄ‚îÄ Panel 1: Budget Proportions (Pie) ‚îÄ‚îÄ\n",
    "    ax1 = fig.add_subplot(2, 2, 1)\n",
    "    components = list(base_budget.keys())\n",
    "    values = list(base_budget.values())\n",
    "    colors = [COMPONENT_COLORS.get(c, '#999') for c in components]\n",
    "\n",
    "    wedges, texts, autotexts = ax1.pie(\n",
    "        values, labels=None, autopct='%1.0f%%', colors=colors,\n",
    "        startangle=90, pctdistance=0.75,\n",
    "        wedgeprops=dict(width=0.5, edgecolor='white', linewidth=2)\n",
    "    )\n",
    "    for t in autotexts:\n",
    "        t.set_fontsize(9)\n",
    "        t.set_fontweight('bold')\n",
    "\n",
    "    ax1.set_title('Budget Proportions (128K Model)', fontsize=12, fontweight='bold', pad=15)\n",
    "    ax1.legend(components, loc='center left', bbox_to_anchor=(-0.3, 0.5), fontsize=8)\n",
    "\n",
    "    # ‚îÄ‚îÄ Panel 2: RAG Overflow Progression ‚îÄ‚îÄ\n",
    "    ax2 = fig.add_subplot(2, 2, 2)\n",
    "\n",
    "    rag_scenarios = [40_000, 60_000, 80_000, 100_000]\n",
    "    scenario_labels = ['40K (Light)', '60K (Normal)', '80K (Heavy)', '100K (Extreme)']\n",
    "\n",
    "    for i, (rag_val, label) in enumerate(zip(rag_scenarios, scenario_labels)):\n",
    "        scenario = base_budget.copy()\n",
    "        scenario[\"Retrieved Context (RAG)\"] = rag_val\n",
    "\n",
    "        left = 0\n",
    "        for comp, val in scenario.items():\n",
    "            color = COMPONENT_COLORS.get(comp, '#999')\n",
    "            ax2.barh(i, val, left=left, color=color, edgecolor='white',\n",
    "                     linewidth=0.5, height=0.6)\n",
    "            left += val\n",
    "\n",
    "    ax2.axvline(x=128_000, color='red', linewidth=2, linestyle='--', label='128K Limit')\n",
    "    ax2.set_yticks(range(len(scenario_labels)))\n",
    "    ax2.set_yticklabels(scenario_labels, fontsize=9)\n",
    "    ax2.set_xlabel('Tokens', fontsize=10)\n",
    "    ax2.set_title('RAG Overflow Scenarios', fontsize=12, fontweight='bold')\n",
    "    ax2.legend(fontsize=9)\n",
    "    ax2.set_xlim(0, 180_000)\n",
    "\n",
    "    # ‚îÄ‚îÄ Panel 3: Optimal vs Suboptimal ‚îÄ‚îÄ\n",
    "    ax3 = fig.add_subplot(2, 2, 3)\n",
    "\n",
    "    # \"Suboptimal\" = naive equal split\n",
    "    equal_split = {comp: 128_000 // 6 for comp in base_budget.keys()}\n",
    "    # Adjust to sum exactly to 128K\n",
    "    diff = 128_000 - sum(equal_split.values())\n",
    "    first_key = list(equal_split.keys())[0]\n",
    "    equal_split[first_key] += diff\n",
    "\n",
    "    allocations = {\"Optimized\\n(Proportional)\": base_budget, \"Naive\\n(Equal Split)\": equal_split}\n",
    "\n",
    "    for j, (alloc_name, alloc) in enumerate(allocations.items()):\n",
    "        left = 0\n",
    "        for comp, val in alloc.items():\n",
    "            color = COMPONENT_COLORS.get(comp, '#999')\n",
    "            ax3.barh(j, val, left=left, color=color, edgecolor='white',\n",
    "                     linewidth=0.5, height=0.5)\n",
    "            left += val\n",
    "\n",
    "    ax3.axvline(x=128_000, color='red', linewidth=2, linestyle='--')\n",
    "    ax3.set_yticks(range(len(allocations)))\n",
    "    ax3.set_yticklabels(list(allocations.keys()), fontsize=10)\n",
    "    ax3.set_xlabel('Tokens', fontsize=10)\n",
    "    ax3.set_title('Optimized vs Naive Allocation', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xlim(0, 140_000)\n",
    "\n",
    "    # ‚îÄ‚îÄ Panel 4: Multi-Model Scaling ‚îÄ‚îÄ\n",
    "    ax4 = fig.add_subplot(2, 2, 4)\n",
    "\n",
    "    model_names = list(model_limits.keys())\n",
    "    model_totals = list(model_limits.values())\n",
    "\n",
    "    # For each model, show how much of the default budget fits\n",
    "    for i, (name, limit) in enumerate(model_limits.items()):\n",
    "        scaled = scale_budget_to_model(base_budget, 128_000, limit)\n",
    "        left = 0\n",
    "        for comp, val in scaled.items():\n",
    "            color = COMPONENT_COLORS.get(comp, '#999')\n",
    "            ax4.barh(i, val, left=left, color=color, edgecolor='white',\n",
    "                     linewidth=0.5, height=0.5)\n",
    "            left += val\n",
    "\n",
    "        ax4.text(sum(scaled.values()) * 1.05, i, f'{limit:,}',\n",
    "                va='center', fontsize=8, color='gray')\n",
    "\n",
    "    ax4.set_yticks(range(len(model_names)))\n",
    "    ax4.set_yticklabels(model_names, fontsize=9)\n",
    "    ax4.set_xlabel('Tokens', fontsize=10)\n",
    "    ax4.set_title('Budget Scaling by Model Size', fontsize=12, fontweight='bold')\n",
    "    ax4.set_xscale('symlog', linthresh=1000)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "create_dashboard(DEFAULT_BUDGET, MODEL_LIMITS)"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: End To End\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/16_end_to_end.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_16_end_to_end"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. üéØ Training / Results: What We Built\n",
    "\n",
    "Let us run one final end-to-end example that ties everything together. Imagine you are building a customer support chatbot with RAG. Let us plan its token budget."
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world scenario: Customer Support Chatbot\n",
    "print(\"=\" * 60)\n",
    "print(\"  SCENARIO: Customer Support Chatbot with RAG\")\n",
    "print(\"  Model: GPT-4 Turbo (128K context window)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Define our system prompt and measure it\n",
    "system_prompt = \"\"\"You are a helpful customer support agent for TechCorp.\n",
    "\n",
    "Rules:\n",
    "- Always be polite and professional\n",
    "- If you don't know the answer, say so honestly\n",
    "- Reference specific documentation when possible\n",
    "- Never make up product features or pricing\n",
    "- For billing issues, always recommend contacting billing@techcorp.com\n",
    "- Format responses in clear, numbered steps when giving instructions\"\"\"\n",
    "\n",
    "system_tokens = estimate_tokens(system_prompt, method=\"both\")\n",
    "print(f\"\\n1. System prompt measured: {system_tokens}\")\n",
    "\n",
    "# Step 2: Estimate typical conversation history (5 turns)\n",
    "sample_turn = \"Customer: I'm having trouble resetting my password. Agent: I'd be happy to help! Let me walk you through the password reset process step by step.\"\n",
    "turn_tokens = estimate_tokens(sample_turn, method=\"exact\")\n",
    "history_estimate = turn_tokens * 10  # 10 turns of conversation\n",
    "print(f\"2. History estimate (10 turns): ~{history_estimate:,} tokens\")\n",
    "\n",
    "# Step 3: Define constraints based on our application\n",
    "chatbot_constraints = {\n",
    "    \"System Prompt\":       {\"min\": system_tokens[\"exact\"], \"max\": system_tokens[\"exact\"] + 500, \"priority\": 0.05},\n",
    "    \"Conversation History\": {\"min\": 5_000,  \"max\": 25_000, \"priority\": 0.25},\n",
    "    \"Retrieved Context (RAG)\": {\"min\": 30_000, \"max\": 65_000, \"priority\": 0.35},\n",
    "    \"Tool Results\":        {\"min\": 3_000,  \"max\": 12_000, \"priority\": 0.1},\n",
    "    \"User Message\":        {\"min\": 500,    \"max\": 3_000,  \"priority\": 0.05},\n",
    "    \"Reserved for Output\": {\"min\": 15_000, \"max\": 40_000, \"priority\": 0.2},\n",
    "}\n",
    "\n",
    "# Step 4: Optimize\n",
    "chatbot_budget = budget_optimizer(chatbot_constraints, max_tokens=128_000)\n",
    "\n",
    "print(f\"\\n3. Optimized budget:\")\n",
    "for comp, tokens in chatbot_budget.items():\n",
    "    print(f\"   {comp:<30} {tokens:>8,} tokens\")\n",
    "print(f\"   {'‚îÄ' * 40}\")\n",
    "print(f\"   {'TOTAL':<30} {sum(chatbot_budget.values()):>8,} tokens\")\n",
    "\n",
    "# Step 5: Analyze\n",
    "print()\n",
    "report = budget_calculator(chatbot_budget, 128_000)\n",
    "print_budget_report(report)"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization: our chatbot's budget\n",
    "plot_context_budget(chatbot_budget, model_limit=128_000,\n",
    "                    title=\"üéØ Optimized Budget: Customer Support Chatbot (128K)\")"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Reflection And Close\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/17_reflection_and_close.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_17_reflection_and_close"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. ü§î Reflection\n",
    "\n",
    "Let us step back and consolidate what we have learned.\n",
    "\n",
    "**The Big Ideas:**\n",
    "\n",
    "1. **A context window is a fixed-size desk.** Everything ‚Äî system instructions, conversation history, retrieved documents, tool outputs, the user's question, and the model's response ‚Äî must fit on that desk. If it does not fit, something falls off.\n",
    "\n",
    "2. **Tokens are the unit of measurement, not characters or words.** The rough rule of ~4 characters per token is useful for quick estimates, but for production systems, always use the actual tokenizer (tiktoken, sentencepiece, etc.).\n",
    "\n",
    "3. **The six components compete for the same budget:**\n",
    "   $$T_{\\text{total}} = T_{\\text{system}} + T_{\\text{history}} + T_{\\text{RAG}} + T_{\\text{tools}} + T_{\\text{user}} + T_{\\text{reserved}}$$\n",
    "\n",
    "4. **RAG is the biggest budget consumer** in most applications, and also the most variable. Over-retrieval is the most common cause of context overflow.\n",
    "\n",
    "5. **Always reserve output space.** Forgetting to budget for the model's response is a surprisingly common mistake that leads to truncated outputs.\n",
    "\n",
    "6. **Budget optimization is a constrained resource allocation problem.** Define your minimums, set priorities, and let the math distribute the rest.\n",
    "\n",
    "**What is next?** In Part 2 of this series, we will dive into **prompt architecture** ‚Äî how to structure the *content* within each budget slot to maximize the model's performance. Knowing *how much* space you have is only half the battle; knowing *what to put in that space* is where context engineering truly becomes an art."
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary ‚Äî one function to rule them all\n",
    "def context_engineering_summary():\n",
    "    \"\"\"Print a quick-reference summary of context engineering principles.\"\"\"\n",
    "    print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë              CONTEXT ENGINEERING CHEAT SHEET                ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                              ‚ïë\n",
    "‚ïë  Token Estimation:                                           ‚ïë\n",
    "‚ïë    ‚Ä¢ Rough: len(text) // 4                                   ‚ïë\n",
    "‚ïë    ‚Ä¢ Exact: tiktoken.get_encoding(\"cl100k_base\").encode(t)   ‚ïë\n",
    "‚ïë                                                              ‚ïë\n",
    "‚ïë  The Budget Equation:                                        ‚ïë\n",
    "‚ïë    T_total = T_sys + T_hist + T_rag + T_tools + T_user + T_out ‚ïë\n",
    "‚ïë                                                              ‚ïë\n",
    "‚ïë  Typical 128K Allocation:                                    ‚ïë\n",
    "‚ïë    System:  2K  ‚îÇ History: 20K ‚îÇ RAG: 60K                    ‚ïë\n",
    "‚ïë    Tools:  10K  ‚îÇ User:     1K ‚îÇ Output: 35K                 ‚ïë\n",
    "‚ïë                                                              ‚ïë\n",
    "‚ïë  Key Rules:                                                  ‚ïë\n",
    "‚ïë    1. Always reserve output space (‚â•10% of window)           ‚ïë\n",
    "‚ïë    2. RAG is your biggest lever ‚Äî control retrieval volume    ‚ïë\n",
    "‚ïë    3. System prompts are taxed on every call ‚Äî keep concise  ‚ïë\n",
    "‚ïë    4. Measure, don't guess ‚Äî use exact token counts          ‚ïë\n",
    "‚ïë    5. Bigger window ‚â† better ‚Äî irrelevant context hurts      ‚ïë\n",
    "‚ïë                                                              ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "    \"\"\")\n",
    "\n",
    "context_engineering_summary()\n",
    "print(\"‚úÖ Notebook complete! You now have the tools to budget any context window.\")\n",
    "print(\"üìä Next up: Part 2 ‚Äî Prompt Architecture & Information Ordering\")"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "chatbot"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üí¨ AI Teaching Assistant ‚Äî Click ‚ñ∂ to start\n",
    "#@markdown This AI chatbot reads your notebook and can answer questions about any concept, code, or exercise.\n",
    "\n",
    "import json as _json\n",
    "import requests as _requests\n",
    "from google.colab import output as _output\n",
    "from IPython.display import display, HTML as _HTML, Markdown as _Markdown\n",
    "\n",
    "# --- Read notebook content for context ---\n",
    "def _get_notebook_context():\n",
    "    try:\n",
    "        from google.colab import _message\n",
    "        nb = _message.blocking_request(\"get_ipynb\", request=\"\", timeout_sec=10)\n",
    "        cells = nb.get(\"ipynb\", {}).get(\"cells\", [])\n",
    "        parts = []\n",
    "        for cell in cells:\n",
    "            src = \"\".join(cell.get(\"source\", []))\n",
    "            tags = cell.get(\"metadata\", {}).get(\"tags\", [])\n",
    "            if \"chatbot\" in tags:\n",
    "                continue\n",
    "            if src.strip():\n",
    "                ct = cell.get(\"cell_type\", \"unknown\")\n",
    "                parts.append(f\"[{ct.upper()}]\\n{src}\")\n",
    "        return \"\\n\\n---\\n\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"Notebook content unavailable.\"\n",
    "\n",
    "_NOTEBOOK_CONTEXT = _get_notebook_context()\n",
    "_CHAT_HISTORY = []\n",
    "_API_URL = \"https://course-creator-brown.vercel.app/api/chat\"\n",
    "\n",
    "def _notebook_chat(question):\n",
    "    global _CHAT_HISTORY\n",
    "    try:\n",
    "        resp = _requests.post(_API_URL, json={\n",
    "            'question': question,\n",
    "            'context': _NOTEBOOK_CONTEXT[:100000],\n",
    "            'history': _CHAT_HISTORY[-10:],\n",
    "        }, timeout=60)\n",
    "        data = resp.json()\n",
    "        answer = data.get('answer', 'Sorry, I could not generate a response.')\n",
    "        _CHAT_HISTORY.append({'role': 'user', 'content': question})\n",
    "        _CHAT_HISTORY.append({'role': 'assistant', 'content': answer})\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f'Error connecting to teaching assistant: {str(e)}'\n",
    "\n",
    "_output.register_callback('notebook_chat', _notebook_chat)\n",
    "\n",
    "def ask(question):\n",
    "    \"\"\"Ask the AI teaching assistant a question about this notebook.\"\"\"\n",
    "    answer = _notebook_chat(question)\n",
    "    display(_Markdown(answer))\n",
    "\n",
    "print(\"\\u2705 AI Teaching Assistant is ready!\")\n",
    "print(\"\\U0001f4a1 Use the chat below, or call ask(\\'your question\\') in any cell.\")\n",
    "\n",
    "# --- Display chat widget ---\n",
    "display(_HTML('''<style>\n  .vc-wrap{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;max-width:100%;border-radius:16px;overflow:hidden;box-shadow:0 4px 24px rgba(0,0,0,.12);background:#fff;border:1px solid #e5e7eb}\n  .vc-hdr{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;padding:16px 20px;display:flex;align-items:center;gap:12px}\n  .vc-avatar{width:42px;height:42px;background:rgba(255,255,255,.2);border-radius:50%;display:flex;align-items:center;justify-content:center;font-size:22px}\n  .vc-hdr h3{font-size:16px;font-weight:600;margin:0}\n  .vc-hdr p{font-size:12px;opacity:.85;margin:2px 0 0}\n  .vc-msgs{height:420px;overflow-y:auto;padding:16px;background:#f8f9fb;display:flex;flex-direction:column;gap:10px}\n  .vc-msg{display:flex;flex-direction:column;animation:vc-fade .25s ease}\n  .vc-msg.user{align-items:flex-end}\n  .vc-msg.bot{align-items:flex-start}\n  .vc-bbl{max-width:85%;padding:10px 14px;border-radius:16px;font-size:14px;line-height:1.55;word-wrap:break-word}\n  .vc-msg.user .vc-bbl{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border-bottom-right-radius:4px}\n  .vc-msg.bot .vc-bbl{background:#fff;color:#1a1a2e;border:1px solid #e8e8e8;border-bottom-left-radius:4px}\n  .vc-bbl code{background:rgba(0,0,0,.07);padding:2px 6px;border-radius:4px;font-size:13px;font-family:'Fira Code',monospace}\n  .vc-bbl pre{background:#1e1e2e;color:#cdd6f4;padding:12px;border-radius:8px;overflow-x:auto;margin:8px 0;font-size:13px}\n  .vc-bbl pre code{background:none;padding:0;color:inherit}\n  .vc-bbl h3,.vc-bbl h4{margin:10px 0 4px;font-size:15px}\n  .vc-bbl ul,.vc-bbl ol{margin:4px 0;padding-left:20px}\n  .vc-bbl li{margin:2px 0}\n  .vc-chips{display:flex;flex-wrap:wrap;gap:8px;padding:0 16px 12px;background:#f8f9fb}\n  .vc-chip{background:#fff;border:1px solid #d1d5db;border-radius:20px;padding:6px 14px;font-size:12px;cursor:pointer;transition:all .15s;color:#4b5563}\n  .vc-chip:hover{border-color:#667eea;color:#667eea;background:#f0f0ff}\n  .vc-input{display:flex;padding:12px 16px;background:#fff;border-top:1px solid #eee;gap:8px}\n  .vc-input input{flex:1;padding:10px 16px;border:2px solid #e8e8e8;border-radius:24px;font-size:14px;outline:none;transition:border-color .2s}\n  .vc-input input:focus{border-color:#667eea}\n  .vc-input button{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border:none;border-radius:50%;width:42px;height:42px;cursor:pointer;display:flex;align-items:center;justify-content:center;font-size:18px;transition:transform .1s}\n  .vc-input button:hover{transform:scale(1.05)}\n  .vc-input button:disabled{opacity:.5;cursor:not-allowed;transform:none}\n  .vc-typing{display:flex;gap:5px;padding:4px 0}\n  .vc-typing span{width:8px;height:8px;background:#667eea;border-radius:50%;animation:vc-bounce 1.4s infinite ease-in-out}\n  .vc-typing span:nth-child(2){animation-delay:.2s}\n  .vc-typing span:nth-child(3){animation-delay:.4s}\n  @keyframes vc-bounce{0%,80%,100%{transform:scale(0)}40%{transform:scale(1)}}\n  @keyframes vc-fade{from{opacity:0;transform:translateY(8px)}to{opacity:1;transform:translateY(0)}}\n  .vc-note{text-align:center;font-size:11px;color:#9ca3af;padding:8px 16px 12px;background:#fff}\n</style>\n<div class=\"vc-wrap\">\n  <div class=\"vc-hdr\">\n    <div class=\"vc-avatar\">&#129302;</div>\n    <div>\n      <h3>Vizuara Teaching Assistant</h3>\n      <p>Ask me anything about this notebook</p>\n    </div>\n  </div>\n  <div class=\"vc-msgs\" id=\"vcMsgs\">\n    <div class=\"vc-msg bot\">\n      <div class=\"vc-bbl\">&#128075; Hi! I've read through this entire notebook. Ask me about any concept, code block, or exercise &mdash; I'm here to help you learn!</div>\n    </div>\n  </div>\n  <div class=\"vc-chips\" id=\"vcChips\">\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Explain the main concept</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Help with the TODO exercise</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Summarize what I learned</span>\n  </div>\n  <div class=\"vc-input\">\n    <input type=\"text\" id=\"vcIn\" placeholder=\"Ask about concepts, code, exercises...\" />\n    <button id=\"vcSend\" onclick=\"vcSendMsg()\">&#10148;</button>\n  </div>\n  <div class=\"vc-note\">AI-generated &middot; Verify important information &middot; <a href=\"#\" onclick=\"vcClear();return false\" style=\"color:#667eea\">Clear chat</a></div>\n</div>\n<script>\n(function(){\n  var msgs=document.getElementById('vcMsgs'),inp=document.getElementById('vcIn'),\n      btn=document.getElementById('vcSend'),chips=document.getElementById('vcChips');\n\n  function esc(s){var d=document.createElement('div');d.textContent=s;return d.innerHTML}\n\n  function md(t){\n    return t\n      .replace(/```(\\w*)\\n([\\s\\S]*?)```/g,function(_,l,c){return '<pre><code>'+esc(c)+'</code></pre>'})\n      .replace(/`([^`]+)`/g,'<code>$1</code>')\n      .replace(/\\*\\*([^*]+)\\*\\*/g,'<strong>$1</strong>')\n      .replace(/\\*([^*]+)\\*/g,'<em>$1</em>')\n      .replace(/^#### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^## (.+)$/gm,'<h3>$1</h3>')\n      .replace(/^\\d+\\. (.+)$/gm,'<li>$1</li>')\n      .replace(/^- (.+)$/gm,'<li>$1</li>')\n      .replace(/\\n\\n/g,'<br><br>')\n      .replace(/\\n/g,'<br>');\n  }\n\n  function addMsg(text,isUser){\n    var m=document.createElement('div');m.className='vc-msg '+(isUser?'user':'bot');\n    var b=document.createElement('div');b.className='vc-bbl';\n    b.innerHTML=isUser?esc(text):md(text);\n    m.appendChild(b);msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function showTyping(){\n    var m=document.createElement('div');m.className='vc-msg bot';m.id='vcTyping';\n    m.innerHTML='<div class=\"vc-bbl\"><div class=\"vc-typing\"><span></span><span></span><span></span></div></div>';\n    msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function hideTyping(){var e=document.getElementById('vcTyping');if(e)e.remove()}\n\n  window.vcSendMsg=function(){\n    var q=inp.value.trim();if(!q)return;\n    inp.value='';chips.style.display='none';\n    addMsg(q,true);showTyping();btn.disabled=true;\n    google.colab.kernel.invokeFunction('notebook_chat',[q],{})\n      .then(function(r){\n        hideTyping();\n        var a=r.data['application/json'];\n        addMsg(typeof a==='string'?a:JSON.stringify(a),false);\n      })\n      .catch(function(){\n        hideTyping();\n        addMsg('Sorry, I encountered an error. Please check your internet connection and try again.',false);\n      })\n      .finally(function(){btn.disabled=false;inp.focus()});\n  };\n\n  window.vcAsk=function(q){inp.value=q;vcSendMsg()};\n  window.vcClear=function(){\n    msgs.innerHTML='<div class=\"vc-msg bot\"><div class=\"vc-bbl\">&#128075; Chat cleared. Ask me anything!</div></div>';\n    chips.style.display='flex';\n  };\n\n  inp.addEventListener('keypress',function(e){if(e.key==='Enter')vcSendMsg()});\n  inp.focus();\n})();\n</script>'''))"
   ],
   "id": "vizuara_chatbot"
  }
 ]
}