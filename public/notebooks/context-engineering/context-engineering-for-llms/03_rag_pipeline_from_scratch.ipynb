{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Building a RAG Pipeline from Scratch â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1_yOuaRupWcvvBB5tNnjVrtDllXqg6x4Q\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/03_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_01_setup_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Setup Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_01_setup_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Building a RAG Pipeline from Scratch: From Documents to Answers\n",
    "\n",
    "*Part 3 of the Vizuara series on Context Engineering for LLMs*\n",
    "\n",
    "**Estimated time: 40 minutes**\n",
    "\n",
    "In the previous notebooks, we explored what context engineering is and how the context window works. Now we tackle the single most common context engineering pattern in production: **Retrieval-Augmented Generation (RAG)**.\n",
    "\n",
    "By the end of this notebook, you will have built a complete RAG pipeline â€” from raw text to assembled context â€” entirely from scratch. No black-box vector databases. No magic APIs. Just numpy, a small embedding model, and your own code."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://pods.vizuara.ai/courses/context-engineering-for-llms/practice/3/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_02_install_imports",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Install Imports\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_02_install_imports.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Installations\n",
    "\n",
    "Let us install the libraries we need. We use `sentence-transformers` for embedding text (this is a **tool**, not the concept we are learning â€” the concept is the retrieval pipeline itself)."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers matplotlib numpy scikit-learn"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import textwrap\n",
    "import re\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting defaults\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "\n",
    "print(\"Setup complete!\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_03_why_rag",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why Rag\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_03_why_rag.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Imagine you are a brilliant student walking into an open-book exam. Your intelligence is fixed â€” you cannot suddenly become smarter. But your performance depends on **what notes you brought** and **how fast you can find the right page**.\n",
    "\n",
    "Large Language Models face the same challenge. Their \"intelligence\" (weights) is frozen after training. But we can dramatically improve their answers by retrieving the right information and injecting it into their context window. This is RAG.\n",
    "\n",
    "**Here is the problem RAG solves:** An LLM trained in 2024 does not know about events in 2025. It does not know your company's internal documentation. It does not know the specifics of your codebase. But if we *retrieve* the right documents and *augment* the prompt with them, the LLM can answer as if it knew everything.\n",
    "\n",
    "The RAG pipeline has seven steps:\n",
    "\n",
    "1. User sends a query\n",
    "2. Query is converted to an embedding (a vector of numbers)\n",
    "3. Compare against pre-computed document chunk embeddings\n",
    "4. Retrieve top-K most similar chunks\n",
    "5. Pass through a reranker for a second relevance check\n",
    "6. Inject selected chunks into the context window\n",
    "7. LLM generates a response using training knowledge + retrieved context\n",
    "\n",
    "We will build steps 1 through 6 from scratch. Step 7 just needs an LLM API call â€” the interesting engineering is everything before it."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_04_rag_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Rag Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_04_rag_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### The Open-Book Exam Analogy\n",
    "\n",
    "Let us make the three key design decisions in RAG concrete with our exam analogy:\n",
    "\n",
    "| RAG Concept | Exam Analogy | What Happens If You Get It Wrong |\n",
    "|---|---|---|\n",
    "| **Chunk size** | Size of your note cards | Too small = confetti (no context). Too large = wastes space. |\n",
    "| **Top-K** | How many cards you bring | Too few = miss the answer. Too many = noise drowns signal. |\n",
    "| **Reranking** | Sorting best card to top | Without it, the answer is buried in the middle of your notes. |\n",
    "\n",
    "The sweet spot for chunk size is typically **400-1000 tokens** per chunk. We will experiment with this later and see the difference ourselves."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_05_think_about_this",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Think About This\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_05_think_about_this.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤” Think About This\n",
    "\n",
    "Before we write any code, consider:\n",
    "\n",
    "- Why can we not just dump the entire knowledge base into the context window? (Hint: token limits, cost, and the \"Lost in the Middle\" problem)\n",
    "- Why do we need embeddings at all? Could we just use keyword search? (Hint: \"automobile\" and \"car\" share zero keywords but mean the same thing)\n",
    "- Why would we need a reranker if we already have similarity scores? (Hint: embedding similarity is a rough first pass, not a precise relevance judgment)\n",
    "\n",
    "Take a moment to think about these. Then let us build."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_06_cosine_sim_math",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Cosine Sim Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_06_cosine_sim_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Cosine Similarity â€” The Heart of Retrieval\n",
    "\n",
    "The core mathematical operation in RAG is **cosine similarity**. Given a query vector $\\mathbf{q}$ and a document vector $\\mathbf{d}$, cosine similarity measures how \"aligned\" they are:\n",
    "\n",
    "$$\\text{sim}(\\mathbf{q}, \\mathbf{d}) = \\frac{\\mathbf{q} \\cdot \\mathbf{d}}{||\\mathbf{q}|| \\; ||\\mathbf{d}||}$$\n",
    "\n",
    "**What does this mean computationally?**\n",
    "\n",
    "1. Compute the dot product of the two vectors (multiply element-wise, then sum)\n",
    "2. Compute the magnitude (length) of each vector\n",
    "3. Divide the dot product by the product of the magnitudes\n",
    "\n",
    "The result is always between -1 and 1:\n",
    "- **1.0** = vectors point in the same direction (highly similar)\n",
    "- **0.0** = vectors are perpendicular (unrelated)\n",
    "- **-1.0** = vectors point in opposite directions (opposite meaning)"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_07_cosine_sim_ex1",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Cosine Sim Ex1\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_07_cosine_sim_ex1.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worked Example 1: Perfectly Aligned Vectors\n",
    "\n",
    "Let us compute this by hand first, then verify with code.\n",
    "\n",
    "$\\mathbf{q} = [1, 2, 3]$, $\\mathbf{d} = [2, 4, 6]$\n",
    "\n",
    "**Step 1 â€” Dot product:** $1 \\times 2 + 2 \\times 4 + 3 \\times 6 = 2 + 8 + 18 = 28$\n",
    "\n",
    "**Step 2 â€” Magnitudes:** $||\\mathbf{q}|| = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{14} \\approx 3.74$, $||\\mathbf{d}|| = \\sqrt{2^2 + 4^2 + 6^2} = \\sqrt{56} \\approx 7.48$\n",
    "\n",
    "**Step 3 â€” Divide:** $\\text{sim} = \\frac{28}{3.74 \\times 7.48} \\approx 1.0$\n",
    "\n",
    "A similarity of **1.0** â€” perfectly aligned! Notice that $\\mathbf{d} = 2 \\times \\mathbf{q}$. Cosine similarity does not care about magnitude, only direction. These vectors point the same way."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worked Example 1: Perfectly aligned vectors\n",
    "q = np.array([1, 2, 3], dtype=float)\n",
    "d = np.array([2, 4, 6], dtype=float)\n",
    "\n",
    "dot_product = np.dot(q, d)\n",
    "magnitude_q = np.linalg.norm(q)\n",
    "magnitude_d = np.linalg.norm(d)\n",
    "similarity = dot_product / (magnitude_q * magnitude_d)\n",
    "\n",
    "print(f\"q = {q}\")\n",
    "print(f\"d = {d}\")\n",
    "print(f\"Dot product: {dot_product:.1f}\")\n",
    "print(f\"||q|| = {magnitude_q:.4f}\")\n",
    "print(f\"||d|| = {magnitude_d:.4f}\")\n",
    "print(f\"Cosine similarity = {dot_product:.1f} / ({magnitude_q:.4f} Ã— {magnitude_d:.4f}) = {similarity:.4f}\")\n",
    "print(f\"\\nVerdict: {'Perfectly aligned!' if similarity > 0.99 else 'Not aligned'}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_08_cosine_sim_ex2",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Cosine Sim Ex2\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_08_cosine_sim_ex2.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worked Example 2: Barely Related Vectors\n",
    "\n",
    "Now try $\\mathbf{q} = [1, 2, 3]$, $\\mathbf{d'} = [3, -1, 0]$\n",
    "\n",
    "**Dot product:** $1 \\times 3 + 2 \\times (-1) + 3 \\times 0 = 3 - 2 + 0 = 1$\n",
    "\n",
    "**Magnitudes:** $||\\mathbf{q}|| = \\sqrt{14} \\approx 3.74$, $||\\mathbf{d'}|| = \\sqrt{9 + 1 + 0} = \\sqrt{10} \\approx 3.16$\n",
    "\n",
    "**Similarity:** $\\frac{1}{3.74 \\times 3.16} \\approx 0.08$\n",
    "\n",
    "A similarity of **0.08** â€” barely related at all. We would not retrieve this document."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worked Example 2: Barely related vectors\n",
    "d_prime = np.array([3, -1, 0], dtype=float)\n",
    "\n",
    "dot_product_2 = np.dot(q, d_prime)\n",
    "magnitude_d_prime = np.linalg.norm(d_prime)\n",
    "similarity_2 = dot_product_2 / (magnitude_q * magnitude_d_prime)\n",
    "\n",
    "print(f\"q  = {q}\")\n",
    "print(f\"d' = {d_prime}\")\n",
    "print(f\"Dot product: {dot_product_2:.1f}\")\n",
    "print(f\"||d'|| = {magnitude_d_prime:.4f}\")\n",
    "print(f\"Cosine similarity = {dot_product_2:.1f} / ({magnitude_q:.4f} Ã— {magnitude_d_prime:.4f}) = {similarity_2:.4f}\")\n",
    "print(f\"\\nVerdict: {'Barely related' if similarity_2 < 0.2 else 'Related'}\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_09_cosine_sim_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Cosine Sim Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_09_cosine_sim_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Visualization: What Cosine Similarity Looks Like\n",
    "\n",
    "Let us visualize both examples as 2D vectors (projecting to 2D for visibility) to build geometric intuition."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Helper to draw a vector\n",
    "def draw_vector(ax, vec, label, color, offset=(0.1, 0.1)):\n",
    "    ax.annotate('', xy=vec, xytext=(0, 0),\n",
    "                arrowprops=dict(arrowstyle='->', color=color, lw=2.5))\n",
    "    ax.annotate(label, xy=vec + np.array(offset), fontsize=13,\n",
    "                fontweight='bold', color=color)\n",
    "\n",
    "# Example 1: q=[1,2] vs d=[2,4] (perfectly aligned in 2D)\n",
    "ax1 = axes[0]\n",
    "q_2d = np.array([1, 2])\n",
    "d_2d = np.array([2, 4])\n",
    "draw_vector(ax1, q_2d, 'q = [1,2]', '#2196F3', offset=(0.1, -0.3))\n",
    "draw_vector(ax1, d_2d, 'd = [2,4]', '#4CAF50', offset=(0.1, -0.3))\n",
    "ax1.set_xlim(-0.5, 5)\n",
    "ax1.set_ylim(-0.5, 5)\n",
    "ax1.set_aspect('equal')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_title(f'Cosine Similarity = 1.00\\n(Perfectly Aligned)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Dimension 1')\n",
    "ax1.set_ylabel('Dimension 2')\n",
    "\n",
    "# Example 2: q=[1,2] vs d'=[3,-1] (barely related in 2D)\n",
    "ax2 = axes[1]\n",
    "q_2d_b = np.array([1, 2])\n",
    "d_prime_2d = np.array([3, -1])\n",
    "draw_vector(ax2, q_2d_b, 'q = [1,2]', '#2196F3', offset=(0.1, 0.15))\n",
    "draw_vector(ax2, d_prime_2d, \"d' = [3,-1]\", '#F44336', offset=(0.1, -0.3))\n",
    "\n",
    "# Draw the angle arc\n",
    "angle_q = np.degrees(np.arctan2(q_2d_b[1], q_2d_b[0]))\n",
    "angle_d = np.degrees(np.arctan2(d_prime_2d[1], d_prime_2d[0]))\n",
    "arc = mpatches.Arc((0, 0), 2.0, 2.0, angle=0,\n",
    "                    theta1=min(angle_d, angle_q), theta2=max(angle_d, angle_q),\n",
    "                    color='gray', lw=1.5, linestyle='--')\n",
    "ax2.add_patch(arc)\n",
    "ax2.annotate('Î¸ â‰ˆ 85Â°', xy=(1.2, 0.4), fontsize=11, color='gray')\n",
    "\n",
    "ax2.set_xlim(-0.5, 4)\n",
    "ax2.set_ylim(-1.5, 3)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_title(f'Cosine Similarity â‰ˆ 0.08\\n(Barely Related)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Dimension 1')\n",
    "ax2.set_ylabel('Dimension 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Cosine Similarity: Direction Matters, Not Magnitude',\n",
    "             fontsize=15, fontweight='bold', y=1.03)\n",
    "plt.show()"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_10_knowledge_base",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Knowledge Base\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_10_knowledge_base.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It â€” Component by Component\n",
    "\n",
    "Now we build the full RAG pipeline piece by piece. Each component is self-contained and testable.\n",
    "\n",
    "### 4.1 The Knowledge Base\n",
    "\n",
    "First, let us create our knowledge base â€” a collection of documents about machine learning concepts. We include both short definitions and longer explanatory paragraphs, because real knowledge bases have documents of varying lengths."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Short definitions (1 sentence each) ---\n",
    "short_docs = [\n",
    "    \"Batch normalization normalizes layer inputs to stabilize training and allow higher learning rates.\",\n",
    "    \"Dropout randomly deactivates neurons during training to prevent overfitting and improve generalization.\",\n",
    "    \"The transformer architecture uses self-attention mechanisms to process sequences in parallel, replacing recurrence.\",\n",
    "    \"Learning rate scheduling adjusts the optimization step size during training to improve convergence.\",\n",
    "    \"Residual connections help gradients flow through deep networks by adding skip connections between layers.\",\n",
    "    \"Attention mechanisms allow models to focus on relevant parts of the input, weighting importance dynamically.\",\n",
    "    \"Convolutional neural networks use local receptive fields and weight sharing for spatial data like images.\",\n",
    "    \"Recurrent neural networks process sequential data through hidden states that carry information across time steps.\",\n",
    "    \"Generative adversarial networks use a generator and discriminator trained in competition to produce realistic data.\",\n",
    "    \"Transfer learning reuses knowledge from pre-trained models for new tasks, reducing data and compute requirements.\",\n",
    "]\n",
    "\n",
    "# --- Longer explanatory paragraphs (2-3 sentences each) ---\n",
    "long_docs = [\n",
    "    \"Batch normalization was introduced by Ioffe and Szegedy in 2015. It works by normalizing the inputs to each layer so that they have zero mean and unit variance. This reduces internal covariate shift, stabilizes the training process, and allows the use of much higher learning rates without the risk of divergence.\",\n",
    "\n",
    "    \"Dropout was proposed by Srivastava et al. in 2014 as a simple yet effective regularization technique. During training, each neuron is randomly set to zero with probability p (typically 0.5). This prevents neurons from co-adapting and forces the network to learn redundant representations, which improves generalization to unseen data.\",\n",
    "\n",
    "    \"The transformer architecture, introduced in the landmark paper 'Attention Is All You Need' by Vaswani et al. in 2017, replaced recurrence entirely with self-attention. Each token in a sequence attends to every other token, computing attention weights that determine how much to focus on each position. This parallelism enables dramatically faster training compared to RNNs and LSTMs.\",\n",
    "\n",
    "    \"Residual connections, or skip connections, were introduced in ResNet by He et al. in 2015. They allow the gradient to flow directly through the network by adding the input of a block to its output. This simple technique enabled training of networks with over 100 layers, which was previously impossible due to vanishing gradients.\",\n",
    "\n",
    "    \"Transfer learning has become the default paradigm in modern deep learning. Instead of training a model from scratch on a small dataset, practitioners start with a model pre-trained on a large dataset like ImageNet or Common Crawl. The pre-trained weights capture general features that transfer well to downstream tasks, dramatically reducing the amount of task-specific data needed.\",\n",
    "]\n",
    "\n",
    "# Combine into a single knowledge base\n",
    "knowledge_base = short_docs + long_docs\n",
    "\n",
    "print(f\"Knowledge base: {len(knowledge_base)} documents\")\n",
    "print(f\"  - {len(short_docs)} short definitions\")\n",
    "print(f\"  - {len(long_docs)} longer paragraphs\")\n",
    "print(f\"\\nExample short doc:\\n  '{short_docs[0]}'\")\n",
    "print(f\"\\nExample long doc:\\n  '{long_docs[0][:120]}...'\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_11_text_chunking",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Text Chunking\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_11_text_chunking.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Text Chunking â€” Three Strategies\n",
    "\n",
    "Chunking is how we break documents into retrievable pieces. The chunk size determines the granularity of retrieval â€” too small and each chunk lacks context, too large and we waste precious context window space.\n",
    "\n",
    "We implement three strategies:"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_fixed_size(text: str, chunk_size: int = 200, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Strategy 1: Fixed-size chunking.\n",
    "    Split text into chunks of exactly `chunk_size` characters with overlap.\n",
    "    Simple but can break mid-sentence.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end].strip()\n",
    "        if chunk:  # Skip empty chunks\n",
    "            chunks.append(chunk)\n",
    "        start = end - overlap  # Slide window with overlap\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def chunk_by_sentences(text: str, sentences_per_chunk: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Strategy 2: Sentence-based chunking.\n",
    "    Split at sentence boundaries and group N sentences per chunk.\n",
    "    Respects natural language boundaries.\n",
    "    \"\"\"\n",
    "    # Split on sentence-ending punctuation followed by space or end\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    sentences = [s for s in sentences if s.strip()]\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), sentences_per_chunk):\n",
    "        chunk = ' '.join(sentences[i:i + sentences_per_chunk]).strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def chunk_by_paragraphs(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Strategy 3: Paragraph/section-based chunking.\n",
    "    Split at double newlines (paragraph boundaries).\n",
    "    Preserves semantic coherence within each chunk.\n",
    "    \"\"\"\n",
    "    paragraphs = re.split(r'\\n\\s*\\n', text.strip())\n",
    "    chunks = [p.strip() for p in paragraphs if p.strip()]\n",
    "    return chunks\n",
    "\n",
    "\n",
    "print(\"Three chunking strategies defined!\")\n",
    "print(\"  1. chunk_fixed_size(text, chunk_size, overlap)\")\n",
    "print(\"  2. chunk_by_sentences(text, sentences_per_chunk)\")\n",
    "print(\"  3. chunk_by_paragraphs(text)\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_12_chunking_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Chunking Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_12_chunking_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Visualization: How Chunking Strategies Differ\n",
    "\n",
    "Let us see how each strategy splits the same document. This is where intuition meets reality."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A longer sample document to chunk\n",
    "sample_document = \"\"\"Batch normalization was introduced by Ioffe and Szegedy in 2015. It works by normalizing the inputs to each layer so that they have zero mean and unit variance. This reduces internal covariate shift, stabilizes the training process, and allows the use of much higher learning rates without the risk of divergence.\n",
    "\n",
    "The key insight is that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This internal covariate shift makes training difficult because each layer must continuously adapt to a new input distribution.\n",
    "\n",
    "By normalizing inputs, batch normalization allows each layer to learn on a more stable distribution. During inference, the running mean and variance computed during training are used instead of batch statistics, ensuring consistent behavior.\"\"\"\n",
    "\n",
    "strategies = {\n",
    "    'Fixed-size (200 chars)': chunk_fixed_size(sample_document, chunk_size=200, overlap=50),\n",
    "    'Sentence-based (2 per chunk)': chunk_by_sentences(sample_document, sentences_per_chunk=2),\n",
    "    'Paragraph-based': chunk_by_paragraphs(sample_document),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(len(strategies), 1, figsize=(14, 10))\n",
    "colors_list = ['#2196F3', '#4CAF50', '#FF9800']\n",
    "\n",
    "for idx, (name, chunks) in enumerate(strategies.items()):\n",
    "    ax = axes[idx]\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Show each chunk as a colored bar\n",
    "        wrapped = textwrap.fill(chunk[:80] + ('...' if len(chunk) > 80 else ''), width=70)\n",
    "        ax.barh(i, len(chunk), color=colors_list[idx], alpha=0.7, edgecolor='white', height=0.7)\n",
    "        ax.text(5, i, f'Chunk {i+1}: \"{chunk[:55]}...\"' if len(chunk) > 55 else f'Chunk {i+1}: \"{chunk}\"',\n",
    "                va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "    ax.set_title(f'{name}  â†’  {len(chunks)} chunks', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Characters per chunk')\n",
    "    ax.set_yticks([])\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Same Document, Three Chunking Strategies',\n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "for name, chunks in strategies.items():\n",
    "    sizes = [len(c) for c in chunks]\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Chunks: {len(chunks)}, Avg size: {np.mean(sizes):.0f} chars, \"\n",
    "          f\"Min: {min(sizes)}, Max: {max(sizes)}\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_13_embedding",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Embedding\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_13_embedding.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Embedding with Sentence-Transformers\n",
    "\n",
    "Now we convert text into vectors. We use `all-MiniLM-L6-v2` â€” a small, fast model that produces 384-dimensional embeddings. This is a **tool**, not the concept. The concept is what we do with these embeddings."
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a small, fast embedding model (runs on CPU, no GPU needed)\n",
    "print(\"Loading embedding model (this takes ~10 seconds on first run)...\")\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"Model loaded! Embedding dimension: {embed_model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# Embed our entire knowledge base\n",
    "print(f\"\\nEmbedding {len(knowledge_base)} documents...\")\n",
    "start_time = time.time()\n",
    "doc_embeddings = embed_model.encode(knowledge_base, show_progress_bar=True)\n",
    "doc_embeddings = np.array(doc_embeddings)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nDone in {elapsed:.2f}s\")\n",
    "print(f\"Embedding matrix shape: {doc_embeddings.shape}\")\n",
    "print(f\"  â†’ {doc_embeddings.shape[0]} documents, {doc_embeddings.shape[1]} dimensions each\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify: each document is now a vector of 384 numbers. Let us peek at one."
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first document's embedding\n",
    "print(f\"Document: '{knowledge_base[0][:60]}...'\")\n",
    "print(f\"\\nEmbedding (first 20 values of 384):\")\n",
    "print(f\"  {doc_embeddings[0][:20]}\")\n",
    "print(f\"\\nEmbedding norm: {np.linalg.norm(doc_embeddings[0]):.4f}\")\n",
    "print(f\"  (Sentence-transformers normalizes to unit length by default)\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_14_todo_cosine_sim",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo Cosine Sim\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_14_todo_cosine_sim.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Cosine Similarity from Scratch\n",
    "\n",
    "Now for the core algorithm. We implement cosine similarity manually â€” no sklearn, no scipy. Just numpy.\n",
    "\n",
    "### ðŸ”§ TODO 1: Implement Cosine Similarity"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(query_vec: np.ndarray, doc_vecs: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between a query vector and multiple document vectors.\n",
    "\n",
    "    Args:\n",
    "        query_vec: Shape (embedding_dim,) â€” a single query embedding\n",
    "        doc_vecs:  Shape (num_docs, embedding_dim) â€” matrix of document embeddings\n",
    "\n",
    "    Returns:\n",
    "        Shape (num_docs,) â€” cosine similarity score for each document\n",
    "\n",
    "    Formula: sim(q, d) = (q Â· d) / (||q|| * ||d||)\n",
    "\n",
    "    Requirements:\n",
    "        - Use ONLY numpy (no sklearn, no scipy)\n",
    "        - Must handle the case where query_vec is 1D\n",
    "        - Must handle batched document vectors (2D matrix)\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Compute the dot product between query and all docs\n",
    "    #         Hint: matrix multiplication â€” doc_vecs @ query_vec gives (num_docs,)\n",
    "    # Step 2: Compute the magnitude (L2 norm) of the query vector\n",
    "    # Step 3: Compute the magnitude of each document vector\n",
    "    # Step 4: Divide dot products by product of magnitudes\n",
    "    # Step 5: Handle edge case â€” avoid division by zero\n",
    "    # ==============================\n",
    "\n",
    "    dot_products = ???  # YOUR CODE HERE\n",
    "    query_norm = ???    # YOUR CODE HERE\n",
    "    doc_norms = ???     # YOUR CODE HERE\n",
    "    similarities = ???  # YOUR CODE HERE\n",
    "\n",
    "    return similarities"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_15_after_todo_cosine_sim",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: After Todo Cosine Sim\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_15_after_todo_cosine_sim.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Verification: Run this cell to check your implementation\n",
    "\n",
    "# Test 1: Identical vectors should have similarity 1.0\n",
    "test_q = np.array([1.0, 0.0, 0.0])\n",
    "test_d = np.array([[1.0, 0.0, 0.0]])\n",
    "result = cosine_similarity(test_q, test_d)\n",
    "assert np.allclose(result, [1.0], atol=1e-6), f\"Test 1 failed: expected [1.0], got {result}\"\n",
    "\n",
    "# Test 2: Orthogonal vectors should have similarity 0.0\n",
    "test_d2 = np.array([[0.0, 1.0, 0.0]])\n",
    "result2 = cosine_similarity(test_q, test_d2)\n",
    "assert np.allclose(result2, [0.0], atol=1e-6), f\"Test 2 failed: expected [0.0], got {result2}\"\n",
    "\n",
    "# Test 3: Opposite vectors should have similarity -1.0\n",
    "test_d3 = np.array([[-1.0, 0.0, 0.0]])\n",
    "result3 = cosine_similarity(test_q, test_d3)\n",
    "assert np.allclose(result3, [-1.0], atol=1e-6), f\"Test 3 failed: expected [-1.0], got {result3}\"\n",
    "\n",
    "# Test 4: Batched â€” should work with multiple documents at once\n",
    "test_d4 = np.array([[1.0, 0.0, 0.0],\n",
    "                     [0.0, 1.0, 0.0],\n",
    "                     [-1.0, 0.0, 0.0]])\n",
    "result4 = cosine_similarity(test_q, test_d4)\n",
    "assert result4.shape == (3,), f\"Test 4 failed: expected shape (3,), got {result4.shape}\"\n",
    "assert np.allclose(result4, [1.0, 0.0, -1.0], atol=1e-6), f\"Test 4 failed: expected [1, 0, -1], got {result4}\"\n",
    "\n",
    "# Test 5: Worked example from the article\n",
    "q_article = np.array([1.0, 2.0, 3.0])\n",
    "d_article = np.array([[2.0, 4.0, 6.0],\n",
    "                       [3.0, -1.0, 0.0]])\n",
    "result5 = cosine_similarity(q_article, d_article)\n",
    "assert np.allclose(result5[0], 1.0, atol=1e-4), f\"Test 5a failed: expected ~1.0, got {result5[0]}\"\n",
    "assert np.allclose(result5[1], 0.0845, atol=0.01), f\"Test 5b failed: expected ~0.08, got {result5[1]}\"\n",
    "\n",
    "print(\"âœ… All 5 tests passed! Your cosine similarity implementation is correct.\")\n",
    "print(f\"   Article example: sim(q, d) = {result5[0]:.4f}, sim(q, d') = {result5[1]:.4f}\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ **Key insight:** When vectors are already L2-normalized (which sentence-transformers does by default), cosine similarity simplifies to just the dot product:\n",
    "\n",
    "$$\\text{sim}(\\mathbf{q}, \\mathbf{d}) = \\mathbf{q} \\cdot \\mathbf{d} \\quad \\text{(when } ||\\mathbf{q}|| = ||\\mathbf{d}|| = 1\\text{)}$$\n",
    "\n",
    "This is why normalized embeddings are so popular â€” retrieval becomes a single matrix multiplication."
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_16_vector_store",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Vector Store\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_16_vector_store.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Building a Vector Store\n",
    "\n",
    "Now let us build a simple vector store â€” a class that holds document embeddings and supports top-K search."
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A minimal vector store that stores document embeddings\n",
    "    and supports similarity search.\n",
    "\n",
    "    This is what Pinecone, Weaviate, and ChromaDB do at their core,\n",
    "    but we build it ourselves to understand the mechanics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.documents: List[str] = []\n",
    "        self.embeddings: Optional[np.ndarray] = None\n",
    "        self.metadata: List[Dict] = []\n",
    "\n",
    "    def add_documents(self, documents: List[str], embeddings: np.ndarray,\n",
    "                      metadata: Optional[List[Dict]] = None):\n",
    "        \"\"\"Add documents and their embeddings to the store.\"\"\"\n",
    "        self.documents.extend(documents)\n",
    "\n",
    "        if self.embeddings is None:\n",
    "            self.embeddings = embeddings\n",
    "        else:\n",
    "            self.embeddings = np.vstack([self.embeddings, embeddings])\n",
    "\n",
    "        if metadata:\n",
    "            self.metadata.extend(metadata)\n",
    "        else:\n",
    "            self.metadata.extend([{}] * len(documents))\n",
    "\n",
    "        print(f\"Added {len(documents)} documents. Total: {len(self.documents)}\")\n",
    "\n",
    "    def search(self, query_embedding: np.ndarray, top_k: int = 5) -> List[Tuple[str, float, int]]:\n",
    "        \"\"\"\n",
    "        Search for the top-K most similar documents.\n",
    "\n",
    "        Returns: List of (document_text, similarity_score, index) tuples,\n",
    "                 sorted by similarity (highest first).\n",
    "        \"\"\"\n",
    "        if self.embeddings is None:\n",
    "            return []\n",
    "\n",
    "        # Compute cosine similarity using our function\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)\n",
    "\n",
    "        # Get top-K indices (sorted by similarity, descending)\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append((\n",
    "                self.documents[idx],\n",
    "                float(similarities[idx]),\n",
    "                int(idx)\n",
    "            ))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.documents)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"SimpleVectorStore({len(self.documents)} documents, {self.embeddings.shape[1]}d embeddings)\"\n",
    "\n",
    "\n",
    "# Create our vector store and add the knowledge base\n",
    "vector_store = SimpleVectorStore()\n",
    "vector_store.add_documents(\n",
    "    documents=knowledge_base,\n",
    "    embeddings=doc_embeddings,\n",
    "    metadata=[{\"type\": \"short\" if i < len(short_docs) else \"long\",\n",
    "               \"index\": i} for i in range(len(knowledge_base))]\n",
    ")\n",
    "print(f\"\\n{vector_store}\")"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_17_test_retrieval",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Test Retrieval\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_17_test_retrieval.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let Us Test Retrieval!\n",
    "\n",
    "Time to see if our pipeline actually works. Let us query for different ML concepts and see what comes back."
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some test queries\n",
    "test_queries = [\n",
    "    \"How does batch normalization work?\",\n",
    "    \"What prevents neural networks from overfitting?\",\n",
    "    \"How do transformers process sequences?\",\n",
    "    \"What are skip connections and why do they matter?\",\n",
    "    \"How can I use a pre-trained model for a new task?\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RETRIEVAL TEST: Top-3 results for each query\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for query in test_queries:\n",
    "    # Embed the query\n",
    "    query_embedding = embed_model.encode(query)\n",
    "\n",
    "    # Search\n",
    "    results = vector_store.search(query_embedding, top_k=3)\n",
    "\n",
    "    print(f\"\\nðŸ” Query: \\\"{query}\\\"\")\n",
    "    print(\"-\" * 60)\n",
    "    for rank, (doc, score, idx) in enumerate(results, 1):\n",
    "        doc_preview = doc[:90] + \"...\" if len(doc) > 90 else doc\n",
    "        print(f\"  {rank}. [{score:.4f}] {doc_preview}\")\n",
    "    print()"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_18_similarity_heatmap",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Similarity Heatmap\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_18_similarity_heatmap.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Visualization: Similarity Matrix Heatmap\n",
    "\n",
    "Let us visualize the similarity between all queries and all documents. This gives us a bird's-eye view of how well embedding-based retrieval captures semantic relationships."
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed all test queries\n",
    "query_embeddings = embed_model.encode(test_queries)\n",
    "\n",
    "# Compute full similarity matrix: (num_queries Ã— num_docs)\n",
    "similarity_matrix = np.array([\n",
    "    cosine_similarity(q_emb, doc_embeddings)\n",
    "    for q_emb in query_embeddings\n",
    "])\n",
    "\n",
    "# Create short labels\n",
    "query_labels = [q[:35] + '...' if len(q) > 35 else q for q in test_queries]\n",
    "doc_labels = [d[:30] + '...' if len(d) > 30 else d for d in knowledge_base]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "\n",
    "im = ax.imshow(similarity_matrix, cmap='YlOrRd', aspect='auto', vmin=0, vmax=1)\n",
    "plt.colorbar(im, ax=ax, label='Cosine Similarity', shrink=0.8)\n",
    "\n",
    "# Labels\n",
    "ax.set_xticks(range(len(doc_labels)))\n",
    "ax.set_xticklabels(doc_labels, rotation=45, ha='right', fontsize=8)\n",
    "ax.set_yticks(range(len(query_labels)))\n",
    "ax.set_yticklabels(query_labels, fontsize=10)\n",
    "\n",
    "ax.set_xlabel('Documents', fontsize=12)\n",
    "ax.set_ylabel('Queries', fontsize=12)\n",
    "ax.set_title('Queryâ€“Document Similarity Matrix\\n(Brighter = More Similar)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Annotate cells with similarity values\n",
    "for i in range(len(test_queries)):\n",
    "    for j in range(len(knowledge_base)):\n",
    "        val = similarity_matrix[i, j]\n",
    "        if val > 0.4:\n",
    "            ax.text(j, i, f'{val:.2f}', ha='center', va='center',\n",
    "                    fontsize=7, fontweight='bold', color='white' if val > 0.6 else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Highlight: for each query, show the highest-similarity document\n",
    "print(\"\\nBest match for each query:\")\n",
    "for i, query in enumerate(test_queries):\n",
    "    best_idx = np.argmax(similarity_matrix[i])\n",
    "    best_score = similarity_matrix[i, best_idx]\n",
    "    print(f\"  \\\"{query[:40]}...\\\" â†’ doc[{best_idx}] (sim={best_score:.4f})\")"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_19_reranking_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Reranking Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_19_reranking_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_20_todo_heuristic_reranker",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo Heuristic Reranker\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_20_todo_heuristic_reranker.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Reranking â€” The Second Pass\n",
    "\n",
    "Retrieval with embeddings is a fast but rough first pass. A **reranker** does a second, more careful relevance check. In production, cross-encoder models score each (query, document) pair jointly â€” which is slower but much more accurate than comparing independent embeddings.\n",
    "\n",
    "We will build two rerankers:\n",
    "1. A simple heuristic reranker (the TODO)\n",
    "2. A cross-encoder reranker using `cross-encoder/ms-marco-MiniLM-L-6-v2`\n",
    "\n",
    "### ðŸ”§ TODO 2: Implement a Heuristic Reranker"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic_rerank(\n",
    "    query: str,\n",
    "    candidates: List[Tuple[str, float, int]],\n",
    ") -> List[Tuple[str, float, int]]:\n",
    "    \"\"\"\n",
    "    Rerank retrieved candidates using a simple heuristic:\n",
    "    Boost the similarity score if query terms appear literally in the chunk text.\n",
    "\n",
    "    Strategy:\n",
    "        1. Tokenize the query into individual words (lowercase, ignore stopwords)\n",
    "        2. For each candidate, count how many query terms appear in the document text\n",
    "        3. Compute a boost factor: boost = 1 + 0.1 * (matching_terms / total_query_terms)\n",
    "        4. Multiply the original similarity score by the boost factor\n",
    "        5. Return candidates sorted by boosted score (highest first)\n",
    "\n",
    "    Args:\n",
    "        query: The user's query string\n",
    "        candidates: List of (doc_text, similarity_score, doc_index) tuples\n",
    "\n",
    "    Returns:\n",
    "        Reranked list of (doc_text, boosted_score, doc_index) tuples\n",
    "    \"\"\"\n",
    "    stopwords = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'do', 'does',\n",
    "                 'what', 'how', 'why', 'when', 'where', 'which', 'who',\n",
    "                 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of',\n",
    "                 'with', 'from', 'by', 'it', 'its', 'this', 'that', 'can', 'i'}\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Tokenize the query â€” split into lowercase words, remove stopwords\n",
    "    # Step 2: For each candidate (doc_text, score, idx):\n",
    "    #         a. Count how many query terms appear in doc_text.lower()\n",
    "    #         b. Compute boost = 1 + 0.1 * (matching_terms / total_query_terms)\n",
    "    #         c. Compute boosted_score = original_score * boost\n",
    "    # Step 3: Sort candidates by boosted_score (descending)\n",
    "    # Step 4: Return reranked list of (doc_text, boosted_score, idx)\n",
    "    # ==============================\n",
    "\n",
    "    reranked = ???  # YOUR CODE HERE\n",
    "\n",
    "    return reranked"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_21_after_todo_heuristic_reranker",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: After Todo Heuristic Reranker\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_21_after_todo_heuristic_reranker.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Verification: Run this cell to check your heuristic reranker\n",
    "\n",
    "# Test case: query mentions \"batch normalization\"\n",
    "test_candidates = [\n",
    "    (\"Dropout randomly deactivates neurons.\", 0.85, 1),\n",
    "    (\"Batch normalization normalizes layer inputs.\", 0.80, 0),\n",
    "    (\"Transfer learning reuses pre-trained models.\", 0.75, 9),\n",
    "]\n",
    "\n",
    "reranked = heuristic_rerank(\"How does batch normalization work?\", test_candidates)\n",
    "\n",
    "# After reranking, \"Batch normalization\" doc should be boosted above \"Dropout\"\n",
    "assert reranked[0][2] == 0, (\n",
    "    f\"Expected doc index 0 (batch norm) at top, but got index {reranked[0][2]}. \"\n",
    "    f\"The batch norm doc should be boosted because 'batch' and 'normalization' appear in it.\"\n",
    ")\n",
    "assert reranked[0][1] > 0.80, (\n",
    "    f\"Expected boosted score > 0.80, got {reranked[0][1]:.4f}\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Heuristic reranker works correctly!\")\n",
    "print(f\"\\nBefore reranking:\")\n",
    "for doc, score, idx in test_candidates:\n",
    "    print(f\"  [{score:.4f}] (idx={idx}) {doc}\")\n",
    "print(f\"\\nAfter reranking:\")\n",
    "for doc, score, idx in reranked:\n",
    "    print(f\"  [{score:.4f}] (idx={idx}) {doc}\")"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_22_cross_encoder_reranking",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Cross Encoder Reranking\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_22_cross_encoder_reranking.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Encoder Reranking (Production Grade)\n",
    "\n",
    "Now let us load a real cross-encoder model. Unlike our heuristic, a cross-encoder takes the *pair* (query, document) as input and produces a relevance score. This is much more accurate because the model can attend to interactions between query and document tokens."
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "print(\"Loading cross-encoder reranker...\")\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "print(\"Cross-encoder loaded!\")\n",
    "\n",
    "\n",
    "def cross_encoder_rerank(\n",
    "    query: str,\n",
    "    candidates: List[Tuple[str, float, int]],\n",
    "    model: CrossEncoder = cross_encoder\n",
    ") -> List[Tuple[str, float, int]]:\n",
    "    \"\"\"\n",
    "    Rerank candidates using a cross-encoder model.\n",
    "    The cross-encoder scores each (query, document) pair jointly,\n",
    "    which is more accurate than comparing independent embeddings.\n",
    "    \"\"\"\n",
    "    if not candidates:\n",
    "        return []\n",
    "\n",
    "    # Create (query, document) pairs for the cross-encoder\n",
    "    pairs = [(query, doc_text) for doc_text, _, _ in candidates]\n",
    "\n",
    "    # Score all pairs\n",
    "    cross_encoder_scores = model.predict(pairs)\n",
    "\n",
    "    # Replace similarity scores with cross-encoder scores\n",
    "    reranked = [\n",
    "        (doc_text, float(ce_score), idx)\n",
    "        for (doc_text, _, idx), ce_score in zip(candidates, cross_encoder_scores)\n",
    "    ]\n",
    "\n",
    "    # Sort by cross-encoder score (descending)\n",
    "    reranked.sort(key=lambda x: x[1], reverse=True)\n",
    "    return reranked\n",
    "\n",
    "\n",
    "print(\"\\nReady to rerank with cross-encoder!\")"
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_23_reranking_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Reranking Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_23_reranking_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Visualization: How Reranking Changes the Order\n",
    "\n",
    "Let us see the effect of reranking on a concrete example."
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query that benefits from reranking\n",
    "query = \"What technique helps train very deep neural networks with 100+ layers?\"\n",
    "query_emb = embed_model.encode(query)\n",
    "\n",
    "# Get top-5 from vector store (initial retrieval)\n",
    "initial_results = vector_store.search(query_emb, top_k=5)\n",
    "\n",
    "# Rerank with both methods\n",
    "heuristic_results = heuristic_rerank(query, initial_results)\n",
    "crossencoder_results = cross_encoder_rerank(query, initial_results)\n",
    "\n",
    "# Visualize side by side\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "stages = [\n",
    "    (\"Initial Retrieval\\n(Embedding Similarity)\", initial_results, '#2196F3'),\n",
    "    (\"After Heuristic Rerank\\n(Term Matching Boost)\", heuristic_results, '#4CAF50'),\n",
    "    (\"After Cross-Encoder Rerank\\n(Joint Scoring)\", crossencoder_results, '#FF5722'),\n",
    "]\n",
    "\n",
    "for ax, (title, results, color) in zip(axes, stages):\n",
    "    docs_short = [textwrap.fill(doc[:70], width=40) for doc, _, _ in results]\n",
    "    scores = [score for _, score, _ in results]\n",
    "    indices = [idx for _, _, idx in results]\n",
    "\n",
    "    y_pos = range(len(results))\n",
    "    bars = ax.barh(y_pos, scores, color=color, alpha=0.75, edgecolor='white', height=0.6)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels([f'[{idx}] {d[:35]}...' for d, idx in zip(docs_short, indices)],\n",
    "                       fontsize=8)\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Add score labels on bars\n",
    "    for bar, score in zip(bars, scores):\n",
    "        ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                f'{score:.3f}', va='center', fontsize=9)\n",
    "\n",
    "plt.suptitle(f'Query: \"{query}\"', fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show which document is about residual connections (the correct answer)\n",
    "print(f\"\\nðŸ’¡ The correct answer is about residual connections (ResNet, 100+ layers).\")\n",
    "print(f\"   Initial rank: {[idx for _, _, idx in initial_results].index(4) + 1 if 4 in [idx for _, _, idx in initial_results] else 'not in top-5'}\")\n",
    "print(f\"   Cross-encoder rank: {[idx for _, _, idx in crossencoder_results].index(4) + 1 if 4 in [idx for _, _, idx in crossencoder_results] else 'not in top-5'}\")"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_24_context_assembly",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Context Assembly\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_24_context_assembly.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Context Assembly\n",
    "\n",
    "The final step before the LLM sees anything: assembling the retrieved chunks into a structured context. We use XML tags to clearly delineate sections â€” this helps the LLM understand the role of each piece of information."
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_context(\n",
    "    query: str,\n",
    "    retrieved_docs: List[Tuple[str, float, int]],\n",
    "    system_prompt: str = \"You are a helpful ML tutor. Answer based on the provided context.\",\n",
    "    max_context_chars: int = 4000,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Assemble the final context string with XML tags.\n",
    "\n",
    "    Structure:\n",
    "        <system> ... </system>\n",
    "        <retrieved_context>\n",
    "            <chunk relevance=\"0.85\"> ... </chunk>\n",
    "            ...\n",
    "        </retrieved_context>\n",
    "        <query> ... </query>\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "\n",
    "    # System prompt\n",
    "    context_parts.append(f\"<system>\\n{system_prompt}\\n</system>\")\n",
    "\n",
    "    # Retrieved context\n",
    "    context_parts.append(\"\\n<retrieved_context>\")\n",
    "    chars_used = sum(len(p) for p in context_parts)\n",
    "\n",
    "    for doc_text, score, idx in retrieved_docs:\n",
    "        chunk_str = f'  <chunk relevance=\"{score:.4f}\">\\n    {doc_text}\\n  </chunk>'\n",
    "        if chars_used + len(chunk_str) > max_context_chars:\n",
    "            context_parts.append(\"  <!-- Additional chunks truncated to fit context budget -->\")\n",
    "            break\n",
    "        context_parts.append(chunk_str)\n",
    "        chars_used += len(chunk_str)\n",
    "\n",
    "    context_parts.append(\"</retrieved_context>\")\n",
    "\n",
    "    # User query\n",
    "    context_parts.append(f\"\\n<query>\\n{query}\\n</query>\")\n",
    "\n",
    "    return \"\\n\".join(context_parts)\n",
    "\n",
    "\n",
    "# Demo: assemble context for our residual connections query\n",
    "demo_query = \"What technique helps train very deep neural networks with 100+ layers?\"\n",
    "demo_results = cross_encoder_rerank(demo_query, vector_store.search(embed_model.encode(demo_query), top_k=3))\n",
    "demo_context = assemble_context(demo_query, demo_results)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ASSEMBLED CONTEXT (what the LLM would see)\")\n",
    "print(\"=\" * 70)\n",
    "print(demo_context)\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTotal characters: {len(demo_context)}\")\n",
    "print(f\"Estimated tokens: ~{len(demo_context) // 4}\")"
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_25_exp1_chunk_size",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Exp1 Chunk Size\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_25_exp1_chunk_size.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn â€” Experiments\n",
    "\n",
    "### ðŸ”§ Experiment 1: Chunk Size Matters\n",
    "\n",
    "Let us prove empirically that chunk size affects retrieval quality. We will take the longer documents, chunk them at different sizes, embed each set of chunks, and measure how well we retrieve the right information."
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine long docs into a single \"corpus\" for chunking experiments\n",
    "corpus = \"\\n\\n\".join(long_docs)\n",
    "\n",
    "# Chunk at three different sizes\n",
    "chunk_configs = {\n",
    "    'Small (100 chars)': chunk_fixed_size(corpus, chunk_size=100, overlap=20),\n",
    "    'Medium (300 chars)': chunk_fixed_size(corpus, chunk_size=300, overlap=60),\n",
    "    'Large (600 chars)': chunk_fixed_size(corpus, chunk_size=600, overlap=100),\n",
    "}\n",
    "\n",
    "# Test queries with known ground-truth topics\n",
    "experiment_queries = [\n",
    "    (\"What is batch normalization?\", \"batch normalization\"),\n",
    "    (\"How does dropout regularize?\", \"dropout\"),\n",
    "    (\"Explain self-attention in transformers\", \"transformer\"),\n",
    "    (\"What are residual connections?\", \"residual\"),\n",
    "    (\"How does transfer learning work?\", \"transfer learning\"),\n",
    "]\n",
    "\n",
    "print(\"Chunk size experiment: embedding all chunk configurations...\")\n",
    "chunk_results = {}\n",
    "\n",
    "for config_name, chunks in chunk_configs.items():\n",
    "    # Embed chunks\n",
    "    chunk_embs = embed_model.encode(chunks)\n",
    "\n",
    "    # For each query, find the top-1 result and check if it contains the keyword\n",
    "    hits = 0\n",
    "    for query, keyword in experiment_queries:\n",
    "        q_emb = embed_model.encode(query)\n",
    "        sims = cosine_similarity(q_emb, chunk_embs)\n",
    "        top_idx = np.argmax(sims)\n",
    "        top_chunk = chunks[top_idx]\n",
    "\n",
    "        if keyword.lower() in top_chunk.lower():\n",
    "            hits += 1\n",
    "\n",
    "    accuracy = hits / len(experiment_queries) * 100\n",
    "    chunk_results[config_name] = {\n",
    "        'num_chunks': len(chunks),\n",
    "        'avg_size': np.mean([len(c) for c in chunks]),\n",
    "        'accuracy': accuracy,\n",
    "        'chunks': chunks,\n",
    "    }\n",
    "    print(f\"  {config_name}: {len(chunks)} chunks, avg {np.mean([len(c) for c in chunks]):.0f} chars, accuracy {accuracy:.0f}%\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_26_chunk_size_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Chunk Size Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_26_chunk_size_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Visualization: Chunk Size vs Retrieval Quality"
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "config_names = list(chunk_results.keys())\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
    "\n",
    "# Panel 1: Number of chunks\n",
    "ax1 = axes[0]\n",
    "num_chunks = [chunk_results[n]['num_chunks'] for n in config_names]\n",
    "bars1 = ax1.bar(range(len(config_names)), num_chunks, color=colors, alpha=0.8, edgecolor='white')\n",
    "ax1.set_xticks(range(len(config_names)))\n",
    "ax1.set_xticklabels([n.split('(')[0].strip() for n in config_names], fontsize=10)\n",
    "ax1.set_ylabel('Number of Chunks')\n",
    "ax1.set_title('How Many Chunks?', fontweight='bold')\n",
    "for bar, val in zip(bars1, num_chunks):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "             str(val), ha='center', fontweight='bold')\n",
    "\n",
    "# Panel 2: Average chunk size\n",
    "ax2 = axes[1]\n",
    "avg_sizes = [chunk_results[n]['avg_size'] for n in config_names]\n",
    "bars2 = ax2.bar(range(len(config_names)), avg_sizes, color=colors, alpha=0.8, edgecolor='white')\n",
    "ax2.set_xticks(range(len(config_names)))\n",
    "ax2.set_xticklabels([n.split('(')[0].strip() for n in config_names], fontsize=10)\n",
    "ax2.set_ylabel('Avg Characters per Chunk')\n",
    "ax2.set_title('How Big Are Chunks?', fontweight='bold')\n",
    "for bar, val in zip(bars2, avg_sizes):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "             f'{val:.0f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Panel 3: Retrieval accuracy\n",
    "ax3 = axes[2]\n",
    "accuracies = [chunk_results[n]['accuracy'] for n in config_names]\n",
    "bars3 = ax3.bar(range(len(config_names)), accuracies, color=colors, alpha=0.8, edgecolor='white')\n",
    "ax3.set_xticks(range(len(config_names)))\n",
    "ax3.set_xticklabels([n.split('(')[0].strip() for n in config_names], fontsize=10)\n",
    "ax3.set_ylabel('Top-1 Retrieval Accuracy (%)')\n",
    "ax3.set_title('How Accurate Is Retrieval?', fontweight='bold')\n",
    "ax3.set_ylim(0, 110)\n",
    "for bar, val in zip(bars3, accuracies):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "             f'{val:.0f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Chunk Size Experiment: Smaller Isn\\'t Always Better',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key takeaway: Very small chunks lose context, very large chunks\")\n",
    "print(\"   dilute the signal. The medium-sized chunks tend to perform best.\")\n",
    "print(\"   This matches the article's recommendation: 400-1000 tokens per chunk.\")"
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_27_exp2_top_k",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Exp2 Top K\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_27_exp2_top_k.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Top-K Selection\n",
    "\n",
    "How many chunks should we retrieve? Let us measure how retrieval quality changes with K."
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our full knowledge base for this experiment\n",
    "queries_for_topk = [\n",
    "    (\"How does batch normalization work?\", {0, 10}),       # indices of relevant docs\n",
    "    (\"What prevents overfitting?\", {1, 11}),                # dropout docs\n",
    "    (\"Explain the transformer architecture\", {2, 12}),      # transformer docs\n",
    "    (\"What are residual connections?\", {4, 13}),             # residual docs\n",
    "    (\"How does transfer learning work?\", {9, 14}),           # transfer learning docs\n",
    "]\n",
    "\n",
    "k_values = [1, 2, 3, 5, 7, 10]\n",
    "recall_at_k = []\n",
    "precision_at_k = []\n",
    "\n",
    "for k in k_values:\n",
    "    total_recall = 0\n",
    "    total_precision = 0\n",
    "\n",
    "    for query, relevant_indices in queries_for_topk:\n",
    "        q_emb = embed_model.encode(query)\n",
    "        results = vector_store.search(q_emb, top_k=k)\n",
    "        retrieved_indices = {idx for _, _, idx in results}\n",
    "\n",
    "        # Recall: what fraction of relevant docs did we retrieve?\n",
    "        hits = len(retrieved_indices & relevant_indices)\n",
    "        recall = hits / len(relevant_indices)\n",
    "        total_recall += recall\n",
    "\n",
    "        # Precision: what fraction of retrieved docs are relevant?\n",
    "        precision = hits / k\n",
    "        total_precision += precision\n",
    "\n",
    "    recall_at_k.append(total_recall / len(queries_for_topk))\n",
    "    precision_at_k.append(total_precision / len(queries_for_topk))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(k_values, recall_at_k, 'o-', color='#2196F3', linewidth=2.5,\n",
    "        markersize=8, label='Recall @ K', zorder=3)\n",
    "ax.plot(k_values, precision_at_k, 's-', color='#FF5722', linewidth=2.5,\n",
    "        markersize=8, label='Precision @ K', zorder=3)\n",
    "\n",
    "ax.set_xlabel('Top-K (number of retrieved chunks)', fontsize=13)\n",
    "ax.set_ylabel('Score', fontsize=13)\n",
    "ax.set_title('The Precisionâ€“Recall Tradeoff in Top-K Retrieval', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "ax.set_xticks(k_values)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate the sweet spot\n",
    "sweet_k_idx = np.argmax(np.array(recall_at_k) + np.array(precision_at_k))\n",
    "ax.axvline(x=k_values[sweet_k_idx], color='gray', linestyle='--', alpha=0.5)\n",
    "ax.annotate(f'Sweet spot: K={k_values[sweet_k_idx]}',\n",
    "            xy=(k_values[sweet_k_idx], (recall_at_k[sweet_k_idx] + precision_at_k[sweet_k_idx])/2),\n",
    "            xytext=(k_values[sweet_k_idx]+1.5, 0.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='gray'),\n",
    "            fontsize=11, fontweight='bold', color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ’¡ As K increases, recall goes up (we find more relevant docs)\")\n",
    "print(f\"   but precision goes down (more noise). The sweet spot is K={k_values[sweet_k_idx]}.\")"
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_28_full_rag_pipeline",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Full Rag Pipeline\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_28_full_rag_pipeline.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Now let us build the complete end-to-end RAG pipeline as a single class. This brings together every component we built."
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    \"\"\"\n",
    "    Complete RAG Pipeline: From query to assembled context.\n",
    "\n",
    "    Pipeline stages:\n",
    "        1. Embed the query\n",
    "        2. Retrieve top-K candidates from vector store\n",
    "        3. Rerank candidates (cross-encoder)\n",
    "        4. Assemble structured context with XML tags\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_model: SentenceTransformer,\n",
    "        cross_encoder_model: CrossEncoder,\n",
    "        vector_store: SimpleVectorStore,\n",
    "        top_k_retrieve: int = 5,\n",
    "        top_k_rerank: int = 3,\n",
    "        system_prompt: str = \"You are a helpful ML tutor. Answer based on the provided context.\",\n",
    "    ):\n",
    "        self.embed_model = embed_model\n",
    "        self.cross_encoder = cross_encoder_model\n",
    "        self.vector_store = vector_store\n",
    "        self.top_k_retrieve = top_k_retrieve\n",
    "        self.top_k_rerank = top_k_rerank\n",
    "        self.system_prompt = system_prompt\n",
    "\n",
    "    def run(self, query: str, verbose: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Execute the full RAG pipeline.\n",
    "\n",
    "        Returns a dict with:\n",
    "            - query: the original query\n",
    "            - retrieved: initial retrieval results\n",
    "            - reranked: cross-encoder reranked results\n",
    "            - context: the final assembled context string\n",
    "            - timings: time spent on each stage\n",
    "        \"\"\"\n",
    "        timings = {}\n",
    "\n",
    "        # Stage 1: Embed the query\n",
    "        t0 = time.time()\n",
    "        query_embedding = self.embed_model.encode(query)\n",
    "        timings['embed'] = time.time() - t0\n",
    "\n",
    "        # Stage 2: Retrieve top-K candidates\n",
    "        t0 = time.time()\n",
    "        retrieved = self.vector_store.search(query_embedding, top_k=self.top_k_retrieve)\n",
    "        timings['retrieve'] = time.time() - t0\n",
    "\n",
    "        # Stage 3: Rerank with cross-encoder\n",
    "        t0 = time.time()\n",
    "        reranked = cross_encoder_rerank(query, retrieved, self.cross_encoder)\n",
    "        # Keep only top_k_rerank after reranking\n",
    "        reranked = reranked[:self.top_k_rerank]\n",
    "        timings['rerank'] = time.time() - t0\n",
    "\n",
    "        # Stage 4: Assemble context\n",
    "        t0 = time.time()\n",
    "        context = assemble_context(query, reranked, self.system_prompt)\n",
    "        timings['assemble'] = time.time() - t0\n",
    "\n",
    "        result = {\n",
    "            'query': query,\n",
    "            'retrieved': retrieved,\n",
    "            'reranked': reranked,\n",
    "            'context': context,\n",
    "            'timings': timings,\n",
    "        }\n",
    "\n",
    "        if verbose:\n",
    "            total_time = sum(timings.values())\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"RAG Pipeline Results for: \\\"{query}\\\"\")\n",
    "            print(f\"{'='*70}\")\n",
    "            print(f\"\\nStage timings:\")\n",
    "            for stage, t in timings.items():\n",
    "                print(f\"  {stage:>10s}: {t*1000:6.1f} ms\")\n",
    "            print(f\"  {'total':>10s}: {total_time*1000:6.1f} ms\")\n",
    "            print(f\"\\nTop {self.top_k_rerank} reranked results:\")\n",
    "            for rank, (doc, score, idx) in enumerate(reranked, 1):\n",
    "                doc_short = doc[:80] + '...' if len(doc) > 80 else doc\n",
    "                print(f\"  {rank}. [{score:.4f}] {doc_short}\")\n",
    "            print(f\"\\nAssembled context: {len(context)} chars (~{len(context)//4} tokens)\")\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "rag_pipeline = RAGPipeline(\n",
    "    embed_model=embed_model,\n",
    "    cross_encoder_model=cross_encoder,\n",
    "    vector_store=vector_store,\n",
    "    top_k_retrieve=5,\n",
    "    top_k_rerank=3,\n",
    ")\n",
    "\n",
    "print(\"RAG Pipeline ready!\")"
   ],
   "id": "cell_51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_29_run_pipeline",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Run Pipeline\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_29_run_pipeline.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Pipeline End-to-End"
   ],
   "id": "cell_52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on several queries\n",
    "demo_queries = [\n",
    "    \"How can I stabilize training in deep neural networks?\",\n",
    "    \"What technique uses a generator and discriminator?\",\n",
    "    \"How do modern language models handle long sequences?\",\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "for query in demo_queries:\n",
    "    result = rag_pipeline.run(query)\n",
    "    all_results.append(result)\n",
    "    print()"
   ],
   "id": "cell_53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_30_final_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Final Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_30_final_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results â€” Final Multi-Panel Visualization\n",
    "\n",
    "Now for the grand finale: a multi-panel visualization that shows the complete pipeline in action."
   ],
   "id": "cell_54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one query for the detailed visualization\n",
    "final_query = \"How can I stabilize training in deep neural networks?\"\n",
    "final_result = rag_pipeline.run(final_query, verbose=False)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 14))\n",
    "\n",
    "# --- Panel 1: Top-K Retrieved Chunks with Relevance Scores ---\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "retrieved_docs = final_result['reranked']\n",
    "doc_labels_short = [textwrap.fill(doc[:60], width=35) for doc, _, _ in retrieved_docs]\n",
    "scores = [score for _, score, _ in retrieved_docs]\n",
    "\n",
    "# Color by score intensity\n",
    "norm_scores = np.array(scores)\n",
    "norm_scores = (norm_scores - norm_scores.min()) / (norm_scores.max() - norm_scores.min() + 1e-8)\n",
    "bar_colors = plt.cm.YlOrRd(0.3 + 0.7 * norm_scores)\n",
    "\n",
    "bars = ax1.barh(range(len(retrieved_docs)), scores, color=bar_colors,\n",
    "                edgecolor='white', height=0.6)\n",
    "ax1.set_yticks(range(len(doc_labels_short)))\n",
    "ax1.set_yticklabels(doc_labels_short, fontsize=8)\n",
    "ax1.set_xlabel('Relevance Score (Cross-Encoder)', fontsize=10)\n",
    "ax1.set_title('Panel 1: Top-K Retrieved Chunks\\n(Reranked by Relevance)', fontsize=12, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "for bar, score in zip(bars, scores):\n",
    "    ax1.text(bar.get_width() + 0.05, bar.get_y() + bar.get_height()/2,\n",
    "             f'{score:.3f}', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# --- Panel 2: Similarity Heatmap (Query vs All Docs) ---\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "q_emb = embed_model.encode(final_query)\n",
    "all_sims = cosine_similarity(q_emb, doc_embeddings)\n",
    "\n",
    "# Sort for visual clarity\n",
    "sorted_idx = np.argsort(all_sims)[::-1]\n",
    "sorted_sims = all_sims[sorted_idx]\n",
    "sorted_labels = [knowledge_base[i][:40] + '...' for i in sorted_idx]\n",
    "\n",
    "im = ax2.imshow(sorted_sims.reshape(1, -1), cmap='YlOrRd', aspect='auto')\n",
    "ax2.set_xticks(range(len(sorted_labels)))\n",
    "ax2.set_xticklabels(sorted_labels, rotation=60, ha='right', fontsize=7)\n",
    "ax2.set_yticks([])\n",
    "ax2.set_title('Panel 2: Similarity Heatmap\\n(Query vs All Documents)', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax2, label='Cosine Similarity', orientation='horizontal',\n",
    "             pad=0.35, shrink=0.8)\n",
    "\n",
    "# Annotate top-3\n",
    "for i in range(min(3, len(sorted_sims))):\n",
    "    ax2.text(i, 0, f'{sorted_sims[i]:.2f}', ha='center', va='center',\n",
    "             fontsize=8, fontweight='bold', color='white' if sorted_sims[i] > 0.5 else 'black')\n",
    "\n",
    "# --- Panel 3: Chunk Size Experiment ---\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "config_names_short = ['Small\\n(100)', 'Medium\\n(300)', 'Large\\n(600)']\n",
    "experiment_colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
    "exp_accuracies = [chunk_results[n]['accuracy'] for n in chunk_results.keys()]\n",
    "exp_num_chunks = [chunk_results[n]['num_chunks'] for n in chunk_results.keys()]\n",
    "\n",
    "x = np.arange(len(config_names_short))\n",
    "width = 0.35\n",
    "\n",
    "bars_acc = ax3.bar(x - width/2, exp_accuracies, width, label='Accuracy (%)',\n",
    "                    color=experiment_colors, alpha=0.8, edgecolor='white')\n",
    "\n",
    "ax3_twin = ax3.twinx()\n",
    "bars_num = ax3_twin.bar(x + width/2, exp_num_chunks, width, label='Num Chunks',\n",
    "                         color=experiment_colors, alpha=0.3, edgecolor='gray', hatch='//')\n",
    "\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(config_names_short)\n",
    "ax3.set_ylabel('Retrieval Accuracy (%)', fontsize=10)\n",
    "ax3_twin.set_ylabel('Number of Chunks', fontsize=10)\n",
    "ax3.set_title('Panel 3: Chunk Size Experiment', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylim(0, 120)\n",
    "\n",
    "# Combined legend\n",
    "lines1, labels1 = ax3.get_legend_handles_labels()\n",
    "lines2, labels2 = ax3_twin.get_legend_handles_labels()\n",
    "ax3.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=9)\n",
    "\n",
    "for bar, val in zip(bars_acc, exp_accuracies):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "             f'{val:.0f}%', ha='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# --- Panel 4: Assembled Context (Color-Coded) ---\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "ax4.set_xlim(0, 10)\n",
    "ax4.set_ylim(0, 10)\n",
    "ax4.axis('off')\n",
    "ax4.set_title('Panel 4: Assembled Context Structure\\n(Color-Coded by Section)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Draw colored blocks representing XML sections\n",
    "sections = [\n",
    "    ('System Prompt', '#3F51B5', 9.5, 1.2,\n",
    "     'You are a helpful ML tutor.\\nAnswer based on the provided context.'),\n",
    "    ('Retrieved Context', '#4CAF50', 7.5, 4.0, None),\n",
    "    ('User Query', '#9C27B0', 2.8, 1.2, final_query),\n",
    "]\n",
    "\n",
    "for name, color, y, height, content in sections:\n",
    "    rect = plt.Rectangle((0.5, y - height), 9, height,\n",
    "                          linewidth=2, edgecolor=color, facecolor=color,\n",
    "                          alpha=0.15)\n",
    "    ax4.add_patch(rect)\n",
    "    ax4.text(0.7, y - 0.15, f'<{name.lower().replace(\" \", \"_\")}>',\n",
    "             fontsize=9, fontweight='bold', color=color, family='monospace')\n",
    "\n",
    "# Add chunk details in the retrieved context section\n",
    "y_chunk = 7.0\n",
    "for rank, (doc, score, idx) in enumerate(final_result['reranked'], 1):\n",
    "    doc_short = doc[:65] + '...' if len(doc) > 65 else doc\n",
    "    ax4.text(1.0, y_chunk, f'  Chunk {rank} [relevance={score:.3f}]',\n",
    "             fontsize=8, fontweight='bold', color='#2E7D32')\n",
    "    ax4.text(1.0, y_chunk - 0.4, f'  \"{doc_short}\"',\n",
    "             fontsize=7, color='#333', style='italic')\n",
    "    y_chunk -= 1.1\n",
    "\n",
    "# Add the query text\n",
    "ax4.text(1.0, 2.3, f'  \"{final_query}\"',\n",
    "         fontsize=8, color='#6A1B9A', style='italic')\n",
    "\n",
    "# Add token count\n",
    "total_chars = len(final_result['context'])\n",
    "ax4.text(5.0, 0.5, f'Total: {total_chars} chars (~{total_chars//4} tokens)',\n",
    "         fontsize=10, fontweight='bold', ha='center', color='#555',\n",
    "         bbox=dict(boxstyle='round,pad=0.4', facecolor='#f0f0f0', edgecolor='gray'))\n",
    "\n",
    "plt.suptitle(f'RAG Pipeline: Complete Results\\nQuery: \"{final_query}\"',\n",
    "             fontsize=15, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_31_final_context_output",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Final Context Output\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_31_final_context_output.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Final Assembled Context\n",
    "\n",
    "Let us print the actual context string that would be sent to the LLM. This is the final output of the entire pipeline."
   ],
   "id": "cell_56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FINAL ASSEMBLED CONTEXT\")\n",
    "print(\"This is exactly what the LLM would see in its context window.\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "context = final_result['context']\n",
    "\n",
    "# Color-code the output by section (using ANSI codes for terminal)\n",
    "lines = context.split('\\n')\n",
    "for line in lines:\n",
    "    if line.strip().startswith('<system') or line.strip().startswith('</system'):\n",
    "        print(f\"\\033[94m{line}\\033[0m\")  # Blue\n",
    "    elif line.strip().startswith('<retrieved') or line.strip().startswith('</retrieved'):\n",
    "        print(f\"\\033[92m{line}\\033[0m\")  # Green\n",
    "    elif line.strip().startswith('<chunk') or line.strip().startswith('</chunk'):\n",
    "        print(f\"\\033[92m{line}\\033[0m\")  # Green\n",
    "    elif line.strip().startswith('<query') or line.strip().startswith('</query'):\n",
    "        print(f\"\\033[95m{line}\\033[0m\")  # Purple\n",
    "    else:\n",
    "        print(line)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(f\"Context size: {len(context)} characters, ~{len(context)//4} tokens\")\n",
    "print(f\"Pipeline latency: {sum(final_result['timings'].values())*1000:.1f} ms total\")\n",
    "print(\"=\" * 70)"
   ],
   "id": "cell_57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_32_reflection",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Reflection\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_32_reflection.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reflection\n",
    "\n",
    "### ðŸ¤” What Did We Build?\n",
    "\n",
    "We built a complete RAG pipeline from scratch:\n",
    "\n",
    "1. **Text Chunking** â€” Three strategies (fixed-size, sentence-based, paragraph-based) with different tradeoffs\n",
    "2. **Embedding** â€” Converting text to 384-dimensional vectors using sentence-transformers\n",
    "3. **Cosine Similarity** â€” Implemented from scratch with numpy, the mathematical heart of retrieval\n",
    "4. **Vector Store** â€” A simple but complete store with top-K search\n",
    "5. **Reranking** â€” Both a heuristic reranker (your TODO) and a cross-encoder reranker\n",
    "6. **Context Assembly** â€” Structured XML-tagged context ready for any LLM\n",
    "7. **End-to-End Pipeline** â€” All components wired together in a single class\n",
    "\n",
    "### ðŸ’¡ Key Takeaways\n",
    "\n",
    "- **Chunk size matters**: Too small = confetti with no context. Too large = wastes space. Sweet spot: 400-1000 tokens.\n",
    "- **Top-K is a tradeoff**: More K = higher recall but lower precision. Find the balance for your use case.\n",
    "- **Reranking is critical**: Initial embedding similarity is a rough first pass. A cross-encoder reranker significantly improves result ordering.\n",
    "- **Structure helps the LLM**: XML tags (`<system>`, `<retrieved_context>`, `<query>`) make it easy for the model to understand what role each piece of information plays.\n",
    "\n",
    "### ðŸŽ¯ What Comes Next?\n",
    "\n",
    "In the next notebook, we will explore **context failure modes** â€” what happens when the context is poisoned, too long, or contradictory. We will also implement the four core strategies (Write, Select, Compress, Isolate) for managing context at scale."
   ],
   "id": "cell_58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_33_extension_challenges",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Extension Challenges\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_33_extension_challenges.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension Challenges (Optional)\n",
    "\n",
    "Try these if you want to go deeper:"
   ],
   "id": "cell_59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1: Implement overlap-aware sentence chunking\n",
    "# Modify chunk_by_sentences so that adjacent chunks share 1 overlapping sentence.\n",
    "# This preserves context at chunk boundaries.\n",
    "\n",
    "# Challenge 2: Add metadata filtering to the vector store\n",
    "# Extend SimpleVectorStore.search() to accept a filter_fn parameter\n",
    "# that excludes documents based on metadata (e.g., only search \"long\" documents).\n",
    "\n",
    "# Challenge 3: Implement hybrid search\n",
    "# Combine embedding similarity (semantic) with BM25/TF-IDF (keyword) scores.\n",
    "# Weight: final_score = alpha * embedding_sim + (1-alpha) * keyword_score\n",
    "# Try different values of alpha and compare retrieval quality.\n",
    "\n",
    "# Challenge 4: Build a context budget manager\n",
    "# Create a function that takes a token budget and a list of retrieved chunks,\n",
    "# and selects the maximum number of chunks that fit within the budget,\n",
    "# prioritizing higher-relevance chunks.\n",
    "\n",
    "print(\"Try these challenges! Modify the code above and re-run.\")\n",
    "print(\"Each challenge builds on what you already implemented.\")"
   ],
   "id": "cell_60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_34_closing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Wrap-Up: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_34_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Notebook Complete!\n",
    "\n",
    "You have built a complete RAG pipeline from documents to assembled context. Every component â€” chunking, embedding, similarity search, reranking, and context assembly â€” was implemented from scratch so you understand exactly what happens at each stage.\n",
    "\n",
    "The next time you use a RAG system (whether it is LangChain, LlamaIndex, or a custom pipeline), you will know precisely what is happening under the hood. That understanding is what separates an engineer who *uses* tools from one who *builds* them.\n",
    "\n",
    "**Estimated time spent: ~40 minutes**\n",
    "\n",
    "---\n",
    "\n",
    "*Part 3 of the Vizuara series on Context Engineering for LLMs*\n",
    "*Next up: Part 4 â€” Context Failure Modes and How to Fix Them*"
   ],
   "id": "cell_61"
  }
 ]
}