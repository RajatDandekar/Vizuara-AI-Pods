{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Capstone: Personalized FAQ Chatbot â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_01_setup_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Setup Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_01_setup_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1eZ5s4Ln6YU2bla_phSzWPNZl4m1zUzen\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/05_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone: Build a Personalized FAQ Chatbot with Context Engineering\n",
    "\n",
    "*Part 5 (Capstone) of the Vizuara series on Context Engineering for LLMs*\n",
    "*Estimated time: 45 minutes*\n",
    "\n",
    "You have spent four notebooks mastering every dimension of context engineering â€” token budgets, failure modes, retrieval-augmented generation, and the four context strategies. Now it is time to combine all of them into a single, working system that feels genuinely useful.\n",
    "\n",
    "In this capstone, you will build a personalized FAQ chatbot for Vizuara AI Labs. It scrapes a website, chunks and embeds the content, retrieves relevant documents for every question, remembers who the user is across turns, calls tools when needed, and assembles optimal context using every technique from Notebooks 1-4. By the end, you will watch it hold a multi-turn conversation that feels surprisingly intelligent â€” and you will understand exactly why, because you built every piece yourself.\n",
    "\n",
    "The best part: you can swap vizuara.ai for **any** website URL and instantly have a custom chatbot for that site."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_02_ai_assistant",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Ai Assistant\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_02_ai_assistant.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://pods.vizuara.ai/courses/context-engineering-for-llms/practice/5/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_03_install_deps",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Install Deps\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_03_install_deps.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup â€” install all dependencies\n",
    "!pip install -q google-generativeai sentence-transformers beautifulsoup4 requests\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple, Optional, Any, Callable\n",
    "import textwrap\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Setup complete! Ready to build the capstone chatbot.\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_04_why_it_matters",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_04_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Every AI chatbot you interact with â€” customer support bots, coding assistants, documentation helpers â€” faces the same core challenge: **what do I put in the context window?**\n",
    "\n",
    "A naive chatbot just concatenates the user's question with a system prompt and hopes for the best. A well-engineered chatbot:\n",
    "\n",
    "| Dimension | Naive Approach | Context-Engineered Approach |\n",
    "|-----------|---------------|---------------------------|\n",
    "| **Knowledge** | Stuff everything into the prompt | RAG: retrieve only relevant chunks |\n",
    "| **Memory** | No memory â€” every turn is independent | Track user profile, past questions, key facts |\n",
    "| **Tools** | Hardcode everything | Dynamic tool dispatch based on intent |\n",
    "| **Token Budget** | Hope it fits | Explicit budget allocation + compression |\n",
    "| **Failure Modes** | Ignore them | Detect and recover from stale, conflicting, or missing context |\n",
    "\n",
    "Today we build the right column. All of it."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_05_architecture_viz_before",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Architecture Viz Before\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_05_architecture_viz_before.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize what we're building: the full architecture\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 9))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Capstone Architecture: Personalized FAQ Chatbot\",\n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# User input at the top\n",
    "user_box = plt.Rectangle((5.5, 8.5), 3, 1.0, linewidth=2,\n",
    "                           edgecolor='#2196F3', facecolor='#E3F2FD',\n",
    "                           borderRadius=0.1)\n",
    "ax.add_patch(user_box)\n",
    "ax.text(7, 9.0, \"User Message\", ha='center', va='center',\n",
    "        fontsize=12, fontweight='bold', color='#1565C0')\n",
    "\n",
    "# Arrow down from user\n",
    "ax.annotate('', xy=(7, 8.5), xytext=(7, 8.0),\n",
    "           arrowprops=dict(arrowstyle='->', color='#333', lw=2))\n",
    "\n",
    "# Context Assembler in the middle\n",
    "assembler_box = plt.Rectangle((3.5, 5.8), 7, 2.0, linewidth=2.5,\n",
    "                                edgecolor='#FF9800', facecolor='#FFF8E1',\n",
    "                                linestyle='-')\n",
    "ax.add_patch(assembler_box)\n",
    "ax.text(7, 7.2, \"Context Assembler\", ha='center', va='center',\n",
    "        fontsize=13, fontweight='bold', color='#E65100')\n",
    "ax.text(7, 6.5, \"Token Budget | Health Check | Write/Select/Compress/Isolate\",\n",
    "        ha='center', va='center', fontsize=9, color='#BF360C',\n",
    "        style='italic')\n",
    "\n",
    "# Four input sources\n",
    "sources = [\n",
    "    (0.5, 3.5, \"Knowledge\\nBase\\n(RAG)\", '#4CAF50', '#E8F5E9'),\n",
    "    (4.0, 3.5, \"Conversation\\nMemory\", '#9C27B0', '#F3E5F5'),\n",
    "    (7.5, 3.5, \"Tool\\nResults\", '#F44336', '#FFEBEE'),\n",
    "    (11.0, 3.5, \"System\\nPrompt\", '#607D8B', '#ECEFF1'),\n",
    "]\n",
    "\n",
    "for x, y, label, edge_color, face_color in sources:\n",
    "    box = plt.Rectangle((x, y), 2.5, 1.8, linewidth=2,\n",
    "                          edgecolor=edge_color, facecolor=face_color)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x + 1.25, y + 0.9, label, ha='center', va='center',\n",
    "            fontsize=10, fontweight='bold', color=edge_color)\n",
    "    # Arrow up to assembler\n",
    "    ax.annotate('', xy=(x + 1.25, 5.8), xytext=(x + 1.25, 5.3),\n",
    "               arrowprops=dict(arrowstyle='->', color=edge_color, lw=1.5))\n",
    "\n",
    "# Arrow down to LLM\n",
    "ax.annotate('', xy=(7, 5.8), xytext=(7, 3.0),\n",
    "           arrowprops=dict(arrowstyle='->', color='#333', lw=2))\n",
    "\n",
    "# LLM box at the bottom\n",
    "llm_box = plt.Rectangle((4.5, 1.2), 5, 1.5, linewidth=2.5,\n",
    "                          edgecolor='#1A237E', facecolor='#E8EAF6')\n",
    "ax.add_patch(llm_box)\n",
    "ax.text(7, 1.95, \"Gemini 2.0 Flash\", ha='center', va='center',\n",
    "        fontsize=12, fontweight='bold', color='#1A237E')\n",
    "ax.text(7, 1.5, \"Assembled Context  â†’  Response\",\n",
    "        ha='center', va='center', fontsize=9, color='#283593')\n",
    "\n",
    "# Arrow from LLM to response\n",
    "ax.annotate('', xy=(7, 1.2), xytext=(7, 0.7),\n",
    "           arrowprops=dict(arrowstyle='->', color='#333', lw=2))\n",
    "ax.text(7, 0.4, \"Personalized, Context-Aware Response\",\n",
    "        ha='center', va='center', fontsize=11, fontweight='bold',\n",
    "        color='#333')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_06_architecture_viz_after",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Architecture Viz After\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_06_architecture_viz_after.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_07_building_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Building Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_07_building_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Think of a great hotel concierge. When a guest walks up and says \"What's good for dinner?\", the concierge doesn't give a generic answer. They remember:\n",
    "\n",
    "- **Who you are**: \"Ms. Chen, welcome back!\" (memory)\n",
    "- **What you like**: \"Last time you loved the Italian place on 5th\" (profile)\n",
    "- **What's available**: They check their binder of restaurant info (knowledge retrieval)\n",
    "- **What tools they have**: They can call restaurants to check availability (tool use)\n",
    "- **What's relevant**: They don't read you their entire binder â€” just the Italian and seafood sections (context selection)\n",
    "\n",
    "Our chatbot works the same way. The \"magic\" of a great chatbot is not a better model â€” it is better context engineering.\n",
    "\n",
    "### What We Will Build â€” Component by Component\n",
    "\n",
    "1. **Knowledge Base** â€” Scrape a website + load curated content, chunk it into retrievable pieces\n",
    "2. **Vector Store** â€” Embed chunks and find the most relevant ones for each question\n",
    "3. **Conversation Memory** â€” Track user profile, conversation history, and extracted facts\n",
    "4. **Tool Registry** â€” Register callable tools (course lookup, FAQ search, contact info)\n",
    "5. **Context Assembler** â€” Allocate token budget, run health checks, apply the four strategies\n",
    "6. **Chatbot** â€” Tie everything together into a conversational loop\n",
    "\n",
    "Let's build each one."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_08_setting_up_llm",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Setting Up Llm\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_08_setting_up_llm.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Architecture â€” Setting Up the LLM\n",
    "\n",
    "We use Google Gemini (free tier). You will need an API key from [https://aistudio.google.com](https://aistudio.google.com).\n",
    "\n",
    "In Google Colab, store your key using the Secrets panel (key icon in the left sidebar) with the name `GOOGLE_API_KEY`."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_09_configure_gemini",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Configure Gemini\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_09_configure_gemini.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.colab import userdata\n",
    "\n",
    "# Configure Gemini with your API key\n",
    "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
    "\n",
    "# We'll use gemini-2.0-flash â€” fast, capable, and free-tier friendly\n",
    "MODEL_NAME = 'gemini-2.0-flash'\n",
    "\n",
    "# Quick test to make sure the API is working\n",
    "model = genai.GenerativeModel(MODEL_NAME)\n",
    "response = model.generate_content(\"Say 'Context engineering is ready!' in exactly those words.\")\n",
    "print(f\"Model test: {response.text.strip()}\")\n",
    "print(f\"Using model: {MODEL_NAME}\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_10_embedding_model_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Embedding Model Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_10_embedding_model_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a sentence embedding model for our vector store. We use `all-MiniLM-L6-v2` â€” it is small, fast, and runs on CPU without issues."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_11_load_embedding_model",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Load Embedding Model\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_11_load_embedding_model.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load embedding model (runs on CPU, ~80MB download)\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Quick test\n",
    "test_embedding = embedding_model.encode([\"Hello world\"])\n",
    "print(f\"Embedding model loaded!\")\n",
    "print(f\"Embedding dimension: {test_embedding.shape[1]}\")\n",
    "print(f\"Test embedding norm: {np.linalg.norm(test_embedding[0]):.4f}\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_12_component1_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Component1 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_12_component1_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It â€” Component by Component\n",
    "\n",
    "### 4.1 Component 1: Knowledge Base\n",
    "\n",
    "The knowledge base has two sources: a curated dictionary of Vizuara information (provided below), and optionally scraped web content. We chunk all content into smaller pieces suitable for embedding and retrieval."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_13_curated_kb",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Curated Kb\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_13_curated_kb.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curated Vizuara knowledge base â€” this is what makes the chatbot knowledgeable\n",
    "\n",
    "VIZUARA_KNOWLEDGE_BASE = [\n",
    "    {\n",
    "        \"topic\": \"About Vizuara\",\n",
    "        \"content\": \"Vizuara AI Labs is an educational platform that teaches AI and machine learning concepts through in-depth articles, hands-on Google Colab notebooks, and real-world case studies. Founded by Raj Dandekar, Vizuara's mission is to make cutting-edge AI research accessible to everyone â€” from students to practitioners. The name 'Vizuara' combines 'vision' and 'aura', reflecting the goal of illuminating complex AI concepts with clarity.\"\n",
    "    },\n",
    "    {\n",
    "        \"topic\": \"Courses and Content\",\n",
    "        \"content\": \"Vizuara offers courses on topics including: Reinforcement Learning and DeepSeek-R1, Diffusion Models for Video Generation, World Action Models for Robotics, Vision-Language-Action Models, 5D Parallelism for GPU Programming, Agentic RAG, Context Engineering for LLMs, and more. Each course includes a detailed Substack article, multiple Google Colab teaching notebooks, and case studies with real datasets.\"\n",
    "    },\n",
    "    {\n",
    "        \"topic\": \"Teaching Philosophy\",\n",
    "        \"content\": \"Vizuara's teaching philosophy is 'from first principles'. Every concept is built from the ground up â€” students implement core algorithms from scratch using PyTorch, understand the mathematics behind each technique, and produce tangible outputs (trained models, visualizations, working systems). No black boxes. No copy-pasting from StackOverflow. The goal is deep understanding, not surface-level familiarity.\"\n",
    "    },\n",
    "    {\n",
    "        \"topic\": \"Substack Publication\",\n",
    "        \"content\": \"Vizuara publishes in-depth technical articles on Substack at vizuara.substack.com. Each article is 3,000-8,000 words long, includes custom figures, LaTeX equations, and runnable code. Articles cover the latest AI research papers and translate them into practical, implementable knowledge. The writing style is warm, pedagogical, and assumes an intelligent reader who wants to understand WHY, not just HOW.\"\n",
    "    },\n",
    "    {\n",
    "        \"topic\": \"Google Colab Notebooks\",\n",
    "        \"content\": \"Each Vizuara article comes with companion Google Colab notebooks that let students build the concepts hands-on. Notebooks follow a structured pedagogy: motivation â†’ intuition â†’ math â†’ incremental implementation â†’ exercises â†’ final output. All notebooks run on Google Colab's free T4 GPU and typically take 30-45 minutes to complete. Students implement everything from scratch â€” no high-level wrappers that hide the core mechanics.\"\n",
    "    },\n",
    "    {\n",
    "        \"topic\": \"Case Studies\",\n",
    "        \"content\": \"Vizuara case studies connect AI theory to real-world business problems. Each case study uses real, publicly available datasets (never toy data), frames the problem with business context, and guides students through building a complete ML solution. Case studies include a Colab notebook, production design considerations, and are solvable on a T4 GPU in under 90 minutes.\"\n",
    "    },\n",
    "    {\n",
    "        \"topic\": \"Pricing and Access\",\n",
    "        \"content\": \"Vizuara's Substack articles are free to read. Google Colab notebooks are freely accessible. The platform is committed to making AI education accessible to everyone regardless of financial situation. Students can access all content at vizuara.substack.com and the companion notebooks through Google Colab links provided in each article.\"\n",
    "    },\n",
    "    {\n",
    "        \"topic\": \"Technology Stack\",\n",
    "        \"content\": \"The Vizuara course platform is built with Next.js 16, React 19, Tailwind CSS v4, and Framer Motion. The AI generation pipeline uses Claude (Anthropic) for content generation and Google Gemini for figure creation. Notebooks primarily use PyTorch as the deep learning framework, with matplotlib for visualizations.\"\n",
    "    },\n",
    "    {\n",
    "        \"topic\": \"Contact and Community\",\n",
    "        \"content\": \"Students can reach Vizuara through the Substack publication comments, or through the website at vizuara.ai. Vizuara encourages questions and discussion â€” the goal is to build a community of learners who help each other understand complex AI concepts.\"\n",
    "    },\n",
    "    {\n",
    "        \"topic\": \"FAQ - Prerequisites\",\n",
    "        \"content\": \"Most Vizuara courses assume familiarity with Python programming and basic linear algebra. Some courses require knowledge of calculus and probability. Each article and notebook clearly states its prerequisites. If you can write a Python function and understand matrix multiplication, you can follow most Vizuara content.\"\n",
    "    },\n",
    "    {\n",
    "        \"topic\": \"FAQ - GPU Requirements\",\n",
    "        \"content\": \"All Vizuara notebooks are designed to run on Google Colab's free T4 GPU. You do not need your own GPU or any paid compute. Simply open the Colab link, select 'GPU' under Runtime â†’ Change runtime type, and run the cells. Training times are kept under 10 minutes per notebook.\"\n",
    "    },\n",
    "    {\n",
    "        \"topic\": \"FAQ - How to Get Started\",\n",
    "        \"content\": \"Start with any topic that interests you â€” each course is self-contained. Read the Substack article first to understand the big picture, then work through the companion notebooks. The notebooks build concepts incrementally, so start from Notebook 01 and work through in order. Don't skip the intuition-building sections â€” they make the math and code much easier to understand.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(VIZUARA_KNOWLEDGE_BASE)} curated knowledge entries\")\n",
    "for entry in VIZUARA_KNOWLEDGE_BASE:\n",
    "    print(f\"  - {entry['topic']}: {len(entry['content'])} chars\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_14_knowledgebase_class_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Knowledgebase Class Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_14_knowledgebase_class_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the `KnowledgeBase` class that handles loading curated content, scraping web pages, and chunking everything into retrieval-ready pieces."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_15_knowledgebase_class_impl",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Knowledgebase Class Impl\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_15_knowledgebase_class_impl.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class KnowledgeBase:\n",
    "    \"\"\"\n",
    "    Manages the chatbot's knowledge: curated entries + scraped web content.\n",
    "    Chunks all content into pieces suitable for embedding and retrieval.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.documents = []  # List of {\"text\": ..., \"source\": ..., \"topic\": ...}\n",
    "        self.chunks = []     # List of {\"text\": ..., \"source\": ..., \"topic\": ..., \"chunk_id\": ...}\n",
    "\n",
    "    def load_curated(self, knowledge_list: List[Dict]) -> int:\n",
    "        \"\"\"Load curated knowledge base entries.\"\"\"\n",
    "        for entry in knowledge_list:\n",
    "            self.documents.append({\n",
    "                \"text\": entry[\"content\"],\n",
    "                \"source\": \"curated\",\n",
    "                \"topic\": entry[\"topic\"]\n",
    "            })\n",
    "        print(f\"Loaded {len(knowledge_list)} curated documents\")\n",
    "        return len(knowledge_list)\n",
    "\n",
    "    def scrape_website(self, url: str, max_pages: int = 5) -> int:\n",
    "        \"\"\"\n",
    "        Scrape a URL and add content to the knowledge base.\n",
    "        Extracts text from paragraphs, headings, and list items.\n",
    "        \"\"\"\n",
    "        scraped_count = 0\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (compatible; VizuaraBot/1.0; Educational)'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Remove script and style elements\n",
    "            for tag in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
    "                tag.decompose()\n",
    "\n",
    "            # Extract text from meaningful elements\n",
    "            elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'li', 'td',\n",
    "                                       'article', 'section', 'div'])\n",
    "\n",
    "            page_text = []\n",
    "            for elem in elements:\n",
    "                text = elem.get_text(strip=True)\n",
    "                if len(text) > 30:  # Skip very short fragments\n",
    "                    page_text.append(text)\n",
    "\n",
    "            if page_text:\n",
    "                # Combine into a single document\n",
    "                full_text = \"\\n\".join(page_text)\n",
    "                self.documents.append({\n",
    "                    \"text\": full_text,\n",
    "                    \"source\": f\"scraped:{url}\",\n",
    "                    \"topic\": f\"Web content from {url}\"\n",
    "                })\n",
    "                scraped_count = 1\n",
    "                print(f\"Scraped {url}: {len(full_text)} chars, \"\n",
    "                      f\"{len(page_text)} text blocks\")\n",
    "            else:\n",
    "                print(f\"No substantial text found at {url}\")\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Could not scrape {url}: {e}\")\n",
    "            print(\"(This is fine â€” the curated knowledge base is sufficient)\")\n",
    "\n",
    "        return scraped_count\n",
    "\n",
    "    def chunk(self, chunk_size: int = 500, overlap: int = 50) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Chunk all documents into smaller pieces for embedding.\n",
    "        Uses character-level chunking with overlap for context continuity.\n",
    "        \"\"\"\n",
    "        self.chunks = []\n",
    "        chunk_id = 0\n",
    "\n",
    "        for doc in self.documents:\n",
    "            text = doc[\"text\"]\n",
    "            # Split into sentences first for cleaner chunk boundaries\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "\n",
    "            current_chunk = \"\"\n",
    "            for sentence in sentences:\n",
    "                if len(current_chunk) + len(sentence) > chunk_size and current_chunk:\n",
    "                    self.chunks.append({\n",
    "                        \"text\": current_chunk.strip(),\n",
    "                        \"source\": doc[\"source\"],\n",
    "                        \"topic\": doc[\"topic\"],\n",
    "                        \"chunk_id\": chunk_id\n",
    "                    })\n",
    "                    chunk_id += 1\n",
    "                    # Keep overlap: take the last `overlap` chars of the current chunk\n",
    "                    if len(current_chunk) > overlap:\n",
    "                        current_chunk = current_chunk[-overlap:] + \" \" + sentence\n",
    "                    else:\n",
    "                        current_chunk = sentence\n",
    "                else:\n",
    "                    current_chunk = (current_chunk + \" \" + sentence).strip()\n",
    "\n",
    "            # Don't forget the last chunk\n",
    "            if current_chunk.strip():\n",
    "                self.chunks.append({\n",
    "                    \"text\": current_chunk.strip(),\n",
    "                    \"source\": doc[\"source\"],\n",
    "                    \"topic\": doc[\"topic\"],\n",
    "                    \"chunk_id\": chunk_id\n",
    "                })\n",
    "                chunk_id += 1\n",
    "\n",
    "        print(f\"Created {len(self.chunks)} chunks from {len(self.documents)} documents\")\n",
    "        print(f\"Average chunk size: {np.mean([len(c['text']) for c in self.chunks]):.0f} chars\")\n",
    "        return self.chunks\n",
    "\n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Return statistics about the knowledge base.\"\"\"\n",
    "        return {\n",
    "            \"num_documents\": len(self.documents),\n",
    "            \"num_chunks\": len(self.chunks),\n",
    "            \"total_chars\": sum(len(c[\"text\"]) for c in self.chunks),\n",
    "            \"avg_chunk_size\": np.mean([len(c[\"text\"]) for c in self.chunks]) if self.chunks else 0,\n",
    "            \"sources\": list(set(c[\"source\"] for c in self.chunks))\n",
    "        }"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_16_build_kb",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Build Kb\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_16_build_kb.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build our knowledge base\n",
    "kb = KnowledgeBase()\n",
    "\n",
    "# Load curated content (always available)\n",
    "kb.load_curated(VIZUARA_KNOWLEDGE_BASE)\n",
    "\n",
    "# Try to scrape the live website (may fail â€” that's fine)\n",
    "kb.scrape_website(\"https://vizuara.ai\")\n",
    "\n",
    "# Chunk everything\n",
    "chunks = kb.chunk(chunk_size=400, overlap=50)\n",
    "\n",
    "# Show stats\n",
    "stats = kb.get_stats()\n",
    "print(f\"\\nKnowledge Base Stats:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_17_peek_chunks_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Peek Chunks Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_17_peek_chunks_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's peek at a few chunks to make sure the chunking looks reasonable."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_18_peek_chunks_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Peek Chunks Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_18_peek_chunks_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first 5 chunks\n",
    "print(\"Sample chunks:\")\n",
    "print(\"=\" * 70)\n",
    "for i, chunk in enumerate(chunks[:5]):\n",
    "    print(f\"\\nChunk {i} (source: {chunk['source']}, topic: {chunk['topic']})\")\n",
    "    print(f\"  Length: {len(chunk['text'])} chars\")\n",
    "    print(f\"  Text: {chunk['text'][:120]}...\")\n",
    "    print(\"-\" * 70)"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_19_component2_vectorstore_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Component2 Vectorstore Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_19_component2_vectorstore_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Component 2: Vector Store\n",
    "\n",
    "The vector store embeds all chunks and provides similarity search. This is the RAG engine â€” given a user query, it finds the most relevant pieces of knowledge."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_20_vectorstore_class_impl",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Vectorstore Class Impl\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_20_vectorstore_class_impl.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    Simple but functional vector store using sentence-transformers.\n",
    "    Embeds text chunks and provides cosine similarity search.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: SentenceTransformer):\n",
    "        self.model = model\n",
    "        self.embeddings = None        # numpy array of shape (N, dim)\n",
    "        self.texts = []               # raw text for each entry\n",
    "        self.metadata = []            # metadata dicts for each entry\n",
    "        self.embedding_dim = model.get_sentence_embedding_dimension()\n",
    "\n",
    "    def add(self, texts: List[str], metadata: Optional[List[Dict]] = None):\n",
    "        \"\"\"Embed and store texts with optional metadata.\"\"\"\n",
    "        if not texts:\n",
    "            return\n",
    "\n",
    "        # Compute embeddings in batch\n",
    "        new_embeddings = self.model.encode(texts, show_progress_bar=False,\n",
    "                                            normalize_embeddings=True)\n",
    "\n",
    "        if self.embeddings is None:\n",
    "            self.embeddings = new_embeddings\n",
    "        else:\n",
    "            self.embeddings = np.vstack([self.embeddings, new_embeddings])\n",
    "\n",
    "        self.texts.extend(texts)\n",
    "\n",
    "        if metadata:\n",
    "            self.metadata.extend(metadata)\n",
    "        else:\n",
    "            self.metadata.extend([{} for _ in texts])\n",
    "\n",
    "        print(f\"Added {len(texts)} entries. Total: {len(self.texts)}\")\n",
    "\n",
    "    def search(self, query: str, top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Return top-K most similar texts using cosine similarity.\n",
    "        Returns list of {\"text\": ..., \"score\": ..., \"metadata\": ...}\n",
    "        \"\"\"\n",
    "        if self.embeddings is None or len(self.texts) == 0:\n",
    "            return []\n",
    "\n",
    "        # Embed the query\n",
    "        query_embedding = self.model.encode([query], normalize_embeddings=True)\n",
    "\n",
    "        # Cosine similarity (embeddings are already normalized)\n",
    "        similarities = np.dot(self.embeddings, query_embedding.T).flatten()\n",
    "\n",
    "        # Get top-K indices\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"score\": float(similarities[idx]),\n",
    "                \"metadata\": self.metadata[idx]\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Return vector store statistics.\"\"\"\n",
    "        return {\n",
    "            \"num_entries\": len(self.texts),\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"memory_mb\": (self.embeddings.nbytes / 1e6) if self.embeddings is not None else 0\n",
    "        }"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_21_build_vectorstore",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Build Vectorstore\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_21_build_vectorstore.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vector store from our chunked knowledge base\n",
    "vector_store = VectorStore(embedding_model)\n",
    "\n",
    "# Add all chunks\n",
    "chunk_texts = [c[\"text\"] for c in chunks]\n",
    "chunk_metadata = [{\"source\": c[\"source\"], \"topic\": c[\"topic\"],\n",
    "                    \"chunk_id\": c[\"chunk_id\"]} for c in chunks]\n",
    "\n",
    "vector_store.add(chunk_texts, chunk_metadata)\n",
    "\n",
    "# Show stats\n",
    "vs_stats = vector_store.get_stats()\n",
    "print(f\"\\nVector Store Stats:\")\n",
    "for key, value in vs_stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_22_test_retrieval_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Test Retrieval Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_22_test_retrieval_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the retrieval with a few sample queries to make sure it works."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_23_test_retrieval_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Test Retrieval Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_23_test_retrieval_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval quality\n",
    "test_queries = [\n",
    "    \"What courses does Vizuara offer?\",\n",
    "    \"Do I need a GPU?\",\n",
    "    \"How do I get started?\",\n",
    "    \"Who founded Vizuara?\",\n",
    "    \"Are the notebooks free?\"\n",
    "]\n",
    "\n",
    "print(\"Retrieval Quality Test\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for query in test_queries:\n",
    "    results = vector_store.search(query, top_k=2)\n",
    "    print(f\"\\nQuery: \\\"{query}\\\"\")\n",
    "    for i, r in enumerate(results):\n",
    "        print(f\"  Result {i+1} (score={r['score']:.3f}, topic={r['metadata']['topic']}):\")\n",
    "        print(f\"    {r['text'][:100]}...\")\n",
    "    print(\"-\" * 70)"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_24_viz1_retrieval_heatmap_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Viz1 Retrieval Heatmap Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_24_viz1_retrieval_heatmap_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Checkpoint 1: Retrieval Similarity Heatmap\n",
    "\n",
    "Let's visualize how well our vector store distinguishes between different types of questions. We will embed several queries and compute their similarity to all chunks, creating a heatmap."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_25_viz1_retrieval_heatmap_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Viz1 Retrieval Heatmap Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_25_viz1_retrieval_heatmap_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Query-Chunk similarity heatmap\n",
    "viz_queries = [\n",
    "    \"What courses are available?\",\n",
    "    \"Do I need a GPU to run notebooks?\",\n",
    "    \"Who is the founder?\",\n",
    "    \"What is the teaching philosophy?\",\n",
    "    \"How much does it cost?\",\n",
    "    \"What programming language is used?\"\n",
    "]\n",
    "\n",
    "# Embed queries\n",
    "query_embeddings = embedding_model.encode(viz_queries, normalize_embeddings=True)\n",
    "\n",
    "# Compute similarities against all chunks\n",
    "similarity_matrix = np.dot(query_embeddings, vector_store.embeddings.T)\n",
    "\n",
    "# Get chunk labels (truncated topic names)\n",
    "chunk_labels = [c[\"topic\"][:20] for c in chunks]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 5))\n",
    "im = ax.imshow(similarity_matrix, cmap='YlOrRd', aspect='auto',\n",
    "               vmin=0, vmax=1)\n",
    "\n",
    "ax.set_yticks(range(len(viz_queries)))\n",
    "ax.set_yticklabels([q[:40] for q in viz_queries], fontsize=9)\n",
    "ax.set_xlabel(\"Knowledge Base Chunks\", fontsize=11)\n",
    "ax.set_ylabel(\"User Queries\", fontsize=11)\n",
    "ax.set_title(\"RAG Retrieval: Query-Chunk Similarity Heatmap\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "cbar.set_label(\"Cosine Similarity\", fontsize=10)\n",
    "\n",
    "# Mark the top-1 chunk for each query\n",
    "for i in range(len(viz_queries)):\n",
    "    best_j = np.argmax(similarity_matrix[i])\n",
    "    ax.plot(best_j, i, 'w*', markersize=12, markeredgecolor='black',\n",
    "            markeredgewidth=1)\n",
    "\n",
    "ax.text(0.02, -0.18, \"Stars mark the top-1 retrieval for each query\",\n",
    "        transform=ax.transAxes, fontsize=9, style='italic', color='#555')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… The heatmap shows each query lighting up different chunks â€”\")\n",
    "print(\"   our retrieval is correctly matching questions to relevant content.\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_26_component3_memory_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Component3 Memory Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_26_component3_memory_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Component 3: Conversation Memory\n",
    "\n",
    "This is what makes the chatbot feel personal. The memory system tracks:\n",
    "- **User profile**: name, interests, experience level, goals\n",
    "- **Conversation history**: full turn-by-turn record\n",
    "- **Key facts**: important things extracted from the conversation"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_27_memory_class_impl",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Memory Class Impl\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_27_memory_class_impl.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationMemory:\n",
    "    \"\"\"\n",
    "    Persistent memory across conversation turns.\n",
    "    Tracks user profile, conversation history, and extracted key facts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.user_profile = {}          # name, interests, experience, goals\n",
    "        self.conversation_history = []  # list of {\"role\": ..., \"content\": ...}\n",
    "        self.key_facts = []             # important facts from the conversation\n",
    "        self.turn_count = 0\n",
    "\n",
    "    def add_turn(self, role: str, content: str):\n",
    "        \"\"\"Add a conversation turn (role='user' or 'assistant').\"\"\"\n",
    "        self.conversation_history.append({\n",
    "            \"role\": role,\n",
    "            \"content\": content,\n",
    "            \"turn\": self.turn_count\n",
    "        })\n",
    "        if role == \"user\":\n",
    "            self.turn_count += 1\n",
    "            # Auto-extract facts from user messages\n",
    "            self.extract_facts(content)\n",
    "\n",
    "    def extract_facts(self, message: str):\n",
    "        \"\"\"\n",
    "        Extract key facts from a user message for long-term storage.\n",
    "        Uses simple pattern matching to identify personal information.\n",
    "        \"\"\"\n",
    "        message_lower = message.lower()\n",
    "\n",
    "        # Extract name\n",
    "        name_patterns = [\n",
    "            r\"(?:my name is|i'm|i am|call me|this is)\\s+([A-Z][a-z]+)\",\n",
    "            r\"(?:hi|hello|hey),?\\s+(?:my name is|i'm|i am)\\s+([A-Z][a-z]+)\",\n",
    "        ]\n",
    "        for pattern in name_patterns:\n",
    "            match = re.search(pattern, message, re.IGNORECASE)\n",
    "            if match:\n",
    "                name = match.group(1).capitalize()\n",
    "                self.update_profile(\"name\", name)\n",
    "                self.key_facts.append(f\"User's name is {name}\")\n",
    "                break\n",
    "\n",
    "        # Extract interests\n",
    "        interest_patterns = [\n",
    "            r\"(?:interested in|want to learn|learning about|curious about|\"\n",
    "            r\"fascinated by|working on|studying)\\s+(.+?)(?:\\.|$|,)\",\n",
    "        ]\n",
    "        for pattern in interest_patterns:\n",
    "            match = re.search(pattern, message_lower)\n",
    "            if match:\n",
    "                interest = match.group(1).strip().rstrip('.')\n",
    "                if len(interest) > 3 and len(interest) < 100:\n",
    "                    current = self.user_profile.get(\"interests\", [])\n",
    "                    if isinstance(current, list):\n",
    "                        current.append(interest)\n",
    "                    else:\n",
    "                        current = [interest]\n",
    "                    self.update_profile(\"interests\", current)\n",
    "                    self.key_facts.append(f\"User is interested in: {interest}\")\n",
    "\n",
    "        # Extract experience level\n",
    "        experience_patterns = [\n",
    "            (r\"(?:i'm a |i am a |i'm |i am )?(beginner|newbie|just starting|new to)\",\n",
    "             \"beginner\"),\n",
    "            (r\"(?:i'm |i am |i have )?(intermediate|some experience|familiar with)\",\n",
    "             \"intermediate\"),\n",
    "            (r\"(?:i'm |i am |i have )?(advanced|expert|experienced|senior|years of experience)\",\n",
    "             \"advanced\"),\n",
    "            (r\"(?:i'm a |i am a )?(student|undergrad|grad student|phd)\",\n",
    "             \"student\"),\n",
    "            (r\"(?:i'm a |i am a )?(researcher|professor|scientist)\",\n",
    "             \"researcher\"),\n",
    "            (r\"(?:i'm a |i am a )?(developer|engineer|programmer|software)\",\n",
    "             \"developer\"),\n",
    "        ]\n",
    "        for pattern, level in experience_patterns:\n",
    "            if re.search(pattern, message_lower):\n",
    "                self.update_profile(\"experience_level\", level)\n",
    "                self.key_facts.append(f\"User's experience level: {level}\")\n",
    "                break\n",
    "\n",
    "        # Extract goals\n",
    "        goal_patterns = [\n",
    "            r\"(?:i want to|i'd like to|my goal is to|hoping to|trying to)\\s+(.+?)(?:\\.|$|,)\",\n",
    "        ]\n",
    "        for pattern in goal_patterns:\n",
    "            match = re.search(pattern, message_lower)\n",
    "            if match:\n",
    "                goal = match.group(1).strip().rstrip('.')\n",
    "                if len(goal) > 5:\n",
    "                    self.update_profile(\"goal\", goal)\n",
    "                    self.key_facts.append(f\"User's goal: {goal}\")\n",
    "\n",
    "    def update_profile(self, key: str, value):\n",
    "        \"\"\"Update user profile (e.g., name, interest area).\"\"\"\n",
    "        self.user_profile[key] = value\n",
    "\n",
    "    def get_relevant_history(self, query: str = \"\", max_turns: int = 5) -> List[Dict]:\n",
    "        \"\"\"Get recent relevant conversation history.\"\"\"\n",
    "        # Return the most recent turns (simple recency-based approach)\n",
    "        recent = self.conversation_history[-(max_turns * 2):]\n",
    "        return recent\n",
    "\n",
    "    def get_profile_summary(self) -> str:\n",
    "        \"\"\"Get a human-readable summary of what we know about the user.\"\"\"\n",
    "        if not self.user_profile:\n",
    "            return \"No user information collected yet.\"\n",
    "\n",
    "        parts = []\n",
    "        if \"name\" in self.user_profile:\n",
    "            parts.append(f\"Name: {self.user_profile['name']}\")\n",
    "        if \"interests\" in self.user_profile:\n",
    "            interests = self.user_profile[\"interests\"]\n",
    "            if isinstance(interests, list):\n",
    "                parts.append(f\"Interests: {', '.join(interests)}\")\n",
    "            else:\n",
    "                parts.append(f\"Interests: {interests}\")\n",
    "        if \"experience_level\" in self.user_profile:\n",
    "            parts.append(f\"Experience: {self.user_profile['experience_level']}\")\n",
    "        if \"goal\" in self.user_profile:\n",
    "            parts.append(f\"Goal: {self.user_profile['goal']}\")\n",
    "\n",
    "        return \" | \".join(parts) if parts else \"No user information collected yet.\"\n",
    "\n",
    "    def get_state(self) -> Dict:\n",
    "        \"\"\"Return full memory state for visualization.\"\"\"\n",
    "        return {\n",
    "            \"user_profile\": dict(self.user_profile),\n",
    "            \"turn_count\": self.turn_count,\n",
    "            \"key_facts\": list(self.key_facts),\n",
    "            \"history_length\": len(self.conversation_history)\n",
    "        }"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_28_test_memory",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Narration: Test Memory\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_28_test_memory.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the memory system\n",
    "test_memory = ConversationMemory()\n",
    "\n",
    "# Simulate extracting facts from messages\n",
    "test_messages = [\n",
    "    \"Hi, my name is Alex and I'm interested in diffusion models\",\n",
    "    \"I'm a grad student working on computer vision\",\n",
    "    \"I want to build generative models for medical imaging\",\n",
    "]\n",
    "\n",
    "print(\"Testing Memory System\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for msg in test_messages:\n",
    "    test_memory.add_turn(\"user\", msg)\n",
    "    print(f\"\\nMessage: \\\"{msg}\\\"\")\n",
    "    print(f\"Profile: {test_memory.get_profile_summary()}\")\n",
    "    print(f\"Key facts: {test_memory.key_facts}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# Clean up test\n",
    "del test_memory\n",
    "print(\"\\nâœ… Memory extraction working correctly!\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_29_component4_tools_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Component4 Tools Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_29_component4_tools_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Component 4: Tool System\n",
    "\n",
    "Tools let the chatbot take actions beyond just generating text. We build a registry that lets us define, describe, and execute tools dynamically."
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_30_toolregistry_class_impl",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Toolregistry Class Impl\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_30_toolregistry_class_impl.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolRegistry:\n",
    "    \"\"\"\n",
    "    Registry for chatbot tools. Each tool has a name, function,\n",
    "    description, and parameter spec.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tools = {}  # name -> {\"func\": ..., \"description\": ..., \"params\": ...}\n",
    "\n",
    "    def register(self, name: str, func: Callable, description: str,\n",
    "                 params: Optional[Dict] = None):\n",
    "        \"\"\"Register a tool with its function and description.\"\"\"\n",
    "        self.tools[name] = {\n",
    "            \"func\": func,\n",
    "            \"description\": description,\n",
    "            \"params\": params or {}\n",
    "        }\n",
    "        print(f\"  Registered tool: {name}\")\n",
    "\n",
    "    def get_tool_descriptions(self) -> str:\n",
    "        \"\"\"Get formatted descriptions for all tools (for the LLM prompt).\"\"\"\n",
    "        descriptions = []\n",
    "        for name, info in self.tools.items():\n",
    "            param_str = \"\"\n",
    "            if info[\"params\"]:\n",
    "                param_str = f\" Parameters: {json.dumps(info['params'])}\"\n",
    "            descriptions.append(f\"- {name}: {info['description']}{param_str}\")\n",
    "        return \"\\n\".join(descriptions)\n",
    "\n",
    "    def execute(self, tool_name: str, **kwargs) -> str:\n",
    "        \"\"\"Execute a tool and return its string result.\"\"\"\n",
    "        if tool_name not in self.tools:\n",
    "            return f\"Error: Unknown tool '{tool_name}'\"\n",
    "        try:\n",
    "            result = self.tools[tool_name][\"func\"](**kwargs)\n",
    "            return str(result)\n",
    "        except Exception as e:\n",
    "            return f\"Error executing {tool_name}: {str(e)}\"\n",
    "\n",
    "    def detect_tool_need(self, message: str) -> Optional[Tuple[str, Dict]]:\n",
    "        \"\"\"\n",
    "        Simple heuristic to detect if a message needs a tool.\n",
    "        Returns (tool_name, kwargs) or None.\n",
    "        \"\"\"\n",
    "        message_lower = message.lower()\n",
    "\n",
    "        # Course lookup triggers\n",
    "        course_triggers = [\"what courses\", \"which courses\", \"course list\",\n",
    "                           \"available courses\", \"what do you offer\",\n",
    "                           \"what topics\", \"what can i learn\",\n",
    "                           \"other courses\", \"more courses\"]\n",
    "        if any(trigger in message_lower for trigger in course_triggers):\n",
    "            # Try to extract a topic\n",
    "            topic_match = re.search(\n",
    "                r\"(?:about|on|for|related to|in)\\s+(.+?)(?:\\?|$|\\.)\",\n",
    "                message_lower\n",
    "            )\n",
    "            topic = topic_match.group(1).strip() if topic_match else \"\"\n",
    "            return (\"course_lookup\", {\"topic\": topic})\n",
    "\n",
    "        # Contact triggers\n",
    "        contact_triggers = [\"contact\", \"reach\", \"email\", \"get in touch\",\n",
    "                            \"how to contact\", \"talk to someone\"]\n",
    "        if any(trigger in message_lower for trigger in contact_triggers):\n",
    "            return (\"contact_info\", {})\n",
    "\n",
    "        # FAQ triggers (broad â€” catches many questions)\n",
    "        faq_triggers = [\"how do i\", \"what is\", \"do i need\", \"can i\",\n",
    "                         \"prerequisite\", \"requirement\", \"gpu\", \"laptop\",\n",
    "                         \"free\", \"cost\", \"price\", \"get started\"]\n",
    "        if any(trigger in message_lower for trigger in faq_triggers):\n",
    "            return (\"faq_search\", {\"question\": message})\n",
    "\n",
    "        return None"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_31_define_tool_functions_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Define Tool Functions Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_31_define_tool_functions_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the actual tool functions and register them."
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_32_tool_functions_and_register",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Tool Functions And Register\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_32_tool_functions_and_register.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Tool Functions ----\n",
    "\n",
    "COURSE_CATALOG = {\n",
    "    \"reinforcement learning\": {\n",
    "        \"title\": \"Reinforcement Learning & DeepSeek-R1\",\n",
    "        \"description\": \"Learn RL from scratch â€” bandits, Q-learning, policy gradients, PPO, and how DeepSeek-R1 uses RL to improve reasoning.\",\n",
    "        \"notebooks\": 5,\n",
    "        \"prerequisites\": \"Python, basic probability\"\n",
    "    },\n",
    "    \"diffusion models\": {\n",
    "        \"title\": \"Diffusion Models for Video Generation\",\n",
    "        \"description\": \"Build diffusion models from first principles â€” noise schedules, UNets, video generation with factorized space-time attention.\",\n",
    "        \"notebooks\": 3,\n",
    "        \"prerequisites\": \"Python, PyTorch basics, linear algebra\"\n",
    "    },\n",
    "    \"world models\": {\n",
    "        \"title\": \"World Action Models for Robotics\",\n",
    "        \"description\": \"From Ha & Schmidhuber to GENIE2 â€” learn how robots build internal models of the world.\",\n",
    "        \"notebooks\": 6,\n",
    "        \"prerequisites\": \"Python, PyTorch, basic RL concepts\"\n",
    "    },\n",
    "    \"vla\": {\n",
    "        \"title\": \"Vision-Language-Action Models\",\n",
    "        \"description\": \"Build VLA models for autonomous driving â€” vision encoders, action tokenization, and diffusion action decoders.\",\n",
    "        \"notebooks\": 4,\n",
    "        \"prerequisites\": \"Python, PyTorch, transformers basics\"\n",
    "    },\n",
    "    \"parallelism\": {\n",
    "        \"title\": \"5D Parallelism for GPU Programming\",\n",
    "        \"description\": \"Data, tensor, pipeline, sequence, and expert parallelism â€” train models across thousands of GPUs.\",\n",
    "        \"notebooks\": 6,\n",
    "        \"prerequisites\": \"Python, basic distributed computing concepts\"\n",
    "    },\n",
    "    \"context engineering\": {\n",
    "        \"title\": \"Context Engineering for LLMs\",\n",
    "        \"description\": \"Master the art of what goes into the context window â€” token budgets, RAG, memory, tools, and the four strategies.\",\n",
    "        \"notebooks\": 5,\n",
    "        \"prerequisites\": \"Python, basic LLM familiarity\"\n",
    "    },\n",
    "    \"vision transformers\": {\n",
    "        \"title\": \"Vision Transformers from Scratch\",\n",
    "        \"description\": \"Build ViT from scratch â€” patch embeddings, self-attention on image patches, complete classification pipeline.\",\n",
    "        \"notebooks\": 3,\n",
    "        \"prerequisites\": \"Python, PyTorch, basic deep learning\"\n",
    "    },\n",
    "    \"diffusion llms\": {\n",
    "        \"title\": \"Diffusion Language Models from Scratch\",\n",
    "        \"description\": \"A new paradigm â€” generate text through iterative unmasking instead of autoregressive decoding.\",\n",
    "        \"notebooks\": 4,\n",
    "        \"prerequisites\": \"Python, PyTorch, transformer basics\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def course_lookup(topic: str = \"\") -> str:\n",
    "    \"\"\"Look up available Vizuara courses, optionally filtered by topic.\"\"\"\n",
    "    topic_lower = topic.lower().strip()\n",
    "\n",
    "    if topic_lower:\n",
    "        # Find matching courses\n",
    "        matches = []\n",
    "        for key, course in COURSE_CATALOG.items():\n",
    "            if (topic_lower in key or\n",
    "                topic_lower in course[\"title\"].lower() or\n",
    "                topic_lower in course[\"description\"].lower()):\n",
    "                matches.append(course)\n",
    "\n",
    "        if matches:\n",
    "            result = f\"Found {len(matches)} course(s) related to '{topic}':\\n\"\n",
    "            for c in matches:\n",
    "                result += (f\"\\n- {c['title']}: {c['description']} \"\n",
    "                          f\"({c['notebooks']} notebooks, \"\n",
    "                          f\"prerequisites: {c['prerequisites']})\")\n",
    "            return result\n",
    "        else:\n",
    "            result = f\"No courses directly matching '{topic}'. Here are all available courses:\\n\"\n",
    "            for c in COURSE_CATALOG.values():\n",
    "                result += f\"\\n- {c['title']}: {c['description']}\"\n",
    "            return result\n",
    "    else:\n",
    "        result = f\"Vizuara offers {len(COURSE_CATALOG)} courses:\\n\"\n",
    "        for c in COURSE_CATALOG.values():\n",
    "            result += (f\"\\n- {c['title']} ({c['notebooks']} notebooks)\")\n",
    "        return result\n",
    "\n",
    "\n",
    "def contact_info() -> str:\n",
    "    \"\"\"Get Vizuara contact information.\"\"\"\n",
    "    return (\"You can reach Vizuara through:\\n\"\n",
    "            \"- Website: vizuara.ai\\n\"\n",
    "            \"- Substack: vizuara.substack.com (comment on any article)\\n\"\n",
    "            \"- Articles and notebooks: vizuara.substack.com\\n\"\n",
    "            \"Vizuara encourages questions and discussion from all learners.\")\n",
    "\n",
    "\n",
    "def faq_search(question: str) -> str:\n",
    "    \"\"\"Search the FAQ knowledge base for an answer.\"\"\"\n",
    "    # Use the vector store to find relevant FAQ entries\n",
    "    results = vector_store.search(question, top_k=2)\n",
    "    if results and results[0][\"score\"] > 0.3:\n",
    "        faq_text = \"Relevant FAQ information:\\n\"\n",
    "        for r in results:\n",
    "            if r[\"score\"] > 0.3:\n",
    "                faq_text += f\"\\n[{r['metadata'].get('topic', 'FAQ')}]: {r['text']}\"\n",
    "        return faq_text\n",
    "    return \"No specific FAQ entry found for this question.\"\n",
    "\n",
    "\n",
    "# Register all tools\n",
    "tools = ToolRegistry()\n",
    "print(\"Registering tools:\")\n",
    "tools.register(\"course_lookup\", course_lookup,\n",
    "               \"Look up available Vizuara courses on a topic\",\n",
    "               {\"topic\": \"string (optional) - topic to filter by\"})\n",
    "tools.register(\"contact_info\", contact_info,\n",
    "               \"Get Vizuara contact information\")\n",
    "tools.register(\"faq_search\", faq_search,\n",
    "               \"Search the FAQ knowledge base for an answer\",\n",
    "               {\"question\": \"string - the question to search for\"})\n",
    "\n",
    "print(f\"\\n{len(tools.tools)} tools registered.\")\n",
    "print(f\"\\nTool descriptions for the LLM:\\n{tools.get_tool_descriptions()}\")"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_33_test_tools",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Narration: Test Tools\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_33_test_tools.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test each tool\n",
    "print(\"Tool Execution Tests\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. Course lookup (topic='diffusion'):\")\n",
    "print(tools.execute(\"course_lookup\", topic=\"diffusion\"))\n",
    "\n",
    "print(\"\\n2. Contact info:\")\n",
    "print(tools.execute(\"contact_info\"))\n",
    "\n",
    "print(\"\\n3. FAQ search (question='Do I need a GPU?'):\")\n",
    "print(tools.execute(\"faq_search\", question=\"Do I need a GPU?\"))\n",
    "\n",
    "print(\"\\nâœ… All tools working correctly!\")"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_34_todo1_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo1 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_34_todo1_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn â€” TODO 1: Improve Fact Extraction\n",
    "\n",
    "The `extract_facts` method above uses basic regex patterns. Your task is to make it smarter."
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_35_todo1_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo1 Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_35_todo1_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_56_todo2_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo2 Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_56_todo2_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO ============\n",
    "# Improve the ConversationMemory.extract_facts() method.\n",
    "#\n",
    "# Currently it catches simple patterns like \"my name is Alex\" and\n",
    "# \"I'm interested in diffusion models\". Your job: add MORE patterns\n",
    "# to extract richer user information.\n",
    "#\n",
    "# Specifically, implement extraction for:\n",
    "#\n",
    "# 1. OCCUPATION: Detect phrases like \"I work as a ...\", \"I'm a data scientist\",\n",
    "#    \"my job is ...\", \"I work at ...\" and store in profile as \"occupation\".\n",
    "#\n",
    "# 2. PREFERRED LEARNING STYLE: Detect \"I prefer ...\", \"I learn best by ...\",\n",
    "#    \"I like hands-on ...\", \"I'm more of a visual learner\" etc.\n",
    "#    Store in profile as \"learning_style\".\n",
    "#\n",
    "# 3. PRIOR KNOWLEDGE: Detect \"I already know ...\", \"I've used ...\",\n",
    "#    \"I'm familiar with ...\", \"I've worked with ...\" and store as\n",
    "#    a list in profile[\"prior_knowledge\"].\n",
    "#\n",
    "# Test your implementation with the test messages below.\n",
    "\n",
    "class ImprovedConversationMemory(ConversationMemory):\n",
    "    \"\"\"Extended memory with better fact extraction.\"\"\"\n",
    "\n",
    "    def extract_facts(self, message: str):\n",
    "        \"\"\"Extract key facts including occupation, learning style, prior knowledge.\"\"\"\n",
    "        # First, call the parent's extraction for name, interests, etc.\n",
    "        super().extract_facts(message)\n",
    "\n",
    "        message_lower = message.lower()\n",
    "\n",
    "        # --- YOUR CODE HERE ---\n",
    "\n",
    "        # 1. OCCUPATION extraction\n",
    "        # Hint: Look for patterns like \"I work as a\", \"I'm a data scientist\",\n",
    "        #       \"my job is\", \"I work at\"\n",
    "        occupation_patterns = [\n",
    "            # Add your regex patterns here\n",
    "            # Example: r\"(?:i work as a|i'm a|i am a)\\s+(.+?)(?:\\.|$|,| and)\"\n",
    "        ]\n",
    "        # for pattern in occupation_patterns:\n",
    "        #     match = re.search(pattern, message_lower)\n",
    "        #     if match:\n",
    "        #         self.update_profile(\"occupation\", match.group(1).strip())\n",
    "        #         self.key_facts.append(f\"User works as: {match.group(1).strip()}\")\n",
    "        #         break\n",
    "\n",
    "        # 2. LEARNING STYLE extraction\n",
    "        # Hint: Look for \"I prefer\", \"I learn best\", \"I'm a visual/hands-on learner\"\n",
    "        pass\n",
    "\n",
    "        # 3. PRIOR KNOWLEDGE extraction\n",
    "        # Hint: Look for \"I already know\", \"I've used\", \"I'm familiar with\"\n",
    "        # Store as a list in profile[\"prior_knowledge\"]\n",
    "        pass\n",
    "\n",
    "        # --- END YOUR CODE ---\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "test_memory_2 = ImprovedConversationMemory()\n",
    "test_messages_2 = [\n",
    "    \"Hi, my name is Sarah and I work as a data scientist at Google\",\n",
    "    \"I learn best by doing â€” hands-on coding is my thing\",\n",
    "    \"I already know PyTorch and I've used transformers before\",\n",
    "    \"I'm interested in learning about diffusion models for my research\",\n",
    "]\n",
    "\n",
    "print(\"Testing Improved Memory Extraction\")\n",
    "print(\"=\" * 70)\n",
    "for msg in test_messages_2:\n",
    "    test_memory_2.add_turn(\"user\", msg)\n",
    "    print(f\"\\nMessage: \\\"{msg}\\\"\")\n",
    "    print(f\"Profile: {test_memory_2.user_profile}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(f\"\\nAll key facts: {test_memory_2.key_facts}\")\n",
    "\n",
    "# Expected output should include:\n",
    "# - name: Sarah\n",
    "# - occupation: data scientist\n",
    "# - learning_style: hands-on / learning by doing\n",
    "# - prior_knowledge: [PyTorch, transformers]\n",
    "# - interests: [diffusion models]\n",
    "\n",
    "del test_memory_2"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_36_todo1_after",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo1 After\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_36_todo1_after.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_37_component5_assembler_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Component5 Assembler Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_37_component5_assembler_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Component 5: Context Assembler\n",
    "\n",
    "This is the heart of the system â€” where all four notebooks come together. The Context Assembler takes the user's query plus all the components (memory, RAG results, tool outputs) and assembles them into an optimal prompt that fits within the token budget.\n",
    "\n",
    "It applies the techniques from every notebook:\n",
    "- **Notebook 1**: Token budget calculation and allocation\n",
    "- **Notebook 2**: Health checks for context failure modes\n",
    "- **Notebook 3**: RAG retrieval and relevance scoring\n",
    "- **Notebook 4**: The four strategies â€” write, select, compress, isolate"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_38_assembler_class_impl",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Assembler Class Impl\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_38_assembler_class_impl.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextAssembler:\n",
    "    \"\"\"\n",
    "    Assembles optimal context for the LLM within a token budget.\n",
    "    Uses all four strategies from Notebooks 1-4.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_tokens: int = 30000):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.assembly_log = []  # Track what went into the context\n",
    "\n",
    "    def estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Rough token estimation: ~4 characters per token for English.\"\"\"\n",
    "        return len(text) // 4\n",
    "\n",
    "    def compress_text(self, text: str, target_ratio: float = 0.6) -> str:\n",
    "        \"\"\"\n",
    "        Strategy: COMPRESS â€” reduce text while preserving key information.\n",
    "        Simple extractive compression: keep the most information-dense sentences.\n",
    "        \"\"\"\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        if len(sentences) <= 2:\n",
    "            return text\n",
    "\n",
    "        # Score sentences by information density (length + unique words)\n",
    "        scored = []\n",
    "        for s in sentences:\n",
    "            words = set(s.lower().split())\n",
    "            # Prefer longer sentences with more unique words\n",
    "            score = len(words) * 0.7 + len(s) * 0.01\n",
    "            scored.append((score, s))\n",
    "\n",
    "        scored.sort(reverse=True)\n",
    "\n",
    "        # Keep top sentences until we hit target ratio\n",
    "        target_len = int(len(text) * target_ratio)\n",
    "        kept = []\n",
    "        current_len = 0\n",
    "        for score, s in scored:\n",
    "            if current_len + len(s) <= target_len:\n",
    "                kept.append(s)\n",
    "                current_len += len(s)\n",
    "\n",
    "        # Re-order by original position\n",
    "        ordered = [s for s in sentences if s in kept]\n",
    "        return \" \".join(ordered) if ordered else text\n",
    "\n",
    "    def health_check(self, context_parts: Dict[str, str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Strategy: Health check from Notebook 2.\n",
    "        Detect potential failure modes in the assembled context.\n",
    "        \"\"\"\n",
    "        warnings_list = []\n",
    "\n",
    "        # Check for stale context (no recent conversation history)\n",
    "        if \"history\" in context_parts:\n",
    "            if len(context_parts[\"history\"]) < 10:\n",
    "                warnings_list.append(\"SPARSE_HISTORY: Very little conversation context\")\n",
    "\n",
    "        # Check for conflicting information\n",
    "        all_text = \" \".join(context_parts.values()).lower()\n",
    "        if \"free\" in all_text and \"paid\" in all_text:\n",
    "            warnings_list.append(\"POTENTIAL_CONFLICT: Both 'free' and 'paid' mentioned\")\n",
    "\n",
    "        # Check for missing critical context\n",
    "        if \"retrieved\" not in context_parts or not context_parts[\"retrieved\"]:\n",
    "            warnings_list.append(\"NO_RETRIEVAL: No RAG results included\")\n",
    "\n",
    "        # Check for context overflow risk\n",
    "        total_tokens = sum(self.estimate_tokens(v) for v in context_parts.values())\n",
    "        if total_tokens > self.max_tokens * 0.9:\n",
    "            warnings_list.append(f\"NEAR_OVERFLOW: {total_tokens}/{self.max_tokens} tokens used\")\n",
    "\n",
    "        return warnings_list\n",
    "\n",
    "    def assemble(self, query: str, system_prompt: str,\n",
    "                 memory: ConversationMemory,\n",
    "                 retrieved_docs: List[Dict],\n",
    "                 tool_results: Optional[str] = None) -> Tuple[str, Dict]:\n",
    "        \"\"\"\n",
    "        Assemble optimal context within token budget.\n",
    "        Returns (assembled_prompt, assembly_metadata).\n",
    "        \"\"\"\n",
    "        self.assembly_log = []\n",
    "        budget = self.max_tokens\n",
    "\n",
    "        # ---- STRATEGY: WRITE (Notebook 4) ----\n",
    "        # Craft a dynamic system prompt that includes user-specific context\n",
    "        profile_summary = memory.get_profile_summary()\n",
    "        dynamic_system = system_prompt\n",
    "        if profile_summary != \"No user information collected yet.\":\n",
    "            dynamic_system += f\"\\n\\nUser profile: {profile_summary}\"\n",
    "        system_tokens = self.estimate_tokens(dynamic_system)\n",
    "        budget -= system_tokens\n",
    "        self.assembly_log.append(f\"System prompt: {system_tokens} tokens\")\n",
    "\n",
    "        # ---- STRATEGY: SELECT (Notebook 4) ----\n",
    "        # Select the most relevant conversation history\n",
    "        history = memory.get_relevant_history(query, max_turns=5)\n",
    "        history_text = \"\"\n",
    "        if history:\n",
    "            history_parts = []\n",
    "            for turn in history:\n",
    "                role_label = \"User\" if turn[\"role\"] == \"user\" else \"Assistant\"\n",
    "                history_parts.append(f\"{role_label}: {turn['content']}\")\n",
    "            history_text = \"\\n\".join(history_parts)\n",
    "\n",
    "            history_tokens = self.estimate_tokens(history_text)\n",
    "            # If history is too long, compress it\n",
    "            if history_tokens > budget * 0.3:\n",
    "                history_text = self.compress_text(history_text, target_ratio=0.5)\n",
    "                history_tokens = self.estimate_tokens(history_text)\n",
    "                self.assembly_log.append(\n",
    "                    f\"History: {history_tokens} tokens (COMPRESSED)\")\n",
    "            else:\n",
    "                self.assembly_log.append(f\"History: {history_tokens} tokens\")\n",
    "            budget -= history_tokens\n",
    "\n",
    "        # ---- STRATEGY: SELECT + COMPRESS (Notebooks 3 & 4) ----\n",
    "        # Include retrieved documents, prioritized by relevance score\n",
    "        retrieved_text = \"\"\n",
    "        if retrieved_docs:\n",
    "            doc_parts = []\n",
    "            for i, doc in enumerate(retrieved_docs):\n",
    "                if doc[\"score\"] > 0.25:  # Relevance threshold\n",
    "                    text = doc[\"text\"]\n",
    "                    doc_tokens = self.estimate_tokens(text)\n",
    "                    if doc_tokens > budget * 0.15:\n",
    "                        text = self.compress_text(text, target_ratio=0.5)\n",
    "                    doc_parts.append(\n",
    "                        f\"[Source: {doc['metadata'].get('topic', 'unknown')} | \"\n",
    "                        f\"Relevance: {doc['score']:.2f}]\\n{text}\")\n",
    "            retrieved_text = \"\\n\\n\".join(doc_parts)\n",
    "            ret_tokens = self.estimate_tokens(retrieved_text)\n",
    "            budget -= ret_tokens\n",
    "            self.assembly_log.append(\n",
    "                f\"Retrieved docs: {ret_tokens} tokens \"\n",
    "                f\"({len(doc_parts)} docs)\")\n",
    "\n",
    "        # ---- STRATEGY: ISOLATE (Notebook 4) ----\n",
    "        # Tool results get their own clearly-delimited section\n",
    "        tool_text = \"\"\n",
    "        if tool_results:\n",
    "            tool_text = f\"[TOOL RESULTS]\\n{tool_results}\\n[/TOOL RESULTS]\"\n",
    "            tool_tokens = self.estimate_tokens(tool_text)\n",
    "            budget -= tool_tokens\n",
    "            self.assembly_log.append(f\"Tool results: {tool_tokens} tokens\")\n",
    "\n",
    "        # ---- HEALTH CHECK (Notebook 2) ----\n",
    "        context_parts = {\n",
    "            \"system\": dynamic_system,\n",
    "            \"history\": history_text,\n",
    "            \"retrieved\": retrieved_text,\n",
    "            \"tools\": tool_text,\n",
    "            \"query\": query,\n",
    "        }\n",
    "        warnings_list = self.health_check(context_parts)\n",
    "        if warnings_list:\n",
    "            self.assembly_log.append(f\"Health warnings: {warnings_list}\")\n",
    "\n",
    "        # ---- FINAL ASSEMBLY ----\n",
    "        # Build the complete prompt with clear section delimiters\n",
    "        sections = [dynamic_system]\n",
    "\n",
    "        if history_text:\n",
    "            sections.append(f\"\\n--- Conversation History ---\\n{history_text}\")\n",
    "\n",
    "        if retrieved_text:\n",
    "            sections.append(\n",
    "                f\"\\n--- Relevant Knowledge ---\\n{retrieved_text}\")\n",
    "\n",
    "        if tool_text:\n",
    "            sections.append(f\"\\n{tool_text}\")\n",
    "\n",
    "        if memory.key_facts:\n",
    "            facts_text = \"\\n\".join(f\"- {f}\" for f in memory.key_facts[-5:])\n",
    "            sections.append(\n",
    "                f\"\\n--- Key Facts About This User ---\\n{facts_text}\")\n",
    "\n",
    "        sections.append(f\"\\n--- Current Question ---\\n{query}\")\n",
    "\n",
    "        assembled = \"\\n\".join(sections)\n",
    "\n",
    "        # Token budget report\n",
    "        total_tokens = self.estimate_tokens(assembled)\n",
    "        metadata = {\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"utilization\": total_tokens / self.max_tokens,\n",
    "            \"assembly_log\": self.assembly_log,\n",
    "            \"warnings\": warnings_list,\n",
    "            \"sections\": {\n",
    "                \"system\": system_tokens,\n",
    "                \"history\": self.estimate_tokens(history_text),\n",
    "                \"retrieved\": self.estimate_tokens(retrieved_text),\n",
    "                \"tools\": self.estimate_tokens(tool_text),\n",
    "                \"query\": self.estimate_tokens(query),\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return assembled, metadata"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_39_test_assembler",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Narration: Test Assembler\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_39_test_assembler.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the context assembler with a mock scenario\n",
    "test_assembler = ContextAssembler(max_tokens=30000)\n",
    "test_mem = ConversationMemory()\n",
    "test_mem.add_turn(\"user\", \"Hi, my name is Alex and I'm interested in diffusion models\")\n",
    "test_mem.add_turn(\"assistant\", \"Hi Alex! Great to hear you're interested in diffusion models.\")\n",
    "\n",
    "test_system = (\"You are a friendly assistant for Vizuara AI Labs. \"\n",
    "               \"Help students learn about courses and AI concepts.\")\n",
    "\n",
    "test_retrieved = vector_store.search(\"diffusion models course\", top_k=3)\n",
    "\n",
    "assembled, meta = test_assembler.assemble(\n",
    "    query=\"What prerequisites do I need for the diffusion models course?\",\n",
    "    system_prompt=test_system,\n",
    "    memory=test_mem,\n",
    "    retrieved_docs=test_retrieved,\n",
    "    tool_results=None\n",
    ")\n",
    "\n",
    "print(\"Context Assembly Report\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total tokens: {meta['total_tokens']} / {meta['max_tokens']}\")\n",
    "print(f\"Utilization: {meta['utilization']:.1%}\")\n",
    "print(f\"\\nSection breakdown:\")\n",
    "for section, tokens in meta['sections'].items():\n",
    "    bar = \"â–ˆ\" * (tokens // 5) if tokens > 0 else \"\"\n",
    "    print(f\"  {section:12s}: {tokens:5d} tokens {bar}\")\n",
    "print(f\"\\nAssembly log:\")\n",
    "for entry in meta['assembly_log']:\n",
    "    print(f\"  - {entry}\")\n",
    "if meta['warnings']:\n",
    "    print(f\"\\nHealth warnings:\")\n",
    "    for w in meta['warnings']:\n",
    "        print(f\"  âš  {w}\")\n",
    "\n",
    "print(f\"\\n--- First 500 chars of assembled context ---\")\n",
    "print(assembled[:500])\n",
    "\n",
    "del test_assembler, test_mem"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_40_viz2_token_budget_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Viz2 Token Budget Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_40_viz2_token_budget_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Checkpoint 2: Token Budget Allocation\n",
    "\n",
    "Let's visualize how the context assembler allocates the token budget across different sections. This is the Notebook 1 technique in action."
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_41_viz2_token_budget_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Viz2 Token Budget Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_41_viz2_token_budget_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Token budget allocation as a stacked bar + pie chart\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sections = meta['sections']\n",
    "section_names = list(sections.keys())\n",
    "section_values = list(sections.values())\n",
    "colors = ['#1976D2', '#7B1FA2', '#388E3C', '#D32F2F', '#F57C00']\n",
    "\n",
    "# Stacked horizontal bar\n",
    "left = 0\n",
    "for i, (name, val) in enumerate(zip(section_names, section_values)):\n",
    "    ax1.barh(0, val, left=left, color=colors[i], edgecolor='white',\n",
    "             height=0.6, label=f\"{name} ({val} tokens)\")\n",
    "    if val > 20:\n",
    "        ax1.text(left + val/2, 0, f\"{name}\\n{val}\",\n",
    "                 ha='center', va='center', fontsize=8,\n",
    "                 fontweight='bold', color='white')\n",
    "    left += val\n",
    "\n",
    "# Show remaining budget\n",
    "remaining = meta['max_tokens'] - sum(section_values)\n",
    "ax1.barh(0, remaining, left=left, color='#EEEEEE', edgecolor='white',\n",
    "         height=0.6, label=f\"remaining ({remaining} tokens)\")\n",
    "ax1.text(left + remaining/2, 0, f\"available\\n{remaining}\",\n",
    "         ha='center', va='center', fontsize=8, color='#999')\n",
    "\n",
    "ax1.set_xlim(0, meta['max_tokens'])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_xlabel(\"Tokens\", fontsize=11)\n",
    "ax1.set_title(\"Token Budget Allocation (Notebook 1 Technique)\",\n",
    "              fontsize=12, fontweight='bold')\n",
    "ax1.legend(loc='upper right', fontsize=8)\n",
    "\n",
    "# Pie chart of used tokens\n",
    "used_names = [n for n, v in zip(section_names, section_values) if v > 0]\n",
    "used_values = [v for v in section_values if v > 0]\n",
    "used_colors = [c for c, v in zip(colors, section_values) if v > 0]\n",
    "\n",
    "ax2.pie(used_values, labels=used_names, colors=used_colors,\n",
    "        autopct='%1.0f%%', startangle=90, textprops={'fontsize': 10})\n",
    "ax2.set_title(f\"Context Composition\\n({sum(used_values)} tokens used)\",\n",
    "              fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle(\"How the Context Window Is Filled\",\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_42_component6_chatbot_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Component6 Chatbot Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_42_component6_chatbot_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Component 6: The Chatbot\n",
    "\n",
    "Now we tie everything together. The `VizuaraChatbot` class orchestrates all components into a single, coherent conversational agent."
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_43_chatbot_class_impl",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Chatbot Class Impl\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_43_chatbot_class_impl.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHATBOT_SYSTEM_PROMPT = \"\"\"You are a friendly, knowledgeable assistant for Vizuara AI Labs. You help students learn about Vizuara's courses, articles, and teaching philosophy.\n",
    "\n",
    "Key behaviors:\n",
    "- Be warm and encouraging â€” students are here to learn\n",
    "- If you know the user's name, use it naturally in your responses\n",
    "- Reference the user's interests when suggesting content\n",
    "- If asked about something not in your knowledge base, say so honestly\n",
    "- Always encourage hands-on learning with the Colab notebooks\n",
    "- Keep responses concise but helpful (2-4 sentences for simple questions, more for complex ones)\n",
    "- When recommending courses, mention the number of notebooks and prerequisites\n",
    "- If the user seems new, be extra welcoming and suggest starting points\"\"\"\n",
    "\n",
    "\n",
    "class VizuaraChatbot:\n",
    "    \"\"\"\n",
    "    The complete FAQ chatbot â€” ties together knowledge base, vector store,\n",
    "    memory, tools, and context assembler into a conversational agent.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key_configured: bool = True):\n",
    "        # Initialize all components\n",
    "        self.knowledge_base = kb           # Already built above\n",
    "        self.vector_store = vector_store   # Already built above\n",
    "        self.memory = ConversationMemory()\n",
    "        self.tools = tools                 # Already built above\n",
    "        self.assembler = ContextAssembler(max_tokens=30000)\n",
    "        self.model = genai.GenerativeModel(MODEL_NAME)\n",
    "        self.system_prompt = CHATBOT_SYSTEM_PROMPT\n",
    "\n",
    "        # Conversation metadata for visualization\n",
    "        self.turn_metadata = []\n",
    "\n",
    "    def chat(self, user_message: str) -> str:\n",
    "        \"\"\"\n",
    "        Process a user message and generate a response.\n",
    "        This is the main entry point â€” it orchestrates all components.\n",
    "        \"\"\"\n",
    "        turn_info = {\n",
    "            \"turn\": self.memory.turn_count + 1,\n",
    "            \"user_message\": user_message,\n",
    "            \"tools_called\": [],\n",
    "            \"docs_retrieved\": [],\n",
    "            \"memory_updates\": [],\n",
    "            \"token_usage\": {}\n",
    "        }\n",
    "\n",
    "        # Step 1: Update memory with the user's message\n",
    "        profile_before = dict(self.memory.user_profile)\n",
    "        self.memory.add_turn(\"user\", user_message)\n",
    "        profile_after = dict(self.memory.user_profile)\n",
    "\n",
    "        # Track what changed in memory\n",
    "        for key in profile_after:\n",
    "            if key not in profile_before or profile_after[key] != profile_before[key]:\n",
    "                turn_info[\"memory_updates\"].append(\n",
    "                    f\"{key}: {profile_after[key]}\")\n",
    "\n",
    "        # Step 2: Check if tools are needed\n",
    "        tool_result = None\n",
    "        tool_detection = self.tools.detect_tool_need(user_message)\n",
    "        if tool_detection:\n",
    "            tool_name, tool_kwargs = tool_detection\n",
    "            tool_result = self.tools.execute(tool_name, **tool_kwargs)\n",
    "            turn_info[\"tools_called\"].append({\n",
    "                \"name\": tool_name,\n",
    "                \"args\": tool_kwargs,\n",
    "                \"result_preview\": tool_result[:100] + \"...\"\n",
    "                                   if len(tool_result) > 100 else tool_result\n",
    "            })\n",
    "\n",
    "        # Step 3: Retrieve relevant documents\n",
    "        retrieved = self.vector_store.search(user_message, top_k=3)\n",
    "        turn_info[\"docs_retrieved\"] = [\n",
    "            {\"topic\": r[\"metadata\"].get(\"topic\", \"unknown\"),\n",
    "             \"score\": round(r[\"score\"], 3)}\n",
    "            for r in retrieved\n",
    "        ]\n",
    "\n",
    "        # Step 4: Assemble context (the core of context engineering!)\n",
    "        assembled_context, assembly_meta = self.assembler.assemble(\n",
    "            query=user_message,\n",
    "            system_prompt=self.system_prompt,\n",
    "            memory=self.memory,\n",
    "            retrieved_docs=retrieved,\n",
    "            tool_results=tool_result\n",
    "        )\n",
    "        turn_info[\"token_usage\"] = assembly_meta\n",
    "\n",
    "        # Step 5: Generate response via Gemini\n",
    "        try:\n",
    "            response = self.model.generate_content(assembled_context)\n",
    "            assistant_message = response.text.strip()\n",
    "        except Exception as e:\n",
    "            assistant_message = (\n",
    "                f\"I apologize, but I encountered an error generating a response. \"\n",
    "                f\"Please try again. (Error: {str(e)[:100]})\")\n",
    "\n",
    "        # Step 6: Update conversation history with the response\n",
    "        self.memory.add_turn(\"assistant\", assistant_message)\n",
    "\n",
    "        # Store metadata for this turn\n",
    "        self.turn_metadata.append(turn_info)\n",
    "\n",
    "        return assistant_message\n",
    "\n",
    "    def get_turn_report(self, turn_index: int = -1) -> Dict:\n",
    "        \"\"\"Get detailed metadata for a specific turn.\"\"\"\n",
    "        if not self.turn_metadata:\n",
    "            return {}\n",
    "        return self.turn_metadata[turn_index]\n",
    "\n",
    "    def get_memory_state(self) -> Dict:\n",
    "        \"\"\"Get the current memory state.\"\"\"\n",
    "        return self.memory.get_state()\n",
    "\n",
    "    def visualize_turn(self, turn_index: int = -1):\n",
    "        \"\"\"Visualize what happened during a specific turn.\"\"\"\n",
    "        if not self.turn_metadata:\n",
    "            print(\"No turns to visualize yet.\")\n",
    "            return\n",
    "\n",
    "        info = self.turn_metadata[turn_index]\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"  Turn {info['turn']} Analysis\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "\n",
    "        print(f\"\\n  User: \\\"{info['user_message'][:80]}...\\\"\"\n",
    "              if len(info['user_message']) > 80\n",
    "              else f\"\\n  User: \\\"{info['user_message']}\\\"\")\n",
    "\n",
    "        # Memory updates\n",
    "        if info['memory_updates']:\n",
    "            print(f\"\\n  Memory Updates:\")\n",
    "            for update in info['memory_updates']:\n",
    "                print(f\"    + {update}\")\n",
    "        else:\n",
    "            print(f\"\\n  Memory Updates: (none)\")\n",
    "\n",
    "        # Tools called\n",
    "        if info['tools_called']:\n",
    "            print(f\"\\n  Tools Called:\")\n",
    "            for tool in info['tools_called']:\n",
    "                print(f\"    -> {tool['name']}({tool['args']})\")\n",
    "                print(f\"       Result: {tool['result_preview']}\")\n",
    "        else:\n",
    "            print(f\"\\n  Tools Called: (none)\")\n",
    "\n",
    "        # Documents retrieved\n",
    "        if info['docs_retrieved']:\n",
    "            print(f\"\\n  RAG Retrieved:\")\n",
    "            for doc in info['docs_retrieved']:\n",
    "                score_bar = \"â–ˆ\" * int(doc['score'] * 20)\n",
    "                print(f\"    [{doc['score']:.3f}] {score_bar} {doc['topic']}\")\n",
    "        else:\n",
    "            print(f\"\\n  RAG Retrieved: (none)\")\n",
    "\n",
    "        # Token usage\n",
    "        usage = info['token_usage']\n",
    "        if usage:\n",
    "            print(f\"\\n  Token Budget:\")\n",
    "            total = usage.get('total_tokens', 0)\n",
    "            max_t = usage.get('max_tokens', 30000)\n",
    "            pct = usage.get('utilization', 0)\n",
    "            bar_len = int(pct * 40)\n",
    "            bar = \"â–ˆ\" * bar_len + \"â–‘\" * (40 - bar_len)\n",
    "            print(f\"    [{bar}] {total}/{max_t} ({pct:.1%})\")\n",
    "\n",
    "        print(f\"\\n{'=' * 70}\")"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_44_build_chatbot",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Narration: Build Chatbot\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_44_build_chatbot.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the chatbot!\n",
    "print(\"Initializing Vizuara FAQ Chatbot...\")\n",
    "print(\"-\" * 40)\n",
    "chatbot = VizuaraChatbot()\n",
    "print(\"-\" * 40)\n",
    "print(\"\\nâœ… Chatbot ready! All components connected:\")\n",
    "print(f\"   Knowledge Base: {len(kb.chunks)} chunks\")\n",
    "print(f\"   Vector Store:   {vector_store.get_stats()['num_entries']} embeddings\")\n",
    "print(f\"   Tools:          {len(tools.tools)} tools registered\")\n",
    "print(f\"   Model:          {MODEL_NAME}\")\n",
    "print(f\"   Max tokens:     {chatbot.assembler.max_tokens}\")"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_45_live_demo_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Live Demo Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_45_live_demo_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together â€” The Live Demo\n",
    "\n",
    "This is the moment of truth. We run a multi-turn conversation that demonstrates every feature: memory, RAG, tools, and context assembly working together.\n",
    "\n",
    "Watch what happens at each turn â€” the chatbot remembers who the user is, retrieves relevant knowledge, calls tools when needed, and personalizes every response."
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_46_demo_turn1",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Narration: Demo Turn1\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_46_demo_turn1.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 1: Introduction with personal info\n",
    "print(\"â•”\" + \"â•\" * 68 + \"â•—\")\n",
    "print(\"â•‘  LIVE DEMO: Multi-Turn Conversation with the Vizuara Chatbot    â•‘\")\n",
    "print(\"â•š\" + \"â•\" * 68 + \"â•\")\n",
    "\n",
    "user_msg_1 = \"Hi, my name is Alex and I'm interested in learning about diffusion models\"\n",
    "\n",
    "print(f\"\\nðŸ‘¤ User: {user_msg_1}\")\n",
    "response_1 = chatbot.chat(user_msg_1)\n",
    "print(f\"\\nðŸ¤– Chatbot: {response_1}\")\n",
    "chatbot.visualize_turn(-1)"
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_47_demo_turn2",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Narration: Demo Turn2\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_47_demo_turn2.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 2: Follow-up that requires memory + RAG\n",
    "user_msg_2 = \"What prerequisites do I need?\"\n",
    "\n",
    "print(f\"\\nðŸ‘¤ User: {user_msg_2}\")\n",
    "response_2 = chatbot.chat(user_msg_2)\n",
    "print(f\"\\nðŸ¤– Chatbot: {response_2}\")\n",
    "chatbot.visualize_turn(-1)"
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the chatbot remembers that Alex asked about diffusion models â€” even though Turn 2 does not mention diffusion models at all. The conversation history provides the context the model needs to give a relevant answer."
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_48_demo_turn3",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Narration: Demo Turn3\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_48_demo_turn3.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 3: GPU/hardware question (triggers FAQ search)\n",
    "user_msg_3 = \"Can I run the notebooks on my laptop?\"\n",
    "\n",
    "print(f\"\\nðŸ‘¤ User: {user_msg_3}\")\n",
    "response_3 = chatbot.chat(user_msg_3)\n",
    "print(f\"\\nðŸ¤– Chatbot: {response_3}\")\n",
    "chatbot.visualize_turn(-1)"
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_49_demo_turn4",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Narration: Demo Turn4\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_49_demo_turn4.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 4: Course discovery (triggers course_lookup tool)\n",
    "user_msg_4 = \"What other courses do you have?\"\n",
    "\n",
    "print(f\"\\nðŸ‘¤ User: {user_msg_4}\")\n",
    "response_4 = chatbot.chat(user_msg_4)\n",
    "print(f\"\\nðŸ¤– Chatbot: {response_4}\")\n",
    "chatbot.visualize_turn(-1)"
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_50_demo_turn5",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Narration: Demo Turn5\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_50_demo_turn5.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 5: Teaching philosophy (personalized retrieval)\n",
    "user_msg_5 = \"Tell me more about the teaching philosophy\"\n",
    "\n",
    "print(f\"\\nðŸ‘¤ User: {user_msg_5}\")\n",
    "response_5 = chatbot.chat(user_msg_5)\n",
    "print(f\"\\nðŸ¤– Chatbot: {response_5}\")\n",
    "chatbot.visualize_turn(-1)"
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_51_viz3_dashboard_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Viz3 Dashboard Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_51_viz3_dashboard_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Checkpoint 3: Conversation Dashboard\n",
    "\n",
    "Now let's build a comprehensive dashboard showing everything that happened across all 5 turns â€” memory evolution, tool usage, retrieval scores, and token consumption."
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_52_viz3_dashboard_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Viz3 Dashboard Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_52_viz3_dashboard_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full conversation dashboard\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "turns = list(range(1, len(chatbot.turn_metadata) + 1))\n",
    "\n",
    "# ---- Panel 1: Token usage per turn ----\n",
    "ax = axes[0, 0]\n",
    "token_totals = []\n",
    "section_data = {\n",
    "    \"system\": [], \"history\": [], \"retrieved\": [], \"tools\": [], \"query\": []\n",
    "}\n",
    "\n",
    "for info in chatbot.turn_metadata:\n",
    "    usage = info.get(\"token_usage\", {})\n",
    "    sections = usage.get(\"sections\", {})\n",
    "    total = usage.get(\"total_tokens\", 0)\n",
    "    token_totals.append(total)\n",
    "    for key in section_data:\n",
    "        section_data[key].append(sections.get(key, 0))\n",
    "\n",
    "# Stacked bar chart\n",
    "bottom = np.zeros(len(turns))\n",
    "colors_sections = ['#1976D2', '#7B1FA2', '#388E3C', '#D32F2F', '#F57C00']\n",
    "for i, (key, values) in enumerate(section_data.items()):\n",
    "    ax.bar(turns, values, bottom=bottom, label=key,\n",
    "           color=colors_sections[i], edgecolor='white', width=0.6)\n",
    "    bottom += np.array(values)\n",
    "\n",
    "ax.set_xlabel(\"Turn\", fontsize=11)\n",
    "ax.set_ylabel(\"Tokens\", fontsize=11)\n",
    "ax.set_title(\"Token Usage Per Turn\", fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=9, loc='upper left')\n",
    "ax.set_xticks(turns)\n",
    "\n",
    "# ---- Panel 2: RAG retrieval scores ----\n",
    "ax = axes[0, 1]\n",
    "for i, info in enumerate(chatbot.turn_metadata):\n",
    "    docs = info.get(\"docs_retrieved\", [])\n",
    "    scores = [d[\"score\"] for d in docs]\n",
    "    topics = [d[\"topic\"][:15] for d in docs]\n",
    "    x_positions = [turns[i] - 0.2, turns[i], turns[i] + 0.2][:len(scores)]\n",
    "    colors_rag = ['#4CAF50', '#81C784', '#C8E6C9'][:len(scores)]\n",
    "    bars = ax.bar(x_positions, scores, width=0.18, color=colors_rag,\n",
    "                  edgecolor='white')\n",
    "\n",
    "ax.axhline(y=0.25, color='red', linestyle='--', alpha=0.5,\n",
    "           label='Relevance threshold')\n",
    "ax.set_xlabel(\"Turn\", fontsize=11)\n",
    "ax.set_ylabel(\"Cosine Similarity\", fontsize=11)\n",
    "ax.set_title(\"RAG Retrieval Scores\", fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(turns)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# ---- Panel 3: Memory evolution ----\n",
    "ax = axes[1, 0]\n",
    "profile_keys_over_time = []\n",
    "facts_over_time = []\n",
    "\n",
    "# Replay memory evolution\n",
    "replay_mem = ConversationMemory()\n",
    "for info in chatbot.turn_metadata:\n",
    "    replay_mem.add_turn(\"user\", info[\"user_message\"])\n",
    "    profile_keys_over_time.append(len(replay_mem.user_profile))\n",
    "    facts_over_time.append(len(replay_mem.key_facts))\n",
    "\n",
    "ax.plot(turns, profile_keys_over_time, 'o-', color='#9C27B0',\n",
    "        linewidth=2, markersize=8, label='Profile fields')\n",
    "ax.plot(turns, facts_over_time, 's-', color='#FF9800',\n",
    "        linewidth=2, markersize=8, label='Key facts stored')\n",
    "ax.fill_between(turns, profile_keys_over_time, alpha=0.1, color='#9C27B0')\n",
    "ax.fill_between(turns, facts_over_time, alpha=0.1, color='#FF9800')\n",
    "ax.set_xlabel(\"Turn\", fontsize=11)\n",
    "ax.set_ylabel(\"Count\", fontsize=11)\n",
    "ax.set_title(\"Memory Growth Over Conversation\", fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_xticks(turns)\n",
    "\n",
    "# ---- Panel 4: Tool calls per turn ----\n",
    "ax = axes[1, 1]\n",
    "tool_calls = [len(info.get(\"tools_called\", [])) for info in chatbot.turn_metadata]\n",
    "tool_names = []\n",
    "for info in chatbot.turn_metadata:\n",
    "    called = info.get(\"tools_called\", [])\n",
    "    if called:\n",
    "        tool_names.append(called[0][\"name\"])\n",
    "    else:\n",
    "        tool_names.append(\"(none)\")\n",
    "\n",
    "bar_colors_tools = ['#E0E0E0' if t == 0 else '#F44336' for t in tool_calls]\n",
    "bars = ax.bar(turns, tool_calls, color=bar_colors_tools,\n",
    "              edgecolor='white', width=0.6)\n",
    "\n",
    "for i, (count, name) in enumerate(zip(tool_calls, tool_names)):\n",
    "    ax.text(turns[i], count + 0.05, name, ha='center', va='bottom',\n",
    "            fontsize=8, rotation=30, color='#333')\n",
    "\n",
    "ax.set_xlabel(\"Turn\", fontsize=11)\n",
    "ax.set_ylabel(\"Tools Called\", fontsize=11)\n",
    "ax.set_title(\"Tool Usage Per Turn\", fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(turns)\n",
    "ax.set_ylim(0, max(tool_calls) + 1 if max(tool_calls) > 0 else 2)\n",
    "\n",
    "plt.suptitle(\"Conversation Dashboard â€” All Systems Working Together\",\n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_53_memory_state_summary_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Memory State Summary Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_53_memory_state_summary_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at the full memory state after all 5 turns."
   ],
   "id": "cell_51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_54_memory_state_summary_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Narration: Memory State Summary Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_54_memory_state_summary_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory state summary\n",
    "state = chatbot.get_memory_state()\n",
    "\n",
    "print(\"Memory State After 5 Turns\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n  User Profile:\")\n",
    "for key, value in state[\"user_profile\"].items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "print(f\"\\n  Key Facts ({len(state['key_facts'])}):\")\n",
    "for fact in state[\"key_facts\"]:\n",
    "    print(f\"    - {fact}\")\n",
    "print(f\"\\n  Conversation turns: {state['turn_count']}\")\n",
    "print(f\"  History entries: {state['history_length']}\")\n",
    "print(f\"\\n  Profile summary: {chatbot.memory.get_profile_summary()}\")"
   ],
   "id": "cell_52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_55_todo2_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo2 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_55_todo2_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Your Turn â€” TODO 2: Build a Learning Path Recommender\n",
    "\n",
    "Now you build a new tool and register it with the chatbot. This is the capstone exercise â€” you are extending a working system with a new capability."
   ],
   "id": "cell_53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO ============\n",
    "# Implement a learning path recommender tool and register it.\n",
    "#\n",
    "# The tool should:\n",
    "# 1. Take a user's interests and experience level as input\n",
    "# 2. Recommend a sequence of 3-5 Vizuara courses in a logical order\n",
    "# 3. Explain WHY each course is recommended and how they build on each other\n",
    "#\n",
    "# Use the COURSE_CATALOG dictionary defined earlier.\n",
    "#\n",
    "# Hint: Think about prerequisite chains. For example:\n",
    "#   - A beginner interested in generative AI might start with\n",
    "#     Vision Transformers â†’ Diffusion Models â†’ Diffusion LLMs\n",
    "#   - Someone interested in robotics might go:\n",
    "#     World Models â†’ VLA Models\n",
    "#   - Someone interested in systems might start with:\n",
    "#     5D Parallelism â†’ Context Engineering\n",
    "\n",
    "def recommend_learning_path(interests: str = \"\",\n",
    "                             experience_level: str = \"beginner\") -> str:\n",
    "    \"\"\"\n",
    "    Recommend a personalized learning path through Vizuara courses.\n",
    "\n",
    "    Args:\n",
    "        interests: comma-separated list of user interests\n",
    "        experience_level: beginner, intermediate, or advanced\n",
    "\n",
    "    Returns:\n",
    "        A formatted learning path recommendation string.\n",
    "    \"\"\"\n",
    "    # --- YOUR CODE HERE ---\n",
    "    #\n",
    "    # Step 1: Parse the interests string into a list of topics\n",
    "    # Step 2: Match interests to courses in COURSE_CATALOG\n",
    "    # Step 3: Order courses logically (foundational â†’ advanced)\n",
    "    # Step 4: If experience_level is \"advanced\", skip intro courses\n",
    "    # Step 5: Format and return a numbered learning path\n",
    "    #\n",
    "    # Example output:\n",
    "    # \"Recommended Learning Path for 'generative AI' (beginner):\n",
    "    #\n",
    "    #  1. Vision Transformers from Scratch (3 notebooks)\n",
    "    #     â†’ Start here to understand attention and patches\n",
    "    #\n",
    "    #  2. Diffusion Models for Video Generation (3 notebooks)\n",
    "    #     â†’ Build on transformer knowledge to generate images and video\n",
    "    #\n",
    "    #  3. Diffusion Language Models from Scratch (4 notebooks)\n",
    "    #     â†’ Apply diffusion to text â€” a new generation paradigm\n",
    "    #\n",
    "    #  Total: 10 notebooks, ~5-7 hours of hands-on learning\"\n",
    "\n",
    "    result = f\"Recommended Learning Path (experience: {experience_level}):\\n\\n\"\n",
    "\n",
    "    # Placeholder â€” replace with your implementation\n",
    "    result += \"(TODO: Implement learning path logic here)\\n\"\n",
    "    result += \"\\nHint: Use COURSE_CATALOG to find matching courses,\\n\"\n",
    "    result += \"then order them from foundational to advanced.\"\n",
    "\n",
    "    return result\n",
    "    # --- END YOUR CODE ---\n",
    "\n",
    "\n",
    "# Register it with the tool registry\n",
    "# (Uncomment and complete after implementing)\n",
    "# chatbot.tools.register(\n",
    "#     \"recommend_learning_path\",\n",
    "#     recommend_learning_path,\n",
    "#     \"Recommend a personalized sequence of Vizuara courses based on \"\n",
    "#     \"interests and experience level\",\n",
    "#     {\"interests\": \"string - user's interests\",\n",
    "#      \"experience_level\": \"string - beginner/intermediate/advanced\"}\n",
    "# )\n",
    "\n",
    "# Test your implementation:\n",
    "# print(recommend_learning_path(\"generative AI, diffusion models\", \"beginner\"))\n",
    "# print(recommend_learning_path(\"robotics, autonomous driving\", \"intermediate\"))"
   ],
   "id": "cell_54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_57_todo2_after",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo2 After\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_57_todo2_after.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_58_viz_context_window_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Viz Context Window Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_58_viz_context_window_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Putting It All Together â€” Full Context Window Visualization\n",
    "\n",
    "Let's build one final visualization: a detailed view of exactly what is inside the context window for a single turn. This is the ultimate diagnostic â€” you can see every piece of context that influenced the model's response."
   ],
   "id": "cell_55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_59_viz_context_window_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Viz Context Window Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_59_viz_context_window_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the most interesting turn (Turn 4: course lookup with tools)\n",
    "turn_idx = 3  # 0-indexed (Turn 4)\n",
    "info = chatbot.turn_metadata[turn_idx]\n",
    "usage = info.get(\"token_usage\", {})\n",
    "sections = usage.get(\"sections\", {})\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 12)\n",
    "ax.axis('off')\n",
    "ax.set_title(f\"Inside the Context Window â€” Turn {turn_idx + 1}\",\n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# Draw the context window as a tall rectangle\n",
    "context_rect = plt.Rectangle((0.5, 0.5), 9, 11, linewidth=3,\n",
    "                                edgecolor='#333', facecolor='#FAFAFA',\n",
    "                                linestyle='-')\n",
    "ax.add_patch(context_rect)\n",
    "ax.text(5, 11.7, f\"Context Window ({usage.get('total_tokens', 0)} / \"\n",
    "        f\"{usage.get('max_tokens', 30000)} tokens)\",\n",
    "        ha='center', fontsize=12, fontweight='bold', color='#333')\n",
    "\n",
    "# Draw sections as colored blocks inside the context window\n",
    "section_config = [\n",
    "    (\"System Prompt\", '#1976D2', '#E3F2FD', sections.get('system', 0),\n",
    "     \"Personality + user profile\"),\n",
    "    (\"Conversation History\", '#7B1FA2', '#F3E5F5', sections.get('history', 0),\n",
    "     \"Recent turns for continuity\"),\n",
    "    (\"Retrieved Knowledge (RAG)\", '#388E3C', '#E8F5E9', sections.get('retrieved', 0),\n",
    "     \"Top-K relevant chunks\"),\n",
    "    (\"Tool Results\", '#D32F2F', '#FFEBEE', sections.get('tools', 0),\n",
    "     \"Course catalog, FAQ data\"),\n",
    "    (\"User Query\", '#F57C00', '#FFF3E0', sections.get('query', 0),\n",
    "     \"The actual question\"),\n",
    "]\n",
    "\n",
    "total_used = sum(s[3] for s in section_config)\n",
    "y_pos = 10.5\n",
    "for name, edge, face, tokens, desc in section_config:\n",
    "    if tokens == 0:\n",
    "        continue\n",
    "    # Height proportional to token usage\n",
    "    height = max(0.5, (tokens / max(total_used, 1)) * 8)\n",
    "    rect = plt.Rectangle((1.0, y_pos - height), 8, height,\n",
    "                           linewidth=2, edgecolor=edge, facecolor=face,\n",
    "                           alpha=0.8)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(5, y_pos - height/2, f\"{name}\\n{tokens} tokens\",\n",
    "            ha='center', va='center', fontsize=10, fontweight='bold',\n",
    "            color=edge)\n",
    "    ax.text(9.3, y_pos - height/2, desc, ha='left', va='center',\n",
    "            fontsize=8, color='#666', style='italic')\n",
    "    y_pos -= height + 0.15\n",
    "\n",
    "# Show remaining budget\n",
    "remaining = usage.get('max_tokens', 30000) - total_used\n",
    "if remaining > 0:\n",
    "    remaining_height = max(0.3, (remaining / usage.get('max_tokens', 30000)) * 8)\n",
    "    remaining_height = min(remaining_height, y_pos - 0.7)\n",
    "    if remaining_height > 0:\n",
    "        rect = plt.Rectangle((1.0, y_pos - remaining_height), 8,\n",
    "                               remaining_height, linewidth=1,\n",
    "                               edgecolor='#CCC', facecolor='#F5F5F5',\n",
    "                               linestyle='--', alpha=0.5)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(5, y_pos - remaining_height/2, f\"Available: {remaining} tokens\",\n",
    "                ha='center', va='center', fontsize=9, color='#AAA')\n",
    "\n",
    "# Annotations\n",
    "ax.text(5, -0.3, \"Each turn, the Context Assembler fills this window optimally\",\n",
    "        ha='center', fontsize=10, style='italic', color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_60_interactive_chat_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Interactive Chat Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_60_interactive_chat_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Live Demo â€” Interactive Chat Mode\n",
    "\n",
    "Let's run one more conversation to show the chatbot's full capabilities. This time we will add a few more turns and watch the memory grow."
   ],
   "id": "cell_57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_61_interactive_chat_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Narration: Interactive Chat Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_61_interactive_chat_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional conversation turns\n",
    "additional_turns = [\n",
    "    \"I'm a beginner with Python but I know some math. Where should I start?\",\n",
    "    \"That sounds great! How long will each notebook take?\",\n",
    "    \"Can you remind me what my name is?\"\n",
    "]\n",
    "\n",
    "print(\"â•”\" + \"â•\" * 68 + \"â•—\")\n",
    "print(\"â•‘  EXTENDED DEMO: More Turns to Show Memory Persistence           â•‘\")\n",
    "print(\"â•š\" + \"â•\" * 68 + \"â•\")\n",
    "\n",
    "for user_msg in additional_turns:\n",
    "    print(f\"\\nðŸ‘¤ User: {user_msg}\")\n",
    "    response = chatbot.chat(user_msg)\n",
    "    print(f\"\\nðŸ¤– Chatbot: {response}\")\n",
    "    chatbot.visualize_turn(-1)\n",
    "    print()"
   ],
   "id": "cell_58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_62_final_viz_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Final Viz Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_62_final_viz_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Visualization: Complete Conversation Flow\n",
    "\n",
    "This diagram shows the full conversation arc â€” every turn, every component that was activated, and how memory accumulated over the entire interaction."
   ],
   "id": "cell_59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_63_final_viz_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Final Viz Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_63_final_viz_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline visualization of the full conversation\n",
    "n_turns = len(chatbot.turn_metadata)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, max(8, n_turns * 1.2)))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, n_turns + 1)\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Complete Conversation Flow â€” Every Component in Action\",\n",
    "             fontsize=15, fontweight='bold', pad=20)\n",
    "\n",
    "for i, info in enumerate(chatbot.turn_metadata):\n",
    "    y = n_turns - i  # Top to bottom\n",
    "\n",
    "    # Turn number\n",
    "    ax.text(0.3, y, f\"T{info['turn']}\", fontsize=12, fontweight='bold',\n",
    "            ha='center', va='center',\n",
    "            bbox=dict(boxstyle='circle', facecolor='#1976D2', edgecolor='white'),\n",
    "            color='white')\n",
    "\n",
    "    # User message (truncated)\n",
    "    msg = info['user_message'][:50]\n",
    "    if len(info['user_message']) > 50:\n",
    "        msg += \"...\"\n",
    "    ax.text(1.0, y + 0.15, f\"User: {msg}\", fontsize=8.5, va='center',\n",
    "            color='#333', fontweight='bold')\n",
    "\n",
    "    # Component indicators\n",
    "    x_start = 1.0\n",
    "    indicators = []\n",
    "\n",
    "    # Memory updates\n",
    "    if info.get('memory_updates'):\n",
    "        indicators.append((\"MEM\", '#9C27B0'))\n",
    "\n",
    "    # Tools\n",
    "    if info.get('tools_called'):\n",
    "        tool_name = info['tools_called'][0]['name']\n",
    "        indicators.append((f\"TOOL:{tool_name}\", '#D32F2F'))\n",
    "\n",
    "    # RAG\n",
    "    docs = info.get('docs_retrieved', [])\n",
    "    high_score_docs = [d for d in docs if d['score'] > 0.3]\n",
    "    if high_score_docs:\n",
    "        indicators.append((f\"RAG({len(high_score_docs)})\", '#388E3C'))\n",
    "\n",
    "    # Token usage\n",
    "    total_tokens = info.get('token_usage', {}).get('total_tokens', 0)\n",
    "    indicators.append((f\"{total_tokens}tok\", '#F57C00'))\n",
    "\n",
    "    indicator_text = \" | \".join(f\"{label}\" for label, _ in indicators)\n",
    "    indicator_colors = [c for _, c in indicators]\n",
    "\n",
    "    x_pos = 1.0\n",
    "    for label, color in indicators:\n",
    "        ax.text(x_pos, y - 0.2, label, fontsize=7, va='center',\n",
    "                color='white', fontweight='bold',\n",
    "                bbox=dict(boxstyle='round,pad=0.2', facecolor=color,\n",
    "                         edgecolor='none', alpha=0.8))\n",
    "        x_pos += len(label) * 0.11 + 0.4\n",
    "\n",
    "    # Connecting line\n",
    "    if i < n_turns - 1:\n",
    "        ax.plot([0.3, 0.3], [y - 0.4, y - 0.6], '-', color='#1976D2',\n",
    "                linewidth=2, alpha=0.3)\n",
    "\n",
    "# Legend at the bottom\n",
    "legend_items = [\n",
    "    (\"MEM = Memory updated\", '#9C27B0'),\n",
    "    (\"TOOL = Tool called\", '#D32F2F'),\n",
    "    (\"RAG = Docs retrieved\", '#388E3C'),\n",
    "    (\"tok = Tokens used\", '#F57C00'),\n",
    "]\n",
    "for j, (label, color) in enumerate(legend_items):\n",
    "    ax.text(0.5 + j * 2.5, -0.3, label, fontsize=8, va='center',\n",
    "            color=color, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_64_reflection",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Reflection\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_64_reflection.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Reflection\n",
    "\n",
    "### What You Built\n",
    "\n",
    "Let's take a step back and appreciate what just happened. You built a complete, working chatbot that uses **every** technique from this course:\n",
    "\n",
    "| Notebook | Technique | Where It Shows Up |\n",
    "|----------|-----------|------------------|\n",
    "| **Notebook 1** | Token budget management | `ContextAssembler.assemble()` â€” explicit budget allocation per section |\n",
    "| **Notebook 2** | Failure mode detection | `ContextAssembler.health_check()` â€” stale, conflicting, missing, overflow |\n",
    "| **Notebook 3** | RAG retrieval | `VectorStore.search()` â€” cosine similarity over embedded chunks |\n",
    "| **Notebook 4** | Write strategy | Dynamic system prompt with user profile injected |\n",
    "| **Notebook 4** | Select strategy | `ConversationMemory.get_relevant_history()` â€” recent turns only |\n",
    "| **Notebook 4** | Compress strategy | `ContextAssembler.compress_text()` â€” extractive compression |\n",
    "| **Notebook 4** | Isolate strategy | Tool results in `[TOOL RESULTS]...[/TOOL RESULTS]` delimiters |\n",
    "\n",
    "This is context engineering in practice. The model did not change between turns â€” only the **context** changed. And that context is what makes the chatbot feel intelligent, personalized, and useful."
   ],
   "id": "cell_61"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_65_reflection_questions",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Reflection Questions\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_65_reflection_questions.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Questions\n",
    "\n",
    "1. **Why does the chatbot feel \"smart\" even though it has no training on Vizuara data?** Because context engineering puts the right information in the right place at the right time. The model's intelligence comes from the context we assemble, not from fine-tuning.\n",
    "\n",
    "2. **What would break if we removed the memory system?** The chatbot would forget the user's name by Turn 2. It would not know what \"prerequisites\" refers to (diffusion models). Every turn would be an isolated question with no continuity.\n",
    "\n",
    "3. **Why do we compress conversation history instead of keeping all of it?** Token budget is finite. A 50-turn conversation would consume the entire context window, leaving no room for RAG results or tool outputs. Compression and selection ensure we keep what matters.\n",
    "\n",
    "4. **How would you adapt this chatbot for a different website?** Change two things: (1) replace `VIZUARA_KNOWLEDGE_BASE` with content about your target site, and (2) update the system prompt. Everything else â€” the RAG pipeline, memory, tools, context assembler â€” works unchanged."
   ],
   "id": "cell_62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_66_challenges",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Challenges\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_66_challenges.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Challenges\n",
    "\n",
    "1. **Semantic memory retrieval**: Instead of returning the most recent conversation history, embed each past turn and retrieve the most *semantically relevant* ones for the current query.\n",
    "\n",
    "2. **Multi-hop tool chains**: Make the chatbot capable of calling multiple tools in sequence. For example: first call `faq_search` to check if the FAQ has an answer, and if not, call `course_lookup` as a fallback.\n",
    "\n",
    "3. **Context window visualization**: Build a real-time widget that shows the context window contents as the conversation progresses, updating after each turn.\n",
    "\n",
    "4. **Deploy to Gradio**: Wrap the `VizuaraChatbot` in a Gradio interface so anyone can interact with it through a web UI."
   ],
   "id": "cell_63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_67_closing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Wrap-Up: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_67_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final celebration\n",
    "print()\n",
    "print(\"â•”\" + \"â•\" * 68 + \"â•—\")\n",
    "print(\"â•‘                                                                    â•‘\")\n",
    "print(\"â•‘   ðŸŽ‰  Congratulations!                                             â•‘\")\n",
    "print(\"â•‘                                                                    â•‘\")\n",
    "print(\"â•‘   You have built a complete context engineering pipeline:          â•‘\")\n",
    "print(\"â•‘                                                                    â•‘\")\n",
    "print(\"â•‘     âœ… Knowledge Base with web scraping + curated content          â•‘\")\n",
    "print(\"â•‘     âœ… Vector Store with semantic embedding and retrieval          â•‘\")\n",
    "print(\"â•‘     âœ… Conversation Memory with fact extraction                    â•‘\")\n",
    "print(\"â•‘     âœ… Tool Registry with dynamic dispatch                        â•‘\")\n",
    "print(\"â•‘     âœ… Context Assembler using ALL four strategies                 â•‘\")\n",
    "print(\"â•‘     âœ… Working chatbot that feels genuinely intelligent            â•‘\")\n",
    "print(\"â•‘                                                                    â•‘\")\n",
    "print(\"â•‘   The model didn't change. Only the CONTEXT changed.              â•‘\")\n",
    "print(\"â•‘   That is the power of context engineering.                        â•‘\")\n",
    "print(\"â•‘                                                                    â•‘\")\n",
    "print(\"â•š\" + \"â•\" * 68 + \"â•\")\n",
    "print()\n",
    "print(\"  Series Complete: Context Engineering for LLMs\")\n",
    "print(\"  Notebooks 1-5 by Vizuara\")\n",
    "print()\n",
    "print(\"  Next step: swap vizuara.ai for YOUR website and\")\n",
    "print(\"  build a custom chatbot in under 10 minutes.\")\n",
    "print()"
   ],
   "id": "cell_64"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series Recap: Context Engineering for LLMs\n",
    "\n",
    "Congratulations on completing all 5 notebooks! Here is what we covered:\n",
    "\n",
    "| Notebook | Topic | Key Takeaway |\n",
    "|----------|-------|-------------|\n",
    "| **1** | Token Budget Management | The context window is finite â€” allocate tokens deliberately, not randomly |\n",
    "| **2** | Context Failure Modes | Stale, conflicting, missing, and overflowing context silently degrade quality |\n",
    "| **3** | RAG from Scratch | Embed, store, and retrieve â€” let the model see only what it needs |\n",
    "| **4** | The Four Strategies | Write, select, compress, isolate â€” four tools to shape optimal context |\n",
    "| **5** | Capstone Chatbot | All techniques compose into a system that feels like intelligence |\n",
    "\n",
    "The core lesson: **LLM performance is bounded by context quality.** A perfectly tuned model with bad context will always lose to a decent model with excellent context. Context engineering is the highest-leverage skill in applied AI."
   ],
   "id": "cell_65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final stats\n",
    "print(\"FINAL PROJECT STATS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  Components built:        6\")\n",
    "print(f\"  Knowledge base entries:  {len(VIZUARA_KNOWLEDGE_BASE)}\")\n",
    "print(f\"  Chunks indexed:          {len(kb.chunks)}\")\n",
    "print(f\"  Vector embeddings:       {vector_store.get_stats()['num_entries']}\")\n",
    "print(f\"  Tools registered:        {len(tools.tools)}\")\n",
    "print(f\"  Conversation turns:      {chatbot.memory.turn_count}\")\n",
    "print(f\"  User facts extracted:    {len(chatbot.memory.key_facts)}\")\n",
    "print(f\"  Profile fields:          {len(chatbot.memory.user_profile)}\")\n",
    "print()\n",
    "print(\"  User profile: \" + chatbot.memory.get_profile_summary())\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"  Thank you for learning with Vizuara!\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "cell_66"
  }
 ]
}