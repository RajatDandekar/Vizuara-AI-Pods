{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Training a Reasoning Model with GRPO -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Reasoning Model with GRPO -- Vizuara\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "DeepSeek-R1 demonstrated something remarkable: a language model can learn to reason step by step purely through reinforcement learning, without being explicitly taught chain-of-thought reasoning. The key ingredient? **GRPO with verifiable rewards**.\n",
    "\n",
    "In this notebook, we will replicate this idea at a small scale. We will train a tiny transformer model to solve simple arithmetic problems (addition and multiplication) using GRPO. The reward is binary -- 1 if the answer is correct, 0 if wrong. No reward model needed. No critic network needed.\n",
    "\n",
    "By the end, you will have a working GRPO training pipeline that you can scale up."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### The Simplest RL Setup\n",
    "\n",
    "Imagine teaching a child arithmetic. You give them a problem: \"What is 7 + 5?\"\n",
    "\n",
    "- If they answer \"12\" -- correct! Reward = 1.\n",
    "- If they answer \"11\" -- wrong. Reward = 0.\n",
    "\n",
    "Now imagine you give them the SAME problem 8 times, and they give different answers each time (because they are still learning and sometimes guess):\n",
    "\n",
    "| Attempt | Answer | Correct? | Reward |\n",
    "|---------|--------|----------|--------|\n",
    "| 1       | 12     | Yes      | 1      |\n",
    "| 2       | 11     | No       | 0      |\n",
    "| 3       | 12     | Yes      | 1      |\n",
    "| 4       | 13     | No       | 0      |\n",
    "| 5       | 12     | Yes      | 1      |\n",
    "| 6       | 10     | No       | 0      |\n",
    "| 7       | 12     | Yes      | 1      |\n",
    "| 8       | 11     | No       | 0      |\n",
    "\n",
    "GRPO normalizes these rewards: mean=0.5, std=0.5.\n",
    "- Correct answers get advantage = +1.0\n",
    "- Wrong answers get advantage = -1.0\n",
    "\n",
    "The model learns: \"increase the probability of generating '12', decrease everything else.\" This is exactly what we want."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the binary reward setup\n",
    "rewards = torch.tensor([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0])\n",
    "mean_r = rewards.mean()\n",
    "std_r = rewards.std()\n",
    "advantages = (rewards - mean_r) / std_r\n",
    "\n",
    "print(\"Binary rewards:\", rewards.numpy())\n",
    "print(f\"Mean: {mean_r:.1f}, Std: {std_r:.3f}\")\n",
    "print(f\"Advantages: {advantages.numpy().round(2)}\")\n",
    "print(f\"\\nCorrect answers: advantage = {advantages[0]:.2f}\")\n",
    "print(f\"Wrong answers:   advantage = {advantages[1]:.2f}\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### GRPO with Verifiable Rewards\n",
    "\n",
    "The full GRPO objective with binary verifiable rewards:\n",
    "\n",
    "$$\\mathcal{J}(\\theta) = \\mathbb{E}\\left[\\frac{1}{G}\\sum_{i=1}^G \\frac{1}{|o_i|}\\sum_{t=1}^{|o_i|} \\left(\\min(r_{i,t}\\hat{A}_i, \\text{clip}(r_{i,t}, 1-\\epsilon, 1+\\epsilon)\\hat{A}_i) - \\beta D_\\text{KL}\\right)\\right]$$\n",
    "\n",
    "where $\\hat{A}_i = \\frac{r_i - \\mu}{\\sigma}$ and $r_i \\in \\{0, 1\\}$ (correct or not).\n",
    "\n",
    "Let us plug in numbers for G=4 with rewards $\\{1, 0, 1, 0\\}$:\n",
    "- $\\mu = 0.5$, $\\sigma = 0.5$\n",
    "- Advantages: $\\{(1-0.5)/0.5, (0-0.5)/0.5, ...\\} = \\{+1, -1, +1, -1\\}$\n",
    "\n",
    "With ratio $r_{i,t} = 1.1$ for a correct response:\n",
    "- Unclipped: $1.1 \\times 1.0 = 1.1$\n",
    "- Clipped: $\\min(1.1, 1.2) \\times 1.0 = 1.1$\n",
    "- No clipping needed (ratio within bounds). The correct response gets reinforced."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the GRPO objective numerically for our simple case\n",
    "ratio = 1.1\n",
    "A = 1.0\n",
    "epsilon = 0.2\n",
    "\n",
    "unclipped = ratio * A\n",
    "clipped = min(max(ratio, 1 - epsilon), 1 + epsilon) * A\n",
    "objective = min(unclipped, clipped)\n",
    "\n",
    "print(f\"Ratio: {ratio}\")\n",
    "print(f\"Advantage: {A}\")\n",
    "print(f\"Unclipped: {unclipped}\")\n",
    "print(f\"Clipped:   {clipped}\")\n",
    "print(f\"Objective: {objective}\")\n",
    "print(f\"\\nThis positive objective reinforces the correct response.\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### Component 1: The Arithmetic Dataset"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArithmeticDataset:\n",
    "    \"\"\"\n",
    "    Generate simple arithmetic problems for GRPO training.\n",
    "    Each problem has a verifiable answer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_num=20, operations=['+']):\n",
    "        self.max_num = max_num\n",
    "        self.operations = operations\n",
    "\n",
    "    def generate_problem(self):\n",
    "        \"\"\"Generate a random arithmetic problem.\"\"\"\n",
    "        a = np.random.randint(1, self.max_num + 1)\n",
    "        b = np.random.randint(1, self.max_num + 1)\n",
    "        op = np.random.choice(self.operations)\n",
    "\n",
    "        if op == '+':\n",
    "            answer = a + b\n",
    "        elif op == '*':\n",
    "            answer = a * b\n",
    "        else:\n",
    "            answer = a + b\n",
    "\n",
    "        problem = f\"{a}{op}{b}=\"\n",
    "        return problem, str(answer)\n",
    "\n",
    "    def generate_batch(self, batch_size=16):\n",
    "        \"\"\"Generate a batch of problems.\"\"\"\n",
    "        problems = []\n",
    "        answers = []\n",
    "        for _ in range(batch_size):\n",
    "            p, a = self.generate_problem()\n",
    "            problems.append(p)\n",
    "            answers.append(a)\n",
    "        return problems, answers\n",
    "\n",
    "# Test the dataset\n",
    "dataset = ArithmeticDataset(max_num=15, operations=['+'])\n",
    "for _ in range(5):\n",
    "    problem, answer = dataset.generate_problem()\n",
    "    print(f\"  {problem}{answer}\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 2: Character-Level Tokenizer"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    \"\"\"Simple character-level tokenizer for arithmetic expressions.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.chars = list(\"0123456789+=*<>\")  # < = start, > = end\n",
    "        self.char_to_id = {c: i for i, c in enumerate(self.chars)}\n",
    "        self.id_to_char = {i: c for i, c in enumerate(self.chars)}\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.pad_id = self.vocab_size  # padding token\n",
    "        self.start_id = self.char_to_id['<']\n",
    "        self.end_id = self.char_to_id['>']\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to token IDs.\"\"\"\n",
    "        return [self.char_to_id.get(c, self.pad_id) for c in text]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"Decode token IDs to text.\"\"\"\n",
    "        return ''.join([self.id_to_char.get(i, '?') for i in ids if i in self.id_to_char])\n",
    "\n",
    "tokenizer = CharTokenizer()\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Encode '7+5=12': {tokenizer.encode('7+5=12')}\")\n",
    "print(f\"Decode back: {tokenizer.decode(tokenizer.encode('7+5=12'))}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 3: Tiny Transformer Model"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A small transformer for character-level arithmetic.\n",
    "    ~50K parameters -- trainable in minutes on a CPU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size=16, d_model=64, n_heads=4, n_layers=2, max_len=20):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=128,\n",
    "            dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass. x: (batch, seq_len)\"\"\"\n",
    "        B, T = x.shape\n",
    "        pos = torch.arange(T, device=x.device).unsqueeze(0)\n",
    "\n",
    "        # Causal mask\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(T).to(x.device)\n",
    "\n",
    "        h = self.embedding(x) + self.pos_embedding(pos)\n",
    "        h = self.transformer(h, mask=mask, is_causal=True)\n",
    "        logits = self.head(h)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, prompt_ids, max_new_tokens=5, temperature=1.0):\n",
    "        \"\"\"Generate tokens autoregressively.\"\"\"\n",
    "        generated = prompt_ids.clone()\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            if generated.shape[1] >= self.max_len:\n",
    "                break\n",
    "            logits = self.forward(generated)\n",
    "            next_logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "        return generated\n",
    "\n",
    "# Create model\n",
    "model = TinyTransformer(vocab_size=tokenizer.vocab_size + 1, d_model=64, n_heads=4, n_layers=2).to(device)\n",
    "ref_model = TinyTransformer(vocab_size=tokenizer.vocab_size + 1, d_model=64, n_heads=4, n_layers=2).to(device)\n",
    "ref_model.load_state_dict(model.state_dict())  # Reference = copy of initial model\n",
    "\n",
    "param_count = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {param_count:,}\")\n",
    "print(f\"Reference model: frozen copy\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 4: Verifiable Reward Function"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(problem: str, generated: str, correct_answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Binary reward: 1 if the generated answer matches the correct answer, 0 otherwise.\n",
    "    This is the 'verifiable reward' -- no reward model needed!\n",
    "    \"\"\"\n",
    "    # Extract just the digits after '='\n",
    "    generated_clean = ''.join(c for c in generated if c.isdigit())\n",
    "    correct_clean = ''.join(c for c in correct_answer if c.isdigit())\n",
    "\n",
    "    if generated_clean == correct_clean:\n",
    "        return 1.0\n",
    "    return 0.0\n",
    "\n",
    "# Test\n",
    "print(compute_reward(\"7+5=\", \"12\", \"12\"))  # Should be 1.0\n",
    "print(compute_reward(\"7+5=\", \"11\", \"12\"))  # Should be 0.0\n",
    "print(compute_reward(\"7+5=\", \"12>\", \"12\")) # Should be 1.0"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 5: GRPO Training Functions"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(model, input_ids, target_ids):\n",
    "    \"\"\"\n",
    "    Compute log probabilities of target tokens under the model.\n",
    "\n",
    "    Args:\n",
    "        model: The language model\n",
    "        input_ids: (batch, seq_len) input token IDs\n",
    "        target_ids: (batch, seq_len) target token IDs (shifted by 1)\n",
    "    Returns:\n",
    "        log_probs: (batch, seq_len-1) log probabilities of each target token\n",
    "    \"\"\"\n",
    "    logits = model(input_ids[:, :-1])  # (batch, seq_len-1, vocab)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # Gather log probs of the target tokens\n",
    "    target = target_ids[:, 1:]  # Shifted targets\n",
    "    token_log_probs = log_probs.gather(2, target.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    return token_log_probs\n",
    "\n",
    "\n",
    "def compute_grpo_advantages(rewards):\n",
    "    \"\"\"Group-relative advantage computation.\"\"\"\n",
    "    mean_r = rewards.mean()\n",
    "    std_r = rewards.std()\n",
    "    if std_r < 1e-8:\n",
    "        return torch.zeros_like(rewards)\n",
    "    return (rewards - mean_r) / std_r\n",
    "\n",
    "\n",
    "def grpo_loss(log_probs, old_log_probs, ref_log_probs, advantages, mask, epsilon=0.2, beta=0.04):\n",
    "    \"\"\"Compute the GRPO loss.\"\"\"\n",
    "    ratio = torch.exp(log_probs - old_log_probs)\n",
    "    adv = advantages.unsqueeze(1)\n",
    "\n",
    "    surr1 = ratio * adv\n",
    "    surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * adv\n",
    "    policy_obj = torch.min(surr1, surr2)\n",
    "\n",
    "    kl_ratio = torch.exp(ref_log_probs - log_probs)\n",
    "    kl = kl_ratio - torch.log(kl_ratio) - 1.0\n",
    "\n",
    "    per_token = policy_obj - beta * kl\n",
    "    per_response = (per_token * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "\n",
    "    return -per_response.mean()\n",
    "\n",
    "print(\"All GRPO training components ready!\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Implement the Full GRPO Training Step"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_training_step(model, ref_model, tokenizer, dataset, optimizer,\n",
    "                        G=8, max_new_tokens=4, epsilon=0.2, beta=0.04):\n",
    "    \"\"\"\n",
    "    TODO: Implement one complete GRPO training step.\n",
    "\n",
    "    Steps:\n",
    "    1. Generate a problem from the dataset\n",
    "    2. Encode the problem as prompt tokens\n",
    "    3. Generate G completions from the model\n",
    "    4. Compute rewards (binary: correct/incorrect)\n",
    "    5. Compute group-relative advantages\n",
    "    6. Compute log probs under current, old, and ref policies\n",
    "    7. Compute GRPO loss and backpropagate\n",
    "\n",
    "    Hints:\n",
    "    - Use model.generate() for sampling completions\n",
    "    - Use get_log_probs() for computing token-level log probs\n",
    "    - Use compute_grpo_advantages() for advantages\n",
    "    - Use grpo_loss() for the loss computation\n",
    "    - Remember to detach old_log_probs (they should not have gradients)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    # Step 1: Generate problem\n",
    "    problem, correct_answer = dataset.generate_problem()\n",
    "\n",
    "    # Step 2: Encode prompt\n",
    "    prompt_ids = torch.tensor([tokenizer.encode(problem)]).to(device)\n",
    "    prompt_len = prompt_ids.shape[1]\n",
    "\n",
    "    # Step 3: Generate G completions\n",
    "    completions = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(G):\n",
    "            comp = model.generate(prompt_ids, max_new_tokens=max_new_tokens, temperature=1.0)\n",
    "            completions.append(comp)\n",
    "\n",
    "    # Step 4: Compute rewards\n",
    "    rewards = []\n",
    "    for comp in completions:\n",
    "        generated_text = tokenizer.decode(comp[0, prompt_len:].tolist())\n",
    "        r = compute_reward(problem, generated_text, correct_answer)\n",
    "        rewards.append(r)\n",
    "    rewards = torch.tensor(rewards).to(device)\n",
    "\n",
    "    # Step 5: Compute advantages\n",
    "    advantages = compute_grpo_advantages(rewards)\n",
    "\n",
    "    # Skip if all rewards are the same (nothing to learn)\n",
    "    if advantages.abs().sum() < 1e-8:\n",
    "        return 0.0, rewards.mean().item(), 0.0\n",
    "\n",
    "    # Step 6: Compute log probs\n",
    "    # Pad completions to same length\n",
    "    max_len = max(c.shape[1] for c in completions)\n",
    "    padded = torch.zeros(G, max_len, dtype=torch.long).to(device)\n",
    "    mask = torch.zeros(G, max_len - 1).to(device)\n",
    "\n",
    "    for i, comp in enumerate(completions):\n",
    "        L = comp.shape[1]\n",
    "        padded[i, :L] = comp[0]\n",
    "        # Mask: only count completion tokens (after prompt)\n",
    "        mask[i, prompt_len:L-1] = 1.0\n",
    "\n",
    "    log_probs = get_log_probs(model, padded, padded)\n",
    "    with torch.no_grad():\n",
    "        old_log_probs = log_probs.detach()\n",
    "        ref_log_probs = get_log_probs(ref_model, padded, padded)\n",
    "\n",
    "    # Step 7: Compute loss and backprop\n",
    "    loss = grpo_loss(log_probs, old_log_probs, ref_log_probs, advantages, mask, epsilon, beta)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), rewards.mean().item(), rewards.sum().item() / G\n",
    "\n",
    "# Test the training step\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "loss, mean_reward, accuracy = grpo_training_step(model, ref_model, tokenizer, dataset, optimizer)\n",
    "print(f\"Loss: {loss:.4f}, Mean reward: {mean_reward:.3f}, Accuracy: {accuracy:.1%}\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Add Format Reward\n",
    "\n",
    "DeepSeek-R1 uses a format reward in addition to the correctness reward. Implement a combined reward."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_combined_reward(problem, generated, correct_answer, format_weight=0.1):\n",
    "    \"\"\"\n",
    "    TODO: Implement a combined reward that includes:\n",
    "    1. Correctness reward (binary: 0 or 1) -- weight = 1.0\n",
    "    2. Format reward: bonus for clean output (just digits, no garbage) -- weight = format_weight\n",
    "\n",
    "    Hints:\n",
    "    - Correctness: same as compute_reward()\n",
    "    - Format: check if generated contains ONLY digits\n",
    "    - Combined: correctness + format_weight * format_score\n",
    "    \"\"\"\n",
    "    # Correctness\n",
    "    correct = compute_reward(problem, generated, correct_answer)\n",
    "\n",
    "    # Format: does the output contain only digits?\n",
    "    clean = ''.join(c for c in generated if c.isdigit() or c == '>')\n",
    "    format_score = 1.0 if len(clean) == len(generated) and len(generated) > 0 else 0.0\n",
    "\n",
    "    return correct + format_weight * format_score\n",
    "\n",
    "# Test\n",
    "print(compute_combined_reward(\"7+5=\", \"12\", \"12\"))     # Correct + clean\n",
    "print(compute_combined_reward(\"7+5=\", \"12???\", \"12\"))   # Correct but messy\n",
    "print(compute_combined_reward(\"7+5=\", \"11\", \"12\"))      # Wrong but clean"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full GRPO training loop\n",
    "def train_grpo(model, ref_model, tokenizer, dataset, n_steps=500, G=8, log_every=50):\n",
    "    \"\"\"Train the model using GRPO.\"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "    history = defaultdict(list)\n",
    "\n",
    "    print(\"Starting GRPO training...\")\n",
    "    print(f\"Group size G={G}, Steps={n_steps}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        loss, mean_reward, accuracy = grpo_training_step(\n",
    "            model, ref_model, tokenizer, dataset, optimizer, G=G\n",
    "        )\n",
    "\n",
    "        history['loss'].append(loss)\n",
    "        history['reward'].append(mean_reward)\n",
    "        history['accuracy'].append(accuracy)\n",
    "\n",
    "        if (step + 1) % log_every == 0:\n",
    "            avg_loss = np.mean(history['loss'][-log_every:])\n",
    "            avg_reward = np.mean(history['reward'][-log_every:])\n",
    "            avg_acc = np.mean(history['accuracy'][-log_every:])\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            print(f\"Step {step+1:4d} | Loss: {avg_loss:7.4f} | \"\n",
    "                  f\"Reward: {avg_reward:.3f} | Acc: {avg_acc:.1%} | \"\n",
    "                  f\"Time: {elapsed:.0f}s\")\n",
    "\n",
    "    return history"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset model\n",
    "torch.manual_seed(42)\n",
    "model = TinyTransformer(vocab_size=tokenizer.vocab_size + 1, d_model=64, n_heads=4, n_layers=2).to(device)\n",
    "ref_model = TinyTransformer(vocab_size=tokenizer.vocab_size + 1, d_model=64, n_heads=4, n_layers=2).to(device)\n",
    "ref_model.load_state_dict(model.state_dict())\n",
    "\n",
    "# Train\n",
    "dataset = ArithmeticDataset(max_num=10, operations=['+'])\n",
    "history = train_grpo(model, ref_model, tokenizer, dataset, n_steps=500, G=8, log_every=50)"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "window = 30\n",
    "\n",
    "# Loss\n",
    "loss_smooth = np.convolve(history['loss'], np.ones(window)/window, mode='valid')\n",
    "axes[0].plot(loss_smooth, linewidth=2, color='red')\n",
    "axes[0].set_title(\"Training Loss\", fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel(\"Step\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Reward\n",
    "reward_smooth = np.convolve(history['reward'], np.ones(window)/window, mode='valid')\n",
    "axes[1].plot(reward_smooth, linewidth=2, color='blue')\n",
    "axes[1].set_title(\"Mean Reward\", fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel(\"Step\")\n",
    "axes[1].set_ylabel(\"Reward\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "acc_smooth = np.convolve(history['accuracy'], np.ones(window)/window, mode='valid')\n",
    "axes[2].plot(acc_smooth, linewidth=2, color='green')\n",
    "axes[2].set_title(\"Accuracy\", fontsize=13, fontweight='bold')\n",
    "axes[2].set_xlabel(\"Step\")\n",
    "axes[2].set_ylabel(\"Accuracy\")\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"GRPO Training Results\", fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"grpo_training_results.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "print(\"=\" * 50)\n",
    "print(\"Evaluation: Testing the GRPO-trained model\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "correct = 0\n",
    "total = 20\n",
    "\n",
    "for _ in range(total):\n",
    "    problem, answer = dataset.generate_problem()\n",
    "    prompt_ids = torch.tensor([tokenizer.encode(problem)]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(prompt_ids, max_new_tokens=4, temperature=0.1)\n",
    "\n",
    "    generated = tokenizer.decode(output[0, prompt_ids.shape[1]:].tolist())\n",
    "    is_correct = compute_reward(problem, generated, answer)\n",
    "    correct += is_correct\n",
    "\n",
    "    status = \"CORRECT\" if is_correct else \"WRONG\"\n",
    "    print(f\"  {problem}{answer}  |  Model: {generated:<6}  [{status}]\")\n",
    "\n",
    "print(f\"\\nAccuracy: {correct}/{total} = {correct/total:.0%}\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"GRPO Training Pipeline -- Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"What we built:\")\n",
    "print(\"  1. Character-level tokenizer for arithmetic\")\n",
    "print(\"  2. Tiny transformer model (~50K params)\")\n",
    "print(\"  3. Binary verifiable reward function\")\n",
    "print(\"  4. Full GRPO training loop with:\")\n",
    "print(\"     - Group sampling (G completions per prompt)\")\n",
    "print(\"     - Group-relative advantage normalization\")\n",
    "print(\"     - Clipped surrogate objective\")\n",
    "print(\"     - KL divergence penalty\")\n",
    "print()\n",
    "print(\"This is the SAME approach DeepSeek used for R1,\")\n",
    "print(\"just at a much smaller scale.\")\n",
    "print()\n",
    "print(\"To scale up:\")\n",
    "print(\"  - Replace TinyTransformer with a real LLM\")\n",
    "print(\"  - Use harder math problems\")\n",
    "print(\"  - Increase G to 32-64\")\n",
    "print(\"  - Train for thousands of steps on GPUs\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**Key takeaways from this notebook:**\n",
    "\n",
    "1. GRPO with verifiable rewards requires NO reward model and NO critic -- just a function that checks correctness.\n",
    "2. Binary rewards (correct/incorrect) produce clean +1/-1 advantages after group normalization.\n",
    "3. Even a tiny transformer can learn simple arithmetic through GRPO.\n",
    "4. The same pipeline scales to state-of-the-art reasoning models (DeepSeek-R1).\n",
    "\n",
    "**Reflection questions:**\n",
    "- How would you modify this pipeline to train on multi-step reasoning problems (where intermediate steps matter)?\n",
    "- What happens if the model's accuracy is very low (e.g., 1 out of 8 correct)? Does GRPO still learn?\n",
    "- What if accuracy is very high (e.g., 7 out of 8 correct)? Is there still a learning signal?\n",
    "- How would you add a \"format reward\" to encourage the model to show its work?\n",
    "\n",
    "**Further reading:**\n",
    "- [DeepSeek-R1 paper](https://arxiv.org/abs/2501.12948)\n",
    "- [DeepSeekMath paper (GRPO)](https://arxiv.org/abs/2402.03300)"
   ],
   "id": "cell_29"
  }
 ]
}