{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Implementing the GRPO Loss Function -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the GRPO Loss Function -- Vizuara\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebook, we learned how GRPO computes advantages by normalizing rewards within a group. But advantages alone do not train a model -- we need a **loss function** that uses those advantages to update the policy.\n",
    "\n",
    "The GRPO loss has three key components:\n",
    "1. **Clipped surrogate objective** (same idea as PPO -- prevent overly large updates)\n",
    "2. **Group-relative advantages** (what we built in Notebook 1)\n",
    "3. **KL divergence penalty** (prevent the policy from drifting too far from a reference)\n",
    "\n",
    "In this notebook, you will build each component from scratch, verify them with numerical examples, and combine them into the complete GRPO training objective."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Ready to build the GRPO loss!\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### The Three Guards of GRPO\n",
    "\n",
    "Think of training a language model like steering a ship:\n",
    "\n",
    "1. **The Compass (Advantages):** Points toward better responses. GRPO's compass is the group-relative advantage -- it tells us which responses in the batch were better than average.\n",
    "\n",
    "2. **The Anchor (Clipping):** Prevents the ship from turning too sharply. Even if a response looks amazing, we clip the update to prevent destabilizing the model.\n",
    "\n",
    "3. **The Tether (KL Penalty):** Keeps us from drifting too far from port. The KL divergence penalty ensures the policy stays close to a reference model, preventing reward hacking.\n",
    "\n",
    "Let us build each guard one at a time."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the three components conceptually\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 1. Advantages\n",
    "x = np.linspace(-3, 3, 100)\n",
    "axes[0].fill_between(x[x > 0], 0, x[x > 0], alpha=0.3, color='green', label='Reinforce')\n",
    "axes[0].fill_between(x[x < 0], x[x < 0], 0, alpha=0.3, color='red', label='Penalize')\n",
    "axes[0].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[0].set_title(\"1. Group-Relative Advantages\", fontweight='bold')\n",
    "axes[0].set_xlabel(\"Advantage Value\")\n",
    "axes[0].legend()\n",
    "\n",
    "# 2. Clipping\n",
    "ratio = np.linspace(0.5, 2.0, 100)\n",
    "clipped = np.clip(ratio, 0.8, 1.2)\n",
    "axes[1].plot(ratio, ratio, 'b--', alpha=0.5, label='Unclipped ratio')\n",
    "axes[1].plot(ratio, clipped, 'r-', linewidth=2, label='Clipped ratio')\n",
    "axes[1].axvline(x=0.8, color='gray', linestyle=':', alpha=0.5)\n",
    "axes[1].axvline(x=1.2, color='gray', linestyle=':', alpha=0.5)\n",
    "axes[1].set_title(\"2. Ratio Clipping (epsilon=0.2)\", fontweight='bold')\n",
    "axes[1].set_xlabel(\"Policy Ratio\")\n",
    "axes[1].legend()\n",
    "\n",
    "# 3. KL Penalty\n",
    "r = np.linspace(0.1, 5, 100)\n",
    "kl = r - np.log(r) - 1\n",
    "axes[2].plot(r, kl, 'purple', linewidth=2)\n",
    "axes[2].axvline(x=1.0, color='green', linestyle='--', alpha=0.5, label='No drift (KL=0)')\n",
    "axes[2].set_title(\"3. KL Divergence Penalty\", fontweight='bold')\n",
    "axes[2].set_xlabel(\"pi_ref / pi_theta\")\n",
    "axes[2].set_ylabel(\"KL value\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"grpo_three_components.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Component 1: The Importance Sampling Ratio\n",
    "\n",
    "For each token at position $t$ in response $i$:\n",
    "\n",
    "$$r_{i,t}(\\theta) = \\frac{\\pi_\\theta(o_{i,t} \\mid q, o_{i,<t})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t} \\mid q, o_{i,<t})}$$\n",
    "\n",
    "This ratio tells us how much more (or less) likely the current policy is to produce this token compared to the old policy.\n",
    "\n",
    "Let us work through a numerical example. Suppose:\n",
    "- Current policy probability: $\\pi_\\theta = 0.3$\n",
    "- Old policy probability: $\\pi_{\\theta_\\text{old}} = 0.2$\n",
    "\n",
    "$$r_{i,t} = \\frac{0.3}{0.2} = 1.5$$\n",
    "\n",
    "The new policy is 50% more likely to produce this token.\n",
    "\n",
    "In log space (which we use in practice for numerical stability):\n",
    "\n",
    "$$\\log r_{i,t} = \\log \\pi_\\theta - \\log \\pi_{\\theta_\\text{old}} = \\log(0.3) - \\log(0.2) = -1.204 - (-1.609) = 0.405$$\n",
    "\n",
    "$$r_{i,t} = e^{0.405} = 1.5$$\n",
    "\n",
    "This is exactly what we want.\n",
    "\n",
    "### Component 2: The Clipped Surrogate\n",
    "\n",
    "$$L_{\\text{clip}} = \\min\\left(r_{i,t} \\cdot \\hat{A}_i, \\; \\text{clip}(r_{i,t}, 1-\\epsilon, 1+\\epsilon) \\cdot \\hat{A}_i\\right)$$\n",
    "\n",
    "With $\\epsilon = 0.2$, the ratio is clipped to $[0.8, 1.2]$.\n",
    "\n",
    "Numerical example with $r_{i,t} = 1.5$, $\\hat{A}_i = 2.0$:\n",
    "- Unclipped: $1.5 \\times 2.0 = 3.0$\n",
    "- Clipped ratio: $\\text{clip}(1.5, 0.8, 1.2) = 1.2$\n",
    "- Clipped: $1.2 \\times 2.0 = 2.4$\n",
    "- $\\min(3.0, 2.4) = 2.4$ (clipping reduces the update)\n",
    "\n",
    "### Component 3: The KL Penalty\n",
    "\n",
    "$$D_{\\text{KL}} = \\frac{\\pi_{\\text{ref}}}{\\pi_\\theta} - \\log\\frac{\\pi_{\\text{ref}}}{\\pi_\\theta} - 1$$\n",
    "\n",
    "Verification when $\\pi_\\theta = \\pi_{\\text{ref}}$:\n",
    "$$D_{\\text{KL}} = 1 - \\log(1) - 1 = 0$$\n",
    "\n",
    "When $\\frac{\\pi_{\\text{ref}}}{\\pi_\\theta} = 2$ (policy has drifted):\n",
    "$$D_{\\text{KL}} = 2 - \\log(2) - 1 = 2 - 0.693 - 1 = 0.307$$\n",
    "\n",
    "The penalty grows as the policy drifts further from the reference."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the KL divergence formula numerically\n",
    "def kl_divergence_approx(ref_prob, theta_prob):\n",
    "    \"\"\"KL approximation used in GRPO.\"\"\"\n",
    "    ratio = ref_prob / theta_prob\n",
    "    return ratio - torch.log(ratio) - 1.0\n",
    "\n",
    "# Case 1: No drift\n",
    "kl_no_drift = kl_divergence_approx(torch.tensor(0.3), torch.tensor(0.3))\n",
    "print(f\"KL when pi_ref = pi_theta: {kl_no_drift.item():.6f} (should be ~0)\")\n",
    "\n",
    "# Case 2: Small drift\n",
    "kl_small = kl_divergence_approx(torch.tensor(0.3), torch.tensor(0.25))\n",
    "print(f\"KL with small drift:       {kl_small.item():.6f}\")\n",
    "\n",
    "# Case 3: Large drift\n",
    "kl_large = kl_divergence_approx(torch.tensor(0.3), torch.tensor(0.1))\n",
    "print(f\"KL with large drift:       {kl_large.item():.6f}\")\n",
    "\n",
    "print(\"\\nAs expected, KL increases as the policy drifts further from reference.\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### Building the Complete GRPO Loss"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grpo_advantages(rewards: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute group-relative advantages (from Notebook 1).\"\"\"\n",
    "    mean_r = rewards.mean()\n",
    "    std_r = rewards.std()\n",
    "    if std_r < 1e-8:\n",
    "        return torch.zeros_like(rewards)\n",
    "    return (rewards - mean_r) / std_r\n",
    "\n",
    "\n",
    "def compute_grpo_loss(\n",
    "    log_probs: torch.Tensor,       # (G, T) log probs under current policy\n",
    "    old_log_probs: torch.Tensor,   # (G, T) log probs under old policy\n",
    "    ref_log_probs: torch.Tensor,   # (G, T) log probs under reference policy\n",
    "    advantages: torch.Tensor,       # (G,) per-response advantages\n",
    "    mask: torch.Tensor,             # (G, T) attention mask (1 for valid, 0 for padding)\n",
    "    epsilon: float = 0.2,\n",
    "    beta: float = 0.04,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute the full GRPO loss with all three components.\n",
    "\n",
    "    Returns a dict with the total loss and individual components for logging.\n",
    "    \"\"\"\n",
    "    # --- Component 1: Importance sampling ratio ---\n",
    "    ratio = torch.exp(log_probs - old_log_probs)  # (G, T)\n",
    "\n",
    "    # --- Component 2: Clipped surrogate ---\n",
    "    adv = advantages.unsqueeze(1)  # (G, 1) -> broadcasts to (G, T)\n",
    "    surr1 = ratio * adv\n",
    "    surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * adv\n",
    "    policy_loss = torch.min(surr1, surr2)  # (G, T)\n",
    "\n",
    "    # --- Component 3: KL divergence penalty ---\n",
    "    kl_ratio = torch.exp(ref_log_probs - log_probs)  # pi_ref / pi_theta\n",
    "    kl_div = kl_ratio - torch.log(kl_ratio) - 1.0    # (G, T)\n",
    "\n",
    "    # --- Combine ---\n",
    "    per_token_objective = policy_loss - beta * kl_div  # (G, T)\n",
    "\n",
    "    # Average over valid tokens, then over group\n",
    "    per_response = (per_token_objective * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "    loss = -per_response.mean()  # Negate for minimization\n",
    "\n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'policy_loss': -(policy_loss * mask).sum() / mask.sum(),\n",
    "        'kl_div': (kl_div * mask).sum() / mask.sum(),\n",
    "        'mean_ratio': (ratio * mask).sum() / mask.sum(),\n",
    "    }\n",
    "\n",
    "print(\"GRPO loss function defined!\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with Synthetic Data"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data to test the loss function\n",
    "G = 4   # Group size\n",
    "T = 10  # Sequence length\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Simulate log probabilities\n",
    "# Current policy, old policy, and reference policy\n",
    "base_log_probs = torch.randn(G, T) * 0.5 - 2.0  # Base log probs\n",
    "old_log_probs = base_log_probs.detach().clone()   # Old = current initially\n",
    "ref_log_probs = base_log_probs + torch.randn(G, T) * 0.1  # Ref slightly different\n",
    "\n",
    "# Simulate a small policy update\n",
    "log_probs = old_log_probs + torch.randn(G, T) * 0.05\n",
    "log_probs.requires_grad_(True)\n",
    "\n",
    "# Rewards and advantages\n",
    "rewards = torch.tensor([0.8, 0.3, 0.9, 0.1])\n",
    "advantages = compute_grpo_advantages(rewards)\n",
    "\n",
    "# Mask (all tokens valid in this example)\n",
    "mask = torch.ones(G, T)\n",
    "\n",
    "# Compute loss\n",
    "result = compute_grpo_loss(log_probs, old_log_probs, ref_log_probs, advantages, mask)\n",
    "\n",
    "print(\"=== GRPO Loss Components ===\")\n",
    "print(f\"Total loss:    {result['loss'].item():.4f}\")\n",
    "print(f\"Policy loss:   {result['policy_loss'].item():.4f}\")\n",
    "print(f\"KL divergence: {result['kl_div'].item():.4f}\")\n",
    "print(f\"Mean ratio:    {result['mean_ratio'].item():.4f}\")\n",
    "print(f\"\\nAdvantages:    {advantages.numpy().round(3)}\")\n",
    "print(f\"Rewards:       {rewards.numpy()}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify gradients flow correctly\n",
    "result['loss'].backward()\n",
    "print(\"Gradient norm:\", log_probs.grad.norm().item())\n",
    "print(\"Gradients are flowing correctly!\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Visualize the Effect of Epsilon on Clipping"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clipping_effect(advantages, epsilon_values=[0.1, 0.2, 0.3, 0.5]):\n",
    "    \"\"\"\n",
    "    TODO: For each epsilon value, compute the clipped surrogate loss\n",
    "    across a range of ratios (0.5 to 2.0) and plot the results.\n",
    "\n",
    "    For a POSITIVE advantage:\n",
    "    - Unclipped: ratio * A (linear, increasing)\n",
    "    - Clipped: clip(ratio, 1-eps, 1+eps) * A (flat outside bounds)\n",
    "    - Surrogate: min(unclipped, clipped)\n",
    "\n",
    "    For a NEGATIVE advantage:\n",
    "    - The clipping works differently (think about why!)\n",
    "\n",
    "    Hints:\n",
    "    1. Create ratio values: torch.linspace(0.5, 2.0, 100)\n",
    "    2. For positive A, surr = min(ratio*A, clip(ratio)*A)\n",
    "    3. Plot all epsilon values on the same axes\n",
    "    \"\"\"\n",
    "    ratios = torch.linspace(0.5, 2.0, 200)\n",
    "    A_pos = 1.0  # Positive advantage\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    for eps in epsilon_values:\n",
    "        clipped_ratios = torch.clamp(ratios, 1 - eps, 1 + eps)\n",
    "\n",
    "        # Positive advantage\n",
    "        surr1 = ratios * A_pos\n",
    "        surr2 = clipped_ratios * A_pos\n",
    "        surr_pos = torch.min(surr1, surr2)\n",
    "\n",
    "        # Negative advantage\n",
    "        A_neg = -1.0\n",
    "        surr1_neg = ratios * A_neg\n",
    "        surr2_neg = clipped_ratios * A_neg\n",
    "        surr_neg = torch.min(surr1_neg, surr2_neg)\n",
    "\n",
    "        axes[0].plot(ratios.numpy(), surr_pos.numpy(), label=f'eps={eps}', linewidth=2)\n",
    "        axes[1].plot(ratios.numpy(), surr_neg.numpy(), label=f'eps={eps}', linewidth=2)\n",
    "\n",
    "    axes[0].set_title(\"Positive Advantage (A=1.0)\", fontsize=13, fontweight='bold')\n",
    "    axes[0].set_xlabel(\"Policy Ratio\")\n",
    "    axes[0].set_ylabel(\"Surrogate Objective\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1].set_title(\"Negative Advantage (A=-1.0)\", fontsize=13, fontweight='bold')\n",
    "    axes[1].set_xlabel(\"Policy Ratio\")\n",
    "    axes[1].set_ylabel(\"Surrogate Objective\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle(\"Effect of Epsilon on Clipped Surrogate\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"clipping_effect.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "visualize_clipping_effect(None)"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement the KL-Annealing Schedule"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_annealing_schedule(step: int, total_steps: int, beta_init: float = 0.001, beta_final: float = 0.1):\n",
    "    \"\"\"\n",
    "    TODO: Implement a KL annealing schedule that linearly increases\n",
    "    beta from beta_init to beta_final over the course of training.\n",
    "\n",
    "    Early in training: low beta = allow exploration\n",
    "    Late in training: high beta = stay close to reference\n",
    "\n",
    "    Args:\n",
    "        step: Current training step\n",
    "        total_steps: Total number of training steps\n",
    "        beta_init: Initial beta value\n",
    "        beta_final: Final beta value\n",
    "    Returns:\n",
    "        beta: Current beta value\n",
    "\n",
    "    Hint: Linear interpolation: beta = beta_init + (beta_final - beta_init) * (step / total_steps)\n",
    "    \"\"\"\n",
    "    progress = min(step / total_steps, 1.0)\n",
    "    beta = beta_init + (beta_final - beta_init) * progress\n",
    "    return beta\n",
    "\n",
    "# Verify the schedule\n",
    "steps = list(range(1000))\n",
    "betas = [kl_annealing_schedule(s, 1000) for s in steps]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(steps, betas, linewidth=2, color='purple')\n",
    "plt.xlabel(\"Training Step\", fontsize=12)\n",
    "plt.ylabel(\"Beta (KL coefficient)\", fontsize=12)\n",
    "plt.title(\"KL Annealing Schedule\", fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(\"kl_annealing.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Beta at step 0:   {betas[0]:.4f}\")\n",
    "print(f\"Beta at step 500: {betas[500]:.4f}\")\n",
    "print(f\"Beta at step 999: {betas[999]:.4f}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the full GRPO loss on varying scenarios\n",
    "scenarios = {\n",
    "    \"All good responses\": torch.tensor([0.8, 0.9, 0.85, 0.95]),\n",
    "    \"Mixed responses\": torch.tensor([0.1, 0.9, 0.3, 0.7]),\n",
    "    \"All bad responses\": torch.tensor([0.1, 0.15, 0.05, 0.2]),\n",
    "    \"One outlier\": torch.tensor([0.2, 0.2, 0.2, 0.95]),\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Scenario':<25} {'Advantages':<35} {'Loss':<10}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, rewards in scenarios.items():\n",
    "    advantages = compute_grpo_advantages(rewards)\n",
    "\n",
    "    # Create synthetic log probs\n",
    "    G, T = len(rewards), 10\n",
    "    log_probs = torch.randn(G, T) * 0.5 - 2.0\n",
    "    log_probs.requires_grad_(True)\n",
    "    old_log_probs = log_probs.detach()\n",
    "    ref_log_probs = log_probs.detach() + torch.randn(G, T) * 0.1\n",
    "    mask = torch.ones(G, T)\n",
    "\n",
    "    result = compute_grpo_loss(log_probs, old_log_probs, ref_log_probs, advantages, mask)\n",
    "    adv_str = \", \".join([f\"{a:.2f}\" for a in advantages.tolist()])\n",
    "    print(f\"{name:<25} [{adv_str}]  {result['loss'].item():<10.4f}\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate how the GRPO loss guides training\n",
    "# We will optimize a simple parameter to demonstrate\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Goal: learn theta such that reward is maximized\n",
    "theta = torch.tensor([0.0], requires_grad=True)\n",
    "optimizer = torch.optim.Adam([theta], lr=0.01)\n",
    "\n",
    "loss_history = []\n",
    "theta_history = []\n",
    "kl_history = []\n",
    "\n",
    "for step in range(300):\n",
    "    # Simulate G=8 responses with rewards depending on theta\n",
    "    noise = torch.randn(8)\n",
    "    rewards = -torch.abs(theta.item() + noise - 3.0)  # Optimal at theta=3\n",
    "\n",
    "    advantages = compute_grpo_advantages(rewards)\n",
    "\n",
    "    # Simulate policy update direction based on advantages\n",
    "    weighted = (advantages * noise).mean()\n",
    "    loss = -weighted\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_history.append(loss.item())\n",
    "    theta_history.append(theta.item())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].plot(theta_history, linewidth=2, color='blue')\n",
    "axes[0].axhline(y=3.0, color='green', linestyle='--', label='Optimal')\n",
    "axes[0].set_xlabel(\"Step\")\n",
    "axes[0].set_ylabel(\"Theta\")\n",
    "axes[0].set_title(\"Parameter Convergence\", fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(loss_history, linewidth=1, color='red', alpha=0.5)\n",
    "# Smoothed\n",
    "window = 20\n",
    "smoothed = np.convolve(loss_history, np.ones(window)/window, mode='valid')\n",
    "axes[1].plot(range(window-1, len(loss_history)), smoothed, linewidth=2, color='red')\n",
    "axes[1].set_xlabel(\"Step\")\n",
    "axes[1].set_ylabel(\"Loss\")\n",
    "axes[1].set_title(\"Training Loss\", fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"grpo_loss_training.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final theta: {theta_history[-1]:.3f} (target: 3.000)\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"GRPO Loss Function Summary\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"The GRPO objective combines three components:\")\n",
    "print()\n",
    "print(\"1. CLIPPED SURROGATE: min(ratio * A, clip(ratio) * A)\")\n",
    "print(\"   - Prevents overly large policy updates\")\n",
    "print(\"   - Epsilon = 0.2 is standard\")\n",
    "print()\n",
    "print(\"2. GROUP-RELATIVE ADVANTAGES: A_i = (r_i - mean) / std\")\n",
    "print(\"   - No critic needed\")\n",
    "print(\"   - Computed per-response, not per-token\")\n",
    "print()\n",
    "print(\"3. KL PENALTY: beta * (pi_ref/pi_theta - log(pi_ref/pi_theta) - 1)\")\n",
    "print(\"   - Keeps policy close to reference\")\n",
    "print(\"   - beta = 0.04 is typical\")\n",
    "print()\n",
    "print(\"Full loss: -E[min(ratio*A, clip(ratio)*A) - beta*KL]\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**Key takeaways from this notebook:**\n",
    "\n",
    "1. The GRPO loss has three components: clipped surrogate, group-relative advantages, and KL penalty.\n",
    "2. The importance sampling ratio compares current and old policy probabilities per-token.\n",
    "3. Clipping prevents catastrophically large updates (same mechanism as PPO).\n",
    "4. The KL penalty is added directly to the loss (unlike PPO where it is a constraint).\n",
    "5. The advantage is per-response (not per-token), which is simpler but coarser.\n",
    "\n",
    "**Reflection questions:**\n",
    "- Why is the KL divergence computed per-token but the advantage is per-response? What are the implications?\n",
    "- If you increase beta, what happens to the policy's behavior? What about training speed?\n",
    "- How does the choice of epsilon interact with the group size G?\n",
    "\n",
    "**Next notebook:** We will bring everything together and train a small language model to solve math problems using GRPO with verifiable rewards -- the same approach DeepSeek used for R1."
   ],
   "id": "cell_22"
  }
 ]
}