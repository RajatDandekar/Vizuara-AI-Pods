{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Understanding GRPO: From PPO to Group-Relative Advantages -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding GRPO: From PPO to Group-Relative Advantages -- Vizuara\n",
    "\n",
    "## 1. Why Does This Matter?\n",
    "\n",
    "Have you ever wondered how models like DeepSeek-R1 learn to reason step by step? The answer involves reinforcement learning, but not the kind you might expect.\n",
    "\n",
    "Traditional RLHF (Reinforcement Learning from Human Feedback) uses PPO -- an algorithm that requires a **critic network** as large as the language model itself. For a 7B parameter model, this means keeping ~28B parameters in memory across four separate models. That is expensive.\n",
    "\n",
    "**Group-Relative Policy Optimization (GRPO)** eliminates the critic entirely by using a beautifully simple idea: instead of learning to predict how good a response is, just generate multiple responses and compare them to each other.\n",
    "\n",
    "In this notebook, you will:\n",
    "- Understand why PPO needs a critic and why that is expensive\n",
    "- Build the core intuition behind group-relative advantages\n",
    "- Implement advantage computation from scratch\n",
    "- Visualize how group normalization works compared to a learned critic\n",
    "\n",
    "Let us start by importing our tools."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "\n",
    "print(\"All imports successful!\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### The Teacher Grading Essays\n",
    "\n",
    "Imagine a teacher grading 30 essays. There are two approaches:\n",
    "\n",
    "**Approach 1 - Absolute scoring:** Read each essay and assign a score from 1-10. This requires a detailed rubric and is hard to be consistent.\n",
    "\n",
    "**Approach 2 - Relative scoring:** Read ALL essays, then rank them. \"This one is above average. This one is below average.\" No rubric needed -- just compare within the group.\n",
    "\n",
    "GRPO uses Approach 2. Let us see why this is powerful."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate absolute scoring (like a critic network)\n",
    "# The critic tries to predict exact value for each response\n",
    "np.random.seed(42)\n",
    "\n",
    "# True quality of 8 responses (unknown in practice)\n",
    "true_quality = np.array([3.2, 7.1, 2.0, 8.5, 5.0, 6.3, 1.5, 4.8])\n",
    "\n",
    "# Critic's predictions (noisy -- the critic is imperfect)\n",
    "critic_noise = np.random.normal(0, 2.0, size=8)\n",
    "critic_predictions = true_quality + critic_noise\n",
    "\n",
    "# PPO advantages: quality - critic_prediction\n",
    "ppo_advantages = true_quality - critic_predictions\n",
    "\n",
    "print(\"=== PPO (Critic-Based) Advantages ===\")\n",
    "print(f\"True quality:       {true_quality}\")\n",
    "print(f\"Critic predictions: {np.round(critic_predictions, 2)}\")\n",
    "print(f\"PPO advantages:     {np.round(ppo_advantages, 2)}\")\n",
    "print(f\"\\nNotice how noisy the critic makes the advantages!\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now simulate GRPO: just normalize rewards within the group\n",
    "# No critic needed!\n",
    "\n",
    "rewards = true_quality  # Direct rewards (or reward model scores)\n",
    "mean_r = rewards.mean()\n",
    "std_r = rewards.std()\n",
    "\n",
    "grpo_advantages = (rewards - mean_r) / std_r\n",
    "\n",
    "print(\"=== GRPO (Group-Relative) Advantages ===\")\n",
    "print(f\"Rewards:           {rewards}\")\n",
    "print(f\"Group mean:        {mean_r:.2f}\")\n",
    "print(f\"Group std:         {std_r:.2f}\")\n",
    "print(f\"GRPO advantages:   {np.round(grpo_advantages, 2)}\")\n",
    "print(f\"\\nNo noise from a critic -- just clean normalization!\")"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors_ppo = ['green' if a > 0 else 'red' for a in ppo_advantages]\n",
    "colors_grpo = ['green' if a > 0 else 'red' for a in grpo_advantages]\n",
    "\n",
    "axes[0].bar(range(8), ppo_advantages, color=colors_ppo, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title(\"PPO Advantages (Critic-Based)\", fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel(\"Response Index\")\n",
    "axes[0].set_ylabel(\"Advantage\")\n",
    "axes[0].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "axes[0].set_xticks(range(8))\n",
    "\n",
    "axes[1].bar(range(8), grpo_advantages, color=colors_grpo, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title(\"GRPO Advantages (Group-Relative)\", fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel(\"Response Index\")\n",
    "axes[1].set_ylabel(\"Advantage\")\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "axes[1].set_xticks(range(8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ppo_vs_grpo_advantages.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"GRPO advantages cleanly separate good from bad responses!\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### PPO's Advantage Estimation\n",
    "\n",
    "In PPO, the advantage for token $t$ is computed using Generalized Advantage Estimation (GAE):\n",
    "\n",
    "$$\\hat{A}_t = \\sum_{l=0}^{T-t} (\\gamma \\lambda)^l \\delta_{t+l}$$\n",
    "\n",
    "where $\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$\n",
    "\n",
    "This requires a learned value function $V(s)$ -- the critic network.\n",
    "\n",
    "Let us plug in numbers. Suppose $\\gamma = 0.99$, $\\lambda = 0.95$, $r_t = 1$, $V(s_t) = 5.0$, $V(s_{t+1}) = 5.5$:\n",
    "\n",
    "$$\\delta_t = 1 + 0.99 \\times 5.5 - 5.0 = 1 + 5.445 - 5.0 = 1.445$$\n",
    "\n",
    "The problem: $V(s)$ must be accurate for this to work. If the critic is wrong, the advantages are wrong.\n",
    "\n",
    "### GRPO's Group-Relative Advantage\n",
    "\n",
    "GRPO replaces all of the above with:\n",
    "\n",
    "$$\\hat{A}_i = \\frac{r_i - \\mu}{\\sigma}$$\n",
    "\n",
    "where $\\mu = \\frac{1}{G}\\sum_{j=1}^G r_j$ and $\\sigma = \\sqrt{\\frac{1}{G}\\sum_{j=1}^G (r_j - \\mu)^2}$\n",
    "\n",
    "Let us verify this with numbers. Suppose $G=5$ responses have rewards $\\{2.0, 3.5, 1.0, 4.0, 2.5\\}$:\n",
    "\n",
    "- $\\mu = (2.0+3.5+1.0+4.0+2.5)/5 = 2.6$\n",
    "- $\\sigma = \\sqrt{(0.36+0.81+2.56+1.96+0.01)/5} = \\sqrt{1.14} \\approx 1.07$\n",
    "- $\\hat{A}_1 = (2.0-2.6)/1.07 = -0.56$\n",
    "- $\\hat{A}_4 = (4.0-2.6)/1.07 = +1.31$ (best in group)\n",
    "\n",
    "This is exactly what we want -- the best response gets the highest advantage."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grpo_advantages(rewards: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute group-relative advantages from a batch of rewards.\n",
    "\n",
    "    Args:\n",
    "        rewards: Tensor of shape (G,) -- rewards for G completions\n",
    "    Returns:\n",
    "        advantages: Tensor of shape (G,) -- normalized advantages\n",
    "    \"\"\"\n",
    "    mean_reward = rewards.mean()\n",
    "    std_reward = rewards.std()\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if std_reward < 1e-8:\n",
    "        return torch.zeros_like(rewards)\n",
    "\n",
    "    advantages = (rewards - mean_reward) / std_reward\n",
    "    return advantages\n",
    "\n",
    "# Verify with our example\n",
    "rewards = torch.tensor([2.0, 3.5, 1.0, 4.0, 2.5])\n",
    "advantages = compute_grpo_advantages(rewards)\n",
    "\n",
    "print(\"Rewards:    \", rewards.numpy())\n",
    "print(\"Advantages: \", advantages.numpy().round(2))\n",
    "print(f\"\\nBest response (idx {advantages.argmax()}): A = {advantages.max():.2f}\")\n",
    "print(f\"Worst response (idx {advantages.argmin()}): A = {advantages.min():.2f}\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### Component 1: Group Sampling\n",
    "\n",
    "The first step in GRPO is generating $G$ different completions for the same prompt."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLanguageModel(nn.Module):\n",
    "    \"\"\"A tiny 'language model' for demonstration purposes.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size=100, hidden_dim=64, max_len=20):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.head = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns logits for each position.\"\"\"\n",
    "        emb = self.embedding(x)\n",
    "        out, _ = self.rnn(emb)\n",
    "        logits = self.head(out)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, prompt_ids, max_new_tokens=10, temperature=1.0):\n",
    "        \"\"\"Auto-regressively generate tokens.\"\"\"\n",
    "        generated = prompt_ids.clone()\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self.forward(generated)\n",
    "            next_logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "        return generated\n",
    "\n",
    "# Create model\n",
    "torch.manual_seed(42)\n",
    "model = SimpleLanguageModel(vocab_size=100, hidden_dim=64)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_group(model, prompt_ids, G=8, max_new_tokens=10, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample G completions for the same prompt.\n",
    "    This is the first step of GRPO.\n",
    "    \"\"\"\n",
    "    completions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(G):\n",
    "            output = model.generate(\n",
    "                prompt_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature\n",
    "            )\n",
    "            completions.append(output)\n",
    "\n",
    "    return completions\n",
    "\n",
    "# Sample a group of completions\n",
    "prompt = torch.tensor([[1, 5, 10]])  # A simple prompt\n",
    "G = 8\n",
    "completions = sample_group(model, prompt, G=G, max_new_tokens=8)\n",
    "\n",
    "print(f\"Prompt: {prompt[0].tolist()}\")\n",
    "print(f\"\\nGenerated {G} completions:\")\n",
    "for i, comp in enumerate(completions):\n",
    "    tokens = comp[0, prompt.shape[1]:].tolist()  # Only new tokens\n",
    "    print(f\"  Response {i+1}: {tokens}\")\n",
    "\n",
    "print(f\"\\nNotice: each response is different due to sampling!\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 2: Reward Scoring\n",
    "\n",
    "In practice, rewards come from a reward model or verifiable rules. Here we will use a simple rule-based reward."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_reward_function(completion_ids, target_token=42):\n",
    "    \"\"\"\n",
    "    Simple reward: how many times does the target token appear?\n",
    "    In real GRPO, this could be:\n",
    "    - A reward model score\n",
    "    - Binary correctness (math problems)\n",
    "    - Code test pass rate\n",
    "    \"\"\"\n",
    "    new_tokens = completion_ids[0, 3:]  # Skip prompt tokens\n",
    "    count = (new_tokens == target_token).sum().item()\n",
    "    return float(count) / len(new_tokens)\n",
    "\n",
    "# Score all completions\n",
    "rewards = torch.tensor([\n",
    "    simple_reward_function(comp)\n",
    "    for comp in completions\n",
    "])\n",
    "\n",
    "print(\"Rewards for each completion:\")\n",
    "for i, (comp, r) in enumerate(zip(completions, rewards)):\n",
    "    tokens = comp[0, 3:].tolist()\n",
    "    print(f\"  Response {i+1}: reward = {r:.3f}  tokens = {tokens}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 3: Putting It Together -- Group-Relative Advantage Pipeline"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_advantage_pipeline(model, prompt_ids, reward_fn, G=8, max_new_tokens=10):\n",
    "    \"\"\"\n",
    "    Complete GRPO advantage computation pipeline.\n",
    "\n",
    "    1. Sample G completions\n",
    "    2. Score each with reward function\n",
    "    3. Normalize rewards within the group\n",
    "    \"\"\"\n",
    "    # Step 1: Sample group\n",
    "    completions = sample_group(model, prompt_ids, G=G, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Step 2: Score each\n",
    "    rewards = torch.tensor([reward_fn(comp) for comp in completions])\n",
    "\n",
    "    # Step 3: Compute group-relative advantages\n",
    "    advantages = compute_grpo_advantages(rewards)\n",
    "\n",
    "    return completions, rewards, advantages\n",
    "\n",
    "# Run the pipeline\n",
    "completions, rewards, advantages = grpo_advantage_pipeline(\n",
    "    model, prompt, simple_reward_function, G=8\n",
    ")\n",
    "\n",
    "print(\"=== GRPO Advantage Pipeline Results ===\")\n",
    "print(f\"{'Response':<10} {'Reward':<10} {'Advantage':<12} {'Action'}\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(rewards)):\n",
    "    action = \"REINFORCE\" if advantages[i] > 0 else \"PENALIZE\" if advantages[i] < -0.5 else \"NEUTRAL\"\n",
    "    print(f\"{i+1:<10} {rewards[i]:<10.3f} {advantages[i]:<12.3f} {action}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Implement Advantage Computation with Clipping\n",
    "\n",
    "The basic GRPO advantage is $(r_i - \\mu) / \\sigma$, but what happens when all rewards are very similar? The advantages become very large due to small $\\sigma$. Implement a version that clips the advantages."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clipped_advantages(\n",
    "    rewards: torch.Tensor,\n",
    "    max_advantage: float = 3.0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute group-relative advantages with clipping.\n",
    "\n",
    "    Args:\n",
    "        rewards: Tensor of shape (G,) -- rewards for G completions\n",
    "        max_advantage: Maximum absolute advantage value\n",
    "    Returns:\n",
    "        advantages: Tensor of shape (G,) -- clipped normalized advantages\n",
    "\n",
    "    TODO: Implement this function.\n",
    "    Hints:\n",
    "    1. Compute mean and std of rewards\n",
    "    2. Normalize: (rewards - mean) / std\n",
    "    3. Clip advantages to [-max_advantage, max_advantage]\n",
    "    4. Handle the case where std is very small (< 1e-8)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    mean_reward = rewards.mean()\n",
    "    std_reward = rewards.std()\n",
    "\n",
    "    if std_reward < 1e-8:\n",
    "        return torch.zeros_like(rewards)\n",
    "\n",
    "    advantages = (rewards - mean_reward) / std_reward\n",
    "    advantages = torch.clamp(advantages, -max_advantage, max_advantage)\n",
    "\n",
    "    return advantages\n",
    "\n",
    "    raise NotImplementedError(\"Implement clipped advantages!\")\n",
    "\n",
    "# Test your implementation\n",
    "test_rewards = torch.tensor([1.0, 1.01, 1.02, 1.0, 0.99])  # Very similar rewards\n",
    "clipped_adv = compute_clipped_advantages(test_rewards, max_advantage=2.0)\n",
    "print(\"Similar rewards:\", test_rewards.numpy())\n",
    "print(\"Clipped advantages:\", clipped_adv.numpy().round(3))\n",
    "assert torch.all(torch.abs(clipped_adv) <= 2.0), \"Advantages should be clipped!\"\n",
    "print(\"Passed!\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Experiment with Group Size G\n",
    "\n",
    "How does the group size G affect the quality of advantage estimates?"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_group_size(model, prompt_ids, reward_fn, G_values=[2, 4, 8, 16, 32]):\n",
    "    \"\"\"\n",
    "    TODO: For each group size G, run the GRPO advantage pipeline\n",
    "    multiple times and compute the variance of the advantages.\n",
    "\n",
    "    Hints:\n",
    "    1. For each G in G_values, run grpo_advantage_pipeline N=20 times\n",
    "    2. Collect the advantage of the BEST response each time\n",
    "    3. Compute mean and std of these best advantages\n",
    "    4. Plot: X=G, Y=mean best advantage, error bars = std\n",
    "\n",
    "    Expected result: larger G gives more stable advantages.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    N_trials = 20\n",
    "\n",
    "    for G in G_values:\n",
    "        best_advantages = []\n",
    "        for _ in range(N_trials):\n",
    "            _, rewards, advantages = grpo_advantage_pipeline(\n",
    "                model, prompt_ids, reward_fn, G=G\n",
    "            )\n",
    "            best_advantages.append(advantages.max().item())\n",
    "\n",
    "        results[G] = {\n",
    "            'mean': np.mean(best_advantages),\n",
    "            'std': np.std(best_advantages)\n",
    "        }\n",
    "        print(f\"G={G:3d}: mean best advantage = {results[G]['mean']:.3f} +/- {results[G]['std']:.3f}\")\n",
    "\n",
    "    # Plot\n",
    "    G_vals = list(results.keys())\n",
    "    means = [results[g]['mean'] for g in G_vals]\n",
    "    stds = [results[g]['std'] for g in G_vals]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.errorbar(G_vals, means, yerr=stds, marker='o', capsize=5, linewidth=2)\n",
    "    plt.xlabel(\"Group Size G\", fontsize=12)\n",
    "    plt.ylabel(\"Best Advantage (mean +/- std)\", fontsize=12)\n",
    "    plt.title(\"Effect of Group Size on Advantage Stability\", fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(\"group_size_effect.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "experiment_group_size(model, prompt, simple_reward_function)"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Now let us combine everything into a visualization that shows the full GRPO advantage computation pipeline."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_grpo_pipeline(rewards, title=\"GRPO Advantage Computation\"):\n",
    "    \"\"\"Visualize the complete GRPO pipeline for a single group.\"\"\"\n",
    "    advantages = compute_grpo_advantages(rewards)\n",
    "    mean_r = rewards.mean().item()\n",
    "    std_r = rewards.std().item()\n",
    "    G = len(rewards)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "    # Panel 1: Raw rewards\n",
    "    colors = plt.cm.viridis(np.linspace(0.3, 0.9, G))\n",
    "    axes[0].bar(range(G), rewards.numpy(), color=colors, edgecolor='black')\n",
    "    axes[0].axhline(y=mean_r, color='red', linestyle='--', linewidth=2, label=f'Mean = {mean_r:.2f}')\n",
    "    axes[0].set_title(\"Step 1: Raw Rewards\", fontsize=13, fontweight='bold')\n",
    "    axes[0].set_xlabel(\"Response Index\")\n",
    "    axes[0].set_ylabel(\"Reward\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Panel 2: Centered (subtract mean)\n",
    "    centered = (rewards - mean_r).numpy()\n",
    "    colors_centered = ['green' if c > 0 else 'red' for c in centered]\n",
    "    axes[1].bar(range(G), centered, color=colors_centered, alpha=0.7, edgecolor='black')\n",
    "    axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[1].set_title(f\"Step 2: Subtract Mean ({mean_r:.2f})\", fontsize=13, fontweight='bold')\n",
    "    axes[1].set_xlabel(\"Response Index\")\n",
    "    axes[1].set_ylabel(\"Centered Reward\")\n",
    "\n",
    "    # Panel 3: Final advantages (divide by std)\n",
    "    colors_adv = ['green' if a > 0 else 'red' for a in advantages.numpy()]\n",
    "    axes[2].bar(range(G), advantages.numpy(), color=colors_adv, alpha=0.7, edgecolor='black')\n",
    "    axes[2].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[2].set_title(f\"Step 3: Divide by Std ({std_r:.2f})\", fontsize=13, fontweight='bold')\n",
    "    axes[2].set_xlabel(\"Response Index\")\n",
    "    axes[2].set_ylabel(\"GRPO Advantage\")\n",
    "\n",
    "    plt.suptitle(title, fontsize=15, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"grpo_pipeline_visualization.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return advantages\n",
    "\n",
    "# Demonstrate with example rewards\n",
    "example_rewards = torch.tensor([2.0, 3.5, 1.0, 4.0, 2.5, 3.0, 1.5, 3.8])\n",
    "advantages = visualize_grpo_pipeline(example_rewards)"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "Let us now see how group-relative advantages compare to random advantages in a simplified training simulation."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_training(n_steps=200, G=8, use_grpo=True):\n",
    "    \"\"\"\n",
    "    Simulate GRPO training on a simple optimization problem.\n",
    "\n",
    "    The 'policy' adjusts a parameter theta to maximize reward.\n",
    "    Reward = -|theta - 3.0| (optimal at theta=3.0)\n",
    "    \"\"\"\n",
    "    theta = torch.tensor([0.0], requires_grad=True)\n",
    "    optimizer = torch.optim.SGD([theta], lr=0.05)\n",
    "\n",
    "    history = []\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        # Sample G 'completions' (here: theta + noise)\n",
    "        noise = torch.randn(G) * 0.5\n",
    "        samples = theta.item() + noise\n",
    "\n",
    "        # Compute rewards\n",
    "        rewards = -torch.abs(samples - 3.0)\n",
    "\n",
    "        if use_grpo:\n",
    "            # GRPO: normalize within group\n",
    "            advantages = compute_grpo_advantages(rewards)\n",
    "        else:\n",
    "            # No normalization (vanilla PG)\n",
    "            advantages = rewards\n",
    "\n",
    "        # Compute surrogate loss\n",
    "        # Higher advantage = push theta toward that sample\n",
    "        weighted_direction = (advantages * noise).mean()\n",
    "        loss = -weighted_direction\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        history.append(theta.item())\n",
    "\n",
    "    return history\n",
    "\n",
    "# Compare GRPO vs Vanilla\n",
    "torch.manual_seed(42)\n",
    "history_grpo = simulate_training(n_steps=200, G=8, use_grpo=True)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "history_vanilla = simulate_training(n_steps=200, G=8, use_grpo=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_grpo, label='GRPO (Group-Relative)', linewidth=2, color='blue')\n",
    "plt.plot(history_vanilla, label='Vanilla PG (No Normalization)', linewidth=2, color='orange')\n",
    "plt.axhline(y=3.0, color='green', linestyle='--', linewidth=1, label='Optimal (theta=3.0)')\n",
    "plt.xlabel(\"Training Step\", fontsize=12)\n",
    "plt.ylabel(\"Parameter Value (theta)\", fontsize=12)\n",
    "plt.title(\"GRPO vs Vanilla Policy Gradient\", fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(\"grpo_vs_vanilla_training.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"GRPO final theta:   {history_grpo[-1]:.3f} (target: 3.000)\")\n",
    "print(f\"Vanilla final theta: {history_vanilla[-1]:.3f} (target: 3.000)\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary visualization: The GRPO advantage computation is simple but powerful\n",
    "print(\"=\" * 60)\n",
    "print(\"GRPO Advantage Computation Summary\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"1. Sample G completions for the same prompt\")\n",
    "print(\"2. Score each with a reward function\")\n",
    "print(\"3. Normalize: A_i = (r_i - mean(r)) / std(r)\")\n",
    "print()\n",
    "print(\"That's it! No critic network. No GAE. No value function.\")\n",
    "print()\n",
    "print(\"Key properties:\")\n",
    "print(\"  - Self-calibrating (works for any reward scale)\")\n",
    "print(\"  - Memory efficient (no critic network)\")\n",
    "print(\"  - Larger G = more stable advantages\")\n",
    "print(\"  - Typically G = 4 to 64 in practice\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "**Key takeaways from this notebook:**\n",
    "\n",
    "1. PPO requires a critic network (value function) to compute advantages. This doubles the memory cost.\n",
    "2. GRPO eliminates the critic by normalizing rewards within a group of sampled responses.\n",
    "3. The group-relative advantage formula is $\\hat{A}_i = (r_i - \\mu) / \\sigma$ -- simple z-score normalization.\n",
    "4. Larger group sizes produce more stable advantage estimates, but with diminishing returns.\n",
    "\n",
    "**Reflection questions:**\n",
    "- Why does normalizing by the group standard deviation help compared to just subtracting the mean?\n",
    "- What happens if all G responses get the same reward? Is the GRPO advantage well-defined?\n",
    "- How might the choice of G interact with the diversity of the policy's outputs?\n",
    "\n",
    "**Next notebook:** We will implement the full GRPO loss function including the clipped surrogate objective and KL penalty, and train a small language model using GRPO."
   ],
   "id": "cell_26"
  }
 ]
}