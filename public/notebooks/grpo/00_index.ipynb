{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "GRPO Notebook Index -- Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group-Relative Policy Optimization (GRPO) -- Notebook Index\n",
    "\n",
    "Welcome to the GRPO notebook series! These notebooks guide you through understanding, implementing, and training with Group-Relative Policy Optimization -- the algorithm behind DeepSeek-R1.\n",
    "\n",
    "## Learning Path\n",
    "\n",
    "### Notebook 1: Understanding GRPO\n",
    "**From PPO to Group-Relative Advantages**\n",
    "\n",
    "- Why PPO's critic is expensive for LLMs\n",
    "- The group-relative advantage formula\n",
    "- Implementing advantage computation from scratch\n",
    "- Visualizing PPO vs GRPO advantages\n",
    "\n",
    "### Notebook 2: Implementing the GRPO Loss Function\n",
    "**Clipped Surrogate + KL Penalty**\n",
    "\n",
    "- The three components: clipping, advantages, KL penalty\n",
    "- Building each component step by step\n",
    "- Numerical verification of the KL divergence formula\n",
    "- Testing with synthetic data\n",
    "\n",
    "### Notebook 3: Training a Reasoning Model with GRPO\n",
    "**End-to-End GRPO Training Pipeline**\n",
    "\n",
    "- Character-level arithmetic as a testbed\n",
    "- Binary verifiable rewards (no reward model needed)\n",
    "- Full GRPO training loop\n",
    "- Training curves and evaluation\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of neural networks and PyTorch\n",
    "- Familiarity with language models (tokenization, generation)\n",
    "- Some exposure to reinforcement learning concepts (rewards, policies)\n",
    "\n",
    "## How to Use These Notebooks\n",
    "\n",
    "1. Run each notebook in order (1 -> 2 -> 3)\n",
    "2. Complete the TODO exercises to deepen your understanding\n",
    "3. Experiment with the hyperparameters (G, epsilon, beta)\n",
    "4. Each notebook is self-contained and runs on Google Colab with a T4 GPU"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ready to start learning GRPO!\")\n",
    "print()\n",
    "print(\"Notebook 1: Understanding GRPO\")\n",
    "print(\"  -> Open 01_understanding_grpo.ipynb\")\n",
    "print()\n",
    "print(\"Notebook 2: Implementing the GRPO Loss\")\n",
    "print(\"  -> Open 02_implementing_grpo_loss.ipynb\")\n",
    "print()\n",
    "print(\"Notebook 3: Training with GRPO\")\n",
    "print(\"  -> Open 03_training_reasoning_with_grpo.ipynb\")"
   ],
   "id": "cell_2"
  }
 ]
}