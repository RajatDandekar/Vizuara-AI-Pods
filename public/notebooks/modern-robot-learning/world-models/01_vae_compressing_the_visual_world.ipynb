{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "VAE: Compressing the Visual World â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1BTEmYxTT95GZ05nMjwvaGWt0KwF-7ifD\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ VAE: Compressing the Visual World from First Principles\n",
    "\n",
    "*Part 1 of the Vizuara series on World Models*\n",
    "*Estimated time: 45 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://pods.vizuara.ai/courses/world-models/practice/1/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_why_it_matters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Imagine you are building an AI agent that learns to drive a car in a video game. At every moment, the agent sees a 64Ã—64 pixel image â€” that is 12,288 numbers (64 Ã— 64 Ã— 3 color channels). Trying to make decisions directly from 12,288 numbers is like trying to navigate a city by memorizing every single brick in every building. It is overwhelmingly detailed and computationally expensive.\n",
    "\n",
    "What if we could teach the agent to \"sketch\" the scene instead? Capture just the *essence* â€” the curve of the road, the position of the car, the upcoming turn â€” in a tiny vector of just 32 numbers?\n",
    "\n",
    "This is exactly what a **Variational Autoencoder (VAE)** does. It is the \"Vision\" component (V) of the World Model architecture, and it compresses raw images into a compact **latent code** that preserves everything the agent needs to know.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Build a VAE from scratch in PyTorch\n",
    "- Train it on MNIST digits (a clean, fast testbed)\n",
    "- Watch it reconstruct images from just 16 numbers\n",
    "- Interpolate smoothly between digits in latent space â€” seeing how the VAE organizes its internal \"sketch pad\"\n",
    "\n",
    "Let us start."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup and GPU check\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Building Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_building_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_building_intuition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### The Core Idea: Compression and Reconstruction\n",
    "\n",
    "Think about how a talented sketch artist works. Someone shows them a detailed photograph, and in seconds they capture the essence in a quick pencil sketch â€” the overall shape, the key features, the spatial relationships. That sketch is dramatically simpler than the photograph, but it contains all the information needed to understand the scene.\n",
    "\n",
    "A VAE works the same way:\n",
    "1. The **Encoder** is the sketch artist â€” it looks at the full image and produces a tiny \"sketch\" (the latent code)\n",
    "2. The **Decoder** is like an artist who reconstructs the full image from just the sketch\n",
    "\n",
    "But here is what makes a VAE special compared to a regular autoencoder: the encoder does not output a single fixed sketch. Instead, it outputs a *probability distribution* â€” a cloud of possible sketches.\n",
    "\n",
    "### ðŸ¤” Think About This\n",
    "\n",
    "Why would we want the encoder to output a distribution instead of a single point?\n",
    "\n",
    "Hint: Think about what happens when you try to compress similar but slightly different images. A photo of the digit \"7\" written with a slight lean versus a perfectly vertical \"7\" â€” should they map to the exact same latent code?\n",
    "\n",
    "The answer is no. By outputting a distribution, the VAE acknowledges that compression is inherently uncertain. Slightly different inputs can map to overlapping regions in latent space, which means the latent space becomes *smooth* â€” nearby points decode to similar images. This smoothness is what allows us to interpolate between images later."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Mathematics\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_mathematics.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_mathematics"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### The Reparameterization Trick\n",
    "\n",
    "The encoder outputs two vectors for each input image: a **mean** $\\mu$ and a **log-variance** $\\log \\sigma^2$. To get the latent code $z$, we sample from this distribution:\n",
    "\n",
    "$$z = \\mu + \\sigma \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 1)$$\n",
    "\n",
    "This equation says: take the mean $\\mu$, then add some noise scaled by $\\sigma$. The noise $\\epsilon$ is sampled from a standard normal distribution.\n",
    "\n",
    "Computationally: if $\\mu = 0.5$ and $\\sigma = 0.2$, and we happen to sample $\\epsilon = 0.3$:\n",
    "\n",
    "$$z = 0.5 + 0.2 \\times 0.3 = 0.56$$\n",
    "\n",
    "Why this trick? During training, we need gradients to flow through the sampling operation. We cannot backpropagate through a random sample. But by rewriting the sample as $\\mu + \\sigma \\cdot \\epsilon$, the randomness is isolated in $\\epsilon$ (which does not depend on any parameters), and gradients flow cleanly through $\\mu$ and $\\sigma$.\n",
    "\n",
    "### The VAE Loss Function\n",
    "\n",
    "The VAE is trained with two objectives:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{VAE}} = \\underbrace{\\text{BCE}(x, \\hat{x})}_{\\text{Reconstruction Loss}} + \\underbrace{D_{\\text{KL}}(q(z|x) \\| \\mathcal{N}(0, 1))}_{\\text{KL Divergence}}$$\n",
    "\n",
    "- **Reconstruction loss**: How well does the decoded image $\\hat{x}$ match the original $x$? We use binary cross-entropy (BCE) for this.\n",
    "- **KL divergence**: How far is the encoder's distribution from a standard normal? This regularizer keeps the latent space well-organized.\n",
    "\n",
    "The KL divergence has a beautiful closed-form solution for Gaussians:\n",
    "\n",
    "$$D_{\\text{KL}} = -\\frac{1}{2} \\sum_{j=1}^{d} \\left(1 + \\log \\sigma_j^2 - \\mu_j^2 - \\sigma_j^2\\right)$$\n",
    "\n",
    "Computationally: for each dimension $j$ of the latent code, we compute this term using $\\mu_j$ and $\\log \\sigma_j^2$ â€” both of which the encoder directly outputs. No sampling needed!"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Encoder\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_encoder.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_encoder"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It â€” Component by Component\n",
    "\n",
    "### 4.1 The Encoder\n",
    "\n",
    "The encoder takes a 28Ã—28 grayscale image (MNIST) and compresses it into parameters of a distribution in latent space."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 16  # We will compress 784 pixels into just 16 numbers!\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=LATENT_DIM):\n",
    "        super().__init__()\n",
    "        # 28x28 = 784 -> 400 -> latent_dim\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc_mu = nn.Linear(400, latent_dim)      # Mean of the distribution\n",
    "        self.fc_logvar = nn.Linear(400, latent_dim)   # Log-variance of the distribution\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the image: (batch, 1, 28, 28) -> (batch, 784)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        h = F.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "# Test the encoder\n",
    "encoder = Encoder().to(device)\n",
    "dummy_input = torch.randn(4, 1, 28, 28).to(device)\n",
    "mu, logvar = encoder(dummy_input)\n",
    "print(f\"Input shape:   {dummy_input.shape}\")\n",
    "print(f\"mu shape:      {mu.shape}\")\n",
    "print(f\"logvar shape:  {logvar.shape}\")\n",
    "print(f\"Compression:   784 values -> {LATENT_DIM} (mu) + {LATENT_DIM} (logvar)\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the encoder outputs TWO vectors of size 16 â€” the mean and the log-variance. These parameterize a 16-dimensional Gaussian distribution in latent space.\n",
    "\n",
    "### 4.2 The Reparameterization Trick\n",
    "\n",
    "This is the clever bit that makes VAE training possible. We need to sample from the encoder's distribution, but in a way that allows gradients to flow."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Reparam And Decoder\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_reparam_and_decoder.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_reparam_and_decoder"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, logvar):\n",
    "    \"\"\"\n",
    "    Sample z from q(z|x) using the reparameterization trick.\n",
    "\n",
    "    Instead of sampling z ~ N(mu, sigma^2) directly,\n",
    "    we sample epsilon ~ N(0, 1) and compute z = mu + sigma * epsilon.\n",
    "    This lets gradients flow through mu and sigma.\n",
    "    \"\"\"\n",
    "    std = torch.exp(0.5 * logvar)   # sigma = exp(0.5 * log(sigma^2))\n",
    "    eps = torch.randn_like(std)      # Sample from N(0, 1)\n",
    "    z = mu + std * eps               # The trick!\n",
    "    return z\n",
    "\n",
    "# Test it\n",
    "z = reparameterize(mu, logvar)\n",
    "print(f\"Sampled z shape: {z.shape}\")\n",
    "print(f\"First sample z[0]: {z[0].detach().cpu().numpy().round(3)}\")\n",
    "\n",
    "# Sample again â€” different z because epsilon is random!\n",
    "z2 = reparameterize(mu, logvar)\n",
    "print(f\"Second sample z[0]: {z2.detach().cpu().numpy().round(3)}\")\n",
    "print(f\"Same mu, different samples â€” this is the stochasticity!\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The Decoder\n",
    "\n",
    "The decoder takes a latent code $z$ and reconstructs the original image."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=LATENT_DIM):\n",
    "        super().__init__()\n",
    "        # latent_dim -> 400 -> 784\n",
    "        self.fc1 = nn.Linear(latent_dim, 400)\n",
    "        self.fc2 = nn.Linear(400, 784)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = F.relu(self.fc1(z))\n",
    "        # Sigmoid to get pixel values in [0, 1]\n",
    "        reconstruction = torch.sigmoid(self.fc2(h))\n",
    "        return reconstruction.view(-1, 1, 28, 28)\n",
    "\n",
    "# Test the decoder\n",
    "decoder = Decoder().to(device)\n",
    "recon = decoder(z)\n",
    "print(f\"Latent z shape:       {z.shape}\")\n",
    "print(f\"Reconstruction shape: {recon.shape}\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Before Training Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_before_training_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_before_training_viz"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualization: What does random decoding look like before training?\n",
    "fig, axes = plt.subplots(1, 8, figsize=(16, 2))\n",
    "for i in range(8):\n",
    "    random_z = torch.randn(1, LATENT_DIM).to(device)\n",
    "    with torch.no_grad():\n",
    "        img = decoder(random_z).cpu().squeeze()\n",
    "    axes[i].imshow(img, cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'z sample {i+1}', fontsize=9)\n",
    "plt.suptitle('Decoded random latent codes (BEFORE training) â€” just noise!', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Complete Vae\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_complete_vae.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_complete_vae"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The Complete VAE\n",
    "\n",
    "Now let us put encoder, reparameterization, and decoder together."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=LATENT_DIM):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction, mu, logvar\n",
    "\n",
    "vae = VAE().to(device)\n",
    "total_params = sum(p.numel() for p in vae.parameters())\n",
    "print(f\"Total VAE parameters: {total_params:,}\")\n",
    "print(f\"Compression ratio: 784 / {LATENT_DIM} = {784/LATENT_DIM:.0f}x\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo Loss\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_todo_loss.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_todo_loss"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ðŸ”§ Your Turn: Implement the VAE Loss Function\n",
    "\n",
    "Now it is your turn. Implement the VAE loss â€” the sum of reconstruction loss and KL divergence."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(reconstruction, original, mu, logvar):\n",
    "    \"\"\"\n",
    "    Compute the VAE loss = Reconstruction Loss + KL Divergence.\n",
    "\n",
    "    Args:\n",
    "        reconstruction: Decoded image, shape (batch, 1, 28, 28)\n",
    "        original: Original image, shape (batch, 1, 28, 28)\n",
    "        mu: Encoder mean, shape (batch, latent_dim)\n",
    "        logvar: Encoder log-variance, shape (batch, latent_dim)\n",
    "\n",
    "    Returns:\n",
    "        Total loss (scalar), reconstruction loss (scalar), KL loss (scalar)\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Compute reconstruction loss using F.binary_cross_entropy\n",
    "    #         Remember to set reduction='sum' and flatten the images\n",
    "    #         Hint: F.binary_cross_entropy(recon.view(-1, 784), orig.view(-1, 784), reduction='sum')\n",
    "    #\n",
    "    # Step 2: Compute KL divergence using the closed-form formula:\n",
    "    #         KL = -0.5 * sum(1 + logvar - mu^2 - exp(logvar))\n",
    "    #\n",
    "    # Step 3: Return the sum of both losses\n",
    "    # ==============================\n",
    "\n",
    "    recon_loss = ???  # YOUR CODE HERE\n",
    "    kl_loss = ???     # YOUR CODE HERE\n",
    "\n",
    "    return recon_loss + kl_loss, recon_loss, kl_loss"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Verification: Test your loss function\n",
    "test_recon = torch.sigmoid(torch.randn(2, 1, 28, 28)).to(device)\n",
    "test_orig = torch.rand(2, 1, 28, 28).to(device)\n",
    "test_mu = torch.randn(2, LATENT_DIM).to(device)\n",
    "test_logvar = torch.zeros(2, LATENT_DIM).to(device)  # sigma=1 -> KL should be 0\n",
    "\n",
    "total, recon, kl = vae_loss(test_recon, test_orig, test_mu, test_logvar)\n",
    "\n",
    "# When logvar=0 and mu!=0, KL = 0.5 * sum(mu^2)\n",
    "expected_kl = 0.5 * test_mu.pow(2).sum()\n",
    "assert torch.allclose(kl, expected_kl, atol=1e-4), \\\n",
    "    f\"âŒ KL divergence incorrect. Expected {expected_kl.item():.2f}, got {kl.item():.2f}\"\n",
    "assert recon.item() > 0, \"âŒ Reconstruction loss should be positive\"\n",
    "print(f\"âœ… Loss function works!\")\n",
    "print(f\"   Reconstruction loss: {recon.item():.2f}\")\n",
    "print(f\"   KL divergence: {kl.item():.2f}\")\n",
    "print(f\"   Total loss: {total.item():.2f}\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Stop And Think\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_stop_and_think.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_stop_and_think"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### âœ‹ Stop and Think\n",
    "\n",
    "Before we train, predict:\n",
    "1. What will happen to the reconstruction loss over training? (It should decrease â€” images get sharper)\n",
    "2. What will happen to the KL loss? (It starts near 0, then increases as the encoder learns, then stabilizes)\n",
    "3. After training, if we sample random $z \\sim \\mathcal{N}(0,1)$ and decode, what will we see? (Recognizable digits!)\n",
    "\n",
    "*Take a moment. Then scroll down.*\n",
    "\n",
    "---"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_11_training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training the VAE"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Image shape: {train_dataset[0][0].shape}\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "vae = VAE().to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 15\n",
    "train_losses = []\n",
    "recon_losses = []\n",
    "kl_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    vae.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_recon = 0\n",
    "    epoch_kl = 0\n",
    "\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        recon, mu, logvar = vae(data)\n",
    "        loss, recon_loss, kl_loss = vae_loss(recon, data, mu, logvar)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_recon += recon_loss.item()\n",
    "        epoch_kl += kl_loss.item()\n",
    "\n",
    "    # Average over all samples\n",
    "    n = len(train_loader.dataset)\n",
    "    train_losses.append(epoch_loss / n)\n",
    "    recon_losses.append(epoch_recon / n)\n",
    "    kl_losses.append(epoch_kl / n)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:2d}/{num_epochs} | \"\n",
    "          f\"Loss: {epoch_loss/n:.2f} | \"\n",
    "          f\"Recon: {epoch_recon/n:.2f} | \"\n",
    "          f\"KL: {epoch_kl/n:.2f}\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(range(1, num_epochs+1), train_losses, 'b-', linewidth=2, label='Total Loss')\n",
    "ax1.plot(range(1, num_epochs+1), recon_losses, 'r--', linewidth=2, label='Reconstruction')\n",
    "ax1.plot(range(1, num_epochs+1), kl_losses, 'g--', linewidth=2, label='KL Divergence')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss (per sample)', fontsize=12)\n",
    "ax1.set_title('VAE Training Curves', fontsize=14)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Ratio of losses\n",
    "ratios = [r / (r + k) for r, k in zip(recon_losses, kl_losses)]\n",
    "ax2.plot(range(1, num_epochs+1), ratios, 'purple', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Recon / Total Loss', fontsize=12)\n",
    "ax2.set_title('Reconstruction vs KL Balance', fontsize=14)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Results\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_results.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_12_results"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Seeing the Results\n",
    "\n",
    "### 7.1 Original vs. Reconstructed"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Compare originals and reconstructions\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    test_data = next(iter(test_loader))[0][:10].to(device)\n",
    "    recon, _, _ = vae(test_data)\n",
    "\n",
    "fig, axes = plt.subplots(2, 10, figsize=(20, 4))\n",
    "for i in range(10):\n",
    "    axes[0, i].imshow(test_data[i].cpu().squeeze(), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    axes[1, i].imshow(recon[i].cpu().squeeze(), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "axes[0, 0].set_ylabel('Original', fontsize=12, rotation=0, labelpad=60)\n",
    "axes[1, 0].set_ylabel('Reconstructed', fontsize=12, rotation=0, labelpad=60)\n",
    "plt.suptitle('VAE Reconstruction: 784 pixels â†’ 16 latent numbers â†’ 784 pixels', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Generating New Digits from Random Latent Codes"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Sample random z from N(0,1) and decode â€” the VAE generates new digits!\n",
    "vae.eval()\n",
    "fig, axes = plt.subplots(2, 10, figsize=(20, 4))\n",
    "for i in range(20):\n",
    "    row, col = i // 10, i % 10\n",
    "    random_z = torch.randn(1, LATENT_DIM).to(device)\n",
    "    with torch.no_grad():\n",
    "        generated = vae.decoder(random_z).cpu().squeeze()\n",
    "    axes[row, col].imshow(generated, cmap='gray')\n",
    "    axes[row, col].axis('off')\n",
    "plt.suptitle('Generated digits from random z ~ N(0,1) â€” the VAE has learned to dream!', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Interpolation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_interpolation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_13_interpolation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ðŸŽ¯ Final Output: Latent Space Interpolation\n",
    "\n",
    "This is the moment that makes it all click. We will pick two digits, encode them to latent space, and smoothly interpolate between them. If the VAE has learned a good latent space, we should see a smooth morphing from one digit to another."
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find specific digits for interpolation\n",
    "def get_digit(dataset, digit, index=0):\n",
    "    \"\"\"Get the index-th example of a specific digit.\"\"\"\n",
    "    count = 0\n",
    "    for img, label in dataset:\n",
    "        if label == digit:\n",
    "            if count == index:\n",
    "                return img.unsqueeze(0)\n",
    "            count += 1\n",
    "    return None\n",
    "\n",
    "vae.eval()\n",
    "pairs = [(1, 7), (3, 8), (4, 9), (2, 0)]\n",
    "n_steps = 10\n",
    "\n",
    "fig, axes = plt.subplots(len(pairs), n_steps, figsize=(20, 2 * len(pairs)))\n",
    "\n",
    "for row, (d1, d2) in enumerate(pairs):\n",
    "    img1 = get_digit(test_dataset, d1).to(device)\n",
    "    img2 = get_digit(test_dataset, d2).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mu1, _ = vae.encoder(img1)\n",
    "        mu2, _ = vae.encoder(img2)\n",
    "\n",
    "        for col, alpha in enumerate(np.linspace(0, 1, n_steps)):\n",
    "            z_interp = (1 - alpha) * mu1 + alpha * mu2\n",
    "            generated = vae.decoder(z_interp).cpu().squeeze()\n",
    "            axes[row, col].imshow(generated, cmap='gray')\n",
    "            axes[row, col].axis('off')\n",
    "\n",
    "    axes[row, 0].set_ylabel(f'{d1} â†’ {d2}', fontsize=11, rotation=0, labelpad=40)\n",
    "\n",
    "plt.suptitle('Latent Space Interpolation â€” Smooth Morphing Between Digits!', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"ðŸŽ‰ The VAE has learned a smooth, organized latent space!\")\n",
    "print(\"   This is exactly the kind of representation a World Model needs.\")\n",
    "print(\"   Next: we will learn to PREDICT how this latent space evolves over time.\")"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_14_closing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### ðŸ¤” Reflection Questions\n",
    "1. What happens if you increase the latent dimension from 16 to 64? Will reconstructions improve? What about interpolation?\n",
    "2. What happens if you remove the KL term entirely (set it to 0)? This gives you a regular autoencoder â€” will interpolation still work smoothly?\n",
    "3. Why is the KL term important for World Models specifically? (Hint: think about what the Memory component needs to do with the latent codes)\n",
    "\n",
    "### ðŸ† Optional Challenges\n",
    "1. **Convolutional VAE**: Replace the fully-connected layers with convolutional layers. Do the reconstructions get sharper?\n",
    "2. **Conditional generation**: Modify the decoder to also take a digit label as input. Can you generate any digit on demand?\n",
    "3. **Beta-VAE**: Multiply the KL term by a factor $\\beta > 1$. How does this affect the latent space organization?\n",
    "\n",
    "### What is Next?\n",
    "\n",
    "We have built the \"eyes\" of our World Model â€” the VAE that compresses visual observations into a compact latent code. But seeing is not enough. The agent also needs to *predict* what will happen next. In the next notebook, we will build the **MDN-RNN** â€” the Memory component that learns the dynamics of the world in latent space."
   ],
   "id": "cell_29"
  }
 ]
}