{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "02_grpo_from_scratch"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO from Scratch -- Vizuara\n",
    "\n",
    "In this notebook, we will build Group Relative Policy Optimization (GRPO) from scratch. GRPO is the RL algorithm behind DeepSeek-R1 that teaches language models to reason. By the end, you will have a working GRPO implementation that you understand line by line.\n",
    "\n",
    "**What you will build:** A complete GRPO training loop that computes group-relative advantages, clipped surrogate losses, and KL penalties.\n",
    "```"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Supervised fine-tuning teaches a model the *format* of reasoning, but not the *quality*. To teach a model which reasoning strategies actually lead to correct answers, we need reinforcement learning.\n",
    "\n",
    "GRPO (Group Relative Policy Optimization) is the algorithm that made DeepSeek-R1 possible. It is elegant, efficient, and does not require a separate value network (critic). Instead, it uses group statistics as a baseline \u2014 a beautifully simple idea that works remarkably well.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand how GRPO computes advantages without a critic\n",
    "- Implement the clipped surrogate objective from scratch\n",
    "- Visualize how clipping prevents catastrophic policy updates\n",
    "- Implement the KL divergence penalty\n",
    "```"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Imagine you are a teacher grading a math exam. You have 4 students who each attempted the same problem:\n",
    "- Student A: Correct answer (reward = 1)\n",
    "- Student B: Wrong answer (reward = 0)\n",
    "- Student C: Correct answer (reward = 1)\n",
    "- Student D: Wrong answer (reward = 0)\n",
    "\n",
    "The class average is 0.5. Students A and C are **above average** \u2014 you want to encourage their approaches. Students B and D are **below average** \u2014 you want to discourage theirs.\n",
    "\n",
    "This is exactly what GRPO does! For each prompt, it generates multiple completions (the \"group\"), scores them, and then encourages the good ones while discouraging the bad ones \u2014 all relative to the group average.\n",
    "\n",
    "### Think About This\n",
    "- Why is using the group average better than using a fixed baseline of 0?\n",
    "- What happens if ALL completions in the group get the same reward?\n",
    "```"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Group-Relative Advantages\n",
    "\n",
    "For a group of $G$ completions with rewards $\\mathbf{r} = [r_1, r_2, \\ldots, r_G]$, the advantage of the $i$-th completion is:\n",
    "\n",
    "$$\\hat{A}_i = \\frac{r_i - \\text{mean}(\\mathbf{r})}{\\text{std}(\\mathbf{r}) + \\epsilon}$$\n",
    "\n",
    "**Computational meaning:** This is z-score normalization. Completions above the group mean get positive advantages (encouraged), below get negative (discouraged). The standard deviation normalizes the scale.\n",
    "\n",
    "### The Clipped Surrogate Objective\n",
    "\n",
    "$$\\mathcal{L}_{\\text{GRPO}} = -\\frac{1}{G}\\sum_{i=1}^{G} \\min\\left(\\frac{\\pi_\\theta(y_i|x)}{\\pi_{\\text{old}}(y_i|x)} \\hat{A}_i,\\; \\text{clip}\\left(\\frac{\\pi_\\theta(y_i|x)}{\\pi_{\\text{old}}(y_i|x)}, 1-\\epsilon, 1+\\epsilon\\right) \\hat{A}_i\\right)$$\n",
    "\n",
    "**Computational meaning:** The ratio $\\frac{\\pi_\\theta}{\\pi_{\\text{old}}}$ measures how much the policy has changed. If it changes too much (ratio far from 1), the clipping limits the gradient signal, preventing catastrophic updates.\n",
    "\n",
    "### KL Divergence Penalty\n",
    "\n",
    "$$R_{\\text{total}} = R_{\\text{outcome}} - \\beta \\cdot D_{\\text{KL}}(\\pi_\\theta \\| \\pi_{\\text{ref}})$$\n",
    "\n",
    "**Computational meaning:** We subtract a penalty proportional to how much the current policy has diverged from the reference. This prevents the model from \"forgetting\" useful behaviors.\n",
    "```"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us compute group-relative advantages manually\n",
    "rewards = torch.tensor([1.0, 0.0, 1.0, 0.0])\n",
    "\n",
    "mean_r = rewards.mean()\n",
    "std_r = rewards.std()\n",
    "advantages = (rewards - mean_r) / (std_r + 1e-8)\n",
    "\n",
    "print(\"=== Group-Relative Advantage Computation ===\")\n",
    "print(f\"Rewards:    {rewards.tolist()}\")\n",
    "print(f\"Mean:       {mean_r:.2f}\")\n",
    "print(f\"Std:        {std_r:.2f}\")\n",
    "print(f\"Advantages: {advantages.tolist()}\")\n",
    "print(f\"\\nPositive advantages -> encourage these completions\")\n",
    "print(f\"Negative advantages -> discourage these completions\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 Group-Relative Advantage Function\n",
    "```"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_group_advantages(rewards):\n",
    "    \"\"\"\n",
    "    Compute group-relative advantages via z-score normalization.\n",
    "\n",
    "    Args:\n",
    "        rewards: Tensor of shape (G,) with reward for each completion\n",
    "\n",
    "    Returns:\n",
    "        advantages: Tensor of shape (G,) with normalized advantages\n",
    "    \"\"\"\n",
    "    mean = rewards.mean()\n",
    "    std = rewards.std()\n",
    "\n",
    "    # If all rewards are the same, advantages are all zero\n",
    "    if std < 1e-8:\n",
    "        return torch.zeros_like(rewards)\n",
    "\n",
    "    advantages = (rewards - mean) / (std + 1e-8)\n",
    "    return advantages\n",
    "\n",
    "# Test with different reward distributions\n",
    "test_cases = [\n",
    "    torch.tensor([1.0, 0.0, 1.0, 0.0]),   # Binary: half correct\n",
    "    torch.tensor([1.0, 1.0, 1.0, 0.0]),   # Mostly correct\n",
    "    torch.tensor([1.0, 0.0, 0.0, 0.0]),   # Mostly wrong\n",
    "    torch.tensor([1.0, 1.0, 1.0, 1.0]),   # All correct\n",
    "]\n",
    "\n",
    "for rewards in test_cases:\n",
    "    adv = compute_group_advantages(rewards)\n",
    "    print(f\"Rewards: {rewards.tolist()} -> Advantages: {[f'{a:.2f}' for a in adv.tolist()]}\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Checkpoint: How Advantages Change with Group Composition\n",
    "```"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "labels = [\"2 correct\\n2 wrong\", \"3 correct\\n1 wrong\", \"1 correct\\n3 wrong\", \"All correct\"]\n",
    "\n",
    "for ax, rewards, label in zip(axes, test_cases, labels):\n",
    "    adv = compute_group_advantages(rewards)\n",
    "    colors = ['#4CAF50' if a > 0 else '#F44336' if a < 0 else '#9E9E9E'\n",
    "              for a in adv.tolist()]\n",
    "    ax.bar(range(len(adv)), adv.tolist(), color=colors)\n",
    "    ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "    ax.set_title(label, fontsize=11)\n",
    "    ax.set_xlabel(\"Completion\")\n",
    "    ax.set_ylabel(\"Advantage\")\n",
    "    ax.set_ylim(-2.5, 2.5)\n",
    "\n",
    "plt.suptitle(\"Group-Relative Advantages for Different Reward Distributions\", fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Probability Ratio and Clipping\n",
    "\n",
    "The probability ratio tells us how much the policy has changed for a given completion.\n",
    "```"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clipped_objective(ratios, advantages, epsilon=0.2):\n",
    "    \"\"\"\n",
    "    Compute the clipped surrogate objective.\n",
    "\n",
    "    Args:\n",
    "        ratios: Probability ratios pi_new / pi_old, shape (G,)\n",
    "        advantages: Group-relative advantages, shape (G,)\n",
    "        epsilon: Clipping parameter (default 0.2)\n",
    "\n",
    "    Returns:\n",
    "        objective: Per-completion clipped objective, shape (G,)\n",
    "    \"\"\"\n",
    "    # Unclipped objective\n",
    "    unclipped = ratios * advantages\n",
    "\n",
    "    # Clipped objective\n",
    "    clipped_ratios = torch.clamp(ratios, 1 - epsilon, 1 + epsilon)\n",
    "    clipped = clipped_ratios * advantages\n",
    "\n",
    "    # Take the minimum (pessimistic bound)\n",
    "    objective = torch.min(unclipped, clipped)\n",
    "\n",
    "    return objective\n",
    "\n",
    "# Demonstrate with a concrete example\n",
    "ratio = torch.tensor(1.5)  # Policy changed significantly\n",
    "advantage = torch.tensor(1.0)  # This was a good completion\n",
    "epsilon = 0.2\n",
    "\n",
    "obj = compute_clipped_objective(ratio, advantage, epsilon)\n",
    "print(f\"Ratio: {ratio.item():.1f}, Advantage: {advantage.item():.1f}\")\n",
    "print(f\"Unclipped: {ratio.item() * advantage.item():.1f}\")\n",
    "print(f\"Clipped ratio: {torch.clamp(ratio, 1-epsilon, 1+epsilon).item():.1f}\")\n",
    "print(f\"Clipped objective: {obj.item():.1f}\")\n",
    "print(f\"\\nClipping prevented the objective from being {ratio.item() * advantage.item():.1f}\")\n",
    "print(f\"and capped it at {obj.item():.1f} \u2014 trust region enforced!\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Checkpoint: The Clipping Function\n",
    "```"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clipping for both positive and negative advantages\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ratios_range = torch.linspace(0.3, 2.5, 200)\n",
    "epsilon = 0.2\n",
    "\n",
    "# Positive advantage\n",
    "A_pos = 1.0\n",
    "unclipped_pos = ratios_range * A_pos\n",
    "clipped_pos = compute_clipped_objective(ratios_range, torch.full_like(ratios_range, A_pos), epsilon)\n",
    "\n",
    "ax1.plot(ratios_range, unclipped_pos, '--', color='#2196F3', linewidth=2, label='Unclipped')\n",
    "ax1.plot(ratios_range, clipped_pos, '-', color='#FF9800', linewidth=2.5, label='Clipped')\n",
    "ax1.axvline(x=1-epsilon, color='gray', linewidth=0.5, linestyle=':')\n",
    "ax1.axvline(x=1+epsilon, color='gray', linewidth=0.5, linestyle=':')\n",
    "ax1.axvline(x=1, color='black', linewidth=0.5, linestyle='-')\n",
    "ax1.fill_betweenx([0, 3], 1-epsilon, 1+epsilon, alpha=0.1, color='green')\n",
    "ax1.set_title('Positive Advantage (A > 0)', fontsize=13)\n",
    "ax1.set_xlabel('Probability Ratio', fontsize=11)\n",
    "ax1.set_ylabel('Objective', fontsize=11)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.set_ylim(-0.5, 3)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Negative advantage\n",
    "A_neg = -1.0\n",
    "unclipped_neg = ratios_range * A_neg\n",
    "clipped_neg = compute_clipped_objective(ratios_range, torch.full_like(ratios_range, A_neg), epsilon)\n",
    "\n",
    "ax2.plot(ratios_range, unclipped_neg, '--', color='#2196F3', linewidth=2, label='Unclipped')\n",
    "ax2.plot(ratios_range, clipped_neg, '-', color='#FF9800', linewidth=2.5, label='Clipped')\n",
    "ax2.axvline(x=1-epsilon, color='gray', linewidth=0.5, linestyle=':')\n",
    "ax2.axvline(x=1+epsilon, color='gray', linewidth=0.5, linestyle=':')\n",
    "ax2.axvline(x=1, color='black', linewidth=0.5, linestyle='-')\n",
    "ax2.fill_betweenx([-3, 0], 1-epsilon, 1+epsilon, alpha=0.1, color='green')\n",
    "ax2.set_title('Negative Advantage (A < 0)', fontsize=13)\n",
    "ax2.set_xlabel('Probability Ratio', fontsize=11)\n",
    "ax2.set_ylabel('Objective', fontsize=11)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.set_ylim(-3, 0.5)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('PPO/GRPO Clipping Mechanism', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 KL Divergence Penalty\n",
    "```"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl_penalty(log_probs_current, log_probs_reference):\n",
    "    \"\"\"\n",
    "    Compute per-token KL divergence between current and reference policy.\n",
    "\n",
    "    Args:\n",
    "        log_probs_current: Log probs from current policy, shape (seq_len,)\n",
    "        log_probs_reference: Log probs from reference policy, shape (seq_len,)\n",
    "\n",
    "    Returns:\n",
    "        kl_div: Scalar KL divergence\n",
    "    \"\"\"\n",
    "    # KL(pi_theta || pi_ref) = sum(pi_theta * (log pi_theta - log pi_ref))\n",
    "    # Simplified: for the sampled tokens, KL approx = sum(log_p_current - log_p_ref)\n",
    "    kl_div = (log_probs_current - log_probs_reference).sum()\n",
    "    return kl_div\n",
    "\n",
    "# Example\n",
    "log_probs_current = torch.tensor([-0.5, -0.8, -0.3, -1.0])\n",
    "log_probs_reference = torch.tensor([-0.6, -0.9, -0.4, -1.1])\n",
    "\n",
    "kl = compute_kl_penalty(log_probs_current, log_probs_reference)\n",
    "print(f\"Current log probs:   {log_probs_current.tolist()}\")\n",
    "print(f\"Reference log probs: {log_probs_reference.tolist()}\")\n",
    "print(f\"KL divergence: {kl.item():.3f}\")\n",
    "print(f\"\\nSmall KL -> policy has not changed much (good)\")\n",
    "print(f\"Large KL -> policy has drifted far (penalized)\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn -- TODO Exercises\n",
    "\n",
    "### TODO 1: Complete the GRPO Loss Function\n",
    "\n",
    "Combine all components into a single GRPO loss function.\n",
    "```"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_loss(log_probs_new, log_probs_old, log_probs_ref,\n",
    "              rewards, epsilon=0.2, beta=0.1):\n",
    "    \"\"\"\n",
    "    Complete GRPO loss with group-relative advantages, clipping, and KL penalty.\n",
    "\n",
    "    Args:\n",
    "        log_probs_new: Log probs under current policy, shape (G,)\n",
    "        log_probs_old: Log probs under old policy (before update), shape (G,)\n",
    "        log_probs_ref: Log probs under reference policy (frozen), shape (G,)\n",
    "        rewards: Rewards for each completion, shape (G,)\n",
    "        epsilon: Clipping parameter\n",
    "        beta: KL penalty weight\n",
    "\n",
    "    Returns:\n",
    "        loss: Scalar loss to minimize\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Compute group-relative advantages\n",
    "    advantages = ???  # YOUR CODE HERE\n",
    "\n",
    "    # Step 2: Compute probability ratios\n",
    "    ratios = ???  # YOUR CODE HERE: exp(log_new - log_old)\n",
    "\n",
    "    # Step 3: Compute KL penalty for each completion\n",
    "    kl_penalties = ???  # YOUR CODE HERE: log_new - log_ref\n",
    "\n",
    "    # Step 4: Adjust rewards with KL penalty\n",
    "    adjusted_rewards = rewards - beta * kl_penalties.detach()\n",
    "\n",
    "    # Step 5: Recompute advantages with adjusted rewards\n",
    "    advantages = compute_group_advantages(adjusted_rewards)\n",
    "\n",
    "    # Step 6: Compute clipped surrogate objective\n",
    "    unclipped = ratios * advantages\n",
    "    clipped = torch.clamp(ratios, 1 - epsilon, 1 + epsilon) * advantages\n",
    "    surrogate = torch.min(unclipped, clipped)\n",
    "\n",
    "    # Step 7: The loss is the negative mean (we want to maximize the objective)\n",
    "    loss = -surrogate.mean()\n",
    "    # ==============================\n",
    "\n",
    "    return loss"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: Test your GRPO loss function\n",
    "log_probs_new = torch.tensor([-1.0, -2.0, -0.5, -3.0])\n",
    "log_probs_old = torch.tensor([-1.1, -1.9, -0.6, -2.8])\n",
    "log_probs_ref = torch.tensor([-1.2, -2.1, -0.7, -3.1])\n",
    "rewards = torch.tensor([1.0, 0.0, 1.0, 0.0])\n",
    "\n",
    "try:\n",
    "    loss = grpo_loss(log_probs_new, log_probs_old, log_probs_ref, rewards)\n",
    "    print(f\"GRPO Loss: {loss.item():.4f}\")\n",
    "    assert isinstance(loss.item(), float), \"Loss should be a scalar float\"\n",
    "    print(\"Your GRPO loss function works correctly!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Check your implementation above.\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Visualize How KL Penalty Affects Total Reward\n",
    "\n",
    "Vary beta from 0 to 1 and plot how the total reward changes for a fixed outcome reward and KL divergence.\n",
    "```"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO ============\n",
    "# Plot total_reward = R_outcome - beta * D_KL\n",
    "# for beta in [0, 0.01, 0.05, 0.1, 0.2, 0.5, 1.0]\n",
    "# and D_KL values of [1.0, 5.0, 10.0]\n",
    "#\n",
    "# betas = ???\n",
    "# kl_values = ???\n",
    "# R_outcome = 1.0\n",
    "#\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# for kl in kl_values:\n",
    "#     total_rewards = ???\n",
    "#     plt.plot(betas, total_rewards, label=f'KL = {kl}')\n",
    "# plt.xlabel('Beta')\n",
    "# plt.ylabel('Total Reward')\n",
    "# plt.legend()\n",
    "# plt.title('Effect of KL Penalty on Total Reward')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.show()\n",
    "# =============================="
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us run a complete GRPO step on simulated data to see all the pieces working together.\n",
    "```"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated GRPO training step\n",
    "G = 8  # Group size\n",
    "NUM_STEPS = 100\n",
    "\n",
    "# Simulate a model that gradually improves\n",
    "reward_history = []\n",
    "loss_history = []\n",
    "\n",
    "for step in range(NUM_STEPS):\n",
    "    # Simulate improving rewards over time\n",
    "    base_prob = min(0.3 + step * 0.005, 0.8)\n",
    "    rewards = torch.bernoulli(torch.full((G,), base_prob))\n",
    "\n",
    "    # Simulated log probabilities (getting closer to optimal)\n",
    "    log_probs_new = -torch.rand(G) * (2.0 - step * 0.01)\n",
    "    log_probs_old = log_probs_new - 0.1 * torch.randn(G)\n",
    "    log_probs_ref = -torch.rand(G) * 2.0\n",
    "\n",
    "    # Compute GRPO components\n",
    "    advantages = compute_group_advantages(rewards)\n",
    "    ratios = torch.exp(log_probs_new - log_probs_old)\n",
    "    obj = compute_clipped_objective(ratios, advantages)\n",
    "\n",
    "    loss = -obj.mean()\n",
    "\n",
    "    reward_history.append(rewards.mean().item())\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "print(f\"Initial avg reward: {sum(reward_history[:5])/5:.3f}\")\n",
    "print(f\"Final avg reward:   {sum(reward_history[-5:])/5:.3f}\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Checkpoint: Simulated GRPO Training\n",
    "```"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Reward curve\n",
    "window = 10\n",
    "smoothed_rewards = [sum(reward_history[max(0,i-window):i+1]) / min(i+1, window)\n",
    "                    for i in range(len(reward_history))]\n",
    "ax1.plot(smoothed_rewards, color='#4CAF50', linewidth=2)\n",
    "ax1.set_xlabel('Training Step', fontsize=11)\n",
    "ax1.set_ylabel('Average Reward', fontsize=11)\n",
    "ax1.set_title('Reward Over Training', fontsize=13)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss curve\n",
    "smoothed_loss = [sum(loss_history[max(0,i-window):i+1]) / min(i+1, window)\n",
    "                 for i in range(len(loss_history))]\n",
    "ax2.plot(smoothed_loss, color='#2196F3', linewidth=2)\n",
    "ax2.set_xlabel('Training Step', fontsize=11)\n",
    "ax2.set_ylabel('GRPO Loss', fontsize=11)\n",
    "ax2.set_title('Loss Over Training', fontsize=13)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "In a real setting, the GRPO training loop would:\n",
    "1. Sample prompts from the GSM8K dataset\n",
    "2. Generate G completions from the current model\n",
    "3. Score each completion with the verifiable reward\n",
    "4. Compute advantages, ratios, and the clipped loss\n",
    "5. Backpropagate and update the model\n",
    "\n",
    "The key insight: because we use group-relative baselines instead of a learned value function, we save memory and complexity. The group itself provides the baseline.\n",
    "```"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output\n",
    "\n",
    "We have built GRPO from scratch! Here is a summary of the components:\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| Group-relative advantages | Replace the value network with simple statistics |\n",
    "| Probability ratio | Measure how much the policy changed |\n",
    "| Clipping | Prevent catastrophic policy updates |\n",
    "| KL penalty | Prevent drift from the reference model |\n",
    "```"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Think About This\n",
    "1. What happens if the group size G is very small (e.g., 2)? Very large (e.g., 64)?\n",
    "2. How does the choice of epsilon affect training stability vs speed?\n",
    "3. Why is GRPO particularly well-suited for reasoning tasks where rewards are binary?\n",
    "\n",
    "### What Comes Next\n",
    "In Notebook 3, we will put everything together \u2014 combining SFT from Notebook 1 with GRPO from this notebook to train a full reasoning model on the GSM8K math dataset.\n",
    "\n",
    "### Key Takeaway\n",
    "GRPO's genius is its simplicity: by using group statistics instead of a learned critic, it eliminates an entire network while maintaining the benefits of policy optimization with trust regions.\n",
    "```"
   ],
   "id": "cell_28"
  }
 ]
}