{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "04_emergent_reasoning_and_distillation"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emergent Reasoning Behaviors and Distillation -- Vizuara\n",
    "\n",
    "In this notebook, we explore the fascinating emergent behaviors that arise when models are trained with RL, and implement a simple rejection sampling and distillation pipeline.\n",
    "\n",
    "**What you will build:** Analysis tools for detecting emergent reasoning patterns, and a distillation pipeline that transfers reasoning from a large model to a small one."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "torch.manual_seed(42)"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "When DeepSeek trained their R1 model with RL, something unexpected happened. The model spontaneously developed reasoning strategies that were never taught:\n",
    "- **Self-verification:** checking its own calculations\n",
    "- **Backtracking:** catching and correcting mistakes\n",
    "- **Adaptive depth:** thinking longer on harder problems\n",
    "\n",
    "These behaviors emerged purely from the reward signal. Understanding how and why they emerge is crucial for building better reasoning models. And distillation lets us transfer these capabilities to smaller, more deployable models."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Imagine you are training for a math competition. Nobody tells you to double-check your work. Nobody tells you to re-read the problem when confused. But over time, you discover that these strategies help you score higher.\n",
    "\n",
    "RL does the same thing for language models. The model discovers reasoning strategies because they lead to higher rewards, not because they are explicitly taught.\n",
    "\n",
    "### Think About This\n",
    "- Why would self-verification emerge from a binary (correct/incorrect) reward?\n",
    "- Why would the model learn to produce longer reasoning for harder problems?"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Rejection Sampling for Distillation\n",
    "\n",
    "Given a large model $\\pi_L$ and a dataset of prompts $\\{x_1, \\ldots, x_N\\}$:\n",
    "\n",
    "1. For each prompt $x_i$, sample $K$ completions: $y_{i,1}, \\ldots, y_{i,K} \\sim \\pi_L(\\cdot | x_i)$\n",
    "2. Keep only correct completions: $\\mathcal{D}_{\\text{filtered}} = \\{(x_i, y_{i,k}) : R(y_{i,k}) = 1\\}$\n",
    "3. Train small model $\\pi_S$ via SFT on $\\mathcal{D}_{\\text{filtered}}$\n",
    "\n",
    "$$\\mathcal{L}_{\\text{distill}} = -\\sum_{(x,y) \\in \\mathcal{D}_{\\text{filtered}}} \\sum_t \\log p_{\\pi_S}(y_t | y_{<t}, x)$$\n",
    "\n",
    "**Computational meaning:** We use the large model as a \"solution generator,\" filter for quality, then train the small model on the best solutions. The small model learns from the large model's successful reasoning patterns."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 Analyzing Emergent Behaviors\n",
    "\n",
    "Let us build tools to detect emergent reasoning patterns in model outputs."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_self_verification(text):\n",
    "    \"\"\"Detect if the model checks its own work.\"\"\"\n",
    "    patterns = [\n",
    "        r'[Ll]et me (verify|check|double.?check)',\n",
    "        r'[Vv]erif(y|ying|ication)',\n",
    "        r'[Cc]heck(ing)?\\s*(this|that|my|the)',\n",
    "        r'[Yy]es,?\\s*(that|this) is correct',\n",
    "        r'[Cc]orrect[.!]',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        if re.search(pattern, text):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def detect_backtracking(text):\n",
    "    \"\"\"Detect if the model corrects a mistake.\"\"\"\n",
    "    patterns = [\n",
    "        r'[Ww]ait',\n",
    "        r'[Aa]ctually',\n",
    "        r'[Ll]et me reconsider',\n",
    "        r'[Tt]hat\\'s (wrong|incorrect|not right)',\n",
    "        r'[Ii] made (a|an) (error|mistake)',\n",
    "        r'[Ll]et me (redo|recalculate)',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        if re.search(pattern, text):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def measure_reasoning_depth(text):\n",
    "    \"\"\"Count the number of reasoning steps.\"\"\"\n",
    "    steps = re.findall(r'[Ss]tep \\d+', text)\n",
    "    if steps:\n",
    "        return len(steps)\n",
    "    # Count sentences in the think block\n",
    "    think_match = re.search(r'<think>(.*?)</think>', text, re.DOTALL)\n",
    "    if think_match:\n",
    "        sentences = [s.strip() for s in think_match.group(1).split('.') if s.strip()]\n",
    "        return len(sentences)\n",
    "    return 0\n",
    "\n",
    "# Test on example reasoning traces\n",
    "examples = [\n",
    "    \"<think>\\nStep 1: 3 * 7 = 21. Step 2: Let me verify: 7+7+7 = 21. Correct.\\n</think>\\nThe answer is 21.\",\n",
    "    \"<think>\\nStep 1: 5 * 3 = 18. Wait, that's wrong. 5 * 3 = 15. Let me recalculate.\\n</think>\\nThe answer is 15.\",\n",
    "    \"<think>\\nStep 1: 2 + 2 = 4.\\n</think>\\nThe answer is 4.\",\n",
    "    \"<think>\\nStep 1: First, find 20% of 150.\\nStep 2: 20% = 0.20.\\nStep 3: 0.20 * 150 = 30.\\nStep 4: Let me check: 10% of 150 = 15, so 20% = 30. Yes, correct.\\n</think>\\nThe answer is 30.\",\n",
    "]\n",
    "\n",
    "print(\"=== Analyzing Reasoning Traces ===\\n\")\n",
    "for i, ex in enumerate(examples):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Self-verification: {detect_self_verification(ex)}\")\n",
    "    print(f\"  Backtracking: {detect_backtracking(ex)}\")\n",
    "    print(f\"  Reasoning depth: {measure_reasoning_depth(ex)} steps\")\n",
    "    print()"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Checkpoint: Emergent Behavior Frequency"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate analyzing many outputs from a trained model\n",
    "# In practice, these would come from actual model generations\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulated data: behavior frequency over training\n",
    "training_steps = np.arange(0, 1000, 50)\n",
    "\n",
    "# Self-verification increases with training\n",
    "verification_rate = 1 / (1 + np.exp(-(training_steps - 300) / 100)) * 0.6 + np.random.randn(len(training_steps)) * 0.03\n",
    "\n",
    "# Backtracking peaks mid-training then stabilizes\n",
    "backtracking_rate = 0.3 * np.exp(-((training_steps - 500) ** 2) / (2 * 200**2)) + 0.1 + np.random.randn(len(training_steps)) * 0.02\n",
    "\n",
    "# Average reasoning depth increases\n",
    "avg_depth = 2 + 3 / (1 + np.exp(-(training_steps - 400) / 150)) + np.random.randn(len(training_steps)) * 0.3\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "axes[0].plot(training_steps, np.clip(verification_rate, 0, 1), color='#4CAF50', linewidth=2)\n",
    "axes[0].set_xlabel('Training Steps'); axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Self-Verification Rate'); axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "axes[1].plot(training_steps, np.clip(backtracking_rate, 0, 1), color='#FF9800', linewidth=2)\n",
    "axes[1].set_xlabel('Training Steps'); axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Backtracking Rate'); axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(0, 0.5)\n",
    "\n",
    "axes[2].plot(training_steps, np.clip(avg_depth, 1, 8), color='#2196F3', linewidth=2)\n",
    "axes[2].set_xlabel('Training Steps'); axes[2].set_ylabel('Avg Steps')\n",
    "axes[2].set_title('Average Reasoning Depth'); axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Emergent Behaviors During RL Training', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Rejection Sampling\n",
    "\n",
    "The core of distillation: generate many solutions, keep only the correct ones."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejection_sampling(generator_fn, problems, K=8):\n",
    "    \"\"\"\n",
    "    Generate K solutions per problem, keep only correct ones.\n",
    "\n",
    "    Args:\n",
    "        generator_fn: Function that generates a completion given a prompt\n",
    "        problems: List of (prompt, ground_truth) tuples\n",
    "        K: Number of solutions to generate per problem\n",
    "\n",
    "    Returns:\n",
    "        filtered_data: List of (prompt, correct_completion) tuples\n",
    "    \"\"\"\n",
    "    filtered_data = []\n",
    "    stats = {\"total_generated\": 0, \"total_kept\": 0}\n",
    "\n",
    "    for prompt, truth in problems:\n",
    "        correct_solutions = []\n",
    "        for _ in range(K):\n",
    "            completion = generator_fn(prompt)\n",
    "            stats[\"total_generated\"] += 1\n",
    "\n",
    "            # Check if correct\n",
    "            predicted = re.search(r'[Tt]he answer is[:\\s]*(\\-?[\\d,\\.]+)', completion)\n",
    "            if predicted and predicted.group(1).strip('.') == truth:\n",
    "                correct_solutions.append(completion)\n",
    "\n",
    "        # Keep all correct solutions\n",
    "        for sol in correct_solutions:\n",
    "            filtered_data.append((prompt, sol))\n",
    "            stats[\"total_kept\"] += 1\n",
    "\n",
    "    return filtered_data, stats\n",
    "\n",
    "# Simulate a \"large model\" that gets 60% of problems right\n",
    "def simulated_large_model(prompt):\n",
    "    \"\"\"Simulate a large model generating chain-of-thought solutions.\"\"\"\n",
    "    # In reality, this would be model.generate(...)\n",
    "    # Here we simulate with template-based responses\n",
    "    if np.random.random() < 0.6:  # 60% correct\n",
    "        # Generate a correct chain-of-thought\n",
    "        numbers = re.findall(r'\\d+', prompt)\n",
    "        if len(numbers) >= 2:\n",
    "            a, b = int(numbers[0]), int(numbers[1])\n",
    "            if '*' in prompt or 'times' in prompt:\n",
    "                answer = a * b\n",
    "                return f\"<think>\\nStep 1: {a} * {b} = {answer}.\\nStep 2: Let me verify: {answer}.\\n</think>\\nThe answer is {answer}.\"\n",
    "            elif '+' in prompt:\n",
    "                answer = a + b\n",
    "                return f\"<think>\\nStep 1: {a} + {b} = {answer}.\\n</think>\\nThe answer is {answer}.\"\n",
    "            elif '-' in prompt:\n",
    "                answer = a - b\n",
    "                return f\"<think>\\nStep 1: {a} - {b} = {answer}.\\n</think>\\nThe answer is {answer}.\"\n",
    "    # Wrong answer\n",
    "    return f\"<think>\\nI think the answer is 999.\\n</think>\\nThe answer is 999.\"\n",
    "\n",
    "# Test rejection sampling\n",
    "test_problems = [\n",
    "    (\"What is 4 * 8?\", \"32\"),\n",
    "    (\"What is 17 + 25?\", \"42\"),\n",
    "    (\"What is 90 - 34?\", \"56\"),\n",
    "    (\"What is 6 * 7?\", \"42\"),\n",
    "]\n",
    "\n",
    "filtered, stats = rejection_sampling(simulated_large_model, test_problems, K=8)\n",
    "print(f\"Generated: {stats['total_generated']}\")\n",
    "print(f\"Kept (correct): {stats['total_kept']}\")\n",
    "print(f\"Acceptance rate: {stats['total_kept']/stats['total_generated']*100:.1f}%\")\n",
    "print(f\"\\nFiltered dataset size: {len(filtered)}\")\n",
    "for prompt, sol in filtered[:3]:\n",
    "    print(f\"\\n  Q: {prompt}\")\n",
    "    print(f\"  A: {sol[:100]}\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Distillation: Train a Smaller Model"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill(small_model, tokenizer, filtered_data, epochs=20, lr=5e-5):\n",
    "    \"\"\"Fine-tune a small model on filtered (correct) solutions from the large model.\"\"\"\n",
    "    optimizer = torch.optim.AdamW(small_model.parameters(), lr=lr)\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        small_model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for prompt, completion in filtered_data:\n",
    "            text = f\"Question: {prompt}\\n{completion}\"\n",
    "            tokens = tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
    "                             max_length=256, padding=\"max_length\")\n",
    "            tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "            labels = tokens[\"input_ids\"].clone()\n",
    "            labels[tokens[\"attention_mask\"] == 0] = -100\n",
    "\n",
    "            loss = small_model(**tokens, labels=labels).loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / max(len(filtered_data), 1)\n",
    "        losses.append(avg_loss)\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Distillation Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "# Create a fresh small model for distillation\n",
    "from transformers import AutoModelForCausalLM\n",
    "small_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "small_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(\"Distilling large model reasoning into small model...\")\n",
    "distill_losses = distill(small_model, tokenizer, filtered)\n",
    "print(\"Distillation complete!\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Checkpoint: Distillation Training"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(distill_losses, color='#9C27B0', linewidth=2)\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "plt.title('Distillation Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn -- TODO Exercises\n",
    "\n",
    "### TODO 1: Vary Acceptance Rate\n",
    "\n",
    "Modify the rejection sampling to use different acceptance thresholds and observe how dataset size changes."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO ============\n",
    "# Run rejection_sampling with K = 2, 4, 8, 16, 32\n",
    "# Plot: K vs acceptance rate and K vs filtered dataset size\n",
    "#\n",
    "# k_values = [2, 4, 8, 16, 32]\n",
    "# acceptance_rates = []\n",
    "# dataset_sizes = []\n",
    "#\n",
    "# for k in k_values:\n",
    "#     filtered, stats = rejection_sampling(simulated_large_model, test_problems, K=k)\n",
    "#     rate = stats['total_kept'] / stats['total_generated']\n",
    "#     acceptance_rates.append(rate)\n",
    "#     dataset_sizes.append(len(filtered))\n",
    "#\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "# ax1.plot(k_values, acceptance_rates, 'o-')\n",
    "# ax1.set_xlabel('K (samples per problem)')\n",
    "# ax1.set_ylabel('Acceptance Rate')\n",
    "# ax2.plot(k_values, dataset_sizes, 'o-')\n",
    "# ax2.set_xlabel('K'); ax2.set_ylabel('Filtered Dataset Size')\n",
    "# plt.tight_layout(); plt.show()\n",
    "# =============================="
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Analyze Reasoning Depth vs Problem Difficulty\n",
    "\n",
    "Create problems of varying difficulty and measure how reasoning depth changes."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO ============\n",
    "# Create easy, medium, and hard problems\n",
    "# Generate solutions and measure average reasoning depth for each\n",
    "#\n",
    "# easy = [(\"What is 2 + 3?\", \"5\"), (\"What is 4 * 2?\", \"8\")]\n",
    "# medium = [(\"What is 15% of 200?\", \"30\"), (\"What is 125 - 67?\", \"58\")]\n",
    "# hard = [(\"If 3 boxes of 12 items each, remove 7. How many left?\", \"29\")]\n",
    "#\n",
    "# Measure reasoning depth for each difficulty level\n",
    "# =============================="
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us run the complete distillation pipeline and compare the distilled model with the original."
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the distilled model on test problems\n",
    "print(\"=== Distilled Model Evaluation ===\\n\")\n",
    "from transformers import AutoModelForCausalLM as AMLM\n",
    "\n",
    "def generate_from_model(model, tokenizer, prompt, max_new=100):\n",
    "    text = f\"Question: {prompt}\\n\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=max_new,\n",
    "                           do_sample=True, temperature=0.7,\n",
    "                           pad_token_id=tokenizer.eos_token_id)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=False)[len(text):]\n",
    "\n",
    "for prompt, truth in test_problems:\n",
    "    comp = generate_from_model(small_model, tokenizer, prompt)\n",
    "    r = compute_reward(comp, truth) if 'compute_reward' in dir() else 0\n",
    "    print(f\"Q: {prompt} (Truth: {truth})\")\n",
    "    print(f\"A: {comp[:200]}\")\n",
    "    print(f\"Correct: {'Yes' if r else 'Check manually'}\")\n",
    "    print(\"-\" * 50)"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "The distillation pipeline demonstrates a key finding from DeepSeek-R1:\n",
    "- A 7B model trained on distilled data from the 671B model can outperform the 70B model trained with RL directly\n",
    "- The quality of the training data matters more than the training procedure\n",
    "- Rejection sampling is a simple but powerful way to curate high-quality data"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output\n",
    "\n",
    "We have built a complete understanding of reasoning model training:\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| Emergent behavior analysis | Detect self-verification, backtracking, depth |\n",
    "| Rejection sampling | Generate many, keep only correct solutions |\n",
    "| Distillation via SFT | Transfer reasoning from large to small model |\n",
    "\n",
    "The complete pipeline: **Large model + RL -> Generate solutions -> Filter correct -> Train small model**"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Think About This\n",
    "1. Is distillation \"cheating\"? The small model never did RL itself.\n",
    "2. What reasoning behaviors might a 671B model develop that a 7B model cannot?\n",
    "3. Could we iterate: distill, then RL-train the distilled model, then distill again?\n",
    "\n",
    "### Key Takeaway\n",
    "Emergent reasoning behaviors (self-verification, backtracking, adaptive depth) arise naturally from RL with verifiable rewards. Distillation is a practical way to make these capabilities accessible in smaller models. The combination of RL for discovery and SFT for transfer is one of the most powerful paradigms in modern AI."
   ],
   "id": "cell_25"
  }
 ]
}
