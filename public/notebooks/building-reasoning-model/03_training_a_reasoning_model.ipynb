{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "03_training_a_reasoning_model"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Reasoning Model End-to-End -- Vizuara\n",
    "\n",
    "In this notebook, we combine SFT and GRPO to train a language model that reasons through math problems. This is the full pipeline from base model to reasoning model.\n",
    "\n",
    "**What you will build:** A model fine-tuned with SFT then GRPO on math problems, producing chain-of-thought reasoning.\n",
    "```"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"PyTorch: {torch.__version__}, Device: {device}\")\n",
    "torch.manual_seed(42)"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In Notebooks 1 and 2, we built individual components: SFT and GRPO. Now we combine them into the full pipeline. By the end, you will see a small model develop step-by-step reasoning ability.\n",
    "\n",
    "**Teaser output from our trained model:**\n",
    "```\n",
    "Q: If a box has 3 rows of 4 apples, remove 5. How many left?\n",
    "<think> 3 rows of 4 = 3*4 = 12. Remove 5: 12-5 = 7. </think>\n",
    "The answer is 7.\n",
    "```\n",
    "```"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Think of two-phase training like teaching a student:\n",
    "- **Phase 1 (SFT):** Show worked examples. Student learns the *format*.\n",
    "- **Phase 2 (GRPO):** Practice exams with grading. Student learns *what works*.\n",
    "\n",
    "Neither alone suffices. SFT without RL = plausible but wrong reasoning. RL without SFT = no idea how to structure reasoning.\n",
    "\n",
    "### Think About This\n",
    "- Why must SFT come before RL?\n",
    "- What if SFT was very long but RL very short?\n",
    "```"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "**SFT loss:** $\\mathcal{L}_{\\text{SFT}} = -\\sum_{t} \\log p_\\theta(y_t | y_{<t}, x)$\n",
    "\n",
    "**GRPO objective:**\n",
    "$$\\mathcal{L}_{\\text{GRPO}} = -\\frac{1}{G}\\sum_{i=1}^{G} \\min\\left(r_i \\hat{A}_i, \\text{clip}(r_i, 1-\\epsilon, 1+\\epsilon) \\hat{A}_i\\right)$$\n",
    "\n",
    "where $\\hat{A}_i = \\frac{R_i - \\bar{R}}{\\sigma_R}$ and $r_i = \\frac{\\pi_\\theta(y_i|x)}{\\pi_{\\text{old}}(y_i|x)}$.\n",
    "\n",
    "**Computationally:** SFT minimizes prediction error on worked examples. GRPO maximizes probability of correct completions relative to the group.\n",
    "```"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 Setup: Model and Data\n",
    "```"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "special_tokens = {\"additional_special_tokens\": [\"<think>\", \"</think>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"Model: {MODEL_NAME} ({sum(p.numel() for p in model.parameters()):,} params)\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT training data\n",
    "sft_data = [\n",
    "    (\"What is 3 * 7?\", \"<think>\\n3 * 7 = 21.\\n</think>\\nThe answer is 21.\"),\n",
    "    (\"What is 15 + 28?\", \"<think>\\n15 + 28 = 43.\\n</think>\\nThe answer is 43.\"),\n",
    "    (\"What is 100 - 37?\", \"<think>\\n100 - 37 = 63.\\n</think>\\nThe answer is 63.\"),\n",
    "    (\"What is 8 * 6?\", \"<think>\\n8 * 6 = 48.\\n</think>\\nThe answer is 48.\"),\n",
    "    (\"What is 50% of 80?\", \"<think>\\n50% = 0.5. 0.5 * 80 = 40.\\n</think>\\nThe answer is 40.\"),\n",
    "    (\"What is 9 * 9?\", \"<think>\\n9 * 9 = 81.\\n</think>\\nThe answer is 81.\"),\n",
    "    (\"What is 200 - 85?\", \"<think>\\n200 - 85 = 115.\\n</think>\\nThe answer is 115.\"),\n",
    "    (\"If 24 cookies shared among 6, how many each?\", \"<think>\\n24 / 6 = 4.\\n</think>\\nThe answer is 4.\"),\n",
    "]\n",
    "\n",
    "# RL problems with ground truth\n",
    "rl_problems = [\n",
    "    (\"What is 4 * 8?\", \"32\"), (\"What is 17 + 25?\", \"42\"),\n",
    "    (\"What is 90 - 34?\", \"56\"), (\"What is 6 * 7?\", \"42\"),\n",
    "    (\"What is 20% of 50?\", \"10\"), (\"3 boxes of 5 items, total?\", \"15\"),\n",
    "    (\"What is 12 * 3?\", \"36\"), (\"What is 150 - 67?\", \"83\"),\n",
    "]\n",
    "print(f\"SFT examples: {len(sft_data)}, RL problems: {len(rl_problems)}\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(text):\n",
    "    \"\"\"Extract numerical answer from model output.\"\"\"\n",
    "    match = re.search(r'[Tt]he answer is[:\\s]*(\\-?[\\d,\\.]+)', text)\n",
    "    if match:\n",
    "        return match.group(1).replace(',', '').strip('.')\n",
    "    numbers = re.findall(r'\\-?[\\d]+', text)\n",
    "    return numbers[-1] if numbers else None\n",
    "\n",
    "def compute_reward(completion, ground_truth):\n",
    "    \"\"\"Binary verifiable reward.\"\"\"\n",
    "    predicted = extract_answer(completion)\n",
    "    return 1.0 if predicted and predicted.strip() == ground_truth.strip() else 0.0\n",
    "\n",
    "# Test\n",
    "print(compute_reward(\"<think>\\n32.\\n</think>\\nThe answer is 32.\", \"32\"))  # Should be 1.0\n",
    "print(compute_reward(\"<think>\\n30.\\n</think>\\nThe answer is 30.\", \"32\"))  # Should be 0.0"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Phase 1: SFT Training\n",
    "```"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sft(model, tokenizer, data, epochs=40, lr=5e-5):\n",
    "    \"\"\"Run SFT warmup phase.\"\"\"\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for prompt, completion in data:\n",
    "            text = f\"Question: {prompt}\\n{completion}\"\n",
    "            tokens = tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
    "                             max_length=256, padding=\"max_length\")\n",
    "            tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "            labels = tokens[\"input_ids\"].clone()\n",
    "            labels[tokens[\"attention_mask\"] == 0] = -100\n",
    "            loss = model(**tokens, labels=labels).loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        losses.append(epoch_loss / len(data))\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"SFT Epoch {epoch+1}, Loss: {losses[-1]:.4f}\")\n",
    "    return losses\n",
    "\n",
    "print(\"Phase 1: SFT Training\")\n",
    "sft_losses = run_sft(model, tokenizer, sft_data)"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Checkpoint: SFT Loss\n",
    "```"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(sft_losses, color='#2196F3', linewidth=2)\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('SFT Training Loss')\n",
    "plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Save Reference Model\n",
    "```"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_model = copy.deepcopy(model)\n",
    "ref_model.eval()\n",
    "for p in ref_model.parameters():\n",
    "    p.requires_grad = False\n",
    "print(\"Reference model saved (frozen)\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Phase 2: GRPO Training\n",
    "```"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_new=128):\n",
    "    \"\"\"Generate completion from model.\"\"\"\n",
    "    text = f\"Question: {prompt}\\n\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=max_new,\n",
    "                           do_sample=True, temperature=0.8,\n",
    "                           pad_token_id=tokenizer.eos_token_id)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=False)[len(text):], out[0]\n",
    "\n",
    "def log_prob(model, toks):\n",
    "    \"\"\"Compute total log probability of a token sequence.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(toks.unsqueeze(0).to(device)).logits[0, :-1]\n",
    "        targets = toks[1:].to(device)\n",
    "        lp = F.log_softmax(logits, dim=-1)\n",
    "        return lp.gather(1, targets.unsqueeze(1)).squeeze().sum().item()"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn -- TODO Exercises\n",
    "\n",
    "### TODO 1: Complete the GRPO Training Loop\n",
    "```"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_train(model, ref_model, tokenizer, problems,\n",
    "               steps=50, G=4, beta=0.05, lr=1e-6):\n",
    "    \"\"\"GRPO training loop.\"\"\"\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    rewards_hist, losses_hist = [], []\n",
    "\n",
    "    for step in range(steps):\n",
    "        idx = np.random.randint(len(problems))\n",
    "        prompt, truth = problems[idx]\n",
    "\n",
    "        # Generate G completions\n",
    "        comps, toks_list = [], []\n",
    "        for _ in range(G):\n",
    "            c, t = generate(model, tokenizer, prompt)\n",
    "            comps.append(c); toks_list.append(t)\n",
    "\n",
    "        # Compute rewards\n",
    "        rewards = torch.tensor([compute_reward(c, truth) for c in comps])\n",
    "\n",
    "        # ============ TODO ============\n",
    "        # Compute group-relative advantages\n",
    "        if rewards.std() < 1e-8:\n",
    "            advantages = torch.zeros_like(rewards)\n",
    "        else:\n",
    "            advantages = ???  # YOUR CODE: (rewards - mean) / std\n",
    "\n",
    "        # Compute log probs\n",
    "        lp_curr = torch.tensor([log_prob(model, t) for t in toks_list])\n",
    "        lp_ref = torch.tensor([log_prob(ref_model, t) for t in toks_list])\n",
    "\n",
    "        # KL penalty\n",
    "        kl = (lp_curr - lp_ref)\n",
    "        adj_rewards = rewards - beta * kl\n",
    "        if adj_rewards.std() > 1e-8:\n",
    "            advantages = (adj_rewards - adj_rewards.mean()) / (adj_rewards.std() + 1e-8)\n",
    "\n",
    "        # Policy gradient update (simplified)\n",
    "        loss = ???  # YOUR CODE: -(lp_curr * advantages).mean()\n",
    "        # ==============================\n",
    "\n",
    "        if hasattr(loss, 'backward'):\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        rewards_hist.append(rewards.mean().item())\n",
    "        if (step+1) % 10 == 0:\n",
    "            print(f\"Step {step+1}, Avg Reward: {np.mean(rewards_hist[-10:]):.3f}\")\n",
    "\n",
    "    return rewards_hist\n",
    "\n",
    "print(\"Phase 2: GRPO Training\")\n",
    "rl_rewards = grpo_train(model, ref_model, tokenizer, rl_problems)"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "assert len(rl_rewards) > 0, \"No rewards recorded\"\n",
    "print(f\"Training complete! Final avg reward: {np.mean(rl_rewards[-10:]):.3f}\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Compare Pre-RL vs Post-RL Outputs\n",
    "```"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO ============\n",
    "# Generate from both ref_model and model on rl_problems[:4]\n",
    "# Compare reasoning quality\n",
    "# for prompt, truth in rl_problems[:4]:\n",
    "#     before, _ = generate(ref_model, tokenizer, prompt)\n",
    "#     after, _ = generate(model, tokenizer, prompt)\n",
    "#     print(f\"Q: {prompt} (Truth: {truth})\")\n",
    "#     print(f\"Before: {before[:150]}\")\n",
    "#     print(f\"After:  {after[:150]}\\n\")\n",
    "# =============================="
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "```"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Final Evaluation ===\\n\")\n",
    "correct = 0\n",
    "for prompt, truth in rl_problems:\n",
    "    comp, _ = generate(model, tokenizer, prompt)\n",
    "    r = compute_reward(comp, truth)\n",
    "    correct += int(r)\n",
    "    print(f\"Q: {prompt} | Truth: {truth} | {'CORRECT' if r else 'WRONG'}\")\n",
    "    print(f\"  {comp[:200]}\\n\")\n",
    "print(f\"Accuracy: {correct}/{len(rl_problems)} ({100*correct/len(rl_problems):.0f}%)\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "```"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax1.plot(sft_losses, color='#2196F3', linewidth=2)\n",
    "ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Phase 1: SFT'); ax1.grid(True, alpha=0.3)\n",
    "\n",
    "w = 5\n",
    "sm = [np.mean(rl_rewards[max(0,i-w):i+1]) for i in range(len(rl_rewards))]\n",
    "ax2.plot(sm, color='#4CAF50', linewidth=2)\n",
    "ax2.set_xlabel('Step'); ax2.set_ylabel('Avg Reward')\n",
    "ax2.set_title('Phase 2: GRPO'); ax2.set_ylim(-0.1, 1.1); ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Complete Training Pipeline', fontsize=14, y=1.02)\n",
    "plt.tight_layout(); plt.show()"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output\n",
    "\n",
    "We trained a reasoning model through the full two-phase pipeline:\n",
    "1. **SFT:** Learned the `<think>` format from worked examples\n",
    "2. **GRPO:** Learned which reasoning strategies lead to correct answers\n",
    "\n",
    "Even with GPT-2 (124M params) and 8 toy problems, we observe the model developing reasoning behavior. At DeepSeek scale (671B params, GSM8K), this produces remarkable emergent reasoning.\n",
    "```"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Think About This\n",
    "1. How would a larger model and dataset change the results?\n",
    "2. DeepSeek-R1 found RL alone (no SFT) can work. Why?\n",
    "3. What happens if we increase the group size G from 4 to 64?\n",
    "\n",
    "### What Comes Next\n",
    "Notebook 4 covers emergent behaviors (self-verification, backtracking) and distillation from large to small models.\n",
    "\n",
    "### Key Takeaway\n",
    "SFT provides scaffolding (format). RL fills it with substance (correctness). Together they produce genuine reasoning.\n",
    "```"
   ],
   "id": "cell_29"
  }
 ]
}