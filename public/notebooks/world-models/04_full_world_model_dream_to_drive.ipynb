{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Full World Model: Dream to Drive ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1eON7mr-rji_beKMz7kB1vmHnfJTuQczF\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Full World Model: Dream to Drive from First Principles\n",
    "\n",
    "*Part 4 of the Vizuara series on World Models*\n",
    "*Estimated time: 60 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ü§ñ AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** ‚Äî it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[üëâ Open AI Teaching Assistant](https://course-creator-brown.vercel.app/courses/world-models/practice/4/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_why_it_matters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous three notebooks, we built each component of the World Model separately:\n",
    "- **V (Vision)**: A VAE that compresses images into latent codes\n",
    "- **M (Memory)**: An MDN-RNN that predicts future latent states\n",
    "- **C (Controller)**: A linear policy trained with CMA-ES\n",
    "\n",
    "Now it is time to wire them together and see the complete pipeline in action. The agent will:\n",
    "1. Collect observations from a real environment\n",
    "2. Train V and M on those observations\n",
    "3. Use the trained V and M to *dream* ‚Äî simulate future experiences\n",
    "4. Evolve a Controller C entirely inside these dreams\n",
    "5. Deploy the trained agent back to the real environment\n",
    "\n",
    "This is the full World Model loop. The agent learns to drive by practicing in its own imagination.\n",
    "\n",
    "By the end of this notebook, you will see the entire pipeline working end-to-end: data collection, VAE training, MDN-RNN training, dream-based controller evolution, and finally ‚Äî the agent driving in the real environment and in its own dreams, side by side.\n",
    "\n",
    "We will use a simplified 2D navigation environment (fast and visual) to keep training times under 10 minutes on a T4 GPU."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle, FancyArrowPatch\n",
    "from matplotlib.collections import LineCollection\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Intuition And Model Exploitation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_intuition_and_model_exploitation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_intuition_and_model_exploitation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### The Flight Simulator Analogy\n",
    "\n",
    "Real flight data was used to build a flight simulator. Once the simulator exists, a pilot can train for thousands of hours without ever flying a real airplane ‚Äî no fuel costs, no weather risks, no danger.\n",
    "\n",
    "The World Model works exactly the same way:\n",
    "- **Phase 1 (Build the simulator)**: Collect real data ‚Üí Train V and M ‚Üí Now we have a \"dream simulator\"\n",
    "- **Phase 2 (Train in the simulator)**: Evolve Controller C by running it inside the dream\n",
    "\n",
    "The key insight: after Phase 1, **no more real-world interaction is needed**. The Controller trains purely in imagination.\n",
    "\n",
    "### ü§î Think About This\n",
    "\n",
    "What could go wrong with this approach? If the dream is slightly inaccurate ‚Äî say, the agent discovers that the dream has a \"cheat code\" where a certain action sequence always gives high reward (even though it would not work in reality) ‚Äî the controller will learn to exploit this bug. This is called **model exploitation**, and it is one of the biggest challenges in model-based RL.\n",
    "\n",
    "Think of a pilot who trains in a simulator where gravity is 5% weaker. They learn to fly, but their intuitions about landing and turning are slightly off. When they get into a real plane, those small errors compound."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Environment\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_environment.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_environment"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Environment: 2D Navigation\n",
    "\n",
    "We will build a simple but rich 2D environment. The agent is a circle that must reach a goal while avoiding obstacles. The observation is a 32√ó32 image (rendered top-down view), and the actions are continuous 2D velocities."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNavEnv:\n",
    "    \"\"\"\n",
    "    Simple 2D navigation environment with image observations.\n",
    "    Agent must reach the goal while staying in bounds.\n",
    "    \"\"\"\n",
    "    def __init__(self, size=5.0, img_size=32):\n",
    "        self.size = size\n",
    "        self.img_size = img_size\n",
    "        self.agent_pos = None\n",
    "        self.goal_pos = None\n",
    "        self.obstacles = []\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_pos = np.array([0.0, 0.0])\n",
    "        self.goal_pos = np.array([\n",
    "            np.random.uniform(2.0, 4.0),\n",
    "            np.random.uniform(-3.0, 3.0)\n",
    "        ])\n",
    "        # Random obstacles\n",
    "        self.obstacles = []\n",
    "        for _ in range(3):\n",
    "            pos = np.array([\n",
    "                np.random.uniform(-3.0, 3.0),\n",
    "                np.random.uniform(-3.0, 3.0)\n",
    "            ])\n",
    "            # Ensure obstacles are not too close to start or goal\n",
    "            if np.linalg.norm(pos) > 1.5 and np.linalg.norm(pos - self.goal_pos) > 1.5:\n",
    "                self.obstacles.append(pos)\n",
    "        return self._render()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action: 2D velocity, clipped to [-1, 1]\n",
    "        Returns:\n",
    "            obs (image), reward, done\n",
    "        \"\"\"\n",
    "        action = np.clip(action, -1, 1)\n",
    "        self.agent_pos = self.agent_pos + action * 0.3\n",
    "\n",
    "        # Compute reward\n",
    "        dist_to_goal = np.linalg.norm(self.agent_pos - self.goal_pos)\n",
    "        reward = -dist_to_goal * 0.1  # Closer = better\n",
    "\n",
    "        # Bonus for reaching goal\n",
    "        done = False\n",
    "        if dist_to_goal < 0.5:\n",
    "            reward += 10.0\n",
    "            done = True\n",
    "\n",
    "        # Penalty for hitting obstacles\n",
    "        for obs_pos in self.obstacles:\n",
    "            if np.linalg.norm(self.agent_pos - obs_pos) < 0.5:\n",
    "                reward -= 2.0\n",
    "\n",
    "        # Penalty for going out of bounds\n",
    "        if np.any(np.abs(self.agent_pos) > self.size):\n",
    "            reward -= 1.0\n",
    "            self.agent_pos = np.clip(self.agent_pos, -self.size, self.size)\n",
    "\n",
    "        return self._render(), reward, done\n",
    "\n",
    "    def _render(self):\n",
    "        \"\"\"Render the environment as a 32x32 RGB image.\"\"\"\n",
    "        img = np.zeros((self.img_size, self.img_size, 3), dtype=np.float32)\n",
    "\n",
    "        def to_pixel(pos):\n",
    "            px = int((pos[0] + self.size) / (2 * self.size) * (self.img_size - 1))\n",
    "            py = int((pos[1] + self.size) / (2 * self.size) * (self.img_size - 1))\n",
    "            return np.clip(px, 0, self.img_size - 1), np.clip(py, 0, self.img_size - 1)\n",
    "\n",
    "        # Draw goal (green)\n",
    "        gx, gy = to_pixel(self.goal_pos)\n",
    "        for dx in range(-2, 3):\n",
    "            for dy in range(-2, 3):\n",
    "                x, y = np.clip(gx + dx, 0, self.img_size-1), np.clip(gy + dy, 0, self.img_size-1)\n",
    "                img[y, x] = [0, 1, 0]\n",
    "\n",
    "        # Draw obstacles (red)\n",
    "        for obs_pos in self.obstacles:\n",
    "            ox, oy = to_pixel(obs_pos)\n",
    "            for dx in range(-1, 2):\n",
    "                for dy in range(-1, 2):\n",
    "                    x, y = np.clip(ox + dx, 0, self.img_size-1), np.clip(oy + dy, 0, self.img_size-1)\n",
    "                    img[y, x] = [1, 0, 0]\n",
    "\n",
    "        # Draw agent (blue)\n",
    "        ax, ay = to_pixel(self.agent_pos)\n",
    "        for dx in range(-1, 2):\n",
    "            for dy in range(-1, 2):\n",
    "                x, y = np.clip(ax + dx, 0, self.img_size-1), np.clip(ay + dy, 0, self.img_size-1)\n",
    "                img[y, x] = [0, 0.5, 1]\n",
    "\n",
    "        return img\n",
    "\n",
    "# Test the environment\n",
    "env = SimpleNavEnv()\n",
    "obs = env.reset()\n",
    "print(f\"Observation shape: {obs.shape}\")\n",
    "print(f\"Observation range: [{obs.min():.2f}, {obs.max():.2f}]\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize the environment\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "env = SimpleNavEnv()\n",
    "obs = env.reset()\n",
    "axes[0].imshow(obs)\n",
    "axes[0].set_title('t=0 (start)', fontsize=11)\n",
    "axes[0].axis('off')\n",
    "\n",
    "for i in range(1, 5):\n",
    "    action = np.random.uniform(-1, 1, size=2)\n",
    "    obs, reward, done = env.step(action)\n",
    "    axes[i].imshow(obs)\n",
    "    axes[i].set_title(f't={i*5} (r={reward:.2f})', fontsize=11)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('2D Navigation Environment: Blue=Agent, Green=Goal, Red=Obstacles', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Collect Data\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_collect_data.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_collect_data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phase 1: Collect Real Data\n",
    "\n",
    "The first step is to collect a dataset of real interactions using a random policy."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(n_episodes=200, max_steps=50):\n",
    "    \"\"\"Collect rollouts from the real environment using random actions.\"\"\"\n",
    "    all_observations = []\n",
    "    all_actions = []\n",
    "    all_rewards = []\n",
    "\n",
    "    env = SimpleNavEnv()\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        episode_obs = [obs]\n",
    "        episode_actions = []\n",
    "        episode_rewards = []\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = np.random.uniform(-1, 1, size=2)\n",
    "            next_obs, reward, done = env.step(action)\n",
    "\n",
    "            episode_obs.append(next_obs)\n",
    "            episode_actions.append(action)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        all_observations.append(np.array(episode_obs))\n",
    "        all_actions.append(np.array(episode_actions))\n",
    "        all_rewards.append(np.array(episode_rewards))\n",
    "\n",
    "    return all_observations, all_actions, all_rewards\n",
    "\n",
    "print(\"Collecting data from real environment with random policy...\")\n",
    "observations, actions, rewards = collect_data(n_episodes=300, max_steps=50)\n",
    "print(f\"Collected {len(observations)} episodes\")\n",
    "print(f\"Total frames: {sum(len(o) for o in observations)}\")\n",
    "print(f\"Average episode length: {np.mean([len(a) for a in actions]):.1f}\")\n",
    "print(f\"Average episode reward: {np.mean([r.sum() for r in rewards]):.2f}\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Vae Architecture\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_vae_architecture.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_vae_architecture"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Phase 2: Train the Vision (V)"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare flat dataset of all frames for VAE training\n",
    "all_frames = np.concatenate(observations, axis=0)  # (N, 32, 32, 3)\n",
    "# Convert to (N, 3, 32, 32) for PyTorch\n",
    "all_frames_torch = torch.tensor(all_frames, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "print(f\"Total frames for VAE training: {all_frames_torch.shape}\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 8  # Compress 32x32x3=3072 values to 8 numbers\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=LATENT_DIM):\n",
    "        super().__init__()\n",
    "        # Encoder: 32x32x3 -> latent_dim\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 4, stride=2, padding=1),  nn.ReLU(),  # -> 16x16\n",
    "            nn.Conv2d(16, 32, 4, stride=2, padding=1), nn.ReLU(),  # -> 8x8\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1), nn.ReLU(),  # -> 4x4\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(64 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64 * 4 * 4, latent_dim)\n",
    "\n",
    "        # Decoder: latent_dim -> 32x32x3\n",
    "        self.fc_dec = nn.Linear(latent_dim, 64 * 4 * 4)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1), nn.ReLU(),  # -> 8x8\n",
    "            nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1), nn.ReLU(),  # -> 16x16\n",
    "            nn.ConvTranspose2d(16, 3, 4, stride=2, padding=1),  nn.Sigmoid(), # -> 32x32\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.enc(x).flatten(1)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.fc_dec(z).view(-1, 64, 4, 4)\n",
    "        return self.dec(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "def vae_loss(recon, original, mu, logvar):\n",
    "    \"\"\"\n",
    "    Compute VAE loss = Reconstruction + KL Divergence.\n",
    "\n",
    "    Args:\n",
    "        recon: Reconstructed image, shape (batch, 3, 32, 32)\n",
    "        original: Original image, shape (batch, 3, 32, 32)\n",
    "        mu: Encoder mean, shape (batch, latent_dim)\n",
    "        logvar: Encoder log-variance, shape (batch, latent_dim)\n",
    "\n",
    "    Returns:\n",
    "        total_loss, recon_loss, kl_loss\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Compute reconstruction loss using F.binary_cross_entropy\n",
    "    #         with reduction='sum'\n",
    "    #         Hint: F.binary_cross_entropy(recon, original, reduction='sum')\n",
    "    #\n",
    "    # Step 2: Compute KL divergence:\n",
    "    #         KL = -0.5 * sum(1 + logvar - mu^2 - exp(logvar))\n",
    "    #\n",
    "    # Step 3: Return (total, recon, kl)\n",
    "    # ==============================\n",
    "\n",
    "    recon_loss = ???  # YOUR CODE HERE\n",
    "    kl_loss = ???     # YOUR CODE HERE\n",
    "\n",
    "    return recon_loss + kl_loss, recon_loss, kl_loss"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification of your VAE loss\n",
    "test_r = torch.sigmoid(torch.randn(2, 3, 32, 32)).to(device)\n",
    "test_o = torch.rand(2, 3, 32, 32).to(device)\n",
    "test_m = torch.zeros(2, LATENT_DIM).to(device)\n",
    "test_lv = torch.zeros(2, LATENT_DIM).to(device)\n",
    "t_total, t_recon, t_kl = vae_loss(test_r, test_o, test_m, test_lv)\n",
    "assert t_kl.item() == 0.0, f\"‚ùå KL should be 0 when mu=0, logvar=0, got {t_kl.item():.4f}\"\n",
    "assert t_recon.item() > 0, \"‚ùå Reconstruction loss should be positive\"\n",
    "print(f\"‚úÖ VAE loss works! Recon: {t_recon.item():.2f}, KL: {t_kl.item():.2f}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Vae Todo Followup And Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_vae_todo_followup_and_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_vae_todo_followup_and_training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us train the VAE on our collected frames."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the VAE\n",
    "vae = VAE().to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "dataset = torch.utils.data.TensorDataset(all_frames_torch)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "print(\"Training VAE...\")\n",
    "vae_losses = []\n",
    "for epoch in range(20):\n",
    "    vae.train()\n",
    "    epoch_loss = 0\n",
    "    for (batch,) in loader:\n",
    "        batch = batch.to(device)\n",
    "        recon, mu, logvar = vae(batch)\n",
    "        loss, _, _ = vae_loss(recon, batch, mu, logvar)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    avg = epoch_loss / len(all_frames_torch)\n",
    "    vae_losses.append(avg)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"  Epoch {epoch+1:2d}/20 | Loss: {avg:.2f}\")\n",
    "\n",
    "print(\"VAE training complete!\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä VAE results: original vs reconstructed\n",
    "vae.eval()\n",
    "sample_frames = all_frames_torch[:8].to(device)\n",
    "with torch.no_grad():\n",
    "    recon, _, _ = vae(sample_frames)\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(20, 5))\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(sample_frames[i].cpu().permute(1, 2, 0).numpy())\n",
    "    axes[0, i].axis('off')\n",
    "    axes[1, i].imshow(recon[i].cpu().permute(1, 2, 0).numpy())\n",
    "    axes[1, i].axis('off')\n",
    "axes[0, 0].set_ylabel('Original', fontsize=12, rotation=0, labelpad=50)\n",
    "axes[1, 0].set_ylabel('Reconstructed', fontsize=12, rotation=0, labelpad=50)\n",
    "plt.suptitle(f'VAE: 3,072 pixel values ‚Üí {LATENT_DIM} latent numbers ‚Üí 3,072 pixels', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Mdnrnn Architecture\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_mdnrnn_architecture.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_mdnrnn_architecture"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Phase 3: Train the Memory (M)\n",
    "\n",
    "Now we encode all frames into latent space and train the MDN-RNN on sequences of $(z_t, a_t, z_{t+1})$."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all frames to latent space\n",
    "print(\"Encoding all frames to latent space...\")\n",
    "vae.eval()\n",
    "encoded_episodes = []\n",
    "\n",
    "for episode_obs in observations:\n",
    "    frames = torch.tensor(episode_obs, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n",
    "    with torch.no_grad():\n",
    "        mu, _ = vae.encode(frames)\n",
    "    encoded_episodes.append(mu.cpu())\n",
    "\n",
    "print(f\"Encoded {len(encoded_episodes)} episodes to latent dim {LATENT_DIM}\")\n",
    "print(f\"Example latent shape: {encoded_episodes[0].shape}\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_DIM = 2\n",
    "HIDDEN_DIM = 32\n",
    "N_GAUSSIANS = 3\n",
    "\n",
    "class MDNHead(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, n_gaussians):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.n_gaussians = n_gaussians\n",
    "        self.fc_pi = nn.Linear(hidden_dim, output_dim * n_gaussians)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, output_dim * n_gaussians)\n",
    "        self.fc_sigma = nn.Linear(hidden_dim, output_dim * n_gaussians)\n",
    "\n",
    "    def forward(self, h):\n",
    "        K, D = self.n_gaussians, self.output_dim\n",
    "        pi = F.softmax(self.fc_pi(h).view(-1, D, K), dim=-1)\n",
    "        mu = self.fc_mu(h).view(-1, D, K)\n",
    "        sigma = torch.exp(self.fc_sigma(h).view(-1, D, K))\n",
    "        return pi, mu, sigma\n",
    "\n",
    "class MDNRNN(nn.Module):\n",
    "    def __init__(self, latent_dim=LATENT_DIM, action_dim=ACTION_DIM,\n",
    "                 hidden_dim=HIDDEN_DIM, n_gaussians=N_GAUSSIANS):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(latent_dim + action_dim, hidden_dim, batch_first=True)\n",
    "        self.mdn = MDNHead(hidden_dim, latent_dim, n_gaussians)\n",
    "\n",
    "    def forward(self, z, a, hidden=None):\n",
    "        x = torch.cat([z, a], dim=-1)\n",
    "        h_seq, hidden = self.lstm(x, hidden)\n",
    "        batch, seq_len, _ = h_seq.shape\n",
    "        pi, mu, sigma = self.mdn(h_seq.reshape(-1, self.hidden_dim))\n",
    "        return pi, mu, sigma, hidden, h_seq\n",
    "\n",
    "def mdn_loss(pi, mu, sigma, z_next):\n",
    "    z_next = z_next.unsqueeze(-1)\n",
    "    log_probs = -0.5 * ((z_next - mu) / sigma) ** 2 - torch.log(sigma) - 0.5 * np.log(2 * np.pi)\n",
    "    log_probs = log_probs + torch.log(pi + 1e-8)\n",
    "    log_likelihood = torch.logsumexp(log_probs, dim=-1)\n",
    "    return -log_likelihood.sum(dim=-1).mean()"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Mdnrnn Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_mdnrnn_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_mdnrnn_training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With V trained, we can encode all frames to latent space and then train M on sequences of transitions."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequences for MDN-RNN training\n",
    "def prepare_sequences(encoded_episodes, actions_list, seq_len=20):\n",
    "    \"\"\"Cut episodes into fixed-length sequences for training.\"\"\"\n",
    "    z_inputs, a_inputs, z_targets = [], [], []\n",
    "\n",
    "    for z_ep, a_ep in zip(encoded_episodes, actions_list):\n",
    "        a_ep = torch.tensor(a_ep, dtype=torch.float32)\n",
    "        T = min(len(z_ep) - 1, len(a_ep))\n",
    "\n",
    "        for start in range(0, T - seq_len, seq_len // 2):\n",
    "            end = start + seq_len\n",
    "            if end > T:\n",
    "                break\n",
    "            z_inputs.append(z_ep[start:end])\n",
    "            a_inputs.append(a_ep[start:end])\n",
    "            z_targets.append(z_ep[start+1:end+1])\n",
    "\n",
    "    return (torch.stack(z_inputs), torch.stack(a_inputs), torch.stack(z_targets))\n",
    "\n",
    "z_train, a_train, z_target_train = prepare_sequences(encoded_episodes, actions)\n",
    "print(f\"Training sequences: {z_train.shape[0]}\")\n",
    "print(f\"Sequence length: {z_train.shape[1]}\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the MDN-RNN\n",
    "rnn = MDNRNN().to(device)\n",
    "rnn_optimizer = optim.Adam(rnn.parameters(), lr=1e-3)\n",
    "\n",
    "rnn_dataset = torch.utils.data.TensorDataset(z_train, a_train, z_target_train)\n",
    "rnn_loader = torch.utils.data.DataLoader(rnn_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(\"Training MDN-RNN...\")\n",
    "rnn_losses = []\n",
    "for epoch in range(40):\n",
    "    rnn.train()\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "    for z_batch, a_batch, z_next_batch in rnn_loader:\n",
    "        z_batch = z_batch.to(device)\n",
    "        a_batch = a_batch.to(device)\n",
    "        z_next_batch = z_next_batch.to(device)\n",
    "\n",
    "        pi, mu, sigma, _, _ = rnn(z_batch, a_batch)\n",
    "        z_next_flat = z_next_batch.reshape(-1, LATENT_DIM)\n",
    "\n",
    "        loss = mdn_loss(pi, mu, sigma, z_next_flat)\n",
    "\n",
    "        rnn_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(rnn.parameters(), 1.0)\n",
    "        rnn_optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg = epoch_loss / n_batches\n",
    "    rnn_losses.append(avg)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"  Epoch {epoch+1:2d}/40 | Loss: {avg:.4f}\")\n",
    "\n",
    "print(\"MDN-RNN training complete!\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Training curves for both V and M\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(vae_losses, 'b-', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('VAE Loss', fontsize=12)\n",
    "ax1.set_title('Phase 2a: VAE Training', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(rnn_losses, 'r-', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('MDN-RNN Loss (NLL)', fontsize=12)\n",
    "ax2.set_title('Phase 2b: MDN-RNN Training', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Training the World Model: V and M', fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Controller And Dream Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_controller_and_dream_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_controller_and_dream_intro"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Phase 4: Dream and Evolve the Controller\n",
    "\n",
    "This is the key section. We will train the Controller **entirely inside the learned world model** ‚Äî never touching the real environment again!"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, action_dim)\n",
    "\n",
    "    def forward(self, z, h):\n",
    "        x = torch.cat([z, h], dim=-1)\n",
    "        return torch.tanh(self.fc(x))\n",
    "\n",
    "    def get_num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    def set_params(self, flat_params):\n",
    "        idx = 0\n",
    "        for p in self.parameters():\n",
    "            n = p.numel()\n",
    "            p.data = torch.tensor(flat_params[idx:idx+n], dtype=torch.float32).reshape(p.shape)\n",
    "            idx += n\n",
    "\n",
    "    def get_params(self):\n",
    "        return np.concatenate([p.data.cpu().numpy().flatten() for p in self.parameters()])"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Dream Rollout Todo\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_dream_rollout_todo.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_dream_rollout_todo"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Your Turn: Implement the Dream Rollout\n",
    "\n",
    "This is the core of the World Model: running the Controller inside the learned dream."
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dream_rollout(controller, vae, rnn, max_steps=50, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Run a complete episode inside the dream (learned world model).\n",
    "\n",
    "    1. Start from a random initial latent state\n",
    "    2. At each step, the Controller chooses an action\n",
    "    3. The MDN-RNN predicts the next latent state\n",
    "    4. Reward is computed from the latent state (proxy for goal distance)\n",
    "\n",
    "    Returns:\n",
    "        total_reward: sum of rewards over the dream episode\n",
    "        trajectory: list of (z, a, r) tuples\n",
    "    \"\"\"\n",
    "    vae.eval()\n",
    "    rnn.eval()\n",
    "    controller.eval()\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Get a starting observation from a real environment reset,\n",
    "    #         encode it with the VAE to get z_0\n",
    "    #         env = SimpleNavEnv()\n",
    "    #         obs = env.reset()\n",
    "    #         With torch.no_grad(): encode the obs to get z_t (use mu only)\n",
    "    #\n",
    "    # Step 2: Initialize the RNN hidden state to None\n",
    "    #\n",
    "    # Step 3: Loop for max_steps:\n",
    "    #   a) Get the LSTM hidden state h_t (or zeros if hidden is None)\n",
    "    #   b) Use the Controller to pick an action: a_t = controller(z_t, h_t)\n",
    "    #   c) Feed (z_t, a_t) through the RNN to get predicted (pi, mu, sigma)\n",
    "    #      and updated hidden state\n",
    "    #   d) Sample z_{t+1} from the MDN (pick component, sample from Gaussian)\n",
    "    #   e) Compute a reward proxy: e.g., negative distance of z from a target\n",
    "    #   f) Update z_t = z_{t+1}\n",
    "    #\n",
    "    # Step 4: Return total_reward\n",
    "    # ==============================\n",
    "\n",
    "    total_reward = ???  # YOUR CODE HERE\n",
    "\n",
    "    return total_reward"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Dream Todo Followup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_dream_todo_followup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_11_dream_todo_followup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification: test dream rollout with a random controller\n",
    "test_ctrl = Controller(input_dim=LATENT_DIM + HIDDEN_DIM, action_dim=ACTION_DIM)\n",
    "reward = dream_rollout(test_ctrl, vae, rnn)\n",
    "print(f\"Random controller dream reward: {reward:.2f}\")\n",
    "print(\"‚úÖ Dream rollout works!\" if isinstance(reward, (int, float)) else \"‚ùå Check your implementation\")"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Cmaes Evolution\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_cmaes_evolution.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_12_cmaes_evolution"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolving the Controller Inside the Dream"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCMAES:\n",
    "    def __init__(self, num_params, population_size=32, sigma_init=0.5, elite_ratio=0.25):\n",
    "        self.num_params = num_params\n",
    "        self.pop_size = population_size\n",
    "        self.sigma = sigma_init\n",
    "        self.elite_size = max(1, int(population_size * elite_ratio))\n",
    "        self.mean = np.zeros(num_params)\n",
    "        self.best_rewards = []\n",
    "        self.mean_rewards = []\n",
    "\n",
    "    def sample_population(self):\n",
    "        noise = np.random.randn(self.pop_size, self.num_params)\n",
    "        return self.mean + self.sigma * noise\n",
    "\n",
    "    def update(self, population, rewards):\n",
    "        ranked_idx = np.argsort(rewards)[::-1]\n",
    "        elite_idx = ranked_idx[:self.elite_size]\n",
    "        weights = np.log(self.elite_size + 0.5) - np.log(np.arange(1, self.elite_size + 1))\n",
    "        weights = weights / weights.sum()\n",
    "        self.mean = np.sum(weights[:, np.newaxis] * population[elite_idx], axis=0)\n",
    "        self.best_rewards.append(rewards[ranked_idx[0]])\n",
    "        self.mean_rewards.append(np.mean(rewards))\n",
    "\n",
    "    def get_best(self):\n",
    "        return self.mean.copy()\n",
    "\n",
    "# Evolve!\n",
    "input_dim = LATENT_DIM + HIDDEN_DIM\n",
    "template = Controller(input_dim, ACTION_DIM)\n",
    "num_params = template.get_num_params()\n",
    "print(f\"Controller parameters: {num_params}\")\n",
    "\n",
    "cmaes = SimpleCMAES(num_params, population_size=32, sigma_init=0.5)\n",
    "\n",
    "N_GENERATIONS = 30\n",
    "print(f\"\\nEvolving controller inside the dream for {N_GENERATIONS} generations...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for gen in range(N_GENERATIONS):\n",
    "    pop = cmaes.sample_population()\n",
    "    rewards = np.zeros(len(pop))\n",
    "\n",
    "    for i in range(len(pop)):\n",
    "        ctrl = Controller(input_dim, ACTION_DIM)\n",
    "        ctrl.set_params(pop[i])\n",
    "        # Average over 3 dream rollouts for robustness\n",
    "        r = np.mean([dream_rollout(ctrl, vae, rnn) for _ in range(3)])\n",
    "        rewards[i] = r\n",
    "\n",
    "    cmaes.update(pop, rewards)\n",
    "    if (gen + 1) % 5 == 0:\n",
    "        print(f\"  Gen {gen+1:3d}/{N_GENERATIONS} | \"\n",
    "              f\"Best: {cmaes.best_rewards[-1]:7.2f} | \"\n",
    "              f\"Mean: {cmaes.mean_rewards[-1]:7.2f}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Controller evolution complete!\")\n",
    "\n",
    "# Create the best controller\n",
    "best_controller = Controller(input_dim, ACTION_DIM)\n",
    "best_controller.set_params(cmaes.get_best())"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Evolution curve\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "gens = range(1, len(cmaes.best_rewards) + 1)\n",
    "ax.plot(gens, cmaes.best_rewards, 'b-', linewidth=2, label='Best in generation')\n",
    "ax.fill_between(gens, cmaes.mean_rewards, cmaes.best_rewards, alpha=0.2, color='blue')\n",
    "ax.plot(gens, cmaes.mean_rewards, 'r--', linewidth=1.5, alpha=0.7, label='Mean of generation')\n",
    "ax.set_xlabel('Generation', fontsize=12)\n",
    "ax.set_ylabel('Dream Reward', fontsize=12)\n",
    "ax.set_title('Controller Evolution Inside the Dream', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Moment Of Truth Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_moment_of_truth_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_13_moment_of_truth_intro"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üéØ Final Output: Real Environment vs Dream ‚Äî Side by Side\n",
    "\n",
    "Now the moment of truth. We deploy the dream-trained controller to the real environment and compare."
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Real Vs Dream Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_real_vs_dream_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_14_real_vs_dream_code"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_real_episode(controller, vae, rnn, env, max_steps=50):\n",
    "    \"\"\"Run the controller in the REAL environment.\"\"\"\n",
    "    obs = env.reset()\n",
    "    frames = [obs.copy()]\n",
    "    total_reward = 0\n",
    "    hidden = None\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Encode the real observation\n",
    "        frame_tensor = torch.tensor(obs, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            mu, _ = vae.encode(frame_tensor)\n",
    "            z_t = mu.squeeze(0)\n",
    "\n",
    "            # Get hidden state\n",
    "            if hidden is not None:\n",
    "                h_t = hidden[0].squeeze(0).squeeze(0)\n",
    "            else:\n",
    "                h_t = torch.zeros(HIDDEN_DIM).to(device)\n",
    "\n",
    "            # Controller picks action\n",
    "            action = controller(z_t, h_t).cpu().numpy()\n",
    "\n",
    "            # Update RNN hidden state\n",
    "            z_input = z_t.unsqueeze(0).unsqueeze(0)\n",
    "            a_input = torch.tensor(action, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            _, _, _, hidden, _ = rnn(z_input, a_input, hidden)\n",
    "\n",
    "        obs, reward, done = env.step(action)\n",
    "        frames.append(obs.copy())\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return frames, total_reward\n",
    "\n",
    "def run_dream_episode(controller, vae, rnn, initial_obs, max_steps=50):\n",
    "    \"\"\"Run the controller in the DREAM (using the world model to generate frames).\"\"\"\n",
    "    vae.eval()\n",
    "    rnn.eval()\n",
    "\n",
    "    frame_tensor = torch.tensor(initial_obs, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        mu, _ = vae.encode(frame_tensor)\n",
    "    z_t = mu.squeeze(0)\n",
    "\n",
    "    dreamed_frames = []\n",
    "    hidden = None\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        with torch.no_grad():\n",
    "            # Decode current z to an image (the dream frame)\n",
    "            img = vae.decode(z_t.unsqueeze(0)).cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
    "            dreamed_frames.append(img)\n",
    "\n",
    "            # Get hidden state\n",
    "            if hidden is not None:\n",
    "                h_t = hidden[0].squeeze(0).squeeze(0)\n",
    "            else:\n",
    "                h_t = torch.zeros(HIDDEN_DIM).to(device)\n",
    "\n",
    "            # Controller picks action\n",
    "            action = controller(z_t, h_t)\n",
    "\n",
    "            # MDN-RNN predicts next state\n",
    "            z_input = z_t.unsqueeze(0).unsqueeze(0)\n",
    "            a_input = action.unsqueeze(0).unsqueeze(0)\n",
    "            pi, mu, sigma, hidden, _ = rnn(z_input, a_input, hidden)\n",
    "\n",
    "            # Sample next z from MDN\n",
    "            pi_np = pi[0, :, :].cpu().numpy()\n",
    "            mu_np = mu[0, :, :].cpu().numpy()\n",
    "            sigma_np = sigma[0, :, :].cpu().numpy()\n",
    "\n",
    "            z_next = np.zeros(LATENT_DIM)\n",
    "            for d in range(LATENT_DIM):\n",
    "                k = np.random.choice(len(pi_np[d]), p=pi_np[d])\n",
    "                z_next[d] = np.random.normal(mu_np[d, k], sigma_np[d, k])\n",
    "\n",
    "            z_t = torch.tensor(z_next, dtype=torch.float32).to(device)\n",
    "\n",
    "    return dreamed_frames\n",
    "\n",
    "# Run both!\n",
    "env = SimpleNavEnv()\n",
    "real_frames, real_reward = run_real_episode(best_controller, vae, rnn, env)\n",
    "dream_frames = run_dream_episode(best_controller, vae, rnn, real_frames[0])\n",
    "\n",
    "print(f\"Real environment reward: {real_reward:.2f}\")"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Final Comparison\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/15_final_comparison.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_15_final_comparison"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä THE FINAL COMPARISON: Real vs Dream\n",
    "n_show = min(8, len(real_frames), len(dream_frames))\n",
    "step_indices = np.linspace(0, min(len(real_frames), len(dream_frames)) - 1, n_show, dtype=int)\n",
    "\n",
    "fig, axes = plt.subplots(2, n_show, figsize=(n_show * 3, 6))\n",
    "\n",
    "for col, idx in enumerate(step_indices):\n",
    "    # Real frames\n",
    "    if idx < len(real_frames):\n",
    "        axes[0, col].imshow(real_frames[idx])\n",
    "    axes[0, col].axis('off')\n",
    "    axes[0, col].set_title(f't={idx}', fontsize=10)\n",
    "\n",
    "    # Dream frames\n",
    "    if idx < len(dream_frames):\n",
    "        axes[1, col].imshow(np.clip(dream_frames[idx], 0, 1))\n",
    "    axes[1, col].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('REAL\\nEnvironment', fontsize=13, rotation=0, labelpad=70, fontweight='bold')\n",
    "axes[1, 0].set_ylabel(\"AGENT'S\\nDREAM\", fontsize=13, rotation=0, labelpad=70, fontweight='bold')\n",
    "\n",
    "plt.suptitle('üéØ Real Observations vs. Agent\\'s Dream ‚Äî The World Model in Action!',\n",
    "             fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéâ Congratulations! You have built a complete World Model from scratch!\")\n",
    "print(\"\")\n",
    "print(\"The agent learned to:\")\n",
    "print(\"  1. SEE ‚Äî compress images into 8 latent numbers (VAE)\")\n",
    "print(\"  2. REMEMBER & PREDICT ‚Äî learn how the world evolves (MDN-RNN)\")\n",
    "print(\"  3. DECIDE ‚Äî pick actions with just a linear layer (Controller)\")\n",
    "print(\"  4. DREAM ‚Äî train entirely in its own imagination (CMA-ES in dream)\")\n",
    "print(\"\")\n",
    "print(\"This is the same architecture that achieved near-human performance\")\n",
    "print(\"on CarRacing ‚Äî with a controller of only 867 parameters!\")"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Reflection\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/16_reflection.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_16_reflection"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Series Finale\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/17_series_finale.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_17_series_finale"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "1. Look at the dream frames vs real frames. Where do they diverge most? What does this tell you about the world model's limitations?\n",
    "2. The controller was trained entirely in the dream. If the dream is inaccurate in some way, how would that affect the controller's behavior in the real environment?\n",
    "3. Why did Ha and Schmidhuber use CMA-ES instead of backpropagation for the controller? Could you backpropagate through the entire dream? (Hint: look up \"Dreamer\" by Hafner et al.)\n",
    "\n",
    "### üèÜ Optional Challenges\n",
    "1. **Better VAE**: Use a convolutional VAE with skip connections. Do the dreams look more realistic?\n",
    "2. **Dream length**: Try training with longer dream rollouts (100+ steps). Does the controller improve or does compounding error become a problem?\n",
    "3. **CarRacing**: Scale this pipeline to the OpenAI CarRacing-v2 environment with 64√ó64 images, 32-dim latent space, and 256-dim hidden state. This is the original World Models setup!\n",
    "4. **Model exploitation**: Intentionally make the world model worse (train it less). Does the controller learn to \"cheat\" by exploiting inaccuracies?\n",
    "\n",
    "### The Big Picture\n",
    "\n",
    "You have now implemented the complete World Models paper (Ha & Schmidhuber, 2018) from scratch. This architecture ‚Äî learn a compressed representation, learn the dynamics, train a policy in imagination ‚Äî has become the foundation for modern approaches like Dreamer, DreamerV2, DreamerV3, and even connects to Yann LeCun's JEPA vision for next-generation AI.\n",
    "\n",
    "The core insight remains powerful: **agents that can imagine and plan outperform agents that only react.**"
   ],
   "id": "cell_35"
  }
 ]
}