{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "MDN-RNN: Learning to Predict the Future ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1ib7EvcVmMjv6ft1HT4xBaqCJ7ohoI9f5\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_why_it_matters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ MDN-RNN: Learning to Predict the Future from First Principles\n",
    "\n",
    "*Part 2 of the Vizuara series on World Models*\n",
    "*Estimated time: 50 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebook, we built a VAE that compresses images into a tiny latent code. Our agent can now \"see\" the world in compressed form. But seeing the present is not enough ‚Äî **the agent needs to predict what will happen next.**\n",
    "\n",
    "Think about watching a movie. If you pause at any frame, your brain can guess what comes next. If a ball is flying through the air, you predict it will continue on its trajectory. If a car is turning left, you expect the road to curve.\n",
    "\n",
    "The **MDN-RNN** (Mixture Density Network + Recurrent Neural Network) is the \"Memory\" component of the World Model. Given the current latent state $z_t$ and an action $a_t$, it predicts a *distribution* over possible next states $z_{t+1}$.\n",
    "\n",
    "But here is the twist: the future is not deterministic. At an intersection, the road could curve left *or* right. A single-point prediction would average these possibilities and predict \"straight\" ‚Äî which is wrong in both cases! The MDN handles this by predicting a **mixture of Gaussians** ‚Äî multiple possible futures, each with its own probability.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Build an MDN-RNN that predicts the next latent state\n",
    "- Train it on synthetic sequences to learn dynamics\n",
    "- Visualize multimodal predictions (multiple possible futures)\n",
    "- Generate \"dreamed\" sequences ‚Äî the agent imagining its own future"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_intuition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### Why Not Just Predict a Single Point?\n",
    "\n",
    "Imagine you are driving and approaching a T-junction. The road will either go left or go right. If a neural network is forced to make a single prediction for \"where will the road be next?\", it will average the two possibilities and predict *straight ahead* ‚Äî a place where the road definitely does not go!\n",
    "\n",
    "This is the \"regression to the mean\" problem. When the future is multimodal (has multiple distinct possibilities), a single-point prediction fails.\n",
    "\n",
    "### The Mixture of Gaussians Solution\n",
    "\n",
    "Instead of one prediction, we output $K$ predictions ‚Äî each representing a possible future. Each prediction is a Gaussian distribution (a bell curve) with its own:\n",
    "- **Mean** $\\mu_i$: the center of the predicted future\n",
    "- **Standard deviation** $\\sigma_i$: how uncertain we are about that future\n",
    "- **Mixing coefficient** $\\pi_i$: how likely that future is\n",
    "\n",
    "The overall prediction is a weighted sum of these Gaussians. This can represent arbitrarily complex distributions.\n",
    "\n",
    "### ü§î Think About This\n",
    "\n",
    "If $K = 1$, the MDN reduces to a standard regression network. What value of $K$ do you think is enough for most environments? (The original World Models paper uses $K = 5$.)\n",
    "\n",
    "Consider: in CarRacing, the road can curve left, go straight, or curve right. That is 3 modes, so $K = 5$ gives us some extra capacity. In general, $K$ does not need to be very large because the latent space is already compressed."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Mathematics\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_mathematics.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_mathematics"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### The Mixture Density Network\n",
    "\n",
    "Given the LSTM hidden state $h_t$, the MDN outputs parameters for $K$ Gaussian components:\n",
    "\n",
    "$$P(z_{t+1}) = \\sum_{i=1}^{K} \\pi_i \\cdot \\mathcal{N}(z_{t+1} \\mid \\mu_i, \\sigma_i^2)$$\n",
    "\n",
    "This equation says: the probability of the next latent state $z_{t+1}$ is a weighted sum of $K$ Gaussian distributions. Each component $i$ has a mixing coefficient $\\pi_i$ (how likely that future is), a mean $\\mu_i$, and a variance $\\sigma_i^2$.\n",
    "\n",
    "Computationally: for each dimension of the latent code, we evaluate $K$ Gaussian densities and weight them by $\\pi_i$.\n",
    "\n",
    "### Constraints on the Parameters\n",
    "\n",
    "The mixing coefficients must form a valid probability distribution:\n",
    "$$\\sum_{i=1}^{K} \\pi_i = 1, \\quad \\pi_i > 0$$\n",
    "\n",
    "We enforce this using softmax. The standard deviations must be positive:\n",
    "$$\\sigma_i > 0$$\n",
    "\n",
    "We enforce this by outputting $\\log \\sigma_i$ and exponentiating.\n",
    "\n",
    "### The MDN Loss (Negative Log-Likelihood)\n",
    "\n",
    "To train the MDN, we minimize the negative log-likelihood of the observed next state $z_{t+1}$:\n",
    "\n",
    "$$\\mathcal{L} = -\\log \\left( \\sum_{i=1}^{K} \\pi_i \\cdot \\mathcal{N}(z_{t+1} \\mid \\mu_i, \\sigma_i^2) \\right)$$\n",
    "\n",
    "This equation says: compute the probability that the observed $z_{t+1}$ was generated by our mixture, then take the negative log. Lower loss means the mixture assigns higher probability to the actual outcome.\n",
    "\n",
    "Computationally: we use the log-sum-exp trick for numerical stability."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Gaussian Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_gaussian_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_gaussian_viz"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It ‚Äî Component by Component\n",
    "\n",
    "### 4.1 Visualizing Gaussian Mixtures\n",
    "\n",
    "Before we build the network, let us understand what mixtures of Gaussians look like."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_pdf(x, mu, sigma):\n",
    "    \"\"\"Compute the Gaussian probability density function.\"\"\"\n",
    "    return (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "def plot_mixture(pis, mus, sigmas, title=\"Gaussian Mixture\"):\n",
    "    \"\"\"Plot a mixture of Gaussians.\"\"\"\n",
    "    x = np.linspace(-5, 5, 500)\n",
    "    mixture = np.zeros_like(x)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    colors = ['#2196F3', '#FF9800', '#4CAF50', '#E91E63', '#9C27B0']\n",
    "\n",
    "    for i, (pi, mu, sigma) in enumerate(zip(pis, mus, sigmas)):\n",
    "        component = pi * gaussian_pdf(x, mu, sigma)\n",
    "        mixture += component\n",
    "        ax.fill_between(x, component, alpha=0.3, color=colors[i % len(colors)],\n",
    "                        label=f'Component {i+1}: œÄ={pi:.2f}, Œº={mu:.1f}, œÉ={sigma:.1f}')\n",
    "\n",
    "    ax.plot(x, mixture, 'k-', linewidth=2, label='Full mixture')\n",
    "    ax.set_xlabel('z (latent state)', fontsize=12)\n",
    "    ax.set_ylabel('Probability Density', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example: T-junction ‚Äî road goes left OR right\n",
    "plot_mixture(\n",
    "    pis=[0.6, 0.3, 0.1],\n",
    "    mus=[2.0, -1.5, 0.0],\n",
    "    sigmas=[0.3, 0.4, 0.8],\n",
    "    title=\"MDN Prediction at T-junction: Three Possible Futures\"\n",
    ")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Lstm Component\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_lstm_component.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_lstm_component"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tall blue peak says \"60% chance the road curves left.\" The orange peak says \"30% chance it curves right.\" The small green peak says \"10% chance it goes straight.\" A single Gaussian could never capture this structure!\n",
    "\n",
    "### 4.2 The LSTM Component\n",
    "\n",
    "The LSTM accumulates temporal context ‚Äî it remembers what has happened in the past."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldLSTM(nn.Module):\n",
    "    \"\"\"LSTM that processes sequences of (latent_state, action) pairs.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, action_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=latent_dim + action_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, z, a, hidden=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z: latent states, shape (batch, seq_len, latent_dim)\n",
    "            a: actions, shape (batch, seq_len, action_dim)\n",
    "            hidden: optional (h_0, c_0) tuple\n",
    "        Returns:\n",
    "            h: hidden states, shape (batch, seq_len, hidden_dim)\n",
    "            hidden: final (h_n, c_n)\n",
    "        \"\"\"\n",
    "        x = torch.cat([z, a], dim=-1)\n",
    "        h, hidden = self.lstm(x, hidden)\n",
    "        return h, hidden\n",
    "\n",
    "# Test\n",
    "latent_dim, action_dim, hidden_dim = 4, 2, 32\n",
    "lstm = WorldLSTM(latent_dim, action_dim, hidden_dim).to(device)\n",
    "test_z = torch.randn(8, 10, latent_dim).to(device)  # batch=8, seq_len=10\n",
    "test_a = torch.randn(8, 10, action_dim).to(device)\n",
    "h, hidden = lstm(test_z, test_a)\n",
    "print(f\"Input: z {test_z.shape}, a {test_a.shape}\")\n",
    "print(f\"Output: h {h.shape}\")\n",
    "print(f\"Hidden: h_n {hidden[0].shape}, c_n {hidden[1].shape}\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Mdn Head\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_mdn_head.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_mdn_head"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The MDN Head\n",
    "\n",
    "This component takes the LSTM hidden state and produces the parameters of a Gaussian mixture."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDNHead(nn.Module):\n",
    "    \"\"\"Mixture Density Network head ‚Äî outputs (pi, mu, sigma) for K Gaussians.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, output_dim, n_gaussians):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.n_gaussians = n_gaussians\n",
    "\n",
    "        # Each Gaussian needs: pi (1), mu (output_dim), sigma (output_dim)\n",
    "        self.fc_pi = nn.Linear(hidden_dim, output_dim * n_gaussians)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, output_dim * n_gaussians)\n",
    "        self.fc_sigma = nn.Linear(hidden_dim, output_dim * n_gaussians)\n",
    "\n",
    "    def forward(self, h):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: LSTM hidden states, shape (batch * seq_len, hidden_dim)\n",
    "        Returns:\n",
    "            pi: mixing coefficients, shape (batch * seq_len, output_dim, K)\n",
    "            mu: means, shape (batch * seq_len, output_dim, K)\n",
    "            sigma: std devs, shape (batch * seq_len, output_dim, K)\n",
    "        \"\"\"\n",
    "        K = self.n_gaussians\n",
    "        D = self.output_dim\n",
    "\n",
    "        pi = self.fc_pi(h).view(-1, D, K)\n",
    "        pi = F.softmax(pi, dim=-1)               # Sum to 1 over K\n",
    "\n",
    "        mu = self.fc_mu(h).view(-1, D, K)\n",
    "\n",
    "        sigma = self.fc_sigma(h).view(-1, D, K)\n",
    "        sigma = torch.exp(sigma)                  # Positive!\n",
    "\n",
    "        return pi, mu, sigma\n",
    "\n",
    "# Test\n",
    "mdn_head = MDNHead(hidden_dim=32, output_dim=4, n_gaussians=3).to(device)\n",
    "test_h = torch.randn(80, 32).to(device)  # 80 = batch(8) * seq_len(10)\n",
    "pi, mu, sigma = mdn_head(test_h)\n",
    "print(f\"pi shape: {pi.shape}   (batch*seq, latent_dim, K)\")\n",
    "print(f\"mu shape: {mu.shape}\")\n",
    "print(f\"sigma shape: {sigma.shape}\")\n",
    "print(f\"pi sums to 1? {pi[0, 0].sum().item():.6f}\")\n",
    "print(f\"sigma all positive? {(sigma > 0).all().item()}\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Complete Model\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_complete_model.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_complete_model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The Complete MDN-RNN"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDNRNN(nn.Module):\n",
    "    \"\"\"MDN-RNN: Predicts next latent state as a mixture of Gaussians.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=4, action_dim=2, hidden_dim=64, n_gaussians=3):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_gaussians = n_gaussians\n",
    "\n",
    "        self.lstm = nn.LSTM(latent_dim + action_dim, hidden_dim, batch_first=True)\n",
    "        self.mdn = MDNHead(hidden_dim, latent_dim, n_gaussians)\n",
    "\n",
    "    def forward(self, z, a, hidden=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z: shape (batch, seq_len, latent_dim)\n",
    "            a: shape (batch, seq_len, action_dim)\n",
    "        Returns:\n",
    "            pi, mu, sigma: MDN parameters\n",
    "            hidden: LSTM hidden state\n",
    "        \"\"\"\n",
    "        x = torch.cat([z, a], dim=-1)\n",
    "        h_seq, hidden = self.lstm(x, hidden)\n",
    "\n",
    "        # Reshape for MDN head: (batch * seq_len, hidden_dim)\n",
    "        batch_size, seq_len, _ = h_seq.shape\n",
    "        h_flat = h_seq.reshape(-1, self.hidden_dim)\n",
    "\n",
    "        pi, mu, sigma = self.mdn(h_flat)\n",
    "        return pi, mu, sigma, hidden, h_seq\n",
    "\n",
    "    def get_hidden_state(self, z, a, hidden=None):\n",
    "        \"\"\"Run one step and return the hidden state vector.\"\"\"\n",
    "        x = torch.cat([z, a], dim=-1)\n",
    "        _, hidden = self.lstm(x, hidden)\n",
    "        return hidden\n",
    "\n",
    "model = MDNRNN(latent_dim=4, action_dim=2, hidden_dim=64, n_gaussians=3).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"MDN-RNN parameters: {total_params:,}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo Loss\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_todo_loss.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_todo_loss"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîß Your Turn: Implement the MDN Loss Function\n",
    "\n",
    "The MDN loss is the negative log-likelihood of the observed next state under the predicted mixture of Gaussians."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_loss(pi, mu, sigma, z_next):\n",
    "    \"\"\"\n",
    "    Compute the MDN negative log-likelihood loss.\n",
    "\n",
    "    Args:\n",
    "        pi: mixing coefficients, shape (N, latent_dim, K)\n",
    "        mu: means, shape (N, latent_dim, K)\n",
    "        sigma: std devs, shape (N, latent_dim, K)\n",
    "        z_next: actual next latent state, shape (N, latent_dim)\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar, negative log-likelihood averaged over batch\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Expand z_next to match the shape of mu:\n",
    "    #         z_next.unsqueeze(-1) gives shape (N, latent_dim, 1)\n",
    "    #\n",
    "    # Step 2: Compute the Gaussian log-probability for each component:\n",
    "    #         log N(z | mu, sigma) = -0.5 * ((z - mu) / sigma)^2 - log(sigma) - 0.5 * log(2*pi)\n",
    "    #\n",
    "    # Step 3: Add log(pi) to get the log of (pi * N(z | mu, sigma))\n",
    "    #\n",
    "    # Step 4: Use torch.logsumexp over the K dimension to get\n",
    "    #         log(sum_k pi_k * N(z | mu_k, sigma_k))\n",
    "    #\n",
    "    # Step 5: Sum over the latent_dim dimension, then average over batch\n",
    "    #         Return the NEGATIVE of this (we want negative log-likelihood)\n",
    "    # ==============================\n",
    "\n",
    "    loss = ???  # YOUR CODE HERE\n",
    "\n",
    "    return loss"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification\n",
    "# When mu matches z_next exactly and sigma is small, loss should be low\n",
    "N, D, K = 100, 4, 3\n",
    "test_z_next = torch.randn(N, D).to(device)\n",
    "\n",
    "# Create a mixture where component 0 is centered exactly on z_next\n",
    "test_pi = torch.zeros(N, D, K).to(device)\n",
    "test_pi[:, :, 0] = 0.8\n",
    "test_pi[:, :, 1] = 0.15\n",
    "test_pi[:, :, 2] = 0.05\n",
    "\n",
    "test_mu = torch.randn(N, D, K).to(device)\n",
    "test_mu[:, :, 0] = test_z_next  # First component matches target\n",
    "\n",
    "test_sigma = torch.ones(N, D, K).to(device) * 0.1  # Tight distribution\n",
    "\n",
    "loss_good = mdn_loss(test_pi, test_mu, test_sigma, test_z_next)\n",
    "\n",
    "# Now offset the means ‚Äî loss should be higher\n",
    "test_mu_bad = test_mu.clone()\n",
    "test_mu_bad[:, :, 0] += 5.0\n",
    "loss_bad = mdn_loss(test_pi, test_mu_bad, test_sigma, test_z_next)\n",
    "\n",
    "assert loss_bad > loss_good, \\\n",
    "    f\"‚ùå Loss should be higher when means are wrong. Good: {loss_good:.2f}, Bad: {loss_bad:.2f}\"\n",
    "print(f\"‚úÖ MDN loss function works!\")\n",
    "print(f\"   Loss (good prediction): {loss_good.item():.2f}\")\n",
    "print(f\"   Loss (bad prediction):  {loss_bad.item():.2f}\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Verification And Training Data\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_verification_and_training_data.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_verification_and_training_data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training on Synthetic Dynamics\n",
    "\n",
    "Let us create a simple 2D environment where an agent moves around, and train the MDN-RNN to predict the dynamics."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n_sequences=500, seq_len=20, latent_dim=2, action_dim=2):\n",
    "    \"\"\"\n",
    "    Generate synthetic sequences of (z, a) pairs.\n",
    "    The dynamics: z_{t+1} = z_t + 0.3 * tanh(a_t) + noise\n",
    "    With a twist: at certain positions, the dynamics bifurcate (two possible outcomes).\n",
    "    \"\"\"\n",
    "    all_z = []\n",
    "    all_a = []\n",
    "\n",
    "    for _ in range(n_sequences):\n",
    "        z_seq = torch.zeros(seq_len + 1, latent_dim)\n",
    "        a_seq = torch.randn(seq_len, action_dim) * 0.5\n",
    "\n",
    "        z_seq[0] = torch.randn(latent_dim) * 0.5  # Random start\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            # Base dynamics\n",
    "            dz = 0.3 * torch.tanh(a_seq[t])\n",
    "\n",
    "            # Add bifurcation: when z[0] > 0.5, randomly go left or right\n",
    "            if z_seq[t, 0] > 0.5:\n",
    "                if torch.rand(1) > 0.5:\n",
    "                    dz[1] += 0.4   # Go up\n",
    "                else:\n",
    "                    dz[1] -= 0.4   # Go down\n",
    "\n",
    "            z_seq[t + 1] = z_seq[t] + dz + 0.05 * torch.randn(latent_dim)\n",
    "\n",
    "        all_z.append(z_seq)\n",
    "        all_a.append(a_seq)\n",
    "\n",
    "    z_data = torch.stack(all_z)    # (n_seq, seq_len+1, latent_dim)\n",
    "    a_data = torch.stack(all_a)    # (n_seq, seq_len, action_dim)\n",
    "    return z_data, a_data\n",
    "\n",
    "z_data, a_data = generate_synthetic_data()\n",
    "print(f\"z_data shape: {z_data.shape}\")\n",
    "print(f\"a_data shape: {a_data.shape}\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize some trajectories\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "for i in range(50):\n",
    "    traj = z_data[i].numpy()\n",
    "    ax.plot(traj[:, 0], traj[:, 1], alpha=0.3, linewidth=1)\n",
    "    ax.scatter(traj[0, 0], traj[0, 1], c='green', s=20, zorder=5)\n",
    "    ax.scatter(traj[-1, 0], traj[-1, 1], c='red', s=20, zorder=5)\n",
    "\n",
    "ax.axvline(x=0.5, color='orange', linestyle='--', alpha=0.5, label='Bifurcation boundary')\n",
    "ax.set_xlabel('z[0]', fontsize=12)\n",
    "ax.set_ylabel('z[1]', fontsize=12)\n",
    "ax.set_title('Synthetic Trajectories (green=start, red=end)', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Notice how trajectories split when z[0] > 0.5 ‚Äî this is the bifurcation!\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_11_training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the MDN-RNN"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "z_input = z_data[:, :-1, :].to(device)   # z_t (all but last)\n",
    "z_target = z_data[:, 1:, :].to(device)   # z_{t+1} (all but first)\n",
    "a_input = a_data.to(device)               # a_t\n",
    "\n",
    "latent_dim = 2\n",
    "action_dim = 2\n",
    "model = MDNRNN(latent_dim=latent_dim, action_dim=action_dim,\n",
    "               hidden_dim=64, n_gaussians=5).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 80\n",
    "losses = []\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    # Simple batching\n",
    "    perm = torch.randperm(z_input.size(0))\n",
    "    for start in range(0, z_input.size(0), batch_size):\n",
    "        idx = perm[start:start + batch_size]\n",
    "        z_batch = z_input[idx]\n",
    "        a_batch = a_input[idx]\n",
    "        z_next_batch = z_target[idx]\n",
    "\n",
    "        pi, mu, sigma, _, _ = model(z_batch, a_batch)\n",
    "\n",
    "        # Reshape target to (batch * seq_len, latent_dim)\n",
    "        z_next_flat = z_next_batch.reshape(-1, latent_dim)\n",
    "        loss = mdn_loss(pi, mu, sigma, z_next_flat)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    losses.append(avg_loss)\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{num_epochs} | Loss: {avg_loss:.4f}\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Training curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('MDN NLL Loss', fontsize=12)\n",
    "plt.title('MDN-RNN Training Curve', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Multimodal Predictions\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_multimodal_predictions.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_12_multimodal_predictions"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing the Multimodal Predictions\n",
    "\n",
    "This is where the MDN shines. Let us look at predictions near the bifurcation boundary."
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Predict from a point near the bifurcation\n",
    "model.eval()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "test_positions = [\n",
    "    (torch.tensor([[[0.0, 0.0]]]), \"z[0]=0.0 (before bifurcation)\"),\n",
    "    (torch.tensor([[[0.5, 0.0]]]), \"z[0]=0.5 (at bifurcation)\"),\n",
    "    (torch.tensor([[[1.0, 0.0]]]), \"z[0]=1.0 (after bifurcation)\"),\n",
    "]\n",
    "\n",
    "for ax, (z_pos, title) in zip(axes, test_positions):\n",
    "    z_pos = z_pos.to(device)\n",
    "    test_action = torch.zeros(1, 1, action_dim).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pi, mu, sigma, _, _ = model(z_pos, test_action)\n",
    "\n",
    "    pi_np = pi[0, 1, :].cpu().numpy()     # Predictions for z[1] dimension\n",
    "    mu_np = mu[0, 1, :].cpu().numpy()\n",
    "    sigma_np = sigma[0, 1, :].cpu().numpy()\n",
    "\n",
    "    x = np.linspace(-3, 3, 300)\n",
    "    mixture = np.zeros_like(x)\n",
    "    colors = ['#2196F3', '#FF9800', '#4CAF50', '#E91E63', '#9C27B0']\n",
    "\n",
    "    for i in range(len(pi_np)):\n",
    "        if pi_np[i] > 0.05:  # Only show significant components\n",
    "            component = pi_np[i] * gaussian_pdf(x, mu_np[i], sigma_np[i])\n",
    "            mixture += component\n",
    "            ax.fill_between(x, component, alpha=0.3, color=colors[i],\n",
    "                            label=f'œÄ={pi_np[i]:.2f}, Œº={mu_np[i]:.2f}')\n",
    "\n",
    "    ax.plot(x, mixture, 'k-', linewidth=2)\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    ax.set_xlabel('Predicted z[1]', fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('MDN Predictions: Unimodal ‚Üí Bimodal Near Bifurcation', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"üí° Notice: near the bifurcation (z[0]‚âà0.5), the MDN uses MULTIPLE Gaussians!\")\n",
    "print(\"   A single Gaussian would predict the average ‚Äî which is wrong for both modes.\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Dreaming\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_dreaming.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_13_dreaming"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üéØ Final Output: Dreaming ‚Äî Generating Imagined Sequences\n",
    "\n",
    "Now for the most exciting part. We will use the trained MDN-RNN to \"dream\" ‚Äî generate imagined future trajectories by sampling from its own predictions, step by step."
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dream(model, z_start, actions, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate a dreamed trajectory by sampling from the MDN-RNN's predictions.\n",
    "\n",
    "    Args:\n",
    "        model: trained MDNRNN\n",
    "        z_start: starting latent state, shape (1, latent_dim)\n",
    "        actions: sequence of actions, shape (seq_len, action_dim)\n",
    "        temperature: sampling temperature (higher = more random)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    trajectory = [z_start.cpu().numpy().squeeze()]\n",
    "    z_t = z_start.unsqueeze(0).to(device)  # (1, 1, latent_dim)\n",
    "    hidden = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t in range(len(actions)):\n",
    "            a_t = actions[t].unsqueeze(0).unsqueeze(0).to(device)  # (1, 1, action_dim)\n",
    "\n",
    "            pi, mu, sigma, hidden, _ = model(z_t, a_t, hidden)\n",
    "\n",
    "            # Scale sigma by temperature\n",
    "            sigma = sigma * temperature\n",
    "\n",
    "            # Sample which Gaussian component to use\n",
    "            pi_np = pi[0, :, :].cpu().numpy()  # (latent_dim, K)\n",
    "            mu_np = mu[0, :, :].cpu().numpy()\n",
    "            sigma_np = sigma[0, :, :].cpu().numpy()\n",
    "\n",
    "            z_next = np.zeros(z_start.shape[-1])\n",
    "            for d in range(z_start.shape[-1]):\n",
    "                # Pick component\n",
    "                k = np.random.choice(len(pi_np[d]), p=pi_np[d])\n",
    "                # Sample from that component\n",
    "                z_next[d] = np.random.normal(mu_np[d, k], sigma_np[d, k])\n",
    "\n",
    "            trajectory.append(z_next)\n",
    "            z_t = torch.tensor(z_next, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "    return np.array(trajectory)\n",
    "\n",
    "# Dream multiple trajectories from the same starting point\n",
    "z_start = torch.tensor([0.5, 0.0])\n",
    "actions = torch.randn(30, action_dim) * 0.3\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: multiple dreams (low temperature ‚Äî confident)\n",
    "ax = axes[0]\n",
    "for i in range(20):\n",
    "    traj = dream(model, z_start, actions, temperature=0.8)\n",
    "    color = plt.cm.viridis(i / 20)\n",
    "    ax.plot(traj[:, 0], traj[:, 1], alpha=0.5, linewidth=1.5, color=color)\n",
    "ax.scatter([z_start[0]], [z_start[1]], c='red', s=100, zorder=10, label='Start')\n",
    "ax.set_title('20 Dreams (temperature=0.8)', fontsize=13)\n",
    "ax.set_xlabel('z[0]', fontsize=12)\n",
    "ax.set_ylabel('z[1]', fontsize=12)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: multiple dreams (high temperature ‚Äî explorative)\n",
    "ax = axes[1]\n",
    "for i in range(20):\n",
    "    traj = dream(model, z_start, actions, temperature=1.5)\n",
    "    color = plt.cm.magma(i / 20)\n",
    "    ax.plot(traj[:, 0], traj[:, 1], alpha=0.5, linewidth=1.5, color=color)\n",
    "ax.scatter([z_start[0]], [z_start[1]], c='red', s=100, zorder=10, label='Start')\n",
    "ax.set_title('20 Dreams (temperature=1.5 ‚Äî more explorative)', fontsize=13)\n",
    "ax.set_xlabel('z[0]', fontsize=12)\n",
    "ax.set_ylabel('z[1]', fontsize=12)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('üéØ The MDN-RNN Dreaming: Imagined Future Trajectories', fontsize=15, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéâ The MDN-RNN can dream! Each dream is different because the future is stochastic.\")\n",
    "print(\"   Lower temperature = more confident dreams (clustered).\")\n",
    "print(\"   Higher temperature = more explorative dreams (spread out).\")\n",
    "print(\"   This is exactly what the World Model uses to train its Controller!\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Real Vs Dream\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_real_vs_dream.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_14_real_vs_dream"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Compare dreamed vs real trajectories\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Real trajectories (from training data) in blue\n",
    "for i in range(30):\n",
    "    traj = z_data[i].numpy()\n",
    "    ax.plot(traj[:, 0], traj[:, 1], alpha=0.15, linewidth=1, color='blue')\n",
    "ax.plot([], [], color='blue', alpha=0.5, label='Real trajectories')\n",
    "\n",
    "# Dreamed trajectories in red\n",
    "for i in range(30):\n",
    "    z_start_rand = z_data[np.random.randint(len(z_data)), 0, :]\n",
    "    actions_rand = a_data[np.random.randint(len(a_data))]\n",
    "    traj = dream(model, z_start_rand, actions_rand, temperature=1.0)\n",
    "    ax.plot(traj[:, 0], traj[:, 1], alpha=0.15, linewidth=1, color='red')\n",
    "ax.plot([], [], color='red', alpha=0.5, label='Dreamed trajectories')\n",
    "\n",
    "ax.set_xlabel('z[0]', fontsize=12)\n",
    "ax.set_ylabel('z[1]', fontsize=12)\n",
    "ax.set_title('Real vs Dreamed Trajectories ‚Äî Do They Match?', fontsize=14)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Reflection And Close\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/15_reflection_and_close.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_15_reflection_and_close"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "1. What happens to the dreams if you train with $K = 1$ (single Gaussian)? Would the bifurcation behavior be captured?\n",
    "2. Why does temperature affect dream diversity? How does this relate to the exploration-exploitation tradeoff in RL?\n",
    "3. The LSTM hidden state acts as the agent's \"memory.\" What information is it storing that a simple feedforward network could not capture?\n",
    "\n",
    "### üèÜ Optional Challenges\n",
    "1. **Longer horizons**: Generate dreams of 100+ steps. Do they stay realistic or diverge? This is the \"compounding error\" problem.\n",
    "2. **Learned temperature**: Instead of a fixed temperature, make it a learned parameter. When should the model be more uncertain?\n",
    "3. **Attention-based dynamics**: Replace the LSTM with a Transformer. Does it predict better over long sequences?\n",
    "\n",
    "### What is Next?\n",
    "\n",
    "We have the Vision (VAE) and the Memory (MDN-RNN). The final piece is the Controller ‚Äî a remarkably simple linear layer that decides what action to take. But the twist is *how* we train it: using CMA-ES, an evolutionary strategy that optimizes the controller entirely inside the learned dream. That is next!"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "chatbot"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üí¨ AI Teaching Assistant ‚Äî Click ‚ñ∂ to start\n",
    "#@markdown This AI chatbot reads your notebook and can answer questions about any concept, code, or exercise.\n",
    "\n",
    "import json as _json\n",
    "import requests as _requests\n",
    "from google.colab import output as _output\n",
    "from IPython.display import display, HTML as _HTML, Markdown as _Markdown\n",
    "\n",
    "# --- Read notebook content for context ---\n",
    "def _get_notebook_context():\n",
    "    try:\n",
    "        from google.colab import _message\n",
    "        nb = _message.blocking_request(\"get_ipynb\", request=\"\", timeout_sec=10)\n",
    "        cells = nb.get(\"ipynb\", {}).get(\"cells\", [])\n",
    "        parts = []\n",
    "        for cell in cells:\n",
    "            src = \"\".join(cell.get(\"source\", []))\n",
    "            tags = cell.get(\"metadata\", {}).get(\"tags\", [])\n",
    "            if \"chatbot\" in tags:\n",
    "                continue\n",
    "            if src.strip():\n",
    "                ct = cell.get(\"cell_type\", \"unknown\")\n",
    "                parts.append(f\"[{ct.upper()}]\\n{src}\")\n",
    "        return \"\\n\\n---\\n\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"Notebook content unavailable.\"\n",
    "\n",
    "_NOTEBOOK_CONTEXT = _get_notebook_context()\n",
    "_CHAT_HISTORY = []\n",
    "_API_URL = \"https://course-creator-brown.vercel.app/api/chat\"\n",
    "\n",
    "def _notebook_chat(question):\n",
    "    global _CHAT_HISTORY\n",
    "    try:\n",
    "        resp = _requests.post(_API_URL, json={\n",
    "            'question': question,\n",
    "            'context': _NOTEBOOK_CONTEXT[:100000],\n",
    "            'history': _CHAT_HISTORY[-10:],\n",
    "        }, timeout=60)\n",
    "        data = resp.json()\n",
    "        answer = data.get('answer', 'Sorry, I could not generate a response.')\n",
    "        _CHAT_HISTORY.append({'role': 'user', 'content': question})\n",
    "        _CHAT_HISTORY.append({'role': 'assistant', 'content': answer})\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f'Error connecting to teaching assistant: {str(e)}'\n",
    "\n",
    "_output.register_callback('notebook_chat', _notebook_chat)\n",
    "\n",
    "def ask(question):\n",
    "    \"\"\"Ask the AI teaching assistant a question about this notebook.\"\"\"\n",
    "    answer = _notebook_chat(question)\n",
    "    display(_Markdown(answer))\n",
    "\n",
    "print(\"\\u2705 AI Teaching Assistant is ready!\")\n",
    "print(\"\\U0001f4a1 Use the chat below, or call ask(\\'your question\\') in any cell.\")\n",
    "\n",
    "# --- Display chat widget ---\n",
    "display(_HTML('''<style>\n  .vc-wrap{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;max-width:100%;border-radius:16px;overflow:hidden;box-shadow:0 4px 24px rgba(0,0,0,.12);background:#fff;border:1px solid #e5e7eb}\n  .vc-hdr{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;padding:16px 20px;display:flex;align-items:center;gap:12px}\n  .vc-avatar{width:42px;height:42px;background:rgba(255,255,255,.2);border-radius:50%;display:flex;align-items:center;justify-content:center;font-size:22px}\n  .vc-hdr h3{font-size:16px;font-weight:600;margin:0}\n  .vc-hdr p{font-size:12px;opacity:.85;margin:2px 0 0}\n  .vc-msgs{height:420px;overflow-y:auto;padding:16px;background:#f8f9fb;display:flex;flex-direction:column;gap:10px}\n  .vc-msg{display:flex;flex-direction:column;animation:vc-fade .25s ease}\n  .vc-msg.user{align-items:flex-end}\n  .vc-msg.bot{align-items:flex-start}\n  .vc-bbl{max-width:85%;padding:10px 14px;border-radius:16px;font-size:14px;line-height:1.55;word-wrap:break-word}\n  .vc-msg.user .vc-bbl{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border-bottom-right-radius:4px}\n  .vc-msg.bot .vc-bbl{background:#fff;color:#1a1a2e;border:1px solid #e8e8e8;border-bottom-left-radius:4px}\n  .vc-bbl code{background:rgba(0,0,0,.07);padding:2px 6px;border-radius:4px;font-size:13px;font-family:'Fira Code',monospace}\n  .vc-bbl pre{background:#1e1e2e;color:#cdd6f4;padding:12px;border-radius:8px;overflow-x:auto;margin:8px 0;font-size:13px}\n  .vc-bbl pre code{background:none;padding:0;color:inherit}\n  .vc-bbl h3,.vc-bbl h4{margin:10px 0 4px;font-size:15px}\n  .vc-bbl ul,.vc-bbl ol{margin:4px 0;padding-left:20px}\n  .vc-bbl li{margin:2px 0}\n  .vc-chips{display:flex;flex-wrap:wrap;gap:8px;padding:0 16px 12px;background:#f8f9fb}\n  .vc-chip{background:#fff;border:1px solid #d1d5db;border-radius:20px;padding:6px 14px;font-size:12px;cursor:pointer;transition:all .15s;color:#4b5563}\n  .vc-chip:hover{border-color:#667eea;color:#667eea;background:#f0f0ff}\n  .vc-input{display:flex;padding:12px 16px;background:#fff;border-top:1px solid #eee;gap:8px}\n  .vc-input input{flex:1;padding:10px 16px;border:2px solid #e8e8e8;border-radius:24px;font-size:14px;outline:none;transition:border-color .2s}\n  .vc-input input:focus{border-color:#667eea}\n  .vc-input button{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border:none;border-radius:50%;width:42px;height:42px;cursor:pointer;display:flex;align-items:center;justify-content:center;font-size:18px;transition:transform .1s}\n  .vc-input button:hover{transform:scale(1.05)}\n  .vc-input button:disabled{opacity:.5;cursor:not-allowed;transform:none}\n  .vc-typing{display:flex;gap:5px;padding:4px 0}\n  .vc-typing span{width:8px;height:8px;background:#667eea;border-radius:50%;animation:vc-bounce 1.4s infinite ease-in-out}\n  .vc-typing span:nth-child(2){animation-delay:.2s}\n  .vc-typing span:nth-child(3){animation-delay:.4s}\n  @keyframes vc-bounce{0%,80%,100%{transform:scale(0)}40%{transform:scale(1)}}\n  @keyframes vc-fade{from{opacity:0;transform:translateY(8px)}to{opacity:1;transform:translateY(0)}}\n  .vc-note{text-align:center;font-size:11px;color:#9ca3af;padding:8px 16px 12px;background:#fff}\n</style>\n<div class=\"vc-wrap\">\n  <div class=\"vc-hdr\">\n    <div class=\"vc-avatar\">&#129302;</div>\n    <div>\n      <h3>Vizuara Teaching Assistant</h3>\n      <p>Ask me anything about this notebook</p>\n    </div>\n  </div>\n  <div class=\"vc-msgs\" id=\"vcMsgs\">\n    <div class=\"vc-msg bot\">\n      <div class=\"vc-bbl\">&#128075; Hi! I've read through this entire notebook. Ask me about any concept, code block, or exercise &mdash; I'm here to help you learn!</div>\n    </div>\n  </div>\n  <div class=\"vc-chips\" id=\"vcChips\">\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Explain the main concept</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Help with the TODO exercise</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Summarize what I learned</span>\n  </div>\n  <div class=\"vc-input\">\n    <input type=\"text\" id=\"vcIn\" placeholder=\"Ask about concepts, code, exercises...\" />\n    <button id=\"vcSend\" onclick=\"vcSendMsg()\">&#10148;</button>\n  </div>\n  <div class=\"vc-note\">AI-generated &middot; Verify important information &middot; <a href=\"#\" onclick=\"vcClear();return false\" style=\"color:#667eea\">Clear chat</a></div>\n</div>\n<script>\n(function(){\n  var msgs=document.getElementById('vcMsgs'),inp=document.getElementById('vcIn'),\n      btn=document.getElementById('vcSend'),chips=document.getElementById('vcChips');\n\n  function esc(s){var d=document.createElement('div');d.textContent=s;return d.innerHTML}\n\n  function md(t){\n    return t\n      .replace(/```(\\w*)\\n([\\s\\S]*?)```/g,function(_,l,c){return '<pre><code>'+esc(c)+'</code></pre>'})\n      .replace(/`([^`]+)`/g,'<code>$1</code>')\n      .replace(/\\*\\*([^*]+)\\*\\*/g,'<strong>$1</strong>')\n      .replace(/\\*([^*]+)\\*/g,'<em>$1</em>')\n      .replace(/^#### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^## (.+)$/gm,'<h3>$1</h3>')\n      .replace(/^\\d+\\. (.+)$/gm,'<li>$1</li>')\n      .replace(/^- (.+)$/gm,'<li>$1</li>')\n      .replace(/\\n\\n/g,'<br><br>')\n      .replace(/\\n/g,'<br>');\n  }\n\n  function addMsg(text,isUser){\n    var m=document.createElement('div');m.className='vc-msg '+(isUser?'user':'bot');\n    var b=document.createElement('div');b.className='vc-bbl';\n    b.innerHTML=isUser?esc(text):md(text);\n    m.appendChild(b);msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function showTyping(){\n    var m=document.createElement('div');m.className='vc-msg bot';m.id='vcTyping';\n    m.innerHTML='<div class=\"vc-bbl\"><div class=\"vc-typing\"><span></span><span></span><span></span></div></div>';\n    msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function hideTyping(){var e=document.getElementById('vcTyping');if(e)e.remove()}\n\n  window.vcSendMsg=function(){\n    var q=inp.value.trim();if(!q)return;\n    inp.value='';chips.style.display='none';\n    addMsg(q,true);showTyping();btn.disabled=true;\n    google.colab.kernel.invokeFunction('notebook_chat',[q],{})\n      .then(function(r){\n        hideTyping();\n        var a=r.data['application/json'];\n        addMsg(typeof a==='string'?a:JSON.stringify(a),false);\n      })\n      .catch(function(){\n        hideTyping();\n        addMsg('Sorry, I encountered an error. Please check your internet connection and try again.',false);\n      })\n      .finally(function(){btn.disabled=false;inp.focus()});\n  };\n\n  window.vcAsk=function(q){inp.value=q;vcSendMsg()};\n  window.vcClear=function(){\n    msgs.innerHTML='<div class=\"vc-msg bot\"><div class=\"vc-bbl\">&#128075; Chat cleared. Ask me anything!</div></div>';\n    chips.style.display='flex';\n  };\n\n  inp.addEventListener('keypress',function(e){if(e.key==='Enter')vcSendMsg()});\n  inp.focus();\n})();\n</script>'''))"
   ],
   "id": "vizuara_chatbot"
  }
 ]
}