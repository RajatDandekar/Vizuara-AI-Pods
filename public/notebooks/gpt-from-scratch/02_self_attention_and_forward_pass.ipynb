{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "02_self_attention_and_forward_pass \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention and the Forward Pass: Inside the Transformer Block\n",
    "\n",
    "*Part 2 of the Vizuara series on Building a GPT-Style Model from Scratch*\n",
    "*Estimated time: 60 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebook, we converted text into embedding vectors. But embeddings alone are not enough -- the word \"bank\" has the same embedding whether it appears in \"river bank\" or \"bank account.\" For the model to understand language, each token needs to be aware of its surrounding context.\n",
    "\n",
    "**Self-attention** is the mechanism that makes this possible. It is, without exaggeration, the single most important innovation in modern language modeling. Self-attention allows every token to look at every other token (subject to causal masking) and decide which tokens are relevant to its meaning. This is what transforms a bag of isolated word vectors into a rich, context-aware representation of language.\n",
    "\n",
    "In this notebook, we will build the complete forward pass of a GPT model from scratch:\n",
    "- Scaled dot-product attention with causal masking\n",
    "- Multi-head attention\n",
    "- Residual connections and layer normalization\n",
    "- The feed-forward network\n",
    "- The full Transformer block, stacked into a GPT\n",
    "\n",
    "By the end, you will have a working forward pass that takes token embeddings and produces next-token predictions."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### The Library Analogy\n",
    "\n",
    "Imagine you are at a library with a question (your **Query**). You walk up to each book on the shelf and read its title (the **Key**). The degree to which your question matches a book's title determines how much attention you pay to its content (the **Value**).\n",
    "\n",
    "Self-attention works exactly this way. Each token generates:\n",
    "- A **Query**: \"What am I looking for?\"\n",
    "- A **Key**: \"What do I contain?\"\n",
    "- A **Value**: \"What information do I carry?\"\n",
    "\n",
    "The Query of one token is compared against the Keys of all other tokens. High similarity means \"pay attention to this token's Value.\" Low similarity means \"ignore it.\"\n",
    "\n",
    "### Why Causal Masking?\n",
    "\n",
    "In GPT, each token can only attend to tokens that came *before* it -- never to future tokens. Think about it: when you are predicting the next word after \"The cat sat on the,\" you should not be able to peek at the answer. The model must generate tokens left to right, one at a time, using only the past as context.\n",
    "\n",
    "This is enforced by setting the attention scores for future positions to negative infinity before the softmax. After softmax, these become zero -- the model literally cannot see the future.\n",
    "\n",
    "### Why Multiple Heads?\n",
    "\n",
    "A single attention head can only learn one pattern of attention. But language is full of simultaneous relationships. In the sentence \"The cat, which was orange, sat on the warm mat,\" we simultaneously need:\n",
    "- \"sat\" to relate to \"cat\" (subject-verb)\n",
    "- \"orange\" to relate to \"cat\" (adjective-noun)\n",
    "- \"warm\" to relate to \"mat\" (adjective-noun)\n",
    "\n",
    "Multiple heads allow the model to track all of these patterns in parallel, each head specializing in a different type of relationship.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "When you read the sentence \"The animal didn't cross the street because it was too tired,\" how do you know that \"it\" refers to \"the animal\" rather than \"the street\"? Your brain is performing something very similar to self-attention -- looking back at earlier words and determining which ones are relevant to understanding the current word."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Scaled Dot-Product Attention\n",
    "\n",
    "For input $X \\in \\mathbb{R}^{T \\times d}$, we compute:\n",
    "\n",
    "$$Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V$$\n",
    "\n",
    "This says: multiply the input by three different learned weight matrices to produce Queries, Keys, and Values. Each is a linear projection of the same input, but the different weight matrices allow each to capture different aspects of the input.\n",
    "\n",
    "The attention scores are:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + \\text{Mask}\\right) V$$\n",
    "\n",
    "Breaking this down computationally:\n",
    "1. $QK^T$ computes the dot product between every pair of Query and Key vectors -- this is a $(T \\times T)$ matrix of raw similarity scores.\n",
    "2. Dividing by $\\sqrt{d_k}$ keeps the values in a stable range for softmax. Without this, large $d_k$ values would push dot products to extreme values.\n",
    "3. Adding the Mask sets future positions to $-\\infty$.\n",
    "4. Softmax converts the scores to probabilities (each row sums to 1).\n",
    "5. Multiplying by $V$ produces a weighted sum of Value vectors.\n",
    "\n",
    "### Layer Normalization\n",
    "\n",
    "$$\\text{LayerNorm}(\\mathbf{x}) = \\gamma \\cdot \\frac{\\mathbf{x} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "This normalizes each vector to have zero mean and unit variance, then applies a learned scale ($\\gamma$) and shift ($\\beta$). Computationally: subtract the mean, divide by the standard deviation, then rescale. This keeps activations stable across layers.\n",
    "\n",
    "For a concrete example, if $\\mathbf{x} = [4.0, 2.0, 6.0, 0.0]$:\n",
    "- Mean: $\\mu = 3.0$\n",
    "- Variance: $\\sigma^2 = 5.0$\n",
    "- Normalized: $[0.447, -0.447, 1.342, -1.342]$\n",
    "\n",
    "### Feed-Forward Network\n",
    "\n",
    "$$\\text{FFN}(\\mathbf{x}) = W_2 \\cdot \\text{GELU}(W_1 \\mathbf{x} + b_1) + b_2$$\n",
    "\n",
    "This expands the representation from $d_{\\text{model}}$ to $4 \\times d_{\\text{model}}$, applies a non-linear activation (GELU), and compresses back. Think of it as the model's \"thinking\" step -- attention gathers information, and the FFN processes it.\n",
    "\n",
    "### Residual Connections\n",
    "\n",
    "$$\\mathbf{x}_{\\text{out}} = \\mathbf{x}_{\\text{in}} + f(\\mathbf{x}_{\\text{in}})$$\n",
    "\n",
    "Adding the input back to the output creates a \"gradient highway\" -- during backpropagation, gradients can flow directly through the skip connection even if the transformation $f$ has small gradients. This is essential for training deep networks."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 Scaled Dot-Product Attention (Single Head)\n",
    "\n",
    "Let us start by building the most fundamental piece: scaled dot-product attention for a single head."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "\n",
    "    Args:\n",
    "        Q: queries, shape (B, T, d_k) or (B, n_heads, T, d_k)\n",
    "        K: keys, shape (B, T, d_k) or (B, n_heads, T, d_k)\n",
    "        V: values, shape (B, T, d_k) or (B, n_heads, T, d_k)\n",
    "        mask: boolean mask, True = positions to mask (set to -inf)\n",
    "\n",
    "    Returns:\n",
    "        output: attention-weighted values\n",
    "        attention_weights: the attention weight matrix\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "\n",
    "    # Step 1: Compute raw attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))  # (B, ..., T, T)\n",
    "\n",
    "    # Step 2: Scale by sqrt(d_k)\n",
    "    scores = scores / (d_k ** 0.5)\n",
    "\n",
    "    # Step 3: Apply causal mask (set future to -inf)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "\n",
    "    # Step 4: Softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # Step 5: Weighted sum of values\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "# Test with the exact example from the article\n",
    "Q = torch.tensor([[[1., 0., 1., 0.],\n",
    "                    [0., 1., 0., 1.],\n",
    "                    [1., 1., 0., 0.]]])  # (1, 3, 4)\n",
    "\n",
    "K = torch.tensor([[[1., 1., 0., 0.],\n",
    "                    [0., 0., 1., 1.],\n",
    "                    [1., 0., 1., 0.]]])  # (1, 3, 4)\n",
    "\n",
    "V = torch.tensor([[[1., 0., 0., 0.],\n",
    "                    [0., 1., 0., 0.],\n",
    "                    [0., 0., 1., 0.]]])  # (1, 3, 4) -- identity-like for clarity\n",
    "\n",
    "# Create causal mask\n",
    "T = 3\n",
    "mask = torch.triu(torch.ones(T, T), diagonal=1).bool()\n",
    "print(\"Causal mask (True = blocked):\")\n",
    "print(mask.int())\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "print(f\"\\nAttention weights:\\n{weights[0].detach().numpy().round(3)}\")\n",
    "print(f\"\\nOutput:\\n{output[0].detach().numpy().round(3)}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the attention mechanism step by step\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "# Raw scores\n",
    "raw_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "im0 = axes[0].imshow(raw_scores[0].detach().numpy(), cmap='YlOrRd', vmin=-1, vmax=3)\n",
    "axes[0].set_title('Step 1: Raw Scores\\n(Q @ K^T)')\n",
    "axes[0].set_xlabel('Key Position')\n",
    "axes[0].set_ylabel('Query Position')\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[0].text(j, i, f'{raw_scores[0,i,j]:.1f}', ha='center', va='center')\n",
    "plt.colorbar(im0, ax=axes[0])\n",
    "\n",
    "# Scaled scores\n",
    "d_k = Q.shape[-1]\n",
    "scaled = raw_scores / (d_k ** 0.5)\n",
    "im1 = axes[1].imshow(scaled[0].detach().numpy(), cmap='YlOrRd', vmin=-0.5, vmax=1.5)\n",
    "axes[1].set_title(f'Step 2: Scaled\\n(/ sqrt({d_k}) = / {d_k**0.5:.1f})')\n",
    "axes[1].set_xlabel('Key Position')\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[1].text(j, i, f'{scaled[0,i,j]:.2f}', ha='center', va='center')\n",
    "plt.colorbar(im1, ax=axes[1])\n",
    "\n",
    "# After masking\n",
    "masked = scaled.clone()\n",
    "masked[0] = masked[0].masked_fill(mask, float('-inf'))\n",
    "display_masked = masked[0].clone()\n",
    "display_masked[display_masked == float('-inf')] = -3  # For display\n",
    "im2 = axes[2].imshow(display_masked.detach().numpy(), cmap='YlOrRd', vmin=-3, vmax=1.5)\n",
    "axes[2].set_title('Step 3: After Causal Mask\\n(future = -inf)')\n",
    "axes[2].set_xlabel('Key Position')\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        val = masked[0, i, j].item()\n",
    "        text = '-inf' if val == float('-inf') else f'{val:.2f}'\n",
    "        axes[2].text(j, i, text, ha='center', va='center', fontsize=9)\n",
    "plt.colorbar(im2, ax=axes[2])\n",
    "\n",
    "# After softmax\n",
    "im3 = axes[3].imshow(weights[0].detach().numpy(), cmap='YlOrRd', vmin=0, vmax=1)\n",
    "axes[3].set_title('Step 4: After Softmax\\n(each row sums to 1)')\n",
    "axes[3].set_xlabel('Key Position')\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[3].text(j, i, f'{weights[0,i,j]:.2f}', ha='center', va='center')\n",
    "plt.colorbar(im3, ax=axes[3])\n",
    "\n",
    "plt.suptitle('Scaled Dot-Product Attention: Step by Step', fontsize=14, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Causal Self-Attention Module\n",
    "\n",
    "Now let us wrap this into a proper PyTorch module with learnable Q, K, V projections."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Single-head causal self-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        Q = self.W_q(x)  # (B, T, d_model)\n",
    "        K = self.W_k(x)  # (B, T, d_model)\n",
    "        V = self.W_v(x)  # (B, T, d_model)\n",
    "\n",
    "        # Causal mask\n",
    "        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
    "\n",
    "        output, weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        return output, weights\n",
    "\n",
    "# Test\n",
    "attn = CausalSelfAttention(d_model=64)\n",
    "x = torch.randn(2, 10, 64)  # batch=2, seq_len=10, d_model=64\n",
    "out, w = attn(x)\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Attention weights shape: {w.shape}\")\n",
    "print(f\"Row 0 sums to: {w[0, 0].sum().item():.4f}\")\n",
    "print(f\"Row 5 sums to: {w[0, 5].sum().item():.4f}\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Multi-Head Self-Attention\n",
    "\n",
    "The key idea: split the embedding dimension across multiple heads, run attention independently in each head, concatenate the results, and project back."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCausalAttention(nn.Module):\n",
    "    \"\"\"Multi-head causal self-attention (the real thing).\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "        # Combined QKV projection (efficiency trick)\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Compute Q, K, V for all heads at once\n",
    "        qkv = self.qkv(x)  # (B, T, 3 * d_model)\n",
    "        qkv = qkv.reshape(B, T, 3, self.n_heads, self.d_k)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, n_heads, T, d_k)\n",
    "        Q, K, V = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # Causal mask\n",
    "        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
    "\n",
    "        # Scaled dot-product attention per head\n",
    "        scores = (Q @ K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        out = weights @ V  # (B, n_heads, T, d_k)\n",
    "\n",
    "        # Concatenate heads and project\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)  # (B, T, d_model)\n",
    "        out = self.proj(out)\n",
    "\n",
    "        return out, weights\n",
    "\n",
    "# Test\n",
    "mha = MultiHeadCausalAttention(d_model=64, n_heads=4)\n",
    "x = torch.randn(2, 10, 64)\n",
    "out, weights = mha(x)\n",
    "print(f\"Input shape:            {x.shape}\")\n",
    "print(f\"Output shape:           {out.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}  (batch, heads, T, T)\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns across heads\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "for h in range(4):\n",
    "    im = axes[h].imshow(weights[0, h].detach().numpy(), cmap='viridis', vmin=0)\n",
    "    axes[h].set_title(f'Head {h+1}')\n",
    "    axes[h].set_xlabel('Key Position')\n",
    "    if h == 0:\n",
    "        axes[h].set_ylabel('Query Position')\n",
    "    plt.colorbar(im, ax=axes[h])\n",
    "\n",
    "plt.suptitle('Attention Patterns Across 4 Heads (random init)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The Feed-Forward Network"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, expansion_factor=4):\n",
    "        super().__init__()\n",
    "        d_ff = d_model * expansion_factor\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Test and show the expansion\n",
    "ffn = FeedForward(d_model=64)\n",
    "x = torch.randn(1, 10, 64)\n",
    "out = ffn(x)\n",
    "print(f\"Input shape:  {x.shape}  (d_model = 64)\")\n",
    "print(f\"Internal:     (1, 10, 256)  (expanded 4x)\")\n",
    "print(f\"Output shape: {out.shape}  (compressed back)\")\n",
    "print(f\"\\nFFN parameters: {sum(p.numel() for p in ffn.parameters()):,}\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 The Complete Transformer Block"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"One Transformer block: attention + FFN, each with residual + layer norm.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadCausalAttention(d_model, n_heads)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-norm architecture (GPT-2 style)\n",
    "        attn_out, weights = self.attn(self.ln1(x))\n",
    "        x = x + attn_out          # Residual connection\n",
    "        x = x + self.ffn(self.ln2(x))  # Residual connection\n",
    "        return x, weights\n",
    "\n",
    "# Test\n",
    "block = TransformerBlock(d_model=64, n_heads=4)\n",
    "x = torch.randn(2, 10, 64)\n",
    "out, weights = block(x)\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Block parameters: {sum(p.numel() for p in block.parameters()):,}\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the residual connection effect\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "x = torch.randn(1, 8, 64)\n",
    "with torch.no_grad():\n",
    "    attn_out, _ = block.attn(block.ln1(x))\n",
    "    after_residual = x + attn_out\n",
    "\n",
    "data = [x[0].numpy(), attn_out[0].detach().numpy(), after_residual[0].detach().numpy()]\n",
    "titles = ['Input x', 'Attention Output', 'x + Attention(x)\\n(Residual Connection)']\n",
    "\n",
    "for ax, d, t in zip(axes, data, titles):\n",
    "    im = ax.imshow(d, aspect='auto', cmap='RdBu_r', vmin=-3, vmax=3)\n",
    "    ax.set_xlabel('Dimension')\n",
    "    ax.set_ylabel('Position')\n",
    "    ax.set_title(t)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Notice: the residual output preserves the overall structure of the input,\")\n",
    "print(\"with subtle modifications from the attention layer. This is by design --\")\n",
    "print(\"each layer makes small, incremental changes rather than completely transforming the data.\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Implement the Full GPT Forward Pass"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete GPT model: embeddings + N transformer blocks + output head.\n",
    "\n",
    "    Architecture:\n",
    "        1. Token embedding + positional embedding\n",
    "        2. N stacked Transformer blocks\n",
    "        3. Final layer norm\n",
    "        4. Linear projection to vocabulary size\n",
    "\n",
    "    Args:\n",
    "        vocab_size: number of tokens in vocabulary\n",
    "        d_model: embedding and hidden dimension\n",
    "        n_heads: number of attention heads per block\n",
    "        n_layers: number of Transformer blocks\n",
    "        max_seq_len: maximum sequence length\n",
    "\n",
    "    Forward:\n",
    "        Input: token_ids of shape (batch_size, seq_len)\n",
    "        Output: logits of shape (batch_size, seq_len, vocab_size)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, max_seq_len):\n",
    "        super().__init__()\n",
    "        # ============ TODO ============\n",
    "        # Step 1: Token embedding table (nn.Embedding)\n",
    "        # Step 2: Positional embedding table (nn.Embedding)\n",
    "        # Step 3: Stack of N Transformer blocks (nn.ModuleList)\n",
    "        # Step 4: Final layer norm (nn.LayerNorm)\n",
    "        # Step 5: Output projection head (nn.Linear, no bias)\n",
    "        # ==============================\n",
    "        self.token_emb = None   # YOUR CODE HERE\n",
    "        self.pos_emb = None     # YOUR CODE HERE\n",
    "        self.blocks = None      # YOUR CODE HERE\n",
    "        self.ln_f = None        # YOUR CODE HERE\n",
    "        self.head = None        # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx: (batch_size, seq_len) tensor of token IDs\n",
    "\n",
    "        Returns:\n",
    "            logits: (batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        # ============ TODO ============\n",
    "        # Step 1: Get batch size B and sequence length T\n",
    "        # Step 2: Look up token embeddings\n",
    "        # Step 3: Look up positional embeddings for positions 0..T-1\n",
    "        # Step 4: Add token + positional embeddings\n",
    "        # Step 5: Pass through each Transformer block\n",
    "        # Step 6: Apply final layer norm\n",
    "        # Step 7: Project to vocabulary size\n",
    "        # ==============================\n",
    "        logits = None  # YOUR CODE HERE\n",
    "        return logits"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "model = GPT(vocab_size=256, d_model=64, n_heads=4, n_layers=4, max_seq_len=128)\n",
    "test_input = torch.randint(0, 256, (2, 20))  # batch=2, seq_len=20\n",
    "logits = model(test_input)\n",
    "\n",
    "assert logits is not None, \"Forward returned None\"\n",
    "assert logits.shape == (2, 20, 256), f\"Expected (2, 20, 256), got {logits.shape}\"\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Output shape: {logits.shape}\")\n",
    "print(f\"All assertions passed! Your GPT forward pass is correct.\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Analyze Layer-by-Layer Representations"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_layer_representations(model, text, tokenizer):\n",
    "    \"\"\"\n",
    "    Pass text through the model and visualize how representations\n",
    "    change at each layer.\n",
    "\n",
    "    Steps:\n",
    "        1. Tokenize the text and embed it\n",
    "        2. Pass through each Transformer block, saving the output\n",
    "        3. Compute cosine similarity between all token pairs at each layer\n",
    "        4. Plot the similarity matrices side by side\n",
    "\n",
    "    This shows how the model builds increasingly context-aware\n",
    "    representations as data flows through the layers.\n",
    "\n",
    "    Args:\n",
    "        model: your GPT instance\n",
    "        text: string to analyze\n",
    "        tokenizer: CharTokenizer instance\n",
    "\n",
    "    Returns:\n",
    "        list of similarity matrices, one per layer\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Hint: You need to manually run each component of the forward pass\n",
    "    # and save intermediate results. Start with embedding, then loop\n",
    "    # through model.blocks, saving the output of each block.\n",
    "    # ==============================\n",
    "    pass"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full forward pass demonstration\n",
    "model = GPT(vocab_size=256, d_model=64, n_heads=4, n_layers=4, max_seq_len=128)\n",
    "\n",
    "# Character tokenizer\n",
    "class CharTokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 256\n",
    "        self.char_to_id = {chr(i): i for i in range(256)}\n",
    "        self.id_to_char = {i: chr(i) for i in range(256)}\n",
    "    def encode(self, text):\n",
    "        return [self.char_to_id.get(ch, 0) for ch in text]\n",
    "    def decode(self, ids):\n",
    "        return ''.join(self.id_to_char.get(i, '?') for i in ids)\n",
    "\n",
    "tokenizer = CharTokenizer()\n",
    "text = \"The cat sat on the mat.\"\n",
    "token_ids = torch.tensor([tokenizer.encode(text)])\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(token_ids)\n",
    "\n",
    "print(f\"Input:  '{text}'\")\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"\\nAt each position, the model outputs a probability distribution\")\n",
    "print(f\"over {logits.shape[-1]} possible next characters.\")\n",
    "\n",
    "# Show predicted next characters (before training -- expect garbage)\n",
    "probs = F.softmax(logits[0], dim=-1)\n",
    "for i, char in enumerate(text):\n",
    "    top_prob, top_idx = probs[i].topk(3)\n",
    "    preds = [(tokenizer.decode([idx.item()]), f\"{p.item():.3f}\") for idx, p in zip(top_idx, top_prob)]\n",
    "    print(f\"  After '{text[:i+1]}' -> top predictions: {preds}\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the full GPT model on Shakespeare-like text\n",
    "import torch.nn.functional as F\n",
    "\n",
    "train_text = \"\"\"\n",
    "To be or not to be that is the question\n",
    "Whether tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune\n",
    "Or to take arms against a sea of troubles\n",
    "And by opposing end them To die to sleep\n",
    "No more and by a sleep to say we end\n",
    "The heartache and the thousand natural shocks\n",
    "That flesh is heir to tis a consummation\n",
    "Devoutly to be wished To die to sleep\n",
    "To sleep perchance to dream ay there is the rub\n",
    "For in that sleep of death what dreams may come\n",
    "\"\"\" * 20\n",
    "\n",
    "model = GPT(vocab_size=256, d_model=64, n_heads=4, n_layers=4, max_seq_len=128)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "data = torch.tensor(tokenizer.encode(train_text))\n",
    "seq_len = 64\n",
    "\n",
    "losses = []\n",
    "for step in range(1000):\n",
    "    idx = torch.randint(0, len(data) - seq_len - 1, (16,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in idx])\n",
    "\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits.view(-1, 256), y.view(-1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    if step % 200 == 0:\n",
    "        print(f\"Step {step}: loss = {loss.item():.4f}\")\n",
    "\n",
    "print(f\"\\nFinal loss: {losses[-1]:.4f}\")"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(losses, alpha=0.3, color='blue')\n",
    "window = 30\n",
    "smoothed = [np.mean(losses[max(0,i-window):i+1]) for i in range(len(losses))]\n",
    "axes[0].plot(smoothed, color='blue', linewidth=2)\n",
    "axes[0].axhline(y=np.log(256), color='red', linestyle='--', label=f'Random: {np.log(256):.2f}')\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# Attention visualization from trained model\n",
    "with torch.no_grad():\n",
    "    test_text = \"To be or not to be\"\n",
    "    test_ids = torch.tensor([tokenizer.encode(test_text)])\n",
    "    # Get attention weights from the first block\n",
    "    h = model.token_emb(test_ids) + model.pos_emb(torch.arange(test_ids.shape[1]))\n",
    "    _, attn_weights = model.blocks[0].attn(model.blocks[0].ln1(h))\n",
    "\n",
    "chars = list(test_text)\n",
    "im = axes[1].imshow(attn_weights[0, 0, :len(chars), :len(chars)].numpy(), cmap='viridis')\n",
    "axes[1].set_xticks(range(len(chars)))\n",
    "axes[1].set_xticklabels(chars, fontfamily='monospace')\n",
    "axes[1].set_yticks(range(len(chars)))\n",
    "axes[1].set_yticklabels(chars, fontfamily='monospace')\n",
    "axes[1].set_title('Learned Attention Pattern (Head 1, Layer 1)')\n",
    "plt.colorbar(im, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with the trained model\n",
    "def generate(model, tokenizer, prompt, max_new_tokens=100, temperature=0.8):\n",
    "    \"\"\"Generate text autoregressively.\"\"\"\n",
    "    model.eval()\n",
    "    ids = tokenizer.encode(prompt)\n",
    "    context = torch.tensor([ids])\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop to max_seq_len if needed\n",
    "        if context.shape[1] > 128:\n",
    "            context = context[:, -128:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(context)\n",
    "\n",
    "        # Get logits for the last position\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        context = torch.cat([context, next_token], dim=1)\n",
    "\n",
    "    generated = tokenizer.decode(context[0].tolist())\n",
    "    return generated\n",
    "\n",
    "# Generate with different prompts\n",
    "print(\"=\" * 60)\n",
    "print(\"  TEXT GENERATION FROM TRAINED GPT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in [\"To be\", \"The \", \"and by\"]:\n",
    "    result = generate(model, tokenizer, prompt, max_new_tokens=80)\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(f\"Generated: {result}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nYou have built the complete forward pass of a GPT model from scratch!\")\n",
    "print(\"The model takes text, embeds it, processes it through Transformer blocks,\")\n",
    "print(\"and generates new text one character at a time.\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "1. We used Pre-LN (layer norm before attention/FFN) rather than Post-LN (after). Why does GPT-2 prefer Pre-LN? What happens to training stability if you switch to Post-LN?\n",
    "2. Each attention head gets d_model / n_heads dimensions. If d_model = 64 and n_heads = 4, each head has only 16 dimensions. Is this enough? What is the tradeoff between more heads with fewer dimensions vs fewer heads with more dimensions?\n",
    "3. The FFN expands to 4x the model dimension. What do you think would happen if you used 2x instead? Or 8x?\n",
    "\n",
    "### Optional Challenges\n",
    "1. Modify the model to use dropout (add nn.Dropout after attention and FFN). Train with and without dropout and compare the gap between training and validation loss.\n",
    "2. Implement \"attention head pruning\" -- after training, zero out one head at a time and measure the impact on loss. Which heads are most important?\n",
    "3. Increase the number of layers from 4 to 12 and observe what happens to training stability. Does Pre-LN really help?"
   ],
   "id": "cell_30"
  }
 ]
}