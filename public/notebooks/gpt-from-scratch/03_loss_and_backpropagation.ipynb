{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "03_loss_and_backpropagation \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions and Backpropagation: How GPT Learns\n",
    "\n",
    "*Part 3 of the Vizuara series on Building a GPT-Style Model from Scratch*\n",
    "*Estimated time: 50 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "We have built the forward pass of a GPT model -- text goes in, probability distributions come out. But how does the model actually *learn*? How does it go from random predictions to generating coherent text?\n",
    "\n",
    "The answer lies in two ideas that are deceptively simple yet profoundly powerful:\n",
    "\n",
    "1. **The loss function** measures how wrong the model is. It assigns a single number to the quality of the model's predictions -- lower is better.\n",
    "2. **Backpropagation** computes the gradient of this loss with respect to every parameter in the model, telling each parameter exactly how to change to make the predictions better.\n",
    "\n",
    "These two ideas, combined in a loop, are the entire learning algorithm. Every AI model you have ever heard of -- GPT-4, DALL-E, AlphaFold -- learns through this same loop: predict, measure error, compute gradients, update weights. In this notebook, we will build both the loss function and the backpropagation process from first principles, with full numerical examples.\n",
    "\n",
    "By the end, you will understand exactly what `loss.backward()` does under the hood."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### The Mountain Analogy\n",
    "\n",
    "Imagine you are standing on a mountain in thick fog. You cannot see the valley below, but you want to get down. What do you do? You feel the slope of the ground beneath your feet and take a small step in the downhill direction. Then you feel the slope again and take another step. Repeat thousands of times, and you reach the valley.\n",
    "\n",
    "This is gradient descent. The \"mountain\" is the loss landscape -- a surface where the height represents how wrong the model is. The \"slope\" is the gradient -- the direction of steepest ascent. By moving in the opposite direction (downhill), we reduce the loss.\n",
    "\n",
    "### Why Negative Log?\n",
    "\n",
    "The cross-entropy loss uses the negative log of the predicted probability. Why not just use the probability directly? Consider two scenarios:\n",
    "- The model assigns 90% to the correct answer. Probability-based loss: 0.10. Log-based loss: 0.105.\n",
    "- The model assigns 1% to the correct answer. Probability-based loss: 0.99. Log-based loss: 4.605.\n",
    "\n",
    "The log-based loss penalizes confident wrong answers *much* more severely. Assigning 1% to the correct answer is catastrophic -- the loss is 46x higher than the 90% case. This strong penalty creates a powerful incentive for the model to avoid being confidently wrong, which is exactly the behavior we want.\n",
    "\n",
    "### The Chain Rule -- One Sentence\n",
    "\n",
    "Backpropagation is the chain rule of calculus, applied systematically from the loss backwards through every layer of the network. If the loss depends on A, which depends on B, which depends on C, then the gradient of the loss with respect to C is: (gradient of loss w.r.t. A) times (gradient of A w.r.t. B) times (gradient of B w.r.t. C). That is it. The rest is bookkeeping.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "When you make a prediction and get it wrong, not every part of your reasoning contributed equally to the error. Some parts were on the right track; others led you astray. Backpropagation solves exactly this attribution problem -- it tells each parameter in the network precisely how much it contributed to the error, and in which direction it should change."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "For a single prediction, the cross-entropy loss is:\n",
    "\n",
    "$$\\mathcal{L} = -\\log P(\\text{correct token})$$\n",
    "\n",
    "Computationally: look up the probability the model assigned to the correct answer, take the natural log, and negate it. If the model assigned probability 0.7, the loss is $-\\log(0.7) = 0.357$. If it assigned 0.01, the loss is $-\\log(0.01) = 4.605$.\n",
    "\n",
    "For a full sequence of $N$ tokens:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{total}} = -\\frac{1}{N}\\sum_{i=1}^{N} \\log P_\\theta(t_i \\mid t_1, \\ldots, t_{i-1})$$\n",
    "\n",
    "This says: for each position, measure how well the model predicted the actual next token, and average across all positions.\n",
    "\n",
    "### The Gradient of Cross-Entropy\n",
    "\n",
    "The gradient of cross-entropy loss with respect to the logits has a beautifully simple form:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\text{logit}_i} = p_i - \\mathbb{1}_{i=c}$$\n",
    "\n",
    "where $p_i$ is the softmax probability for token $i$, and $c$ is the index of the correct token. Computationally: the gradient for every token is just its predicted probability. For the correct token, subtract 1. This means: push the correct token's logit higher (its gradient is negative), push all others lower (their gradients are positive).\n",
    "\n",
    "### The Chain Rule\n",
    "\n",
    "For a composite function $y = f(g(x))$:\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{dy}{dg} \\cdot \\frac{dg}{dx}$$\n",
    "\n",
    "In a neural network with many layers, this chain extends through every layer from the loss back to the first parameter. The beauty is that each layer only needs to know its own local gradient and the gradient coming from the layer above.\n",
    "\n",
    "### Gradient Descent Update\n",
    "\n",
    "$$\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\theta}$$\n",
    "\n",
    "This says: adjust each parameter by a small step (controlled by learning rate $\\eta$) in the direction that decreases the loss. If the gradient is positive, the parameter is contributing to increasing the loss, so we decrease it. If negative, we increase it."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 Cross-Entropy Loss from Scratch\n",
    "\n",
    "Let us implement cross-entropy loss manually, step by step, to see exactly what PyTorch's `F.cross_entropy` does under the hood."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cross_entropy_from_scratch(logits, targets):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss manually.\n",
    "\n",
    "    Args:\n",
    "        logits: (N, C) raw model outputs (before softmax)\n",
    "        targets: (N,) integer class labels\n",
    "\n",
    "    Returns:\n",
    "        scalar loss value\n",
    "    \"\"\"\n",
    "    # Step 1: Softmax to get probabilities\n",
    "    # Subtract max for numerical stability (log-sum-exp trick)\n",
    "    logits_shifted = logits - logits.max(dim=-1, keepdim=True).values\n",
    "    exp_logits = torch.exp(logits_shifted)\n",
    "    probs = exp_logits / exp_logits.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    # Step 2: Extract probability of correct class for each sample\n",
    "    N = logits.shape[0]\n",
    "    correct_probs = probs[torch.arange(N), targets]\n",
    "\n",
    "    # Step 3: Negative log\n",
    "    loss = -torch.log(correct_probs + 1e-10)  # epsilon for numerical safety\n",
    "\n",
    "    # Step 4: Average over all samples\n",
    "    return loss.mean()\n",
    "\n",
    "# Test: compare with PyTorch\n",
    "logits = torch.randn(5, 10)  # 5 predictions, 10 classes\n",
    "targets = torch.randint(0, 10, (5,))\n",
    "\n",
    "our_loss = cross_entropy_from_scratch(logits, targets)\n",
    "pytorch_loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "print(f\"Our implementation:    {our_loss.item():.6f}\")\n",
    "print(f\"PyTorch F.cross_entropy: {pytorch_loss.item():.6f}\")\n",
    "print(f\"Difference: {abs(our_loss.item() - pytorch_loss.item()):.8f}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the negative log function\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: -log(p) curve\n",
    "p = np.linspace(0.01, 1.0, 200)\n",
    "loss = -np.log(p)\n",
    "\n",
    "axes[0].plot(p, loss, 'b-', linewidth=2)\n",
    "axes[0].axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "\n",
    "# Mark key points\n",
    "points = [(0.01, -np.log(0.01), 'Terrible\\nprediction'),\n",
    "          (0.5, -np.log(0.5), 'Coin flip'),\n",
    "          (1.0, -np.log(1.0), 'Perfect')]\n",
    "for px, py, label in points:\n",
    "    axes[0].plot(px, py, 'ro', markersize=8)\n",
    "    axes[0].annotate(label, (px, py), textcoords=\"offset points\",\n",
    "                     xytext=(10, 10), fontsize=10)\n",
    "\n",
    "axes[0].set_xlabel('P(correct token)', fontsize=12)\n",
    "axes[0].set_ylabel('Loss = -log(P)', fontsize=12)\n",
    "axes[0].set_title('Cross-Entropy Loss: Why -log?', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Full example with a 4-token sequence\n",
    "token_names = ['The', 'cat', 'sat', 'on']\n",
    "pred_probs = [0.3, 0.7, 0.1]  # P(next correct token)\n",
    "token_losses = [-np.log(p) for p in pred_probs]\n",
    "avg_loss = np.mean(token_losses)\n",
    "\n",
    "x_pos = range(len(pred_probs))\n",
    "bars = axes[1].bar(x_pos, token_losses, color=['#ff7f0e', '#2ca02c', '#d62728'],\n",
    "                   alpha=0.8, edgecolor='black')\n",
    "axes[1].axhline(y=avg_loss, color='blue', linestyle='--',\n",
    "                label=f'Average loss = {avg_loss:.3f}')\n",
    "\n",
    "for i, (prob, loss_val) in enumerate(zip(pred_probs, token_losses)):\n",
    "    axes[1].text(i, loss_val + 0.05, f'P={prob}\\nL={loss_val:.3f}',\n",
    "                ha='center', fontsize=10)\n",
    "\n",
    "tick_labels = [f'After \"{token_names[i]}\"\\npredict \"{token_names[i+1]}\"'\n",
    "               for i in range(len(pred_probs))]\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(tick_labels, fontsize=9)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].set_title('Loss Across a Sequence: \"The cat sat on\"', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Gradient of Cross-Entropy (Manual)"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_gradient(logits, targets):\n",
    "    \"\"\"\n",
    "    Compute the gradient of cross-entropy loss w.r.t. logits -- manually.\n",
    "\n",
    "    The gradient has a beautiful form:\n",
    "        dL/d(logit_i) = softmax(logit_i) - 1{i == target}\n",
    "\n",
    "    Args:\n",
    "        logits: (N, C) raw model outputs\n",
    "        targets: (N,) integer class labels\n",
    "\n",
    "    Returns:\n",
    "        grad: (N, C) gradient matrix\n",
    "    \"\"\"\n",
    "    # Softmax\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # Start with softmax probabilities\n",
    "    grad = probs.clone()\n",
    "\n",
    "    # Subtract 1 at the correct class position\n",
    "    N = logits.shape[0]\n",
    "    grad[torch.arange(N), targets] -= 1.0\n",
    "\n",
    "    # Average over batch\n",
    "    grad = grad / N\n",
    "\n",
    "    return grad\n",
    "\n",
    "# Compare with PyTorch autograd\n",
    "logits = torch.randn(3, 5, requires_grad=True)\n",
    "targets = torch.tensor([2, 0, 4])\n",
    "\n",
    "# PyTorch gradient\n",
    "loss = F.cross_entropy(logits, targets)\n",
    "loss.backward()\n",
    "pytorch_grad = logits.grad.clone()\n",
    "\n",
    "# Our gradient\n",
    "our_grad = cross_entropy_gradient(logits.detach(), targets)\n",
    "\n",
    "print(\"PyTorch autograd gradient:\")\n",
    "print(pytorch_grad.numpy().round(4))\n",
    "print(\"\\nOur manual gradient:\")\n",
    "print(our_grad.numpy().round(4))\n",
    "print(f\"\\nMax difference: {(pytorch_grad - our_grad).abs().max().item():.8f}\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what the gradient means\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "logits_example = torch.tensor([[2.0, 1.0, 0.5, -1.0, 0.0]])\n",
    "target = torch.tensor([0])  # Correct answer is token 0\n",
    "\n",
    "probs = F.softmax(logits_example, dim=-1)[0]\n",
    "grad = cross_entropy_gradient(logits_example, target)[0]\n",
    "\n",
    "# Plot 1: Logits\n",
    "axes[0].bar(range(5), logits_example[0].numpy(), color='steelblue', edgecolor='black')\n",
    "axes[0].set_title('Logits (Raw Scores)', fontsize=13)\n",
    "axes[0].set_xlabel('Token ID')\n",
    "axes[0].set_ylabel('Logit Value')\n",
    "\n",
    "# Plot 2: Probabilities (after softmax)\n",
    "colors = ['green' if i == 0 else 'salmon' for i in range(5)]\n",
    "axes[1].bar(range(5), probs.numpy(), color=colors, edgecolor='black')\n",
    "axes[1].set_title('Probabilities (After Softmax)', fontsize=13)\n",
    "axes[1].set_xlabel('Token ID')\n",
    "axes[1].set_ylabel('Probability')\n",
    "axes[1].annotate('Correct\\ntoken', xy=(0, probs[0].item()), fontsize=10,\n",
    "                 xytext=(0.5, probs[0].item() + 0.05))\n",
    "\n",
    "# Plot 3: Gradients\n",
    "grad_colors = ['green' if g < 0 else 'red' for g in grad.numpy()]\n",
    "axes[2].bar(range(5), grad.numpy(), color=grad_colors, edgecolor='black')\n",
    "axes[2].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[2].set_title('Gradients (Direction of Update)', fontsize=13)\n",
    "axes[2].set_xlabel('Token ID')\n",
    "axes[2].set_ylabel('dL/d(logit)')\n",
    "axes[2].annotate('Push UP\\n(increase prob)', xy=(0, grad[0].item()),\n",
    "                 xytext=(0.8, grad[0].item() - 0.08), fontsize=9,\n",
    "                 arrowprops=dict(arrowstyle='->', color='green'))\n",
    "\n",
    "plt.suptitle('Cross-Entropy Gradient: \"Push correct up, push others down\"',\n",
    "             fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The Chain Rule in Action\n",
    "\n",
    "Let us trace backpropagation through a tiny network to see exactly how the chain rule works."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A minimal 2-layer network to trace gradients manually\n",
    "class TinyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Parameter(torch.tensor([[0.5, -0.3],\n",
    "                                              [0.2, 0.8]]))\n",
    "        self.w2 = nn.Parameter(torch.tensor([[0.4],\n",
    "                                              [-0.6]]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.relu(x @ self.w1)       # Hidden layer\n",
    "        out = h @ self.w2                   # Output\n",
    "        return out\n",
    "\n",
    "net = TinyNet()\n",
    "x = torch.tensor([[1.0, 2.0]])\n",
    "target = torch.tensor([[1.0]])\n",
    "\n",
    "# Forward pass (manual trace)\n",
    "h_pre = x @ net.w1                          # Before ReLU\n",
    "h = torch.relu(h_pre)                       # After ReLU\n",
    "out = h @ net.w2                            # Final output\n",
    "loss = (out - target) ** 2                  # MSE loss\n",
    "\n",
    "print(\"=== FORWARD PASS ===\")\n",
    "print(f\"Input x:         {x.numpy()}\")\n",
    "print(f\"W1:\\n{net.w1.detach().numpy()}\")\n",
    "print(f\"h (before ReLU): {h_pre.detach().numpy()}\")\n",
    "print(f\"h (after ReLU):  {h.detach().numpy()}\")\n",
    "print(f\"W2:\\n{net.w2.detach().numpy()}\")\n",
    "print(f\"Output:          {out.item():.4f}\")\n",
    "print(f\"Target:          {target.item():.4f}\")\n",
    "print(f\"Loss (MSE):      {loss.item():.4f}\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now trace the backward pass manually using the chain rule\n",
    "print(\"\\n=== BACKWARD PASS (Chain Rule) ===\")\n",
    "\n",
    "# dL/d(out) = 2 * (out - target)\n",
    "dL_dout = 2 * (out - target)\n",
    "print(f\"dL/d(out) = 2*(out - target) = {dL_dout.item():.4f}\")\n",
    "\n",
    "# dL/d(W2) = h^T @ dL_dout  (chain rule through matmul)\n",
    "dL_dW2 = h.t() @ dL_dout\n",
    "print(f\"dL/d(W2) = h^T @ dL_dout = {dL_dW2.detach().numpy().round(4)}\")\n",
    "\n",
    "# dL/d(h) = dL_dout @ W2^T\n",
    "dL_dh = dL_dout @ net.w2.t()\n",
    "print(f\"dL/d(h)  = dL_dout @ W2^T = {dL_dh.detach().numpy().round(4)}\")\n",
    "\n",
    "# dL/d(h_pre) = dL/d(h) * ReLU_derivative\n",
    "# ReLU derivative: 1 if h_pre > 0, else 0\n",
    "relu_mask = (h_pre > 0).float()\n",
    "dL_dh_pre = dL_dh * relu_mask\n",
    "print(f\"ReLU mask: {relu_mask.detach().numpy()}\")\n",
    "print(f\"dL/d(h_pre) = {dL_dh_pre.detach().numpy().round(4)}\")\n",
    "\n",
    "# dL/d(W1) = x^T @ dL_dh_pre\n",
    "dL_dW1 = x.t() @ dL_dh_pre\n",
    "print(f\"dL/d(W1) = x^T @ dL_dh_pre =\\n{dL_dW1.detach().numpy().round(4)}\")\n",
    "\n",
    "# Compare with PyTorch autograd\n",
    "loss_auto = ((net(x) - target) ** 2).sum()\n",
    "loss_auto.backward()\n",
    "print(f\"\\n=== PyTorch Autograd (should match) ===\")\n",
    "print(f\"dL/d(W1) =\\n{net.w1.grad.numpy().round(4)}\")\n",
    "print(f\"dL/d(W2) =\\n{net.w2.grad.numpy().round(4)}\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Visualizing Gradient Flow Through Residual Connections"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate why residual connections help gradient flow\n",
    "\n",
    "def simulate_gradient_flow(n_layers, use_residual=True):\n",
    "    \"\"\"Simulate gradient magnitude through N layers.\"\"\"\n",
    "    gradient = 1.0\n",
    "    gradients = [gradient]\n",
    "\n",
    "    for _ in range(n_layers):\n",
    "        # Simulate a layer transformation (random scaling)\n",
    "        layer_grad = np.random.uniform(0.3, 0.9)\n",
    "\n",
    "        if use_residual:\n",
    "            # Residual: gradient = 1 + layer_grad (always >= 1)\n",
    "            gradient = gradient * (1 + layer_grad)\n",
    "        else:\n",
    "            # No residual: gradient = layer_grad (can shrink to 0)\n",
    "            gradient = gradient * layer_grad\n",
    "\n",
    "        gradients.append(gradient)\n",
    "\n",
    "    return gradients\n",
    "\n",
    "# Run many simulations\n",
    "np.random.seed(42)\n",
    "n_sims = 100\n",
    "n_layers = 12\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Without residual connections\n",
    "for _ in range(n_sims):\n",
    "    grads = simulate_gradient_flow(n_layers, use_residual=False)\n",
    "    axes[0].plot(grads, alpha=0.1, color='red')\n",
    "\n",
    "avg_no_res = np.mean([simulate_gradient_flow(n_layers, False) for _ in range(1000)], axis=0)\n",
    "axes[0].plot(avg_no_res, 'r-', linewidth=3, label='Average')\n",
    "axes[0].set_xlabel('Layer (from output to input)')\n",
    "axes[0].set_ylabel('Gradient Magnitude')\n",
    "axes[0].set_title('WITHOUT Residual Connections\\n(Vanishing Gradient Problem)')\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# With residual connections\n",
    "for _ in range(n_sims):\n",
    "    grads = simulate_gradient_flow(n_layers, use_residual=True)\n",
    "    axes[1].plot(grads, alpha=0.1, color='green')\n",
    "\n",
    "avg_res = np.mean([simulate_gradient_flow(n_layers, True) for _ in range(1000)], axis=0)\n",
    "axes[1].plot(avg_res, 'g-', linewidth=3, label='Average')\n",
    "axes[1].set_xlabel('Layer (from output to input)')\n",
    "axes[1].set_ylabel('Gradient Magnitude')\n",
    "axes[1].set_title('WITH Residual Connections\\n(Gradient Highway)')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Gradient Flow Through 12 Transformer Layers', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Without residual connections, gradients vanish exponentially.\")\n",
    "print(\"With residual connections, gradients are preserved -- training works!\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Implement the Full Training Step"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, x, y, optimizer, vocab_size=256):\n",
    "    \"\"\"\n",
    "    Perform one complete training step: forward pass, loss, backward, update.\n",
    "\n",
    "    Args:\n",
    "        model: GPT model instance\n",
    "        x: input token IDs, shape (batch_size, seq_len)\n",
    "        y: target token IDs, shape (batch_size, seq_len)\n",
    "        optimizer: torch optimizer\n",
    "        vocab_size: vocabulary size for cross-entropy\n",
    "\n",
    "    Returns:\n",
    "        loss_value: the scalar loss for this step\n",
    "\n",
    "    Steps:\n",
    "        1. Forward pass: get logits from the model\n",
    "        2. Reshape logits to (batch_size * seq_len, vocab_size)\n",
    "        3. Reshape targets to (batch_size * seq_len,)\n",
    "        4. Compute cross-entropy loss\n",
    "        5. Zero the gradients (optimizer.zero_grad())\n",
    "        6. Backward pass (loss.backward())\n",
    "        7. Update weights (optimizer.step())\n",
    "        8. Return the loss value as a float\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Implement all 8 steps above.\n",
    "    # The reshape is needed because PyTorch's cross_entropy\n",
    "    # expects (N, C) logits and (N,) targets.\n",
    "    # ==============================\n",
    "    loss_value = None  # YOUR CODE HERE\n",
    "    return loss_value"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification -- build a small model and run one training step\n",
    "import torch.nn.functional as F_ver\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size=256, d_model=32, n_heads=2, n_layers=2, max_seq_len=64):\n",
    "        super().__init__()\n",
    "        self.tok = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos = nn.Embedding(max_seq_len, d_model)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        x = self.tok(idx) + self.pos(torch.arange(T, device=idx.device))\n",
    "        x = self.ln(x)\n",
    "        return self.head(x)\n",
    "\n",
    "mini = MiniGPT()\n",
    "opt = torch.optim.Adam(mini.parameters(), lr=1e-3)\n",
    "x_test = torch.randint(0, 256, (4, 16))\n",
    "y_test = torch.randint(0, 256, (4, 16))\n",
    "\n",
    "# Get loss before training\n",
    "with torch.no_grad():\n",
    "    logits_before = mini(x_test)\n",
    "    loss_before = F_ver.cross_entropy(logits_before.view(-1, 256), y_test.view(-1))\n",
    "\n",
    "# Run one training step\n",
    "loss_val = training_step(mini, x_test, y_test, opt)\n",
    "assert loss_val is not None, \"training_step returned None\"\n",
    "assert isinstance(loss_val, float), \"training_step should return a float\"\n",
    "\n",
    "# Loss should have decreased (or at least the gradient should be non-zero)\n",
    "print(f\"Loss before: {loss_before.item():.4f}\")\n",
    "print(f\"Loss after 1 step: {loss_val:.4f}\")\n",
    "print(\"Training step works correctly!\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Gradient Magnitude Analysis"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gradients(model, x, y, vocab_size=256):\n",
    "    \"\"\"\n",
    "    Analyze gradient magnitudes across different parts of the model.\n",
    "\n",
    "    Run a forward+backward pass and report the mean, max, and std\n",
    "    of gradient magnitudes for each named parameter group:\n",
    "    - Embedding layers (token_emb, pos_emb)\n",
    "    - Attention weights (qkv, proj)\n",
    "    - FFN weights\n",
    "    - Output head\n",
    "    - Layer norm parameters\n",
    "\n",
    "    This reveals which parts of the model are learning fastest/slowest.\n",
    "\n",
    "    Args:\n",
    "        model: GPT model instance\n",
    "        x: input token IDs\n",
    "        y: target token IDs\n",
    "        vocab_size: vocabulary size\n",
    "\n",
    "    Returns:\n",
    "        dict mapping parameter_group_name -> {'mean': float, 'max': float, 'std': float}\n",
    "\n",
    "    Steps:\n",
    "        1. Forward pass\n",
    "        2. Compute loss\n",
    "        3. Backward pass\n",
    "        4. Iterate over model.named_parameters()\n",
    "        5. Group parameters by name pattern (contains 'emb', 'qkv', etc.)\n",
    "        6. For each group, compute mean/max/std of gradient magnitudes\n",
    "        7. Print a formatted table\n",
    "        8. Create a bar chart of mean gradient magnitudes by group\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Implement the gradient analysis.\n",
    "    # Hint: Use param.grad.abs() to get gradient magnitudes.\n",
    "    # Remember to call optimizer.zero_grad() first to clear old gradients.\n",
    "    # ==============================\n",
    "    pass"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete training pipeline with loss tracking and analysis\n",
    "\n",
    "class CharTokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 256\n",
    "    def encode(self, text):\n",
    "        return [ord(ch) for ch in text]\n",
    "    def decode(self, ids):\n",
    "        return ''.join(chr(i) for i in ids)\n",
    "\n",
    "class SimpleGPT(nn.Module):\n",
    "    \"\"\"Minimal GPT for training demonstration.\"\"\"\n",
    "    def __init__(self, vocab_size=256, d_model=64, n_heads=4, n_layers=4, max_seq_len=128):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, d_model)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model, n_heads, dim_feedforward=4*d_model,\n",
    "                                       dropout=0.0, batch_first=True, norm_first=True)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        x = self.token_emb(idx) + self.pos_emb(torch.arange(T, device=idx.device))\n",
    "        mask = torch.triu(torch.ones(T, T, device=idx.device), diagonal=1).bool()\n",
    "        for block in self.blocks:\n",
    "            x = block(x, src_mask=mask, is_causal=True)\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)\n",
    "\n",
    "tokenizer = CharTokenizer()\n",
    "\n",
    "# Training data\n",
    "text = open('/dev/stdin', 'r').read() if False else \"\"\"\n",
    "All that glitters is not gold Often have you heard that told\n",
    "Many a man his life hath sold But my outside to behold\n",
    "Gilded tombs do worms enfold Had you been as wise as bold\n",
    "Young in limbs in judgment old Your answer had not been inscrolled\n",
    "\"\"\" * 50\n",
    "\n",
    "data = torch.tensor(tokenizer.encode(text))\n",
    "print(f\"Training data: {len(data)} characters\")\n",
    "print(f\"Unique characters: {len(set(data.tolist()))}\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with detailed tracking\n",
    "model = SimpleGPT()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "seq_len = 64\n",
    "batch_size = 16\n",
    "n_steps = 1500\n",
    "\n",
    "losses = []\n",
    "learning_rates = []\n",
    "grad_norms = []\n",
    "\n",
    "for step in range(n_steps):\n",
    "    # Sample batch\n",
    "    idx = torch.randint(0, len(data) - seq_len - 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in idx])\n",
    "\n",
    "    # Forward\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits.view(-1, 256), y.view(-1))\n",
    "\n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Track gradient norm\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            total_norm += p.grad.norm().item() ** 2\n",
    "    grad_norms.append(total_norm ** 0.5)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    if step % 300 == 0:\n",
    "        print(f\"Step {step:4d}: loss = {loss.item():.4f}, grad_norm = {grad_norms[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal loss: {losses[-1]:.4f}\")\n",
    "print(f\"Random baseline: {np.log(256):.4f}\")\n",
    "print(f\"Improvement: {np.log(256) - losses[-1]:.4f}\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive training analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss curve\n",
    "axes[0,0].plot(losses, alpha=0.3, color='blue')\n",
    "window = 50\n",
    "smoothed = [np.mean(losses[max(0,i-window):i+1]) for i in range(len(losses))]\n",
    "axes[0,0].plot(smoothed, 'b-', linewidth=2, label='Smoothed')\n",
    "axes[0,0].axhline(y=np.log(256), color='red', linestyle='--', label=f'Random: {np.log(256):.2f}')\n",
    "axes[0,0].set_xlabel('Step')\n",
    "axes[0,0].set_ylabel('Loss')\n",
    "axes[0,0].set_title('Training Loss')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient norms over time\n",
    "axes[0,1].plot(grad_norms, alpha=0.3, color='orange')\n",
    "smoothed_gn = [np.mean(grad_norms[max(0,i-window):i+1]) for i in range(len(grad_norms))]\n",
    "axes[0,1].plot(smoothed_gn, color='orange', linewidth=2)\n",
    "axes[0,1].set_xlabel('Step')\n",
    "axes[0,1].set_ylabel('Gradient L2 Norm')\n",
    "axes[0,1].set_title('Gradient Norms Over Training')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss distribution: early vs late\n",
    "axes[1,0].hist(losses[:100], bins=30, alpha=0.5, label='First 100 steps', color='red')\n",
    "axes[1,0].hist(losses[-100:], bins=30, alpha=0.5, label='Last 100 steps', color='green')\n",
    "axes[1,0].set_xlabel('Loss')\n",
    "axes[1,0].set_ylabel('Count')\n",
    "axes[1,0].set_title('Loss Distribution: Early vs Late Training')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# Per-parameter gradient analysis (final step)\n",
    "param_grads = {}\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        category = name.split('.')[0]\n",
    "        if category not in param_grads:\n",
    "            param_grads[category] = []\n",
    "        param_grads[category].append(param.grad.abs().mean().item())\n",
    "\n",
    "categories = list(param_grads.keys())\n",
    "means = [np.mean(v) for v in param_grads.values()]\n",
    "axes[1,1].barh(categories, means, color='steelblue', edgecolor='black')\n",
    "axes[1,1].set_xlabel('Mean |Gradient|')\n",
    "axes[1,1].set_title('Gradient Magnitudes by Component (Final Step)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text and show the learning in action\n",
    "def generate(model, tokenizer, prompt, max_new=100, temperature=0.8):\n",
    "    model.eval()\n",
    "    ids = torch.tensor([tokenizer.encode(prompt)])\n",
    "    for _ in range(max_new):\n",
    "        if ids.shape[1] > 128:\n",
    "            ids = ids[:, -128:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(ids)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, 1)\n",
    "        ids = torch.cat([ids, next_id], dim=1)\n",
    "    return tokenizer.decode(ids[0].tolist())\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  GPT TRAINED WITH CROSS-ENTROPY + BACKPROPAGATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in [\"All that \", \"Young in \", \"Gilded \"]:\n",
    "    text = generate(model, tokenizer, prompt, max_new=80)\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(f\"Output: {text}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Starting loss:  {losses[0]:.4f} (random)\")\n",
    "print(f\"  Final loss:     {losses[-1]:.4f}\")\n",
    "print(f\"  Improvement:    {((losses[0] - losses[-1]) / losses[0] * 100):.1f}%\")\n",
    "print(f\"  Total steps:    {n_steps}\")\n",
    "print(f\"  Parameters:     {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "print(\"\\nYou have trained a GPT model using cross-entropy loss\")\n",
    "print(\"and backpropagation -- the exact same algorithm used to train GPT-4!\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "1. Cross-entropy loss treats all wrong answers equally -- predicting \"cat\" when the answer is \"car\" gets the same penalty as predicting \"cat\" when the answer is \"refrigerator.\" Is this a problem? How might you address it?\n",
    "2. We used a fixed learning rate. What would happen if the learning rate is too large? Too small? How does AdamW's adaptive learning rate help?\n",
    "3. The gradient of softmax cross-entropy has the elegant form $p_i - \\mathbb{1}_{i=c}$. Why is this gradient zero-sum (sums to zero across all classes)? What does this mean for the weight updates?\n",
    "\n",
    "### Optional Challenges\n",
    "1. Implement gradient clipping (cap the gradient norm to a maximum value). Train with and without clipping -- does it affect stability for your small model?\n",
    "2. Implement a learning rate warmup schedule: start with a tiny learning rate and linearly increase it over the first 100 steps. Does this improve early training?\n",
    "3. Track the cross-entropy loss separately for each character position in the sequence. Does the model learn to predict some positions better than others? Which ones, and why?"
   ],
   "id": "cell_28"
  }
 ]
}