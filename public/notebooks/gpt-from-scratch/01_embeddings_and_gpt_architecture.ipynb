{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "01_embeddings_and_gpt_architecture \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings and the GPT Architecture: From Text to Vectors\n",
    "\n",
    "*Part 1 of the Vizuara series on Building a GPT-Style Model from Scratch*\n",
    "*Estimated time: 45 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Every GPT model -- from GPT-2 to GPT-4 -- starts the same way: raw text goes in, and the first thing the model must do is convert that text into numbers. Neural networks cannot read words. They only understand vectors of floating-point numbers. So the very first question in building a language model is: **how do we represent language as numbers?**\n",
    "\n",
    "This is not a minor implementation detail. The quality of these representations determines everything downstream. If the model cannot distinguish \"bank\" (financial) from \"bank\" (river), or if it cannot tell the difference between \"dog bites man\" and \"man bites dog,\" then no amount of clever architecture will save it.\n",
    "\n",
    "In this notebook, we will build the input pipeline of a GPT model from scratch. By the end, you will have a working system that:\n",
    "- Converts raw text into token IDs\n",
    "- Maps those IDs to learned embedding vectors\n",
    "- Adds positional information so the model knows word order\n",
    "- Understands why GPT uses a decoder-only architecture\n",
    "\n",
    "Let us begin."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### Why Can't We Just Use One-Hot Vectors?\n",
    "\n",
    "The simplest way to represent a word as a number is a one-hot vector. If our vocabulary has 10,000 words, we create a vector of 10,000 zeros and put a 1 at the position corresponding to our word. \"Cat\" might be [0, 0, ..., 1, ..., 0] with the 1 at position 891.\n",
    "\n",
    "But this has two fatal problems:\n",
    "\n",
    "1. **No notion of similarity.** The one-hot vectors for \"cat\" and \"kitten\" are just as different as \"cat\" and \"refrigerator.\" Every pair of words is equally distant. This means the model has to learn from scratch that similar words are related.\n",
    "\n",
    "2. **Enormous dimensionality.** If our vocabulary has 50,000 tokens, every word is a 50,000-dimensional vector. Most of those dimensions are wasted zeros.\n",
    "\n",
    "The solution is **dense embeddings** -- compact vectors (typically 64 to 12,288 dimensions) where similar words naturally end up close together. The model *learns* these vectors during training.\n",
    "\n",
    "### Why Does Position Matter?\n",
    "\n",
    "Consider these two sentences:\n",
    "- \"The dog chased the cat.\"\n",
    "- \"The cat chased the dog.\"\n",
    "\n",
    "Same words, completely different meanings. If we simply looked up embedding vectors for each word without caring about order, these two sentences would be indistinguishable. The model needs to know not just *what* each word is, but *where* it appears. This is exactly what positional embeddings provide.\n",
    "\n",
    "### Why Decoder-Only?\n",
    "\n",
    "The original Transformer had an encoder and a decoder. GPT throws away the encoder and keeps only the decoder. Why?\n",
    "\n",
    "Think about it this way: if you are writing a story, you do not need to first read and encode a separate input in another language. You simply need to look at what you have written so far and decide what word comes next. That is exactly what a decoder-only model does. It reads left to right, one token at a time, and each token can only look at the tokens that came before it -- never at future tokens. This is called **causal masking**, and it is the defining feature of GPT's architecture.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "If you were designing a system to predict the next word, what information would you need at each step? Think about what you do when you fill in the blank: \"The cat sat on the ___.\" You look at all the previous words, understand their meaning and their order, and make a prediction. That is exactly the problem GPT solves."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Token Embeddings\n",
    "\n",
    "Given a vocabulary of size $V$ and an embedding dimension $d_{\\text{model}}$, the token embedding table is a matrix:\n",
    "\n",
    "$$E_{\\text{token}} \\in \\mathbb{R}^{V \\times d_{\\text{model}}}$$\n",
    "\n",
    "For a token with ID $i$, the embedding is simply row $i$ of this matrix:\n",
    "\n",
    "$$\\mathbf{e}_{\\text{token}} = E_{\\text{token}}[i] \\in \\mathbb{R}^{d_{\\text{model}}}$$\n",
    "\n",
    "Computationally, this is just a table lookup -- no matrix multiplication needed. We index into a table with $V$ rows and $d_{\\text{model}}$ columns, and pull out one row. If $V = 10{,}000$ and $d_{\\text{model}} = 64$, this table has 640,000 learnable parameters.\n",
    "\n",
    "### Positional Embeddings\n",
    "\n",
    "GPT uses learned positional embeddings. The positional embedding table is:\n",
    "\n",
    "$$E_{\\text{pos}} \\in \\mathbb{R}^{T_{\\max} \\times d_{\\text{model}}}$$\n",
    "\n",
    "where $T_{\\max}$ is the maximum sequence length. For a token at position $j$:\n",
    "\n",
    "$$\\mathbf{e}_{\\text{pos}} = E_{\\text{pos}}[j] \\in \\mathbb{R}^{d_{\\text{model}}}$$\n",
    "\n",
    "### Combined Input\n",
    "\n",
    "The final input vector for the token at position $i$ is the element-wise sum:\n",
    "\n",
    "$$\\mathbf{x}_i = \\mathbf{e}_{\\text{token}}(i) + \\mathbf{e}_{\\text{pos}}(i)$$\n",
    "\n",
    "Computationally, this means: look up the token embedding (what word this is), look up the positional embedding (where this word appears), and add them together element-wise. The result is a single vector that encodes both identity and position.\n",
    "\n",
    "For example, if the token embedding is $[0.5, 0.3, -0.1, 0.7]$ and the positional embedding is $[0.1, -0.2, 0.4, 0.0]$, the combined vector is:\n",
    "\n",
    "$$[0.5 + 0.1,\\; 0.3 + (-0.2),\\; -0.1 + 0.4,\\; 0.7 + 0.0] = [0.6,\\; 0.1,\\; 0.3,\\; 0.7]$$"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It -- Component by Component\n",
    "\n",
    "### 4.1 Character-Level Tokenizer\n",
    "\n",
    "Before we can embed tokens, we need a tokenizer. Real GPT models use BPE (Byte Pair Encoding), but for our from-scratch implementation, we will use character-level tokenization. Every ASCII character becomes a token."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Character-level tokenizer\n",
    "class CharTokenizer:\n",
    "    \"\"\"A simple character-level tokenizer.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Use all printable ASCII characters\n",
    "        self.chars = [chr(i) for i in range(256)]\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_id = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.id_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert text to a list of token IDs.\"\"\"\n",
    "        return [self.char_to_id.get(ch, 0) for ch in text]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"Convert a list of token IDs back to text.\"\"\"\n",
    "        return ''.join(self.id_to_char.get(i, '?') for i in ids)\n",
    "\n",
    "# Test it\n",
    "tokenizer = CharTokenizer()\n",
    "text = \"The cat sat on the mat.\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(f\"Text:      '{text}'\")\n",
    "print(f\"Token IDs: {ids}\")\n",
    "print(f\"Decoded:   '{tokenizer.decode(ids)}'\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the simplest possible tokenizer. Each character maps to its ASCII code. Real GPT models use subword tokenization (BPE), which is more efficient, but character-level works perfectly for learning the concepts.\n",
    "\n",
    "### 4.2 Token Embedding Table\n",
    "\n",
    "Now let us build the token embedding layer. This is simply a lookup table implemented as `nn.Embedding`."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token Embedding\n",
    "VOCAB_SIZE = 256    # ASCII characters\n",
    "D_MODEL = 64        # Embedding dimension\n",
    "\n",
    "token_embedding = nn.Embedding(VOCAB_SIZE, D_MODEL)\n",
    "\n",
    "# Look up embeddings for our tokens\n",
    "token_ids = torch.tensor(tokenizer.encode(\"cat\"))\n",
    "embeddings = token_embedding(token_ids)\n",
    "\n",
    "print(f\"Input token IDs: {token_ids}\")\n",
    "print(f\"Token IDs shape: {token_ids.shape}\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"\\nEmbedding for 'c' (first 10 dims): {embeddings[0, :10].detach().numpy().round(3)}\")\n",
    "print(f\"Embedding for 'a' (first 10 dims): {embeddings[1, :10].detach().numpy().round(3)}\")\n",
    "print(f\"Embedding for 't' (first 10 dims): {embeddings[2, :10].detach().numpy().round(3)}\")\n",
    "print(f\"\\nTotal embedding parameters: {VOCAB_SIZE * D_MODEL:,}\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each character gets its own 64-dimensional vector. These vectors are initialized randomly, and the model will learn to adjust them during training so that similar characters end up with similar vectors."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the raw embeddings\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Embedding vectors for a few characters\n",
    "chars_to_show = ['a', 'b', 'c', 'x', 'y', 'z', ' ', '.', '0', '1']\n",
    "char_ids = torch.tensor([tokenizer.char_to_id[c] for c in chars_to_show])\n",
    "char_embeds = token_embedding(char_ids).detach().numpy()\n",
    "\n",
    "im = axes[0].imshow(char_embeds, aspect='auto', cmap='RdBu_r')\n",
    "axes[0].set_yticks(range(len(chars_to_show)))\n",
    "axes[0].set_yticklabels([repr(c) for c in chars_to_show])\n",
    "axes[0].set_xlabel('Embedding Dimension')\n",
    "axes[0].set_title('Token Embedding Vectors (before training)')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Plot 2: Cosine similarity between characters\n",
    "from torch.nn.functional import cosine_similarity\n",
    "all_embeds = token_embedding(char_ids)\n",
    "n = len(chars_to_show)\n",
    "sim_matrix = torch.zeros(n, n)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        sim_matrix[i, j] = cosine_similarity(\n",
    "            all_embeds[i].unsqueeze(0), all_embeds[j].unsqueeze(0)\n",
    "        )\n",
    "\n",
    "im2 = axes[1].imshow(sim_matrix.detach().numpy(), cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "axes[1].set_xticks(range(n))\n",
    "axes[1].set_xticklabels([repr(c) for c in chars_to_show], rotation=45)\n",
    "axes[1].set_yticks(range(n))\n",
    "axes[1].set_yticklabels([repr(c) for c in chars_to_show])\n",
    "axes[1].set_title('Cosine Similarity Between Embeddings\\n(before training -- expect random)')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, the similarity matrix looks random -- all characters are equally dissimilar. After training, you would see clusters: lowercase letters would be similar to each other, digits would cluster together, and so on.\n",
    "\n",
    "### 4.3 Positional Embedding Table"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Embedding\n",
    "MAX_SEQ_LEN = 128   # Maximum sequence length\n",
    "\n",
    "pos_embedding = nn.Embedding(MAX_SEQ_LEN, D_MODEL)\n",
    "\n",
    "# Look up positional embeddings for positions 0, 1, 2\n",
    "positions = torch.arange(3)\n",
    "pos_embeds = pos_embedding(positions)\n",
    "\n",
    "print(f\"Positions: {positions}\")\n",
    "print(f\"Positional embeddings shape: {pos_embeds.shape}\")\n",
    "print(f\"\\nPosition 0 (first 10 dims): {pos_embeds[0, :10].detach().numpy().round(3)}\")\n",
    "print(f\"Position 1 (first 10 dims): {pos_embeds[1, :10].detach().numpy().round(3)}\")\n",
    "print(f\"Position 2 (first 10 dims): {pos_embeds[2, :10].detach().numpy().round(3)}\")\n",
    "print(f\"\\nTotal positional parameters: {MAX_SEQ_LEN * D_MODEL:,}\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize positional embeddings\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Positional embedding vectors\n",
    "all_pos = torch.arange(MAX_SEQ_LEN)\n",
    "all_pos_embeds = pos_embedding(all_pos).detach().numpy()\n",
    "\n",
    "axes[0].imshow(all_pos_embeds, aspect='auto', cmap='RdBu_r')\n",
    "axes[0].set_xlabel('Embedding Dimension')\n",
    "axes[0].set_ylabel('Position')\n",
    "axes[0].set_title(f'Positional Embeddings ({MAX_SEQ_LEN} positions x {D_MODEL} dims)')\n",
    "\n",
    "# Plot 2: Cosine similarity between positions\n",
    "pos_sample = torch.arange(0, 32)\n",
    "pos_sample_embeds = pos_embedding(pos_sample)\n",
    "n_pos = len(pos_sample)\n",
    "pos_sim = torch.zeros(n_pos, n_pos)\n",
    "for i in range(n_pos):\n",
    "    for j in range(n_pos):\n",
    "        pos_sim[i, j] = cosine_similarity(\n",
    "            pos_sample_embeds[i].unsqueeze(0),\n",
    "            pos_sample_embeds[j].unsqueeze(0)\n",
    "        )\n",
    "\n",
    "im = axes[1].imshow(pos_sim.detach().numpy(), cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "axes[1].set_xlabel('Position')\n",
    "axes[1].set_ylabel('Position')\n",
    "axes[1].set_title('Position Similarity (before training)')\n",
    "plt.colorbar(im, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Combining Token and Positional Embeddings"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full input embedding pipeline\n",
    "def embed_input(text, token_emb, pos_emb, tokenizer):\n",
    "    \"\"\"Convert raw text to embedded input vectors.\"\"\"\n",
    "    # Step 1: Tokenize\n",
    "    token_ids = torch.tensor([tokenizer.encode(text)])  # (1, T)\n",
    "    T = token_ids.shape[1]\n",
    "\n",
    "    # Step 2: Token embeddings\n",
    "    tok_emb = token_emb(token_ids)        # (1, T, d_model)\n",
    "\n",
    "    # Step 3: Positional embeddings\n",
    "    positions = torch.arange(T)           # (T,)\n",
    "    p_emb = pos_emb(positions)            # (T, d_model)\n",
    "\n",
    "    # Step 4: Add them together\n",
    "    x = tok_emb + p_emb                   # (1, T, d_model)\n",
    "\n",
    "    return x, token_ids\n",
    "\n",
    "# Test it\n",
    "text = \"Hello\"\n",
    "x, ids = embed_input(text, token_embedding, pos_embedding, tokenizer)\n",
    "print(f\"Input text: '{text}'\")\n",
    "print(f\"Token IDs: {ids[0].tolist()}\")\n",
    "print(f\"Output shape: {x.shape}\")\n",
    "print(f\"  Batch size: {x.shape[0]}\")\n",
    "print(f\"  Sequence length: {x.shape[1]}\")\n",
    "print(f\"  Embedding dim: {x.shape[2]}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the full embedding pipeline\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "text = \"The cat sat\"\n",
    "x, ids = embed_input(text, token_embedding, pos_embedding, tokenizer)\n",
    "\n",
    "tok_only = token_embedding(ids).detach().numpy()[0]\n",
    "pos_only = pos_embedding(torch.arange(ids.shape[1])).detach().numpy()\n",
    "combined = x.detach().numpy()[0]\n",
    "\n",
    "for ax, data, title in zip(axes,\n",
    "    [tok_only, pos_only, combined],\n",
    "    ['Token Embeddings Only', 'Positional Embeddings Only', 'Combined (Token + Position)']):\n",
    "    im = ax.imshow(data, aspect='auto', cmap='RdBu_r')\n",
    "    ax.set_xlabel('Embedding Dimension')\n",
    "    ax.set_ylabel('Token Position')\n",
    "    ax.set_yticks(range(len(text)))\n",
    "    ax.set_yticklabels(list(text), fontfamily='monospace')\n",
    "    ax.set_title(title)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.suptitle(f'Embedding Pipeline for \"{text}\"', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn\n",
    "\n",
    "### TODO 1: Implement the Full Embedding Module"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete GPT input embedding module.\n",
    "\n",
    "    This module takes raw token IDs and produces the combined\n",
    "    token + positional embedding vectors that serve as input\n",
    "    to the Transformer blocks.\n",
    "\n",
    "    Args:\n",
    "        vocab_size: number of tokens in the vocabulary\n",
    "        d_model: embedding dimension\n",
    "        max_seq_len: maximum sequence length\n",
    "\n",
    "    Forward:\n",
    "        Input: token_ids of shape (batch_size, seq_len)\n",
    "        Output: embeddings of shape (batch_size, seq_len, d_model)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, max_seq_len):\n",
    "        super().__init__()\n",
    "        # ============ TODO ============\n",
    "        # Step 1: Create the token embedding table (nn.Embedding)\n",
    "        # Step 2: Create the positional embedding table (nn.Embedding)\n",
    "        # ==============================\n",
    "        self.token_emb = None   # YOUR CODE HERE\n",
    "        self.pos_emb = None     # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_ids: (batch_size, seq_len) tensor of token indices\n",
    "\n",
    "        Returns:\n",
    "            (batch_size, seq_len, d_model) tensor of embeddings\n",
    "        \"\"\"\n",
    "        # ============ TODO ============\n",
    "        # Step 1: Get the sequence length T from token_ids\n",
    "        # Step 2: Look up token embeddings: (B, T, d_model)\n",
    "        # Step 3: Create position indices 0, 1, ..., T-1\n",
    "        # Step 4: Look up positional embeddings: (T, d_model)\n",
    "        # Step 5: Add token + positional embeddings\n",
    "        # ==============================\n",
    "        result = None  # YOUR CODE HERE\n",
    "        return result"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "emb_module = GPTEmbedding(vocab_size=256, d_model=64, max_seq_len=128)\n",
    "test_ids = torch.tensor([[72, 101, 108, 108, 111]])  # \"Hello\"\n",
    "output = emb_module(test_ids)\n",
    "\n",
    "assert output is not None, \"Forward returned None -- did you forget to return the result?\"\n",
    "assert output.shape == (1, 5, 64), f\"Expected shape (1, 5, 64), got {output.shape}\"\n",
    "assert isinstance(emb_module.token_emb, nn.Embedding), \"token_emb should be nn.Embedding\"\n",
    "assert isinstance(emb_module.pos_emb, nn.Embedding), \"pos_emb should be nn.Embedding\"\n",
    "assert emb_module.token_emb.num_embeddings == 256, \"token_emb should have 256 entries\"\n",
    "assert emb_module.pos_emb.num_embeddings == 128, \"pos_emb should have 128 entries\"\n",
    "print(\"All assertions passed! Your embedding module is correct.\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Explore How Position Changes the Representation"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_experiment(embedding_module, token_id, num_positions=20):\n",
    "    \"\"\"\n",
    "    Investigate how positional embeddings change the representation\n",
    "    of the SAME token at different positions.\n",
    "\n",
    "    Place the given token_id at positions 0 through num_positions-1,\n",
    "    compute the cosine similarity between each pair, and visualize\n",
    "    the result.\n",
    "\n",
    "    Args:\n",
    "        embedding_module: your GPTEmbedding instance\n",
    "        token_id: integer token ID to test\n",
    "        num_positions: number of positions to test\n",
    "\n",
    "    Returns:\n",
    "        similarity_matrix: (num_positions, num_positions) numpy array\n",
    "\n",
    "    Steps:\n",
    "        1. Create input tensors: [[token_id]] for each position,\n",
    "           but each with padding so the token appears at a\n",
    "           different position. Actually, a simpler approach:\n",
    "           create a batch of sequences where position i has the\n",
    "           token at index i (use 0 for other positions).\n",
    "        2. Get embeddings from the module.\n",
    "        3. Extract the embedding at the target position for each.\n",
    "        4. Compute cosine similarity between all pairs.\n",
    "        5. Plot the similarity matrix as a heatmap.\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Hint: The simplest approach is to create a (num_positions, num_positions)\n",
    "    # input where each row is all zeros except for the token_id at one position.\n",
    "    # Then get embeddings and extract the non-zero position from each row.\n",
    "    # ==============================\n",
    "    similarity_matrix = None  # YOUR CODE HERE\n",
    "\n",
    "    # Plot\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return similarity_matrix"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us combine our tokenizer and embedding module into a complete input pipeline and test it on real text."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTInputPipeline:\n",
    "    \"\"\"Complete input pipeline: text -> embedded vectors.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model=64, max_seq_len=128):\n",
    "        self.tokenizer = CharTokenizer()\n",
    "        self.embedding = GPTEmbedding(\n",
    "            vocab_size=self.tokenizer.vocab_size,\n",
    "            d_model=d_model,\n",
    "            max_seq_len=max_seq_len\n",
    "        )\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __call__(self, text):\n",
    "        \"\"\"Convert text to embedded vectors.\"\"\"\n",
    "        ids = self.tokenizer.encode(text)\n",
    "        if len(ids) > self.max_seq_len:\n",
    "            ids = ids[:self.max_seq_len]\n",
    "        token_ids = torch.tensor([ids])\n",
    "        return self.embedding(token_ids)\n",
    "\n",
    "# Test the full pipeline\n",
    "pipeline = GPTInputPipeline(d_model=64, max_seq_len=128)\n",
    "text = \"GPT predicts the next token.\"\n",
    "output = pipeline(text)\n",
    "print(f\"Input: '{text}'\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"  -> {output.shape[0]} batch, {output.shape[1]} tokens, {output.shape[2]} dimensions\")\n",
    "print(f\"\\nThe model now has a {output.shape[2]}-dimensional vector for each character,\")\n",
    "print(f\"encoding both WHAT the character is and WHERE it appears.\")"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results\n",
    "\n",
    "We will not train the embedding module in isolation -- it gets trained as part of the full GPT model. But we can demonstrate what *trained* embeddings look like by training a simple next-character prediction model."
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick demo: train embeddings on a small text\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Training data\n",
    "train_text = \"\"\"To be or not to be that is the question whether tis nobler\n",
    "in the mind to suffer the slings and arrows of outrageous fortune or to\n",
    "take arms against a sea of troubles and by opposing end them to die to\n",
    "sleep no more and by a sleep to say we end the heartache and the thousand\n",
    "natural shocks that flesh is heir to\"\"\" * 10\n",
    "\n",
    "# Simple model: embedding + linear head\n",
    "class SimpleCharPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size=256, d_model=64, max_seq_len=128):\n",
    "        super().__init__()\n",
    "        self.emb = GPTEmbedding(vocab_size, d_model, max_seq_len)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.emb(x)\n",
    "        return self.head(h)\n",
    "\n",
    "# Train\n",
    "model = SimpleCharPredictor()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Prepare data\n",
    "tokenizer = CharTokenizer()\n",
    "data = torch.tensor(tokenizer.encode(train_text))\n",
    "seq_len = 32\n",
    "\n",
    "losses = []\n",
    "for step in range(500):\n",
    "    # Random batch\n",
    "    idx = torch.randint(0, len(data) - seq_len - 1, (16,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in idx])\n",
    "\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits.view(-1, 256), y.view(-1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}: loss = {loss.item():.4f}\")\n",
    "\n",
    "print(f\"\\nFinal loss: {losses[-1]:.4f}\")\n",
    "print(f\"Random baseline: {np.log(256):.4f}\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize trained vs untrained embeddings\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Trained embeddings: similarity between characters\n",
    "chars_to_check = list('abcdefghijklmnopqrstuvwxyz .!?0123456789')\n",
    "char_ids = torch.tensor([tokenizer.char_to_id[c] for c in chars_to_check])\n",
    "trained_embeds = model.emb.token_emb(char_ids)\n",
    "\n",
    "n = len(chars_to_check)\n",
    "sim = torch.zeros(n, n)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        sim[i, j] = cosine_similarity(\n",
    "            trained_embeds[i].unsqueeze(0).detach(),\n",
    "            trained_embeds[j].unsqueeze(0).detach()\n",
    "        )\n",
    "\n",
    "im = axes[0].imshow(sim.numpy(), cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "axes[0].set_title('Character Similarity (After Training)')\n",
    "axes[0].set_xticks(range(0, n, 3))\n",
    "axes[0].set_xticklabels([chars_to_check[i] for i in range(0, n, 3)], fontfamily='monospace')\n",
    "axes[0].set_yticks(range(0, n, 3))\n",
    "axes[0].set_yticklabels([chars_to_check[i] for i in range(0, n, 3)], fontfamily='monospace')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Training loss curve\n",
    "axes[1].plot(losses, alpha=0.3, color='blue')\n",
    "# Smoothed\n",
    "window = 20\n",
    "smoothed = [np.mean(losses[max(0,i-window):i+1]) for i in range(len(losses))]\n",
    "axes[1].plot(smoothed, color='blue', linewidth=2, label='Smoothed loss')\n",
    "axes[1].axhline(y=np.log(256), color='red', linestyle='--', label=f'Random baseline ({np.log(256):.2f})')\n",
    "axes[1].set_xlabel('Training Step')\n",
    "axes[1].set_ylabel('Cross-Entropy Loss')\n",
    "axes[1].set_title('Training Loss: Embeddings Learning Character Patterns')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the learned structure in the embeddings\n",
    "print(\"=\" * 60)\n",
    "print(\"  TRAINED EMBEDDING ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Find most similar character pairs\n",
    "pairs = []\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        pairs.append((chars_to_check[i], chars_to_check[j], sim[i, j].item()))\n",
    "\n",
    "pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"\\nMost similar character pairs (after training):\")\n",
    "for c1, c2, s in pairs[:10]:\n",
    "    print(f\"  '{c1}' <-> '{c2}': similarity = {s:.3f}\")\n",
    "\n",
    "print(\"\\nLeast similar character pairs:\")\n",
    "for c1, c2, s in pairs[-5:]:\n",
    "    print(f\"  '{c1}' <-> '{c2}': similarity = {s:.3f}\")\n",
    "\n",
    "# Show embedding norms\n",
    "norms = trained_embeds.detach().norm(dim=1)\n",
    "print(f\"\\nEmbedding norms (mean: {norms.mean():.3f}, std: {norms.std():.3f})\")\n",
    "print(f\"Largest norm: '{chars_to_check[norms.argmax()]}' = {norms.max():.3f}\")\n",
    "print(f\"Smallest norm: '{chars_to_check[norms.argmin()]}' = {norms.min():.3f}\")\n",
    "\n",
    "print(\"\\nYou have built the input pipeline of a GPT model from scratch!\")\n",
    "print(\"Token embeddings + positional embeddings = the model's view of language.\")"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "1. Why does GPT add positional embeddings rather than concatenating them to the token embeddings? What would change if we concatenated instead?\n",
    "2. After training, vowels tend to cluster together in embedding space. Why might this happen, given that the training objective is next-character prediction?\n",
    "3. GPT-2 uses a maximum sequence length of 1024 tokens. What happens if you try to process a sequence of 1025 tokens? How would you handle very long documents?\n",
    "\n",
    "### Optional Challenges\n",
    "1. Replace the character-level tokenizer with a simple word-level tokenizer. How does this change the embedding table size and the quality of learned representations?\n",
    "2. Implement sinusoidal positional embeddings (as in the original Transformer paper) instead of learned ones. Compare the positional similarity matrices -- which shows more structure before training?\n",
    "3. Visualize the embeddings using t-SNE or PCA after training on a larger dataset. Do meaningful clusters emerge?"
   ],
   "id": "cell_29"
  }
 ]
}