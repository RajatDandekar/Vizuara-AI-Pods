{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Building a Mini Vision-Language Model from Scratch \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Mini Vision-Language Model from Scratch\n",
    "\n",
    "**Vizuara AI** | Cross-Attention & Token Alignment Series \u2014 Notebook 3 of 3\n",
    "\n",
    "In Notebooks 1 and 2, we built cross-attention and token alignment as standalone components. Now we put everything together into a **working mini VLM** \u2014 a model that takes image patches and text tokens, processes them through a full transformer block with cross-attention, and produces enriched text representations.\n",
    "\n",
    "We will train this model on a toy image captioning task and visualize what it learns."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "All modern Vision-Language Models \u2014 LLaVA, Flamingo, BLIP-2, GPT-4V \u2014 use the same core pattern:\n",
    "\n",
    "1. **Encode the image** into patch tokens (ViT)\n",
    "2. **Align** image tokens to the language space (projection layer)\n",
    "3. **Cross-attend** \u2014 let text tokens query image tokens\n",
    "4. **Generate** text conditioned on the visual information\n",
    "\n",
    "In this notebook, we build a simplified version of this pipeline. Our model will:\n",
    "- Take synthetic \"images\" (structured patch embeddings)\n",
    "- Take text token sequences\n",
    "- Run cross-attention to enrich text with image info\n",
    "- Learn which patches matter for which words\n",
    "\n",
    "The output: a trained model with visualizable attention patterns showing the model has learned to ground language in vision.\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "### The Full Pipeline\n",
    "\n",
    "Think of a VLM block as a three-stage process for each text token:\n",
    "\n",
    "1. **Self-attention:** \"Let me first understand the context of my sentence\" \u2014 text tokens talk to each other\n",
    "2. **Cross-attention:** \"Now let me look at the image\" \u2014 text tokens query image patches\n",
    "3. **Feed-forward:** \"Let me process all this information\" \u2014 nonlinear transformation\n",
    "\n",
    "Each text token comes out the other side carrying both linguistic context (from self-attention) and visual information (from cross-attention).\n",
    "\n",
    "### Think About This\n",
    "Before we build:\n",
    "- Why does self-attention come BEFORE cross-attention in most architectures?\n",
    "- What role does the feed-forward network play after attention?\n",
    "\n",
    "## 3. The Mathematics\n",
    "\n",
    "### The VLM Block\n",
    "\n",
    "A single VLM block applies three sub-layers with residual connections and layer normalization:\n",
    "\n",
    "$$\\hat{x} = x + \\text{SelfAttention}(\\text{LayerNorm}(x))$$\n",
    "\n",
    "$$\\tilde{x} = \\hat{x} + \\text{CrossAttention}(\\text{LayerNorm}(\\hat{x}), z_{\\text{image}})$$\n",
    "\n",
    "$$y = \\tilde{x} + \\text{FFN}(\\text{LayerNorm}(\\tilde{x}))$$\n",
    "\n",
    "Where:\n",
    "- $x$ = text token sequence\n",
    "- $z_{\\text{image}}$ = aligned image tokens (after projection)\n",
    "- LayerNorm normalizes each token independently\n",
    "- Residual connections ($+$) prevent gradient vanishing\n",
    "\n",
    "**Numerical intuition:** Suppose a text token's value is $[1.0, 2.0]$. After cross-attention, it might become $[0.3, 0.7]$ (information from the image). The residual connection gives $[1.0 + 0.3, 2.0 + 0.7] = [1.3, 2.7]$ \u2014 the original information is preserved, with image info added on top.\n",
    "\n",
    "### Layer Normalization\n",
    "\n",
    "For a vector $x = [x_1, x_2, \\ldots, x_d]$:\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma + \\epsilon} \\cdot \\gamma + \\beta$$\n",
    "\n",
    "Where $\\mu$ and $\\sigma$ are the mean and std of $x$, and $\\gamma, \\beta$ are learnable.\n",
    "\n",
    "**Example:** If $x = [1, 3]$, then $\\mu = 2$, $\\sigma = 1$:\n",
    "\n",
    "$$\\text{LayerNorm}([1, 3]) = [-1, 1] \\cdot \\gamma + \\beta$$\n",
    "\n",
    "This normalizes each token to zero mean and unit variance, stabilizing training.\n",
    "\n",
    "## 4. Let's Build It \u2014 Component by Component\n",
    "\n",
    "### 4.1 The Building Blocks (from Notebooks 1 & 2)"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-implement our core components\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    \"\"\"Multi-head cross-attention: Q from text, K/V from image.\"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_O = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, text_tokens, image_tokens):\n",
    "        n_text = text_tokens.size(0)\n",
    "        n_image = image_tokens.size(0)\n",
    "\n",
    "        Q = self.W_Q(text_tokens).view(n_text, self.num_heads, self.d_k).transpose(0, 1)\n",
    "        K = self.W_K(image_tokens).view(n_image, self.num_heads, self.d_k).transpose(0, 1)\n",
    "        V = self.W_V(image_tokens).view(n_image, self.num_heads, self.d_k).transpose(0, 1)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "\n",
    "        context = context.transpose(0, 1).contiguous().view(n_text, self.d_model)\n",
    "        output = self.W_O(context)\n",
    "\n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention: Q, K, V all from the same input.\"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_O = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n = x.size(0)\n",
    "\n",
    "        Q = self.W_Q(x).view(n, self.num_heads, self.d_k).transpose(0, 1)\n",
    "        K = self.W_K(x).view(n, self.num_heads, self.d_k).transpose(0, 1)\n",
    "        V = self.W_V(x).view(n, self.num_heads, self.d_k).transpose(0, 1)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "\n",
    "        context = context.transpose(0, 1).contiguous().view(n, self.d_model)\n",
    "        return self.W_O(context), attn_weights\n",
    "\n",
    "print(\"Building blocks ready!\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Vision-Language Block"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionLanguageBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single transformer block for a vision-language model.\n",
    "    Sub-layers: self-attention \u2192 cross-attention \u2192 FFN\n",
    "    Each with residual connection and layer norm.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadCrossAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, text_tokens, image_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_tokens:  (n_text, d_model)\n",
    "            image_tokens: (n_image, d_model) \u2014 already aligned\n",
    "        Returns:\n",
    "            output: (n_text, d_model) \u2014 text enriched with image info\n",
    "            cross_attn_weights: (num_heads, n_text, n_image)\n",
    "        \"\"\"\n",
    "        # 1. Self-attention (text talks to text)\n",
    "        normed = self.norm1(text_tokens)\n",
    "        sa_out, _ = self.self_attn(normed)\n",
    "        text_tokens = text_tokens + sa_out  # residual\n",
    "\n",
    "        # 2. Cross-attention (text queries image)\n",
    "        normed = self.norm2(text_tokens)\n",
    "        ca_out, cross_attn_weights = self.cross_attn(normed, image_tokens)\n",
    "        text_tokens = text_tokens + ca_out  # residual\n",
    "\n",
    "        # 3. Feed-forward network\n",
    "        normed = self.norm3(text_tokens)\n",
    "        ff_out = self.ffn(normed)\n",
    "        text_tokens = text_tokens + ff_out  # residual\n",
    "\n",
    "        return text_tokens, cross_attn_weights\n",
    "\n",
    "# Test\n",
    "d_model = 32\n",
    "block = VisionLanguageBlock(d_model=d_model, num_heads=4, d_ff=64)\n",
    "\n",
    "text_in = torch.randn(5, d_model)\n",
    "image_in = torch.randn(9, d_model)\n",
    "\n",
    "text_out, attn = block(text_in, image_in)\n",
    "print(f\"Text input:  {text_in.shape}\")\n",
    "print(f\"Image input: {image_in.shape}\")\n",
    "print(f\"Text output: {text_out.shape} (same shape \u2014 enriched with image info)\")\n",
    "print(f\"Attention:   {attn.shape} (4 heads, 5 text tokens, 9 image patches)\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The Complete Mini-VLM"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO 1 ============\n",
    "# Complete the MiniVLM by filling in the forward method.\n",
    "# The pipeline is:\n",
    "# 1. Project image tokens from d_vision to d_model\n",
    "# 2. Pass text and projected image through VLM blocks\n",
    "# 3. Produce output predictions\n",
    "# ================================\n",
    "\n",
    "class MiniVLM(nn.Module):\n",
    "    \"\"\"\n",
    "    A mini Vision-Language Model.\n",
    "    Takes image patches (d_vision) and text tokens (d_model),\n",
    "    runs cross-attention, outputs predictions.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_vision, d_model, num_heads, d_ff, num_layers, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Token alignment: project image from vision to text space\n",
    "        self.image_proj = nn.Linear(d_vision, d_model)\n",
    "\n",
    "        # Text embedding (simple lookup table)\n",
    "        self.text_embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Stack of VLM blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            VisionLanguageBlock(d_model, num_heads, d_ff)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Output head: predict next token\n",
    "        self.output_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, text_ids, image_patches):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_ids:      (n_text,) \u2014 integer token IDs\n",
    "            image_patches: (n_image, d_vision) \u2014 raw image patch features\n",
    "        Returns:\n",
    "            logits:       (n_text, vocab_size)\n",
    "            all_attn:     list of (num_heads, n_text, n_image) per layer\n",
    "        \"\"\"\n",
    "        # ============ YOUR CODE HERE ============\n",
    "        # Step 1: Embed text tokens\n",
    "        text_tokens = ???  # Use self.text_embedding\n",
    "\n",
    "        # Step 2: Project image tokens\n",
    "        image_tokens = ???  # Use self.image_proj\n",
    "\n",
    "        # Step 3: Pass through VLM blocks\n",
    "        all_attn = []\n",
    "        for block in self.blocks:\n",
    "            text_tokens, attn_weights = ???  # Call block\n",
    "            all_attn.append(attn_weights)\n",
    "\n",
    "        # Step 4: Compute output logits\n",
    "        logits = ???  # Use self.output_head\n",
    "        # ========================================\n",
    "\n",
    "        return logits, all_attn"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification for TODO 1\n",
    "class MiniVLMRef(nn.Module):\n",
    "    def __init__(self, d_vision, d_model, num_heads, d_ff, num_layers, vocab_size):\n",
    "        super().__init__()\n",
    "        self.image_proj = nn.Linear(d_vision, d_model)\n",
    "        self.text_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            VisionLanguageBlock(d_model, num_heads, d_ff)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.output_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, text_ids, image_patches):\n",
    "        text_tokens = self.text_embedding(text_ids)\n",
    "        image_tokens = self.image_proj(image_patches)\n",
    "        all_attn = []\n",
    "        for block in self.blocks:\n",
    "            text_tokens, attn_weights = block(text_tokens, image_tokens)\n",
    "            all_attn.append(attn_weights)\n",
    "        logits = self.output_head(text_tokens)\n",
    "        return logits, all_attn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "ref_model = MiniVLMRef(d_vision=64, d_model=32, num_heads=4, d_ff=64, num_layers=2, vocab_size=100)\n",
    "torch.manual_seed(42)\n",
    "student_model = MiniVLM(d_vision=64, d_model=32, num_heads=4, d_ff=64, num_layers=2, vocab_size=100)\n",
    "student_model.load_state_dict(ref_model.state_dict())\n",
    "\n",
    "test_text = torch.tensor([5, 12, 37, 88])\n",
    "test_image = torch.randn(9, 64)\n",
    "\n",
    "try:\n",
    "    ref_logits, ref_attn = ref_model(test_text, test_image)\n",
    "    stu_logits, stu_attn = student_model(test_text, test_image)\n",
    "\n",
    "    assert stu_logits.shape == (4, 100), f\"Expected (4, 100), got {stu_logits.shape}\"\n",
    "    assert len(stu_attn) == 2, f\"Expected 2 attention layers, got {len(stu_attn)}\"\n",
    "    assert torch.allclose(stu_logits, ref_logits, atol=1e-4), \"Logits don't match\"\n",
    "    print(\"Correct! Your MiniVLM works perfectly.\")\n",
    "    print(f\"  Logits: {stu_logits.shape}\")\n",
    "    print(f\"  Attention layers: {len(stu_attn)}\")\n",
    "    print(f\"  Each attention: {stu_attn[0].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Not quite: {e}\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn \u2014 Train on a Toy Task\n",
    "\n",
    "Let us create a simple toy task: given an \"image\" with a highlighted region, predict which region class the image belongs to."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO 2 ============\n",
    "# Create a simple training loop for the MiniVLM.\n",
    "# We will train it to associate image patches with text tokens.\n",
    "#\n",
    "# The task: given an image where one patch is \"activated\" (has a special pattern),\n",
    "# predict which text token corresponds to that patch.\n",
    "# ================================\n",
    "\n",
    "# Synthetic dataset\n",
    "def generate_batch(batch_size=16, n_patches=9, d_vision=64, vocab_size=10):\n",
    "    \"\"\"Generate a batch of (image, text, target) tuples.\n",
    "\n",
    "    Each image has one \"hot\" patch. The text is a fixed prompt [CLS, 0, 0, 0, PREDICT].\n",
    "    The target is the index of the hot patch (0-8), mapped to a class.\n",
    "    \"\"\"\n",
    "    images = torch.randn(batch_size, n_patches, d_vision) * 0.1  # base noise\n",
    "    targets = torch.randint(0, n_patches, (batch_size,))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Make the target patch distinctive\n",
    "        images[i, targets[i]] = torch.randn(d_vision) * 2.0 + targets[i].float()\n",
    "\n",
    "    # Fixed text prompt: [CLS=0, PAD=1, PAD=1, PAD=1, QUERY=2]\n",
    "    text_ids = torch.tensor([[0, 1, 1, 1, 2]] * batch_size)\n",
    "\n",
    "    return images, text_ids, targets\n",
    "\n",
    "\n",
    "# Training\n",
    "torch.manual_seed(42)\n",
    "model = MiniVLMRef(d_vision=64, d_model=32, num_heads=4, d_ff=64, num_layers=2, vocab_size=10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "for step in range(200):\n",
    "    images, text_ids, targets = generate_batch(batch_size=32)\n",
    "\n",
    "    logits, attn = model(text_ids[0], images[0])  # process one at a time for simplicity\n",
    "\n",
    "    # Use the last token's logits for prediction (like [PREDICT])\n",
    "    # Process batch\n",
    "    batch_logits = []\n",
    "    batch_attn = []\n",
    "    for i in range(images.size(0)):\n",
    "        logit, att = model(text_ids[i], images[i])\n",
    "        batch_logits.append(logit[-1])  # last token prediction\n",
    "        batch_attn.append(att)\n",
    "\n",
    "    batch_logits = torch.stack(batch_logits)  # (batch, vocab_size)\n",
    "\n",
    "    # Map targets to vocab (0-8 -> 0-8, using first 9 vocab entries)\n",
    "    loss = loss_fn(batch_logits, targets)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Accuracy\n",
    "    preds = batch_logits.argmax(dim=-1)\n",
    "    acc = (preds == targets).float().mean().item()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    accuracies.append(acc)\n",
    "\n",
    "    if (step + 1) % 50 == 0:\n",
    "        print(f\"Step {step+1}: loss={loss.item():.4f}, accuracy={acc:.2%}\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(losses, color='#e74c3c', alpha=0.7)\n",
    "ax1.set_xlabel('Training Step')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss', fontsize=13)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.plot(accuracies, color='#2ecc71', alpha=0.7)\n",
    "ax2.set_xlabel('Training Step')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training Accuracy', fontsize=13)\n",
    "ax2.axhline(y=1/9, color='gray', linestyle='--', alpha=0.5, label='Random chance (1/9)')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('MiniVLM Training on Patch Classification Task', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together \u2014 Visualize the Trained Model"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a test example and visualize attention\n",
    "model.eval()\n",
    "\n",
    "test_images, test_text, test_targets = generate_batch(batch_size=4)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(4):\n",
    "    with torch.no_grad():\n",
    "        logits, attn_layers = model(test_text[i], test_images[i])\n",
    "\n",
    "    pred = logits[-1].argmax().item()\n",
    "    target = test_targets[i].item()\n",
    "\n",
    "    # Show image patches (as heatmap of patch norms)\n",
    "    patch_norms = test_images[i].norm(dim=-1).numpy()\n",
    "    axes[0, i].bar(range(9), patch_norms, color=['#e74c3c' if j == target else '#3498db'\n",
    "                                                   for j in range(9)])\n",
    "    axes[0, i].set_title(f'Target patch: {target}, Pred: {pred}',\n",
    "                          fontsize=11, fontweight='bold',\n",
    "                          color='green' if pred == target else 'red')\n",
    "    axes[0, i].set_xlabel('Patch Index')\n",
    "    axes[0, i].set_ylabel('Patch Norm')\n",
    "\n",
    "    # Show attention of the PREDICT token (last text token) to image patches\n",
    "    # Use the last layer's attention, averaged across heads\n",
    "    last_layer_attn = attn_layers[-1].mean(dim=0)  # (n_text, n_image)\n",
    "    predict_token_attn = last_layer_attn[-1].detach().numpy()  # last text token\n",
    "\n",
    "    axes[1, i].bar(range(9), predict_token_attn,\n",
    "                   color=['#e74c3c' if j == target else '#95a5a6' for j in range(9)])\n",
    "    axes[1, i].set_xlabel('Image Patch')\n",
    "    axes[1, i].set_ylabel('Attention Weight')\n",
    "    axes[1, i].set_title('Cross-Attn of [PREDICT] Token', fontsize=10)\n",
    "\n",
    "axes[0, 0].set_ylabel('Patch Norm\\n(higher = activated)', fontsize=10)\n",
    "axes[1, 0].set_ylabel('Attention Weight\\n(higher = more focus)', fontsize=10)\n",
    "\n",
    "plt.suptitle('Trained MiniVLM: Does the Model Attend to the Right Patch?',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Red bars show the target patch.\")\n",
    "print(\"If the model learned correctly, the attention weights (bottom row)\")\n",
    "print(\"should peak at the same position as the activated patch (top row).\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results \u2014 Architecture Comparison"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: 1 layer vs 2 layers vs 4 layers\n",
    "configs = [\n",
    "    (\"1 layer\", 1),\n",
    "    (\"2 layers\", 2),\n",
    "    (\"4 layers\", 4),\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, n_layers in configs:\n",
    "    torch.manual_seed(42)\n",
    "    m = MiniVLMRef(d_vision=64, d_model=32, num_heads=4, d_ff=64,\n",
    "                    num_layers=n_layers, vocab_size=10)\n",
    "    opt = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "\n",
    "    accs = []\n",
    "    for step in range(200):\n",
    "        imgs, txt, tgt = generate_batch(batch_size=32)\n",
    "        batch_logits = []\n",
    "        for i in range(imgs.size(0)):\n",
    "            logit, _ = m(txt[i], imgs[i])\n",
    "            batch_logits.append(logit[-1])\n",
    "        batch_logits = torch.stack(batch_logits)\n",
    "        loss = loss_fn(batch_logits, tgt)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        accs.append((batch_logits.argmax(-1) == tgt).float().mean().item())\n",
    "\n",
    "    results[name] = accs\n",
    "    print(f\"{name}: final accuracy = {accs[-1]:.2%}\")\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "for (name, _), color in zip(configs, colors):\n",
    "    plt.plot(results[name], label=name, color=color, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Depth Comparison on Patch Classification', fontsize=14, fontweight='bold')\n",
    "plt.axhline(y=1/9, color='gray', linestyle='--', alpha=0.5, label='Random chance')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output \u2014 The Complete VLM Pipeline Visualization"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive visualization\n",
    "model.eval()\n",
    "\n",
    "# Generate a single test case\n",
    "torch.manual_seed(99)\n",
    "test_img, test_txt, test_tgt = generate_batch(batch_size=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, attn_layers = model(test_txt[0], test_img[0])\n",
    "\n",
    "pred = logits[-1].argmax().item()\n",
    "target = test_tgt[0].item()\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 4, hspace=0.4, wspace=0.3)\n",
    "\n",
    "# Row 1: Image patches + projection\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "patch_norms = test_img[0].norm(dim=-1).numpy()\n",
    "bars = ax1.bar(range(9), patch_norms,\n",
    "               color=['#e74c3c' if j == target else '#3498db' for j in range(9)],\n",
    "               edgecolor='white', linewidth=0.5)\n",
    "ax1.set_title('Image Patches (red = target)', fontsize=13, fontweight='bold')\n",
    "ax1.set_xlabel('Patch Index')\n",
    "ax1.set_ylabel('Feature Norm')\n",
    "\n",
    "# Row 1: Text tokens\n",
    "ax2 = fig.add_subplot(gs[0, 2:])\n",
    "text_labels = ['[CLS]', 'PAD', 'PAD', 'PAD', '[PREDICT]']\n",
    "text_colors = ['#9b59b6', '#bdc3c7', '#bdc3c7', '#bdc3c7', '#f39c12']\n",
    "ax2.barh(range(5), [1]*5, color=text_colors, edgecolor='white')\n",
    "for i, label in enumerate(text_labels):\n",
    "    ax2.text(0.5, i, label, ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Text Token Sequence', fontsize=13, fontweight='bold')\n",
    "ax2.set_yticks([])\n",
    "ax2.set_xticks([])\n",
    "\n",
    "# Row 2: Cross-attention per head (last layer, PREDICT token)\n",
    "for h in range(4):\n",
    "    ax = fig.add_subplot(gs[1, h])\n",
    "    attn_map = attn_layers[-1][h, -1].detach().numpy().reshape(3, 3)\n",
    "    im = ax.imshow(attn_map, cmap='YlOrRd', vmin=0)\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            ax.text(c, r, f'{attn_map[r,c]:.2f}', ha='center', va='center',\n",
    "                    fontsize=9, color='white' if attn_map[r,c] > 0.13 else 'black')\n",
    "    ax.set_title(f'Head {h+1}', fontsize=12)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Row 3: Average attention + prediction\n",
    "ax5 = fig.add_subplot(gs[2, :2])\n",
    "avg_attn = attn_layers[-1].mean(dim=0)[-1].detach().numpy()\n",
    "ax5.bar(range(9), avg_attn,\n",
    "        color=['#e74c3c' if j == target else '#95a5a6' for j in range(9)])\n",
    "ax5.set_title('Average Cross-Attention (all heads)', fontsize=12, fontweight='bold')\n",
    "ax5.set_xlabel('Image Patch')\n",
    "ax5.set_ylabel('Attention Weight')\n",
    "\n",
    "ax6 = fig.add_subplot(gs[2, 2:])\n",
    "probs = F.softmax(logits[-1], dim=-1).detach().numpy()[:9]\n",
    "ax6.bar(range(9), probs,\n",
    "        color=['#2ecc71' if j == pred else '#bdc3c7' for j in range(9)])\n",
    "ax6.set_title(f'Prediction: patch {pred} (target: {target})',\n",
    "              fontsize=12, fontweight='bold',\n",
    "              color='green' if pred == target else 'red')\n",
    "ax6.set_xlabel('Class (Patch Index)')\n",
    "ax6.set_ylabel('Probability')\n",
    "\n",
    "plt.suptitle('Mini VLM: Complete Cross-Attention Pipeline',\n",
    "             fontsize=16, fontweight='bold', y=1.0)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTarget patch: {target}\")\n",
    "print(f\"Predicted patch: {pred}\")\n",
    "print(f\"Correct: {'Yes!' if pred == target else 'No'}\")\n",
    "print(f\"\\nThis visualization shows the complete VLM pipeline:\")\n",
    "print(\"1. Image patches \u2192 2. Text tokens \u2192 3. Per-head attention \u2192 4. Prediction\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. A **VLM block** has three sub-layers: self-attention (text context), cross-attention (visual grounding), and FFN (nonlinear processing)\n",
    "2. **Residual connections** preserve the original information while adding new information from attention\n",
    "3. **Layer normalization** stabilizes training by normalizing each token independently\n",
    "4. The trained model learns to **focus its cross-attention on the relevant image patch** \u2014 this is visual grounding in action\n",
    "5. **Multiple heads** attend to different aspects of the image simultaneously\n",
    "\n",
    "### What We Built\n",
    "\n",
    "Across these three notebooks, we built every component of a modern VLM from scratch:\n",
    "- **Notebook 1:** Scaled dot-product attention, self-attention, cross-attention\n",
    "- **Notebook 2:** Token alignment (linear/MLP projection), multi-head attention\n",
    "- **Notebook 3:** Full VLM block, training loop, attention visualization\n",
    "\n",
    "### Reflection Questions\n",
    "- How would you modify this model for actual image captioning (generating words sequentially)?\n",
    "- What is the computational cost of cross-attention compared to self-attention? (Hint: think about the shapes of Q, K, V)\n",
    "- In practice, LLaVA skips cross-attention entirely and just concatenates image tokens with text tokens. What are the tradeoffs?\n",
    "- How would you add causal masking to the self-attention (needed for autoregressive generation)?\n",
    "\n",
    "### Going Further\n",
    "- Read the LLaVA paper (Liu et al., 2023) to see how a simple linear projection achieves strong results\n",
    "- Read the Flamingo paper (Alayrac et al., 2022) to see gated cross-attention in action\n",
    "- Try extending this notebook to generate text autoregressively"
   ],
   "id": "cell_19"
  }
 ]
}