{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Token Alignment and Multi-Head Cross-Attention \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Alignment and Multi-Head Cross-Attention\n",
    "\n",
    "**Vizuara AI** | Cross-Attention & Token Alignment Series \u2014 Notebook 2 of 3\n",
    "\n",
    "In Notebook 1, we built cross-attention from scratch. But we assumed that text and image tokens lived in the **same** embedding space. In practice, a Vision Transformer produces 768-dimensional patch embeddings while a language model might use 4096 dimensions. They are incompatible.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Solve the **token alignment problem** with projection layers\n",
    "2. Implement **multi-head cross-attention** where different heads attend to different image regions\n",
    "3. Visualize how each head specializes"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Real vision-language models face a fundamental incompatibility:\n",
    "- A **ViT-B/16** produces patch tokens in $\\mathbb{R}^{768}$\n",
    "- **LLaMA-7B** works in $\\mathbb{R}^{4096}$\n",
    "\n",
    "You cannot compute $QK^\\top$ when Q is 4096-dimensional and K is 768-dimensional \u2014 the dot product requires matching dimensions.\n",
    "\n",
    "Token alignment solves this: a learned projection maps image tokens from the vision space into the language space.\n",
    "\n",
    "Additionally, a single attention head can only focus on one pattern at a time. **Multi-head attention** runs multiple independent attention operations, so different heads can focus on different aspects \u2014 one head for spatial relationships, another for object categories, etc.\n",
    "\n",
    "By the end of this notebook, you will see multiple heads independently attending to different image regions for the same text query.\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "### The Currency Exchange Analogy\n",
    "\n",
    "Token alignment is like currency exchange. Imagine you are traveling from Japan to the US:\n",
    "- Your **yen** (768-dim vision tokens) cannot be directly used at US stores\n",
    "- You exchange them at a **currency exchange counter** (the projection matrix $W_{\\text{proj}}$)\n",
    "- Now you have **dollars** (4096-dim language tokens) that work at US stores\n",
    "\n",
    "The \"exchange rate\" (projection weights) is *learned* during training \u2014 the model discovers the best mapping between vision and language spaces.\n",
    "\n",
    "### The Orchestra Analogy for Multi-Head Attention\n",
    "\n",
    "Imagine an orchestra where multiple musicians each listen to the same piece but focus on different aspects:\n",
    "- **Head 1** (the violinist) focuses on the melody \u2014 the main object in the image\n",
    "- **Head 2** (the drummer) focuses on the rhythm \u2014 spatial relationships\n",
    "- **Head 3** (the bass player) focuses on the harmony \u2014 background context\n",
    "\n",
    "Each head produces its own interpretation, and the final output is a rich combination of all perspectives.\n",
    "\n",
    "### Think About This\n",
    "- Why not just use a single, larger attention head instead of multiple smaller ones?\n",
    "- What information might be lost during the projection from 768 to 4096 dimensions?\n",
    "\n",
    "## 3. The Mathematics\n",
    "\n",
    "### Token Alignment \u2014 Linear Projection\n",
    "\n",
    "The simplest alignment is a linear projection:\n",
    "\n",
    "$$Z_{\\text{image}} = X_{\\text{image}} \\cdot W_{\\text{proj}} + b$$\n",
    "\n",
    "Where:\n",
    "- $X_{\\text{image}} \\in \\mathbb{R}^{n \\times d_{\\text{vision}}}$ \u2014 image patch tokens from ViT\n",
    "- $W_{\\text{proj}} \\in \\mathbb{R}^{d_{\\text{vision}} \\times d_{\\text{text}}}$ \u2014 learned projection matrix\n",
    "- $Z_{\\text{image}} \\in \\mathbb{R}^{n \\times d_{\\text{text}}}$ \u2014 projected tokens, now compatible with text\n",
    "\n",
    "**Numerical example:** Suppose $d_{\\text{vision}} = 4$ and $d_{\\text{text}} = 3$:\n",
    "\n",
    "$$X = [1, 2, 3, 4], \\quad W = \\begin{bmatrix} 0.1 & 0.2 & 0.3 \\\\ 0.4 & 0.5 & 0.6 \\\\ 0.7 & 0.8 & 0.9 \\\\ 1.0 & 1.1 & 1.2 \\end{bmatrix}$$\n",
    "\n",
    "$$Z = [1, 2, 3, 4] \\cdot W = [7.0, 8.0, 9.0]$$\n",
    "\n",
    "Our 4-dim image token is now a 3-dim token compatible with text. This is exactly what we want.\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Instead of one big attention operation, we split into $h$ heads:\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W_O$$\n",
    "\n",
    "Where each head is:\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(Q W_Q^i, K W_K^i, V W_V^i)$$\n",
    "\n",
    "Each head has its own projection matrices of size $d_k = d_{\\text{model}} / h$.\n",
    "\n",
    "**Example:** With $d_{\\text{model}} = 8$ and $h = 2$ heads:\n",
    "- Each head works with $d_k = 4$\n",
    "- Head 1 uses the first 4 dims: $Q_1 = Q \\cdot W_Q^1$ where $W_Q^1 \\in \\mathbb{R}^{8 \\times 4}$\n",
    "- Head 2 uses different projections: $Q_2 = Q \\cdot W_Q^2$ where $W_Q^2 \\in \\mathbb{R}^{8 \\times 4}$\n",
    "- Outputs are concatenated: $[h_1 \\| h_2] \\in \\mathbb{R}^{n \\times 8}$\n",
    "- Then projected: $\\text{output} = [h_1 \\| h_2] \\cdot W_O$\n",
    "\n",
    "## 4. Let's Build It \u2014 Component by Component\n",
    "\n",
    "### 4.1 Token Alignment Module"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAligner(nn.Module):\n",
    "    \"\"\"\n",
    "    Projects image tokens from vision space to language space.\n",
    "    Supports: linear, mlp, or identity (same dimension).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_vision, d_text, method=\"linear\"):\n",
    "        super().__init__()\n",
    "        self.method = method\n",
    "\n",
    "        if method == \"linear\":\n",
    "            self.proj = nn.Linear(d_vision, d_text)\n",
    "        elif method == \"mlp\":\n",
    "            self.proj = nn.Sequential(\n",
    "                nn.Linear(d_vision, d_text),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(d_text, d_text)\n",
    "            )\n",
    "        else:\n",
    "            assert d_vision == d_text, \"Identity requires same dimensions\"\n",
    "            self.proj = nn.Identity()\n",
    "\n",
    "    def forward(self, image_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_tokens: (n_patches, d_vision)\n",
    "        Returns:\n",
    "            projected:    (n_patches, d_text)\n",
    "        \"\"\"\n",
    "        return self.proj(image_tokens)\n",
    "\n",
    "# Test: project from 768-dim vision to 512-dim text\n",
    "aligner = TokenAligner(d_vision=768, d_text=512, method=\"linear\")\n",
    "\n",
    "dummy_image = torch.randn(16, 768)  # 16 patches, 768-dim (like ViT-B)\n",
    "projected = aligner(dummy_image)\n",
    "\n",
    "print(f\"Input:  {dummy_image.shape} (16 patches, 768-dim vision space)\")\n",
    "print(f\"Output: {projected.shape} (16 patches, 512-dim text space)\")\n",
    "print(f\"\\nProjection matrix shape: {aligner.proj.weight.shape}\")\n",
    "print(f\"  That's {768 * 512:,} learnable parameters just for alignment!\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: how does projection change the token distributions?\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Before projection\n",
    "axes[0].hist(dummy_image.flatten().detach().numpy(), bins=50, alpha=0.7, color='#3498db')\n",
    "axes[0].set_title('Image Token Values (768-dim)', fontsize=12)\n",
    "axes[0].set_xlabel('Value')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# After projection\n",
    "axes[1].hist(projected.flatten().detach().numpy(), bins=50, alpha=0.7, color='#e74c3c')\n",
    "axes[1].set_title('Projected Token Values (512-dim)', fontsize=12)\n",
    "axes[1].set_xlabel('Value')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "plt.suptitle('Token Distribution Before and After Projection', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The projection changes the distribution \u2014 the model learns\")\n",
    "print(\"to map vision features into a space that language tokens can use.\")"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Multi-Head Cross-Attention"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head cross-attention.\n",
    "    Q from text, K/V from image, with h parallel attention heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # Q from text, K/V from image\n",
    "        self.W_Q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_O = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, text_tokens, image_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_tokens:  (n_text, d_model)\n",
    "            image_tokens: (n_image, d_model)\n",
    "        Returns:\n",
    "            output: (n_text, d_model)\n",
    "            attn_weights: (num_heads, n_text, n_image)\n",
    "        \"\"\"\n",
    "        n_text = text_tokens.size(0)\n",
    "        n_image = image_tokens.size(0)\n",
    "\n",
    "        # Project\n",
    "        Q = self.W_Q(text_tokens)   # (n_text, d_model)\n",
    "        K = self.W_K(image_tokens)  # (n_image, d_model)\n",
    "        V = self.W_V(image_tokens)  # (n_image, d_model)\n",
    "\n",
    "        # Split into heads: (n, d_model) -> (num_heads, n, d_k)\n",
    "        Q = Q.view(n_text, self.num_heads, self.d_k).transpose(0, 1)\n",
    "        K = K.view(n_image, self.num_heads, self.d_k).transpose(0, 1)\n",
    "        V = V.view(n_image, self.num_heads, self.d_k).transpose(0, 1)\n",
    "\n",
    "        # Scaled dot-product attention per head\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attn_weights = F.softmax(scores, dim=-1)  # (num_heads, n_text, n_image)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        context = torch.matmul(attn_weights, V)  # (num_heads, n_text, d_k)\n",
    "\n",
    "        # Concatenate heads: (num_heads, n_text, d_k) -> (n_text, d_model)\n",
    "        context = context.transpose(0, 1).contiguous().view(n_text, self.d_model)\n",
    "\n",
    "        # Output projection\n",
    "        output = self.W_O(context)\n",
    "\n",
    "        return output, attn_weights\n",
    "\n",
    "# Test\n",
    "d_model = 16\n",
    "num_heads = 4\n",
    "\n",
    "mhca = MultiHeadCrossAttention(d_model, num_heads)\n",
    "\n",
    "text = torch.randn(3, d_model)    # 3 text tokens\n",
    "image = torch.randn(9, d_model)   # 9 image patches (3x3)\n",
    "\n",
    "output, attn = mhca(text, image)\n",
    "print(f\"Output shape: {output.shape} \u2014 same as text input (3 tokens, d_model=16)\")\n",
    "print(f\"Attention shape: {attn.shape} \u2014 {num_heads} heads, each 3x9\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn \u2014 Combine Alignment + Multi-Head Attention"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO 2 ============\n",
    "# Build a complete cross-attention pipeline:\n",
    "# 1. Align image tokens from d_vision to d_text\n",
    "# 2. Run multi-head cross-attention\n",
    "#\n",
    "# Fill in the forward method below.\n",
    "# ================================\n",
    "\n",
    "class AlignedCrossAttention(nn.Module):\n",
    "    \"\"\"Full pipeline: token alignment + multi-head cross-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, d_vision, d_text, num_heads, align_method=\"linear\"):\n",
    "        super().__init__()\n",
    "        self.aligner = TokenAligner(d_vision, d_text, method=align_method)\n",
    "        self.cross_attn = MultiHeadCrossAttention(d_text, num_heads)\n",
    "\n",
    "    def forward(self, text_tokens, image_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_tokens:  (n_text, d_text) \u2014 already in text space\n",
    "            image_tokens: (n_image, d_vision) \u2014 in vision space (different dim!)\n",
    "        Returns:\n",
    "            output: (n_text, d_text)\n",
    "            attn_weights: (num_heads, n_text, n_image)\n",
    "        \"\"\"\n",
    "        # ============ YOUR CODE HERE ============\n",
    "        # Step 1: Align image tokens to text space\n",
    "        aligned_image = ???\n",
    "\n",
    "        # Step 2: Cross-attention with aligned tokens\n",
    "        output, attn_weights = ???\n",
    "        # ========================================\n",
    "\n",
    "        return output, attn_weights"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification for TODO 2\n",
    "d_vision = 768\n",
    "d_text = 256\n",
    "num_heads = 4\n",
    "\n",
    "model = AlignedCrossAttention(d_vision, d_text, num_heads)\n",
    "\n",
    "text_in = torch.randn(5, d_text)     # 5 text tokens, 256-dim\n",
    "image_in = torch.randn(16, d_vision)  # 16 patches, 768-dim\n",
    "\n",
    "try:\n",
    "    out, attn = model(text_in, image_in)\n",
    "    assert out.shape == (5, d_text), f\"Expected (5, {d_text}), got {out.shape}\"\n",
    "    assert attn.shape == (num_heads, 5, 16), f\"Expected ({num_heads}, 5, 16), got {attn.shape}\"\n",
    "    print(\"Correct! Your aligned cross-attention pipeline works.\")\n",
    "    print(f\"  Input:  text {text_in.shape}, image {image_in.shape}\")\n",
    "    print(f\"  Output: {out.shape}\")\n",
    "    print(f\"  Attention: {attn.shape}\")\n",
    "    print(f\"\\nDimension mismatch ({d_vision} vs {d_text}) handled by alignment!\")\n",
    "except Exception as e:\n",
    "    print(f\"Not quite: {e}\")\n",
    "    print(\"Hint: aligned_image = self.aligner(image_tokens)\")\n",
    "    print(\"      output, attn_weights = self.cross_attn(text_tokens, aligned_image)\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together \u2014 Head Specialization Visualization\n",
    "\n",
    "Now let us see the most compelling property of multi-head attention: **different heads attend to different things**."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a structured scenario to see head specialization\n",
    "torch.manual_seed(7)\n",
    "\n",
    "d_model = 32\n",
    "num_heads = 4\n",
    "n_text = 4\n",
    "n_image = 9  # 3x3 grid\n",
    "\n",
    "text_labels = [\"A\", \"dog\", \"catches\", \"frisbee\"]\n",
    "patch_labels = [\"sky-L\", \"sky-C\", \"sky-R\",\n",
    "                \"dog-L\", \"dog-C\", \"dog-R\",\n",
    "                \"gnd-L\", \"frisbee\", \"gnd-R\"]\n",
    "\n",
    "# Create embeddings with structure\n",
    "text_emb = torch.randn(n_text, d_model)\n",
    "image_emb = torch.randn(n_image, d_model)\n",
    "\n",
    "# Bias patches to be semantically related to specific text tokens\n",
    "image_emb[3] += text_emb[1] * 0.8   # dog-L \u2190 dog\n",
    "image_emb[4] += text_emb[1] * 1.0   # dog-C \u2190 dog (strongest)\n",
    "image_emb[5] += text_emb[1] * 0.6   # dog-R \u2190 dog\n",
    "image_emb[7] += text_emb[3] * 1.0   # frisbee patch \u2190 frisbee token\n",
    "image_emb[4] += text_emb[2] * 0.4   # dog-C also relevant to \"catches\"\n",
    "\n",
    "mhca = MultiHeadCrossAttention(d_model, num_heads)\n",
    "output, attn_weights = mhca(text_emb, image_emb)"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all 4 heads for the \"dog\" token\n",
    "fig, axes = plt.subplots(1, num_heads, figsize=(16, 4))\n",
    "\n",
    "for h in range(num_heads):\n",
    "    attn = attn_weights[h, 1].detach().numpy().reshape(3, 3)  # \"dog\" token\n",
    "    axes[h].imshow(attn, cmap='YlOrRd', vmin=0, vmax=attn_weights[:, 1].max().item())\n",
    "\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            axes[h].text(c, r, f'{attn[r,c]:.2f}', ha='center', va='center',\n",
    "                        fontsize=10, color='white' if attn[r,c] > 0.13 else 'black')\n",
    "\n",
    "    axes[h].set_title(f'Head {h+1}', fontsize=13, fontweight='bold')\n",
    "    axes[h].set_xticks([])\n",
    "    axes[h].set_yticks([])\n",
    "\n",
    "plt.suptitle('Multi-Head Attention for Text Token \"dog\" \u2014 Each Head Has Its Own Focus',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: different heads attend to different parts of the image!\")\n",
    "print(\"This is the power of multi-head attention \u2014 parallel, diverse perspectives.\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full visualization: all text tokens, all heads\n",
    "fig, axes = plt.subplots(n_text, num_heads, figsize=(4*num_heads, 4*n_text))\n",
    "\n",
    "for t in range(n_text):\n",
    "    for h in range(num_heads):\n",
    "        attn = attn_weights[h, t].detach().numpy().reshape(3, 3)\n",
    "        ax = axes[t, h]\n",
    "        ax.imshow(attn, cmap='YlOrRd', vmin=0)\n",
    "\n",
    "        for r in range(3):\n",
    "            for c in range(3):\n",
    "                ax.text(c, r, f'{attn[r,c]:.2f}', ha='center', va='center',\n",
    "                        fontsize=8, color='white' if attn[r,c] > 0.13 else 'black')\n",
    "\n",
    "        if t == 0:\n",
    "            ax.set_title(f'Head {h+1}', fontsize=12, fontweight='bold')\n",
    "        if h == 0:\n",
    "            ax.set_ylabel(f'\"{text_labels[t]}\"', fontsize=12, fontweight='bold',\n",
    "                          rotation=0, labelpad=50)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "plt.suptitle('Multi-Head Cross-Attention: Every Text Token x Every Head',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Results \u2014 Does Alignment Actually Help?\n",
    "\n",
    "Let us run a simple experiment: compare cross-attention with and without proper alignment."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: mismatched vs aligned dimensions\n",
    "d_vision = 64\n",
    "d_text = 32\n",
    "\n",
    "torch.manual_seed(42)\n",
    "text_tok = torch.randn(4, d_text)\n",
    "image_tok = torch.randn(9, d_vision)\n",
    "\n",
    "# Without alignment: pad image to match text dim (naive approach)\n",
    "image_padded = torch.zeros(9, d_text)\n",
    "image_padded[:, :d_text] = image_tok[:, :d_text]  # truncate\n",
    "\n",
    "# With alignment: learned projection\n",
    "aligner = TokenAligner(d_vision, d_text, method=\"linear\")\n",
    "image_aligned = aligner(image_tok)\n",
    "\n",
    "# Compare information content\n",
    "print(\"Information comparison:\")\n",
    "print(f\"  Original image tokens \u2014 std: {image_tok.std():.4f}, mean: {image_tok.mean():.4f}\")\n",
    "print(f\"  Truncated (naive)    \u2014 std: {image_padded.std():.4f}, mean: {image_padded.mean():.4f}\")\n",
    "print(f\"  Aligned (learned)    \u2014 std: {image_aligned.std():.4f}, mean: {image_aligned.mean():.4f}\")\n",
    "print(f\"\\nTruncation loses {d_vision - d_text} dimensions of information!\")\n",
    "print(f\"Learned alignment preserves ALL {d_vision} dimensions via a {d_vision}x{d_text} matrix.\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output \u2014 Comparing Linear vs MLP Alignment"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare linear vs MLP alignment\n",
    "torch.manual_seed(42)\n",
    "\n",
    "linear_aligner = TokenAligner(d_vision, d_text, method=\"linear\")\n",
    "mlp_aligner = TokenAligner(d_vision, d_text, method=\"mlp\")\n",
    "\n",
    "image_tok = torch.randn(9, d_vision)\n",
    "\n",
    "linear_out = linear_aligner(image_tok)\n",
    "mlp_out = mlp_aligner(image_tok)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Original\n",
    "axes[0].imshow(image_tok.detach().numpy(), aspect='auto', cmap='coolwarm')\n",
    "axes[0].set_title(f'Original ({d_vision}-dim)', fontsize=12)\n",
    "axes[0].set_ylabel('Patch Index')\n",
    "axes[0].set_xlabel('Dimension')\n",
    "\n",
    "# Linear projection\n",
    "axes[1].imshow(linear_out.detach().numpy(), aspect='auto', cmap='coolwarm')\n",
    "axes[1].set_title(f'Linear Projection ({d_text}-dim)', fontsize=12)\n",
    "axes[1].set_xlabel('Dimension')\n",
    "\n",
    "# MLP projection\n",
    "axes[2].imshow(mlp_out.detach().numpy(), aspect='auto', cmap='coolwarm')\n",
    "axes[2].set_title(f'MLP Projection ({d_text}-dim)', fontsize=12)\n",
    "axes[2].set_xlabel('Dimension')\n",
    "\n",
    "plt.suptitle('Token Representations: Original vs Projected', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key difference: MLP introduces a nonlinearity (GELU),\")\n",
    "print(\"which lets it learn more complex mappings between spaces.\")\n",
    "print(\"Linear is simpler but works surprisingly well (as shown in LLaVA).\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Key Takeaways\n",
    "1. **Token alignment** bridges the dimensionality gap between vision (768-dim) and language (4096-dim) spaces via learned projections\n",
    "2. **Linear projection** is the simplest approach (used in LLaVA) and works remarkably well\n",
    "3. **MLP projection** adds a nonlinearity for more expressive mappings\n",
    "4. **Multi-head attention** runs $h$ parallel attention operations, each with $d_k = d_{\\text{model}} / h$ dimensions\n",
    "5. **Different heads specialize** \u2014 they attend to different image regions for the same text query\n",
    "\n",
    "### Reflection Questions\n",
    "- We used $d_{\\text{model}} / h$ for each head dimension. What happens if we use a different split (e.g., more dimensions for some heads)?\n",
    "- The Q-Former (from BLIP-2) uses learned query tokens to cross-attend to image patches. How is this different from linear projection?\n",
    "- If you increased the number of heads from 4 to 16, what would change about the attention patterns?\n",
    "\n",
    "### Next Steps\n",
    "In the final notebook, we will combine everything into a **working mini Vision-Language Model** that processes real-looking images and text, producing interpretable attention visualizations."
   ],
   "id": "cell_19"
  }
 ]
}