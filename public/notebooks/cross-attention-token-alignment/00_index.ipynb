{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Cross-Attention & Token Alignment \u2014 Series Index"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Attention & Token Alignment \u2014 Notebook Series Index\n",
    "\n",
    "**Vizuara AI** | VLMs from Scratch Course\n",
    "\n",
    "This series of 3 notebooks builds the core mechanism that bridges vision and language in modern AI models: **cross-attention** and **token alignment**.\n",
    "\n",
    "## Learning Path\n",
    "\n",
    "### Notebook 1: Self-Attention and Cross-Attention from Scratch\n",
    "Build the scaled dot-product attention mechanism from first principles, then extend it to cross-attention where queries come from text and keys/values come from images. You will implement both mechanisms and visualize how text tokens attend to image patches.\n",
    "\n",
    "**Key concepts:** Q/K/V projections, scaled dot-product, softmax, cross-modal attention\n",
    "\n",
    "### Notebook 2: Token Alignment and Multi-Head Cross-Attention\n",
    "Solve the dimensionality mismatch between vision and language tokens with projection layers, then implement multi-head cross-attention where different heads attend to different image regions simultaneously.\n",
    "\n",
    "**Key concepts:** linear projection, MLP projection, multi-head attention, head specialization\n",
    "\n",
    "### Notebook 3: Building a Mini Vision-Language Model\n",
    "Combine everything into a working mini VLM that takes image patches and text tokens, processes them through a full transformer block with cross-attention, and produces enriched text representations. Train on a toy task and visualize learned attention patterns.\n",
    "\n",
    "**Key concepts:** VLM block, residual connections, layer normalization, visual grounding\n",
    "\n",
    "## Prerequisites\n",
    "- Basic PyTorch (tensors, nn.Module, training loops)\n",
    "- Matrix multiplication and softmax\n",
    "- Understanding of transformer self-attention (reviewed in Notebook 1)\n",
    "\n",
    "## How to Use\n",
    "Run the notebooks in order (1 \u2192 2 \u2192 3). Each builds on the previous one. Complete the TODO sections before checking the solutions."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ready to start! Open Notebook 1.\")"
   ],
   "id": "cell_2"
  }
 ]
}