{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Self-Attention and Cross-Attention from Scratch \u2014 Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\u2705 GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\u26a0\ufe0f No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Python {sys.version.split()[0]}\")\n",
    "print(f\"\ud83d\udd25 PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"\ud83c\udfb2 Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention and Cross-Attention from Scratch\n",
    "\n",
    "**Vizuara AI** | Cross-Attention & Token Alignment Series \u2014 Notebook 1 of 3\n",
    "\n",
    "In this notebook, we will build the two most important attention mechanisms in modern AI from scratch: **self-attention** (tokens attending to themselves) and **cross-attention** (tokens from one modality querying another).\n",
    "\n",
    "By the end, you will have a working implementation of both mechanisms and see exactly how cross-attention allows text tokens to \"look at\" image patches."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU check and setup\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Imagine you are building a system that can look at a photograph and describe what it sees: \"A dog catches a frisbee in a park.\"\n",
    "\n",
    "When the model generates the word \"frisbee,\" it needs to know **where** in the image the frisbee is. It cannot just look at the entire image uniformly \u2014 it needs to focus on the right patch.\n",
    "\n",
    "**Self-attention** lets tokens within the same sequence communicate. It is the backbone of transformers.\n",
    "\n",
    "**Cross-attention** takes this further: it lets tokens from one sequence (text) query tokens from a completely different sequence (image patches). This is the mechanism that bridges vision and language.\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Implement scaled dot-product attention from scratch\n",
    "- Understand Q, K, V projections intuitively\n",
    "- Extend self-attention to cross-attention\n",
    "- Visualize what each text token \"looks at\" in an image\n",
    "\n",
    "## 2. Building Intuition\n",
    "\n",
    "### The Library Analogy\n",
    "\n",
    "Think of attention like searching in a library:\n",
    "\n",
    "- **Query (Q):** Your question \u2014 \"I want to know about frisbees\"\n",
    "- **Key (K):** The index card for each book \u2014 tells you what each book is about\n",
    "- **Value (V):** The actual content of each book\n",
    "\n",
    "The process:\n",
    "1. Compare your question (Q) with every index card (K) using a dot product\n",
    "2. The most relevant books get the highest scores\n",
    "3. You read a weighted combination of all books (V), weighted by relevance\n",
    "\n",
    "In **self-attention**, you are a book asking questions to other books on the same shelf.\n",
    "In **cross-attention**, you are a text token asking questions to image patches on a different shelf.\n",
    "\n",
    "### Think About This\n",
    "\n",
    "Before we implement anything, ask yourself:\n",
    "- Why do we need THREE separate matrices (Q, K, V) instead of just one?\n",
    "- What would happen if Q and K were the same \u2014 could we still have asymmetric attention?\n",
    "\n",
    "## 3. The Mathematics\n",
    "\n",
    "### Scaled Dot-Product Attention\n",
    "\n",
    "The attention function computes:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "Let us break down what each part means computationally:\n",
    "\n",
    "1. $QK^\\top$ \u2014 Compute similarity scores between every query and every key (dot product)\n",
    "2. $\\frac{1}{\\sqrt{d_k}}$ \u2014 Scale down the scores so softmax does not saturate (large values push softmax to 0/1)\n",
    "3. $\\text{softmax}$ \u2014 Convert raw scores to probabilities (each row sums to 1)\n",
    "4. Multiply by $V$ \u2014 Weighted sum of values, where weights are the attention probabilities\n",
    "\n",
    "### Why the Scaling Factor?\n",
    "\n",
    "Without $\\sqrt{d_k}$, when $d_k$ is large (e.g., 512), the dot products grow large, pushing softmax outputs toward 0 and 1 (the extreme ends). This kills gradients during training.\n",
    "\n",
    "**Numerical example:** If $d_k = 3$, then $\\sqrt{d_k} \\approx 1.73$.\n",
    "\n",
    "If our raw score is 3.0:\n",
    "- Without scaling: $\\text{softmax}(3.0) \\approx 0.95$ (very peaked)\n",
    "- With scaling: $\\text{softmax}(3.0/1.73) \\approx \\text{softmax}(1.73) \\approx 0.85$ (gentler)\n",
    "\n",
    "## 4. Let's Build It \u2014 Component by Component\n",
    "\n",
    "### 4.1 Step 1: Manual Dot-Product Attention\n",
    "\n",
    "Let us start by implementing attention without any learned parameters \u2014 just raw matrix operations."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention manually.\n",
    "\n",
    "    Args:\n",
    "        Q: Query tensor of shape (seq_len_q, d_k)\n",
    "        K: Key tensor of shape (seq_len_k, d_k)\n",
    "        V: Value tensor of shape (seq_len_k, d_v)\n",
    "\n",
    "    Returns:\n",
    "        output: (seq_len_q, d_v) \u2014 weighted combination of values\n",
    "        weights: (seq_len_q, seq_len_k) \u2014 attention weight matrix\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "\n",
    "    # Step 1: Compute raw scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))  # (seq_q, seq_k)\n",
    "    print(f\"  Raw scores shape: {scores.shape}\")\n",
    "    print(f\"  Raw scores:\\n{scores}\")\n",
    "\n",
    "    # Step 2: Scale\n",
    "    scores_scaled = scores / math.sqrt(d_k)\n",
    "    print(f\"\\n  Scaled scores:\\n{scores_scaled}\")\n",
    "\n",
    "    # Step 3: Softmax (row-wise)\n",
    "    weights = F.softmax(scores_scaled, dim=-1)\n",
    "    print(f\"\\n  Attention weights (each row sums to 1):\\n{weights}\")\n",
    "    print(f\"  Row sums: {weights.sum(dim=-1)}\")\n",
    "\n",
    "    # Step 4: Weighted sum of values\n",
    "    output = torch.matmul(weights, V)  # (seq_q, d_v)\n",
    "\n",
    "    return output, weights"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with the exact example from the article\n",
    "Q = torch.tensor([[1.0, 0.0, 1.0],\n",
    "                   [0.0, 1.0, 0.0]])\n",
    "\n",
    "K = torch.tensor([[1.0, 1.0, 0.0],\n",
    "                   [0.0, 0.0, 1.0]])\n",
    "\n",
    "V = torch.tensor([[1.0, 2.0],\n",
    "                   [3.0, 4.0]])\n",
    "\n",
    "print(\"=== Self-Attention Example ===\")\n",
    "print(f\"Q shape: {Q.shape} (2 tokens, d_k=3)\")\n",
    "print(f\"K shape: {K.shape} (2 tokens, d_k=3)\")\n",
    "print(f\"V shape: {V.shape} (2 tokens, d_v=2)\")\n",
    "print()\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "print(f\"\\n  Output:\\n{output}\")\n",
    "print(f\"  Output shape: {output.shape}\")"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint: Understanding the Output\n",
    "\n",
    "Token 1 has attention weights $[0.5, 0.5]$ \u2014 it attends equally to both tokens. So its output is the average: $(1+3)/2 = 2.0$ and $(2+4)/2 = 3.0$.\n",
    "\n",
    "Token 2 has weights $[0.644, 0.356]$ \u2014 it attends more to token 1. So its output leans toward token 1's value.\n",
    "\n",
    "This is exactly what we want. Each token produces a context-aware representation by gathering information from the others.\n",
    "\n",
    "### 4.2 Step 2: Self-Attention with Learned Projections\n",
    "\n",
    "In real transformers, Q, K, V are not given directly \u2014 they are learned projections of the input."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    \"\"\"Self-attention: Q, K, V all come from the SAME input.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_k):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "        # All three projections take the same input\n",
    "        self.W_Q = torch.nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_K = torch.nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_V = torch.nn.Linear(d_model, d_k, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input sequence (seq_len, d_model)\n",
    "        Returns:\n",
    "            output: (seq_len, d_k)\n",
    "            weights: (seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # All projections from the SAME input x\n",
    "        Q = self.W_Q(x)  # (seq_len, d_k)\n",
    "        K = self.W_K(x)  # (seq_len, d_k)\n",
    "        V = self.W_V(x)  # (seq_len, d_k)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(weights, V)\n",
    "\n",
    "        return output, weights"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test self-attention\n",
    "d_model = 8   # input embedding dimension\n",
    "d_k = 4       # attention dimension\n",
    "seq_len = 5   # 5 tokens\n",
    "\n",
    "self_attn = SelfAttention(d_model, d_k)\n",
    "\n",
    "# Random input: 5 tokens, each 8-dimensional\n",
    "x = torch.randn(seq_len, d_model)\n",
    "print(f\"Input x shape: {x.shape} (5 tokens, d_model=8)\")\n",
    "\n",
    "output, weights = self_attn(x)\n",
    "print(f\"Output shape: {output.shape} (5 tokens, d_k=4)\")\n",
    "print(f\"Weights shape: {weights.shape} (5x5 \u2014 each token attends to all 5)\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize self-attention weights\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(weights.detach().numpy(), cmap='Blues', vmin=0, vmax=1)\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.title('Self-Attention Weights (5 tokens)')\n",
    "plt.xticks(range(seq_len), [f'Token {i}' for i in range(seq_len)], rotation=45)\n",
    "plt.yticks(range(seq_len), [f'Token {i}' for i in range(seq_len)])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each row sums to 1:\", weights.sum(dim=-1).detach().numpy())"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Turn \u2014 From Self to Cross-Attention\n",
    "\n",
    "Now comes the key insight. In self-attention, Q, K, and V all come from the **same** input $x$.\n",
    "\n",
    "In cross-attention, we split the sources:\n",
    "- **Q comes from the text** (the decoder side)\n",
    "- **K and V come from the image** (the encoder side)\n",
    "\n",
    "This is the mechanism that lets text tokens \"look at\" image patches."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ TODO 1 ============\n",
    "# Implement CrossAttention by modifying the SelfAttention class.\n",
    "# The key change: Q comes from text_tokens, K and V come from image_tokens.\n",
    "#\n",
    "# Hints:\n",
    "# - W_Q projects text_tokens (not image_tokens!)\n",
    "# - W_K and W_V project image_tokens\n",
    "# - The output shape should be (n_text, d_k), not (n_image, d_k)\n",
    "# ================================\n",
    "\n",
    "class CrossAttention(torch.nn.Module):\n",
    "    \"\"\"Cross-attention: Q from text, K and V from image.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_k):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "        self.W_Q = torch.nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_K = torch.nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_V = torch.nn.Linear(d_model, d_k, bias=False)\n",
    "\n",
    "    def forward(self, text_tokens, image_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_tokens:  (n_text, d_model)\n",
    "            image_tokens: (n_image, d_model)\n",
    "        Returns:\n",
    "            output:  (n_text, d_k)\n",
    "            weights: (n_text, n_image)\n",
    "        \"\"\"\n",
    "        # ============ YOUR CODE HERE ============\n",
    "        Q = ???   # Project text_tokens to queries\n",
    "        K = ???   # Project image_tokens to keys\n",
    "        V = ???   # Project image_tokens to values\n",
    "\n",
    "        scores = ???   # Compute QK^T / sqrt(d_k)\n",
    "        weights = ???  # Apply softmax\n",
    "        output = ???   # Multiply by V\n",
    "        # ========================================\n",
    "\n",
    "        return output, weights"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification cell for TODO 1\n",
    "# Run this to check your CrossAttention implementation\n",
    "\n",
    "# Create a reference implementation\n",
    "class CrossAttentionRef(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_k):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "        self.W_Q = torch.nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_K = torch.nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_V = torch.nn.Linear(d_model, d_k, bias=False)\n",
    "\n",
    "    def forward(self, text_tokens, image_tokens):\n",
    "        Q = self.W_Q(text_tokens)\n",
    "        K = self.W_K(image_tokens)\n",
    "        V = self.W_V(image_tokens)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(weights, V)\n",
    "        return output, weights\n",
    "\n",
    "torch.manual_seed(42)\n",
    "ref = CrossAttentionRef(8, 4)\n",
    "torch.manual_seed(42)\n",
    "student = CrossAttention(8, 4)\n",
    "\n",
    "# Copy weights to ensure same initialization\n",
    "student.load_state_dict(ref.state_dict())\n",
    "\n",
    "text = torch.randn(3, 8)   # 3 text tokens\n",
    "image = torch.randn(6, 8)  # 6 image patches\n",
    "\n",
    "ref_out, ref_w = ref(text, image)\n",
    "try:\n",
    "    stu_out, stu_w = student(text, image)\n",
    "    assert stu_out.shape == (3, 4), f\"Output shape should be (3, 4), got {stu_out.shape}\"\n",
    "    assert stu_w.shape == (3, 6), f\"Weights shape should be (3, 6), got {stu_w.shape}\"\n",
    "    assert torch.allclose(stu_out, ref_out, atol=1e-5), \"Output values don't match\"\n",
    "    assert torch.allclose(stu_w, ref_w, atol=1e-5), \"Weights don't match\"\n",
    "    print(\"Correct! Your cross-attention implementation works perfectly.\")\n",
    "    print(f\"  Output shape: {stu_out.shape} (3 text tokens, d_k=4)\")\n",
    "    print(f\"  Weights shape: {stu_w.shape} (3 text tokens attending to 6 image patches)\")\n",
    "except Exception as e:\n",
    "    print(f\"Not quite: {e}\")\n",
    "    print(\"Hint: Q = self.W_Q(text_tokens), K = self.W_K(image_tokens), V = self.W_V(image_tokens)\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together\n",
    "\n",
    "Let us now use cross-attention on a meaningful example. We will simulate a simple image captioning scenario:\n",
    "- **Image:** 9 patches (3x3 grid), each representing a region of an image\n",
    "- **Text:** 4 tokens representing \"A dog catches frisbee\"\n",
    "\n",
    "We will visualize which image patches each text token attends to."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate image patches and text tokens\n",
    "n_text = 4     # \"A\", \"dog\", \"catches\", \"frisbee\"\n",
    "n_image = 9    # 3x3 grid of image patches\n",
    "d_model = 16\n",
    "\n",
    "# Create meaningful embeddings (not random \u2014 we want to see patterns)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Text token embeddings \u2014 make them somewhat distinct\n",
    "text_embeddings = torch.randn(n_text, d_model)\n",
    "text_labels = [\"A\", \"dog\", \"catches\", \"frisbee\"]\n",
    "\n",
    "# Image patch embeddings \u2014 create patches with different \"content\"\n",
    "# Patches 0-2: sky (top row), 3-5: dog area (middle), 6-8: ground (bottom)\n",
    "image_embeddings = torch.randn(n_image, d_model)\n",
    "# Make dog patches (3-5) similar to \"dog\" text token\n",
    "image_embeddings[3:6] += text_embeddings[1] * 0.5  # bias toward \"dog\"\n",
    "# Make one patch (4) similar to \"catches\" and \"frisbee\"\n",
    "image_embeddings[4] += text_embeddings[2] * 0.3  # bias toward \"catches\"\n",
    "image_embeddings[7] += text_embeddings[3] * 0.5  # bias toward \"frisbee\"\n",
    "\n",
    "patch_labels = [\"sky-L\", \"sky-C\", \"sky-R\",\n",
    "                \"dog-L\", \"dog-C\", \"dog-R\",\n",
    "                \"gnd-L\", \"gnd-C\", \"gnd-R\"]\n",
    "\n",
    "print(f\"Text embeddings: {text_embeddings.shape}\")\n",
    "print(f\"Image embeddings: {image_embeddings.shape}\")"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross-attention\n",
    "torch.manual_seed(42)\n",
    "cross_attn = CrossAttentionRef(d_model, d_k=8)\n",
    "\n",
    "output, weights = cross_attn(text_embeddings, image_embeddings)\n",
    "print(f\"Output shape: {output.shape} \u2014 4 text tokens, enriched with image info\")\n",
    "print(f\"Weights shape: {weights.shape} \u2014 4 text tokens x 9 image patches\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-attention: which image patches does each text token attend to?\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for i, (ax, label) in enumerate(zip(axes, text_labels)):\n",
    "    # Reshape weights to 3x3 grid\n",
    "    attn_map = weights[i].detach().numpy().reshape(3, 3)\n",
    "    im = ax.imshow(attn_map, cmap='YlOrRd', vmin=0, vmax=weights.max().item())\n",
    "    ax.set_title(f'\"{label}\"', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Label each cell\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            ax.text(c, r, f'{attn_map[r, c]:.2f}', ha='center', va='center',\n",
    "                    fontsize=10, color='black' if attn_map[r, c] < 0.15 else 'white')\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.suptitle('Cross-Attention: Which Image Patches Does Each Text Token Attend To?',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Checkpoint\n",
    "\n",
    "Each heatmap shows how one text token distributes its attention across the 9 image patches.\n",
    "\n",
    "Notice:\n",
    "- **\"dog\"** should attend more strongly to the middle row (dog-L, dog-C, dog-R) because we biased those patches\n",
    "- **\"frisbee\"** should attend more to patch gnd-C (index 7) because we biased it\n",
    "- **\"A\"** and other function words tend to distribute attention more uniformly\n",
    "\n",
    "This is the core insight: cross-attention lets each word independently decide which part of the image to focus on.\n",
    "\n",
    "## 7. Training and Results\n",
    "\n",
    "Let us verify our understanding with a quantitative check: do the attention weights actually correlate with semantic similarity?"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare raw similarity between text and image tokens\n",
    "# (before learned projections \u2014 just cosine similarity)\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "print(\"Cosine similarity between text tokens and image patches:\\n\")\n",
    "print(f\"{'':>12}\", end=\"\")\n",
    "for pl in patch_labels:\n",
    "    print(f\"{pl:>8}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for i, tl in enumerate(text_labels):\n",
    "    print(f\"{tl:>12}\", end=\"\")\n",
    "    for j in range(n_image):\n",
    "        sim = cosine_similarity(text_embeddings[i].unsqueeze(0),\n",
    "                                image_embeddings[j].unsqueeze(0)).item()\n",
    "        print(f\"{sim:>8.3f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nNote: Patches we biased toward certain text tokens show higher similarity.\")\n",
    "print(\"Cross-attention learns to exploit (and refine) these similarities.\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Output\n",
    "\n",
    "Let us create a polished visualization combining the image grid with attention overlays."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final polished visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "text_colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "for idx, (ax, label, color) in enumerate(zip(axes.flat, text_labels, text_colors)):\n",
    "    attn_map = weights[idx].detach().numpy().reshape(3, 3)\n",
    "\n",
    "    # Create a pseudo-image background\n",
    "    bg = np.ones((3, 3, 3)) * 0.9  # light gray\n",
    "    # Overlay attention as color intensity\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            intensity = attn_map[r, c] / attn_map.max()\n",
    "            # Blend with the token's color\n",
    "            hex_color = color.lstrip('#')\n",
    "            rgb = np.array([int(hex_color[i:i+2], 16)/255 for i in (0, 2, 4)])\n",
    "            bg[r, c] = bg[r, c] * (1 - intensity) + rgb * intensity\n",
    "\n",
    "    ax.imshow(bg, interpolation='nearest')\n",
    "\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            ax.text(c, r, f'{attn_map[r, c]:.3f}', ha='center', va='center',\n",
    "                    fontsize=12, fontweight='bold',\n",
    "                    color='white' if attn_map[r, c] > 0.12 else 'black')\n",
    "            ax.text(c, r + 0.35, patch_labels[r*3 + c], ha='center', va='center',\n",
    "                    fontsize=7, color='gray')\n",
    "\n",
    "    ax.set_title(f'Text Token: \"{label}\"', fontsize=14, fontweight='bold', color=color)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_frame_on(True)\n",
    "\n",
    "plt.suptitle('Cross-Attention Maps: Text Tokens Querying Image Patches',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey takeaway: Cross-attention lets each text token independently\")\n",
    "print(\"decide which image regions are most relevant to it.\")\n",
    "print(\"This is the mechanism that bridges vision and language.\")"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Key Takeaways\n",
    "1. **Self-attention** computes Q, K, V from the same input \u2014 tokens attend to each other within one sequence\n",
    "2. **Cross-attention** splits the sources: Q from text, K/V from image \u2014 tokens attend across modalities\n",
    "3. The **output always has the same length as Q** \u2014 so cross-attention produces text-aligned representations enriched with image info\n",
    "4. The **scaling factor** $1/\\sqrt{d_k}$ prevents softmax saturation\n",
    "\n",
    "### Reflection Questions\n",
    "- What would happen if we swapped the roles \u2014 Q from image, K/V from text? When might this be useful?\n",
    "- Why do we use separate W_K and W_V matrices instead of just one? (Hint: K determines *where* to look, V determines *what* information to retrieve)\n",
    "- How would attention weights change if we doubled $d_k$ but kept everything else the same?\n",
    "\n",
    "### Next Steps\n",
    "In the next notebook, we will:\n",
    "- Solve the **token alignment problem** \u2014 what happens when image and text tokens have different dimensions?\n",
    "- Implement **multi-head cross-attention** \u2014 running multiple attention operations in parallel"
   ],
   "id": "cell_21"
  }
 ]
}