{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Building a Complete ViT from Scratch ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1IgUoF-zZMZRikv9Wn-4nQSKVIcGjXn0Y\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Building a Complete Vision Transformer from Scratch\n",
    "\n",
    "*Part 3 of 3 in the Vizuara series on Vision Transformers from Scratch*\n",
    "\n",
    "*Estimated time: 60 minutes*\n",
    "\n",
    "In Notebook 1, we learned how to turn images into patch embeddings. In Notebook 2, we built a Transformer encoder from scratch. Now, in this final notebook, we assemble everything into a **complete, trainable Vision Transformer**, train it on CIFAR-10, and visualize what it learns.\n",
    "\n",
    "By the end of this notebook, you will have:\n",
    "- A fully functional ViT that classifies images\n",
    "- Training and validation curves showing it learning\n",
    "- Attention heatmaps revealing what the model focuses on\n",
    "- A position embedding similarity map showing learned 2D spatial structure"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ü§ñ AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** ‚Äî it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[üëâ Open AI Teaching Assistant](https://course-creator-brown.vercel.app/courses/vision-transformers-from-scratch/practice/3/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_why_it_matters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Why Does This Matter?\n",
    "\n",
    "We have built every piece of the Vision Transformer individually. Now comes the satisfying part: **putting it all together** and watching it learn.\n",
    "\n",
    "This is not a toy exercise. The model we build here is architecturally identical to the original ViT paper (Dosovitskiy et al., 2020) ‚Äî just smaller. The same principles scale to ViT-Base (86M parameters), ViT-Large (307M), and ViT-Huge (632M).\n",
    "\n",
    "Here is what we will accomplish in the next 60 minutes:\n",
    "1. **Assemble** the full ViT: patches + embeddings + encoder + classification head\n",
    "2. **Train** it on CIFAR-10 (50,000 images, 10 classes) in under 10 minutes\n",
    "3. **Evaluate** its predictions on unseen test images\n",
    "4. **Visualize** attention maps ‚Äî where does the model look when classifying a cat vs a truck?\n",
    "5. **Analyze** position embeddings ‚Äî does the model learn that patch 0 is top-left and patch 63 is bottom-right?\n",
    "\n",
    "Let us begin."
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Full Pipeline\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_full_pipeline.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_full_pipeline"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Building Intuition\n",
    "\n",
    "Before we write any code, let us walk through the complete ViT pipeline one more time.\n",
    "\n",
    "### The Full Pipeline\n",
    "\n",
    "```\n",
    "Image (32√ó32√ó3)\n",
    "    ‚Üì  Split into 4√ó4 patches\n",
    "64 Patches (each 4√ó4√ó3 = 48 values)\n",
    "    ‚Üì  Linear projection to D=192\n",
    "64 Patch Embeddings (each 192-dim)\n",
    "    ‚Üì  Prepend [CLS] token\n",
    "65 Tokens (each 192-dim)\n",
    "    ‚Üì  Add position embeddings\n",
    "65 Positioned Tokens\n",
    "    ‚Üì  Pass through 6 Transformer blocks\n",
    "65 Encoded Tokens\n",
    "    ‚Üì  Extract [CLS] token (index 0)\n",
    "1 Global Representation (192-dim)\n",
    "    ‚Üì  Layer Norm ‚Üí Linear(192, 10)\n",
    "10 Class Logits ‚Üí Prediction\n",
    "```\n",
    "\n",
    "Every component here was built in Notebooks 1 and 2. Today we connect the wires.\n",
    "\n",
    "Let us verify the dimensions quickly."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick dimension check for our ViT-Tiny pipeline\n",
    "img_size, patch_size, channels = 32, 4, 3\n",
    "embed_dim, depth, num_heads = 192, 6, 3\n",
    "num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "print(\"ViT-Tiny Pipeline Dimensions\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Input image:        {img_size}√ó{img_size}√ó{channels}\")\n",
    "print(f\"Patch size:         {patch_size}√ó{patch_size}\")\n",
    "print(f\"Num patches (N):    {num_patches}\")\n",
    "print(f\"Patch dim (P¬≤¬∑C):   {patch_size**2 * channels}\")\n",
    "print(f\"Embed dim (D):      {embed_dim}\")\n",
    "print(f\"Sequence length:    {num_patches + 1} (patches + CLS)\")\n",
    "print(f\"Encoder depth:      {depth} blocks\")\n",
    "print(f\"Attention heads:    {num_heads}\")\n",
    "print(f\"Head dim (D/H):     {embed_dim // num_heads}\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Cls Token\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_cls_token.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_cls_token"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why [CLS] for Classification?\n",
    "\n",
    "The `[CLS]` token is a clever design choice from BERT that ViT inherits. Think of it this way:\n",
    "\n",
    "**Analogy:** Imagine a meeting with 64 employees (the patch tokens). The `[CLS]` token is like a **manager** who starts the meeting knowing nothing. Over 6 rounds of discussion (Transformer layers), the manager listens to everyone through self-attention. By the end, the manager has synthesized information from all 64 employees into a single, comprehensive summary.\n",
    "\n",
    "That summary ‚Äî the final `[CLS]` representation ‚Äî is what we feed to the classification head.\n",
    "\n",
    "**Why not just average all patch tokens?** You could, and some models do (this is called **global average pooling**). But `[CLS]` is elegant: it gives the model a **dedicated slot** for the global representation without polluting individual patch representations. Each patch token can focus on encoding its local region, while `[CLS]` focuses on the big picture."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Variants And Data Hunger\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_variants_and_data_hunger.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_variants_and_data_hunger"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT Variants and Our \"Tiny\" Model\n",
    "\n",
    "The original paper introduced three sizes:\n",
    "\n",
    "| Variant   | Layers | Hidden Dim | Heads | Parameters |\n",
    "|-----------|--------|------------|-------|------------|\n",
    "| ViT-Base  | 12     | 768        | 12    | 86M        |\n",
    "| ViT-Large | 24     | 1024       | 16    | 307M       |\n",
    "| ViT-Huge  | 32     | 1280       | 16    | 632M       |\n",
    "\n",
    "For CIFAR-10 on a T4 GPU, we will use a **ViT-Tiny** configuration:\n",
    "\n",
    "| Setting    | Value |\n",
    "|------------|-------|\n",
    "| Layers     | 6     |\n",
    "| Hidden Dim | 192   |\n",
    "| Heads      | 3     |\n",
    "| MLP Ratio  | 4.0   |\n",
    "| Parameters | ~2.8M |\n",
    "\n",
    "This is small enough to train in minutes, but large enough to learn meaningful representations."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ViT variant sizes\n",
    "variants = {\n",
    "    'ViT-Tiny (ours)': {'layers': 6, 'dim': 192, 'heads': 3},\n",
    "    'ViT-Base':        {'layers': 12, 'dim': 768, 'heads': 12},\n",
    "    'ViT-Large':       {'layers': 24, 'dim': 1024, 'heads': 16},\n",
    "    'ViT-Huge':        {'layers': 32, 'dim': 1280, 'heads': 16},\n",
    "}\n",
    "\n",
    "print(f\"{'Variant':<18} {'Layers':>6} {'Dim':>6} {'Heads':>6} {'~Params':>10}\")\n",
    "print(\"-\" * 50)\n",
    "for name, v in variants.items():\n",
    "    # Rough param estimate: ~12¬∑D¬≤ per block + embeddings\n",
    "    approx_params = v['layers'] * 12 * v['dim']**2 + v['dim'] * 197\n",
    "    print(f\"{name:<18} {v['layers']:>6} {v['dim']:>6} {v['heads']:>6} {approx_params/1e6:>9.1f}M\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data Hunger Problem\n",
    "\n",
    "ViT has a well-known weakness: **it needs a lot of data**. Unlike CNNs, which have built-in inductive biases (locality, translation equivariance), ViT must learn spatial relationships entirely from data. The original ViT was pre-trained on JFT-300M (300 million images!) before fine-tuning.\n",
    "\n",
    "We only have 50,000 CIFAR-10 images. To compensate, we will use **data augmentation** ‚Äî random crops, flips, and normalization ‚Äî to artificially increase the effective dataset size.\n",
    "\n",
    "> **Think About This:** Our tiny ViT will have ~2.8M parameters. A ResNet-18 has 11M. Given that ViT lacks the locality inductive bias, what accuracy do you predict on CIFAR-10? Higher or lower than a CNN?"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make your prediction before training!\n",
    "# (No peeking ahead ‚Äî write down your guess)\n",
    "my_prediction = \"___\"  # Fill in: e.g., \"75%\", \"85%\", \"60%\"\n",
    "print(f\"My prediction for ViT-Tiny on CIFAR-10: {my_prediction}\")\n",
    "print(\"We will check this at the end of the notebook!\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Mathematics\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_mathematics.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_mathematics"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: The Mathematics\n",
    "\n",
    "Let us formalize the three mathematical ideas we need for the full model.\n",
    "\n",
    "### 3.1 The Classification Head\n",
    "\n",
    "After the Transformer encoder processes all tokens through $L$ layers, we extract the `[CLS]` token from the final layer:\n",
    "\n",
    "$$\\hat{y} = \\text{Linear}(\\text{LayerNorm}(\\mathbf{z}_L^0))$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{z}_L^0 \\in \\mathbb{R}^D$ is the `[CLS]` token at layer $L$ (the superscript 0 means it is the first token)\n",
    "- LayerNorm stabilizes the representation before the final projection\n",
    "- Linear projects from dimension $D$ to $C$ classes (192 ‚Üí 10 for CIFAR-10)\n",
    "\n",
    "This is deliberately simple. The entire \"intelligence\" of the model is in the Transformer encoder; the head just reads off the answer."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Cross-Entropy Loss\n",
    "\n",
    "We train with the standard classification loss:\n",
    "\n",
    "$$\\mathcal{L} = -\\sum_{c=1}^{C} y_c \\log(\\hat{y}_c)$$\n",
    "\n",
    "where $y_c$ is 1 for the true class and 0 elsewhere, and $\\hat{y}_c = \\text{softmax}(\\text{logits})_c$.\n",
    "\n",
    "In practice, since $y$ is one-hot, this simplifies to:\n",
    "\n",
    "$$\\mathcal{L} = -\\log(\\hat{y}_{true})$$\n",
    "\n",
    "The loss is just the **negative log probability** of the correct class. If the model assigns 90% probability to the right answer, the loss is $-\\log(0.9) = 0.105$. If it assigns only 10%, the loss is $-\\log(0.1) = 2.303$. The training process pushes all 2.8M parameters to minimize this value."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Parameter Count Breakdown\n",
    "\n",
    "Let us derive exactly where our ~2.8M parameters come from (ViT-Tiny with $D=192$, $L=6$, $H=3$, patch size $P=4$, CIFAR-10 images $32 \\times 32 \\times 3$):\n",
    "\n",
    "**Patch Embedding:**\n",
    "- Conv2d weight: $(P^2 \\cdot C) \\cdot D = (16 \\cdot 3) \\cdot 192 = 9{,}216$ weights + 192 bias = **9,408**\n",
    "\n",
    "**CLS Token + Position Embeddings:**\n",
    "- CLS token: $1 \\cdot D = 192$\n",
    "- Position embeddings: $(N+1) \\cdot D = 65 \\cdot 192 = 12{,}480$\n",
    "- Total: **12,672**\n",
    "\n",
    "**Per Transformer Block:**\n",
    "- Multi-head attention (Q, K, V projections + output): $4 \\cdot D^2 + 4D = 4 \\cdot 192^2 + 768 = 148{,}224$\n",
    "- MLP (two linear layers with expansion ratio 4): $2 \\cdot 4 \\cdot D^2 + (4D + D) = 8 \\cdot 192^2 + 960 = 295{,}680$\n",
    "- Two LayerNorms: $2 \\cdot 2D = 768$\n",
    "- Per block total: **~444,672**\n",
    "- Times 6 blocks: **~2,668,032**\n",
    "\n",
    "**Final LayerNorm + Classification Head:**\n",
    "- LayerNorm: $2D = 384$\n",
    "- Linear: $D \\cdot C + C = 192 \\cdot 10 + 10 = 1{,}930$\n",
    "- Total: **2,314**\n",
    "\n",
    "**Grand total: ~2,692,426 parameters** (approximately 2.7M)\n",
    "\n",
    "Every single one of these numbers will be adjusted during training to minimize the cross-entropy loss.\n",
    "\n",
    "Let us verify our hand calculation with code."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the parameter count breakdown\n",
    "D = 192          # embed_dim\n",
    "L = 6            # depth\n",
    "P = 4            # patch_size\n",
    "C_in = 3         # channels\n",
    "N = 64           # num_patches\n",
    "C_out = 10       # num_classes\n",
    "\n",
    "patch_embed_params = P * P * C_in * D + D  # Conv2d weight + bias\n",
    "cls_pos_params = D + (N + 1) * D           # CLS token + position embeddings\n",
    "per_block = 4 * D * D + 4 * D + 8 * D * D + (4 * D + D) + 2 * 2 * D  # attn + MLP + norms\n",
    "encoder_params = L * per_block\n",
    "head_params = 2 * D + D * C_out + C_out    # final LN + linear head\n",
    "\n",
    "total = patch_embed_params + cls_pos_params + encoder_params + head_params\n",
    "print(f\"Patch embedding:    {patch_embed_params:>10,}\")\n",
    "print(f\"CLS + Position:     {cls_pos_params:>10,}\")\n",
    "print(f\"Encoder ({L} blocks): {encoder_params:>10,}\")\n",
    "print(f\"Head:               {head_params:>10,}\")\n",
    "print(f\"{'':->35}\")\n",
    "print(f\"Estimated total:    {total:>10,}\")\n",
    "print(f\"\\n(Actual count will be close but may differ\")\n",
    "print(f\" slightly due to bias terms in nn.MultiheadAttention)\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Building Start\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_building_start.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_building_start"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Let Us Build It ‚Äî Component by Component\n",
    "\n",
    "Time to write code. We will build each component cleanly, test it, and then assemble them into the full model."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import everything we need\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Patch Embedding\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_patch_embedding.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_patch_embedding"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Patch Embedding (Recap from Notebook 1)\n",
    "\n",
    "We use the Conv2d trick: a convolution with kernel_size=patch_size and stride=patch_size cleanly splits the image into non-overlapping patches and projects each to the embedding dimension in a single operation."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Convert image into patch embeddings using Conv2d.\n",
    "\n",
    "    Input:  (B, C, H, W) image tensor\n",
    "    Output: (B, num_patches, embed_dim) patch embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=192):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2  # 64 for 32/4\n",
    "\n",
    "        # Conv2d does patch extraction AND linear projection in one step\n",
    "        self.projection = nn.Conv2d(\n",
    "            in_channels, embed_dim,\n",
    "            kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W) -> (B, embed_dim, H/P, W/P)\n",
    "        x = self.projection(x)\n",
    "        # Flatten spatial dims and transpose: (B, embed_dim, N) -> (B, N, embed_dim)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify the shapes are correct."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick shape test\n",
    "patch_embed = PatchEmbedding(img_size=32, patch_size=4, embed_dim=192)\n",
    "dummy_img = torch.randn(2, 3, 32, 32)  # Batch of 2 CIFAR-10 images\n",
    "patches = patch_embed(dummy_img)\n",
    "print(f\"Input shape:  {dummy_img.shape}\")   # (2, 3, 32, 32)\n",
    "print(f\"Output shape: {patches.shape}\")      # (2, 64, 192)\n",
    "print(f\"Number of patches: {patch_embed.num_patches}\")  # 64\n",
    "assert patches.shape == (2, 64, 192), \"Shape mismatch!\"\n",
    "print(\"Patch embedding: OK\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Attention And Block\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_attention_and_block.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_attention_and_block"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Multi-Head Attention (From Scratch ‚Äî Recap from Notebook 2)\n",
    "\n",
    "We build multi-head self-attention from raw matrix operations ‚Äî no `nn.MultiheadAttention` wrapper. This keeps the \"from scratch\" promise of the series and makes the attention weights easy to extract for visualization."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention built from scratch.\n",
    "\n",
    "    Each head independently computes Q, K, V projections and attention.\n",
    "    Heads are concatenated and projected through W_O.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=192, num_heads=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        # Q, K, V projections (combined for efficiency)\n",
    "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        B, N, D = x.shape\n",
    "        # Project to Q, K, V and reshape for multi-head\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, heads, N, head_dim)\n",
    "        q, k, v = qkv.unbind(0)            # each: (B, heads, N, head_dim)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * self.scale  # (B, heads, N, N)\n",
    "        attn_weights = attn_scores.softmax(dim=-1)\n",
    "        attn_weights = self.attn_drop(attn_weights)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        out = (attn_weights @ v)            # (B, heads, N, head_dim)\n",
    "        out = out.transpose(1, 2).reshape(B, N, D)  # (B, N, D)\n",
    "        out = self.out_proj(out)\n",
    "\n",
    "        if return_attention:\n",
    "            return out, attn_weights  # (B, heads, N, N)\n",
    "        return out, None"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the full Transformer block using our from-scratch attention."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A single Transformer block with Pre-Norm architecture.\n",
    "\n",
    "    Pre-Norm: LayerNorm BEFORE attention/MLP (more stable training).\n",
    "    Uses our from-scratch MultiHeadSelfAttention, not nn.MultiheadAttention.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=192, num_heads=3, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        mlp_hidden = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        # Pre-Norm Attention with residual\n",
    "        attn_output, attn_weights = self.attn(\n",
    "            self.norm1(x), return_attention=return_attention\n",
    "        )\n",
    "        x = x + attn_output\n",
    "\n",
    "        # Pre-Norm MLP with residual\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        if return_attention:\n",
    "            return x, attn_weights  # attn_weights: (B, heads, N, N)\n",
    "        return x"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify the block works and inspect the attention weight shapes."
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Transformer block\n",
    "block = TransformerBlock(embed_dim=192, num_heads=3)\n",
    "dummy_tokens = torch.randn(2, 65, 192)  # 64 patches + 1 CLS = 65 tokens\n",
    "\n",
    "# Without attention weights\n",
    "out = block(dummy_tokens)\n",
    "print(f\"Input:  {dummy_tokens.shape}\")  # (2, 65, 192)\n",
    "print(f\"Output: {out.shape}\")           # (2, 65, 192)\n",
    "\n",
    "# With attention weights\n",
    "out, attn = block(dummy_tokens, return_attention=True)\n",
    "print(f\"Attention weights: {attn.shape}\")  # (2, 3, 65, 65) ‚Äî per head\n",
    "print(\"Transformer block: OK\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Full Vit Class\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_full_vit_class.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_full_vit_class"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The Full VisionTransformer Class\n",
    "\n",
    "Now for the main event. We assemble all the pieces into the complete ViT architecture."
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Complete Vision Transformer for image classification.\n",
    "\n",
    "    Architecture:\n",
    "        Image ‚Üí PatchEmbedding ‚Üí [CLS] + PositionEmb ‚Üí TransformerBlocks ‚Üí Classify\n",
    "\n",
    "    Args:\n",
    "        img_size:    Input image size (assumes square images)\n",
    "        patch_size:  Size of each patch\n",
    "        in_channels: Number of input channels (3 for RGB)\n",
    "        num_classes: Number of classification categories\n",
    "        embed_dim:   Transformer hidden dimension D\n",
    "        depth:       Number of Transformer blocks\n",
    "        num_heads:   Number of attention heads per block\n",
    "        mlp_ratio:   MLP hidden dimension as multiple of embed_dim\n",
    "        dropout:     Dropout rate\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=32,\n",
    "        patch_size=4,\n",
    "        in_channels=3,\n",
    "        num_classes=10,\n",
    "        embed_dim=192,\n",
    "        depth=6,\n",
    "        num_heads=3,\n",
    "        mlp_ratio=4.0,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # --- Patch Embedding ---\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            img_size, patch_size, in_channels, embed_dim\n",
    "        )\n",
    "\n",
    "        # --- CLS Token ---\n",
    "        # Learnable token prepended to the sequence\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "        # --- Position Embeddings ---\n",
    "        # Learnable position for each token (CLS + patches)\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, self.num_patches + 1, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.pos_drop = nn.Dropout(dropout)"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue the `__init__` method with the encoder and classification head."
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(VisionTransformer):\n",
    "    \"\"\"Continuing the VisionTransformer definition.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        # This cell just extends the previous class for readability.\n",
    "        # In practice, all of this goes in one __init__.\n",
    "        # We will define the complete class in one block below.\n",
    "        pass"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, let us define the complete class properly in a single clean block. This is more realistic and avoids any inheritance tricks."
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Complete Vision Transformer for image classification.\"\"\"\n",
    "\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3,\n",
    "                 num_classes=10, embed_dim=192, depth=6, num_heads=3,\n",
    "                 mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.depth = depth\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # 1. Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            img_size, patch_size, in_channels, embed_dim\n",
    "        )\n",
    "\n",
    "        # 2. CLS token and position embeddings\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, self.num_patches + 1, embed_dim)\n",
    "        )\n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "\n",
    "        # 3. Transformer encoder (stack of blocks)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        # 4. Classification head\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights following ViT conventions.\"\"\"\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.head.weight, std=0.02)\n",
    "        nn.init.zeros_(self.head.bias)"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Forward Pass\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_forward_pass.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_forward_pass"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the forward method. This is where the entire pipeline comes together."
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(VisionTransformer):\n",
    "    \"\"\"Add forward method to VisionTransformer.\"\"\"\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        \"\"\"\n",
    "        Forward pass of the Vision Transformer.\n",
    "\n",
    "        Args:\n",
    "            x: Input images (B, C, H, W)\n",
    "            return_attention: If True, also return attention weights\n",
    "                              from the last block\n",
    "\n",
    "        Returns:\n",
    "            logits: Class predictions (B, num_classes)\n",
    "            attn_weights: (optional) Attention from last layer (B, heads, N, N)\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # Step 1: Patch embedding ‚Äî (B, C, H, W) -> (B, num_patches, D)\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # Step 2: Prepend CLS token ‚Äî (B, num_patches, D) -> (B, num_patches+1, D)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "\n",
    "        # Step 3: Add position embeddings\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # Step 4: Pass through Transformer blocks\n",
    "        attn_weights = None\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            if return_attention and i == self.depth - 1:\n",
    "                # Get attention from the last layer only\n",
    "                x, attn_weights = block(x, return_attention=True)\n",
    "            else:\n",
    "                x = block(x)\n",
    "\n",
    "        # Step 5: Extract CLS token (index 0)\n",
    "        cls_output = x[:, 0]  # (B, D)\n",
    "\n",
    "        # Step 6: Classification head\n",
    "        cls_output = self.norm(cls_output)\n",
    "        logits = self.head(cls_output)\n",
    "\n",
    "        if return_attention:\n",
    "            return logits, attn_weights\n",
    "        return logits"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify the full model works and count its parameters."
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ViT-Tiny model\n",
    "model = VisionTransformer(\n",
    "    img_size=32, patch_size=4, in_channels=3, num_classes=10,\n",
    "    embed_dim=192, depth=6, num_heads=3, mlp_ratio=4.0, dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters:     {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size:           {total_params * 4 / 1024 / 1024:.1f} MB (float32)\")\n",
    "\n",
    "# Forward pass test\n",
    "dummy_batch = torch.randn(4, 3, 32, 32).to(device)\n",
    "logits = model(dummy_batch)\n",
    "print(f\"\\nInput shape:  {dummy_batch.shape}\")\n",
    "print(f\"Output shape: {logits.shape}\")       # Should be (4, 10)\n",
    "assert logits.shape == (4, 10), \"Output shape mismatch!\"\n",
    "\n",
    "# Test with attention weights\n",
    "logits, attn = model(dummy_batch, return_attention=True)\n",
    "print(f\"Attention shape: {attn.shape}\")      # Should be (4, 3, 65, 65)\n",
    "print(\"\\nFull model: OK! Ready to train.\")"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Data Preparation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_data_preparation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_11_data_preparation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Data Preparation\n",
    "\n",
    "CIFAR-10 consists of 60,000 32x32 color images in 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. We use 50,000 for training and 10,000 for testing.\n",
    "\n",
    "Data augmentation is crucial for ViT ‚Äî without it, the model overfits quickly due to its lack of inductive biases."
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 class names\n",
    "CIFAR10_CLASSES = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "    'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]\n",
    "\n",
    "# Training transforms: augmentation + normalization\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2470, 0.2435, 0.2616]\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Test transforms: normalization only (no augmentation)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2470, 0.2435, 0.2616]\n",
    "    ),\n",
    "])"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load CIFAR-10\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=train_transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=test_transform\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=128, shuffle=True,\n",
    "    num_workers=2, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=128, shuffle=False,\n",
    "    num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Training samples:   {len(train_dataset):,}\")\n",
    "print(f\"Test samples:       {len(test_dataset):,}\")\n",
    "print(f\"Training batches:   {len(train_loader)}\")\n",
    "print(f\"Test batches:       {len(test_loader)}\")\n",
    "print(f\"Classes:            {CIFAR10_CLASSES}\")"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize a batch of training images to see what our model will be working with."
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a batch of training images\n",
    "# We need to un-normalize for display\n",
    "def unnormalize(img_tensor):\n",
    "    \"\"\"Reverse CIFAR-10 normalization for visualization.\"\"\"\n",
    "    mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.2470, 0.2435, 0.2616]).view(3, 1, 1)\n",
    "    return (img_tensor * std + mean).clamp(0, 1)\n",
    "\n",
    "# Get a batch\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(14, 4))\n",
    "fig.suptitle('Sample Training Images (with augmentation)', fontsize=14, fontweight='bold')\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = unnormalize(images[i]).permute(1, 2, 0).numpy()\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(CIFAR10_CLASSES[labels[i]], fontsize=9)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Training Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_training_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_12_training_setup"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Training Setup\n",
    "\n",
    "We use **AdamW** (Adam with decoupled weight decay) and a **learning rate schedule** with linear warmup followed by cosine decay. This is the standard recipe for training Vision Transformers."
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 25\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 0.05\n",
    "WARMUP_EPOCHS = 5\n",
    "\n",
    "# Print hyperparameter summary\n",
    "print(\"=\" * 50)\n",
    "print(\"         TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Model:          ViT-Tiny\")\n",
    "print(f\"  Parameters:     {total_params:,}\")\n",
    "print(f\"  Epochs:         {EPOCHS}\")\n",
    "print(f\"  Batch size:     128\")\n",
    "print(f\"  Learning rate:  {LEARNING_RATE}\")\n",
    "print(f\"  Weight decay:   {WEIGHT_DECAY}\")\n",
    "print(f\"  Warmup epochs:  {WARMUP_EPOCHS}\")\n",
    "print(f\"  Optimizer:      AdamW\")\n",
    "print(f\"  LR schedule:    Linear warmup + Cosine decay\")\n",
    "print(f\"  Device:         {device}\")\n",
    "print(\"=\" * 50)"
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    betas=(0.9, 0.999),\n",
    ")\n",
    "\n",
    "# Learning rate scheduler: linear warmup then cosine decay\n",
    "def lr_lambda(epoch):\n",
    "    \"\"\"Linear warmup for warmup_epochs, then cosine decay.\"\"\"\n",
    "    if epoch < WARMUP_EPOCHS:\n",
    "        return (epoch + 1) / WARMUP_EPOCHS\n",
    "    else:\n",
    "        progress = (epoch - WARMUP_EPOCHS) / (EPOCHS - WARMUP_EPOCHS)\n",
    "        return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Visualize the LR schedule\n",
    "lrs = [lr_lambda(e) * LEARNING_RATE for e in range(EPOCHS)]\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(range(EPOCHS), lrs, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule: Warmup + Cosine Decay')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo Forward\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_todo_forward.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_13_todo_forward"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Your Turn!\n",
    "\n",
    "Before we run the full training, let us make sure you understand the key pieces by implementing them yourself.\n",
    "\n",
    "### TODO 1: Implement the VisionTransformer Forward Pass\n",
    "\n",
    "The `__init__` is done for you. Your job is to implement the `forward()` method that connects all the pieces."
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformerTODO(nn.Module):\n",
    "    \"\"\"Vision Transformer ‚Äî implement the forward pass!\"\"\"\n",
    "\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3,\n",
    "                 num_classes=10, embed_dim=192, depth=6, num_heads=3,\n",
    "                 mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.depth = depth\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Implement the full ViT forward pass.\n",
    "\n",
    "        Steps:\n",
    "          1. Apply self.patch_embed to get patch tokens (B, N, D)\n",
    "          2. Expand self.cls_token to batch size and prepend it (B, N+1, D)\n",
    "          3. Add self.pos_embed and apply self.pos_drop\n",
    "          4. Pass through each block in self.blocks\n",
    "          5. Extract the CLS token (index 0) from the output\n",
    "          6. Apply self.norm then self.head\n",
    "          7. Return the logits (B, num_classes)\n",
    "\n",
    "        Hints:\n",
    "          - cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "          - Use torch.cat([cls_tokens, x], dim=1) to prepend\n",
    "          - x[:, 0] extracts the first token from each batch\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # ============ YOUR CODE HERE ============\n",
    "        # TODO: Implement the 7 steps above\n",
    "\n",
    "        raise NotImplementedError(\"Implement the forward pass!\")\n",
    "        # ========================================="
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verification for TODO 1 ---\n",
    "# Uncomment after implementing:\n",
    "\n",
    "# model_todo = VisionTransformerTODO().to(device)\n",
    "# test_input = torch.randn(4, 3, 32, 32).to(device)\n",
    "# test_output = model_todo(test_input)\n",
    "# assert test_output.shape == (4, 10), f\"Expected (4, 10), got {test_output.shape}\"\n",
    "#\n",
    "# # Check that we can compute loss\n",
    "# test_labels = torch.randint(0, 10, (4,)).to(device)\n",
    "# loss = criterion(test_output, test_labels)\n",
    "# loss.backward()\n",
    "# print(f\"Output shape: {test_output.shape} -- Correct!\")\n",
    "# print(f\"Loss value:   {loss.item():.4f} -- Gradient flows!\")\n",
    "# print(\"TODO 1: PASSED!\")"
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_todo_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_14_todo_training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement the Training Step\n",
    "\n",
    "Implement a function that trains the model for one epoch. This is the standard PyTorch training loop ‚Äî but writing it yourself helps cement the pattern."
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_TODO(model, dataloader, optimizer, scheduler, criterion, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "\n",
    "    Steps for each batch:\n",
    "      1. Move images and labels to device\n",
    "      2. Zero the optimizer gradients\n",
    "      3. Forward pass: logits = model(images)\n",
    "      4. Compute loss: loss = criterion(logits, labels)\n",
    "      5. Backward pass: loss.backward()\n",
    "      6. Optimizer step: optimizer.step()\n",
    "      7. Track running loss and accuracy\n",
    "\n",
    "    After all batches:\n",
    "      8. Step the scheduler (once per epoch)\n",
    "\n",
    "    Returns:\n",
    "        avg_loss (float): Average loss over the epoch\n",
    "        accuracy (float): Training accuracy (0-100)\n",
    "\n",
    "    Hints:\n",
    "      - predictions = logits.argmax(dim=1) gives predicted classes\n",
    "      - Compare predictions == labels and count correct ones\n",
    "      - Don't forget model.train() at the start!\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        # ============ YOUR CODE HERE ============\n",
    "        # TODO: Implement steps 1-7\n",
    "\n",
    "        raise NotImplementedError(\"Implement the training step!\")\n",
    "        # =========================================\n",
    "\n",
    "    # Step 8: Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    accuracy = 100.0 * correct / total\n",
    "    return avg_loss, accuracy"
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verification for TODO 2 ---\n",
    "# Uncomment after implementing:\n",
    "\n",
    "# # Reset the model for a fair test\n",
    "# model_todo2 = VisionTransformer(\n",
    "#     img_size=32, patch_size=4, in_channels=3, num_classes=10,\n",
    "#     embed_dim=192, depth=6, num_heads=3, mlp_ratio=4.0, dropout=0.1\n",
    "# ).to(device)\n",
    "# opt_todo2 = optim.AdamW(model_todo2.parameters(), lr=3e-4, weight_decay=0.05)\n",
    "# sched_todo2 = optim.lr_scheduler.LambdaLR(opt_todo2, lr_lambda)\n",
    "#\n",
    "# loss1, acc1 = train_one_epoch_TODO(\n",
    "#     model_todo2, train_loader, opt_todo2, sched_todo2, criterion, device\n",
    "# )\n",
    "# print(f\"Epoch 1 ‚Äî Loss: {loss1:.4f}, Accuracy: {acc1:.1f}%\")\n",
    "#\n",
    "# loss2, acc2 = train_one_epoch_TODO(\n",
    "#     model_todo2, train_loader, opt_todo2, sched_todo2, criterion, device\n",
    "# )\n",
    "# print(f\"Epoch 2 ‚Äî Loss: {loss2:.4f}, Accuracy: {acc2:.1f}%\")\n",
    "#\n",
    "# assert loss2 < loss1, f\"Loss should decrease! Epoch 1: {loss1:.4f}, Epoch 2: {loss2:.4f}\"\n",
    "# print(\"TODO 2: PASSED! Loss is decreasing.\")"
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Training Begins\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/15_training_begins.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_15_training_begins"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Training\n",
    "\n",
    "Now let us train our ViT-Tiny on CIFAR-10. This is the moment of truth ‚Äî can a hand-built Transformer learn to classify images?\n",
    "\n",
    "First, we define clean training and evaluation functions."
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, scheduler, criterion, device):\n",
    "    \"\"\"Train for one epoch. Returns (avg_loss, accuracy).\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        predicted = logits.argmax(dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    scheduler.step()\n",
    "    return running_loss / len(dataloader), 100.0 * correct / total"
   ],
   "id": "cell_51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate model on a dataset. Returns (avg_loss, accuracy).\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        predicted = logits.argmax(dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return running_loss / len(dataloader), 100.0 * correct / total"
   ],
   "id": "cell_52"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us reinitialize the model fresh and start training. On a T4 GPU, this should take approximately 5-8 minutes."
   ],
   "id": "cell_53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fresh model for training\n",
    "model = VisionTransformer(\n",
    "    img_size=32, patch_size=4, in_channels=3, num_classes=10,\n",
    "    embed_dim=192, depth=6, num_heads=3, mlp_ratio=4.0, dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY, betas=(0.9, 0.999)\n",
    ")\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Starting training with {total_params:,} parameters...\")\n",
    "print(f\"{'Epoch':>5} | {'Train Loss':>10} | {'Train Acc':>9} | \"\n",
    "      f\"{'Val Loss':>8} | {'Val Acc':>7} | {'LR':>10}\")\n",
    "print(\"-\" * 65)"
   ],
   "id": "cell_54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Training Loop\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/16_training_loop.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_16_training_loop"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_acc': [], 'lr': []\n",
    "}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model, train_loader, optimizer, scheduler, criterion, device\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    # Record\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['lr'].append(current_lr)\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"{epoch+1:>5d} | {train_loss:>10.4f} | {train_acc:>8.2f}% | \"\n",
    "          f\"{val_loss:>8.4f} | {val_acc:>6.2f}% | {current_lr:>10.6f}\")\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {max(history['val_acc']):.2f}% \"\n",
    "      f\"(epoch {np.argmax(history['val_acc'])+1})\")"
   ],
   "id": "cell_55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Training Curves\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/17_training_curves.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_17_training_curves"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the training curves. These tell us a lot about the training dynamics."
   ],
   "id": "cell_56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Loss curves\n",
    "ax1 = axes[0]\n",
    "ax1.plot(history['train_loss'], label='Train Loss', color='#2196F3', linewidth=2)\n",
    "ax1.plot(history['val_loss'], label='Val Loss', color='#FF5722', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy curves\n",
    "ax2 = axes[1]\n",
    "ax2.plot(history['train_acc'], label='Train Accuracy', color='#4CAF50', linewidth=2)\n",
    "ax2.plot(history['val_acc'], label='Val Accuracy', color='#9C27B0', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "best_epoch = np.argmax(history['val_acc'])\n",
    "ax2.axvline(x=best_epoch, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.annotate(f\"Best: {max(history['val_acc']):.1f}%\",\n",
    "             xy=(best_epoch, max(history['val_acc'])),\n",
    "             fontsize=10, fontweight='bold',\n",
    "             xytext=(best_epoch + 1, max(history['val_acc']) - 5),\n",
    "             arrowprops=dict(arrowstyle='->', color='gray'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_57"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to notice about the training curves:\n",
    "\n",
    "- **Warmup phase** (epochs 1-5): The loss drops rapidly as the learning rate ramps up. The model is going from random initialization to something useful.\n",
    "- **Training vs validation gap**: If train accuracy is much higher than val accuracy, that is overfitting. Some gap is expected, but if it is too large, we need more regularization or data.\n",
    "- **Cosine decay**: The learning rate gradually decreases, allowing the model to fine-tune its weights in later epochs.\n",
    "\n",
    "Our ViT-Tiny should achieve around **75-82%** validation accuracy on CIFAR-10. This is respectable for a 2.8M parameter model with no pre-training! For context, a ResNet-18 (11M params) gets ~93% with the same training recipe ‚Äî the gap comes from the inductive bias advantage of convolutions."
   ],
   "id": "cell_58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Predictions\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/18_predictions.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_18_predictions"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Final Output\n",
    "\n",
    "This is the payoff of the entire series. We will create a comprehensive visualization that shows our ViT in action.\n",
    "\n",
    "### Panel 1: Predictions on Test Images\n",
    "\n",
    "Let us see how our trained model performs on individual test images."
   ],
   "id": "cell_59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_predictions(model, dataloader, device, n_samples=10):\n",
    "    \"\"\"Get predictions for the first n_samples images.\"\"\"\n",
    "    model.eval()\n",
    "    images, labels = next(iter(dataloader))\n",
    "    images = images[:n_samples].to(device)\n",
    "    labels = labels[:n_samples]\n",
    "\n",
    "    logits = model(images)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    preds = logits.argmax(dim=1).cpu()\n",
    "    confidences = probs.max(dim=1).values.cpu()\n",
    "\n",
    "    return images.cpu(), labels, preds, confidences"
   ],
   "id": "cell_60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels, preds, confidences = get_predictions(model, test_loader, device)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('Model Predictions on Test Images', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = unnormalize(images[i]).permute(1, 2, 0).numpy()\n",
    "    ax.imshow(img)\n",
    "\n",
    "    true_label = CIFAR10_CLASSES[labels[i]]\n",
    "    pred_label = CIFAR10_CLASSES[preds[i]]\n",
    "    conf = confidences[i].item()\n",
    "    correct = preds[i] == labels[i]\n",
    "\n",
    "    color = '#2E7D32' if correct else '#C62828'\n",
    "    symbol = 'correct' if correct else 'WRONG'\n",
    "    ax.set_title(f'Pred: {pred_label} ({conf:.0%})\\nTrue: {true_label} [{symbol}]',\n",
    "                 fontsize=9, color=color, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_61"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Attention Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/19_attention_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_19_attention_viz"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Panel 2: Attention Visualization\n",
    "\n",
    "This is one of the most illuminating visualizations in deep learning. We will see **where the model looks** when classifying an image ‚Äî specifically, what the `[CLS]` token attends to in the final Transformer layer."
   ],
   "id": "cell_62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_attention_maps(model, images, device):\n",
    "    \"\"\"Extract CLS token attention from the last layer.\n",
    "\n",
    "    Returns:\n",
    "        attn_maps: (B, num_heads, num_patches) ‚Äî CLS attention to each patch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    images = images.to(device)\n",
    "    logits, attn_weights = model(images, return_attention=True)\n",
    "\n",
    "    # attn_weights: (B, num_heads, N+1, N+1) where N+1 includes CLS\n",
    "    # We want: CLS (row 0) attending to patches (columns 1:)\n",
    "    cls_attn = attn_weights[:, :, 0, 1:]  # (B, num_heads, num_patches)\n",
    "\n",
    "    return cls_attn.cpu(), logits.argmax(dim=1).cpu()"
   ],
   "id": "cell_63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(images, attn_maps, labels, preds, n_images=3, patch_size=4):\n",
    "    \"\"\"Visualize CLS attention overlaid on images.\"\"\"\n",
    "    num_heads = attn_maps.shape[1]\n",
    "    grid_size = int(math.sqrt(attn_maps.shape[2]))  # 8 for 64 patches\n",
    "\n",
    "    fig, axes = plt.subplots(n_images, num_heads + 1,\n",
    "                              figsize=(3.5 * (num_heads + 1), 3.5 * n_images))\n",
    "    fig.suptitle('Attention Maps: What [CLS] Focuses On (Last Layer)',\n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "    for i in range(n_images):\n",
    "        img = unnormalize(images[i]).permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Original image\n",
    "        axes[i, 0].imshow(img)\n",
    "        pred_name = CIFAR10_CLASSES[preds[i]]\n",
    "        true_name = CIFAR10_CLASSES[labels[i]]\n",
    "        axes[i, 0].set_title(f'Original\\nPred: {pred_name}\\nTrue: {true_name}',\n",
    "                              fontsize=10)\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # Per-head attention\n",
    "        for h in range(num_heads):\n",
    "            attn = attn_maps[i, h].reshape(grid_size, grid_size).numpy()\n",
    "\n",
    "            # Upsample attention to image size\n",
    "            attn_upsampled = np.kron(attn, np.ones((patch_size, patch_size)))\n",
    "\n",
    "            axes[i, h + 1].imshow(img)\n",
    "            axes[i, h + 1].imshow(attn_upsampled, alpha=0.6,\n",
    "                                   cmap='hot', interpolation='bilinear')\n",
    "            axes[i, h + 1].set_title(f'Head {h+1}', fontsize=10)\n",
    "            axes[i, h + 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "cell_64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a fresh batch of test images\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "sample_images = test_images[:3]\n",
    "sample_labels = test_labels[:3]\n",
    "\n",
    "# Extract attention maps\n",
    "attn_maps, preds = get_attention_maps(model, sample_images, device)\n",
    "print(f\"Attention map shape: {attn_maps.shape}\")  # (3, 3, 64)\n",
    "\n",
    "# Visualize!\n",
    "visualize_attention(sample_images, attn_maps, sample_labels, preds,\n",
    "                    n_images=3, patch_size=4)"
   ],
   "id": "cell_65"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the attention maps closely! Each head learns to attend to different aspects of the image:\n",
    "- Some heads focus on the **object** (the cat's body, the truck's outline)\n",
    "- Some focus on **edges** and boundaries\n",
    "- Some attend more **uniformly**, gathering global context\n",
    "\n",
    "This is the power of multi-head attention ‚Äî the model develops multiple complementary \"ways of looking\" at the image, all in parallel."
   ],
   "id": "cell_66"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Position Embeddings\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/20_position_embeddings.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_20_position_embeddings"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Panel 3: Learned Position Embeddings\n",
    "\n",
    "This is a fascinating visualization. Recall that our model received **1D position indices** (0, 1, 2, ..., 63), yet the patches are arranged on a **2D grid** (8x8). Did the model learn the 2D spatial structure from data alone?"
   ],
   "id": "cell_67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def visualize_position_embeddings(model):\n",
    "    \"\"\"Visualize learned position embedding similarity.\"\"\"\n",
    "    # Extract position embeddings (skip CLS at index 0)\n",
    "    pos_embed = model.pos_embed[0, 1:, :].cpu()  # (64, 192)\n",
    "\n",
    "    # Compute cosine similarity between all pairs\n",
    "    pos_embed_norm = F.normalize(pos_embed, dim=1)\n",
    "    similarity = torch.mm(pos_embed_norm, pos_embed_norm.t()).numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Full similarity matrix\n",
    "    im = axes[0].imshow(similarity, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    axes[0].set_title('Position Embedding Cosine Similarity\\n(64 patches x 64 patches)',\n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[0].set_xlabel('Patch Index')\n",
    "    axes[0].set_ylabel('Patch Index')\n",
    "    plt.colorbar(im, ax=axes[0], shrink=0.8)\n",
    "\n",
    "    # Show similarity for specific patches\n",
    "    # Pick patch 0 (top-left), 4 (top-middle), 28 (center), 63 (bottom-right)\n",
    "    interesting_patches = [0, 4, 28, 63]\n",
    "    grid_size = 8\n",
    "\n",
    "    axes[1].set_title('Similarity to Selected Patches\\n(brighter = more similar)',\n",
    "                       fontsize=12, fontweight='bold')\n",
    "\n",
    "    combined = np.zeros((grid_size * 2, grid_size * 2))\n",
    "    titles = ['Patch 0\\n(top-left)', 'Patch 4\\n(top-mid)',\n",
    "              'Patch 28\\n(center)', 'Patch 63\\n(bottom-right)']\n",
    "\n",
    "    for idx, patch_idx in enumerate(interesting_patches):\n",
    "        row, col = idx // 2, idx % 2\n",
    "        sim_map = similarity[patch_idx].reshape(grid_size, grid_size)\n",
    "        combined[row*grid_size:(row+1)*grid_size,\n",
    "                 col*grid_size:(col+1)*grid_size] = sim_map\n",
    "\n",
    "    im2 = axes[1].imshow(combined, cmap='viridis')\n",
    "\n",
    "    # Add grid lines to separate the 4 panels\n",
    "    axes[1].axhline(y=grid_size - 0.5, color='white', linewidth=2)\n",
    "    axes[1].axvline(x=grid_size - 0.5, color='white', linewidth=2)\n",
    "\n",
    "    # Label each sub-panel\n",
    "    for idx in range(4):\n",
    "        row, col = idx // 2, idx % 2\n",
    "        axes[1].text(col * grid_size + grid_size / 2, row * grid_size + 1,\n",
    "                     titles[idx], ha='center', va='top',\n",
    "                     fontsize=9, fontweight='bold', color='white')\n",
    "\n",
    "    axes[1].axis('off')\n",
    "    plt.colorbar(im2, ax=axes[1], shrink=0.8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return similarity\n",
    "\n",
    "similarity = visualize_position_embeddings(model)"
   ],
   "id": "cell_68"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The position embedding similarity matrix should reveal a striking pattern: **patches that are spatially close on the 2D grid have similar embeddings**, even though we only gave the model 1D indices!\n",
    "\n",
    "On the left matrix, look for a block-diagonal structure ‚Äî groups of 8 patches (one row of the 8x8 grid) cluster together. On the right, notice that each patch is most similar to its immediate neighbors and least similar to patches far away.\n",
    "\n",
    "This is a remarkable result. The model **discovered 2D geometry** purely from the training signal. Nobody told it that patch 0 is next to patch 1 horizontally and next to patch 8 vertically ‚Äî it learned this from seeing thousands of images where nearby patches tend to share visual features."
   ],
   "id": "cell_69"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Per Class\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/21_per_class.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_21_per_class"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Panel 4: Per-Class Accuracy\n",
    "\n",
    "Not all classes are equally easy for a Vision Transformer. Let us see which CIFAR-10 classes our model handles well and which ones are challenging."
   ],
   "id": "cell_70"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def per_class_accuracy(model, dataloader, device, class_names):\n",
    "    \"\"\"Compute accuracy for each class.\"\"\"\n",
    "    model.eval()\n",
    "    class_correct = torch.zeros(len(class_names))\n",
    "    class_total = torch.zeros(len(class_names))\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits = model(images)\n",
    "        preds = logits.argmax(dim=1)\n",
    "\n",
    "        for c in range(len(class_names)):\n",
    "            mask = labels == c\n",
    "            class_total[c] += mask.sum().item()\n",
    "            class_correct[c] += (preds[mask] == labels[mask]).sum().item()\n",
    "\n",
    "    accuracies = (class_correct / class_total * 100).numpy()\n",
    "    return accuracies"
   ],
   "id": "cell_71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_accs = per_class_accuracy(model, test_loader, device, CIFAR10_CLASSES)\n",
    "\n",
    "# Sort by accuracy for the plot\n",
    "sorted_indices = np.argsort(class_accs)\n",
    "sorted_names = [CIFAR10_CLASSES[i] for i in sorted_indices]\n",
    "sorted_accs = class_accs[sorted_indices]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors = plt.cm.RdYlGn(sorted_accs / 100)  # Red for low, green for high\n",
    "bars = ax.barh(range(10), sorted_accs, color=colors, edgecolor='gray', linewidth=0.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (acc, bar) in enumerate(zip(sorted_accs, bars)):\n",
    "    ax.text(acc + 0.5, i, f'{acc:.1f}%', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_yticks(range(10))\n",
    "ax.set_yticklabels(sorted_names, fontsize=11)\n",
    "ax.set_xlabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Per-Class Accuracy on CIFAR-10', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(0, 105)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "ax.axvline(x=np.mean(class_accs), color='blue', linestyle='--',\n",
    "           linewidth=1.5, label=f'Mean: {np.mean(class_accs):.1f}%')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_72"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical patterns you might observe:\n",
    "- **Ship** and **automobile** tend to score well ‚Äî they have distinctive geometric shapes\n",
    "- **Cat** and **dog** are often confused with each other ‚Äî they share similar shapes and textures\n",
    "- **Bird** and **deer** can be tricky ‚Äî they appear against varied backgrounds\n",
    "\n",
    "These patterns mirror what CNNs struggle with too, but the specific errors may differ because ViT has a different inductive bias."
   ],
   "id": "cell_73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Grand Summary\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/22_grand_summary.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_22_grand_summary"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Grand Summary Figure\n",
    "\n",
    "Let us bring together the key results into one comprehensive figure."
   ],
   "id": "cell_74"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"     VISION TRANSFORMER TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Architecture:       ViT-Tiny (D=192, L=6, H=3)\")\n",
    "print(f\"  Parameters:         {total_params:,}\")\n",
    "print(f\"  Training epochs:    {EPOCHS}\")\n",
    "print(f\"  Final train acc:    {history['train_acc'][-1]:.2f}%\")\n",
    "print(f\"  Final val acc:      {history['val_acc'][-1]:.2f}%\")\n",
    "print(f\"  Best val acc:       {max(history['val_acc']):.2f}% (epoch {np.argmax(history['val_acc'])+1})\")\n",
    "print(f\"  Mean per-class acc: {np.mean(class_accs):.2f}%\")\n",
    "print(f\"  Best class:         {CIFAR10_CLASSES[np.argmax(class_accs)]} ({np.max(class_accs):.1f}%)\")\n",
    "print(f\"  Worst class:        {CIFAR10_CLASSES[np.argmin(class_accs)]} ({np.min(class_accs):.1f}%)\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "cell_75"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"  You built, trained, and analyzed a Vision Transformer from scratch!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n  In this 3-notebook series, you have:\")\n",
    "print(\"    Notebook 1: Converted images into patch embeddings\")\n",
    "print(\"    Notebook 2: Built a Transformer encoder with multi-head attention\")\n",
    "print(\"    Notebook 3: Assembled the full ViT, trained it, and visualized\")\n",
    "print(\"                what it learned\")\n",
    "print(\"\\n  Key takeaways:\")\n",
    "print(f\"    - ViT-Tiny with {total_params:,} params achieves ~{max(history['val_acc']):.0f}% on CIFAR-10\")\n",
    "print(\"    - Attention heads learn to focus on different image regions\")\n",
    "print(\"    - Position embeddings discover 2D spatial structure from 1D inputs\")\n",
    "print(\"    - ViT needs more data than CNNs but scales better with compute\")\n",
    "print()"
   ],
   "id": "cell_76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Reflection\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/23_reflection.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_23_reflection"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "\n",
    "Take a moment to think about what we have built and observed.\n",
    "\n",
    "**1. The Accuracy Gap**\n",
    "\n",
    "Our ViT-Tiny achieved ~78% on CIFAR-10. A ResNet-18 gets ~93% with similar training. Why the gap?\n",
    "\n",
    "The key insight: **ViT has no built-in knowledge about images**. A CNN knows that nearby pixels are related (locality) and that features should work regardless of position (translation equivariance). ViT must learn all of this from data alone. With only 50,000 training images, it simply does not have enough examples to fully learn these spatial priors.\n",
    "\n",
    "What would help ViT catch up?\n",
    "- **More data**: The original ViT was pre-trained on 300M images. With enough data, ViT actually surpasses CNNs.\n",
    "- **Data-efficient tricks**: DeiT showed that with strong augmentation, regularization, and knowledge distillation, ViT can be competitive on ImageNet even without massive pre-training.\n",
    "- **Hybrid architectures**: Models like CoAtNet combine convolutional stems with Transformer encoders to get the best of both worlds."
   ],
   "id": "cell_77"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Attention Patterns**\n",
    "\n",
    "Look back at the attention visualization. Different heads attend to different things ‚Äî this is called **attention head specialization**. You might see:\n",
    "- Heads that attend to edges and boundaries\n",
    "- Heads that focus on the central object\n",
    "- Heads that capture global context more uniformly\n",
    "\n",
    "This diversity is not explicitly programmed ‚Äî it emerges from training. The model finds it useful to look at images in multiple complementary ways, and the multi-head mechanism provides the capacity for this.\n",
    "\n",
    "**3. Position Embeddings Learning 2D Structure**\n",
    "\n",
    "Perhaps the most surprising result: position embeddings given only 1D indices (0, 1, 2, ..., 63) learned to encode 2D spatial relationships. How?\n",
    "\n",
    "During training, the model sees that patch 0 (top-left) and patch 1 (one step right) consistently share visual features, while patch 0 and patch 63 (bottom-right) rarely do. Over thousands of gradient updates, the position embeddings adjust to reflect these statistical patterns ‚Äî which happen to encode 2D geometry. The model does not \"know\" about 2D; it just learns which positions tend to have similar content."
   ],
   "id": "cell_78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Challenges\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/24_challenges.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_24_challenges"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Challenges\n",
    "\n",
    "If you want to go deeper, try these extensions:\n",
    "\n",
    "**Challenge 1: Global Average Pooling vs [CLS] Token**\n",
    "\n",
    "Replace the [CLS] token approach with global average pooling:"
   ],
   "id": "cell_79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of: cls_output = x[:, 0]\n",
    "# Try:        cls_output = x[:, 1:].mean(dim=1)  # Average all patch tokens"
   ],
   "id": "cell_80"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does accuracy change? Does training behave differently? For small models, GAP is sometimes slightly better because it uses information from all tokens directly."
   ],
   "id": "cell_81"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 2: Patch Size Ablation**\n",
    "\n",
    "Try different patch sizes and compare accuracy vs training speed:"
   ],
   "id": "cell_82"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch size 2: 256 patches ‚Äî more detail, slower training\n",
    "# Patch size 4: 64 patches  ‚Äî our default\n",
    "# Patch size 8: 16 patches  ‚Äî very coarse, fast training"
   ],
   "id": "cell_83"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smaller patches give the model finer-grained information but increase the sequence length (and thus computation) quadratically. There is a sweet spot for each image resolution.\n",
    "\n",
    "**Challenge 3: DeiT-style Knowledge Distillation**\n",
    "\n",
    "Train a ResNet-18 teacher, then add a distillation token to ViT that learns to match the teacher's predictions. This can boost ViT accuracy significantly on small datasets. The DeiT paper showed this brings ViT to within 1% of CNNs on ImageNet without any external data.\n",
    "\n",
    "**Challenge 4: Scale Up to ViT-Small**"
   ],
   "id": "cell_84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT-Small: embed_dim=384, depth=8, num_heads=6\n",
    "# ~22M parameters ‚Äî much larger, needs longer training\n",
    "model_small = VisionTransformer(\n",
    "    embed_dim=384, depth=8, num_heads=6, mlp_ratio=4.0, dropout=0.1\n",
    ")"
   ],
   "id": "cell_85"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the accuracy improvement vs the increase in training time."
   ],
   "id": "cell_86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick comparison: how model size scales with config\n",
    "configs = {\n",
    "    'ViT-Tiny (ours)': (192, 6, 3),\n",
    "    'ViT-Small':       (384, 8, 6),\n",
    "    'ViT-Base':        (768, 12, 12),\n",
    "}\n",
    "for name, (d, l, h) in configs.items():\n",
    "    p = l * (4*d*d + 8*d*d) + d*197 + 48*d  # rough estimate\n",
    "    print(f\"{name:<18}: D={d}, L={l}, H={h}, ~{p/1e6:.1f}M params\")"
   ],
   "id": "cell_87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/25_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_25_closing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bigger Picture: ViT and Its Descendants\n",
    "\n",
    "The Vision Transformer opened the floodgates for Transformer-based vision models. Here are the key descendants:\n",
    "\n",
    "| Model | Year | Key Innovation |\n",
    "|-------|------|---------------|\n",
    "| **DeiT** | 2021 | Data-efficient training + distillation token |\n",
    "| **Swin Transformer** | 2021 | Hierarchical + shifted windows for efficiency |\n",
    "| **BEiT** | 2021 | BERT-style pre-training for vision (masked image modeling) |\n",
    "| **MAE** | 2022 | Masked Autoencoder ‚Äî mask 75% of patches and reconstruct |\n",
    "| **DINO / DINOv2** | 2021-23 | Self-supervised learning discovers objects without labels |\n",
    "| **EVA** | 2023 | Scaled ViT to 1B+ params with improved training |\n",
    "\n",
    "Each of these builds on the core ViT architecture we implemented today. The patch embedding, position embedding, Transformer encoder, and classification head ‚Äî these components remain fundamentally the same, even as the training methodology and scale evolve."
   ],
   "id": "cell_88"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Unifying Vision\n",
    "\n",
    "The most profound contribution of ViT was not architectural ‚Äî it was **philosophical**. Before ViT, vision and language were separate worlds with different architectures (CNNs vs Transformers). ViT showed that a single architecture, the Transformer, can handle both modalities.\n",
    "\n",
    "This unification led directly to:\n",
    "- **CLIP** (connecting vision and language in a shared space)\n",
    "- **GPT-4V / Gemini** (multimodal models that see and read)\n",
    "- **Stable Diffusion** (Transformers generating images from text)\n",
    "\n",
    "The journey from \"An Image is Worth 16x16 Words\" to today's multimodal AI is a straight line. And you just built the starting point from scratch."
   ],
   "id": "cell_89"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What We Built Across Three Notebooks\n",
    "\n",
    "| Notebook | Topic | Key Concept |\n",
    "|----------|-------|-------------|\n",
    "| **1** | Input Pipeline | Images are just matrices of patches; Conv2d is an efficient patching trick |\n",
    "| **2** | Transformer Encoder | Self-attention lets every patch see every other patch; multi-head gives diversity |\n",
    "| **3** | Complete ViT | Assembly + training reveals attention patterns and learned spatial structure |\n",
    "\n",
    "**The core insight of this series**: A Vision Transformer is simpler than you might think. It is a patch embedding, position embedding, a stack of attention + MLP blocks, and a linear head. Nothing more. The magic comes from scale and data."
   ],
   "id": "cell_90"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Congratulations! You have built, trained, and analyzed a Vision\")\n",
    "print(\"Transformer entirely from scratch.\")\n",
    "print()\n",
    "print(\"You now understand not just WHAT a ViT does, but WHY each component\")\n",
    "print(\"exists and HOW they work together. This foundation will serve you\")\n",
    "print(\"well as you explore more advanced architectures.\")\n",
    "print()\n",
    "print(\"The complete code from this series can be adapted for any image\")\n",
    "print(\"classification task. Try it on your own datasets!\")"
   ],
   "id": "cell_91"
  }
 ]
}