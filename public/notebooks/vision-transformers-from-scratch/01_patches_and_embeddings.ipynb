{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Patches & Embeddings â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1Ai6MxOfcX1s3XFnM-rGmw-MmfctdBvBe\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_setup"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patches & Embeddings: Teaching a Transformer to See\n",
    "\n",
    "*Part 1 of the Vizuara series on Vision Transformers from Scratch*\n",
    "*Estimated time: 45 minutes*\n",
    "\n",
    "In this notebook, we will build the **input pipeline** for a Vision Transformer (ViT) completely from scratch. By the end, you will understand how an image gets converted into a sequence of tokens â€” the same kind of sequence a language model would process.\n",
    "\n",
    "**What we will build today:**\n",
    "- Patch extraction from images\n",
    "- Flattening and linear projection (patch embeddings)\n",
    "- The [CLS] classification token\n",
    "- Learnable position embeddings\n",
    "\n",
    "**Series roadmap:**\n",
    "- **Notebook 1 (this one):** Patches & Embeddings\n",
    "- **Notebook 2:** Multi-Head Self-Attention for Images\n",
    "- **Notebook 3:** The Full ViT â€” Assembly, Training, and Evaluation"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://pods.vizuara.ai/courses/vision-transformers-from-scratch/practice/1/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install any missing dependencies (Colab-friendly)\n",
    "!pip install -q torch torchvision matplotlib numpy scikit-learn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_why_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_why_matters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Why Does This Matter?\n",
    "\n",
    "The **Vision Transformer (ViT)** rests on a beautifully simple idea: **an image is just a sentence made of patches**.\n",
    "\n",
    "Natural Language Processing had its revolution when Transformers replaced recurrent networks. Words get turned into tokens, tokens get embedded into vectors, and attention does the rest. ViT asks: *what if we did exactly the same thing with images?*\n",
    "\n",
    "Instead of word tokens, we have **patch tokens**. Instead of a vocabulary embedding table, we have a **linear projection layer**. Instead of positional encoding over a 1D sequence, we have **position embeddings that must somehow capture 2D spatial structure**."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds that entire input pipeline. By the end, you will see something remarkable: even with completely random, untrained position embeddings, the cosine similarity matrix already hints at structure. After training, these embeddings **learn 2D spatial relationships on their own** â€” without anyone telling the model that images are 2D.\n",
    "\n",
    "This is the foundation. Everything else in the Vision Transformer â€” attention, classification, training â€” builds directly on what we construct today.\n",
    "\n",
    "Let us get a quick sense of the numbers before diving in."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Key Numbers\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_key_numbers.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_key_numbers"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick preview: the key numbers for ViT-Base\n",
    "print(\"Vision Transformer Input Pipeline â€” Key Numbers\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Image size:        224 x 224 pixels\")\n",
    "print(f\"Patch size:        16 x 16 pixels\")\n",
    "print(f\"Number of patches: (224/16)^2 = {(224//16)**2}\")\n",
    "print(f\"Patch dimension:   16 x 16 x 3 = {16*16*3}\")\n",
    "print(f\"Sequence length:   {(224//16)**2} patches + 1 [CLS] = {(224//16)**2 + 1}\")\n",
    "print(f\"\\nFor CIFAR-10 (our implementation):\")\n",
    "print(f\"Image size:        32 x 32 pixels\")\n",
    "print(f\"Patch size:        4 x 4 pixels\")\n",
    "print(f\"Number of patches: (32/4)^2 = {(32//4)**2}\")\n",
    "print(f\"Patch dimension:   4 x 4 x 3 = {4*4*3}\")\n",
    "print(f\"Sequence length:   {(32//4)**2} patches + 1 [CLS] = {(32//4)**2 + 1}\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_intuition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Building Intuition\n",
    "\n",
    "### The Reading Analogy\n",
    "\n",
    "Consider three ways to process information:\n",
    "\n",
    "**Reading a book:** You consume it **word by word**, left to right, line by line. Each word is a discrete token. This is how Transformers process language.\n",
    "\n",
    "**Looking at a painting with a magnifying glass:** You slide a small window across the canvas, examining local neighborhoods of pixels. The window shares its parameters everywhere it goes. This is how **Convolutional Neural Networks** process images â€” local receptive fields, shared weights, sliding across the image.\n",
    "\n",
    "**Looking at a painting through a grid:** You divide the canvas into a grid of squares, examine each square as a complete unit, and then reason about how they relate to each other. This is how a **Vision Transformer** processes images.\n",
    "\n",
    "The ViT approach is radical in its simplicity. No convolutions. No pooling. No inductive bias about local neighborhoods. Just: chop the image into patches, embed them, and let attention figure out the rest."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought Experiment: The Shuffled Photo\n",
    "\n",
    "Imagine you take a photograph and cut it into a 14x14 grid of squares. Now shuffle all 196 squares randomly. Could you still recognize what is in the photo?\n",
    "\n",
    "Surprisingly, **mostly yes**. Each patch carries enough local information â€” a piece of fur, part of an eye, a section of sky â€” that you could probably identify the subject. Patches are information-rich units.\n",
    "\n",
    "But could you **reconstruct** the original scene? Absolutely not. You would need to know **where each patch belongs**. This is exactly why we need **position embeddings** â€” they tell the Transformer where each patch came from in the original image.\n",
    "\n",
    "> **Think About This:** If we treat each patch as a \"word,\" what is the \"vocabulary size\" of our visual language? In English, the vocabulary is finite â€” maybe 50,000 words. But for image patches, every possible 16x16 arrangement of RGB pixels is a valid \"word.\" The vocabulary is effectively **infinite**. This is why we cannot use a lookup table (like word embeddings) and must instead use a **learned linear projection** to map raw pixels into embedding space.\n",
    "\n",
    "Let us quantify this. How large is the \"vocabulary\" of image patches compared to natural language?"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Visual Vocab\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_visual_vocab.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_visual_vocab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How big is the visual \"vocabulary\"?\n",
    "import math\n",
    "\n",
    "# English vocabulary: ~50,000 common words\n",
    "english_vocab = 50_000\n",
    "\n",
    "# A 16x16 RGB patch: each pixel has 256 values per channel\n",
    "# Total possible patches = 256^(16*16*3) â€” astronomically large\n",
    "patch_pixels = 16 * 16 * 3  # 768 values\n",
    "print(f\"English vocabulary: ~{english_vocab:,} words\")\n",
    "print(f\"Pixels per ViT patch: {patch_pixels}\")\n",
    "print(f\"Possible patches: 256^{patch_pixels} â‰ˆ 10^{patch_pixels * math.log10(256):.0f}\")\n",
    "print(f\"\\nThat is why we CANNOT use a lookup table!\")\n",
    "print(f\"Instead, we learn a linear projection from pixel space to embedding space.\")"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Math Patches\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_math_patches.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_math_patches"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: The Mathematics\n",
    "\n",
    "Let us walk through the math carefully. After every equation, we will translate it into concrete computation.\n",
    "\n",
    "### 3.1 How Many Patches?\n",
    "\n",
    "Given an image of height $H$ and width $W$, divided into non-overlapping patches of size $P \\times P$:\n",
    "\n",
    "$$N = \\frac{H}{P} \\times \\frac{W}{P} = \\left(\\frac{H}{P}\\right)^2 \\quad \\text{(for square images)}$$\n",
    "\n",
    "**Computationally:** Divide the image height by the patch size and square it. For the standard ViT-Base configuration with 224x224 images and 16x16 patches:\n",
    "\n",
    "$$N = \\left(\\frac{224}{16}\\right)^2 = 14^2 = 196 \\text{ patches}$$\n",
    "\n",
    "For our CIFAR-10 implementation (32x32 images, 4x4 patches):\n",
    "\n",
    "$$N = \\left(\\frac{32}{4}\\right)^2 = 8^2 = 64 \\text{ patches}$$\n",
    "\n",
    "Each of these patches becomes one \"token\" in our sequence â€” analogous to one word in a sentence."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Flattening a Patch\n",
    "\n",
    "Each patch is a small image of size $P \\times P \\times C$ (where $C$ is the number of channels â€” 3 for RGB). We flatten it into a single vector:\n",
    "\n",
    "$$\\mathbf{x}_p^i \\in \\mathbb{R}^{P^2 \\cdot C}$$\n",
    "\n",
    "**Computationally:** Take each 16x16x3 patch and unroll it into a single vector. The dimension is $16 \\times 16 \\times 3 = 768$. For our CIFAR-10 setup: $4 \\times 4 \\times 3 = 48$.\n",
    "\n",
    "Think of it like this: you are reading all the pixel values of a patch from left to right, top to bottom, channel by channel, and writing them into one long list.\n",
    "\n",
    "Let us verify these dimensions in code."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify patch dimensions for different configurations\n",
    "configs = [\n",
    "    (\"ViT-Base (ImageNet)\", 224, 16, 3),\n",
    "    (\"Our CIFAR-10 setup\", 32, 4, 3),\n",
    "    (\"Toy example\", 8, 4, 1),\n",
    "]\n",
    "\n",
    "print(f\"{'Config':<25} {'Patches':>8} {'Patch Dim':>10} {'Seq Len':>8}\")\n",
    "print(\"-\" * 55)\n",
    "for name, img, p, c in configs:\n",
    "    n = (img // p) ** 2\n",
    "    dim = p * p * c\n",
    "    print(f\"{name:<25} {n:>8} {dim:>10} {n+1:>8}\")"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Math Projection\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_math_projection.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_math_projection"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Linear Projection (Patch Embedding)\n",
    "\n",
    "The raw flattened patch vector has dimension $P^2 \\cdot C$. We project it into the model's hidden dimension $D$ using a learnable weight matrix:\n",
    "\n",
    "$$\\mathbf{z}_0^i = \\mathbf{x}_p^i \\, \\mathbf{E} + \\mathbf{b}$$\n",
    "\n",
    "where $\\mathbf{E} \\in \\mathbb{R}^{(P^2 \\cdot C) \\times D}$ and $\\mathbf{b} \\in \\mathbb{R}^D$.\n",
    "\n",
    "**Computationally:** This is a single matrix multiplication â€” exactly `nn.Linear(patch_dim, embed_dim)`. It takes our 768-dimensional raw pixel vector and maps it into, say, a 512-dimensional embedding space. The weight matrix $\\mathbf{E}$ is learned during training, so the model discovers which combinations of pixel values are important.\n",
    "\n",
    "Does this remind you of something? In NLP, a word embedding layer maps a one-hot word vector into a dense embedding. Here, instead of one-hot vectors, we have raw pixel vectors â€” but the idea is identical."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 The Full Input Sequence\n",
    "\n",
    "The complete input to the Transformer is:\n",
    "\n",
    "$$\\mathbf{z}_0 = [\\mathbf{x}_{\\text{cls}};\\; \\mathbf{x}_p^1 \\mathbf{E};\\; \\mathbf{x}_p^2 \\mathbf{E};\\; \\ldots;\\; \\mathbf{x}_p^N \\mathbf{E}] + \\mathbf{e}_{\\text{pos}}$$\n",
    "\n",
    "where $\\mathbf{x}_{\\text{cls}} \\in \\mathbb{R}^D$ is the learnable [CLS] token and $\\mathbf{e}_{\\text{pos}} \\in \\mathbb{R}^{(N+1) \\times D}$ are the position embeddings.\n",
    "\n",
    "**Computationally:** Four steps:\n",
    "1. Flatten each patch into a vector (reshape)\n",
    "2. Project each vector through the linear layer (matrix multiply)\n",
    "3. Prepend the [CLS] token to get $N+1$ tokens\n",
    "4. Add position embeddings (element-wise addition)\n",
    "\n",
    "The output shape is $(B, N+1, D)$ â€” a batch of sequences, each with $N+1$ tokens of dimension $D$."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Toy Example\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_toy_example.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_toy_example"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Toy Example: Walking Through the Math\n",
    "\n",
    "Let us trace through a tiny example to make this completely concrete.\n",
    "\n",
    "**Setup:** 8x8 grayscale image, patch size 4."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy example: 8x8 grayscale image, patch size 4\n",
    "H, W, C = 8, 8, 1\n",
    "P = 4\n",
    "D = 6  # tiny embedding dimension for illustration\n",
    "\n",
    "# Number of patches\n",
    "N = (H // P) * (W // P)\n",
    "patch_dim = P * P * C\n",
    "\n",
    "print(f\"Image size: {H}x{W}x{C}\")\n",
    "print(f\"Patch size: {P}x{P}\")\n",
    "print(f\"Number of patches: N = ({H}/{P})^2 = {N}\")\n",
    "print(f\"Flattened patch dimension: {P}x{P}x{C} = {patch_dim}\")\n",
    "print(f\"Embedding dimension: D = {D}\")\n",
    "print(f\"Projection matrix E shape: ({patch_dim}, {D})\")\n",
    "print(f\"Position embeddings shape: ({N}+1, {D}) = ({N+1}, {D})\")\n",
    "print(f\"\\nFinal output shape: (batch, {N+1}, {D})\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a tiny image with recognizable values so we can trace each step by hand."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tiny 8x8 grayscale \"image\" with recognizable values\n",
    "toy_image = torch.arange(64, dtype=torch.float32).reshape(1, 1, 8, 8)\n",
    "print(\"Our 8x8 'image' (values 0-63):\")\n",
    "print(toy_image.squeeze().numpy().astype(int))"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract 4 patches of size 4x4\n",
    "patches = toy_image.unfold(2, P, P).unfold(3, P, P)  # (1, 1, 2, 2, 4, 4)\n",
    "patches = patches.contiguous().reshape(1, N, patch_dim)  # (1, 4, 16)\n",
    "\n",
    "print(f\"Patches shape: {patches.shape}  ->  (batch=1, N={N}, patch_dim={patch_dim})\")\n",
    "print()\n",
    "for i in range(N):\n",
    "    row, col = i // (W // P), i % (W // P)\n",
    "    print(f\"Patch {i} (row={row}, col={col}): {patches[0, i].numpy().astype(int)}\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how Patch 0 contains values 0-7 and 8-15 (the top-left 4x4 block), while Patch 3 contains values 36-39, 44-47, 52-55, 60-63 (the bottom-right block). The patches tile the image perfectly.\n",
    "\n",
    "Now let us project, prepend [CLS], and add position embeddings."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Linear projection from patch_dim=16 to D=6\n",
    "projection = nn.Linear(patch_dim, D, bias=False)\n",
    "projected = projection(patches)  # (1, 4, 6)\n",
    "print(f\"After projection: {projected.shape}  ->  (batch=1, N={N}, D={D})\")\n",
    "print(f\"Weight matrix E shape: {projection.weight.shape}\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Prepend [CLS] token\n",
    "cls_token = nn.Parameter(torch.randn(1, 1, D))\n",
    "tokens = torch.cat([cls_token.expand(1, -1, -1), projected], dim=1)\n",
    "print(f\"After prepending [CLS]: {tokens.shape}  ->  (batch=1, N+1={N+1}, D={D})\")\n",
    "\n",
    "# Step 4: Add position embeddings\n",
    "pos_embed = nn.Parameter(torch.randn(1, N + 1, D))\n",
    "output = tokens + pos_embed\n",
    "print(f\"After adding position embeddings: {output.shape}\")\n",
    "print(f\"\\nFinal output: {output.shape} â€” ready for the Transformer encoder!\")"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Cifar Patches\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_cifar_patches.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_cifar_patches"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Let Us Build It â€” Component by Component\n",
    "\n",
    "Now we move from toy examples to real images. We will use CIFAR-10 (32x32 RGB images) and build each component one at a time, with visualizations after each step."
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts to [0, 1] range\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Grab a batch of images\n",
    "dataloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=False)\n",
    "images, labels = next(iter(dataloader))\n",
    "\n",
    "# Class names for display\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(f\"Batch shape: {images.shape}\")  # (16, 3, 32, 32)\n",
    "print(f\"First few labels: {[class_names[l] for l in labels[:5].tolist()]}\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Extracting Patches\n",
    "\n",
    "Our first task: take an image of shape $(C, H, W)$ and chop it into a grid of non-overlapping patches. Each patch is a small $P \\times P \\times C$ sub-image."
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(images, patch_size):\n",
    "    \"\"\"\n",
    "    Extract non-overlapping patches from a batch of images.\n",
    "\n",
    "    Args:\n",
    "        images: Tensor of shape (B, C, H, W)\n",
    "        patch_size: Side length of each square patch\n",
    "\n",
    "    Returns:\n",
    "        patches: Tensor of shape (B, N, patch_dim)\n",
    "            where N = (H/P)*(W/P) and patch_dim = P*P*C\n",
    "    \"\"\"\n",
    "    B, C, H, W = images.shape\n",
    "    P = patch_size\n",
    "    assert H % P == 0 and W % P == 0, \\\n",
    "        f\"Image size ({H}x{W}) must be divisible by patch size ({P})\"\n",
    "\n",
    "    n_h, n_w = H // P, W // P\n",
    "    N = n_h * n_w\n",
    "\n",
    "    # (B, C, H, W) -> (B, C, n_h, P, n_w, P) -> (B, n_h, n_w, C, P, P) -> (B, N, P*P*C)\n",
    "    patches = images.reshape(B, C, n_h, P, n_w, P)\n",
    "    patches = patches.permute(0, 2, 4, 1, 3, 5)\n",
    "    patches = patches.reshape(B, N, -1)\n",
    "    return patches"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract patches from our CIFAR-10 images\n",
    "PATCH_SIZE = 4\n",
    "patches = extract_patches(images, PATCH_SIZE)\n",
    "\n",
    "B, N, patch_dim = patches.shape\n",
    "n_h = n_w = int(N ** 0.5)\n",
    "\n",
    "print(f\"Input images:  {images.shape}\")         # (16, 3, 32, 32)\n",
    "print(f\"Patches:       {patches.shape}\")         # (16, 64, 48)\n",
    "print(f\"Grid:          {n_h} x {n_w} = {N} patches\")\n",
    "print(f\"Patch dim:     {PATCH_SIZE}x{PATCH_SIZE}x3 = {patch_dim}\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Patch Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_patch_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_patch_viz"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at that! Our 32x32 image is now a sequence of 64 tokens, each with 48 dimensions. Let us visualize what these patches look like."
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: original image with its patch grid overlay\n",
    "sample_idx = 0\n",
    "sample_image = images[sample_idx]  # (3, 32, 32)\n",
    "sample_patches = patches[sample_idx]  # (64, 48)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.imshow(sample_image.permute(1, 2, 0).numpy())\n",
    "ax.set_title(f\"Original: {class_names[labels[sample_idx]]}\", fontsize=14)\n",
    "\n",
    "# Draw grid lines showing patch boundaries\n",
    "for i in range(1, n_h):\n",
    "    ax.axhline(y=i * PATCH_SIZE - 0.5, color='white', linewidth=1, alpha=0.8)\n",
    "    ax.axvline(x=i * PATCH_SIZE - 0.5, color='white', linewidth=1, alpha=0.8)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "plt.show()"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display each patch individually in a grid â€” each is one \"token\"\n",
    "fig, axes = plt.subplots(n_h, n_w, figsize=(8, 8))\n",
    "fig.suptitle(\"Individual Patches (each is one 'token')\", fontsize=14, y=1.02)\n",
    "\n",
    "for i in range(n_h):\n",
    "    for j in range(n_w):\n",
    "        patch_idx = i * n_w + j\n",
    "        # Reshape flat patch back to image: (48,) -> (3,4,4) -> (4,4,3)\n",
    "        patch_img = sample_patches[patch_idx].reshape(3, PATCH_SIZE, PATCH_SIZE)\n",
    "        patch_img = patch_img.permute(1, 2, 0).numpy()\n",
    "        patch_img = np.clip(patch_img, 0, 1)\n",
    "\n",
    "        axes[i, j].imshow(patch_img)\n",
    "        axes[i, j].set_xticks([])\n",
    "        axes[i, j].set_yticks([])\n",
    "        axes[i, j].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Each patch is a {PATCH_SIZE}x{PATCH_SIZE}x3 = {patch_dim}-dimensional vector.\")\n",
    "print(\"These are our 'words' â€” the input tokens to the Transformer.\")"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Linear Vs Conv\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_linear_vs_conv.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_11_linear_vs_conv"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Flattening and Linear Projection\n",
    "\n",
    "The patches are already flattened (that is what `extract_patches` returns). Now we need to project them from the raw pixel dimension ($P^2 \\cdot C = 48$) into the model's hidden dimension $D$.\n",
    "\n",
    "There are two equivalent ways to do this:\n",
    "1. **Manual reshape + `nn.Linear`** â€” conceptually clear\n",
    "2. **`nn.Conv2d` with kernel_size=P, stride=P** â€” computationally efficient\n",
    "\n",
    "Let us implement both and verify they are equivalent."
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EMBED_DIM = 128  # Model hidden dimension D\n",
    "\n",
    "# Approach 1: nn.Linear on flattened patches\n",
    "linear_proj = nn.Linear(patch_dim, EMBED_DIM)\n",
    "\n",
    "# Project the patches\n",
    "patch_embeddings_linear = linear_proj(patches)  # (B, N, D)\n",
    "print(f\"Linear projection output: {patch_embeddings_linear.shape}\")"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2: nn.Conv2d with kernel_size=patch_size, stride=patch_size\n",
    "# This extracts patches AND projects them in one operation!\n",
    "conv_proj = nn.Conv2d(\n",
    "    in_channels=3,\n",
    "    out_channels=EMBED_DIM,\n",
    "    kernel_size=PATCH_SIZE,\n",
    "    stride=PATCH_SIZE,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "# Apply to original images (not flattened patches)\n",
    "conv_output = conv_proj(images)  # (B, D, n_h, n_w)\n",
    "# Reshape to sequence: (B, D, n_h, n_w) -> (B, N, D)\n",
    "patch_embeddings_conv = conv_output.flatten(2).transpose(1, 2)\n",
    "print(f\"Conv2d projection output: {patch_embeddings_conv.shape}\")"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us prove these two approaches are mathematically equivalent by copying the weights from one to the other and comparing outputs."
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify equivalence: copy Linear weights into Conv2d\n",
    "with torch.no_grad():\n",
    "    # Conv2d weight: (D, C, P, P) = (128, 3, 4, 4)\n",
    "    # Linear weight: (D, P*P*C)  = (128, 48)\n",
    "    # Same parameters, just reshaped\n",
    "    conv_proj.weight.copy_(\n",
    "        linear_proj.weight.reshape(EMBED_DIM, 3, PATCH_SIZE, PATCH_SIZE)\n",
    "    )\n",
    "    conv_proj.bias.copy_(linear_proj.bias)\n",
    "\n",
    "conv_out = conv_proj(images).flatten(2).transpose(1, 2)\n",
    "linear_out = linear_proj(patches)\n",
    "\n",
    "diff = (conv_out - linear_out).abs().max().item()\n",
    "print(f\"Max difference: {diff:.2e}\")\n",
    "print(\"They are equivalent!\" if diff < 1e-5 else \"Something went wrong.\")"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Conv2d approach is preferred in practice because it is slightly more memory-efficient â€” it avoids creating an intermediate tensor of flattened patches. But conceptually, it is doing exactly the same thing: extracting non-overlapping patches and multiplying each by a weight matrix.\n",
    "\n",
    "Now let us visualize how the linear projection reshapes the distribution of patch vectors."
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Pca Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_pca_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_12_pca_viz"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA to visualize high-dimensional vectors in 2D\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "raw_vecs = patches[0].detach().numpy()        # (64, 48)\n",
    "proj_vecs = patch_embeddings_linear[0].detach().numpy()  # (64, 128)\n",
    "\n",
    "pca_raw = PCA(n_components=2).fit_transform(raw_vecs)\n",
    "pca_proj = PCA(n_components=2).fit_transform(proj_vecs)\n",
    "\n",
    "# Color by grid position (top-left=dark, bottom-right=light)\n",
    "colors = np.arange(N)"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(pca_raw[:, 0], pca_raw[:, 1],\n",
    "                c=colors, cmap='viridis', s=60, edgecolors='k', linewidth=0.5)\n",
    "axes[0].set_title(\"Raw Patch Vectors (PCA to 2D)\", fontsize=13)\n",
    "axes[0].set_xlabel(\"PC 1\")\n",
    "axes[0].set_ylabel(\"PC 2\")\n",
    "\n",
    "sc = axes[1].scatter(pca_proj[:, 0], pca_proj[:, 1],\n",
    "                     c=colors, cmap='viridis', s=60, edgecolors='k', linewidth=0.5)\n",
    "axes[1].set_title(\"Projected Patch Vectors (PCA to 2D)\", fontsize=13)\n",
    "axes[1].set_xlabel(\"PC 1\")\n",
    "axes[1].set_ylabel(\"PC 2\")\n",
    "\n",
    "plt.colorbar(sc, ax=axes[1], label=\"Patch index (top-left to bottom-right)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Notice how the linear projection reshapes the distribution â€”\")\n",
    "print(\"it learns to spread patches in ways useful for attention.\")"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Cls And Position\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_cls_and_position.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_13_cls_and_position"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The [CLS] Token\n",
    "\n",
    "In the original ViT paper, a special learnable **[CLS] token** is prepended to the patch sequence. This token does not correspond to any image patch. Instead, it serves as a **global summary** of the entire image.\n",
    "\n",
    "Why? At the end of the Transformer encoder, the representation at the [CLS] position is used for classification. Through self-attention, this token can attend to all patches and aggregate their information."
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The [CLS] token is a learnable parameter vector of dimension D\n",
    "cls_token = nn.Parameter(torch.randn(1, 1, EMBED_DIM))\n",
    "\n",
    "print(f\"[CLS] token shape: {cls_token.shape}\")\n",
    "print(f\"Initialized to random values (will be learned during training)\")\n",
    "print(f\"First few values: {cls_token.data[0, 0, :8].numpy().round(3)}\")"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepend [CLS] to the sequence of patch embeddings\n",
    "batch_size = patch_embeddings_linear.shape[0]\n",
    "cls_tokens = cls_token.expand(batch_size, -1, -1)  # (B, 1, D)\n",
    "\n",
    "# Concatenate: [CLS] + patch embeddings\n",
    "tokens = torch.cat([cls_tokens, patch_embeddings_linear], dim=1)\n",
    "\n",
    "print(f\"Patch embeddings shape: {patch_embeddings_linear.shape}\")  # (B, 64, 128)\n",
    "print(f\"After prepending [CLS]:  {tokens.shape}\")  # (B, 65, 128)\n",
    "print(f\"\\nSequence length went from {N} to {N+1}\")\n",
    "print(f\"Token 0 = [CLS] (global summary)\")\n",
    "print(f\"Tokens 1-{N} = patch embeddings\")"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [CLS] token starts as random noise, but during training it learns to become a powerful aggregator. Through self-attention, it can \"query\" every patch in the image and compose a holistic representation. This is a direct analog of BERT's [CLS] token in NLP.\n",
    "\n",
    "### 4.4 Position Embeddings\n",
    "\n",
    "Without position information, the Transformer would treat our patch sequence as a **bag of patches** â€” it would have no idea that Patch 0 is in the top-left and Patch 63 is in the bottom-right. Self-attention is permutation-equivariant by design.\n",
    "\n",
    "Position embeddings fix this. We add a learnable vector to each token that encodes its position in the sequence."
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learnable position embeddings: one per token (including [CLS])\n",
    "num_tokens = N + 1  # 64 patches + 1 [CLS] = 65\n",
    "pos_embedding = nn.Parameter(torch.randn(1, num_tokens, EMBED_DIM))\n",
    "\n",
    "print(f\"Position embeddings shape: {pos_embedding.shape}\")  # (1, 65, 128)\n",
    "print(f\"One embedding for [CLS] + one for each of the {N} patches\")"
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add position embeddings to the token sequence (element-wise)\n",
    "embedded_tokens = tokens + pos_embedding\n",
    "\n",
    "print(f\"Before position embeddings: {tokens.shape}\")\n",
    "print(f\"After position embeddings:  {embedded_tokens.shape}\")\n",
    "print(f\"\\nShape unchanged â€” position info is encoded by shifting\")\n",
    "print(f\"each token's vector in embedding space.\")"
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Pos Embed Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_pos_embed_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_14_pos_embed_viz"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualize the position embeddings. Even though they are randomly initialized (untrained), the cosine similarity matrix will show us what structure we expect to emerge after training."
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity between all pairs of position embeddings\n",
    "pos_emb = pos_embedding.squeeze(0).detach()  # (65, 128)\n",
    "pos_norm = pos_emb / pos_emb.norm(dim=1, keepdim=True)\n",
    "cos_sim = pos_norm @ pos_norm.T  # (65, 65)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "im = ax.imshow(cos_sim.numpy(), cmap='coolwarm', vmin=-1, vmax=1)\n",
    "ax.set_title(\"Position Embedding Cosine Similarity\\n(including [CLS] at index 0)\", fontsize=12)\n",
    "ax.set_xlabel(\"Token position\")\n",
    "ax.set_ylabel(\"Token position\")\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at something more interesting: pick a reference patch and see how similar it is to all other patches in the 2D grid."
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity from one reference patch to all others, reshaped to 2D grid\n",
    "patch_pos = pos_emb[1:]  # Remove [CLS], shape: (64, 128)\n",
    "patch_norm = patch_pos / patch_pos.norm(dim=1, keepdim=True)\n",
    "patch_cos_sim = patch_norm @ patch_norm.T  # (64, 64)\n",
    "\n",
    "ref_idx = N // 2 + n_w // 2  # roughly center patch\n",
    "ref_sim = patch_cos_sim[ref_idx].reshape(n_h, n_w).numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "im = ax.imshow(ref_sim, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "ax.set_title(f\"Similarity of center patch (idx={ref_idx}) to all patches\\n\"\n",
    "             f\"(Random init â€” no spatial structure yet)\", fontsize=12)\n",
    "ax.set_xlabel(\"Patch column\")\n",
    "ax.set_ylabel(\"Patch row\")\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Right now this looks like noise â€” the position embeddings are random.\")\n",
    "print(\"After training, nearby patches will have HIGH similarity,\")\n",
    "print(\"revealing learned 2D structure!\")"
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo1\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/15_todo1.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_15_todo1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Your Turn\n",
    "\n",
    "Time to test your understanding! Fill in the missing code to build the complete `PatchEmbedding` module.\n",
    "\n",
    "### TODO 1: Complete the PatchEmbedding Forward Pass"
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Converts an image into a sequence of patch embeddings\n",
    "    with [CLS] token and position embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_dim = patch_size * patch_size * in_channels\n",
    "\n",
    "        # Linear projection from flattened patch to embedding\n",
    "        self.projection = nn.Linear(self.patch_dim, embed_dim)\n",
    "\n",
    "        # Learnable [CLS] token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "\n",
    "        # Learnable position embeddings\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, self.num_patches + 1, embed_dim)\n",
    "        )"
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # (continued from PatchEmbedding class above)\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass: (B, C, H, W) -> (B, num_patches+1, embed_dim)\"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        P = self.patch_size\n",
    "\n",
    "        # TODO: Fill in the forward pass\n",
    "        # Hint 1: x.reshape(B, C, H//P, P, W//P, P)\n",
    "        #   -> permute to (B, H//P, W//P, C, P, P)\n",
    "        #   -> reshape to (B, N, patch_dim)\n",
    "        # Hint 2: Project with self.projection\n",
    "        # Hint 3: self.cls_token.expand(B, -1, -1)\n",
    "        #   -> torch.cat([cls_tokens, ...], dim=1)\n",
    "        # Hint 4: Add self.pos_embedding\n",
    "\n",
    "        output = ???  # Your implementation here\n",
    "\n",
    "        return output"
   ],
   "id": "cell_51"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification Cell for TODO 1\n",
    "\n",
    "Run this cell to check your implementation. Do not modify it."
   ],
   "id": "cell_52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification for TODO 1\n",
    "try:\n",
    "    model_test = PatchEmbedding(\n",
    "        img_size=32, patch_size=4, in_channels=3, embed_dim=128\n",
    "    )\n",
    "    test_input = torch.randn(2, 3, 32, 32)\n",
    "    test_output = model_test(test_input)\n",
    "\n",
    "    expected_shape = (2, 65, 128)  # batch=2, 64+1 tokens, D=128\n",
    "    assert test_output.shape == expected_shape, \\\n",
    "        f\"Shape mismatch: got {test_output.shape}, expected {expected_shape}\"\n",
    "\n",
    "    # Check gradients flow\n",
    "    loss = test_output.sum()\n",
    "    loss.backward()\n",
    "    assert model_test.projection.weight.grad is not None\n",
    "    assert model_test.cls_token.grad is not None\n",
    "    assert model_test.pos_embedding.grad is not None\n",
    "\n",
    "    print(\"TODO 1 PASSED!\")\n",
    "    print(f\"   Output shape: {test_output.shape}\")\n",
    "    print(f\"   Gradients flow to all learnable parameters.\")\n",
    "except Exception as e:\n",
    "    print(f\"TODO 1 FAILED: {e}\")\n",
    "    print(\"   Review your implementation and try again.\")"
   ],
   "id": "cell_53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo2\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/16_todo2.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_16_todo2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Patch Extraction with Conv2d\n",
    "\n",
    "Implement the patch extraction using `nn.Conv2d` instead of manual reshaping. The key insight: a Conv2d with `kernel_size=P` and `stride=P` extracts non-overlapping patches and projects them simultaneously."
   ],
   "id": "cell_54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddingConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Same as PatchEmbedding, but uses Conv2d for extraction + projection.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # ============================================\n",
    "        # TODO: Create a Conv2d that extracts patches and projects\n",
    "        # them in one operation.\n",
    "        # Hint: kernel_size and stride should both be patch_size.\n",
    "        #       out_channels should be embed_dim.\n",
    "        # ============================================\n",
    "        self.projection = ???  # Your Conv2d layer here\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, self.num_patches + 1, embed_dim)\n",
    "        )"
   ],
   "id": "cell_55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # (continued from PatchEmbeddingConv class above)\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # ============================================\n",
    "        # TODO: Apply Conv2d, reshape to (B, N, D),\n",
    "        # prepend [CLS], add position embeddings.\n",
    "        #\n",
    "        # Hint: Conv2d output: (B, embed_dim, n_h, n_w)\n",
    "        #   -> .flatten(2) -> (B, embed_dim, N)\n",
    "        #   -> .transpose(1, 2) -> (B, N, embed_dim)\n",
    "        # ============================================\n",
    "        output = ???  # Your implementation here\n",
    "\n",
    "        return output"
   ],
   "id": "cell_56"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification Cell for TODO 2"
   ],
   "id": "cell_57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification for TODO 2\n",
    "try:\n",
    "    model_conv = PatchEmbeddingConv(\n",
    "        img_size=32, patch_size=4, in_channels=3, embed_dim=128\n",
    "    )\n",
    "    test_input = torch.randn(2, 3, 32, 32)\n",
    "    test_output = model_conv(test_input)\n",
    "\n",
    "    expected_shape = (2, 65, 128)\n",
    "    assert test_output.shape == expected_shape, \\\n",
    "        f\"Shape mismatch: got {test_output.shape}, expected {expected_shape}\"\n",
    "\n",
    "    conv_layer = model_conv.projection\n",
    "    assert isinstance(conv_layer, nn.Conv2d), \"projection should be nn.Conv2d\"\n",
    "    assert conv_layer.kernel_size == (4, 4), \\\n",
    "        f\"kernel_size should be (4,4), got {conv_layer.kernel_size}\"\n",
    "    assert conv_layer.stride == (4, 4), \\\n",
    "        f\"stride should be (4,4), got {conv_layer.stride}\"\n",
    "\n",
    "    print(\"TODO 2 PASSED!\")\n",
    "    print(f\"   Output shape: {test_output.shape}\")\n",
    "    print(f\"   Conv2d kernel: {conv_layer.kernel_size}, stride: {conv_layer.stride}\")\n",
    "except Exception as e:\n",
    "    print(f\"TODO 2 FAILED: {e}\")\n",
    "    print(\"   Review your Conv2d parameters and try again.\")"
   ],
   "id": "cell_58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Complete Module\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/17_complete_module.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_17_complete_module"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Putting It All Together\n",
    "\n",
    "Here is the complete, clean `PatchEmbedding` class â€” our reference implementation that combines everything we have built."
   ],
   "id": "cell_59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddingComplete(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete patch embedding module for Vision Transformers.\n",
    "\n",
    "    Takes an image tensor and produces a sequence of patch embeddings\n",
    "    with a prepended [CLS] token and learned position embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Conv2d-based patch extraction + projection\n",
    "        self.projection = nn.Conv2d(\n",
    "            in_channels, embed_dim,\n",
    "            kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, self.num_patches + 1, embed_dim)\n",
    "        )\n",
    "        self._init_weights()"
   ],
   "id": "cell_60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # (continued from PatchEmbeddingComplete)\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Xavier uniform initialization for stable training.\"\"\"\n",
    "        nn.init.xavier_uniform_(self.projection.weight.view(\n",
    "            self.projection.weight.size(0), -1\n",
    "        ))\n",
    "        nn.init.zeros_(self.projection.bias)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.pos_embedding, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # Extract patches and project via Conv2d: (B,C,H,W) -> (B,D,n_h,n_w)\n",
    "        patch_embeddings = self.projection(x)\n",
    "\n",
    "        # Reshape to sequence: (B,D,n_h,n_w) -> (B,N,D)\n",
    "        patch_embeddings = patch_embeddings.flatten(2).transpose(1, 2)\n",
    "\n",
    "        # Prepend [CLS] token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        tokens = torch.cat([cls_tokens, patch_embeddings], dim=1)\n",
    "\n",
    "        # Add position embeddings\n",
    "        return tokens + self.pos_embedding"
   ],
   "id": "cell_61"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us feed a batch of real CIFAR-10 images through and trace the shape at every stage."
   ],
   "id": "cell_62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and run\n",
    "model = PatchEmbeddingComplete(\n",
    "    img_size=32, patch_size=4, in_channels=3, embed_dim=128\n",
    ")\n",
    "output = model(images)\n",
    "\n",
    "# Shape trace\n",
    "print(\"=\" * 60)\n",
    "print(\"Shape Trace Through PatchEmbeddingComplete\")\n",
    "print(\"=\" * 60)\n",
    "with torch.no_grad():\n",
    "    x = images\n",
    "    print(f\"Input images:            {x.shape}\")\n",
    "    proj = model.projection(x)\n",
    "    print(f\"After Conv2d:            {proj.shape}\")\n",
    "    flat = proj.flatten(2).transpose(1, 2)\n",
    "    print(f\"After flatten+transpose: {flat.shape}\")\n",
    "    cls = model.cls_token.expand(x.shape[0], -1, -1)\n",
    "    cat = torch.cat([cls, flat], dim=1)\n",
    "    print(f\"After [CLS] prepend:     {cat.shape}\")\n",
    "    final = cat + model.pos_embedding\n",
    "    print(f\"After pos embedding:     {final.shape}\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "cell_63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter count\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\nFinal output: {output.shape}\")\n",
    "print(f\"  {output.shape[0]} images, each as {output.shape[1]} tokens \"\n",
    "      f\"(1 [CLS] + {model.num_patches} patches), dim {output.shape[2]}\")\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"  Conv2d projection: \"\n",
    "      f\"{sum(p.numel() for p in model.projection.parameters()):,}\")\n",
    "print(f\"  [CLS] token:       {model.cls_token.numel():,}\")\n",
    "print(f\"  Position embed:    {model.pos_embedding.numel():,}\")\n",
    "print(f\"\\nThis is ready to feed into a Transformer encoder!\")"
   ],
   "id": "cell_64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Final Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/18_final_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_18_final_viz"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Final Output\n",
    "\n",
    "Let us create a comprehensive visualization of our complete patch embedding pipeline."
   ],
   "id": "cell_65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a visually interesting image\n",
    "sample_idx = 7  # Try different indices!\n",
    "sample_image = images[sample_idx]\n",
    "sample_label = class_names[labels[sample_idx]]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_output = model(sample_image.unsqueeze(0))\n",
    "\n",
    "print(f\"Selected image: {sample_label}\")\n",
    "print(f\"Output shape: {sample_output.shape}\")"
   ],
   "id": "cell_66"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Panel 1:** Original image with patch grid overlay."
   ],
   "id": "cell_67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "img_np = np.clip(sample_image.permute(1, 2, 0).numpy(), 0, 1)\n",
    "ax.imshow(img_np)\n",
    "\n",
    "# Draw patch grid\n",
    "for i in range(1, n_h):\n",
    "    ax.axhline(y=i * PATCH_SIZE - 0.5, color='yellow', linewidth=1.5, alpha=0.9)\n",
    "    ax.axvline(x=i * PATCH_SIZE - 0.5, color='yellow', linewidth=1.5, alpha=0.9)\n",
    "\n",
    "# Label select patches\n",
    "for i in range(n_h):\n",
    "    for j in range(n_w):\n",
    "        if (i + j) % 3 == 0:\n",
    "            num = i * n_w + j\n",
    "            ax.text(j * PATCH_SIZE + PATCH_SIZE/2,\n",
    "                    i * PATCH_SIZE + PATCH_SIZE/2,\n",
    "                    str(num), ha='center', va='center',\n",
    "                    fontsize=6, color='yellow', fontweight='bold')\n",
    "\n",
    "ax.set_title(f\"Original Image ({n_h}x{n_w} = {N} patches)\", fontsize=13)\n",
    "ax.set_xticks([]); ax.set_yticks([])\n",
    "plt.show()"
   ],
   "id": "cell_68"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Panel 2:** Individual patches displayed in a grid."
   ],
   "id": "cell_69"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(n_h, n_w, figsize=(8, 8))\n",
    "fig.suptitle(\"64 Patches â€” Each Becomes One Token\", fontsize=14, y=1.02)\n",
    "\n",
    "sp = extract_patches(sample_image.unsqueeze(0), PATCH_SIZE)[0]\n",
    "\n",
    "for i in range(n_h):\n",
    "    for j in range(n_w):\n",
    "        idx = i * n_w + j\n",
    "        p_img = sp[idx].reshape(3, PATCH_SIZE, PATCH_SIZE)\n",
    "        p_img = np.clip(p_img.permute(1, 2, 0).numpy(), 0, 1)\n",
    "        axes[i, j].imshow(p_img)\n",
    "        axes[i, j].set_xticks([])\n",
    "        axes[i, j].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_70"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Panel 3:** Position embedding cosine similarity heatmaps. We show how each patch's position embedding relates to all other patches."
   ],
   "id": "cell_71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute patch-only position embedding similarities\n",
    "pos_emb_final = model.pos_embedding.squeeze(0).detach()\n",
    "patch_pos_emb = pos_emb_final[1:]  # (64, 128) â€” exclude [CLS]\n",
    "patch_pos_norm = patch_pos_emb / patch_pos_emb.norm(dim=1, keepdim=True)\n",
    "patch_cos_sim = patch_pos_norm @ patch_pos_norm.T\n",
    "\n",
    "# 8 reference patches at diverse grid positions\n",
    "ref_positions = [0, 3, 7, 24, 31, 39, 56, 63]\n",
    "ref_labels = [\"Top-left\", \"Top-mid\", \"Top-right\", \"Mid-left\",\n",
    "              \"Center\", \"Mid-right\", \"Bot-left\", \"Bot-right\"]"
   ],
   "id": "cell_72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(16, 7))\n",
    "fig.suptitle(\n",
    "    \"Position Embedding Similarities\\n\"\n",
    "    \"(Random init â€” 2D structure emerges after training)\",\n",
    "    fontsize=14, y=1.05\n",
    ")\n",
    "\n",
    "for ax_idx, (ref_idx, label) in enumerate(zip(ref_positions, ref_labels)):\n",
    "    row, col = ax_idx // 4, ax_idx % 4\n",
    "    sim_map = patch_cos_sim[ref_idx].reshape(n_h, n_w).numpy()\n",
    "\n",
    "    im = axes[row, col].imshow(sim_map, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    axes[row, col].set_title(f\"{label}\\n(patch {ref_idx})\", fontsize=10)\n",
    "    axes[row, col].set_xticks([])\n",
    "    axes[row, col].set_yticks([])\n",
    "\n",
    "    # Mark the reference patch with a star\n",
    "    r, c = ref_idx // n_w, ref_idx % n_w\n",
    "    axes[row, col].plot(c, r, 'k*', markersize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"  PIPELINE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Input:   {sample_image.shape} image ({sample_label})\")\n",
    "print(f\"  Patches: {N} patches of size {PATCH_SIZE}x{PATCH_SIZE}x3\")\n",
    "print(f\"  Patch dim: {PATCH_SIZE * PATCH_SIZE * 3}\")\n",
    "print(f\"  Embed dim: {EMBED_DIM}\")\n",
    "print(f\"  Output:  {sample_output.shape}\")\n",
    "print(f\"    Token 0:     [CLS] (global summary)\")\n",
    "print(f\"    Tokens 1-{N}: patch embeddings with position info\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Congratulations! You have built the input pipeline\")\n",
    "print(\"for a Vision Transformer!\")\n",
    "print()\n",
    "print(\"This sequence of 65 token embeddings is exactly what gets\")\n",
    "print(\"fed into the Transformer encoder. In Notebook 2, we will\")\n",
    "print(\"build multi-head self-attention to let these tokens\")\n",
    "print(\"communicate with each other.\")"
   ],
   "id": "cell_74"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/19_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_19_closing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "\n",
    "Take a moment to think about these before moving on:\n",
    "\n",
    "1. **Patch size trade-off:** What happens to the number of patches if we use 8x8 patches instead of 4x4 on our 32x32 images? We would get $(32/8)^2 = 16$ patches instead of 64. Fewer patches means a shorter sequence, which means **quadratically less computation** in self-attention (attention is $O(N^2)$). But each patch covers more of the image, so we lose fine-grained spatial detail. This is a fundamental resolution-vs-computation trade-off.\n",
    "\n",
    "2. **Why not raw pixels?** Why do we use a learnable linear projection instead of feeding raw pixel vectors directly to the Transformer? Raw pixel values live in a space that is not aligned with the features the Transformer needs. The linear projection learns to extract useful initial features â€” edges, color gradients, textures â€” that give the Transformer a better starting representation. It is also a dimensionality change: we can project 48-dimensional raw patches into whatever dimension $D$ we want.\n",
    "\n",
    "3. **1D vs 2D position embeddings:** Our position embeddings are 1D â€” one vector per position in the flattened sequence. The model has to **learn** that position 0 and position 1 are horizontally adjacent, while position 0 and position 8 are vertically adjacent. Remarkably, it does learn this! An alternative is 2D position embeddings that explicitly encode (row, column) â€” the original ViT paper tried this and found it performs similarly, suggesting that 1D learnable embeddings are sufficient."
   ],
   "id": "cell_75"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Challenges\n",
    "\n",
    "If you want to go deeper, try these."
   ],
   "id": "cell_76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1: Try different patch sizes\n",
    "# Uncomment and run to see how patch size affects the grid\n",
    "\n",
    "# for ps in [2, 4, 8, 16]:\n",
    "#     if 32 % ps == 0:\n",
    "#         n = (32 // ps) ** 2\n",
    "#         dim = ps * ps * 3\n",
    "#         m = PatchEmbeddingComplete(img_size=32, patch_size=ps, embed_dim=128)\n",
    "#         out = m(images[:1])\n",
    "#         print(f\"Patch size {ps:2d}: {n:4d} patches, \"\n",
    "#               f\"patch_dim={dim:4d}, output={out.shape}\")"
   ],
   "id": "cell_77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 2: Implement 2D sinusoidal position embeddings\n",
    "# In the original Transformer (Vaswani et al.), position embeddings used\n",
    "# sine and cosine functions. Can you adapt that to 2D?\n",
    "#\n",
    "# Hint: Create separate sin/cos embeddings for row and column,\n",
    "# then concatenate. Each position (r, c) gets:\n",
    "#   pos_embed(r,c) = [sin_embed(r); cos_embed(r);\n",
    "#                     sin_embed(c); cos_embed(c)]\n",
    "#\n",
    "# def get_2d_sinusoidal_pos_embed(embed_dim, grid_size):\n",
    "#     \"\"\"\n",
    "#     Args:\n",
    "#         embed_dim: Total dimension (must be divisible by 4)\n",
    "#         grid_size: Patches along each axis\n",
    "#     Returns:\n",
    "#         pos_embed: shape (grid_size*grid_size, embed_dim)\n",
    "#     \"\"\"\n",
    "#     pass  # Your implementation here!"
   ],
   "id": "cell_78"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Next?\n",
    "\n",
    "In **Notebook 2: Multi-Head Self-Attention for Images**, we will build the core mechanism that makes Transformers so powerful. Our 65-token sequence from this notebook will become the input, and attention will let each token \"look at\" every other token to build richer representations.\n",
    "\n",
    "The key question we will answer: **How does a patch in the bottom-right corner of an image learn that it is related to a patch in the top-left corner?** The answer is self-attention â€” and we will build it from scratch.\n",
    "\n",
    "See you there!"
   ],
   "id": "cell_79"
  }
 ]
}