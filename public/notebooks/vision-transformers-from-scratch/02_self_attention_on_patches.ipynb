{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Self-Attention on Image Patches â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1jACsP0mL4_YTew5Kgn2Z9K-lgjFDsGnq\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_setup"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention on Image Patches: How Patches Learn to Talk to Each Other\n",
    "\n",
    "*Part 2 of the Vizuara series on Vision Transformers from Scratch*\n",
    "\n",
    "*Estimated time: 60 minutes*\n",
    "\n",
    "In Notebook 1 we learned how to chop an image into patches and project them into an embedding space. But those patch tokens are still strangers to each other â€” each one knows only about the small rectangle of pixels it came from. Self-attention is the mechanism that lets patches communicate, compare, and build a shared understanding of the entire image.\n",
    "\n",
    "By the end of this notebook, you will have built every component of a Transformer encoder block from scratch â€” and you will **see** attention in action through heatmaps that reveal which patches are paying attention to which."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://pods.vizuara.ai/courses/vision-transformers-from-scratch/practice/2/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_02_why_it_matters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Why Does This Matter?\n",
    "\n",
    "Let us take stock of where we are in our journey.\n",
    "\n",
    "In **Notebook 1**, we turned a 32x32 CIFAR-10 image into a sequence of 64 patch tokens (using 4x4 patches), each represented as a 128-dimensional vector. We also prepended a special `[CLS]` token and added positional embeddings, giving us a sequence of shape `(65, 128)`.\n",
    "\n",
    "But right now, **each patch is an island**. Patch 17 knows it contains some brown fur texture, but it has no idea that Patch 18 next to it also has fur, or that Patch 42 far away contains a dog's ear. The patches cannot see each other.\n",
    "\n",
    "**Self-attention changes everything.** It is the mechanism by which each patch broadcasts a question to every other patch: *\"Hey, how relevant are you to me?\"* Based on the answers, each patch builds a richer, context-aware representation that incorporates information from the entire image.\n",
    "\n",
    "By the end of this notebook, we will:\n",
    "- Understand the Query-Key-Value mechanism intuitively and mathematically\n",
    "- Build scaled dot-product attention from raw matrix multiplications\n",
    "- Implement multi-head attention, the MLP block, and the full Transformer encoder block\n",
    "- Visualize attention heatmaps showing which patches attend to which â€” and watch attention patterns become more semantic in deeper layers"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us set up our environment right away\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All imports ready!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Conference Analogy\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_conference_analogy.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_03_conference_analogy"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Building Intuition\n",
    "\n",
    "Before we touch any code or equations, let us build a strong mental model for what self-attention does.\n",
    "\n",
    "### The Conference Analogy\n",
    "\n",
    "Imagine 65 people standing in a conference hall. Each person holds one piece of a jigsaw puzzle (one image patch). Right now, each person can only see their own piece â€” they have no idea what the full picture looks like.\n",
    "\n",
    "**Self-attention is the networking session.** Each person walks around the room and asks every other person: *\"How relevant is your piece to mine?\"* The person holding a dog's face piece will find that the person holding the dog's body piece is highly relevant, while the person holding a blue sky piece is less so.\n",
    "\n",
    "After this round of conversations, each person updates their understanding. The dog-face person now thinks: *\"I'm a dog face, and I'm connected to a body below me and an ear to my left.\"* They have gone from knowing about a 4x4 pixel patch to understanding their role in the bigger picture."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Qkv Framework\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_qkv_framework.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_04_qkv_framework"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Query-Key-Value Framework\n",
    "\n",
    "Self-attention uses three learned projections â€” Query, Key, and Value â€” and it helps to understand *why* we need three separate things rather than one.\n",
    "\n",
    "- **Query (Q)** = *\"What am I looking for?\"* â€” Like typing a search query into Google. A patch of a dog's face might be looking for body parts, other facial features, or context clues.\n",
    "\n",
    "- **Key (K)** = *\"What do I contain?\"* â€” Like the title or metadata of a webpage. A patch describes itself so others can find it. The dog-body patch advertises: \"I contain fur and torso-like structure.\"\n",
    "\n",
    "- **Value (V)** = *\"Here is my actual content.\"* â€” Like the full text of the webpage. Once a match is found between Query and Key, the Value is what actually gets passed along.\n",
    "\n",
    "**Why three matrices, not one?** Because the question *\"What am I looking for?\"* is fundamentally different from *\"What do I contain.\"* A patch of a dog's face might **look for** body and leg patches (that is its Query), but **describe itself** as containing fur texture and an eye (that is its Key). These are different roles that require different learned projections."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Think About This\n",
    "\n",
    "*In natural language, the word \"it\" needs to attend to different words in the sentence to figure out what \"it\" refers to. What is the equivalent situation in images? When would one patch need to \"look at\" a distant patch to understand itself?*\n",
    "\n",
    "Think about an image of a person throwing a ball. The patch containing the hand needs to attend to the ball patch (even though they may be far apart) to understand the action. The patch containing a shadow on the ground needs to attend to the object casting that shadow. Self-attention gives patches this ability to reach across the image and gather relevant context."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Quadratic Cost\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_quadratic_cost.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_05_quadratic_cost"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick thought experiment: how many patch-to-patch comparisons\n",
    "# does self-attention make?\n",
    "num_patches = 64 + 1  # 64 image patches + 1 CLS token\n",
    "comparisons = num_patches * num_patches\n",
    "print(f\"With {num_patches} tokens, self-attention computes:\")\n",
    "print(f\"  {comparisons:,} pairwise attention scores\")\n",
    "print(f\"  That is a {num_patches}x{num_patches} attention matrix!\")\n",
    "print(f\"\\nThis is why self-attention is O(N^2) â€” and why\")\n",
    "print(f\"efficient attention variants are an active research area.\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Math Qkv\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_math_qkv.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_06_math_qkv"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: The Mathematics\n",
    "\n",
    "Now let us formalize the intuition with precise mathematics. After every equation, we will explain exactly what the computer is doing.\n",
    "\n",
    "### 3.1 Setup\n",
    "\n",
    "Let us say we have a sequence of $N$ patch tokens, each of dimension $d$. We stack them into a matrix $z \\in \\mathbb{R}^{N \\times d}$.\n",
    "\n",
    "### 3.2 Query, Key, Value Projections\n",
    "\n",
    "$$Q = zW^Q, \\quad K = zW^K, \\quad V = zW^V$$\n",
    "\n",
    "where $W^Q, W^K, W^V \\in \\mathbb{R}^{d \\times d_k}$.\n",
    "\n",
    "**What the computer does:** Take our patch embeddings $z$ (shape $N \\times d$) and multiply by three separate weight matrices. This gives each token three different \"views\" of itself â€” a query view, a key view, and a value view. Each view has dimension $d_k$ (typically $d_k = d / h$ where $h$ is the number of attention heads)."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Math Multihead Block\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_math_multihead_block.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_07_math_multihead_block"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Attention Scores and Weights\n",
    "\n",
    "$$\\text{scores} = \\frac{QK^T}{\\sqrt{d_k}}$$\n",
    "\n",
    "**What the computer does:** Compute the dot product of every query with every key. The result is an $N \\times N$ matrix where entry $(i, j)$ measures how much token $i$'s query matches token $j$'s key â€” i.e., how relevant token $j$ is to token $i$. We divide by $\\sqrt{d_k}$ to prevent the scores from becoming too large. When $d_k$ is large, dot products tend to grow in magnitude, which pushes softmax into regions where it produces near-one-hot outputs. This kills gradients and makes training unstable.\n",
    "\n",
    "$$\\alpha = \\text{softmax}(\\text{scores})$$\n",
    "\n",
    "**What the computer does:** Apply softmax along each row, converting raw scores into a probability distribution. After this, each row of $\\alpha$ sums to 1. Entry $\\alpha_{ij}$ tells us what fraction of its attention token $i$ pays to token $j$.\n",
    "\n",
    "### 3.4 Attention Output\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\alpha V$$\n",
    "\n",
    "**What the computer does:** For each token $i$, compute a weighted sum of all value vectors, where the weights are the attention probabilities. If patch $i$ strongly attends to patch $j$ (large $\\alpha_{ij}$), then the output for patch $i$ will be heavily influenced by patch $j$'s value vector.\n",
    "\n",
    "### 3.5 Multi-Head Attention\n",
    "\n",
    "$$\\text{MultiHead}(z) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) \\, W^O$$\n",
    "\n",
    "where each $\\text{head}_i = \\text{Attention}(zW_i^Q, zW_i^K, zW_i^V)$.\n",
    "\n",
    "**What the computer does:** Run $h$ independent attention operations in parallel, each operating on a $d_k$-dimensional slice of the embedding. Then concatenate all the outputs (giving us back dimension $d$) and project through one final matrix $W^O$. Each head can learn a *different* attention pattern â€” one head might focus on spatial neighbors, another on color similarity, another on semantic relationships."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 The Transformer Block\n",
    "\n",
    "$$z'_l = \\text{MHSA}(\\text{LN}(z_{l-1})) + z_{l-1}$$\n",
    "$$z_l = \\text{MLP}(\\text{LN}(z'_l)) + z'_l$$\n",
    "\n",
    "**What the computer does:** This is the pre-norm Transformer block. First, apply LayerNorm, then multi-head self-attention, then add the input back (residual connection). Then apply LayerNorm again, pass through an MLP, and add back again. The residual connections are critical â€” they keep gradients flowing through deep networks and let each layer learn *incremental refinements* rather than full transformations.\n",
    "\n",
    "### 3.7 The MLP\n",
    "\n",
    "$$\\text{MLP}(z) = \\text{GELU}(zW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "where $W_1 \\in \\mathbb{R}^{d \\times 4d}$ and $W_2 \\in \\mathbb{R}^{4d \\times d}$.\n",
    "\n",
    "**What the computer does:** Expand to 4x the dimension (e.g., 128 to 512), apply GELU activation (a smooth variant of ReLU), then compress back to the original dimension. This gives each token a chance to process information through a much higher-dimensional space â€” like temporarily unfolding a piece of paper to write more notes before folding it back up."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check: let us see the dimensions we will be working with\n",
    "print(\"Our ViT configuration for this notebook:\")\n",
    "print(f\"  Image: 32x32, Patch: 4x4\")\n",
    "print(f\"  Number of patches: {(32//4)**2} = 64\")\n",
    "print(f\"  Sequence length (with CLS): 65\")\n",
    "print(f\"  Embedding dim (d): 128\")\n",
    "print(f\"  Num heads (h): 4\")\n",
    "print(f\"  Head dim (d_k = d/h): {128//4}\")\n",
    "print(f\"  MLP hidden dim (4*d): {4*128}\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Toy Example\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_toy_example.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_08_toy_example"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Concrete Toy Example\n",
    "\n",
    "Let us work through attention with 3 patches and $d_k = 2$ to see every step.\n",
    "\n",
    "| Patch | Meaning | Query | Key | Value |\n",
    "|-------|---------|-------|-----|-------|\n",
    "| Patch 1 | Dog face | [1.0, 0.0] | [1.0, 0.0] | [0.9, 0.1] |\n",
    "| Patch 2 | Dog body | [0.8, 0.2] | [0.9, 0.1] | [0.7, 0.3] |\n",
    "| Patch 3 | Sky | [0.0, 1.0] | [0.1, 0.9] | [0.2, 0.8] |\n",
    "\n",
    "**Step 1: Compute scores** $= QK^T / \\sqrt{d_k}$\n",
    "\n",
    "$QK^T = \\begin{bmatrix} 1\\cdot1 + 0\\cdot0 & 1\\cdot0.9 + 0\\cdot0.1 & 1\\cdot0.1 + 0\\cdot0.9 \\\\ 0.8\\cdot1 + 0.2\\cdot0 & 0.8\\cdot0.9 + 0.2\\cdot0.1 & 0.8\\cdot0.1 + 0.2\\cdot0.9 \\\\ 0\\cdot1 + 1\\cdot0 & 0\\cdot0.9 + 1\\cdot0.1 & 0\\cdot0.1 + 1\\cdot0.9 \\end{bmatrix} = \\begin{bmatrix} 1.0 & 0.9 & 0.1 \\\\ 0.8 & 0.74 & 0.26 \\\\ 0.0 & 0.1 & 0.9 \\end{bmatrix}$\n",
    "\n",
    "Dividing by $\\sqrt{2} \\approx 1.414$:\n",
    "\n",
    "$\\text{scores} = \\begin{bmatrix} 0.707 & 0.636 & 0.071 \\\\ 0.566 & 0.523 & 0.184 \\\\ 0.0 & 0.071 & 0.636 \\end{bmatrix}$"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Softmax** (per row)\n",
    "\n",
    "Row 1: $[0.707, 0.636, 0.071] \\rightarrow e^{0.707} = 2.028, \\; e^{0.636} = 1.889, \\; e^{0.071} = 1.074 \\rightarrow$ sum $= 4.991$\n",
    "$\\alpha_1 = [0.406, 0.379, 0.215]$\n",
    "\n",
    "The dog face attends most to itself (0.406), strongly to the dog body (0.379), and least to the sky (0.215). This makes sense â€” the dog face is most similar to itself and the dog body.\n",
    "\n",
    "**Step 3: Weighted sum of values**\n",
    "\n",
    "Output for Patch 1 = $0.406 \\times [0.9, 0.1] + 0.379 \\times [0.7, 0.3] + 0.215 \\times [0.2, 0.8]$\n",
    "$= [0.365, 0.041] + [0.265, 0.114] + [0.043, 0.172]$\n",
    "$= [0.673, 0.327]$\n",
    "\n",
    "The output has been enriched â€” it is no longer just the dog face's value, but a blend weighted toward semantically similar patches.\n",
    "\n",
    "Let us now verify this computation in code."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying the Toy Example\n",
    "\n",
    "Let us confirm our hand calculations with PyTorch."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our toy example: 3 patches, d_k = 2\n",
    "Q = torch.tensor([[1.0, 0.0],    # Dog face query\n",
    "                   [0.8, 0.2],    # Dog body query\n",
    "                   [0.0, 1.0]])   # Sky query\n",
    "\n",
    "K = torch.tensor([[1.0, 0.0],    # Dog face key\n",
    "                   [0.9, 0.1],    # Dog body key\n",
    "                   [0.1, 0.9]])   # Sky key\n",
    "\n",
    "V = torch.tensor([[0.9, 0.1],    # Dog face value\n",
    "                   [0.7, 0.3],    # Dog body value\n",
    "                   [0.2, 0.8]])   # Sky value\n",
    "\n",
    "d_k = Q.shape[-1]"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute scaled scores\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "print(\"Scores (QK^T / sqrt(d_k)):\")\n",
    "print(scores)\n",
    "\n",
    "# Step 2: Softmax\n",
    "attn_weights = F.softmax(scores, dim=-1)\n",
    "print(\"\\nAttention weights (softmax of scores):\")\n",
    "print(attn_weights)\n",
    "\n",
    "# Step 3: Weighted sum of values\n",
    "output = torch.matmul(attn_weights, V)\n",
    "print(\"\\nAttention output:\")\n",
    "print(output)"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Toy Heatmap\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_toy_heatmap.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_09_toy_heatmap"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualize these attention weights as a heatmap."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "labels = ['Dog Face', 'Dog Body', 'Sky']\n",
    "\n",
    "im = ax.imshow(attn_weights.detach().numpy(), cmap='Blues', vmin=0, vmax=1)\n",
    "ax.set_xticks(range(3))\n",
    "ax.set_yticks(range(3))\n",
    "ax.set_xticklabels(labels, fontsize=11)\n",
    "ax.set_yticklabels(labels, fontsize=11)\n",
    "ax.set_xlabel('Attends TO (Keys)', fontsize=12)\n",
    "ax.set_ylabel('Attends FROM (Queries)', fontsize=12)\n",
    "ax.set_title('Attention Weights â€” Toy Example', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Annotate cells with values\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        val = attn_weights[i, j].item()\n",
    "        color = 'white' if val > 0.5 else 'black'\n",
    "        ax.text(j, i, f'{val:.3f}', ha='center', va='center',\n",
    "                fontsize=13, fontweight='bold', color=color)\n",
    "\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the dog face attends strongly to both itself and the dog body, but weakly to the sky. The sky patch, conversely, attends strongly to itself and weakly to the dog patches. This is exactly the behavior we want â€” semantically related patches find each other."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Building Attention\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_building_attention.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_10_building_attention"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Let Us Build It â€” Component by Component\n",
    "\n",
    "Now we build every component of the Transformer encoder from scratch, starting from the lowest level and working up. No black-box `nn.MultiheadAttention` â€” we will implement everything with raw matrix multiplications so we understand every single operation.\n",
    "\n",
    "### 4.1 Scaled Dot-Product Attention (from scratch)\n",
    "\n",
    "This is the core primitive. Everything else is built on top of it."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention from scratch.\n",
    "    Q, K: (..., seq_len, d_k)  V: (..., seq_len, d_v)\n",
    "    Returns: output (..., seq_len, d_v), attn_weights (..., seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "\n",
    "    # Step 1: Compute raw attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    # Step 2: Apply mask if provided (for causal attention)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # Step 3: Softmax to get attention weights\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # Step 4: Weighted sum of values\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "\n",
    "    return output, attn_weights\n",
    "\n",
    "print(\"scaled_dot_product_attention() defined!\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify our function matches the toy example."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "print(\"Our function output:\")\n",
    "print(output)\n",
    "print(\"\\nMatches our hand calculation? \",\n",
    "      torch.allclose(output, torch.matmul(F.softmax(\n",
    "          torch.matmul(Q, K.T) / math.sqrt(2), dim=-1), V), atol=1e-6))"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Attention Head\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_attention_head.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_11_attention_head"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Single Attention Head\n",
    "\n",
    "Now we wrap the attention computation inside a learnable module. An attention head learns its own $W^Q$, $W^K$, $W^V$ matrices."
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A single attention head with learnable Q, K, V projections.\n",
    "\n",
    "    Args:\n",
    "        embed_dim: Input embedding dimension\n",
    "        head_dim: Dimension of this head (typically embed_dim // num_heads)\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        self.W_K = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        self.W_V = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.W_Q(x)  # (batch, seq_len, head_dim)\n",
    "        K = self.W_K(x)\n",
    "        V = self.W_V(x)\n",
    "        output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "        return output, attn_weights\n",
    "\n",
    "print(\"AttentionHead class defined!\")"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test a single attention head with some random patch embeddings."
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate: batch=1, seq_len=65 (64 patches + CLS), embed_dim=128\n",
    "batch_size = 1\n",
    "seq_len = 65   # 64 patches + 1 CLS token\n",
    "embed_dim = 128\n",
    "head_dim = 32  # Will have 4 heads: 128 / 4 = 32\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "head = AttentionHead(embed_dim, head_dim)\n",
    "output, attn_weights = head(x)\n",
    "\n",
    "print(f\"Input shape:            {x.shape}\")\n",
    "print(f\"Output shape:           {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"\\nEach row sums to: {attn_weights[0, 0].sum().item():.4f}\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Attention Visualization\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_attention_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_12_attention_visualization"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize what this random attention head produces. Even with untrained weights, we can see the structure of the attention matrix."
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Full attention matrix\n",
    "ax = axes[0]\n",
    "im = ax.imshow(attn_weights[0].detach().numpy(), cmap='viridis', aspect='auto')\n",
    "ax.set_xlabel('Key position', fontsize=11)\n",
    "ax.set_ylabel('Query position', fontsize=11)\n",
    "ax.set_title('Full Attention Matrix (65x65)\\nSingle Untrained Head',\n",
    "             fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLS token attention (row 0) â€” what does the CLS token attend to?\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "cls_attn = attn_weights[0, 0].detach().numpy()\n",
    "ax.bar(range(len(cls_attn)), cls_attn, color='steelblue', alpha=0.8)\n",
    "ax.set_xlabel('Token position', fontsize=11)\n",
    "ax.set_ylabel('Attention weight', fontsize=11)\n",
    "ax.set_title('[CLS] Token Attention Distribution\\n(Untrained â€” roughly uniform)',\n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.axhline(y=1/seq_len, color='red', linestyle='--',\n",
    "           alpha=0.7, label=f'Uniform: 1/{seq_len}')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With untrained weights, the attention is roughly uniform â€” the CLS token pays about equal attention to every patch. After training, this will become much more focused and semantically meaningful."
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Multihead\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_multihead.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_13_multihead"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Multi-Head Attention\n",
    "\n",
    "This is where the magic happens. Instead of one attention head with a single perspective, we run multiple heads in parallel. Each head can learn to focus on different types of relationships â€” one might track spatial proximity, another color similarity, another shape correspondence."
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Self-Attention from scratch.\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(embed_dim, self.head_dim)\n",
    "            for _ in range(num_heads)])\n",
    "        self.W_O = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        head_outputs, all_attn_weights = [], []\n",
    "        for head in self.heads:\n",
    "            out, attn_w = head(x)\n",
    "            head_outputs.append(out)\n",
    "            all_attn_weights.append(attn_w)\n",
    "        concatenated = torch.cat(head_outputs, dim=-1)\n",
    "        output = self.W_O(concatenated)\n",
    "        return output, all_attn_weights\n",
    "\n",
    "print(\"MultiHeadAttention class defined!\")"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test multi-head attention and inspect the outputs."
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhsa = MultiHeadAttention(embed_dim=128, num_heads=4)\n",
    "x = torch.randn(1, 65, 128)\n",
    "output, all_attn_weights = mhsa(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of heads: {len(all_attn_weights)}\")\n",
    "print(f\"Each head's attention shape: {all_attn_weights[0].shape}\")"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualize the attention patterns from all 4 heads side by side. Even with random weights, each head will have a slightly different pattern because of their independent weight initializations."
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('Attention Patterns from 4 Different Heads\\n'\n",
    "             '(Untrained â€” each head has unique random patterns)',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    attn = all_attn_weights[idx][0].detach().numpy()\n",
    "    im = ax.imshow(attn, cmap='inferno', aspect='auto')\n",
    "    ax.set_title(f'Head {idx + 1}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Key position')\n",
    "    ax.set_ylabel('Query position')\n",
    "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each head produces a different attention matrix. Once trained, these heads specialize: some might focus on local neighborhoods (like a convolution), others on long-range semantic relationships, and others on global structural patterns. This diversity is a key advantage of multi-head attention over single-head."
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Mlp Gelu\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_mlp_gelu.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_14_mlp_gelu"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The MLP Block\n",
    "\n",
    "After the attention layer mixes information *across* tokens, the MLP processes each token *independently*. It expands the representation to a higher dimension (4x), applies a nonlinearity, and compresses back."
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP block for the Transformer encoder.\n",
    "    Two-layer feedforward network with GELU and 4x expansion.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, mlp_ratio=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)        # (batch, seq_len, hidden_dim)\n",
    "        x = self.act(x)        # GELU activation\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)        # (batch, seq_len, embed_dim)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "print(\"MLP class defined!\")\n",
    "print(f\"For embed_dim=128: input -> 128 -> {128*4} -> 128\")"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us quickly compare GELU to ReLU so we understand why modern Transformers prefer GELU."
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = torch.linspace(-4, 4, 200)\n",
    "relu_out = F.relu(x_range)\n",
    "gelu_out = F.gelu(x_range)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "ax.plot(x_range.numpy(), relu_out.numpy(),\n",
    "        label='ReLU', linewidth=2, color='coral')\n",
    "ax.plot(x_range.numpy(), gelu_out.numpy(),\n",
    "        label='GELU', linewidth=2, color='steelblue')\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Input', fontsize=12)\n",
    "ax.set_ylabel('Output', fontsize=12)\n",
    "ax.set_title('GELU vs ReLU', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GELU is smooth everywhere (no sharp corner at 0) and slightly suppresses small negative values rather than zeroing them out. This smoothness helps with gradient flow during training."
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Transformer Block\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/15_transformer_block.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_15_transformer_block"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Full Transformer Encoder Block\n",
    "\n",
    "Now we combine everything: LayerNorm, Multi-Head Self-Attention, the MLP, and residual connections. This is the complete building block that gets stacked to create the full Transformer encoder."
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer encoder block (pre-norm variant).\n",
    "    x -> LN -> MHSA -> +residual -> LN -> MLP -> +residual -> out\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mhsa = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.mlp = MLP(embed_dim, mlp_ratio, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sub-block 1: LayerNorm -> MHSA -> Residual\n",
    "        normed = self.norm1(x)\n",
    "        attn_output, attn_weights = self.mhsa(normed)\n",
    "        x = x + self.dropout(attn_output)\n",
    "\n",
    "        # Sub-block 2: LayerNorm -> MLP -> Residual\n",
    "        normed = self.norm2(x)\n",
    "        mlp_output = self.mlp(normed)\n",
    "        x = x + self.dropout(mlp_output)\n",
    "\n",
    "        return x, attn_weights\n",
    "\n",
    "print(\"TransformerBlock class defined!\")"
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test it and verify the shapes."
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = TransformerBlock(embed_dim=128, num_heads=4)\n",
    "x = torch.randn(1, 65, 128)  # batch=1, 65 tokens, dim=128\n",
    "\n",
    "output, attn_weights = block(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Shape preserved: {x.shape == output.shape}\")\n",
    "print(f\"Number of attention heads: {len(attn_weights)}\")\n",
    "\n",
    "# Verify residual connection\n",
    "diff = (output - x).abs().mean().item()\n",
    "print(f\"\\nMean absolute diff from input: {diff:.4f}\")\n",
    "print(\"(Non-zero but moderate â€” residual connection is working!)\")"
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo1\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/16_todo1.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_16_todo1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Your Turn\n",
    "\n",
    "Time to test your understanding! Below are three exercises â€” the first two are essential, the third is a stretch goal.\n",
    "\n",
    "### TODO 1: Implement Scaled Dot-Product Attention from Scratch\n",
    "\n",
    "Without looking at our earlier implementation, write `scaled_dot_product_attention_v2`. We provide the function signature and step-by-step hints."
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention_v2(Q, K, V):\n",
    "    \"\"\"\n",
    "    Implement scaled dot-product attention from scratch.\n",
    "\n",
    "    Args:\n",
    "        Q: Query tensor of shape (batch, seq_len, d_k)\n",
    "        K: Key tensor of shape (batch, seq_len, d_k)\n",
    "        V: Value tensor of shape (batch, seq_len, d_v)\n",
    "\n",
    "    Returns:\n",
    "        output: Shape (batch, seq_len, d_v)\n",
    "        attn_weights: Shape (batch, seq_len, seq_len)\n",
    "\n",
    "    Steps:\n",
    "        1. Get d_k from the last dimension of Q\n",
    "        2. Compute scores = Q @ K^T, scale by 1/sqrt(d_k)\n",
    "        3. Apply softmax along the last dimension\n",
    "        4. Multiply attention weights by V\n",
    "        5. Return both output and attention weights\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here (~5 lines)\n",
    "    # d_k = ...\n",
    "    # scores = ...\n",
    "    # attn_weights = ...\n",
    "    # output = ...\n",
    "    # return output, attn_weights\n",
    "    pass"
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification cell** â€” run this after you implement the function above."
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification for TODO 1\n",
    "torch.manual_seed(123)\n",
    "Q_test = torch.randn(2, 10, 32)\n",
    "K_test = torch.randn(2, 10, 32)\n",
    "V_test = torch.randn(2, 10, 32)\n",
    "\n",
    "your_output, your_weights = scaled_dot_product_attention_v2(\n",
    "    Q_test, K_test, V_test)\n",
    "ref_output, ref_weights = scaled_dot_product_attention(\n",
    "    Q_test, K_test, V_test)\n",
    "\n",
    "print(f\"Output shape correct: {your_output.shape == ref_output.shape}\")\n",
    "print(f\"Values match: {torch.allclose(your_output, ref_output, atol=1e-5)}\")\n",
    "print(f\"Weights sum to 1: {torch.allclose(your_weights.sum(-1), torch.ones(2, 10), atol=1e-5)}\")\n",
    "\n",
    "if torch.allclose(your_output, ref_output, atol=1e-5):\n",
    "    print(\"\\nPerfect! Your implementation is correct!\")\n",
    "else:\n",
    "    print(\"\\nNot quite -- check your steps and try again.\")"
   ],
   "id": "cell_51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo2\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/17_todo2.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_17_todo2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement the TransformerBlock Forward Pass\n",
    "\n",
    "We give you the `__init__` method fully defined. Your task is to implement the `forward` method."
   ],
   "id": "cell_52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockV2(nn.Module):\n",
    "    \"\"\"Your implementation of the Transformer encoder block.\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mhsa = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.mlp = MLP(embed_dim, mlp_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Implement the pre-norm Transformer block.\n",
    "\n",
    "        Steps:\n",
    "            1. Apply self.norm1 to x\n",
    "            2. Pass through self.mhsa -> (output, attn_weights)\n",
    "            3. Add residual: add ORIGINAL x back\n",
    "            4. Apply self.norm2 to the result\n",
    "            5. Pass through self.mlp\n",
    "            6. Add residual again\n",
    "            7. Return final output and attn_weights\n",
    "        \"\"\"\n",
    "        # TODO: ~6 lines of code\n",
    "        pass"
   ],
   "id": "cell_53"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification cell** â€” run this after you implement the forward method."
   ],
   "id": "cell_54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification for TODO 2\n",
    "torch.manual_seed(42)\n",
    "block_v2 = TransformerBlockV2(embed_dim=128, num_heads=4)\n",
    "x_test = torch.randn(1, 65, 128)\n",
    "\n",
    "output_v2, weights_v2 = block_v2(x_test)\n",
    "\n",
    "print(f\"Output shape: {output_v2.shape} (expected: [1, 65, 128])\")\n",
    "print(f\"Shape correct: {output_v2.shape == torch.Size([1, 65, 128])}\")\n",
    "print(f\"Number of heads: {len(weights_v2)} (expected: 4)\")\n",
    "\n",
    "diff = (output_v2 - x_test).abs().mean().item()\n",
    "not_identical = not torch.allclose(output_v2, x_test, atol=1e-6)\n",
    "print(f\"Output differs from input: {not_identical}\")\n",
    "\n",
    "if output_v2.shape == torch.Size([1, 65, 128]) and not_identical:\n",
    "    print(\"\\nYour TransformerBlock implementation looks correct!\")\n",
    "else:\n",
    "    print(\"\\nSomething is off -- check your residual connections.\")"
   ],
   "id": "cell_55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Todo3\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/18_todo3.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_18_todo3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 3 (Stretch): Implement Causal Masking\n",
    "\n",
    "This is optional, but it connects to how GPT-style models work. In causal attention, each token can only attend to tokens that come *before* it (and itself). This is enforced by masking future positions with $-\\infty$ before softmax."
   ],
   "id": "cell_56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create a causal attention mask.\n",
    "\n",
    "    Returns:\n",
    "        mask: Shape (seq_len, seq_len)\n",
    "        mask[i][j] = 1 if j <= i, 0 if j > i\n",
    "\n",
    "    Hint: Use torch.tril (lower triangular matrix)\n",
    "    \"\"\"\n",
    "    # TODO: 1 line!\n",
    "    pass\n",
    "\n",
    "\n",
    "def causal_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Scaled dot-product attention with causal masking.\n",
    "\n",
    "    Hint: scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation\n",
    "    pass"
   ],
   "id": "cell_57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Putting Together\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/19_putting_together.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_19_putting_together"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Putting It All Together\n",
    "\n",
    "Now let us stack multiple Transformer blocks to create a mini Transformer encoder, and feed real image patches through it.\n",
    "\n",
    "First, we need to re-implement `PatchEmbedding` from Notebook 1 so this notebook is self-contained."
   ],
   "id": "cell_58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Patch embedding from Notebook 1. Self-contained.\"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=4,\n",
    "                 in_channels=3, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        patch_dim = in_channels * patch_size * patch_size\n",
    "        self.projection = nn.Linear(patch_dim, embed_dim)\n",
    "        self.cls_token = nn.Parameter(\n",
    "            torch.randn(1, 1, embed_dim) * 0.02)\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.randn(1, self.num_patches + 1, embed_dim) * 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, p = x.shape[0], self.patch_size\n",
    "        patches = x.unfold(2, p, p).unfold(3, p, p)\n",
    "        patches = patches.contiguous().view(B, 3, -1, p*p)\n",
    "        patches = patches.permute(0, 2, 1, 3).contiguous()\n",
    "        patches = patches.view(B, -1, 3 * p * p)\n",
    "        emb = self.projection(patches)\n",
    "        cls = self.cls_token.expand(B, -1, -1)\n",
    "        emb = torch.cat([cls, emb], dim=1)\n",
    "        return emb + self.pos_embed\n",
    "\n",
    "print(\"PatchEmbedding defined!\")\n",
    "print(f\"32x32 with 4x4 patches: {(32//4)**2} patches + 1 CLS = 65 tokens\")"
   ],
   "id": "cell_59"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create a small Transformer encoder by stacking 4 blocks."
   ],
   "id": "cell_60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniViTEncoder(nn.Module):\n",
    "    \"\"\"Small Vision Transformer encoder for demonstration.\"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3,\n",
    "                 embed_dim=128, num_heads=4, num_layers=4,\n",
    "                 mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            img_size, patch_size, in_channels, embed_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio)\n",
    "            for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        z = self.patch_embed(x)\n",
    "        all_attention = []\n",
    "        for block in self.blocks:\n",
    "            z, attn_w = block(z)\n",
    "            if return_attention:\n",
    "                all_attention.append(attn_w)\n",
    "        z = self.norm(z)\n",
    "        if return_attention:\n",
    "            return z, all_attention\n",
    "        return z\n",
    "\n",
    "print(\"MiniViTEncoder defined!\")"
   ],
   "id": "cell_61"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build the model and print a summary."
   ],
   "id": "cell_62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = MiniViTEncoder(\n",
    "    img_size=32, patch_size=4, in_channels=3,\n",
    "    embed_dim=128, num_heads=4, num_layers=4)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model architecture:\")\n",
    "print(f\"  Image size:    32 x 32\")\n",
    "print(f\"  Patch size:    4 x 4\")\n",
    "print(f\"  Num patches:   64 (+ 1 CLS = 65 tokens)\")\n",
    "print(f\"  Embed dim:     128\")\n",
    "print(f\"  Num heads:     4 (head_dim = 32)\")\n",
    "print(f\"  Num layers:    4\")\n",
    "print(f\"  MLP ratio:     4 (hidden_dim = 512)\")\n",
    "print(f\"  Total params:  {total_params:,}\")\n",
    "print(f\"\\nTiny model (~{total_params/1e6:.1f}M params).\")\n",
    "print(\"The original ViT-Base has 86M params!\")"
   ],
   "id": "cell_63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Shape Trace\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/20_shape_trace.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_20_shape_trace"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us load a real CIFAR-10 image and pass it through the encoder."
   ],
   "id": "cell_64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "image, label = dataset[0]\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "print(f\"Image class: {class_names[label]}\")\n",
    "print(f\"Image shape: {image.shape}\")"
   ],
   "id": "cell_65"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us trace the shapes through every stage of the forward pass."
   ],
   "id": "cell_66"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = image.unsqueeze(0)  # Add batch dimension: (1, 3, 32, 32)\n",
    "print(f\"Input image:      {x.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = model.patch_embed(x)\n",
    "    print(f\"After patch embed: {z.shape}  (65 tokens x 128 dim)\")\n",
    "\n",
    "    for i, block in enumerate(model.blocks):\n",
    "        z, attn_w = block(z)\n",
    "        print(f\"After block {i+1}:     {z.shape}  \"\n",
    "              f\"({len(attn_w)} heads)\")\n",
    "\n",
    "    z = model.norm(z)\n",
    "    print(f\"After final norm:  {z.shape}\")\n",
    "    cls_output = z[:, 0]\n",
    "    print(f\"\\n[CLS] token:       {cls_output.shape}\")\n",
    "    print(\"This vector would be fed to a classification head.\")"
   ],
   "id": "cell_67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Cls Evolution\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/21_cls_evolution.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_21_cls_evolution"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify that the [CLS] token representation changes across layers â€” showing that each layer refines the representation."
   ],
   "id": "cell_68"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z = model.patch_embed(x)\n",
    "    cls_reps = [z[:, 0].clone()]\n",
    "\n",
    "    for block in model.blocks:\n",
    "        z, _ = block(z)\n",
    "        cls_reps.append(z[:, 0].clone())\n",
    "\n",
    "# Cosine similarity between consecutive layers\n",
    "print(\"Cosine similarity of [CLS] between consecutive layers:\")\n",
    "print(\"=\" * 50)\n",
    "for i in range(len(cls_reps) - 1):\n",
    "    sim = F.cosine_similarity(\n",
    "        cls_reps[i], cls_reps[i+1], dim=-1).item()\n",
    "    src = \"Embed\" if i == 0 else f\"Block {i}\"\n",
    "    print(f\"  {src:>8} -> Block {i+1}: {sim:.4f}\")\n",
    "\n",
    "print(\"\\nValues near 1.0 = small changes (residual at work).\")\n",
    "print(\"Values further from 1.0 = larger transformations.\")"
   ],
   "id": "cell_69"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Final Output Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/22_final_output_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_22_final_output_setup"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Final Output\n",
    "\n",
    "This is the culmination of our work. We will feed CIFAR-10 images through our mini ViT encoder and create detailed attention visualizations that show how different heads and layers attend to different parts of the image."
   ],
   "id": "cell_70"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_with_patches(ax, image_tensor, patch_size=4, title=''):\n",
    "    \"\"\"Display an image with patch grid overlay.\"\"\"\n",
    "    img = image_tensor.permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "    H, W = img.shape[:2]\n",
    "    for i in range(0, H + 1, patch_size):\n",
    "        ax.axhline(y=i - 0.5, color='white', linewidth=0.5, alpha=0.5)\n",
    "    for j in range(0, W + 1, patch_size):\n",
    "        ax.axvline(x=j - 0.5, color='white', linewidth=0.5, alpha=0.5)\n",
    "    ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "print(\"show_image_with_patches() defined!\")"
   ],
   "id": "cell_71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_on_image(ax, image_tensor, attn_weights,\n",
    "                                  patch_size=4, token_idx=0, title=''):\n",
    "    \"\"\"Overlay attention weights from a specific token onto the image.\"\"\"\n",
    "    img = image_tensor.permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
    "    img = np.clip(img, 0, 1)\n",
    "    H, W = img.shape[:2]\n",
    "    grid_h, grid_w = H // patch_size, W // patch_size\n",
    "\n",
    "    # Get attention to patch tokens (skip CLS at index 0)\n",
    "    attn = attn_weights[token_idx, 1:].detach().numpy()\n",
    "    attn_map = attn.reshape(grid_h, grid_w)\n",
    "\n",
    "    # Normalize to [0, 1]\n",
    "    attn_map = (attn_map - attn_map.min()) / \\\n",
    "               (attn_map.max() - attn_map.min() + 1e-8)\n",
    "\n",
    "    # Upsample to image size\n",
    "    attn_resized = np.kron(attn_map, np.ones((patch_size, patch_size)))\n",
    "\n",
    "    ax.imshow(img)\n",
    "    ax.imshow(attn_resized, alpha=0.6, cmap='hot')\n",
    "    ax.set_title(title, fontsize=10, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "print(\"visualize_attention_on_image() defined!\")"
   ],
   "id": "cell_72"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us select a few interesting images from different classes for our visualization."
   ],
   "id": "cell_73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Find images of different classes\n",
    "sample_indices = []\n",
    "seen_classes = set()\n",
    "for i in range(len(dataset)):\n",
    "    _, lbl = dataset[i]\n",
    "    if lbl not in seen_classes and lbl in [3, 5, 0, 8]:\n",
    "        sample_indices.append(i)\n",
    "        seen_classes.add(lbl)\n",
    "    if len(sample_indices) == 4:\n",
    "        break\n",
    "\n",
    "img_tensor, img_label = dataset[sample_indices[0]]\n",
    "print(f\"Visualizing attention for: {class_names[img_label]}\")"
   ],
   "id": "cell_74"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention weights from all layers\n",
    "with torch.no_grad():\n",
    "    output, all_attention = model(\n",
    "        img_tensor.unsqueeze(0), return_attention=True)\n",
    "\n",
    "print(f\"Number of layers: {len(all_attention)}\")\n",
    "print(f\"Heads per layer: {len(all_attention[0])}\")\n",
    "print(f\"Attention shape per head: {all_attention[0][0].shape}\")"
   ],
   "id": "cell_75"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Cls Attention Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/23_cls_attention_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_23_cls_attention_viz"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create the first panel: original image alongside CLS attention from early and late layers."
   ],
   "id": "cell_76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4.5))\n",
    "\n",
    "show_image_with_patches(\n",
    "    axes[0], img_tensor, patch_size=4,\n",
    "    title=f'Original: {class_names[img_label]}\\n(8x8 grid)')\n",
    "\n",
    "visualize_attention_on_image(\n",
    "    axes[1], img_tensor,\n",
    "    all_attention[0][0][0],  # Layer 0, Head 0, Batch 0\n",
    "    patch_size=4, token_idx=0,\n",
    "    title='Layer 1, Head 1\\n[CLS] Attention')\n",
    "\n",
    "visualize_attention_on_image(\n",
    "    axes[2], img_tensor,\n",
    "    all_attention[3][0][0],  # Layer 3, Head 0, Batch 0\n",
    "    patch_size=4, token_idx=0,\n",
    "    title='Layer 4, Head 1\\n[CLS] Attention')\n",
    "\n",
    "fig.suptitle('How the [CLS] Token Attends to Image Patches',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: All Heads Grid\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/24_all_heads_grid.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_24_all_heads_grid"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create the full grid showing all heads across two layers â€” this reveals how different heads specialize."
   ],
   "id": "cell_78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(18, 9))\n",
    "fig.suptitle(\n",
    "    f'Attention Across Heads and Layers â€” '\n",
    "    f'{class_names[img_label].upper()}\\n'\n",
    "    '[CLS] token attention to all image patches',\n",
    "    fontsize=15, fontweight='bold', y=1.02)\n",
    "\n",
    "for layer_idx, layer in enumerate([0, 3]):\n",
    "    for head_idx in range(4):\n",
    "        ax = axes[layer_idx, head_idx]\n",
    "        attn = all_attention[layer][head_idx][0]\n",
    "        visualize_attention_on_image(\n",
    "            ax, img_tensor, attn,\n",
    "            patch_size=4, token_idx=0,\n",
    "            title=f'Layer {layer+1}, Head {head_idx+1}')\n",
    "\n",
    "axes[0, 0].set_ylabel('Layer 1\\n(Early)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Layer 4\\n(Late)', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_79"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also look at the raw attention matrices (not overlaid on images) to see the full token-to-token structure."
   ],
   "id": "cell_80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(20, 9))\n",
    "fig.suptitle('Raw Attention Matrices â€” Full 65x65',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "\n",
    "for layer_idx, layer in enumerate([0, 3]):\n",
    "    for head_idx in range(4):\n",
    "        ax = axes[layer_idx, head_idx]\n",
    "        attn = all_attention[layer][head_idx][0].detach().numpy()\n",
    "        im = ax.imshow(attn, cmap='viridis', aspect='auto')\n",
    "        ax.set_title(f'Layer {layer+1}, Head {head_idx+1}',\n",
    "                     fontsize=11, fontweight='bold')\n",
    "        if head_idx == 0:\n",
    "            ax.set_ylabel(f'Layer {layer+1}', fontsize=11)\n",
    "        if layer_idx == 1:\n",
    "            ax.set_xlabel('Key position', fontsize=10)\n",
    "        plt.colorbar(im, ax=ax, shrink=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_81"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token 0 is [CLS]. Tokens 1-64 are image patches. Look for structure: diagonal patterns suggest local attention, uniform rows suggest global attention, bright spots suggest focused attention."
   ],
   "id": "cell_82"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Cross Image Comparison\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/25_cross_image_comparison.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_25_cross_image_comparison"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us compare attention across multiple images to see how the model adapts its attention to different content."
   ],
   "id": "cell_83"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = len(sample_indices)\n",
    "fig, axes = plt.subplots(n_images, 5, figsize=(22, 4 * n_images))\n",
    "fig.suptitle(\n",
    "    'Attention Comparison Across Different Images\\n'\n",
    "    '[CLS] attention from Layer 4 (deepest layer)',\n",
    "    fontsize=15, fontweight='bold', y=1.02)\n",
    "\n",
    "for row, idx in enumerate(sample_indices):\n",
    "    img, lbl = dataset[idx]\n",
    "    with torch.no_grad():\n",
    "        _, attn_all = model(img.unsqueeze(0), return_attention=True)\n",
    "\n",
    "    show_image_with_patches(\n",
    "        axes[row, 0], img, patch_size=4,\n",
    "        title=f'{class_names[lbl]}')\n",
    "\n",
    "    for head in range(4):\n",
    "        visualize_attention_on_image(\n",
    "            axes[row, head + 1], img,\n",
    "            attn_all[3][head][0],\n",
    "            patch_size=4, token_idx=0,\n",
    "            title=f'Head {head + 1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"  You have built a complete Transformer encoder for images!\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"What we built from scratch in this notebook:\")\n",
    "print(\"  1. Scaled dot-product attention\")\n",
    "print(\"  2. Single attention head with learnable projections\")\n",
    "print(\"  3. Multi-head attention (4 heads)\")\n",
    "print(\"  4. MLP block with GELU and 4x expansion\")\n",
    "print(\"  5. Full Transformer encoder block (pre-norm)\")\n",
    "print(\"  6. Stacked 4 blocks into a mini ViT encoder\")\n",
    "print(\"  7. Visualized attention patterns on real images\")\n",
    "print()\n",
    "print(\"Key insight: Even with random weights, the attention\")\n",
    "print(\"mechanism has the STRUCTURE to learn meaningful patterns.\")\n",
    "print(\"After training, each head specializes, and deeper layers\")\n",
    "print(\"capture increasingly semantic relationships.\")"
   ],
   "id": "cell_85"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Reflection\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/26_reflection.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_26_reflection"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "\n",
    "Take a moment to think about these questions. Better yet, modify the code above to find out!\n",
    "\n",
    "**1. Why does the scaling factor $1/\\sqrt{d_k}$ matter?**\n",
    "What would happen without it? Try removing the `/ math.sqrt(d_k)` from our attention function and see what happens to the attention weights. You should find that softmax produces nearly one-hot distributions â€” almost all the weight goes to a single token. This kills the ability to blend information from multiple tokens and makes gradients vanish.\n",
    "\n",
    "**2. Each attention head has its own $W^Q$, $W^K$, $W^V$. What if all heads shared the same weights?**\n",
    "Try modifying `MultiHeadAttention` so all heads share one set of projection matrices. What happens to the diversity of attention patterns in the visualization? You should see all heads produce nearly identical patterns, losing the benefit of multi-head attention.\n",
    "\n",
    "**3. The residual connection adds the input back. What happens if you remove it?**\n",
    "Try removing the `+ x` from the Transformer block. Train or evaluate â€” you will likely find that the model becomes much harder to train and the representations degrade in deeper layers. Residual connections are essential for gradient flow in deep networks."
   ],
   "id": "cell_86"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Challenges\n",
    "\n",
    "**Challenge 1: Learnable Temperature**\n",
    "Instead of dividing by the fixed $\\sqrt{d_k}$, make the temperature a learnable parameter. Initialize it to $\\sqrt{d_k}$ and let it be updated during training. Does this help the model learn better attention patterns?"
   ],
   "id": "cell_87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Add this to your attention head\n",
    "# self.temperature = nn.Parameter(torch.tensor(math.sqrt(d_k)))\n",
    "# Then use: scores = Q @ K.T / self.temperature"
   ],
   "id": "cell_88"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 2: Visualize CLS Token Evolution with PCA**\n",
    "The [CLS] token representation changes at each layer. Use PCA to project each layer's [CLS] representation to 2D and plot the trajectory."
   ],
   "id": "cell_89"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint:\n",
    "# from sklearn.decomposition import PCA\n",
    "# Collect CLS representations from each layer\n",
    "# Stack them: shape (num_layers+1, embed_dim)\n",
    "# Fit PCA with n_components=2\n",
    "# Plot the 2D trajectory with arrows"
   ],
   "id": "cell_90"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 3: Relative Position Bias (Swin Transformer)**\n",
    "The original ViT uses absolute positional embeddings (added once at the start). The Swin Transformer instead uses a relative position bias added directly to the attention scores. Implement a learnable relative position bias table."
   ],
   "id": "cell_91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint:\n",
    "# The relative position bias B has shape (seq_len, seq_len)\n",
    "# and gets ADDED to attention scores before softmax.\n",
    "# B[i][j] depends on relative position of patch i and j.\n",
    "# Use a learnable lookup table indexed by\n",
    "# (2*grid_h - 1) * (2*grid_w - 1) possible relative positions."
   ],
   "id": "cell_92"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/27_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_27_closing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Next?\n",
    "\n",
    "In **Notebook 3**, we will put everything together into a complete Vision Transformer:\n",
    "- Combine patch embedding (Notebook 1) + Transformer encoder (Notebook 2)\n",
    "- Add the classification head\n",
    "- Train on CIFAR-10 and watch the attention patterns evolve during training\n",
    "- Compare performance to a simple CNN baseline\n",
    "- Implement class attention maps to see what the model has learned\n",
    "\n",
    "The pieces are all in place. Notebook 3 is where we train and see the full power of the Vision Transformer!"
   ],
   "id": "cell_93"
  }
 ]
}