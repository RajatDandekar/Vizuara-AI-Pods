{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Tensor Parallelism from Scratch â€” Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1rhbO-3WtvU0YjQYQrCebpE_IATSZyRCX\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Tensor Parallelism: Splitting Layers Across GPUs\n",
    "\n",
    "*Part 3 of the Vizuara series on 5D Parallelism from Scratch*\n",
    "*Estimated time: 45 minutes*\n",
    "\n",
    "In this notebook, you will tear a single weight matrix apart, distribute its pieces across simulated GPUs, and stitch the results back together â€” all from scratch. By the end, you will have a working Megatron-style tensor-parallel MLP and a tensor-parallel multi-head attention module."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://course-creator-brown.vercel.app/courses/5d-parallelism-from-scratch/practice/3/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# We simulate multi-GPU by splitting tensors on a single device\n",
    "device = torch.device('cpu')\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"We will simulate tensor parallelism on a single device.\")\n",
    "print(\"In production, each 'rank' would be a separate GPU.\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_why_it_matters",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In the previous notebook, we saw that Data Parallelism gives each GPU a **full copy** of the model. That is fine when the model fits on one GPU. But what happens when a **single layer** is too large?\n",
    "\n",
    "Consider a 70B parameter model like Llama 2 70B. Its MLP layers have weight matrices of size **(8192 x 28672)** â€” that is **235 million parameters** in a single matrix, or **470 MB** in fp16. And there are *two* such matrices per MLP block, plus the attention projections.\n",
    "\n",
    "The total parameter memory for just the weights of a 70B model is about **140 GB** in fp16. A single A100 has 80 GB. Even with ZeRO Stage 3, the communication overhead of constantly fetching weight shards can be painful for operations that happen inside every single layer.\n",
    "\n",
    "Tensor Parallelism offers an elegant alternative: **split the weight matrices themselves** across GPUs, so each GPU only stores and computes a fraction of each layer."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us see the concrete numbers\n",
    "hidden_dim = 8192\n",
    "intermediate_dim = 28672  # ~3.5x hidden, common in Llama-style models\n",
    "\n",
    "params_per_mlp_weight = hidden_dim * intermediate_dim\n",
    "bytes_fp16 = params_per_mlp_weight * 2  # 2 bytes per fp16 parameter\n",
    "\n",
    "print(f\"One MLP weight matrix: {hidden_dim} x {intermediate_dim}\")\n",
    "print(f\"Parameters: {params_per_mlp_weight:,} ({params_per_mlp_weight/1e6:.0f}M)\")\n",
    "print(f\"Memory (fp16): {bytes_fp16 / 1e6:.0f} MB\")\n",
    "print(f\"\\nWith 2-way Tensor Parallelism:\")\n",
    "print(f\"  Each GPU stores: {hidden_dim} x {intermediate_dim // 2} = {params_per_mlp_weight // 2:,} params\")\n",
    "print(f\"  Memory per GPU: {bytes_fp16 / 2 / 1e6:.0f} MB  (halved!)\")\n",
    "print(f\"\\nWith 8-way Tensor Parallelism (full NVLink node):\")\n",
    "print(f\"  Each GPU stores: {hidden_dim} x {intermediate_dim // 8} = {params_per_mlp_weight // 8:,} params\")\n",
    "print(f\"  Memory per GPU: {bytes_fp16 / 8 / 1e6:.0f} MB  (8x reduction!)\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_building_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Building Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_building_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "Let us go back to our restaurant analogy. In Data Parallelism, every chef had a complete copy of the recipe and worked on different orders. In Tensor Parallelism, we **tear one page of the recipe in half**.\n",
    "\n",
    "Imagine Chef A and Chef B are making a complex sauce. The recipe says: \"Combine 6 ingredients.\" Instead of each chef handling all 6, Chef A handles ingredients 1-3 and Chef B handles ingredients 4-6. They each do their mixing independently, and at the end, they combine their partial sauces into the final dish.\n",
    "\n",
    "The key insight is this: **we are splitting the computation of a single matrix multiply across GPUs**. Not splitting data, not splitting layers â€” splitting the arithmetic inside one operation."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤” Think About This\n",
    "\n",
    "If $Y = XW$, and the weight matrix $W$ is too large for one GPU, how could you split this computation?\n",
    "\n",
    "There are two natural ways:\n",
    "\n",
    "1. **Split $W$ by columns** (vertically) â€” each GPU gets some output features\n",
    "2. **Split $W$ by rows** (horizontally) â€” each GPU gets part of the dot product\n",
    "\n",
    "Think about what happens to the shapes in each case. What operation do you need at the end to reconstruct the full result?\n",
    "\n",
    "*Take a moment to reason about this before scrolling down.*"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick shape check: let us set up a small example to keep in mind\n",
    "# as we work through the math below\n",
    "X_example = torch.randn(2, 4)  # 2 samples, 4 features\n",
    "W_example = torch.randn(4, 6)  # maps 4 inputs to 6 outputs\n",
    "Y_example = X_example @ W_example\n",
    "print(f\"X: {X_example.shape}  @  W: {W_example.shape}  =  Y: {Y_example.shape}\")\n",
    "print(\"Our goal: compute this same Y, but split the work across 2 GPUs.\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_math_column_split",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Math Column Split\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_math_column_split.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### Column-Wise Split\n",
    "\n",
    "We split the weight matrix $W$ along its **columns** (the output dimension):\n",
    "\n",
    "$$W = [W_1 \\mid W_2]$$\n",
    "\n",
    "The matrix multiply becomes:\n",
    "\n",
    "$$Y = X \\cdot [W_1 \\mid W_2] = [X W_1 \\mid X W_2]$$\n",
    "\n",
    "Computationally, this means: GPU 0 computes $X W_1$ (a subset of output features), GPU 1 computes $X W_2$ (the rest of the output features), and we **concatenate** the results along the feature dimension.\n",
    "\n",
    "No information is lost. No approximation is made. This is **mathematically exact**."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row-Wise Split\n",
    "\n",
    "Alternatively, we split $W$ along its **rows** (the input dimension):\n",
    "\n",
    "$$W = \\begin{bmatrix} W_1 \\\\ W_2 \\end{bmatrix}$$\n",
    "\n",
    "But now we must also split the input $X$ along the corresponding dimension:\n",
    "\n",
    "$$X = [X_1 \\mid X_2]$$\n",
    "\n",
    "The matrix multiply becomes:\n",
    "\n",
    "$$Y = [X_1 \\mid X_2] \\cdot \\begin{bmatrix} W_1 \\\\ W_2 \\end{bmatrix} = X_1 W_1 + X_2 W_2$$\n",
    "\n",
    "Computationally, this means: GPU 0 computes the partial product $X_1 W_1$, GPU 1 computes $X_2 W_2$, and we **sum** them via an AllReduce operation. Each partial product has the **same shape** as the final output â€” we are accumulating partial sums, not concatenating slices."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_math_verification",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_math_verification.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us verify the math identities with our small example\n",
    "W1_col = W_example[:, :3]   # Column split: first 3 output features\n",
    "W2_col = W_example[:, 3:]   # Column split: last 3 output features\n",
    "Y_col = torch.cat([X_example @ W1_col, X_example @ W2_col], dim=1)\n",
    "\n",
    "W1_row = W_example[:2, :]   # Row split: first 2 input features\n",
    "W2_row = W_example[2:, :]   # Row split: last 2 input features\n",
    "Y_row = X_example[:, :2] @ W1_row + X_example[:, 2:] @ W2_row\n",
    "\n",
    "print(f\"Column-wise matches: {torch.allclose(Y_col, Y_example)}\")\n",
    "print(f\"Row-wise matches:    {torch.allclose(Y_row, Y_example)}\")\n",
    "print(\"Both are exact â€” zero error. These are mathematical identities.\")"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Key Difference\n",
    "\n",
    "| | Column-Wise | Row-Wise |\n",
    "|---|---|---|\n",
    "| **What is split** | Output features | Input features |\n",
    "| **Input needed** | Full $X$ on each GPU | Split $X$ across GPUs |\n",
    "| **Recombination** | Concatenate | AllReduce (sum) |\n",
    "| **Output per GPU** | Partial output (fewer columns) | Full-shaped partial sum |\n",
    "\n",
    "This distinction will matter enormously when we wire them together in the Megatron-LM MLP."
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_numerical_example",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_numerical_example.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let Us Build It â€” Component by Component\n",
    "\n",
    "### 4.1 Numerical Example: Seeing the Math with Real Numbers\n",
    "\n",
    "Before writing any classes, let us plug in some simple numbers and verify the math by hand."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create small, concrete matrices\n",
    "# X: (2 x 4) â€” 2 samples, 4 input features\n",
    "# W: (4 x 6) â€” maps 4 inputs to 6 outputs\n",
    "X = torch.tensor([\n",
    "    [1.0, 2.0, 3.0, 4.0],\n",
    "    [5.0, 6.0, 7.0, 8.0]\n",
    "])\n",
    "W = torch.tensor([\n",
    "    [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "    [0.7, 0.8, 0.9, 1.0, 1.1, 1.2],\n",
    "    [1.3, 1.4, 1.5, 1.6, 1.7, 1.8],\n",
    "    [1.9, 2.0, 2.1, 2.2, 2.3, 2.4],\n",
    "])\n",
    "\n",
    "# Ground truth: full matmul\n",
    "Y_full = X @ W\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"W shape:\", W.shape)\n",
    "print(\"Y = X @ W shape:\", Y_full.shape)\n",
    "print(\"\\nFull result Y:\")\n",
    "print(Y_full)"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us split this computation two ways and verify we get the exact same result."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Column-Wise Split ===\n",
    "# Split W along columns: W1 = first 3 cols, W2 = last 3 cols\n",
    "W1_col = W[:, :3]  # shape (4, 3)\n",
    "W2_col = W[:, 3:]  # shape (4, 3)\n",
    "\n",
    "# Each \"GPU\" gets the full X and its shard of W\n",
    "Y1_col = X @ W1_col  # GPU 0: shape (2, 3)\n",
    "Y2_col = X @ W2_col  # GPU 1: shape (2, 3)\n",
    "\n",
    "# Reconstruct by concatenation\n",
    "Y_col = torch.cat([Y1_col, Y2_col], dim=1)  # shape (2, 6)\n",
    "\n",
    "print(\"=== Column-Wise Split ===\")\n",
    "print(f\"W1 shape: {W1_col.shape}, W2 shape: {W2_col.shape}\")\n",
    "print(f\"GPU 0 output: {Y1_col.shape}, GPU 1 output: {Y2_col.shape}\")\n",
    "print(f\"Concatenated: {Y_col.shape}\")\n",
    "print(f\"Matches full result: {torch.allclose(Y_col, Y_full)}\")\n",
    "print(f\"Max error: {(Y_col - Y_full).abs().max().item():.1e}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the row-wise split, where we sum partial products instead of concatenating."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Row-Wise Split ===\n",
    "# Split W along rows: W1 = first 2 rows, W2 = last 2 rows\n",
    "W1_row = W[:2, :]  # shape (2, 6)\n",
    "W2_row = W[2:, :]  # shape (2, 6)\n",
    "\n",
    "# Also split X along columns (matching dimension)\n",
    "X1 = X[:, :2]  # shape (2, 2)\n",
    "X2 = X[:, 2:]  # shape (2, 2)\n",
    "\n",
    "# Each \"GPU\" computes a partial sum\n",
    "Y1_row = X1 @ W1_row  # GPU 0: shape (2, 6)\n",
    "Y2_row = X2 @ W2_row  # GPU 1: shape (2, 6)\n",
    "\n",
    "# Reconstruct by summing (this is the AllReduce)\n",
    "Y_row = Y1_row + Y2_row  # shape (2, 6)\n",
    "\n",
    "print(\"=== Row-Wise Split ===\")\n",
    "print(f\"W1 shape: {W1_row.shape}, W2 shape: {W2_row.shape}\")\n",
    "print(f\"X1 shape: {X1.shape}, X2 shape: {X2.shape}\")\n",
    "print(f\"GPU 0 partial sum: {Y1_row.shape}, GPU 1 partial sum: {Y2_row.shape}\")\n",
    "print(f\"After AllReduce (sum): {Y_row.shape}\")\n",
    "print(f\"Matches full result: {torch.allclose(Y_row, Y_full)}\")\n",
    "print(f\"Max error: {(Y_row - Y_full).abs().max().item():.1e}\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both methods produce **exactly** the same result as the unsplit computation. Zero error. This is not an approximation â€” it is a mathematical identity.\n",
    "\n",
    "### ðŸ“Š Visualization: Column-Wise vs Row-Wise Splits\n",
    "\n",
    "Let us visualize what is happening to the weight matrix in each case."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_split_visualization",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_split_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the full weight matrix and the two splitting strategies\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Full weight matrix\n",
    "im0 = axes[0].imshow(W.numpy(), cmap='RdYlBu_r', aspect='auto')\n",
    "axes[0].set_title('Full Weight Matrix W\\n(4 x 6)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Output features')\n",
    "axes[0].set_ylabel('Input features')\n",
    "plt.colorbar(im0, ax=axes[0], shrink=0.8)\n",
    "\n",
    "# Column-wise split\n",
    "W_col_viz = W.numpy().copy()\n",
    "axes[1].imshow(W_col_viz[:, :3], cmap='Blues', aspect='auto',\n",
    "               extent=[-0.5, 2.5, 3.5, -0.5])\n",
    "axes[1].imshow(W_col_viz[:, 3:], cmap='Greens', aspect='auto',\n",
    "               extent=[3.5, 6.5, 3.5, -0.5], alpha=0.9)\n",
    "axes[1].axvline(x=3.0, color='red', linewidth=3, linestyle='--')\n",
    "axes[1].set_title('Column-Wise Split\\nGPU 0 (blue) | GPU 1 (green)',\n",
    "                   fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Output features')\n",
    "axes[1].set_ylabel('Input features')"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left panel shows the full weight matrix. Now let us add the row-wise split panel."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row-wise split (continuing the figure from the previous cell)\n",
    "axes[2].imshow(W_col_viz[:2, :], cmap='Blues', aspect='auto',\n",
    "               extent=[-0.5, 5.5, 1.5, -0.5])\n",
    "axes[2].imshow(W_col_viz[2:, :], cmap='Greens', aspect='auto',\n",
    "               extent=[-0.5, 5.5, 3.5, 1.5], alpha=0.9)\n",
    "axes[2].axhline(y=1.5, color='red', linewidth=3, linestyle='--')\n",
    "axes[2].set_title('Row-Wise Split\\nGPU 0 (blue) / GPU 1 (green)',\n",
    "                   fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Output features')\n",
    "axes[2].set_ylabel('Input features')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tensor_split_visualization.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"The red dashed lines show where we cut the weight matrix.\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_07_column_parallel_class",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_column_parallel_class.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Column-Wise Parallel Linear Layer\n",
    "\n",
    "Now let us wrap the column-wise split into a reusable class. This simulates what would happen on a multi-GPU system â€” each \"rank\" (GPU index) computes its portion."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnParallelLinear:\n",
    "    \"\"\"\n",
    "    Splits a linear layer along the OUTPUT dimension (columns).\n",
    "\n",
    "    Each GPU stores W[:, start:end] and computes X @ W_shard.\n",
    "    The outputs are concatenated across GPUs to form the full result.\n",
    "    \"\"\"\n",
    "    def __init__(self, weight, bias=None, world_size=2):\n",
    "        self.world_size = world_size\n",
    "        self.full_weight = weight  # (in_features, out_features)\n",
    "        self.full_bias = bias\n",
    "\n",
    "        # Split weight into column shards\n",
    "        out_features = weight.shape[1]\n",
    "        assert out_features % world_size == 0, \\\n",
    "            f\"out_features ({out_features}) must be divisible by world_size ({world_size})\"\n",
    "\n",
    "        shard_size = out_features // world_size\n",
    "        self.shards = [weight[:, i*shard_size:(i+1)*shard_size]\n",
    "                       for i in range(world_size)]\n",
    "        self.bias_shards = None\n",
    "        if bias is not None:\n",
    "            self.bias_shards = [bias[i*shard_size:(i+1)*shard_size]\n",
    "                                for i in range(world_size)]"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each shard holds a vertical slice of the weight matrix. Now let us add the forward methods."
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_parallel_forward(col_linear, x, rank):\n",
    "    \"\"\"Compute output for a specific GPU rank.\"\"\"\n",
    "    out = x @ col_linear.shards[rank]\n",
    "    if col_linear.bias_shards is not None:\n",
    "        out = out + col_linear.bias_shards[rank]\n",
    "    return out\n",
    "\n",
    "def column_parallel_forward_all(col_linear, x):\n",
    "    \"\"\"Simulate all GPUs computing, then concatenating results.\"\"\"\n",
    "    outputs = [column_parallel_forward(col_linear, x, rank)\n",
    "               for rank in range(col_linear.world_size)]\n",
    "    return torch.cat(outputs, dim=-1)\n",
    "\n",
    "# Attach methods to the class for convenient use\n",
    "ColumnParallelLinear.forward = lambda self, x, rank: column_parallel_forward(self, x, rank)\n",
    "ColumnParallelLinear.forward_all_and_gather = lambda self, x: column_parallel_forward_all(self, x)"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test that our column-parallel layer produces the same result as the full computation."
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ColumnParallelLinear\n",
    "torch.manual_seed(42)\n",
    "in_features, out_features = 8, 12\n",
    "W_test = torch.randn(in_features, out_features)\n",
    "b_test = torch.randn(out_features)\n",
    "X_test = torch.randn(4, in_features)  # batch of 4\n",
    "\n",
    "col_linear = ColumnParallelLinear(W_test, b_test, world_size=2)\n",
    "\n",
    "# Full computation (ground truth)\n",
    "Y_expected = X_test @ W_test + b_test\n",
    "\n",
    "# Tensor-parallel computation\n",
    "Y_tp = col_linear.forward_all_and_gather(X_test)\n",
    "\n",
    "print(\"ColumnParallelLinear Test\")\n",
    "print(f\"  Input shape: {X_test.shape}\")\n",
    "print(f\"  Full weight shape: {W_test.shape}\")\n",
    "print(f\"  Shard 0 shape: {col_linear.shards[0].shape}\")\n",
    "print(f\"  Shard 1 shape: {col_linear.shards[1].shape}\")\n",
    "print(f\"  GPU 0 output shape: {col_linear.forward(X_test, 0).shape}\")\n",
    "print(f\"  GPU 1 output shape: {col_linear.forward(X_test, 1).shape}\")\n",
    "print(f\"  Gathered output shape: {Y_tp.shape}\")\n",
    "print(f\"  Matches full computation: {torch.allclose(Y_tp, Y_expected, atol=1e-6)}\")\n",
    "print(f\"  Max error: {(Y_tp - Y_expected).abs().max().item():.2e}\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_08_row_parallel_class",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_row_parallel_class.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Row-Wise Parallel Linear Layer\n",
    "\n",
    "The row-wise split is the complement: we split the weight along the **input** dimension and sum the partial results."
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowParallelLinear:\n",
    "    \"\"\"\n",
    "    Splits a linear layer along the INPUT dimension (rows).\n",
    "\n",
    "    Each GPU stores W[start:end, :] and receives X[:, start:end].\n",
    "    The outputs are summed across GPUs (AllReduce) to form the full result.\n",
    "    \"\"\"\n",
    "    def __init__(self, weight, bias=None, world_size=2):\n",
    "        self.world_size = world_size\n",
    "        self.full_weight = weight  # (in_features, out_features)\n",
    "        self.full_bias = bias\n",
    "\n",
    "        # Split weight into row shards\n",
    "        in_features = weight.shape[0]\n",
    "        assert in_features % world_size == 0, \\\n",
    "            f\"in_features ({in_features}) must be divisible by world_size ({world_size})\"\n",
    "\n",
    "        shard_size = in_features // world_size\n",
    "        self.shards = [weight[i*shard_size:(i+1)*shard_size, :]\n",
    "                       for i in range(world_size)]\n",
    "        self.shard_size = shard_size"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us add the forward methods for the row-parallel layer. Notice how the AllReduce (sum) combines partial outputs."
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_parallel_forward(row_linear, x_shard, rank):\n",
    "    \"\"\"\n",
    "    Compute partial output for a specific GPU rank.\n",
    "    x_shard should be the rank's portion of the input features.\n",
    "    \"\"\"\n",
    "    return x_shard @ row_linear.shards[rank]\n",
    "\n",
    "def row_parallel_forward_all(row_linear, x):\n",
    "    \"\"\"Simulate all GPUs computing partial sums, then AllReduce.\"\"\"\n",
    "    partial_sums = []\n",
    "    for rank in range(row_linear.world_size):\n",
    "        start = rank * row_linear.shard_size\n",
    "        end = (rank + 1) * row_linear.shard_size\n",
    "        x_shard = x[:, start:end]\n",
    "        partial_sums.append(row_parallel_forward(row_linear, x_shard, rank))\n",
    "\n",
    "    # AllReduce: sum all partial outputs\n",
    "    result = sum(partial_sums)\n",
    "    if row_linear.full_bias is not None:\n",
    "        result = result + row_linear.full_bias\n",
    "    return result\n",
    "\n",
    "# Attach methods to the class\n",
    "RowParallelLinear.forward = lambda self, x, rank: row_parallel_forward(self, x, rank)\n",
    "RowParallelLinear.forward_all_and_reduce = lambda self, x: row_parallel_forward_all(self, x)"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify the row-parallel layer matches the full computation."
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RowParallelLinear\n",
    "row_linear = RowParallelLinear(W_test, b_test, world_size=2)\n",
    "\n",
    "# Full computation (ground truth)\n",
    "Y_expected = X_test @ W_test + b_test\n",
    "\n",
    "# Tensor-parallel computation\n",
    "Y_tp_row = row_linear.forward_all_and_reduce(X_test)\n",
    "\n",
    "print(\"RowParallelLinear Test\")\n",
    "print(f\"  Input shape: {X_test.shape}\")\n",
    "print(f\"  Full weight shape: {W_test.shape}\")\n",
    "print(f\"  Shard 0 shape: {row_linear.shards[0].shape}\")\n",
    "print(f\"  Shard 1 shape: {row_linear.shards[1].shape}\")\n",
    "print(f\"  Reduced output shape: {Y_tp_row.shape}\")\n",
    "print(f\"  Matches full computation: {torch.allclose(Y_tp_row, Y_expected, atol=1e-6)}\")\n",
    "print(f\"  Max error: {(Y_tp_row - Y_expected).abs().max().item():.2e}\")"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_09_shards_visualization",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_shards_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Visualization: Weight Shards Side by Side\n",
    "\n",
    "Let us look at the actual weight shards for both split strategies."
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top row: Column-wise split\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 8))\n",
    "\n",
    "im00 = axes[0, 0].imshow(W_test.numpy(), cmap='RdYlBu_r', aspect='auto')\n",
    "axes[0, 0].set_title('Full W (8x12)', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Column Split', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im00, ax=axes[0, 0], shrink=0.7)\n",
    "\n",
    "im01 = axes[0, 1].imshow(col_linear.shards[0].numpy(), cmap='Blues', aspect='auto')\n",
    "axes[0, 1].set_title('GPU 0: W[:, :6]\\n(8x6)', fontweight='bold')\n",
    "plt.colorbar(im01, ax=axes[0, 1], shrink=0.7)\n",
    "\n",
    "im02 = axes[0, 2].imshow(col_linear.shards[1].numpy(), cmap='Greens', aspect='auto')\n",
    "axes[0, 2].set_title('GPU 1: W[:, 6:]\\n(8x6)', fontweight='bold')\n",
    "plt.colorbar(im02, ax=axes[0, 2], shrink=0.7)"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the bottom row showing the row-wise split."
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottom row: Row-wise split\n",
    "im10 = axes[1, 0].imshow(W_test.numpy(), cmap='RdYlBu_r', aspect='auto')\n",
    "axes[1, 0].set_title('Full W (8x12)', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Row Split', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im10, ax=axes[1, 0], shrink=0.7)\n",
    "\n",
    "im11 = axes[1, 1].imshow(row_linear.shards[0].numpy(), cmap='Blues', aspect='auto')\n",
    "axes[1, 1].set_title('GPU 0: W[:4, :]\\n(4x12)', fontweight='bold')\n",
    "plt.colorbar(im11, ax=axes[1, 1], shrink=0.7)\n",
    "\n",
    "im12 = axes[1, 2].imshow(row_linear.shards[1].numpy(), cmap='Greens', aspect='auto')\n",
    "axes[1, 2].set_title('GPU 1: W[4:, :]\\n(4x12)', fontweight='bold')\n",
    "plt.colorbar(im12, ax=axes[1, 2], shrink=0.7)\n",
    "\n",
    "plt.suptitle('Column-Wise vs Row-Wise Weight Sharding',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('weight_shards.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_10_megatron_mlp_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Megatron Mlp Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_megatron_mlp_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The Megatron-LM MLP Block\n",
    "\n",
    "Now comes the elegant part. The Megatron-LM paper showed that you can wire column-wise and row-wise splits together so that **only one AllReduce** is needed per MLP block.\n",
    "\n",
    "Here is the trick:\n",
    "\n",
    "1. **First linear layer** ($W_1$): split **column-wise**. Each GPU computes a subset of the intermediate activations.\n",
    "2. **GeLU activation**: applied **locally** on each GPU. No communication needed! Why? Because each GPU has a complete slice of the intermediate features â€” GeLU is elementwise, so it does not need values from other GPUs.\n",
    "3. **Second linear layer** ($W_2$): split **row-wise**. The input to this layer is already split across GPUs (from step 1), which is exactly what row-wise split expects.\n",
    "4. **AllReduce (sum)**: combine the partial outputs."
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The brilliant insight: the column-wise output (split features) naturally feeds into the row-wise input (split input). No gather or scatter is needed in between. The only communication is a single AllReduce at the very end."
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_11_megatron_mlp_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_megatron_mlp_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MegatronMLP:\n",
    "    \"\"\"\n",
    "    Megatron-LM style tensor-parallel MLP block.\n",
    "\n",
    "    Architecture:\n",
    "        Input -> ColumnParallelLinear -> GeLU -> RowParallelLinear -> AllReduce -> Output\n",
    "\n",
    "    Communication: Only ONE AllReduce per forward pass.\n",
    "    \"\"\"\n",
    "    def __init__(self, W1, b1, W2, b2, world_size=2):\n",
    "        self.world_size = world_size\n",
    "        self.col_linear = ColumnParallelLinear(W1, b1, world_size)\n",
    "        self.row_linear = RowParallelLinear(W2, b2, world_size)\n",
    "\n",
    "        # Store full weights for reference computation\n",
    "        self.W1, self.b1 = W1, b1\n",
    "        self.W2, self.b2 = W2, b2"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single-GPU forward is straightforward: standard linear -> GeLU -> linear."
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def megatron_forward_single_gpu(mlp, x):\n",
    "    \"\"\"Non-parallel forward (ground truth).\"\"\"\n",
    "    h = x @ mlp.W1 + mlp.b1\n",
    "    h = F.gelu(h)\n",
    "    return h @ mlp.W2 + mlp.b2\n",
    "\n",
    "# Attach to the class\n",
    "MegatronMLP.forward_single_gpu = lambda self, x: megatron_forward_single_gpu(self, x)"
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the tensor-parallel version. Each GPU computes its shard, applies GeLU locally, and we sum at the end."
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def megatron_forward_tp(mlp, x):\n",
    "    \"\"\"Tensor-parallel forward (simulated).\"\"\"\n",
    "    # Step 1: Column-wise linear â€” each GPU gets a shard\n",
    "    gpu_intermediates = []\n",
    "    for rank in range(mlp.world_size):\n",
    "        h_shard = mlp.col_linear.forward(x, rank)\n",
    "        gpu_intermediates.append(h_shard)\n",
    "\n",
    "    # Step 2: GeLU â€” applied LOCALLY, no communication!\n",
    "    gpu_activations = [F.gelu(h) for h in gpu_intermediates]\n",
    "\n",
    "    # Step 3: Row-wise linear â€” partial output per GPU\n",
    "    partial_outputs = []\n",
    "    for rank in range(mlp.world_size):\n",
    "        partial = gpu_activations[rank] @ mlp.row_linear.shards[rank]\n",
    "        partial_outputs.append(partial)\n",
    "\n",
    "    # Step 4: AllReduce (sum) â€” the ONLY communication\n",
    "    output = sum(partial_outputs)\n",
    "    if mlp.row_linear.full_bias is not None:\n",
    "        output = output + mlp.row_linear.full_bias\n",
    "    return output\n",
    "\n",
    "MegatronMLP.forward_tp = lambda self, x: megatron_forward_tp(self, x)"
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the Megatron MLP and verify it matches single-GPU computation."
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Megatron MLP\n",
    "torch.manual_seed(42)\n",
    "hidden_dim = 16\n",
    "intermediate_dim = 32  # 2x expansion (real models use ~3.5x)\n",
    "batch_size = 4\n",
    "\n",
    "W1 = torch.randn(hidden_dim, intermediate_dim) * 0.1\n",
    "b1 = torch.randn(intermediate_dim) * 0.01\n",
    "W2 = torch.randn(intermediate_dim, hidden_dim) * 0.1\n",
    "b2 = torch.randn(hidden_dim) * 0.01\n",
    "X_mlp = torch.randn(batch_size, hidden_dim)\n",
    "\n",
    "mlp = MegatronMLP(W1, b1, W2, b2, world_size=2)\n",
    "\n",
    "Y_single = mlp.forward_single_gpu(X_mlp)\n",
    "Y_tp = mlp.forward_tp(X_mlp)\n",
    "\n",
    "print(\"Megatron-LM MLP Test\")\n",
    "print(f\"  Input: ({batch_size}, {hidden_dim})\")\n",
    "print(f\"  Intermediate: ({batch_size}, {intermediate_dim})\")\n",
    "print(f\"  Output: {Y_tp.shape}\")\n",
    "print(f\"  Matches single-GPU: {torch.allclose(Y_tp, Y_single, atol=1e-5)}\")\n",
    "print(f\"  Max error: {(Y_tp - Y_single).abs().max().item():.2e}\")\n",
    "print(f\"\\n  Communication: 1 AllReduce of {Y_tp.numel()} elements\")\n",
    "print(f\"  GeLU applied locally (no communication needed)\")"
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_12_megatron_flow_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_megatron_flow_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Visualization: Megatron MLP Data Flow\n",
    "\n",
    "Let us create a visual showing exactly what each GPU computes and where communication happens."
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure for the Megatron MLP flow diagram\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Megatron-LM Tensor-Parallel MLP (2-way)',\n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# Colors\n",
    "blue = '#3498db'\n",
    "green = '#2ecc71'\n",
    "orange = '#e67e22'\n",
    "red = '#e74c3c'\n",
    "gray = '#95a5a6'\n",
    "\n",
    "# Input X â€” full on both GPUs\n",
    "rect = mpatches.FancyBboxPatch((5, 9), 4, 0.6, boxstyle=\"round,pad=0.1\",\n",
    "                                facecolor=gray, edgecolor='black', linewidth=1.5)\n",
    "ax.add_patch(rect)\n",
    "ax.text(7, 9.3, 'Input X (full copy on both GPUs)',\n",
    "        ha='center', va='center', fontsize=10, fontweight='bold')"
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we draw the column-wise linear layer stage."
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrows from input to column-wise linear\n",
    "ax.annotate('', xy=(5.5, 8.2), xytext=(5.5, 8.8),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color=blue))\n",
    "ax.annotate('', xy=(8.5, 8.2), xytext=(8.5, 8.8),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color=green))\n",
    "\n",
    "# Column-wise linear boxes\n",
    "rect_g0 = mpatches.FancyBboxPatch((3.5, 7.4), 3, 0.6, boxstyle=\"round,pad=0.1\",\n",
    "                                   facecolor=blue, edgecolor='black', linewidth=1.5, alpha=0.3)\n",
    "ax.add_patch(rect_g0)\n",
    "ax.text(5, 7.7, 'GPU 0: X @ W1_col', ha='center', va='center',\n",
    "        fontsize=9, fontweight='bold', color=blue)\n",
    "\n",
    "rect_g1 = mpatches.FancyBboxPatch((7.5, 7.4), 3, 0.6, boxstyle=\"round,pad=0.1\",\n",
    "                                   facecolor=green, edgecolor='black', linewidth=1.5, alpha=0.3)\n",
    "ax.add_patch(rect_g1)\n",
    "ax.text(9, 7.7, 'GPU 1: X @ W2_col', ha='center', va='center',\n",
    "        fontsize=9, fontweight='bold', color=green)\n",
    "ax.text(12.5, 7.7, '<- Column Parallel', ha='center', va='center',\n",
    "        fontsize=9, color='gray', style='italic')"
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the GeLU activation stage â€” applied locally with no communication."
   ],
   "id": "cell_51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrows to GeLU\n",
    "ax.annotate('', xy=(5, 6.6), xytext=(5, 7.2),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color=blue))\n",
    "ax.annotate('', xy=(9, 6.6), xytext=(9, 7.2),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color=green))\n",
    "\n",
    "# GeLU (local, no communication)\n",
    "rect_g0 = mpatches.FancyBboxPatch((3.5, 5.9), 3, 0.6, boxstyle=\"round,pad=0.1\",\n",
    "                                   facecolor=blue, edgecolor='black', linewidth=1.5, alpha=0.3)\n",
    "ax.add_patch(rect_g0)\n",
    "ax.text(5, 6.2, 'GPU 0: GeLU', ha='center', va='center',\n",
    "        fontsize=9, fontweight='bold', color=blue)\n",
    "\n",
    "rect_g1 = mpatches.FancyBboxPatch((7.5, 5.9), 3, 0.6, boxstyle=\"round,pad=0.1\",\n",
    "                                   facecolor=green, edgecolor='black', linewidth=1.5, alpha=0.3)\n",
    "ax.add_patch(rect_g1)\n",
    "ax.text(9, 6.2, 'GPU 1: GeLU', ha='center', va='center',\n",
    "        fontsize=9, fontweight='bold', color=green)\n",
    "ax.text(12.5, 6.2, '<- No Communication!', ha='center', va='center',\n",
    "        fontsize=9, color=red, fontweight='bold')"
   ],
   "id": "cell_52"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the row-wise linear stage and arrows leading to it."
   ],
   "id": "cell_53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrows to row-wise linear\n",
    "ax.annotate('', xy=(5, 5.1), xytext=(5, 5.7),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color=blue))\n",
    "ax.annotate('', xy=(9, 5.1), xytext=(9, 5.7),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color=green))\n",
    "\n",
    "# Row-wise linear boxes\n",
    "rect_g0 = mpatches.FancyBboxPatch((3.5, 4.3), 3, 0.6, boxstyle=\"round,pad=0.1\",\n",
    "                                   facecolor=blue, edgecolor='black', linewidth=1.5, alpha=0.3)\n",
    "ax.add_patch(rect_g0)\n",
    "ax.text(5, 4.6, 'GPU 0: h0 @ W1_row', ha='center', va='center',\n",
    "        fontsize=9, fontweight='bold', color=blue)\n",
    "\n",
    "rect_g1 = mpatches.FancyBboxPatch((7.5, 4.3), 3, 0.6, boxstyle=\"round,pad=0.1\",\n",
    "                                   facecolor=green, edgecolor='black', linewidth=1.5, alpha=0.3)\n",
    "ax.add_patch(rect_g1)\n",
    "ax.text(9, 4.6, 'GPU 1: h1 @ W2_row', ha='center', va='center',\n",
    "        fontsize=9, fontweight='bold', color=green)\n",
    "ax.text(12.5, 4.6, '<- Row Parallel', ha='center', va='center',\n",
    "        fontsize=9, color='gray', style='italic')"
   ],
   "id": "cell_54"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the AllReduce and output â€” the only communication point in the entire MLP."
   ],
   "id": "cell_55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrows converging to AllReduce\n",
    "ax.annotate('', xy=(7, 3.3), xytext=(5, 4.1),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color=blue))\n",
    "ax.annotate('', xy=(7, 3.3), xytext=(9, 4.1),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color=green))\n",
    "\n",
    "# AllReduce circle and output\n",
    "circle = plt.Circle((7, 2.9), 0.5, facecolor=orange,\n",
    "                     edgecolor='black', linewidth=2, alpha=0.5)\n",
    "ax.add_patch(circle)\n",
    "ax.text(7, 2.9, 'AllReduce\\n(Sum)', ha='center', va='center',\n",
    "        fontsize=8, fontweight='bold')\n",
    "ax.text(12.5, 2.9, '<- ONLY communication!', ha='center', va='center',\n",
    "        fontsize=9, color=red, fontweight='bold')\n",
    "\n",
    "ax.annotate('', xy=(7, 1.6), xytext=(7, 2.3),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='black'))\n",
    "\n",
    "rect = mpatches.FancyBboxPatch((5, 1.0), 4, 0.6, boxstyle=\"round,pad=0.1\",\n",
    "                                facecolor=gray, edgecolor='black', linewidth=1.5)\n",
    "ax.add_patch(rect)\n",
    "ax.text(7, 1.3, 'Output Y', ha='center', va='center',\n",
    "        fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('megatron_mlp_flow.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Notice: GeLU is applied locally. Only ONE AllReduce per MLP block!\")"
   ],
   "id": "cell_56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_13_why_gelu_local",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why Gelu Local\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_why_gelu_local.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤” Why Can GeLU Be Applied Locally?\n",
    "\n",
    "This is a subtle but important point. After the column-wise split, GPU 0 has the activations corresponding to the **first half of intermediate features**, and GPU 1 has the **second half**. GeLU is an elementwise function â€” it operates on each element independently. It does not need to \"see\" the other GPU's features. So each GPU can safely apply GeLU to its local shard.\n",
    "\n",
    "If instead we had used a non-elementwise operation here (like softmax across features), we would need communication. The choice of GeLU (or any elementwise activation) is precisely what makes this one-AllReduce design work."
   ],
   "id": "cell_57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_14_tp_attention_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Tp Attention Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_tp_attention_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Tensor-Parallel Multi-Head Attention\n",
    "\n",
    "For self-attention, the splitting strategy is even more natural. Attention heads are **independent** â€” each head computes its own Q, K, V projections and attention scores without needing information from other heads.\n",
    "\n",
    "So we simply assign different heads to different GPUs:\n",
    "- With 8 heads and 2 GPUs, GPU 0 handles heads 0-3, GPU 1 handles heads 4-7\n",
    "- Each GPU computes full attention for its assigned heads\n",
    "- The outputs are concatenated and projected back"
   ],
   "id": "cell_58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_15_tp_attention_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/15_tp_attention_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorParallelMultiHeadAttention:\n",
    "    \"\"\"\n",
    "    Tensor-parallel multi-head attention.\n",
    "\n",
    "    Each GPU handles a subset of the attention heads.\n",
    "    The Q, K, V projections are split column-wise (by heads).\n",
    "    The output projection is split row-wise.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, num_heads, world_size=2):\n",
    "        assert num_heads % world_size == 0, \\\n",
    "            \"num_heads must be divisible by world_size\"\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.world_size = world_size\n",
    "        self.heads_per_gpu = num_heads // world_size\n",
    "\n",
    "        # Full weight matrices (each GPU only stores its shard in practice)\n",
    "        torch.manual_seed(123)\n",
    "        self.W_q = torch.randn(hidden_dim, hidden_dim) * 0.02\n",
    "        self.W_k = torch.randn(hidden_dim, hidden_dim) * 0.02\n",
    "        self.W_v = torch.randn(hidden_dim, hidden_dim) * 0.02\n",
    "        self.W_o = torch.randn(hidden_dim, hidden_dim) * 0.02"
   ],
   "id": "cell_59"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single-GPU forward pass computes all heads together (ground truth)."
   ],
   "id": "cell_60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp_attention_forward_single(attn, x):\n",
    "    \"\"\"Non-parallel forward pass (ground truth).\"\"\"\n",
    "    batch, seq_len, _ = x.shape\n",
    "    d = attn.hidden_dim\n",
    "    nh = attn.num_heads\n",
    "    hd = attn.head_dim\n",
    "\n",
    "    Q = (x @ attn.W_q).view(batch, seq_len, nh, hd).transpose(1, 2)\n",
    "    K = (x @ attn.W_k).view(batch, seq_len, nh, hd).transpose(1, 2)\n",
    "    V = (x @ attn.W_v).view(batch, seq_len, nh, hd).transpose(1, 2)\n",
    "\n",
    "    scores = (Q @ K.transpose(-2, -1)) / (hd ** 0.5)\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    attn_out = (attn_weights @ V).transpose(1, 2).contiguous().view(batch, seq_len, d)\n",
    "\n",
    "    return attn_out @ attn.W_o"
   ],
   "id": "cell_61"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor-parallel version distributes heads across GPUs. Each GPU computes Q, K, V for its heads only."
   ],
   "id": "cell_62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp_attention_forward_parallel(attn, x):\n",
    "    \"\"\"Tensor-parallel forward pass (simulated).\"\"\"\n",
    "    batch, seq_len, _ = x.shape\n",
    "    hd = attn.head_dim\n",
    "    hpg = attn.heads_per_gpu\n",
    "    local_dim = hpg * hd  # Dimension per GPU\n",
    "\n",
    "    partial_outputs = []\n",
    "    for rank in range(attn.world_size):\n",
    "        start = rank * local_dim\n",
    "        end = (rank + 1) * local_dim\n",
    "\n",
    "        # Column-wise split of Q, K, V projections\n",
    "        Q_local = (x @ attn.W_q[:, start:end]).view(\n",
    "            batch, seq_len, hpg, hd).transpose(1, 2)\n",
    "        K_local = (x @ attn.W_k[:, start:end]).view(\n",
    "            batch, seq_len, hpg, hd).transpose(1, 2)\n",
    "        V_local = (x @ attn.W_v[:, start:end]).view(\n",
    "            batch, seq_len, hpg, hd).transpose(1, 2)"
   ],
   "id": "cell_63"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each GPU runs attention locally and computes its partial output projection."
   ],
   "id": "cell_64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Local attention (no communication!)\n",
    "        scores = (Q_local @ K_local.transpose(-2, -1)) / (hd ** 0.5)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_out = (attn_weights @ V_local).transpose(1, 2).contiguous()\n",
    "        attn_out = attn_out.view(batch, seq_len, local_dim)\n",
    "\n",
    "        # Row-wise split of output projection\n",
    "        partial = attn_out @ attn.W_o[start:end, :]\n",
    "        partial_outputs.append(partial)\n",
    "\n",
    "    # AllReduce (sum)\n",
    "    return sum(partial_outputs)"
   ],
   "id": "cell_65"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test that tensor-parallel attention matches the single-GPU version."
   ],
   "id": "cell_66"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tensor-parallel attention\n",
    "torch.manual_seed(42)\n",
    "hidden_dim = 64\n",
    "num_heads = 8\n",
    "seq_len = 16\n",
    "batch_size = 2\n",
    "\n",
    "attn = TensorParallelMultiHeadAttention(hidden_dim, num_heads, world_size=2)\n",
    "X_attn = torch.randn(batch_size, seq_len, hidden_dim)\n",
    "\n",
    "Y_single = tp_attention_forward_single(attn, X_attn)\n",
    "Y_tp = tp_attention_forward_parallel(attn, X_attn)\n",
    "\n",
    "print(\"Tensor-Parallel Multi-Head Attention Test\")\n",
    "print(f\"  Hidden dim: {hidden_dim}, Num heads: {num_heads}, Heads per GPU: {num_heads // 2}\")\n",
    "print(f\"  Input: ({batch_size}, {seq_len}, {hidden_dim})\")\n",
    "print(f\"  Output: {Y_tp.shape}\")\n",
    "print(f\"  Matches single-GPU: {torch.allclose(Y_tp, Y_single, atol=1e-5)}\")\n",
    "print(f\"  Max error: {(Y_tp - Y_single).abs().max().item():.2e}\")\n",
    "print(f\"\\n  Each GPU computes {num_heads // 2} heads independently\")\n",
    "print(f\"  Communication: 1 AllReduce after output projection\")"
   ],
   "id": "cell_67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_16_transition_to_todos",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/16_transition_to_todos.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ðŸ”§ Your Turn\n",
    "\n",
    "### TODO 1: Implement Column-Wise Split and Verify\n",
    "\n",
    "Your task: given a weight matrix and an input, implement the column-wise split manually and verify that concatenating the results matches the full computation."
   ],
   "id": "cell_68"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_17_todo1",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/17_todo1.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_wise_split_matmul(X, W, world_size):\n",
    "    \"\"\"\n",
    "    Split the weight matrix W along columns and compute X @ W\n",
    "    using simulated tensor parallelism.\n",
    "\n",
    "    Args:\n",
    "        X: Input tensor of shape (batch, in_features)\n",
    "        W: Weight matrix of shape (in_features, out_features)\n",
    "        world_size: Number of simulated GPUs\n",
    "\n",
    "    Returns:\n",
    "        Y: Output tensor of shape (batch, out_features)\n",
    "    \"\"\"\n",
    "    out_features = W.shape[1]\n",
    "    shard_size = out_features // world_size\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Split W into `world_size` column shards\n",
    "    # Step 2: Compute X @ W_shard for each shard\n",
    "    # Step 3: Concatenate along the last dimension\n",
    "    # ==============================\n",
    "\n",
    "    gpu_outputs = []\n",
    "    for rank in range(world_size):\n",
    "        W_shard = ???  # YOUR CODE: extract columns\n",
    "        Y_shard = ???  # YOUR CODE: compute the matmul\n",
    "        gpu_outputs.append(Y_shard)\n",
    "\n",
    "    Y = ???  # YOUR CODE: combine the outputs\n",
    "    return Y"
   ],
   "id": "cell_69"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to check your implementation of TODO 1."
   ],
   "id": "cell_70"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification for TODO 1\n",
    "torch.manual_seed(99)\n",
    "X_todo = torch.randn(3, 8)\n",
    "W_todo = torch.randn(8, 12)\n",
    "\n",
    "Y_expected_todo = X_todo @ W_todo\n",
    "\n",
    "for ws in [2, 3, 4]:\n",
    "    Y_yours = column_wise_split_matmul(X_todo, W_todo, world_size=ws)\n",
    "    match = torch.allclose(Y_yours, Y_expected_todo, atol=1e-6)\n",
    "    print(f\"  world_size={ws}: shape={Y_yours.shape}, correct={match}\",\n",
    "          \"PASS\" if match else \"FAIL\")\n",
    "\n",
    "print(\"\\nIf all show PASS, your implementation is correct!\")"
   ],
   "id": "cell_71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_18_todo1_followup",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/18_todo1_followup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”§ TODO 2: Build a Complete Megatron-Style Transformer Block\n",
    "\n",
    "Given `ColumnParallelLinear` and `RowParallelLinear` from above, assemble a complete tensor-parallel Transformer block that includes **both** MLP and attention. The block should follow the standard pre-norm Transformer architecture:\n",
    "\n",
    "$$\\text{Output} = x + \\text{MLP}(\\text{LayerNorm}(x + \\text{Attention}(\\text{LayerNorm}(x))))$$"
   ],
   "id": "cell_72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_19_todo2",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/19_todo2.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorParallelTransformerBlock:\n",
    "    \"\"\"\n",
    "    A single Transformer block with tensor-parallel MLP and attention.\n",
    "\n",
    "    Architecture (pre-norm):\n",
    "        x -> LayerNorm -> TP-Attention -> residual\n",
    "          -> LayerNorm -> TP-MLP -> residual -> output\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, num_heads, intermediate_dim, world_size=2):\n",
    "        self.world_size = world_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Layer norms (not parallelized â€” replicated on each GPU)\n",
    "        self.ln1_weight = torch.ones(hidden_dim)\n",
    "        self.ln1_bias = torch.zeros(hidden_dim)\n",
    "        self.ln2_weight = torch.ones(hidden_dim)\n",
    "        self.ln2_bias = torch.zeros(hidden_dim)\n",
    "\n",
    "        # TP Attention\n",
    "        self.attn = TensorParallelMultiHeadAttention(\n",
    "            hidden_dim, num_heads, world_size)\n",
    "\n",
    "        # TP MLP weights\n",
    "        torch.manual_seed(456)\n",
    "        W1 = torch.randn(hidden_dim, intermediate_dim) * 0.02\n",
    "        b1 = torch.zeros(intermediate_dim)\n",
    "        W2 = torch.randn(intermediate_dim, hidden_dim) * 0.02\n",
    "        b2 = torch.zeros(hidden_dim)\n",
    "        self.mlp = MegatronMLP(W1, b1, W2, b2, world_size)"
   ],
   "id": "cell_73"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer norm helper â€” replicated on every GPU, not parallelized."
   ],
   "id": "cell_74"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_layer_norm(x, weight, bias, eps=1e-5):\n",
    "    \"\"\"Standard layer norm (replicated, not parallelized).\"\"\"\n",
    "    mean = x.mean(dim=-1, keepdim=True)\n",
    "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "    return weight * (x - mean) / torch.sqrt(var + eps) + bias\n",
    "\n",
    "TensorParallelTransformerBlock.layer_norm = staticmethod(block_layer_norm)"
   ],
   "id": "cell_75"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the tensor-parallel forward pass â€” fill in the TODO blanks."
   ],
   "id": "cell_76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_forward_tp(block, x):\n",
    "    \"\"\"\n",
    "    Tensor-parallel forward pass.\n",
    "    Fill in the blanks below.\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Pre-norm + TP Attention + residual\n",
    "    # Step 2: Pre-norm + TP MLP + residual\n",
    "    # ==============================\n",
    "\n",
    "    # Step 1\n",
    "    h = block_layer_norm(x, block.ln1_weight, block.ln1_bias)\n",
    "    attn_out = ???  # YOUR CODE: tensor-parallel attention\n",
    "    x = ???         # YOUR CODE: residual connection\n",
    "\n",
    "    # Step 2\n",
    "    h = block_layer_norm(x, block.ln2_weight, block.ln2_bias)\n",
    "    mlp_out = ???   # YOUR CODE: tensor-parallel MLP\n",
    "    x = ???         # YOUR CODE: residual connection\n",
    "\n",
    "    return x\n",
    "\n",
    "TensorParallelTransformerBlock.forward_tp = lambda self, x: block_forward_tp(self, x)"
   ],
   "id": "cell_77"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the single-GPU forward pass for comparison (ground truth)."
   ],
   "id": "cell_78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_forward_single(block, x):\n",
    "    \"\"\"Non-parallel forward (ground truth).\"\"\"\n",
    "    h = block_layer_norm(x, block.ln1_weight, block.ln1_bias)\n",
    "    attn_out = tp_attention_forward_single(block.attn, h)\n",
    "    x = x + attn_out\n",
    "\n",
    "    h = block_layer_norm(x, block.ln2_weight, block.ln2_bias)\n",
    "    mlp_out = block.mlp.forward_single_gpu(h)\n",
    "    x = x + mlp_out\n",
    "    return x\n",
    "\n",
    "TensorParallelTransformerBlock.forward_single = lambda self, x: block_forward_single(self, x)"
   ],
   "id": "cell_79"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to verify your TODO 2 implementation."
   ],
   "id": "cell_80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification for TODO 2\n",
    "torch.manual_seed(42)\n",
    "block = TensorParallelTransformerBlock(\n",
    "    hidden_dim=64, num_heads=8, intermediate_dim=128, world_size=2\n",
    ")\n",
    "X_block = torch.randn(2, 16, 64)\n",
    "\n",
    "Y_single = block.forward_single(X_block)\n",
    "Y_tp = block.forward_tp(X_block)\n",
    "\n",
    "if Y_tp is not None:\n",
    "    match = torch.allclose(Y_tp, Y_single, atol=1e-4)\n",
    "    max_err = (Y_tp - Y_single).abs().max().item()\n",
    "    print(f\"Output shape: {Y_tp.shape}\")\n",
    "    print(f\"Matches single-GPU: {match} {'PASS' if match else 'FAIL'}\")\n",
    "    print(f\"Max error: {max_err:.2e}\")\n",
    "\n",
    "    if match:\n",
    "        print(\"\\nYour tensor-parallel Transformer block works correctly!\")\n",
    "        print(\"Communication per block: 2 AllReduces (1 for attention, 1 for MLP)\")\n",
    "else:\n",
    "    print(\"forward_tp returned None. Did you fill in the TODO?\")"
   ],
   "id": "cell_81"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_20_todo2_followup",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/20_todo2_followup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Communication Analysis\n",
    "\n",
    "Let us analyze the communication cost of tensor parallelism and understand **why it must stay within a single node**."
   ],
   "id": "cell_82"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_21_communication_analysis",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Communication Analysis\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/21_communication_analysis.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tp_communication(hidden_dim, seq_len, batch_size,\n",
    "                             num_layers, world_size):\n",
    "    \"\"\"\n",
    "    Compute the AllReduce communication volume for TP.\n",
    "\n",
    "    Each Transformer layer has 2 AllReduce operations:\n",
    "    - 1 for attention output projection\n",
    "    - 1 for MLP second linear layer\n",
    "    \"\"\"\n",
    "    bytes_per_element = 2  # fp16\n",
    "    tensor_elements = batch_size * seq_len * hidden_dim\n",
    "    tensor_bytes = tensor_elements * bytes_per_element\n",
    "\n",
    "    # Ring AllReduce: 2 * (N-1)/N * tensor_size\n",
    "    allreduce_bytes = 2 * (world_size - 1) / world_size * tensor_bytes\n",
    "    per_layer_bytes = 2 * allreduce_bytes\n",
    "    total_bytes = per_layer_bytes * num_layers\n",
    "\n",
    "    return {\n",
    "        'per_allreduce_bytes': allreduce_bytes,\n",
    "        'per_layer_bytes': per_layer_bytes,\n",
    "        'total_bytes': total_bytes,\n",
    "        'total_gb': total_bytes / 1e9,\n",
    "    }"
   ],
   "id": "cell_83"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us compute the actual numbers for a Llama 2 70B-scale model."
   ],
   "id": "cell_84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama 2 70B parameters\n",
    "hidden_dim = 8192\n",
    "seq_len = 4096\n",
    "batch_size = 1\n",
    "num_layers = 80\n",
    "\n",
    "print(\"Communication volume for Llama 2 70B-scale model:\")\n",
    "print(f\"  hidden_dim={hidden_dim}, seq_len={seq_len}, layers={num_layers}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for ws in [2, 4, 8]:\n",
    "    stats = compute_tp_communication(hidden_dim, seq_len, batch_size, num_layers, ws)\n",
    "    print(f\"\\n  TP world_size = {ws}:\")\n",
    "    print(f\"    Per AllReduce: {stats['per_allreduce_bytes']/1e6:.1f} MB\")\n",
    "    print(f\"    Per layer: {stats['per_layer_bytes']/1e6:.1f} MB\")\n",
    "    print(f\"    Total (all layers): {stats['total_gb']:.2f} GB\")"
   ],
   "id": "cell_85"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_22_critical_point",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Critical Point\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/22_critical_point.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Critical Point**: This communication happens inside **every single layer**, for **every single micro-batch**. It is not like Data Parallelism where you communicate once per training step â€” tensor parallelism communicates constantly.\n",
    "\n",
    "This is why interconnect bandwidth is the bottleneck."
   ],
   "id": "cell_86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_23_nvlink_vs_infiniband",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/23_nvlink_vs_infiniband.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandwidth comparison: NVLink vs InfiniBand\n",
    "nvlink_bw = 900    # GB/s (H100 NVSwitch)\n",
    "ib_bw = 50         # GB/s (400 Gb/s InfiniBand, realistic)\n",
    "\n",
    "# Time per training step (all layers, one micro-batch)\n",
    "stats_8way = compute_tp_communication(8192, 4096, 1, 80, 8)\n",
    "total_gb = stats_8way['total_gb']\n",
    "\n",
    "time_nvlink = total_gb / nvlink_bw * 1000  # ms\n",
    "time_ib = total_gb / ib_bw * 1000          # ms\n",
    "\n",
    "print(f\"Total communication per micro-batch (TP=8): {total_gb:.2f} GB\")\n",
    "print(f\"\\n  On NVLink (900 GB/s):     {time_nvlink:.2f} ms  -- Fast enough\")\n",
    "print(f\"  On InfiniBand (50 GB/s):  {time_ib:.2f} ms  -- {time_ib/time_nvlink:.0f}x slower!\")\n",
    "print(f\"\\nThis is why TP is ALWAYS within a single node (NVLink).\")\n",
    "print(f\"Cross-node TP would be {time_ib/time_nvlink:.0f}x slower -- a dealbreaker.\")"
   ],
   "id": "cell_87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_24_communication_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/24_communication_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Visualization: Communication Latency vs TP Degree"
   ],
   "id": "cell_88"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left panel: Communication volume\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "world_sizes = [1, 2, 4, 8]\n",
    "volumes = []\n",
    "for ws in world_sizes:\n",
    "    if ws == 1:\n",
    "        volumes.append(0)\n",
    "    else:\n",
    "        stats = compute_tp_communication(8192, 4096, 1, 80, ws)\n",
    "        volumes.append(stats['total_gb'])\n",
    "\n",
    "bars = axes[0].bar(range(len(world_sizes)), volumes,\n",
    "                    color=['#95a5a6', '#3498db', '#2ecc71', '#e67e22'],\n",
    "                    edgecolor='black', linewidth=1.2)\n",
    "axes[0].set_xticks(range(len(world_sizes)))\n",
    "axes[0].set_xticklabels([f'TP={ws}' for ws in world_sizes])\n",
    "axes[0].set_ylabel('Communication Volume (GB)', fontsize=11)\n",
    "axes[0].set_title('Communication per Micro-Batch\\n(Llama 70B-scale)',\n",
    "                   fontsize=12, fontweight='bold')\n",
    "for bar, vol in zip(bars, volumes):\n",
    "    if vol > 0:\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                     f'{vol:.1f} GB', ha='center', fontsize=10)"
   ],
   "id": "cell_89"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the right panel comparing NVLink vs InfiniBand latency."
   ],
   "id": "cell_90"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right panel: Latency comparison â€” compute the times\n",
    "nvlink_times = []\n",
    "ib_times = []\n",
    "for ws in world_sizes:\n",
    "    if ws == 1:\n",
    "        nvlink_times.append(0)\n",
    "        ib_times.append(0)\n",
    "    else:\n",
    "        stats = compute_tp_communication(8192, 4096, 1, 80, ws)\n",
    "        nvlink_times.append(stats['total_gb'] / nvlink_bw * 1000)\n",
    "        ib_times.append(stats['total_gb'] / ib_bw * 1000)\n",
    "\n",
    "x = np.arange(len(world_sizes))\n",
    "width = 0.35\n",
    "axes[1].bar(x - width/2, nvlink_times, width, label='NVLink (900 GB/s)',\n",
    "            color='#2ecc71', edgecolor='black', linewidth=1.2)\n",
    "axes[1].bar(x + width/2, ib_times, width, label='InfiniBand (50 GB/s)',\n",
    "            color='#e74c3c', edgecolor='black', linewidth=1.2)"
   ],
   "id": "cell_91"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the latency comparison axes and render the full figure."
   ],
   "id": "cell_92"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels([f'TP={ws}' for ws in world_sizes])\n",
    "axes[1].set_ylabel('Latency (ms)', fontsize=11)\n",
    "axes[1].set_title('Communication Latency per Micro-Batch\\n(NVLink vs InfiniBand)',\n",
    "                   fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].set_ylim(bottom=0.01)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tp_communication_analysis.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"NVLink is ~18x faster than InfiniBand -- that is why TP stays within one node.\")"
   ],
   "id": "cell_93"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_25_full_transformer",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/25_full_transformer.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ðŸŽ¯ Full Tensor-Parallel Transformer\n",
    "\n",
    "Let us build a multi-layer Transformer, run both single-GPU and tensor-parallel forward passes, and verify they produce identical results."
   ],
   "id": "cell_94"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorParallelTransformer:\n",
    "    \"\"\"\n",
    "    A multi-layer Transformer with tensor parallelism.\n",
    "\n",
    "    Simulates a real TP setup where:\n",
    "    - Q, K, V projections are split column-wise (by heads)\n",
    "    - MLP first linear is column-wise, second is row-wise\n",
    "    - Only 2 AllReduces per layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, hidden_dim, num_heads,\n",
    "                 intermediate_dim, world_size=2):\n",
    "        self.num_layers = num_layers\n",
    "        self.world_size = world_size\n",
    "        self.blocks = []\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            torch.manual_seed(i * 1000)\n",
    "            block = TensorParallelTransformerBlock(\n",
    "                hidden_dim, num_heads, intermediate_dim, world_size\n",
    "            )\n",
    "            self.blocks.append(block)"
   ],
   "id": "cell_95"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add forward methods for both single-GPU and tensor-parallel execution."
   ],
   "id": "cell_96"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_forward_single(model, x):\n",
    "    \"\"\"Non-parallel forward (ground truth).\"\"\"\n",
    "    for block in model.blocks:\n",
    "        x = block.forward_single(x)\n",
    "    return x\n",
    "\n",
    "def transformer_forward_tp(model, x):\n",
    "    \"\"\"Tensor-parallel forward.\"\"\"\n",
    "    for block in model.blocks:\n",
    "        x = block.forward_tp(x)\n",
    "    return x\n",
    "\n",
    "TensorParallelTransformer.forward_single = lambda self, x: transformer_forward_single(self, x)\n",
    "TensorParallelTransformer.forward_tp = lambda self, x: transformer_forward_tp(self, x)"
   ],
   "id": "cell_97"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and test the full transformer."
   ],
   "id": "cell_98"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and test the full transformer\n",
    "torch.manual_seed(42)\n",
    "num_layers = 4\n",
    "hidden_dim = 512\n",
    "num_heads = 8\n",
    "intermediate_dim = 1024  # 2x expansion\n",
    "world_size = 2\n",
    "seq_len = 32\n",
    "batch_size = 2\n",
    "\n",
    "print(\"Building Tensor-Parallel Transformer...\")\n",
    "print(f\"  Layers: {num_layers}\")\n",
    "print(f\"  Hidden dim: {hidden_dim}\")\n",
    "print(f\"  Attention heads: {num_heads} ({num_heads // world_size} per GPU)\")\n",
    "print(f\"  Intermediate dim: {intermediate_dim}\")\n",
    "print(f\"  TP world_size: {world_size}\")\n",
    "print(f\"  Input: ({batch_size}, {seq_len}, {hidden_dim})\")\n",
    "print()\n",
    "\n",
    "model = TensorParallelTransformer(\n",
    "    num_layers, hidden_dim, num_heads, intermediate_dim, world_size)\n",
    "X_final = torch.randn(batch_size, seq_len, hidden_dim)"
   ],
   "id": "cell_99"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run both forward passes and compare."
   ],
   "id": "cell_100"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward passes\n",
    "Y_single = model.forward_single(X_final)\n",
    "Y_tp = model.forward_tp(X_final)\n",
    "\n",
    "match = torch.allclose(Y_tp, Y_single, atol=1e-3)\n",
    "max_err = (Y_tp - Y_single).abs().max().item()\n",
    "mean_err = (Y_tp - Y_single).abs().mean().item()\n",
    "\n",
    "print(f\"Output shape: {Y_tp.shape}\")\n",
    "print(f\"Matches single-GPU: {match}\")\n",
    "print(f\"Max absolute error: {max_err:.2e}\")\n",
    "print(f\"Mean absolute error: {mean_err:.2e}\")"
   ],
   "id": "cell_101"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the detailed communication breakdown."
   ],
   "id": "cell_102"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_26_communication_breakdown",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/26_communication_breakdown.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communication breakdown\n",
    "total_allreduces = num_layers * 2  # 2 per layer (attention + MLP)\n",
    "elements_per_allreduce = batch_size * seq_len * hidden_dim\n",
    "bytes_per_allreduce = elements_per_allreduce * 2  # fp16\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  COMMUNICATION VOLUME BREAKDOWN\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  AllReduces per layer:     2 (attention + MLP)\")\n",
    "print(f\"  Total AllReduces:         {total_allreduces}\")\n",
    "print(f\"  Elements per AllReduce:   {elements_per_allreduce:,}\")\n",
    "print(f\"  Bytes per AllReduce:      {bytes_per_allreduce:,} ({bytes_per_allreduce/1024:.1f} KB)\")\n",
    "print(f\"  Total communication:      {total_allreduces * bytes_per_allreduce:,} bytes\")\n",
    "print(f\"                            ({total_allreduces * bytes_per_allreduce / 1e6:.2f} MB)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"  Memory saved per GPU:     ~{1 - 1/world_size:.0%} of weight memory\")\n",
    "print(f\"  Compute per GPU:          ~{1/world_size:.0%} of FLOPs\")"
   ],
   "id": "cell_103"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_27_error_memory_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/27_error_memory_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Visualization: Error and Memory Analysis"
   ],
   "id": "cell_104"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output comparison heatmap and per-layer error\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 1. Absolute error map\n",
    "diff = (Y_tp - Y_single).abs()\n",
    "im0 = axes[0].imshow(diff[0].detach().numpy(), cmap='hot', aspect='auto')\n",
    "axes[0].set_title(f'Absolute Error Map\\n(batch=0, max={max_err:.1e})',\n",
    "                   fontsize=11, fontweight='bold')\n",
    "axes[0].set_xlabel('Hidden dimension')\n",
    "axes[0].set_ylabel('Sequence position')\n",
    "plt.colorbar(im0, ax=axes[0], shrink=0.8)\n",
    "\n",
    "# 2. Per-layer error accumulation\n",
    "layer_errors = []\n",
    "x_single = X_final.clone()\n",
    "x_tp = X_final.clone()\n",
    "for i, blk in enumerate(model.blocks):\n",
    "    x_single = blk.forward_single(x_single)\n",
    "    x_tp = blk.forward_tp(x_tp)\n",
    "    layer_errors.append((x_tp - x_single).abs().max().item())\n",
    "\n",
    "axes[1].plot(range(1, num_layers + 1), layer_errors, 'o-',\n",
    "             color='#e74c3c', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Layer', fontsize=11)\n",
    "axes[1].set_ylabel('Max Absolute Error', fontsize=11)\n",
    "axes[1].set_title('Error Accumulation Across Layers',\n",
    "                   fontsize=11, fontweight='bold')\n",
    "axes[1].set_xticks(range(1, num_layers + 1))\n",
    "axes[1].grid(True, alpha=0.3)"
   ],
   "id": "cell_105"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the memory savings panel."
   ],
   "id": "cell_106"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Memory savings bar chart\n",
    "tp_degrees = [1, 2, 4, 8]\n",
    "total_params = num_layers * (\n",
    "    3 * hidden_dim * hidden_dim +     # Q, K, V projections\n",
    "    hidden_dim * hidden_dim +          # Output projection\n",
    "    hidden_dim * intermediate_dim +    # MLP W1\n",
    "    intermediate_dim * hidden_dim      # MLP W2\n",
    ")\n",
    "mem_per_gpu = [total_params * 2 / tp / 1e6 for tp in tp_degrees]\n",
    "\n",
    "bars = axes[2].bar(range(len(tp_degrees)), mem_per_gpu,\n",
    "                    color=['#e74c3c', '#3498db', '#2ecc71', '#f39c12'],\n",
    "                    edgecolor='black', linewidth=1.2)\n",
    "axes[2].set_xticks(range(len(tp_degrees)))\n",
    "axes[2].set_xticklabels([f'TP={tp}' for tp in tp_degrees])\n",
    "axes[2].set_ylabel('Weight Memory per GPU (MB)', fontsize=11)\n",
    "axes[2].set_title(f'Memory Savings\\n({num_layers}-layer model)',\n",
    "                   fontsize=11, fontweight='bold')\n",
    "for bar, mem in zip(bars, mem_per_gpu):\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2,\n",
    "                 f'{mem:.1f} MB', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tensor_parallelism_final.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "cell_107"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_28_unified_demo_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/28_unified_demo_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Putting It All Together\n",
    "\n",
    "Now that we have built every component individually, let us combine them into a unified end-to-end demo. We will create a single input, run it through tensor-parallel attention, a tensor-parallel MLP, and the full Transformer â€” then compare all outputs side by side."
   ],
   "id": "cell_108"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified end-to-end demo: all TP components in one pipeline\n",
    "torch.manual_seed(2024)\n",
    "\n",
    "# Shared configuration\n",
    "demo_hidden = 64\n",
    "demo_heads = 8\n",
    "demo_intermediate = 128\n",
    "demo_seq = 16\n",
    "demo_batch = 2\n",
    "demo_ws = 2\n",
    "\n",
    "x_demo = torch.randn(demo_batch, demo_seq, demo_hidden)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  UNIFIED TENSOR PARALLELISM DEMO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Input shape: ({demo_batch}, {demo_seq}, {demo_hidden})\")\n",
    "print(f\"  World size (simulated GPUs): {demo_ws}\")\n",
    "print()"
   ],
   "id": "cell_109"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run the standalone tensor-parallel attention module."
   ],
   "id": "cell_110"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_29_unified_demo_run",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/29_unified_demo_run.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Component 1: TP Attention ---\n",
    "attn_demo = TensorParallelMultiHeadAttention(demo_hidden, demo_heads, demo_ws)\n",
    "y_attn_single = tp_attention_forward_single(attn_demo, x_demo)\n",
    "y_attn_tp = tp_attention_forward_parallel(attn_demo, x_demo)\n",
    "attn_match = torch.allclose(y_attn_tp, y_attn_single, atol=1e-5)\n",
    "attn_err = (y_attn_tp - y_attn_single).abs().max().item()\n",
    "\n",
    "print(\"Component 1: TP Multi-Head Attention\")\n",
    "print(f\"  Heads total: {demo_heads}, Heads per GPU: {demo_heads // demo_ws}\")\n",
    "print(f\"  Output shape: {y_attn_tp.shape}\")\n",
    "print(f\"  Single-GPU match: {attn_match}  (max error: {attn_err:.2e})\")\n",
    "print()"
   ],
   "id": "cell_111"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the Megatron-style tensor-parallel MLP."
   ],
   "id": "cell_112"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Component 2: TP MLP ---\n",
    "torch.manual_seed(2024)\n",
    "W1_demo = torch.randn(demo_hidden, demo_intermediate) * 0.02\n",
    "b1_demo = torch.zeros(demo_intermediate)\n",
    "W2_demo = torch.randn(demo_intermediate, demo_hidden) * 0.02\n",
    "b2_demo = torch.zeros(demo_hidden)\n",
    "\n",
    "mlp_demo = MegatronMLP(W1_demo, b1_demo, W2_demo, b2_demo, demo_ws)\n",
    "\n",
    "# Use a 2D input for the MLP (flatten seq into batch)\n",
    "x_mlp_demo = x_demo.view(-1, demo_hidden)\n",
    "y_mlp_single = mlp_demo.forward_single_gpu(x_mlp_demo)\n",
    "y_mlp_tp = mlp_demo.forward_tp(x_mlp_demo)\n",
    "mlp_match = torch.allclose(y_mlp_tp, y_mlp_single, atol=1e-5)\n",
    "mlp_err = (y_mlp_tp - y_mlp_single).abs().max().item()\n",
    "\n",
    "print(\"Component 2: Megatron TP MLP\")\n",
    "print(f\"  Intermediate dim: {demo_intermediate}\")\n",
    "print(f\"  Output shape: {y_mlp_tp.shape}\")\n",
    "print(f\"  Single-GPU match: {mlp_match}  (max error: {mlp_err:.2e})\")\n",
    "print()"
   ],
   "id": "cell_113"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the full multi-layer Transformer combining both components."
   ],
   "id": "cell_114"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Component 3: Full TP Transformer ---\n",
    "demo_layers = 3\n",
    "model_demo = TensorParallelTransformer(\n",
    "    demo_layers, demo_hidden, demo_heads, demo_intermediate, demo_ws)\n",
    "\n",
    "y_model_single = model_demo.forward_single(x_demo)\n",
    "y_model_tp = model_demo.forward_tp(x_demo)\n",
    "model_match = torch.allclose(y_model_tp, y_model_single, atol=1e-3)\n",
    "model_err = (y_model_tp - y_model_single).abs().max().item()\n",
    "\n",
    "print(f\"Component 3: Full TP Transformer ({demo_layers} layers)\")\n",
    "print(f\"  Output shape: {y_model_tp.shape}\")\n",
    "print(f\"  Single-GPU match: {model_match}  (max error: {model_err:.2e})\")\n",
    "print(f\"  AllReduces per forward: {demo_layers * 2}\")\n",
    "print()"
   ],
   "id": "cell_115"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary table of the unified demo results."
   ],
   "id": "cell_116"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"=\" * 60)\n",
    "print(\"  SUMMARY: ALL COMPONENTS VERIFIED\")\n",
    "print(\"=\" * 60)\n",
    "results = [\n",
    "    (\"TP Attention\", attn_match, attn_err, 1),\n",
    "    (\"TP MLP (Megatron)\", mlp_match, mlp_err, 1),\n",
    "    (f\"TP Transformer ({demo_layers}L)\", model_match, model_err, demo_layers * 2),\n",
    "]\n",
    "print(f\"  {'Component':<25} {'Match':<8} {'Max Error':<12} {'AllReduces'}\")\n",
    "print(f\"  {'-'*25} {'-'*8} {'-'*12} {'-'*10}\")\n",
    "for name, m, e, ar in results:\n",
    "    status = \"PASS\" if m else \"FAIL\"\n",
    "    print(f\"  {name:<25} {status:<8} {e:<12.2e} {ar}\")\n",
    "\n",
    "print()\n",
    "print(\"  Every component produces identical results to single-GPU computation.\")\n",
    "print(\"  The ONLY overhead is the AllReduce communication at known points.\")"
   ],
   "id": "cell_117"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the output distributions for all three components side by side."
   ],
   "id": "cell_118"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_30_unified_demo_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/30_unified_demo_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: output distributions for all 3 components\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, (name, y_s, y_t) in zip(axes, [\n",
    "    (\"TP Attention\", y_attn_single, y_attn_tp),\n",
    "    (\"TP MLP\", y_mlp_single.view(demo_batch, demo_seq, demo_hidden),\n",
    "               y_mlp_tp.view(demo_batch, demo_seq, demo_hidden)),\n",
    "    (f\"TP Transformer ({demo_layers}L)\", y_model_single, y_model_tp),\n",
    "]):\n",
    "    vals_s = y_s.detach().flatten().numpy()\n",
    "    vals_t = y_t.detach().flatten().numpy()\n",
    "    ax.hist(vals_s, bins=50, alpha=0.5, label='Single GPU', color='#3498db')\n",
    "    ax.hist(vals_t, bins=50, alpha=0.5, label='Tensor Parallel', color='#e74c3c')\n",
    "    ax.set_title(name, fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.set_xlabel('Activation value')\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "plt.suptitle('Output Distributions: Single-GPU vs Tensor-Parallel',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('putting_it_all_together.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"The distributions overlap perfectly â€” TP is mathematically exact.\")"
   ],
   "id": "cell_119"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_31_celebration",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Celebration\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/31_celebration.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"  You Have Implemented Tensor Parallelism from Scratch!\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"  What you built:\")\n",
    "print(\"  - Column-wise parallel linear layer\")\n",
    "print(\"  - Row-wise parallel linear layer\")\n",
    "print(\"  - Megatron-LM style tensor-parallel MLP\")\n",
    "print(\"  - Tensor-parallel multi-head attention\")\n",
    "print(\"  - Full tensor-parallel Transformer (4 layers)\")\n",
    "print(\"  - Verified correctness against non-parallel computation\")\n",
    "print(\"  - Analyzed communication costs\")\n",
    "print(\"  - Unified demo combining all components\")\n",
    "print()\n",
    "print(\"  Key takeaways:\")\n",
    "print(\"  - TP splits individual weight matrices across GPUs\")\n",
    "print(\"  - Column split -> concatenate; Row split -> AllReduce (sum)\")\n",
    "print(\"  - Megatron-LM: column-wise first, GeLU locally, row-wise second\")\n",
    "print(\"  - Only 2 AllReduces per Transformer layer\")\n",
    "print(\"  - TP MUST use NVLink (within a single node)\")\n",
    "print()\n",
    "print(\"  Next up: Pipeline Parallelism -- splitting by model DEPTH!\")"
   ],
   "id": "cell_120"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_32_reflection_and_closing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Wrap-Up\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/32_reflection_and_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### ðŸ¤” Reflection Questions\n",
    "\n",
    "1. **Why does Megatron-LM use column-wise split for the first MLP layer and row-wise for the second?**\n",
    "   Think about the shapes. The column-wise output naturally splits the intermediate features across GPUs. The row-wise input expects its input to be split across GPUs. These dovetail perfectly â€” no communication needed between them (except the elementwise GeLU).\n",
    "\n",
    "2. **Why can GeLU be applied locally without communication?**\n",
    "   GeLU is elementwise â€” it operates on each value independently. After the column-wise split, each GPU has a complete (though smaller) set of features. Applying GeLU to these local features is exactly the same as applying it to the corresponding entries in the full tensor."
   ],
   "id": "cell_121"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick experiment: verify that GeLU commutes with column splitting\n",
    "# This cements the intuition from Reflection Question 2\n",
    "torch.manual_seed(42)\n",
    "x_verify = torch.randn(4, 12)\n",
    "full_gelu = F.gelu(x_verify)\n",
    "split_gelu = torch.cat([F.gelu(x_verify[:, :6]), F.gelu(x_verify[:, 6:])], dim=1)\n",
    "print(f\"GeLU(full) == cat(GeLU(split_0), GeLU(split_1)): {torch.allclose(full_gelu, split_gelu)}\")\n",
    "print(\"Elementwise operations always commute with column splitting.\")"
   ],
   "id": "cell_122"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Why is TP limited to within a single node in practice?**\n",
    "   TP communicates inside every single layer (2 AllReduces per layer). For a 70B model with 80 layers, that is 160 AllReduces per micro-batch. This communication must be fast enough to not bottleneck the computation. NVLink provides 900 GB/s, while cross-node InfiniBand is ~18x slower at ~50 GB/s.\n",
    "\n",
    "### ðŸ† Optional Challenges\n",
    "\n",
    "1. **Vary the TP degree**: Modify the code to test TP with `world_size=4` and `world_size=8`. Verify correctness still holds. How does the error change?\n",
    "\n",
    "2. **fp16 precision**: Convert the weights and inputs to `torch.float16` and re-run. Does the error increase significantly? This is a real concern in production systems.\n",
    "\n",
    "3. **Asymmetric split**: What if the weight matrix dimensions are not evenly divisible by `world_size`? Implement a version that handles padding or uneven shards.\n",
    "\n",
    "4. **Compute-to-communication ratio**: For a given `hidden_dim` and `seq_len`, calculate the ratio of FLOPs (compute) to bytes transferred (communication). Plot this ratio as `hidden_dim` grows. Why does TP become more efficient for larger models?"
   ],
   "id": "cell_123"
  }
 ]
}