{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Sequence & Expert Parallelism from Scratch ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1ot40UkYIUGOKX6cO28mOsGHwC-srClZB\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Sequence & Expert Parallelism: The Final Two Dimensions\n",
    "\n",
    "*Part 5 of the Vizuara series on 5D Parallelism from Scratch*\n",
    "*Estimated time: 50 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ü§ñ AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** ‚Äî it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[üëâ Open AI Teaching Assistant](https://course-creator-brown.vercel.app/courses/5d-parallelism-from-scratch/practice/5/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_why_it_matters",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "We have now built three of the five dimensions of modern parallelism: **Data Parallelism** (split the batch), **Tensor Parallelism** (split the layers), and **Pipeline Parallelism** (split the depth). These three alone powered most large model training through 2022.\n",
    "\n",
    "But two critical challenges remain:\n",
    "\n",
    "**Challenge 1 ‚Äî Sequences are getting absurdly long.** GPT-3 was trained with 2,048 tokens of context. Claude now handles 200,000. Google Gemini processes over 1,000,000. The memory required for attention scales as $O(n^2)$ ‚Äî at 128K tokens, a single attention matrix consumes over 60 GB. No single GPU can hold that. We need **Sequence Parallelism**.\n",
    "\n",
    "**Challenge 2 ‚Äî We want massive models but cannot afford to activate all parameters.** DeepSeek-V3 has 671 billion parameters, yet only 37 billion are active for any given token. The trick? A **Mixture of Experts** ‚Äî hundreds of specialist sub-networks, with a learned router that dispatches each token to just a few relevant experts. We need **Expert Parallelism** to place those experts across GPUs.\n",
    "\n",
    "By the end of this notebook, you will have:\n",
    "- Built a **Ring Attention** simulation that distributes a long sequence across 4 GPUs in a ring, with K and V blocks rotating until every query has seen every key\n",
    "- Implemented a complete **Mixture-of-Experts layer** with top-K gating, token routing, and load-balancing loss\n",
    "- Verified both against their non-parallel counterparts\n",
    "- Understood every equation that makes these systems work"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_02_setup_imports",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/02_setup_imports.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup ‚Äî Run this cell first\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import Tuple, List\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_03_sequence_parallelism_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Sequence Parallelism Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/03_sequence_parallelism_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part A: Sequence Parallelism\n",
    "\n",
    "## 2. Building Intuition ‚Äî Sequence Parallelism\n",
    "\n",
    "Imagine you are the editor of a publishing house, and you need to translate a 500-page book into another language. You have four translators on staff. The obvious approach: give each translator 125 pages.\n",
    "\n",
    "This works beautifully for most of the text. But occasionally a pronoun on page 126 refers back to a character introduced on page 40 ‚Äî now translator 2 needs information from translator 1. The translators must coordinate at these boundary points.\n",
    "\n",
    "This is exactly the situation in Transformer models processing long sequences:\n",
    "- **The \"pages\"** are tokens in the sequence\n",
    "- **The \"translators\"** are GPUs\n",
    "- **The \"cross-references\"** are attention ‚Äî every token must attend to every other token\n",
    "\n",
    "There are two flavors of sequence parallelism, each solving a different problem:\n",
    "\n",
    "### Flavor 1: Megatron-Style SP ‚Äî Fixing Waste in Tensor Parallelism\n",
    "\n",
    "Recall from Notebook 3 that Tensor Parallelism (TP) splits the weight matrices across GPUs. The attention and MLP computations are split. But what about **LayerNorm** and **Dropout**? In standard TP, these operations are simply **replicated** on every GPU ‚Äî each GPU computes the identical LayerNorm on the identical input. That is pure waste.\n",
    "\n",
    "Megatron-Style SP fixes this: instead of replicating LayerNorm and Dropout, we **split them along the sequence dimension**. Each GPU handles `seq_len/N` tokens for these operations. The communication pattern changes from AllReduce to ReduceScatter + AllGather, but the total bytes transferred stay the same. We get a memory savings for free.\n",
    "\n",
    "### Flavor 2: Ring Attention ‚Äî For Truly Long Sequences\n",
    "\n",
    "When sequences reach 100K to 1M+ tokens, even the attention computation itself is too large for a single GPU. Ring Attention solves this by arranging GPUs in a logical ring:\n",
    "\n",
    "1. Split the sequence into chunks ‚Äî each GPU holds one chunk of Q, K, and V\n",
    "2. Each GPU computes attention between its local Q and the K, V it currently holds\n",
    "3. Pass K and V to the next GPU in the ring\n",
    "4. Repeat until every Q chunk has seen every K, V chunk\n",
    "5. Combine the partial attention results using a numerically stable trick\n",
    "\n",
    "The beautiful part: while one GPU is computing attention, the next GPU's K, V is already being transmitted. **Communication overlaps with computation.**\n",
    "\n",
    "We will focus our implementation on Ring Attention, since it is the more general and conceptually interesting of the two."
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§î Think About This\n",
    "\n",
    "Before we proceed, consider:\n",
    "- In Ring Attention, each GPU computes partial attention scores across multiple rounds. But softmax requires the *global* maximum for numerical stability. How can each GPU compute a correct softmax if it only sees a fraction of the keys at a time?\n",
    "- Why arrange the GPUs in a *ring* rather than having all GPUs broadcast their K, V to everyone?\n",
    "\n",
    "*Hold these questions in mind. The answers will emerge from the math below.*"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick status check\n",
    "print(\"Section checkpoint ‚úì\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_04_ring_attention_math",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Ring Attention Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/04_ring_attention_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics ‚Äî Ring Attention\n",
    "\n",
    "### Standard Attention (the baseline)\n",
    "\n",
    "For a single head, standard scaled dot-product attention on a full sequence is:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "where $Q, K, V \\in \\mathbb{R}^{n \\times d_k}$ and $n$ is the sequence length. The attention matrix $QK^T$ is $n \\times n$, which is where the $O(n^2)$ memory comes from.\n",
    "\n",
    "**Computationally:** We compute an $n \\times n$ score matrix, apply softmax row-wise, then multiply by $V$ to get the output. At $n = 128{,}000$ and $d_k = 128$, the score matrix alone is $128{,}000^2 \\times 4$ bytes $\\approx$ 62 GB in fp32. That does not fit on any single GPU.\n",
    "\n",
    "### Chunked Attention\n",
    "\n",
    "Suppose we split the sequence into $P$ chunks of size $c = n / P$. GPU $i$ holds:\n",
    "- $Q_i \\in \\mathbb{R}^{c \\times d_k}$ ‚Äî its query chunk\n",
    "- $K_i, V_i \\in \\mathbb{R}^{c \\times d_k}$ ‚Äî its key and value chunks\n",
    "\n",
    "The full attention output for GPU $i$'s queries requires computing against *all* keys and values:\n",
    "\n",
    "$$O_i = \\text{softmax}\\left(\\frac{Q_i [K_0; K_1; \\ldots; K_{P-1}]^T}{\\sqrt{d_k}}\\right) [V_0; V_1; \\ldots; V_{P-1}]$$\n",
    "\n",
    "**Let us plug in some simple numbers.** With $P = 4$ GPUs and $n = 1024$ tokens, each chunk is $c = 256$ tokens. GPU 0 needs to compute attention of its 256 queries against all 1,024 keys. Instead of materializing the full $256 \\times 1024$ score matrix at once, we compute four blocks of $256 \\times 256$ scores ‚Äî one per round ‚Äî and combine them.\n",
    "\n",
    "### The Online Softmax Trick\n",
    "\n",
    "The key mathematical insight is that we can compute softmax **incrementally**. For GPU $i$, after processing the key chunk from round $j$, we have partial scores:\n",
    "\n",
    "$$S_{ij} = \\frac{Q_i K_j^T}{\\sqrt{d_k}} \\in \\mathbb{R}^{c \\times c}$$\n",
    "\n",
    "We maintain running statistics for each query row $q$:\n",
    "- $m_q$: the running maximum score (for numerical stability)\n",
    "- $\\ell_q$: the running sum of exponentiated scores\n",
    "- $o_q$: the running weighted output\n",
    "\n",
    "After seeing a new chunk of scores $s_{\\text{new}}$ (a row of $S_{ij}$):\n",
    "\n",
    "$$m_{\\text{new}} = \\max(m_q, \\max(s_{\\text{new}}))$$\n",
    "$$\\ell_q \\leftarrow \\ell_q \\cdot e^{m_q - m_{\\text{new}}} + \\sum_k e^{s_{\\text{new},k} - m_{\\text{new}}}$$\n",
    "$$o_q \\leftarrow o_q \\cdot e^{m_q - m_{\\text{new}}} + \\sum_k e^{s_{\\text{new},k} - m_{\\text{new}}} \\cdot V_j[k]$$\n",
    "\n",
    "After all $P$ rounds:\n",
    "\n",
    "$$\\text{output}_q = o_q / \\ell_q$$\n",
    "\n",
    "**What this says computationally:** We process one block of keys at a time, updating the running maximum, running sum of exponentials, and running weighted sum. The $e^{m_{\\text{old}} - m_{\\text{new}}}$ correction rescales the old statistics when a new maximum is discovered. After all chunks, we divide by the total sum to get the correct softmax-weighted output ‚Äî identical to computing standard attention on the full sequence."
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_05_standard_attention_baseline",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/05_standard_attention_baseline.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let Us Build It ‚Äî Ring Attention\n",
    "\n",
    "### 4.1 Standard Attention (Baseline)\n",
    "\n",
    "First, let us implement standard attention so we have a ground truth to verify against."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Standard scaled dot-product attention.\n",
    "\n",
    "    Args:\n",
    "        Q: (seq_len, d_k)\n",
    "        K: (seq_len, d_k)\n",
    "        V: (seq_len, d_k)\n",
    "\n",
    "    Returns:\n",
    "        output: (seq_len, d_k)\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    # Score matrix: (seq_len, seq_len)\n",
    "    scores = Q @ K.T / math.sqrt(d_k)\n",
    "    # Softmax over keys dimension\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    # Weighted sum of values\n",
    "    output = attn_weights @ V\n",
    "    return output"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run a quick test to verify our baseline attention implementation works correctly."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "seq_len, d_k = 16, 8\n",
    "Q_test = torch.randn(seq_len, d_k)\n",
    "K_test = torch.randn(seq_len, d_k)\n",
    "V_test = torch.randn(seq_len, d_k)\n",
    "\n",
    "output_standard = standard_attention(Q_test, K_test, V_test)\n",
    "print(f\"Input shape:  Q={Q_test.shape}, K={K_test.shape}, V={V_test.shape}\")\n",
    "print(f\"Output shape: {output_standard.shape}\")\n",
    "print(f\"Output[:2, :4]:\\n{output_standard[:2, :4]}\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_chunking_sequence",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_chunking_sequence.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Chunking the Sequence\n",
    "\n",
    "Now let us split Q, K, V into chunks and assign each to a simulated \"GPU.\""
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_sequence(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor,\n",
    "                   num_gpus: int) -> Tuple[List[torch.Tensor], ...]:\n",
    "    \"\"\"\n",
    "    Split Q, K, V evenly across simulated GPUs.\n",
    "\n",
    "    Args:\n",
    "        Q, K, V: (seq_len, d_k)\n",
    "        num_gpus: number of chunks\n",
    "\n",
    "    Returns:\n",
    "        Q_chunks, K_chunks, V_chunks: lists of (chunk_size, d_k) tensors\n",
    "    \"\"\"\n",
    "    Q_chunks = list(Q.chunk(num_gpus, dim=0))\n",
    "    K_chunks = list(K.chunk(num_gpus, dim=0))\n",
    "    V_chunks = list(V.chunk(num_gpus, dim=0))\n",
    "    return Q_chunks, K_chunks, V_chunks"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a longer sequence, chunk it across 4 simulated GPUs, and inspect the resulting shapes."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a longer sequence and chunk it\n",
    "NUM_GPUS = 4\n",
    "seq_len, d_k = 1024, 64\n",
    "torch.manual_seed(42)\n",
    "\n",
    "Q_full = torch.randn(seq_len, d_k)\n",
    "K_full = torch.randn(seq_len, d_k)\n",
    "V_full = torch.randn(seq_len, d_k)\n",
    "\n",
    "Q_chunks, K_chunks, V_chunks = chunk_sequence(Q_full, K_full, V_full, NUM_GPUS)\n",
    "\n",
    "chunk_size = seq_len // NUM_GPUS\n",
    "print(f\"Full sequence length: {seq_len}\")\n",
    "print(f\"Number of GPUs: {NUM_GPUS}\")\n",
    "print(f\"Chunk size per GPU: {chunk_size}\")\n",
    "for i in range(NUM_GPUS):\n",
    "    print(f\"  GPU {i}: Q={Q_chunks[i].shape}, K={K_chunks[i].shape}, V={V_chunks[i].shape}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_07_ring_attention_step",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/07_ring_attention_step.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Ring Attention ‚Äî Step by Step\n",
    "\n",
    "Here is the core algorithm. Each GPU starts with its own K, V chunk. In each round, every GPU:\n",
    "1. Computes partial attention scores between its Q and the K chunk it currently holds\n",
    "2. Updates running statistics (online softmax)\n",
    "3. Passes its K, V chunk to the next GPU in the ring\n",
    "\n",
    "The function below implements a single step ‚Äî processing one K,V block and updating the running online softmax statistics."
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ring_attention_step(Q_local: torch.Tensor, K_block: torch.Tensor,\n",
    "                        V_block: torch.Tensor, running_max: torch.Tensor,\n",
    "                        running_sum: torch.Tensor, running_output: torch.Tensor,\n",
    "                        d_k: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    One step of ring attention: process one K,V block and update running statistics.\n",
    "\n",
    "    Uses the online softmax trick for numerical stability.\n",
    "\n",
    "    Args:\n",
    "        Q_local: (chunk_size, d_k) ‚Äî this GPU's queries (fixed)\n",
    "        K_block: (chunk_size, d_k) ‚Äî the K chunk currently held\n",
    "        V_block: (chunk_size, d_k) ‚Äî the V chunk currently held\n",
    "        running_max: (chunk_size, 1) ‚Äî running row-wise max of scores\n",
    "        running_sum: (chunk_size, 1) ‚Äî running sum of exp(scores - max)\n",
    "        running_output: (chunk_size, d_k) ‚Äî running weighted output\n",
    "        d_k: dimension for scaling\n",
    "\n",
    "    Returns:\n",
    "        Updated (running_max, running_sum, running_output)\n",
    "    \"\"\"\n",
    "    # Compute attention scores for this block: (chunk_size, chunk_size)\n",
    "    scores = Q_local @ K_block.T / math.sqrt(d_k)\n",
    "\n",
    "    # Row-wise max of the new scores: (chunk_size, 1)\n",
    "    block_max = scores.max(dim=-1, keepdim=True).values"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key insight of online softmax: we update the running maximum and rescale previous statistics. This lets us combine partial attention results from different K,V blocks without ever materializing the full attention matrix."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # (Continued from ring_attention_step)\n",
    "    # New global max across old and new\n",
    "    new_max = torch.maximum(running_max, block_max)\n",
    "\n",
    "    # Rescale old statistics to the new maximum\n",
    "    correction_old = torch.exp(running_max - new_max)\n",
    "    correction_new = torch.exp(scores - new_max)  # (chunk_size, chunk_size)\n",
    "\n",
    "    # Update running sum of exponentials\n",
    "    new_sum = running_sum * correction_old + correction_new.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    # Update running weighted output\n",
    "    new_output = running_output * correction_old + correction_new @ V_block\n",
    "\n",
    "    return new_max, new_sum, new_output"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** The code above shows the logic in two parts for readability, but the actual `ring_attention_step` function is defined as a single unit. Let us now define the complete function for use in the simulation."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ring_attention_step(Q_local, K_block, V_block,\n",
    "                        running_max, running_sum, running_output, d_k):\n",
    "    \"\"\"One step of ring attention with online softmax update.\"\"\"\n",
    "    scores = Q_local @ K_block.T / math.sqrt(d_k)\n",
    "    block_max = scores.max(dim=-1, keepdim=True).values\n",
    "    new_max = torch.maximum(running_max, block_max)\n",
    "    correction_old = torch.exp(running_max - new_max)\n",
    "    correction_new = torch.exp(scores - new_max)\n",
    "    new_sum = running_sum * correction_old + correction_new.sum(dim=-1, keepdim=True)\n",
    "    new_output = running_output * correction_old + correction_new @ V_block\n",
    "    return new_max, new_sum, new_output"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_08_full_ring_simulation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/08_full_ring_simulation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us simulate the full ring. We will track what happens on each GPU across all rounds. Each GPU initializes running statistics, then iterates through `num_gpus` rounds ‚Äî computing attention with whatever K,V block it currently holds and rotating K,V to the next GPU after each round."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ring_attention(Q_chunks: List[torch.Tensor], K_chunks: List[torch.Tensor],\n",
    "                   V_chunks: List[torch.Tensor]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Full ring attention across all simulated GPUs.\n",
    "\n",
    "    Args:\n",
    "        Q_chunks: list of (chunk_size, d_k) query chunks\n",
    "        K_chunks: list of (chunk_size, d_k) key chunks\n",
    "        V_chunks: list of (chunk_size, d_k) value chunks\n",
    "\n",
    "    Returns:\n",
    "        output: (seq_len, d_k) ‚Äî concatenated outputs from all GPUs\n",
    "    \"\"\"\n",
    "    num_gpus = len(Q_chunks)\n",
    "    chunk_size, d_k = Q_chunks[0].shape\n",
    "\n",
    "    # Each GPU maintains running statistics\n",
    "    running_maxes = [torch.full((chunk_size, 1), -float('inf'))\n",
    "                     for _ in range(num_gpus)]\n",
    "    running_sums = [torch.zeros(chunk_size, 1) for _ in range(num_gpus)]\n",
    "    running_outputs = [torch.zeros(chunk_size, d_k) for _ in range(num_gpus)]\n",
    "\n",
    "    # Each GPU starts holding its own K, V chunk\n",
    "    current_kv = [(K_chunks[i].clone(), V_chunks[i].clone())\n",
    "                  for i in range(num_gpus)]"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inner loop: for each round, every GPU computes attention with its current K,V block, then K,V blocks rotate one step clockwise around the ring."
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # (Continued: ring rounds and final normalization)\n",
    "    for round_idx in range(num_gpus):\n",
    "        # Each GPU computes attention with whatever K,V it currently holds\n",
    "        for gpu_id in range(num_gpus):\n",
    "            K_block, V_block = current_kv[gpu_id]\n",
    "            running_maxes[gpu_id], running_sums[gpu_id], running_outputs[gpu_id] = \\\n",
    "                ring_attention_step(\n",
    "                    Q_chunks[gpu_id], K_block, V_block,\n",
    "                    running_maxes[gpu_id], running_sums[gpu_id],\n",
    "                    running_outputs[gpu_id], d_k\n",
    "                )\n",
    "\n",
    "        # Rotate K,V: each GPU sends its K,V to the next GPU in the ring\n",
    "        if round_idx < num_gpus - 1:\n",
    "            new_kv = [None] * num_gpus\n",
    "            for gpu_id in range(num_gpus):\n",
    "                next_gpu = (gpu_id + 1) % num_gpus\n",
    "                new_kv[next_gpu] = current_kv[gpu_id]\n",
    "            current_kv = new_kv\n",
    "\n",
    "    # Final normalization: output = running_output / running_sum\n",
    "    outputs = []\n",
    "    for gpu_id in range(num_gpus):\n",
    "        outputs.append(running_outputs[gpu_id] / running_sums[gpu_id])\n",
    "\n",
    "    return torch.cat(outputs, dim=0)"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, for the simulation to work as a single callable function, here is the complete `ring_attention` assembled:"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ring_attention(Q_chunks, K_chunks, V_chunks):\n",
    "    \"\"\"Full ring attention across all simulated GPUs.\"\"\"\n",
    "    num_gpus = len(Q_chunks)\n",
    "    chunk_size, d_k = Q_chunks[0].shape\n",
    "    running_maxes = [torch.full((chunk_size, 1), -float('inf'))\n",
    "                     for _ in range(num_gpus)]\n",
    "    running_sums = [torch.zeros(chunk_size, 1) for _ in range(num_gpus)]\n",
    "    running_outputs = [torch.zeros(chunk_size, d_k) for _ in range(num_gpus)]\n",
    "    current_kv = [(K_chunks[i].clone(), V_chunks[i].clone())\n",
    "                  for i in range(num_gpus)]\n",
    "    for round_idx in range(num_gpus):\n",
    "        for gpu_id in range(num_gpus):\n",
    "            K_block, V_block = current_kv[gpu_id]\n",
    "            running_maxes[gpu_id], running_sums[gpu_id], \\\n",
    "                running_outputs[gpu_id] = ring_attention_step(\n",
    "                    Q_chunks[gpu_id], K_block, V_block,\n",
    "                    running_maxes[gpu_id], running_sums[gpu_id],\n",
    "                    running_outputs[gpu_id], d_k)\n",
    "        if round_idx < num_gpus - 1:\n",
    "            new_kv = [None] * num_gpus\n",
    "            for gpu_id in range(num_gpus):\n",
    "                new_kv[(gpu_id + 1) % num_gpus] = current_kv[gpu_id]\n",
    "            current_kv = new_kv\n",
    "    outputs = [running_outputs[i] / running_sums[i] for i in range(num_gpus)]\n",
    "    return torch.cat(outputs, dim=0)\n",
    "\n",
    "# Run ring attention\n",
    "output_ring = ring_attention(Q_chunks, K_chunks, V_chunks)\n",
    "print(f\"Ring attention output shape: {output_ring.shape}\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_09_verification",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/09_verification.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Verifying Correctness\n",
    "\n",
    "The moment of truth: does our ring attention produce the same result as standard attention?"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification: Ring Attention vs Standard Attention\n",
    "output_standard_full = standard_attention(Q_full, K_full, V_full)\n",
    "\n",
    "max_error = (output_ring - output_standard_full).abs().max().item()\n",
    "mean_error = (output_ring - output_standard_full).abs().mean().item()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Ring Attention Correctness Check\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Max absolute error:  {max_error:.2e}\")\n",
    "print(f\"Mean absolute error: {mean_error:.2e}\")\n",
    "print(f\"Tolerance:           1e-5\")\n",
    "print()\n",
    "\n",
    "if max_error < 1e-5:\n",
    "    print(\"‚úÖ Ring Attention matches standard attention perfectly!\")\n",
    "    print(\"   The online softmax trick produces numerically identical results.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Errors detected ‚Äî check the implementation.\")\n",
    "\n",
    "# Show a few output values for comparison\n",
    "print(f\"\\nFirst 4 values, first 4 dims:\")\n",
    "print(f\"  Standard: {output_standard_full[:4, :4].tolist()}\")\n",
    "print(f\"  Ring:     {output_ring[:4, :4].tolist()}\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_10_viz_ring_rotation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/10_viz_ring_rotation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Visualization 1: The Ring Rotation\n",
    "\n",
    "Let us visualize how K and V blocks rotate around the ring across rounds. This is the core communication pattern of Ring Attention. We draw the 4 GPUs in a circle for each round, color-coded by which K,V block they currently hold."
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, NUM_GPUS, figsize=(16, 5))\n",
    "fig.suptitle(\"Ring Attention: K,V Block Rotation Across Rounds\",\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "\n",
    "colors = ['#2196F3', '#4CAF50', '#FF9800', '#E91E63']\n",
    "gpu_labels = [f'GPU {i}' for i in range(NUM_GPUS)]\n",
    "\n",
    "for round_idx in range(NUM_GPUS):\n",
    "    ax = axes[round_idx]\n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(f'Round {round_idx}', fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Draw 4 GPUs in a circle\n",
    "    angles = [math.pi/2 - i * 2 * math.pi / NUM_GPUS\n",
    "              for i in range(NUM_GPUS)]\n",
    "    positions = [(math.cos(a), math.sin(a)) for a in angles]\n",
    "\n",
    "    for gpu_id in range(NUM_GPUS):\n",
    "        x, y = positions[gpu_id]\n",
    "        kv_origin = (gpu_id - round_idx) % NUM_GPUS\n",
    "        circle = plt.Circle((x, y), 0.35, color=colors[kv_origin],\n",
    "                            alpha=0.8)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(x, y + 0.08, f'GPU {gpu_id}', ha='center',\n",
    "                va='center', fontsize=8, fontweight='bold', color='white')\n",
    "        ax.text(x, y - 0.12, f'K,V from\\nGPU {kv_origin}',\n",
    "                ha='center', va='center', fontsize=6, color='white')"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add ring arrows showing the K,V transfer direction, and a legend for the color coding."
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Draw ring arrows (except last round)\n",
    "    if round_idx < NUM_GPUS - 1:\n",
    "        for gpu_id in range(NUM_GPUS):\n",
    "            next_gpu = (gpu_id + 1) % NUM_GPUS\n",
    "            x1, y1 = positions[gpu_id]\n",
    "            x2, y2 = positions[next_gpu]\n",
    "            dx, dy = x2 - x1, y2 - y1\n",
    "            length = math.sqrt(dx**2 + dy**2)\n",
    "            # Shorten arrow to not overlap circles\n",
    "            shrink = 0.4 / length\n",
    "            ax.annotate('', xy=(x2 - dx*shrink, y2 - dy*shrink),\n",
    "                       xytext=(x1 + dx*shrink, y1 + dy*shrink),\n",
    "                       arrowprops=dict(arrowstyle='->', color='gray',\n",
    "                                      lw=1.5, alpha=0.5))\n",
    "\n",
    "# Legend\n",
    "legend_patches = [mpatches.Patch(color=colors[i],\n",
    "                  label=f'K,V originally from GPU {i}')\n",
    "                  for i in range(NUM_GPUS)]\n",
    "fig.legend(handles=legend_patches, loc='lower center', ncol=NUM_GPUS,\n",
    "           fontsize=9, bbox_to_anchor=(0.5, -0.05))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_11_viz_attention_blocks",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/11_viz_attention_blocks.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Visualization 2: Attention Matrix Filled Block-by-Block\n",
    "\n",
    "Each round, a new block-column of the attention matrix is computed. Let us visualize this progressive filling. We compute the full score matrix and then show which blocks have been computed after each round."
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, NUM_GPUS + 1, figsize=(18, 4))\n",
    "fig.suptitle(\"Attention Matrix Blocks Computed Per Round\",\n",
    "             fontsize=14, fontweight='bold', y=1.05)\n",
    "\n",
    "# Compute the full attention score matrix for visualization\n",
    "scores_full = Q_full @ K_full.T / math.sqrt(d_k)\n",
    "scores_np = scores_full.detach().numpy()\n",
    "\n",
    "# Progressively show which blocks are computed\n",
    "for round_idx in range(NUM_GPUS):\n",
    "    ax = axes[round_idx]\n",
    "    # Create a masked version showing only blocks computed so far\n",
    "    visible = np.full_like(scores_np, np.nan)\n",
    "    for r in range(round_idx + 1):\n",
    "        for gpu_id in range(NUM_GPUS):\n",
    "            kv_origin = (gpu_id - r) % NUM_GPUS\n",
    "            row_start = gpu_id * chunk_size\n",
    "            row_end = (gpu_id + 1) * chunk_size\n",
    "            col_start = kv_origin * chunk_size\n",
    "            col_end = (kv_origin + 1) * chunk_size\n",
    "            visible[row_start:row_end, col_start:col_end] = \\\n",
    "                scores_np[row_start:row_end, col_start:col_end]\n",
    "\n",
    "    im = ax.imshow(visible, cmap='viridis', aspect='auto',\n",
    "                   vmin=scores_np.min(), vmax=scores_np.max())\n",
    "    ax.set_title(f'After Round {round_idx}', fontsize=10, fontweight='bold')\n",
    "    ax.set_xlabel('Key positions')\n",
    "    if round_idx == 0:\n",
    "        ax.set_ylabel('Query positions')"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add grid lines between chunks to make the block structure clear, and show the full attention matrix for comparison."
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Draw grid lines between chunks\n",
    "    for i in range(1, NUM_GPUS):\n",
    "        ax.axhline(y=i*chunk_size - 0.5, color='white', linewidth=1.5)\n",
    "        ax.axvline(x=i*chunk_size - 0.5, color='white', linewidth=1.5)\n",
    "\n",
    "# Final panel: full matrix\n",
    "ax = axes[NUM_GPUS]\n",
    "im = ax.imshow(scores_np, cmap='viridis', aspect='auto')\n",
    "ax.set_title('Full Attention\\n(standard)', fontsize=10, fontweight='bold')\n",
    "ax.set_xlabel('Key positions')\n",
    "for i in range(1, NUM_GPUS):\n",
    "    ax.axhline(y=i*chunk_size - 0.5, color='white', linewidth=1.5)\n",
    "    ax.axvline(x=i*chunk_size - 0.5, color='white', linewidth=1.5)\n",
    "\n",
    "plt.colorbar(im, ax=axes.tolist(), shrink=0.8, label='Attention score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each round fills one diagonal band of blocks.\")\n",
    "print(\"After 4 rounds, the full attention matrix has been computed ‚Äî \"\n",
    "      \"but no single GPU ever held the entire matrix!\")"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_12_todo1_rotation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/12_todo1_rotation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîß Your Turn ‚Äî Sequence Parallelism\n",
    "\n",
    "### TODO 1: Implement the K,V Ring Rotation\n",
    "\n",
    "In Ring Attention, after each round, every GPU sends its K and V chunk to the next GPU in the ring (clockwise). This is the fundamental communication primitive.\n",
    "\n",
    "Implement the `rotate_kv_ring` function below."
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_kv_ring(kv_pairs: List[Tuple[torch.Tensor, torch.Tensor]]\n",
    "                   ) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Rotate K,V pairs one step clockwise around the ring.\n",
    "\n",
    "    GPU 0's K,V goes to GPU 1, GPU 1's goes to GPU 2,\n",
    "    ..., GPU (P-1)'s goes to GPU 0.\n",
    "\n",
    "    Args:\n",
    "        kv_pairs: list of (K, V) tuples, one per GPU.\n",
    "\n",
    "    Returns:\n",
    "        new_kv_pairs: list of (K, V) tuples after rotation.\n",
    "    \"\"\"\n",
    "    num_gpus = len(kv_pairs)\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Implement one clockwise rotation of K,V pairs.\n",
    "    # After rotation:\n",
    "    #   - GPU 1 should hold what GPU 0 had\n",
    "    #   - GPU 2 should hold what GPU 1 had\n",
    "    #   - ...\n",
    "    #   - GPU 0 should hold what GPU (P-1) had\n",
    "    #\n",
    "    # Hint: This is a circular shift.\n",
    "    # ==============================\n",
    "\n",
    "    new_kv_pairs = ???  # YOUR CODE HERE\n",
    "\n",
    "    return new_kv_pairs"
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_13_todo1_verify",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/13_todo1_verify.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the rotation with identifiable K,V pairs ‚Äî each GPU's tensors are filled with its index so we can easily verify which data moved where."
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification: Test your rotation function\n",
    "\n",
    "# Create identifiable K,V pairs (filled with GPU index)\n",
    "test_kv = [(torch.full((4, 4), float(i)),\n",
    "            torch.full((4, 4), float(i) + 0.5))\n",
    "           for i in range(4)]\n",
    "\n",
    "print(\"Before rotation:\")\n",
    "for i, (k, v) in enumerate(test_kv):\n",
    "    print(f\"  GPU {i}: K[0,0]={k[0,0].item():.1f}, \"\n",
    "          f\"V[0,0]={v[0,0].item():.1f}\")\n",
    "\n",
    "rotated = rotate_kv_ring(test_kv)\n",
    "\n",
    "print(\"\\nAfter one clockwise rotation:\")\n",
    "for i, (k, v) in enumerate(rotated):\n",
    "    print(f\"  GPU {i}: K[0,0]={k[0,0].item():.1f}, \"\n",
    "          f\"V[0,0]={v[0,0].item():.1f}\")"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we verify every GPU received data from its predecessor in the ring."
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify correctness\n",
    "expected_sources = [3, 0, 1, 2]\n",
    "all_correct = True\n",
    "for i, (k, v) in enumerate(rotated):\n",
    "    expected_k = float(expected_sources[i])\n",
    "    expected_v = float(expected_sources[i]) + 0.5\n",
    "    if k[0, 0].item() != expected_k or v[0, 0].item() != expected_v:\n",
    "        print(f\"  ‚ùå GPU {i}: expected K from GPU {expected_sources[i]}, \"\n",
    "              f\"got K[0,0]={k[0,0].item():.1f}\")\n",
    "        all_correct = False\n",
    "\n",
    "if all_correct:\n",
    "    print(\"\\n‚úÖ Rotation is correct! Each GPU received K,V from its predecessor.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Check your rotation logic.\")"
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_14_communication_cost",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Communication Cost\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/14_communication_cost.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Insight ‚Äî Communication Cost of Ring Attention**\n",
    "\n",
    "Let us think about the communication cost. In each round, every GPU sends one K chunk and one V chunk to its neighbor. With $P$ GPUs, chunk size $c = n/P$, and dimension $d_k$:\n",
    "\n",
    "- **Per round:** each GPU sends $2 \\times c \\times d_k$ elements\n",
    "- **Total rounds:** $P - 1$ (the first round uses local K, V so no communication is needed)\n",
    "- **Total per GPU:** $2(P-1) \\times c \\times d_k = 2(P-1) \\times \\frac{n}{P} \\times d_k$\n",
    "\n",
    "As $P$ grows, this approaches $2n \\cdot d_k$ ‚Äî the same as an AllGather of all keys and values. But the ring pattern means we only ever need to hold **two chunks** in memory at once (current and incoming), rather than all $P$ chunks."
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us compute these numbers for a realistic scenario\n",
    "n_tokens = 131072  # 128K context\n",
    "d_model = 128      # head dimension\n",
    "num_heads = 32     # number of attention heads\n",
    "bytes_per_param = 2  # fp16\n",
    "\n",
    "for P in [2, 4, 8, 16]:\n",
    "    chunk = n_tokens // P\n",
    "    # Memory for attention scores: chunk x chunk per head\n",
    "    attn_memory_gb = num_heads * chunk * chunk * bytes_per_param / 1e9\n",
    "    # Memory for K,V per GPU: 2 chunks\n",
    "    kv_memory_gb = 2 * num_heads * chunk * d_model * bytes_per_param / 1e9\n",
    "    # Full attention matrix if no SP\n",
    "    full_attn_gb = num_heads * n_tokens * n_tokens * bytes_per_param / 1e9\n",
    "\n",
    "    print(f\"P={P:2d} GPUs | chunk={chunk:6d} tokens | \"\n",
    "          f\"attn block={attn_memory_gb:6.2f} GB | \"\n",
    "          f\"K,V held={kv_memory_gb:.2f} GB | \"\n",
    "          f\"(vs full attn: {full_attn_gb:.1f} GB)\")"
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_15_transition_to_moe",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Transition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/15_transition_to_moe.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_16_moe_intuition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Moe Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/16_moe_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part B: Expert Parallelism\n",
    "\n",
    "## 6. Building Intuition ‚Äî Expert Parallelism\n",
    "\n",
    "Imagine a high-end restaurant with eight specialist chefs: one for appetizers, one for sushi, one for curries, one for pasta, one for steaks, one for desserts, one for soups, and one for salads.\n",
    "\n",
    "When an order comes in, a **host** (the router) looks at it and decides which chefs should handle it. A ramen order goes to the soup chef (70% contribution) and the appetizer chef for the side dish (30% contribution). A steak dinner might go to the steak chef (80%) and the salad chef (20%). The host does not send every order to every chef ‚Äî that would be chaotic and wasteful.\n",
    "\n",
    "This is exactly how a **Mixture of Experts (MoE)** works:\n",
    "- **The chefs** are expert sub-networks (typically MLPs)\n",
    "- **The host** is a learned router/gating network\n",
    "- **The orders** are tokens flowing through the Transformer\n",
    "- **The key insight**: only a few experts (top-K) are activated per token\n",
    "\n",
    "This means a model can have an enormous total parameter count while using only a fraction for any given input. DeepSeek-V3 has 256 experts with 671B total parameters, but only the top-8 experts are active per token ‚Äî just 37B active parameters. You get the capacity of a 671B model at the inference cost of a 37B model.\n",
    "\n",
    "### Where Does Parallelism Come In?\n",
    "\n",
    "With 256 experts, you cannot fit them all on one GPU. **Expert Parallelism** places different experts on different GPUs. The communication pattern is **All-to-All**: tokens must be dispatched from whatever GPU they are on to the GPU that holds their assigned expert, and then collected back."
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_17_moe_think",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Moe Think\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/17_moe_think.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§î Think About This\n",
    "\n",
    "Before we dive into the math:\n",
    "- If the router learns to send 90% of tokens to Expert 0 and 10% spread across the rest, what happens? (Hint: think about GPU utilization)\n",
    "- Why use top-K (e.g., K=2) instead of sending each token to just one expert?\n",
    "- If we have 8 experts on 8 GPUs, and batch size is 32 tokens, what is the maximum number of tokens any one GPU might have to process?\n",
    "\n",
    "*These are real challenges that MoE systems must solve. We will address all of them.*"
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick status check\n",
    "print(\"Section checkpoint ‚úì\")"
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_18_moe_math",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Moe Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/18_moe_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. The Mathematics ‚Äî Mixture of Experts\n",
    "\n",
    "### The Router / Gating Network\n",
    "\n",
    "Given a token representation $x \\in \\mathbb{R}^{d}$, the router computes gate weights over $N$ experts:\n",
    "\n",
    "$$g(x) = \\text{Softmax}(W_g \\cdot x + b_g)$$\n",
    "\n",
    "where $W_g \\in \\mathbb{R}^{N \\times d}$ is the gating weight matrix. This gives us a probability distribution over experts: $g(x) \\in \\mathbb{R}^N$ with $\\sum_i g_i(x) = 1$.\n",
    "\n",
    "**Computationally:** We multiply the token's hidden state by a small linear layer and apply softmax. The output tells us how relevant each expert is for this particular token.\n",
    "\n",
    "### Top-K Selection\n",
    "\n",
    "We select only the $K$ experts with the highest gate values and renormalize:\n",
    "\n",
    "$$\\text{TopK}_i(x) = \\begin{cases} \\frac{g_i(x)}{\\sum_{j \\in \\text{top-K}} g_j(x)} & \\text{if } i \\in \\text{top-K indices} \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "**Let us plug in some simple numbers.** Suppose we have $N=8$ experts with $K=2$ (top-2 routing). For a given token, the router outputs:\n",
    "\n",
    "$$g(x) = [0.00, 0.05, 0.60, 0.00, 0.00, 0.35, 0.00, 0.00]$$\n",
    "\n",
    "The top-2 experts are Expert 2 ($g_2 = 0.60$) and Expert 5 ($g_5 = 0.35$). After renormalization:\n",
    "\n",
    "$$\\hat{g}_2 = \\frac{0.60}{0.60 + 0.35} = 0.632, \\quad \\hat{g}_5 = \\frac{0.35}{0.60 + 0.35} = 0.368$$\n",
    "\n",
    "### MoE Output\n",
    "\n",
    "The final output is the weighted sum of the selected experts' outputs:\n",
    "\n",
    "$$y = \\sum_{i \\in \\text{top-K}} \\hat{g}_i(x) \\cdot E_i(x)$$\n",
    "\n",
    "where $E_i(x)$ is the output of expert $i$. For our example:\n",
    "\n",
    "$$y = 0.632 \\cdot E_2(x) + 0.368 \\cdot E_5(x)$$\n",
    "\n",
    "**Computationally:** Only 2 of the 8 expert MLPs actually run a forward pass. The other 6 are skipped entirely. This is where the computational savings come from.\n",
    "\n",
    "### Load Balancing Loss\n",
    "\n",
    "Without encouragement, the router will often collapse ‚Äî sending most tokens to just one or two experts while the rest sit idle. This is disastrous for Expert Parallelism because it means some GPUs are overloaded while others are idle.\n",
    "\n",
    "The auxiliary load balancing loss encourages even distribution:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{aux}} = \\alpha \\cdot N \\cdot \\sum_{i=1}^{N} f_i \\cdot P_i$$\n",
    "\n",
    "where:\n",
    "- $f_i = \\frac{1}{T} \\sum_{t=1}^{T} \\mathbb{1}[\\text{token } t \\text{ is routed to expert } i]$ ‚Äî the fraction of tokens assigned to expert $i$\n",
    "- $P_i = \\frac{1}{T} \\sum_{t=1}^{T} g_i(x_t)$ ‚Äî the average gate probability for expert $i$\n",
    "- $\\alpha$ is a small coefficient (typically 0.01)\n",
    "- $N$ is the number of experts (scaling factor)\n",
    "\n",
    "**What this says computationally:** If expert $i$ gets a disproportionate fraction of tokens ($f_i$ is high) AND the router gives it high probability ($P_i$ is high), the loss increases. The $N$ multiplier ensures the loss scale is independent of the number of experts.\n",
    "\n",
    "**Numerical example:** With $N=4$ experts, $T=8$ tokens, $\\alpha = 0.01$:\n",
    "\n",
    "If tokens are perfectly balanced: $f_i = 0.25$ for all $i$, and $P_i \\approx 0.25$:\n",
    "$$\\mathcal{L}_{\\text{aux}} = 0.01 \\times 4 \\times 4 \\times (0.25 \\times 0.25) = 0.01$$\n",
    "\n",
    "If tokens all go to Expert 0: $f_0 = 1.0, P_0 \\approx 1.0$, others zero:\n",
    "$$\\mathcal{L}_{\\text{aux}} = 0.01 \\times 4 \\times (1.0 \\times 1.0) = 0.04$$\n",
    "\n",
    "The collapsed case has 4x higher loss, pushing the router toward balance."
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_19_expert_mlp",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/19_expert_mlp.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Let Us Build It ‚Äî MoE from Scratch\n",
    "\n",
    "### 8.1 Expert MLP\n",
    "\n",
    "Each expert is simply a small two-layer MLP with ReLU activation. In a real model like DeepSeek-V3, each expert has billions of parameters. We will use small dimensions to keep everything fast."
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpertMLP(nn.Module):\n",
    "    \"\"\"A single expert: 2-layer MLP with ReLU.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff)\n",
    "        self.w2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"x: (..., d_model) -> (..., d_model)\"\"\"\n",
    "        return self.w2(self.relu(self.w1(x)))\n",
    "\n",
    "# Test a single expert\n",
    "d_model, d_ff = 64, 128\n",
    "expert = ExpertMLP(d_model, d_ff)\n",
    "test_token = torch.randn(1, d_model)\n",
    "output = expert(test_token)\n",
    "print(f\"Expert input:  {test_token.shape}\")\n",
    "print(f\"Expert output: {output.shape}\")\n",
    "print(f\"Expert parameters: {sum(p.numel() for p in expert.parameters()):,}\")"
   ],
   "id": "cell_51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_20_router",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/20_router.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Router / Gating Network\n",
    "\n",
    "The router is a simple linear layer followed by softmax. It maps each token's hidden state to a probability distribution over experts."
   ],
   "id": "cell_52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Router(nn.Module):\n",
    "    \"\"\"Gating network that routes tokens to experts.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, num_experts: int):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(d_model, num_experts, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (num_tokens, d_model)\n",
    "        Returns:\n",
    "            gate_probs: (num_tokens, num_experts) ‚Äî softmax probs\n",
    "        \"\"\"\n",
    "        logits = self.gate(x)\n",
    "        return F.softmax(logits, dim=-1)"
   ],
   "id": "cell_53"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the router on some random tokens and visualize how it distributes gate weights across the 8 experts."
   ],
   "id": "cell_54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the router\n",
    "num_experts = 8\n",
    "router = Router(d_model, num_experts)\n",
    "\n",
    "# Generate some test tokens\n",
    "num_tokens = 16\n",
    "tokens = torch.randn(num_tokens, d_model)\n",
    "gate_probs = router(tokens)\n",
    "\n",
    "print(f\"Input:  {tokens.shape}\")\n",
    "print(f\"Gates:  {gate_probs.shape}\")\n",
    "print(f\"Sum of gates per token (should be 1.0): \"\n",
    "      f\"{gate_probs.sum(dim=-1)[:4].tolist()}\")\n",
    "print(f\"\\nGate distribution for token 0:\")\n",
    "for i in range(num_experts):\n",
    "    bar = '‚ñà' * int(gate_probs[0, i].item() * 40)\n",
    "    print(f\"  Expert {i}: {gate_probs[0, i].item():.3f} {bar}\")"
   ],
   "id": "cell_55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_21_viz_gate_distribution",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/21_viz_gate_distribution.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Visualization 3: Gate Distribution for Sample Tokens\n",
    "\n",
    "Let us see how the router distributes its attention across experts. The top-2 experts for each token are highlighted with red outlines."
   ],
   "id": "cell_56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle(\"Router Gate Weights for 4 Sample Tokens\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, num_experts))\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    probs = gate_probs[idx].detach().numpy()\n",
    "    bars = ax.bar(range(num_experts), probs, color=colors,\n",
    "                  edgecolor='black', linewidth=0.5)\n",
    "    ax.set_title(f'Token {idx}', fontsize=11)\n",
    "    ax.set_xlabel('Expert index')\n",
    "    ax.set_ylabel('Gate weight')\n",
    "    ax.set_ylim(0, max(probs) * 1.3)\n",
    "    ax.set_xticks(range(num_experts))\n",
    "\n",
    "    # Highlight top-2\n",
    "    top2_indices = np.argsort(probs)[-2:]\n",
    "    for i in top2_indices:\n",
    "        bars[i].set_edgecolor('red')\n",
    "        bars[i].set_linewidth(2.5)\n",
    "        ax.text(i, probs[i] + 0.01, f'{probs[i]:.2f}',\n",
    "                ha='center', fontsize=9, fontweight='bold', color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Red-outlined bars = top-2 selected experts per token.\")"
   ],
   "id": "cell_57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_22_topk_gating",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/22_topk_gating.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Top-K Selection and Routing\n",
    "\n",
    "Now we implement the core routing logic: select the top-K experts for each token and renormalize the gate weights."
   ],
   "id": "cell_58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_gating(gate_probs: torch.Tensor, k: int\n",
    "                 ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Select top-K experts per token and renormalize gate weights.\n",
    "\n",
    "    Args:\n",
    "        gate_probs: (num_tokens, num_experts) ‚Äî softmax probs\n",
    "        k: number of experts to select per token\n",
    "\n",
    "    Returns:\n",
    "        topk_weights: (num_tokens, k) ‚Äî renormalized weights\n",
    "        topk_indices: (num_tokens, k) ‚Äî selected expert indices\n",
    "    \"\"\"\n",
    "    # Get top-K gate values and their indices\n",
    "    topk_weights, topk_indices = torch.topk(gate_probs, k, dim=-1)\n",
    "\n",
    "    # Renormalize so weights sum to 1\n",
    "    topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    return topk_weights, topk_indices"
   ],
   "id": "cell_59"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the top-K gating and inspect which experts get selected for the first few tokens."
   ],
   "id": "cell_60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test top-K gating\n",
    "K = 2\n",
    "weights, indices = top_k_gating(gate_probs, K)\n",
    "\n",
    "print(f\"Top-{K} gating results:\")\n",
    "print(f\"  Weights shape: {weights.shape}\")\n",
    "print(f\"  Indices shape: {indices.shape}\")\n",
    "print(f\"\\nFirst 4 tokens:\")\n",
    "for t in range(4):\n",
    "    experts = indices[t].tolist()\n",
    "    ws = weights[t].tolist()\n",
    "    print(f\"  Token {t}: Expert {experts[0]} (w={ws[0]:.3f}) + \"\n",
    "          f\"Expert {experts[1]} (w={ws[1]:.3f})  [sum={sum(ws):.3f}]\")"
   ],
   "id": "cell_61"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_23_viz_routing_matrix",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/23_viz_routing_matrix.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Visualization 4: Token Routing Matrix\n",
    "\n",
    "Let us visualize which tokens go to which experts ‚Äî a routing matrix that shows the MoE dispatch pattern. Each cell shows the gate weight assigned to that token-expert pair."
   ],
   "id": "cell_62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a routing matrix: (num_tokens, num_experts)\n",
    "routing_matrix = torch.zeros(num_tokens, num_experts)\n",
    "for t in range(num_tokens):\n",
    "    for j in range(K):\n",
    "        expert_idx = indices[t, j].item()\n",
    "        routing_matrix[t, expert_idx] = weights[t, j].item()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "im = ax.imshow(routing_matrix.detach().numpy(), cmap='YlOrRd',\n",
    "               aspect='auto')\n",
    "\n",
    "ax.set_xlabel('Expert Index', fontsize=12)\n",
    "ax.set_ylabel('Token Index', fontsize=12)\n",
    "ax.set_title(f'Token Routing Matrix (Top-{K} Routing)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(range(num_experts))\n",
    "ax.set_xticklabels([f'E{i}' for i in range(num_experts)])\n",
    "ax.set_yticks(range(num_tokens))\n",
    "ax.set_yticklabels([f'T{i}' for i in range(num_tokens)])\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='Gate weight')"
   ],
   "id": "cell_63"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We annotate the cells with their weight values and print the expert load statistics."
   ],
   "id": "cell_64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotate cells with weight values where non-zero\n",
    "for t in range(num_tokens):\n",
    "    for e in range(num_experts):\n",
    "        val = routing_matrix[t, e].item()\n",
    "        if val > 0.01:\n",
    "            ax.text(e, t, f'{val:.2f}', ha='center', va='center',\n",
    "                    fontsize=7,\n",
    "                    color='white' if val > 0.4 else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Expert load statistics\n",
    "tokens_per_expert = (routing_matrix > 0).sum(dim=0).tolist()\n",
    "print(f\"\\nTokens routed to each expert: {tokens_per_expert}\")\n",
    "print(f\"Ideal (perfectly balanced): \"\n",
    "      f\"{num_tokens * K / num_experts:.1f} tokens each\")"
   ],
   "id": "cell_65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_24_full_moe_layer",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/24_full_moe_layer.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Full MoE Layer\n",
    "\n",
    "Now let us assemble all the components into a complete Mixture of Experts layer. The MoE layer contains N expert MLPs and a router. For each input token, the router selects the top-K experts, dispatches the token through those experts, and combines their outputs with the renormalized gate weights."
   ],
   "id": "cell_66"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoELayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture of Experts layer.\n",
    "\n",
    "    Contains N expert MLPs and a router. Each token is dispatched\n",
    "    to the top-K experts, and the output is the weighted sum.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int,\n",
    "                 num_experts: int, top_k: int):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.router = Router(d_model, num_experts)\n",
    "        self.experts = nn.ModuleList([\n",
    "            ExpertMLP(d_model, d_ff) for _ in range(num_experts)\n",
    "        ])"
   ],
   "id": "cell_67"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward pass: route tokens to experts, compute expert outputs, and combine with gate weights."
   ],
   "id": "cell_68"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, x: torch.Tensor\n",
    "                ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (num_tokens, d_model)\n",
    "        Returns:\n",
    "            output: (num_tokens, d_model)\n",
    "            gate_probs: for load balancing loss\n",
    "            topk_indices: for analysis\n",
    "        \"\"\"\n",
    "        num_tokens, d_model = x.shape\n",
    "\n",
    "        # Step 1: Route ‚Äî get gate probabilities\n",
    "        gate_probs = self.router(x)\n",
    "\n",
    "        # Step 2: Select top-K experts\n",
    "        topk_weights, topk_indices = top_k_gating(gate_probs, self.top_k)\n",
    "\n",
    "        # Step 3: Dispatch and compute\n",
    "        output = torch.zeros_like(x)\n",
    "        for t in range(num_tokens):\n",
    "            for j in range(self.top_k):\n",
    "                expert_idx = topk_indices[t, j].item()\n",
    "                weight = topk_weights[t, j]\n",
    "                expert_output = self.experts[expert_idx](x[t:t+1])\n",
    "                output[t] += weight * expert_output.squeeze(0)\n",
    "\n",
    "        return output, gate_probs, topk_indices"
   ],
   "id": "cell_69"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with ring attention, we need the class defined as a single unit for Python. Here is the complete `MoELayer`:"
   ],
   "id": "cell_70"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoELayer(nn.Module):\n",
    "    \"\"\"Mixture of Experts layer with top-K routing.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, num_experts, top_k):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.router = Router(d_model, num_experts)\n",
    "        self.experts = nn.ModuleList([\n",
    "            ExpertMLP(d_model, d_ff) for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        num_tokens, d_model = x.shape\n",
    "        gate_probs = self.router(x)\n",
    "        topk_weights, topk_indices = top_k_gating(\n",
    "            gate_probs, self.top_k)\n",
    "        output = torch.zeros_like(x)\n",
    "        for t in range(num_tokens):\n",
    "            for j in range(self.top_k):\n",
    "                expert_idx = topk_indices[t, j].item()\n",
    "                weight = topk_weights[t, j]\n",
    "                expert_out = self.experts[expert_idx](x[t:t+1])\n",
    "                output[t] += weight * expert_out.squeeze(0)\n",
    "        return output, gate_probs, topk_indices"
   ],
   "id": "cell_71"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the full MoE layer and examine its parameter breakdown ‚Äî total vs active parameters per token."
   ],
   "id": "cell_72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the full MoE layer\n",
    "torch.manual_seed(42)\n",
    "moe = MoELayer(d_model=64, d_ff=128, num_experts=8, top_k=2)\n",
    "\n",
    "test_tokens = torch.randn(16, 64)\n",
    "output, gate_probs, topk_indices = moe(test_tokens)\n",
    "\n",
    "total_params = sum(p.numel() for p in moe.parameters())\n",
    "expert_params = sum(p.numel() for p in moe.experts.parameters())\n",
    "router_params = sum(p.numel() for p in moe.router.parameters())\n",
    "\n",
    "print(f\"MoE Layer Summary:\")\n",
    "print(f\"  Input:           {test_tokens.shape}\")\n",
    "print(f\"  Output:          {output.shape}\")\n",
    "print(f\"  Total params:    {total_params:,}\")\n",
    "print(f\"  Expert params:   {expert_params:,} \"\n",
    "      f\"({expert_params/total_params*100:.1f}%)\")\n",
    "print(f\"  Router params:   {router_params:,} \"\n",
    "      f\"({router_params/total_params*100:.1f}%)\")\n",
    "print(f\"  Params per expert: {expert_params // 8:,}\")\n",
    "print(f\"  Active params per token: \"\n",
    "      f\"{expert_params // 8 * 2 + router_params:,} \"\n",
    "      f\"(2 experts + router)\")\n",
    "print(f\"\\n  Sparsity: {(1 - 2/8)*100:.0f}% of expert params \"\n",
    "      f\"are unused per token\")"
   ],
   "id": "cell_73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_25_load_balancing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/25_load_balancing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Load Balancing Loss\n",
    "\n",
    "Without the auxiliary loss, the router tends to collapse ‚Äî sending most tokens to just one or two experts. Let us implement the loss and compare balanced vs collapsed routing."
   ],
   "id": "cell_74"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_balancing_loss(gate_probs: torch.Tensor,\n",
    "                        topk_indices: torch.Tensor,\n",
    "                        num_experts: int,\n",
    "                        alpha: float = 0.01) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the auxiliary load balancing loss.\n",
    "    L_aux = alpha * N * sum_i(f_i * P_i)\n",
    "\n",
    "    Args:\n",
    "        gate_probs: (num_tokens, num_experts)\n",
    "        topk_indices: (num_tokens, top_k)\n",
    "        num_experts: N\n",
    "        alpha: loss coefficient\n",
    "    Returns:\n",
    "        loss: scalar tensor\n",
    "    \"\"\"\n",
    "    num_tokens = gate_probs.shape[0]\n",
    "\n",
    "    # f_i: fraction of tokens routed to each expert\n",
    "    expert_counts = torch.zeros(num_experts, device=gate_probs.device)\n",
    "    for i in range(num_experts):\n",
    "        expert_counts[i] = (topk_indices == i).any(dim=-1).float().sum()\n",
    "    f = expert_counts / num_tokens\n",
    "\n",
    "    # P_i: average gate probability for each expert\n",
    "    P = gate_probs.mean(dim=0)\n",
    "\n",
    "    # L_aux = alpha * N * sum(f_i * P_i)\n",
    "    loss = alpha * num_experts * (f * P).sum()\n",
    "    return loss"
   ],
   "id": "cell_75"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us compute the loss for our current routing and compare it to the theoretical best (perfectly balanced) and worst (completely collapsed) cases."
   ],
   "id": "cell_76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss for our current routing\n",
    "loss = load_balancing_loss(gate_probs, topk_indices, num_experts=8)\n",
    "print(f\"Load balancing loss: {loss.item():.6f}\")\n",
    "\n",
    "# What would it be if perfectly balanced?\n",
    "perfect_f = torch.full((8,), 1/8)\n",
    "perfect_P = torch.full((8,), 1/8)\n",
    "perfect_loss = 0.01 * 8 * (perfect_f * perfect_P).sum().item()\n",
    "print(f\"Perfect balance loss: {perfect_loss:.6f}\")\n",
    "\n",
    "# What would it be if completely collapsed?\n",
    "collapsed_f = torch.zeros(8)\n",
    "collapsed_f[0] = 1.0\n",
    "collapsed_P = torch.zeros(8)\n",
    "collapsed_P[0] = 1.0\n",
    "collapsed_loss = 0.01 * 8 * (collapsed_f * collapsed_P).sum().item()\n",
    "print(f\"Collapsed routing loss: {collapsed_loss:.6f}\")\n",
    "print(f\"\\nCollapsed is {collapsed_loss/perfect_loss:.0f}x worse than \"\n",
    "      f\"balanced ‚Äî the loss pushes toward balance.\")"
   ],
   "id": "cell_77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_26_viz_training_balancing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/26_viz_training_balancing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Visualization 5: Training the Router with Load Balancing\n",
    "\n",
    "Let us actually train a small MoE and watch the load balancing loss reshape the expert utilization over time. First, we define the training function that tracks expert usage at each step."
   ],
   "id": "cell_78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_moe_with_balancing(num_steps: int = 300,\n",
    "                             alpha: float = 0.01,\n",
    "                             use_aux_loss: bool = True) -> dict:\n",
    "    \"\"\"Train a small MoE on random data and track expert usage.\"\"\"\n",
    "    torch.manual_seed(123)\n",
    "    moe_train = MoELayer(d_model=32, d_ff=64,\n",
    "                         num_experts=8, top_k=2)\n",
    "\n",
    "    optimizer = torch.optim.Adam(moe_train.parameters(), lr=1e-3)\n",
    "    target_fn = nn.Linear(32, 32, bias=False)\n",
    "    target_fn.requires_grad_(False)\n",
    "\n",
    "    history = {'step': [], 'task_loss': [], 'aux_loss': [],\n",
    "               'expert_usage': [], 'max_load': [], 'min_load': []}\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        x = torch.randn(64, 32)\n",
    "        target = target_fn(x).detach()\n",
    "\n",
    "        output, gate_probs, topk_idx = moe_train(x)\n",
    "\n",
    "        task_loss = F.mse_loss(output, target)\n",
    "        aux_loss = load_balancing_loss(\n",
    "            gate_probs, topk_idx, 8, alpha=alpha)\n",
    "\n",
    "        total_loss = task_loss + (aux_loss if use_aux_loss else 0.0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()"
   ],
   "id": "cell_79"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tracking portion: we record expert usage statistics every 10 steps to visualize the training dynamics."
   ],
   "id": "cell_80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Track expert usage\n",
    "        usage = torch.zeros(8)\n",
    "        for i in range(8):\n",
    "            usage[i] = (topk_idx == i).any(dim=-1).float().mean().item()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            history['step'].append(step)\n",
    "            history['task_loss'].append(task_loss.item())\n",
    "            history['aux_loss'].append(aux_loss.item())\n",
    "            history['expert_usage'].append(usage.numpy().copy())\n",
    "            history['max_load'].append(usage.max().item())\n",
    "            history['min_load'].append(usage.min().item())\n",
    "\n",
    "    return history"
   ],
   "id": "cell_81"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us train the MoE both with and without the auxiliary load balancing loss, so we can see the difference."
   ],
   "id": "cell_82"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with and without load balancing\n",
    "print(\"Training WITH load balancing loss...\")\n",
    "hist_balanced = train_moe_with_balancing(\n",
    "    num_steps=300, alpha=0.01, use_aux_loss=True)\n",
    "print(\"Training WITHOUT load balancing loss...\")\n",
    "hist_unbalanced = train_moe_with_balancing(\n",
    "    num_steps=300, alpha=0.01, use_aux_loss=False)\n",
    "print(\"Done!\")"
   ],
   "id": "cell_83"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the expert usage curves over training. The top row shows per-expert utilization; the bottom row shows the gap between the most-used and least-used expert (load imbalance)."
   ],
   "id": "cell_84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle(\"Effect of Load Balancing Loss on Expert Utilization\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Top left: Expert usage over time (with balancing)\n",
    "ax = axes[0, 0]\n",
    "usage_data = np.array(hist_balanced['expert_usage'])\n",
    "for i in range(8):\n",
    "    ax.plot(hist_balanced['step'], usage_data[:, i],\n",
    "            label=f'E{i}', alpha=0.7)\n",
    "ax.set_title('Expert Usage ‚Äî WITH Aux Loss', fontweight='bold')\n",
    "ax.set_xlabel('Training step')\n",
    "ax.set_ylabel('Fraction of tokens')\n",
    "ax.axhline(y=2/8, color='black', linestyle='--', alpha=0.5,\n",
    "           label='Ideal (0.25)')\n",
    "ax.legend(fontsize=7, ncol=3)\n",
    "ax.set_ylim(0, 0.7)"
   ],
   "id": "cell_85"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top right panel: same plot but for training WITHOUT the auxiliary loss, to contrast the behavior."
   ],
   "id": "cell_86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top right: Expert usage over time (without balancing)\n",
    "ax = axes[0, 1]\n",
    "usage_data_ub = np.array(hist_unbalanced['expert_usage'])\n",
    "for i in range(8):\n",
    "    ax.plot(hist_unbalanced['step'], usage_data_ub[:, i],\n",
    "            label=f'E{i}', alpha=0.7)\n",
    "ax.set_title('Expert Usage ‚Äî WITHOUT Aux Loss', fontweight='bold')\n",
    "ax.set_xlabel('Training step')\n",
    "ax.set_ylabel('Fraction of tokens')\n",
    "ax.axhline(y=2/8, color='black', linestyle='--', alpha=0.5,\n",
    "           label='Ideal (0.25)')\n",
    "ax.legend(fontsize=7, ncol=3)\n",
    "ax.set_ylim(0, 0.7)"
   ],
   "id": "cell_87"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottom left panel: the gap between max and min expert load over training (WITH aux loss)."
   ],
   "id": "cell_88"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottom left: Max vs Min load (with balancing)\n",
    "ax = axes[1, 0]\n",
    "ax.fill_between(hist_balanced['step'], hist_balanced['min_load'],\n",
    "                hist_balanced['max_load'], alpha=0.3, color='green',\n",
    "                label='Load range')\n",
    "ax.plot(hist_balanced['step'], hist_balanced['max_load'],\n",
    "        color='green', label='Max load')\n",
    "ax.plot(hist_balanced['step'], hist_balanced['min_load'],\n",
    "        color='green', linestyle='--', label='Min load')\n",
    "ax.set_title('Load Imbalance ‚Äî WITH Aux Loss', fontweight='bold')\n",
    "ax.set_xlabel('Training step')\n",
    "ax.set_ylabel('Fraction of tokens')\n",
    "ax.legend()"
   ],
   "id": "cell_89"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottom right panel: load imbalance WITHOUT the auxiliary loss, then finalize the figure."
   ],
   "id": "cell_90"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottom right: Max vs Min load (without balancing)\n",
    "ax = axes[1, 1]\n",
    "ax.fill_between(hist_unbalanced['step'],\n",
    "                hist_unbalanced['min_load'],\n",
    "                hist_unbalanced['max_load'], alpha=0.3, color='red',\n",
    "                label='Load range')\n",
    "ax.plot(hist_unbalanced['step'], hist_unbalanced['max_load'],\n",
    "        color='red', label='Max load')\n",
    "ax.plot(hist_unbalanced['step'], hist_unbalanced['min_load'],\n",
    "        color='red', linestyle='--', label='Min load')\n",
    "ax.set_title('Load Imbalance ‚Äî WITHOUT Aux Loss', fontweight='bold')\n",
    "ax.set_xlabel('Training step')\n",
    "ax.set_ylabel('Fraction of tokens')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"With load balancing: experts converge toward equal utilization.\")\n",
    "print(\"Without it: some experts dominate while others become \"\n",
    "      \"'dead' experts.\")"
   ],
   "id": "cell_91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_27_todo2_topk_manual",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/27_todo2_topk_manual.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üîß Your Turn ‚Äî Expert Parallelism\n",
    "\n",
    "### TODO 2: Implement Top-K Gating from Scratch\n",
    "\n",
    "Implement the `top_k_gating_manual` function without using `torch.topk`. This deepens your understanding of the selection and renormalization steps."
   ],
   "id": "cell_92"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_gating_manual(gate_logits: torch.Tensor, k: int\n",
    "                        ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Select top-K experts and renormalize weights.\n",
    "    Do NOT use torch.topk ‚Äî implement selection manually.\n",
    "    Args:\n",
    "        gate_logits: (num_tokens, num_experts) ‚Äî raw logits\n",
    "        k: number of experts to select per token\n",
    "    Returns:\n",
    "        topk_weights: (num_tokens, k) ‚Äî renormalized weights\n",
    "        topk_indices: (num_tokens, k) ‚Äî expert indices\n",
    "    \"\"\"\n",
    "    num_tokens, num_experts = gate_logits.shape\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Softmax over experts\n",
    "    gate_probs = ???  # YOUR CODE HERE\n",
    "\n",
    "    # Step 2: Find top-k indices for each token\n",
    "    # Hint: torch.argsort(tensor, dim=-1, descending=True)\n",
    "    topk_indices = ???  # YOUR CODE HERE ‚Äî (num_tokens, k)\n",
    "\n",
    "    # Step 3: Gather the probabilities at those indices\n",
    "    topk_weights = ???  # YOUR CODE HERE ‚Äî (num_tokens, k)\n",
    "\n",
    "    # Step 4: Renormalize\n",
    "    topk_weights = ???  # YOUR CODE HERE\n",
    "    # ==============================\n",
    "\n",
    "    return topk_weights, topk_indices"
   ],
   "id": "cell_93"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_28_todo2_verify",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/28_todo2_verify.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify your manual implementation against the reference `torch.topk` version."
   ],
   "id": "cell_94"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification: Test your manual top-K gating\n",
    "\n",
    "torch.manual_seed(42)\n",
    "test_logits = torch.randn(8, 8)  # 8 tokens, 8 experts\n",
    "k_test = 2\n",
    "\n",
    "# Your implementation\n",
    "your_weights, your_indices = top_k_gating_manual(test_logits, k_test)\n",
    "\n",
    "# Reference implementation using torch.topk\n",
    "ref_probs = F.softmax(test_logits, dim=-1)\n",
    "ref_weights, ref_indices = torch.topk(ref_probs, k_test, dim=-1)\n",
    "ref_weights = ref_weights / ref_weights.sum(dim=-1, keepdim=True)\n",
    "\n",
    "print(\"Your results vs Reference:\")\n",
    "print(f\"  Indices match: {torch.equal(your_indices, ref_indices)}\")\n",
    "print(f\"  Weights close: \"\n",
    "      f\"{torch.allclose(your_weights, ref_weights, atol=1e-5)}\")\n",
    "print(f\"  Weights sum to 1: \"\n",
    "      f\"{torch.allclose(your_weights.sum(dim=-1), torch.ones(8), atol=1e-5)}\")\n",
    "\n",
    "if (torch.equal(your_indices, ref_indices)\n",
    "        and torch.allclose(your_weights, ref_weights, atol=1e-5)):\n",
    "    print(\"\\n‚úÖ Your top-K gating implementation is correct!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Something is off. Check your steps.\")\n",
    "    print(f\"  Your indices[:3]:\\n  {your_indices[:3]}\")\n",
    "    print(f\"  Ref indices[:3]:\\n  {ref_indices[:3]}\")"
   ],
   "id": "cell_95"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_29_todo3_dispatch",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/29_todo3_dispatch.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 3: Implement All-to-All Token Dispatch\n",
    "\n",
    "In Expert Parallelism, each GPU holds one (or a few) experts. Tokens arrive at whichever GPU their data shard is on, but must be dispatched to the GPU that holds their assigned expert. This is the **All-to-All** communication pattern.\n",
    "\n",
    "Implement the dispatch function below: given tokens and their expert assignments, reorganize so that each \"GPU\" receives exactly the tokens assigned to its expert."
   ],
   "id": "cell_96"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_to_all_dispatch(tokens: torch.Tensor,\n",
    "                        expert_assignments: torch.Tensor,\n",
    "                        num_experts: int\n",
    "                        ) -> Tuple[List[torch.Tensor],\n",
    "                                   List[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Dispatch tokens to assigned experts (simulating All-to-All).\n",
    "    Args:\n",
    "        tokens: (num_tokens, d_model)\n",
    "        expert_assignments: (num_tokens,) ‚Äî expert index\n",
    "        num_experts: total number of experts\n",
    "    Returns:\n",
    "        dispatched_tokens: list[num_experts] of (n_i, d_model)\n",
    "        original_indices: list[num_experts] of token indices\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    dispatched_tokens = []\n",
    "    original_indices = []\n",
    "\n",
    "    for expert_id in range(num_experts):\n",
    "        # Find tokens assigned to this expert\n",
    "        mask = ???  # YOUR CODE HERE ‚Äî boolean mask\n",
    "\n",
    "        # Gather the tokens and their original indices\n",
    "        ???  # YOUR CODE HERE\n",
    "\n",
    "    # ==============================\n",
    "\n",
    "    return dispatched_tokens, original_indices"
   ],
   "id": "cell_97"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_30_todo3_verify",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/30_todo3_verify.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the dispatch with a known assignment pattern to verify each expert receives the correct tokens."
   ],
   "id": "cell_98"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification: Test your All-to-All dispatch\n",
    "torch.manual_seed(42)\n",
    "test_tokens = torch.randn(12, 8)  # 12 tokens, d_model=8\n",
    "test_assignments = torch.tensor(\n",
    "    [0, 2, 1, 0, 3, 2, 1, 0, 3, 1, 2, 3])\n",
    "\n",
    "dispatched, orig_idx = all_to_all_dispatch(\n",
    "    test_tokens, test_assignments, num_experts=4)\n",
    "\n",
    "print(\"Dispatch results:\")\n",
    "all_correct = True\n",
    "expected = {0: [0, 3, 7], 1: [2, 6, 9],\n",
    "            2: [1, 5, 10], 3: [4, 8, 11]}\n",
    "for i in range(4):\n",
    "    idx_list = orig_idx[i].tolist() if len(orig_idx[i]) > 0 else []\n",
    "    print(f\"  Expert/GPU {i}: {len(dispatched[i])} tokens, \"\n",
    "          f\"indices = {idx_list}\")\n",
    "    if idx_list != expected[i]:\n",
    "        all_correct = False\n",
    "    for j, orig in enumerate(idx_list):\n",
    "        if not torch.equal(dispatched[i][j], test_tokens[orig]):\n",
    "            all_correct = False"
   ],
   "id": "cell_99"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the final verification result for the dispatch function."
   ],
   "id": "cell_100"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_correct:\n",
    "    print(\"‚úÖ Dispatch is correct! Each expert received the \"\n",
    "          \"right tokens.\")\n",
    "    print(\"   In real EP, this would be an All-to-All collective \"\n",
    "          \"across GPUs.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Check your dispatch logic.\")"
   ],
   "id": "cell_101"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_31_alltoall_cost",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Alltoall Cost\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/31_alltoall_cost.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Insight ‚Äî All-to-All Communication Cost**\n",
    "\n",
    "The All-to-All operation is the most expensive part of Expert Parallelism. Let us understand its cost by computing the bytes transferred for a realistic configuration."
   ],
   "id": "cell_102"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communication analysis for All-to-All dispatch\n",
    "batch_tokens = 4096          # tokens per micro-batch\n",
    "d_model_real = 4096          # hidden dimension (e.g., LLaMA-7B)\n",
    "num_experts_real = 64        # number of experts\n",
    "top_k_real = 2               # top-K routing\n",
    "bytes_per_param = 2          # fp16\n",
    "\n",
    "tokens_dispatched = batch_tokens * top_k_real\n",
    "bytes_per_transfer = d_model_real * bytes_per_param\n",
    "total_comm_bytes = 2 * tokens_dispatched * bytes_per_transfer\n",
    "\n",
    "print(\"All-to-All Communication Cost:\")\n",
    "print(f\"  Batch tokens:    {batch_tokens:,}\")\n",
    "print(f\"  Top-K:           {top_k_real}\")\n",
    "print(f\"  Token transfers: {tokens_dispatched:,} (forward dispatch)\")\n",
    "print(f\"  Bytes per token: {bytes_per_transfer:,} \"\n",
    "      f\"({d_model_real} dims * {bytes_per_param} bytes)\")\n",
    "print(f\"  Total comm:      {total_comm_bytes/1e6:.1f} MB per fwd pass\")\n",
    "print(f\"  (x2 for forward+backward: \"\n",
    "      f\"{2*total_comm_bytes/1e6:.1f} MB per step)\")\n",
    "print(f\"\\nFor comparison: an AllReduce of a \"\n",
    "      f\"{d_model_real}x{d_model_real} matrix \"\n",
    "      f\"= {d_model_real**2 * bytes_per_param / 1e6:.1f} MB\")"
   ],
   "id": "cell_103"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_32_transition_final",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Transition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/32_transition_final.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. üéØ Final Output ‚Äî Putting It All Together\n",
    "\n",
    "### 10.1 Ring Attention: Full Simulation\n",
    "\n",
    "Let us run a complete Ring Attention simulation on a realistic sequence and verify correctness."
   ],
   "id": "cell_104"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_33_ring_full_sim",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/33_ring_full_sim.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ring Attention: full simulation with a longer sequence\n",
    "torch.manual_seed(42)\n",
    "SEQ_LEN = 2048\n",
    "D_K = 64\n",
    "N_GPUS = 4\n",
    "CHUNK = SEQ_LEN // N_GPUS\n",
    "\n",
    "Q_sim = torch.randn(SEQ_LEN, D_K)\n",
    "K_sim = torch.randn(SEQ_LEN, D_K)\n",
    "V_sim = torch.randn(SEQ_LEN, D_K)\n",
    "\n",
    "Q_ch, K_ch, V_ch = chunk_sequence(Q_sim, K_sim, V_sim, N_GPUS)\n",
    "\n",
    "# Run both methods\n",
    "output_std = standard_attention(Q_sim, K_sim, V_sim)\n",
    "output_ring = ring_attention(Q_ch, K_ch, V_ch)\n",
    "\n",
    "error = (output_ring - output_std).abs().max().item()\n",
    "print(f\"Ring Attention on {SEQ_LEN} tokens across {N_GPUS} GPUs\")\n",
    "print(f\"  Chunk size: {CHUNK} tokens per GPU\")\n",
    "print(f\"  Max error vs standard attention: {error:.2e}\")\n",
    "print(f\"  ‚úÖ Match!\" if error < 1e-4 else \"  ‚ö†Ô∏è Mismatch!\")"
   ],
   "id": "cell_105"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also analyze the memory savings Ring Attention provides at different sequence lengths and GPU counts."
   ],
   "id": "cell_106"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory savings analysis\n",
    "print(\"\\nüìä Memory Analysis: Ring Attention vs Standard\")\n",
    "print(\"=\" * 55)\n",
    "for n in [2048, 8192, 32768, 131072, 524288]:\n",
    "    full_mem = n * n * 2 / 1e9  # fp16 attention matrix\n",
    "    for p in [4, 8]:\n",
    "        c = n // p\n",
    "        ring_mem = c * c * 2 / 1e9  # only one block at a time\n",
    "        savings = (1 - ring_mem / full_mem) * 100\n",
    "        if n == 2048 or n == 131072:\n",
    "            print(f\"  n={n:>7,} | P={p} GPUs | \"\n",
    "                  f\"Full: {full_mem:>8.2f} GB | \"\n",
    "                  f\"Ring block: {ring_mem:>8.4f} GB | \"\n",
    "                  f\"Savings: {savings:.1f}%\")\n",
    "    if n == 2048 or n == 131072:\n",
    "        print()"
   ],
   "id": "cell_107"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_34_moe_final_demo",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/34_moe_final_demo.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 MoE: Full Working Layer with Routing Visualization\n",
    "\n",
    "Let us build a final MoE layer and examine its parameter efficiency."
   ],
   "id": "cell_108"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a realistic-ish MoE layer\n",
    "torch.manual_seed(42)\n",
    "D_MODEL_MOE = 128\n",
    "D_FF_MOE = 256\n",
    "N_EXPERTS = 8\n",
    "TOP_K_MOE = 2\n",
    "NUM_TOKENS_MOE = 64\n",
    "\n",
    "moe_final = MoELayer(D_MODEL_MOE, D_FF_MOE, N_EXPERTS, TOP_K_MOE)\n",
    "\n",
    "# Process a batch of tokens\n",
    "input_tokens = torch.randn(NUM_TOKENS_MOE, D_MODEL_MOE)\n",
    "output_tokens, final_gates, final_routing = moe_final(input_tokens)\n",
    "\n",
    "print(f\"MoE Layer ‚Äî Final Demonstration\")\n",
    "print(f\"  Experts:           {N_EXPERTS}\")\n",
    "print(f\"  Top-K:             {TOP_K_MOE}\")\n",
    "print(f\"  Input tokens:      {NUM_TOKENS_MOE}\")\n",
    "print(f\"  Input shape:       {input_tokens.shape}\")\n",
    "print(f\"  Output shape:      {output_tokens.shape}\")\n",
    "\n",
    "total_p = sum(p.numel() for p in moe_final.parameters())\n",
    "active_p = (sum(p.numel() for p in moe_final.experts[0].parameters())\n",
    "            * TOP_K_MOE\n",
    "            + sum(p.numel() for p in moe_final.router.parameters()))\n",
    "print(f\"  Total parameters:  {total_p:,}\")\n",
    "print(f\"  Active per token:  {active_p:,}\")\n",
    "print(f\"  Sparsity:          {(1 - active_p/total_p)*100:.1f}%\")"
   ],
   "id": "cell_109"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_35_viz_overview",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß What to Look For\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/35_viz_overview.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Visualization 6: Complete Overview\n",
    "\n",
    "Now let us create a 4-panel overview showing both Sequence and Expert Parallelism side by side. We start with the Ring Attention schematic and block computation schedule (top row)."
   ],
   "id": "cell_110"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 12))\n",
    "fig.suptitle(\"üéØ Sequence & Expert Parallelism ‚Äî Complete Overview\",\n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "# --- Panel 1: Ring Attention schematic (top left) ---\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "ring_colors = ['#2196F3', '#4CAF50', '#FF9800', '#E91E63']\n",
    "angles_final = [math.pi/2 - i * 2*math.pi/4 for i in range(4)]\n",
    "positions_final = [(0.9*math.cos(a), 0.9*math.sin(a))\n",
    "                   for a in angles_final]\n",
    "\n",
    "# Draw ring\n",
    "theta_ring = np.linspace(0, 2*np.pi, 100)\n",
    "ax1.plot(0.9*np.cos(theta_ring), 0.9*np.sin(theta_ring),\n",
    "         'gray', linewidth=1, alpha=0.3)\n",
    "\n",
    "for i in range(4):\n",
    "    x, y = positions_final[i]\n",
    "    circle_patch = plt.Circle((x, y), 0.25,\n",
    "                               color=ring_colors[i],\n",
    "                               alpha=0.85, zorder=5)\n",
    "    ax1.add_patch(circle_patch)\n",
    "    ax1.text(x, y + 0.05, f'GPU {i}', ha='center', va='center',\n",
    "             fontsize=9, fontweight='bold', color='white', zorder=6)\n",
    "    ax1.text(x, y - 0.08, f'Q[{i*CHUNK}:{(i+1)*CHUNK}]',\n",
    "             ha='center', va='center', fontsize=6,\n",
    "             color='white', zorder=6)"
   ],
   "id": "cell_111"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add curved arrows for K,V transfer direction and labels to Panel 1, then create Panel 2 showing the block computation schedule."
   ],
   "id": "cell_112"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw curved arrows for Panel 1\n",
    "for i in range(4):\n",
    "    arc = mpatches.FancyArrowPatch(\n",
    "        positions_final[i], positions_final[(i+1)%4],\n",
    "        connectionstyle=f\"arc3,rad=0.3\",\n",
    "        arrowstyle='->', color='gray', linewidth=1.5,\n",
    "        mutation_scale=15, zorder=3)\n",
    "    ax1.add_patch(arc)\n",
    "\n",
    "ax1.set_xlim(-1.5, 1.5)\n",
    "ax1.set_ylim(-1.5, 1.5)\n",
    "ax1.set_aspect('equal')\n",
    "ax1.set_title(f'Ring Attention ‚Äî {SEQ_LEN} tokens, {N_GPUS} GPUs',\n",
    "              fontweight='bold', fontsize=11)\n",
    "ax1.text(0, -1.35, 'K,V rotate clockwise each round',\n",
    "         ha='center', fontsize=9, style='italic', color='gray')\n",
    "ax1.axis('off')"
   ],
   "id": "cell_113"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Panel 2 (top right): the block computation schedule showing which round each Q-K block pair is computed in."
   ],
   "id": "cell_114"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Panel 2: Attention blocks (top right) ---\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "block_img = np.zeros((4, 4))\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        block_img[i, j] = (i - j) % 4\n",
    "\n",
    "im2 = ax2.imshow(block_img, cmap='Set2', aspect='equal')\n",
    "ax2.set_xticks(range(4))\n",
    "ax2.set_yticks(range(4))\n",
    "ax2.set_xticklabels([f'K[{i}]' for i in range(4)])\n",
    "ax2.set_yticklabels([f'Q[{i}]' for i in range(4)])\n",
    "ax2.set_xlabel('Key chunks')\n",
    "ax2.set_ylabel('Query chunks')\n",
    "ax2.set_title('Block Computation Schedule',\n",
    "              fontweight='bold', fontsize=11)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        round_num = int(block_img[i, j])\n",
    "        ax2.text(j, i, f'Round {round_num}', ha='center',\n",
    "                 va='center', fontsize=8, fontweight='bold')\n",
    "cbar2 = plt.colorbar(im2, ax=ax2, ticks=[0, 1, 2, 3])\n",
    "cbar2.set_label('Round')"
   ],
   "id": "cell_115"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the bottom row: Panel 3 showing the MoE token routing heatmap, and Panel 4 showing expert utilization."
   ],
   "id": "cell_116"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Panel 3: MoE routing (bottom left) ---\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "routing_viz = torch.zeros(min(32, NUM_TOKENS_MOE), N_EXPERTS)\n",
    "topk_w, topk_i = top_k_gating(\n",
    "    final_gates[:32].detach(), TOP_K_MOE)\n",
    "for t in range(min(32, NUM_TOKENS_MOE)):\n",
    "    for j in range(TOP_K_MOE):\n",
    "        routing_viz[t, topk_i[t, j]] = topk_w[t, j]\n",
    "\n",
    "im3 = ax3.imshow(routing_viz.numpy(), cmap='YlOrRd', aspect='auto')\n",
    "ax3.set_xlabel('Expert index')\n",
    "ax3.set_ylabel('Token index')\n",
    "ax3.set_title(f'MoE Token Routing (Top-{TOP_K_MOE})',\n",
    "              fontweight='bold', fontsize=11)\n",
    "ax3.set_xticks(range(N_EXPERTS))\n",
    "ax3.set_xticklabels([f'E{i}' for i in range(N_EXPERTS)])\n",
    "plt.colorbar(im3, ax=ax3, label='Gate weight')"
   ],
   "id": "cell_117"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Panel 4 (bottom right): expert utilization bar chart with the ideal utilization line."
   ],
   "id": "cell_118"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Panel 4: Expert utilization (bottom right) ---\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "expert_usage_final = torch.zeros(N_EXPERTS)\n",
    "for i in range(N_EXPERTS):\n",
    "    expert_usage_final[i] = (\n",
    "        (final_routing == i).any(dim=-1).float().mean())\n",
    "\n",
    "bars4 = ax4.bar(range(N_EXPERTS),\n",
    "                expert_usage_final.detach().numpy(),\n",
    "                color=[plt.cm.Set2(i/N_EXPERTS)\n",
    "                       for i in range(N_EXPERTS)],\n",
    "                edgecolor='black', linewidth=0.5)\n",
    "ax4.axhline(y=TOP_K_MOE/N_EXPERTS, color='red', linestyle='--',\n",
    "            linewidth=1.5,\n",
    "            label=f'Ideal ({TOP_K_MOE/N_EXPERTS:.2f})')\n",
    "ax4.set_xlabel('Expert index')\n",
    "ax4.set_ylabel('Fraction of tokens')\n",
    "ax4.set_title('Expert Utilization',\n",
    "              fontweight='bold', fontsize=11)\n",
    "ax4.set_xticks(range(N_EXPERTS))\n",
    "ax4.set_xticklabels([f'E{i}' for i in range(N_EXPERTS)])\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_119"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_36_scale_numbers",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Scale Numbers\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/36_scale_numbers.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Scale Numbers: The Real World\n",
    "\n",
    "Let us put these techniques in context with real production systems."
   ],
   "id": "cell_120"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 65)\n",
    "print(\"  How These Techniques Scale in Production\")\n",
    "print(\"=\" * 65)\n",
    "print()\n",
    "print(\"‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
    "print(\"‚îÇ  SEQUENCE PARALLELISM                                    ‚îÇ\")\n",
    "print(\"‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\")\n",
    "print(\"‚îÇ  GPT-3 (2020):    2K context    ‚Üí no SP needed           ‚îÇ\")\n",
    "print(\"‚îÇ  LLaMA-2 (2023):  4K context    ‚Üí Megatron-SP enough     ‚îÇ\")\n",
    "print(\"‚îÇ  Claude 3 (2024): 200K context  ‚Üí Ring Attention needed  ‚îÇ\")\n",
    "print(\"‚îÇ  Gemini 1.5:      1M+ context   ‚Üí Ring Attention + Flash ‚îÇ\")\n",
    "print(\"‚îÇ                                                          ‚îÇ\")\n",
    "print(\"‚îÇ  Memory: 128K tokens ‚Üí 62 GB for attention matrix        ‚îÇ\")\n",
    "print(\"‚îÇ  With 8-way Ring: 8 GB per GPU (8x reduction)            ‚îÇ\")\n",
    "print(\"‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")"
   ],
   "id": "cell_121"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expert Parallelism in real systems, and the full 5D parallelism summary."
   ],
   "id": "cell_122"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
    "print(\"‚îÇ  EXPERT PARALLELISM                                      ‚îÇ\")\n",
    "print(\"‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\")\n",
    "print(\"‚îÇ  Switch Transformer: 128 experts, top-1,  1.6T total     ‚îÇ\")\n",
    "print(\"‚îÇ  Mixtral 8x7B:       8 experts,  top-2,  47B total       ‚îÇ\")\n",
    "print(\"‚îÇ  DeepSeek-V3:       256 experts, top-8, 671B total       ‚îÇ\")\n",
    "print(\"‚îÇ                                                          ‚îÇ\")\n",
    "print(\"‚îÇ  Key insight: 671B parameters at 37B inference cost       ‚îÇ\")\n",
    "print(\"‚îÇ  That is a 18x parameter-to-compute multiplier!          ‚îÇ\")\n",
    "print(\"‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n",
    "print()\n",
    "print(\"üéâ You now understand all 5 dimensions of parallelism!\")\n",
    "print()\n",
    "print(\"   1. Data Parallelism       ‚Äî split the batch\")\n",
    "print(\"   2. Tensor Parallelism     ‚Äî split the layers (columns/rows)\")\n",
    "print(\"   3. Pipeline Parallelism   ‚Äî split the depth (stages)\")\n",
    "print(\"   4. Sequence Parallelism   ‚Äî split the sequence (ring/SP)\")\n",
    "print(\"   5. Expert Parallelism     ‚Äî split the experts (MoE)\")\n",
    "print()\n",
    "print(\"   Modern systems like DeepSeek-V3 use ALL FIVE \"\n",
    "     \"simultaneously.\")"
   ],
   "id": "cell_123"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_37_reflection",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Reflection\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/37_reflection.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Reflection and Next Steps\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "\n",
    "1. **Why does Ring Attention need the online softmax trick?** Consider what would happen if each GPU computed `softmax(Q_local @ K_block.T / sqrt(d_k)) @ V_block` independently and then summed the results. Would the answer be correct? Why or why not?\n",
    "\n",
    "2. **Top-1 vs Top-2 vs Top-8 routing:** With top-1, each token sees only one expert ‚Äî faster but less expressive. With top-8 (like DeepSeek-V3), each token blends 8 expert opinions ‚Äî more expressive but 8x more compute. What is the trade-off? Why did DeepSeek choose top-8?\n",
    "\n",
    "3. **Load balancing failure modes:** Without the auxiliary loss, the router often collapses to using 1-2 experts. Why does this happen from an optimization perspective? (Hint: think about gradient flow ‚Äî if an expert is never selected, does it ever get gradients?)\n",
    "\n",
    "4. **Combining SP and EP:** In a real system, a single Transformer layer might use Tensor Parallelism for attention, Sequence Parallelism for LayerNorm, and Expert Parallelism for the MoE-MLP. How do the communication patterns interact?"
   ],
   "id": "cell_124"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÜ Optional Challenges\n",
    "\n",
    "1. **Causal Ring Attention:** Our implementation computes full (bidirectional) attention. Modify it to support causal masking, where tokens can only attend to earlier positions. What blocks in the attention matrix become zero?\n",
    "\n",
    "2. **Expert Capacity Factor:** In practice, each expert has a maximum number of tokens it can process (the \"capacity\"). Implement a capacity factor of 1.25 ‚Äî each expert can handle at most $1.25 \\times T/N$ tokens. Tokens that exceed capacity are dropped. How does this interact with load balancing?\n",
    "\n",
    "3. **Shared Expert (DeepSeek-V3):** DeepSeek-V3 uses one \"shared\" expert that processes ALL tokens, plus top-K from the remaining routed experts. Implement this: `y = E_shared(x) + sum(g_i * E_i(x))`. Why might this help?\n",
    "\n",
    "4. **Flash Attention integration:** Our ring attention recomputes the full $c \\times c$ attention block. In practice, each block would use Flash Attention (tiled, memory-efficient). Research how Flash Attention tiles the computation and sketch how it would integrate into the ring."
   ],
   "id": "cell_125"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_38_closing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Wrap-Up\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/38_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ Congratulations!\")\n",
    "print()\n",
    "print(\"You have now built and understood:\")\n",
    "print(\"  ‚úÖ Ring Attention ‚Äî distributing long sequences \"\n",
    "     \"across GPUs in a ring\")\n",
    "print(\"  ‚úÖ Online softmax ‚Äî combining partial attention \"\n",
    "     \"results correctly\")\n",
    "print(\"  ‚úÖ MoE routing ‚Äî top-K expert selection \"\n",
    "     \"with renormalized gates\")\n",
    "print(\"  ‚úÖ Load balancing ‚Äî auxiliary loss to prevent \"\n",
    "     \"expert collapse\")\n",
    "print(\"  ‚úÖ All-to-All dispatch ‚Äî the communication pattern \"\n",
    "     \"of Expert Parallelism\")\n",
    "print()\n",
    "print(\"In the final notebook (Notebook 6), we will bring all \"\n",
    "     \"5 dimensions\")\n",
    "print(\"together into a unified parallelism strategy ‚Äî showing \"\n",
    "     \"how real\")\n",
    "print(\"systems like Megatron-LM and DeepSeek-V3 combine \"\n",
    "     \"DP + TP + PP + SP + EP\")\n",
    "print(\"to train models with hundreds of billions of parameters \"\n",
    "     \"across\")\n",
    "print(\"thousands of GPUs.\")"
   ],
   "id": "cell_126"
  }
 ]
}