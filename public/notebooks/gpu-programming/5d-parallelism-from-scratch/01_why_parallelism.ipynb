{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1K3QJmxvgc0_ZwXawSeV_oQll-6WeibaO\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/01_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_01_setup_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Setup Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_01_setup_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_cell"
   },
   "outputs": [],
   "source": [
    "# ðŸ”§ Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸ No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ Python {sys.version.split()[0]}\")\n",
    "print(f\"ðŸ”¥ PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"ðŸŽ² Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_02_notebook_overview",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Notebook Overview\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_02_notebook_overview.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_1"
   },
   "source": [
    "# ðŸš€ Why Do We Need Parallelism? Memory & Compute Bottlenecks\n",
    "\n",
    "*Part 1 of the Vizuara series on 5D Parallelism from Scratch*\n",
    "*Estimated time: 30 minutes*\n",
    "\n",
    "In this notebook, you will learn to calculate exactly how much memory any model needs for training â€” and understand, with concrete numbers, why a single GPU is never enough for modern LLMs. By the end, you will have built an **interactive memory and compute planner** that visualizes the bottlenecks for any model size."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ðŸ¤– AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** â€” it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[ðŸ‘‰ Open AI Teaching Assistant](https://pods.vizuara.ai/courses/5d-parallelism-from-scratch/practice/1/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_03_plot_setup",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Plot Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_03_plot_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_2"
   },
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plot styling for clean, publication-quality figures\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (10, 6),\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.dpi': 100,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "})\n",
    "\n",
    "print(\"Setup complete! Let us begin.\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_04_why_it_matters",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_04_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_3"
   },
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Training modern large language models requires thousands of GPUs working in concert. Llama 3 405B was trained on 16,384 GPUs. DeepSeek-V3 has 671 billion parameters. These numbers sound impressive, but they raise a fundamental question: **why can we not just use one really powerful GPU?**\n",
    "\n",
    "Before we learn *how* to parallelize training (which is what the rest of this series covers), we need to deeply understand *why* parallelism is necessary. By the end of this notebook, you will be able to:\n",
    "\n",
    "- Calculate the exact memory footprint for training any model\n",
    "- Understand why optimizer states dominate the memory budget\n",
    "- Estimate training time and minimum GPU requirements\n",
    "- Build an interactive tool that visualizes all of these bottlenecks\n",
    "\n",
    "Let us start with a preview of what we are building. Here is the kind of output our final memory calculator will produce:"
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_05_teaser_viz_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Teaser Viz Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_05_teaser_viz_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_4"
   },
   "outputs": [],
   "source": [
    "# ðŸ“Š Teaser: preview of our final output â€” a stacked bar chart memory breakdown\n",
    "# (We will build this step by step. For now, just admire the target.)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "models = ['7B', '13B', '70B', '405B', '671B']\n",
    "# Memory in GB for each component (we will derive these numbers soon)\n",
    "weights_gb =     [14,   26,   140,   810,  1342]\n",
    "gradients_gb =   [14,   26,   140,   810,  1342]\n",
    "optimizer_gb =   [84,  156,   840,  4860,  8052]\n",
    "activations_gb = [40,   60,   180,   500,   600]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.5"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_5"
   },
   "source": [
    "We build the stacked bars layer by layer -- each component sits on top of the previous one:"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_06_teaser_viz_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Teaser Viz Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_06_teaser_viz_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_6"
   },
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualization\n",
    "bars1 = ax.bar(x, weights_gb, width, label='Weights (fp16)', color='#3B82F6')\n",
    "bars2 = ax.bar(x, gradients_gb, width, bottom=weights_gb, label='Gradients (fp16)', color='#10B981')\n",
    "bars3 = ax.bar(x, optimizer_gb, width,\n",
    "               bottom=np.array(weights_gb) + np.array(gradients_gb),\n",
    "               label='Optimizer States (fp32)', color='#F59E0B')\n",
    "bars4 = ax.bar(x, activations_gb, width,\n",
    "               bottom=np.array(weights_gb) + np.array(gradients_gb) + np.array(optimizer_gb),\n",
    "               label='Activations (approx)', color='#8B5CF6', alpha=0.7, linestyle='--')\n",
    "\n",
    "ax.axhline(y=80, color='red', linestyle='--', linewidth=2, label='A100 GPU Memory (80 GB)')\n",
    "ax.set_xlabel('Model Size')\n",
    "ax.set_ylabel('Memory (GB)')\n",
    "ax.set_title('Training Memory Requirements vs. GPU Capacity')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_yscale('log')\n",
    "ax.yaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "ax.set_ylim(10, 20000)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"By the end of this notebook, you will know exactly where every byte comes from.\")"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_07_restaurant_analogy",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Restaurant Analogy\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_07_restaurant_analogy.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_7"
   },
   "source": [
    "## 2. Building Intuition: The Restaurant Analogy\n",
    "\n",
    "Before we touch any math, let us build intuition with an analogy.\n",
    "\n",
    "Imagine you run a restaurant that has just received an order for **1,000 plates of biryani** â€” but you only have **one chef** and **one stove**.\n",
    "\n",
    "What happens?\n",
    "\n",
    "Your chef starts cooking. One plate at a time. Each plate takes about 10 minutes. That means it will take roughly **10,000 minutes â€” about 7 days** â€” to finish all 1,000 plates. Your customers have long left by then.\n",
    "\n",
    "Now think about what exactly is going wrong. There are **two** problems:\n",
    "\n",
    "1. **The chef is too slow** â€” one person simply cannot cook 1,000 plates fast enough. This is a *compute bottleneck*.\n",
    "2. **The kitchen is too small** â€” even if the chef were faster, a single stove can only hold one pot at a time. This is a *memory bottleneck*.\n",
    "\n",
    "This is exactly the situation we face when training large language models:\n",
    "- The **chef** is a single GPU\n",
    "- The **recipe** is the model (weights, gradients, optimizer states)\n",
    "- The **1,000 plates** is the massive training dataset\n",
    "- The **kitchen** is the GPU's memory"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_08_analogy_calculation",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Analogy Calculation\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_08_analogy_calculation.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_8"
   },
   "outputs": [],
   "source": [
    "# Let us make this concrete with a simple calculation\n",
    "\n",
    "plates = 1000\n",
    "time_per_plate_minutes = 10\n",
    "\n",
    "total_minutes = plates * time_per_plate_minutes\n",
    "total_hours = total_minutes / 60\n",
    "total_days = total_hours / 24\n",
    "\n",
    "print(f\"One chef, {plates} plates:\")\n",
    "print(f\"  Total time: {total_minutes:,} minutes\")\n",
    "print(f\"           = {total_hours:,.0f} hours\")\n",
    "print(f\"           = {total_days:.1f} days\")\n",
    "print()\n",
    "\n",
    "# What if we add more chefs?\n",
    "for num_chefs in [1, 4, 10, 100]:\n",
    "    days = total_days / num_chefs\n",
    "    print(f\"  {num_chefs:>3} chef(s): {days:>6.1f} days\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_09_five_strategies",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Five Strategies\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_09_five_strategies.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_9"
   },
   "source": [
    "### The Five Strategies\n",
    "\n",
    "So how do we scale this kitchen? There are five fundamental strategies, each solving a different aspect of the problem:\n",
    "\n",
    "| Strategy | Restaurant Analogy | GPU Parallelism |\n",
    "|---|---|---|\n",
    "| **Hire more chefs** | Each chef gets the full recipe, splits orders | **Data Parallelism** |\n",
    "| **Split the recipe** | Tear the recipe in half, each chef handles part | **Tensor Parallelism** |\n",
    "| **Assembly line** | One chef preps, another cooks, another plates | **Pipeline Parallelism** |\n",
    "| **Split the orders** | Each chef handles different portions of a long order | **Sequence Parallelism** |\n",
    "| **Specialist chefs** | One for appetizers, one for mains, one for desserts | **Expert Parallelism** |\n",
    "\n",
    "These five strategies are exactly what companies like Meta, Google, and DeepSeek use to train models across thousands of GPUs. In the rest of this notebook series, we will implement each one from scratch. But first, we need to understand the *why* â€” and that means understanding memory."
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_10_reflection_analogy",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Narration: Reflection Analogy\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_10_reflection_analogy.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_10"
   },
   "source": [
    "### ðŸ¤” Think About This\n",
    "\n",
    "Before we dive into the math, pause and think:\n",
    "\n",
    "1. If you had **4 chefs** and could split work perfectly, how long would 1,000 plates take?\n",
    "2. But what if the **recipe itself** is so complex that no single chef can remember it all? Adding more chefs does not help with that problem.\n",
    "3. Which of the five strategies addresses the \"recipe too complex\" problem?\n",
    "\n",
    "*Take a moment to think, then continue.*"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_11_bottlenecks_viz_compute",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Bottlenecks Viz Compute\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_11_bottlenecks_viz_compute.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_11"
   },
   "outputs": [],
   "source": [
    "# ðŸ“Š Let us visualize the two bottlenecks side by side\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left: Compute bottleneck â€” time vs number of chefs\n",
    "num_chefs = np.array([1, 2, 4, 8, 16, 32, 64, 128])\n",
    "ideal_days = total_days / num_chefs\n",
    "\n",
    "ax1.plot(num_chefs, ideal_days, 'o-', color='#3B82F6', linewidth=2, markersize=8)\n",
    "ax1.axhline(y=1, color='red', linestyle='--', alpha=0.7, label='1 day target')\n",
    "ax1.set_xlabel('Number of Chefs (GPUs)')\n",
    "ax1.set_ylabel('Time (days)')\n",
    "ax1.set_title('Compute Bottleneck\\n\"The chef is too slow\"')\n",
    "ax1.set_xscale('log', base=2)\n",
    "ax1.set_yscale('log')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_12_bottlenecks_viz_memory_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Bottlenecks Viz Memory Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_12_bottlenecks_viz_memory_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_12"
   },
   "source": [
    "Now the memory bottleneck -- model size vs. available GPU memory:"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_13_bottlenecks_viz_memory_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Bottlenecks Viz Memory Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_13_bottlenecks_viz_memory_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_13"
   },
   "outputs": [],
   "source": [
    "# ðŸ“Š Right: Memory bottleneck â€” model size vs GPU memory\n",
    "model_sizes_B = np.array([1, 3, 7, 13, 30, 70, 175, 405, 671])\n",
    "memory_needed_GB = model_sizes_B * 16  # 16 bytes per parameter (we will derive this)\n",
    "gpu_memory = 80  # A100\n",
    "\n",
    "colors = ['#10B981' if m <= gpu_memory else '#EF4444' for m in memory_needed_GB]\n",
    "ax2.bar(range(len(model_sizes_B)), memory_needed_GB, color=colors, edgecolor='white')\n",
    "ax2.axhline(y=gpu_memory, color='red', linestyle='--', linewidth=2, label=f'A100 Memory ({gpu_memory} GB)')\n",
    "ax2.set_xlabel('Model Size')\n",
    "ax2.set_ylabel('Training Memory (GB)')\n",
    "ax2.set_title('Memory Bottleneck\\n\"The kitchen is too small\"')\n",
    "ax2.set_xticks(range(len(model_sizes_B)))\n",
    "ax2.set_xticklabels([f'{s}B' for s in model_sizes_B], rotation=45)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Green = fits on one A100, Red = does not fit.\")\n",
    "print(\"Notice: even a 7B model does not fit!\")"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_14_math_of_memory",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Math Of Memory\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_14_math_of_memory.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_14"
   },
   "source": [
    "## 3. The Mathematics of Memory\n",
    "\n",
    "Now let us formalize exactly where all this memory goes. When we train a model using **mixed-precision training** with the **Adam optimizer** (which is what virtually all modern LLMs use), each parameter requires memory for four things.\n",
    "\n",
    "### 3.1 The Four Memory Components\n",
    "\n",
    "**Component 1: Model Weights (fp16) â€” 2 bytes per parameter**\n",
    "\n",
    "The model weights are stored in 16-bit floating point (fp16 or bfloat16) during the forward and backward pass. Each fp16 value takes 2 bytes.\n",
    "\n",
    "**Component 2: Gradients (fp16) â€” 2 bytes per parameter**\n",
    "\n",
    "During backpropagation, we compute the gradient of the loss with respect to each parameter. These are also stored in fp16, so 2 bytes per parameter.\n",
    "\n",
    "**Component 3: Optimizer States (fp32) â€” 12 bytes per parameter**\n",
    "\n",
    "This is where Adam gets expensive. Adam maintains:\n",
    "- A **fp32 copy** of the weights: 4 bytes per parameter\n",
    "- The **first moment** (moving average of gradients): 4 bytes per parameter\n",
    "- The **second moment** (moving average of squared gradients): 4 bytes per parameter\n",
    "\n",
    "That is $4 + 4 + 4 = 12$ bytes per parameter â€” just for the optimizer.\n",
    "\n",
    "### The Total\n",
    "\n",
    "Putting it all together, the memory for model state (excluding activations) is:\n",
    "\n",
    "$$M_{\\text{model}} = P \\times (2 + 2 + 12) = 16P \\text{ bytes}$$\n",
    "\n",
    "where $P$ is the number of parameters.\n",
    "\n",
    "Computationally, this equation says: for every single parameter in your model, you need to store the parameter itself (2 bytes), its gradient (2 bytes), and three optimizer buffers (12 bytes). The optimizer alone accounts for **75%** of the model state memory."
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_15_memory_breakdown_func",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Memory Breakdown Func\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_15_memory_breakdown_func.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_15"
   },
   "outputs": [],
   "source": [
    "def memory_breakdown_bytes(num_params):\n",
    "    \"\"\"\n",
    "    Calculate memory breakdown for mixed-precision training with Adam.\n",
    "\n",
    "    Args:\n",
    "        num_params: Number of model parameters\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with memory in bytes for each component\n",
    "    \"\"\"\n",
    "    weights_bytes = num_params * 2          # fp16 weights\n",
    "    gradients_bytes = num_params * 2        # fp16 gradients\n",
    "    optimizer_fp32_copy = num_params * 4    # fp32 master weights\n",
    "    optimizer_first_moment = num_params * 4 # Adam m (first moment)\n",
    "    optimizer_second_moment = num_params * 4 # Adam v (second moment)\n",
    "\n",
    "    optimizer_total = optimizer_fp32_copy + optimizer_first_moment + optimizer_second_moment\n",
    "\n",
    "    return {\n",
    "        'weights': weights_bytes,\n",
    "        'gradients': gradients_bytes,\n",
    "        'optimizer_fp32_copy': optimizer_fp32_copy,\n",
    "        'optimizer_first_moment': optimizer_first_moment,\n",
    "        'optimizer_second_moment': optimizer_second_moment,\n",
    "        'optimizer_total': optimizer_total,\n",
    "        'total': weights_bytes + gradients_bytes + optimizer_total,\n",
    "    }"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_16_bytes_to_gb_func",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Bytes To Gb Func\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_16_bytes_to_gb_func.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_16"
   },
   "source": [
    "A small helper to convert raw byte counts to human-readable gigabytes:"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_17"
   },
   "outputs": [],
   "source": [
    "def bytes_to_gb(b):\n",
    "    \"\"\"Convert bytes to gigabytes.\"\"\"\n",
    "    return b / (1024 ** 3)"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_17_7b_model_breakdown",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: 7b Model Breakdown\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_17_7b_model_breakdown.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_18"
   },
   "source": [
    "Let us apply this to a 7B parameter model and see the breakdown:"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_19"
   },
   "outputs": [],
   "source": [
    "# Compute for a 7B model\n",
    "P = 7e9\n",
    "breakdown = memory_breakdown_bytes(P)\n",
    "\n",
    "print(f\"Memory breakdown for a {P/1e9:.0f}B parameter model:\")\n",
    "print(f\"  Weights (fp16):           {bytes_to_gb(breakdown['weights']):>8.1f} GB\")\n",
    "print(f\"  Gradients (fp16):         {bytes_to_gb(breakdown['gradients']):>8.1f} GB\")\n",
    "print(f\"  Optimizer fp32 copy:      {bytes_to_gb(breakdown['optimizer_fp32_copy']):>8.1f} GB\")\n",
    "print(f\"  Optimizer first moment:   {bytes_to_gb(breakdown['optimizer_first_moment']):>8.1f} GB\")\n",
    "print(f\"  Optimizer second moment:  {bytes_to_gb(breakdown['optimizer_second_moment']):>8.1f} GB\")\n",
    "print(f\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(f\"  Optimizer total:          {bytes_to_gb(breakdown['optimizer_total']):>8.1f} GB\")\n",
    "print(f\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(f\"  TOTAL model state:        {bytes_to_gb(breakdown['total']):>8.1f} GB\")\n",
    "print()\n",
    "print(f\"A100 GPU memory: 80 GB\")\n",
    "print(f\"Deficit: {bytes_to_gb(breakdown['total']) - 80:.1f} GB over budget!\")"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_18_optimizer_insight",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Optimizer Insight\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_18_optimizer_insight.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_20"
   },
   "source": [
    "### ðŸ’¡ Key Insight: Optimizer States Dominate\n",
    "\n",
    "Notice something striking: the optimizer states (78 GB) account for **75%** of the total memory. The actual model weights are only 13 GB. This is a crucial insight â€” when we learn about ZeRO (Zero Redundancy Optimizer) in the Data Parallelism notebook, we will see that the first thing it does is shard the optimizer states across GPUs, because that gives the biggest bang for the buck."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_19_7b_viz_pie_chart",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: 7b Viz Pie Chart\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_19_7b_viz_pie_chart.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_21"
   },
   "outputs": [],
   "source": [
    "# ðŸ“Š ation Checkpoint 1: Pie chart of memory components\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart showing the breakdown\n",
    "labels = ['Weights\\n(fp16)', 'Gradients\\n(fp16)', 'Optimizer\\nfp32 copy',\n",
    "          'Optimizer\\n1st moment', 'Optimizer\\n2nd moment']\n",
    "sizes = [\n",
    "    bytes_to_gb(breakdown['weights']),\n",
    "    bytes_to_gb(breakdown['gradients']),\n",
    "    bytes_to_gb(breakdown['optimizer_fp32_copy']),\n",
    "    bytes_to_gb(breakdown['optimizer_first_moment']),\n",
    "    bytes_to_gb(breakdown['optimizer_second_moment']),\n",
    "]\n",
    "colors = ['#3B82F6', '#10B981', '#F59E0B', '#EF4444', '#8B5CF6']\n",
    "explode = (0, 0, 0.05, 0.05, 0.05)  # Emphasize optimizer components\n",
    "\n",
    "wedges, texts, autotexts = ax1.pie(\n",
    "    sizes, labels=labels, autopct='%1.0f%%', colors=colors,\n",
    "    explode=explode, startangle=90, textprops={'fontsize': 10}\n",
    ")\n",
    "ax1.set_title(f'Memory Breakdown: {P/1e9:.0f}B Model\\n(Total: {bytes_to_gb(breakdown[\"total\"]):.0f} GB)')"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_20_7b_viz_stacked_bar",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: 7b Viz Stacked Bar\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_20_7b_viz_stacked_bar.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_22"
   },
   "source": [
    "Now we add a stacked bar comparing the 7B model's memory against GPU capacity:"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_23"
   },
   "outputs": [],
   "source": [
    "# ðŸ“Š Stacked bar for 7B model vs GPU capacity\n",
    "categories = ['Weights', 'Gradients', 'Optimizer']\n",
    "values = [\n",
    "    bytes_to_gb(breakdown['weights']),\n",
    "    bytes_to_gb(breakdown['gradients']),\n",
    "    bytes_to_gb(breakdown['optimizer_total']),\n",
    "]\n",
    "bar_colors = ['#3B82F6', '#10B981', '#F59E0B']\n",
    "\n",
    "bottom = 0\n",
    "for cat, val, col in zip(categories, values, bar_colors):\n",
    "    ax2.bar('7B Model', val, bottom=bottom, color=col, label=f'{cat}: {val:.0f} GB',\n",
    "            edgecolor='white', linewidth=0.5)\n",
    "    bottom += val\n",
    "\n",
    "ax2.axhline(y=80, color='red', linestyle='--', linewidth=2, label='A100 (80 GB)')\n",
    "ax2.set_ylabel('Memory (GB)')\n",
    "ax2.set_title('7B Model: Does It Fit on One A100?')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set_ylim(0, 130)\n",
    "ax2.annotate(\n",
    "    f'Exceeds A100 by\\n{bytes_to_gb(breakdown[\"total\"]) - 80:.0f} GB!',\n",
    "    xy=(0, 80), xytext=(0.5, 60),\n",
    "    fontsize=11, color='red', fontweight='bold',\n",
    "    arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "    ha='center'\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_21_activations_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Activations Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_21_activations_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_24"
   },
   "source": [
    "### 3.2 But Wait â€” What About Activations?\n",
    "\n",
    "The memory breakdown above covers the *model state*: weights, gradients, and optimizer states. But there is another major memory consumer: **activations**.\n",
    "\n",
    "During the forward pass, every layer produces intermediate outputs. These intermediate tensors must be **saved in memory** because the backward pass needs them to compute gradients. This is a fundamental property of backpropagation â€” you cannot compute gradients without the activations from the forward pass.\n",
    "\n",
    "The activation memory depends on the model architecture and the training configuration:\n",
    "\n",
    "$$M_{\\text{act}} \\approx 2 \\times B \\times S \\times H \\times L \\times b_{\\text{elem}}$$\n",
    "\n",
    "where:\n",
    "- $B$ = batch size (number of sequences processed at once)\n",
    "- $S$ = sequence length (number of tokens per sequence)\n",
    "- $H$ = hidden dimension (width of each layer)\n",
    "- $L$ = number of layers (depth of the model)\n",
    "- $b_{\\text{elem}}$ = bytes per element (2 for fp16)\n",
    "- The factor of 2 accounts for both the layer input and the attention intermediate states (a rough approximation)\n",
    "\n",
    "Computationally, this says: for every token in every sequence in the batch, at every layer, we need to store the hidden representation. The factor of 2 is a simplification â€” real Transformers store additional intermediates (attention scores, post-LayerNorm values, etc.), but this gives the right order of magnitude.\n",
    "\n",
    "> **Warning**: This is a simplified formula. The real activation memory includes attention score matrices (which scale as $B \\times S^2$ per head per layer), post-normalization values, MLP intermediates, and more. For a full treatment, see the Megatron-LM paper. Our formula captures the dominant term for typical configurations."
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_22_activation_memory_func",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Activation Memory Func\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_22_activation_memory_func.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_25"
   },
   "outputs": [],
   "source": [
    "def activation_memory_gb(batch_size, seq_len, hidden_dim, num_layers,\n",
    "                          bytes_per_element=2):\n",
    "    \"\"\"\n",
    "    Estimate activation memory for a Transformer model.\n",
    "    Includes hidden states, attention scores, and MLP intermediates.\n",
    "\n",
    "    Args:\n",
    "        batch_size: Number of sequences per batch\n",
    "        seq_len: Tokens per sequence\n",
    "        hidden_dim: Hidden dimension (model width)\n",
    "        num_layers: Number of Transformer layers\n",
    "        bytes_per_element: Bytes per value (2 for fp16/bf16)\n",
    "\n",
    "    Returns:\n",
    "        Estimated activation memory in GB\n",
    "    \"\"\"\n",
    "    # Main hidden state activations: input to each layer\n",
    "    hidden_state_bytes = batch_size * seq_len * hidden_dim * num_layers * bytes_per_element\n",
    "\n",
    "    # Attention intermediates: Q, K, V projections + attention scores\n",
    "    num_heads = max(hidden_dim // 128, 1)  # Typical head dim = 128\n",
    "    attn_scores_bytes = batch_size * num_heads * seq_len * seq_len * bytes_per_element * num_layers\n",
    "\n",
    "    # MLP intermediates: typically 4x expansion\n",
    "    mlp_bytes = batch_size * seq_len * (4 * hidden_dim) * num_layers * bytes_per_element\n",
    "\n",
    "    total_bytes = hidden_state_bytes + attn_scores_bytes + mlp_bytes\n",
    "    return total_bytes / (1024 ** 3)"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_23_activation_scaling",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Activation Scaling\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_23_activation_scaling.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_26"
   },
   "source": [
    "Let us apply this function to a 7B model and see how activation memory scales with batch size:"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_27"
   },
   "outputs": [],
   "source": [
    "# Typical architecture for a 7B model (similar to Llama 2 7B)\n",
    "act_mem = activation_memory_gb(\n",
    "    batch_size=1,\n",
    "    seq_len=2048,\n",
    "    hidden_dim=4096,\n",
    "    num_layers=32\n",
    ")\n",
    "\n",
    "print(f\"Activation memory estimate for 7B model (batch_size=1, seq_len=2048):\")\n",
    "print(f\"  {act_mem:.1f} GB\")\n",
    "print()\n",
    "\n",
    "# Show how it grows with batch size\n",
    "print(\"How activation memory scales with batch size:\")\n",
    "for bs in [1, 2, 4, 8, 16]:\n",
    "    mem = activation_memory_gb(bs, 2048, 4096, 32)\n",
    "    print(f\"  batch_size={bs:>2}: {mem:>7.1f} GB\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_24_activation_viz_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Activation Viz Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_24_activation_viz_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_28"
   },
   "source": [
    "Now let us visualize how activation memory grows with batch size and sequence length:"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_25_activation_viz_batch_size",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Activation Viz Batch Size\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_25_activation_viz_batch_size.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_29"
   },
   "outputs": [],
   "source": [
    "# ðŸ“Š ation Checkpoint 2: Activation memory growth\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# Left: activation memory vs batch size\n",
    "batch_sizes = np.arange(1, 33)\n",
    "act_mems = [activation_memory_gb(bs, 2048, 4096, 32) for bs in batch_sizes]\n",
    "\n",
    "ax1.plot(batch_sizes, act_mems, 'o-', color='#8B5CF6', linewidth=2, markersize=4)\n",
    "ax1.axhline(y=80, color='red', linestyle='--', linewidth=1.5, label='A100 (80 GB)')\n",
    "ax1.fill_between(batch_sizes, act_mems, alpha=0.2, color='#8B5CF6')\n",
    "ax1.set_xlabel('Batch Size')\n",
    "ax1.set_ylabel('Activation Memory (GB)')\n",
    "ax1.set_title('Activation Memory vs. Batch Size\\n(7B model, seq_len=2048)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_26_activation_viz_seq_len_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Activation Viz Seq Len Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_26_activation_viz_seq_len_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_30"
   },
   "source": [
    "Sequence length has an even more dramatic effect because attention scores scale quadratically:"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_27_activation_viz_seq_len_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Activation Viz Seq Len Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_27_activation_viz_seq_len_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_31"
   },
   "outputs": [],
   "source": [
    "# ðŸ“Š Right: activation memory vs sequence length\n",
    "seq_lens = [512, 1024, 2048, 4096, 8192, 16384, 32768]\n",
    "act_mems_seq = [activation_memory_gb(1, sl, 4096, 32) for sl in seq_lens]\n",
    "\n",
    "ax2.bar(range(len(seq_lens)), act_mems_seq, color='#8B5CF6', alpha=0.8, edgecolor='white')\n",
    "ax2.axhline(y=80, color='red', linestyle='--', linewidth=1.5, label='A100 (80 GB)')\n",
    "ax2.set_xlabel('Sequence Length')\n",
    "ax2.set_ylabel('Activation Memory (GB)')\n",
    "ax2.set_title('Activation Memory vs. Sequence Length\\n(7B model, batch_size=1)')\n",
    "ax2.set_xticks(range(len(seq_lens)))\n",
    "ax2.set_xticklabels([f'{sl//1024}K' if sl >= 1024 else str(sl) for sl in seq_lens], rotation=45)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: activation memory grows linearly with batch size,\")\n",
    "print(\"but can grow SUPER-linearly with sequence length (due to attention scores).\")"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_28_build_calculator_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Transition: Build Calculator Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_28_build_calculator_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_32"
   },
   "source": [
    "## 4. Let Us Build It â€” Component by Component\n",
    "\n",
    "### 4.1 A Complete Memory Calculator\n",
    "\n",
    "Now let us put everything together into one clean function that computes the full memory breakdown."
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_29_full_memory_calculator_func",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Full Memory Calculator Func\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_29_full_memory_calculator_func.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_33"
   },
   "outputs": [],
   "source": [
    "def compute_full_memory(\n",
    "    num_params_billions, batch_size=1, seq_len=2048,\n",
    "    hidden_dim=4096, num_layers=32,\n",
    "    weight_bytes=2, grad_bytes=2,\n",
    "    optimizer_bytes=12, activation_bytes_per_elem=2,\n",
    "):\n",
    "    \"\"\"Compute the full training memory breakdown for a model.\n",
    "    Returns dict with weights_gb, gradients_gb, optimizer_gb,\n",
    "    activations_gb, total_gb, and num_params_B.\"\"\"\n",
    "    P = num_params_billions * 1e9\n",
    "\n",
    "    weights_gb = (P * weight_bytes) / (1024**3)\n",
    "    gradients_gb = (P * grad_bytes) / (1024**3)\n",
    "    optimizer_gb = (P * optimizer_bytes) / (1024**3)\n",
    "    activations_gb = activation_memory_gb(\n",
    "        batch_size, seq_len, hidden_dim, num_layers, activation_bytes_per_elem\n",
    "    )\n",
    "    total_gb = weights_gb + gradients_gb + optimizer_gb + activations_gb\n",
    "\n",
    "    return {'weights_gb': weights_gb, 'gradients_gb': gradients_gb,\n",
    "            'optimizer_gb': optimizer_gb, 'activations_gb': activations_gb,\n",
    "            'total_gb': total_gb, 'num_params_B': num_params_billions}"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_30_test_full_calculator",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Test Full Calculator\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_30_test_full_calculator.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_34"
   },
   "source": [
    "Let us test this function with our 7B model and verify the numbers match our earlier calculation:"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_35"
   },
   "outputs": [],
   "source": [
    "# Test with our 7B model\n",
    "result = compute_full_memory(7, batch_size=1, seq_len=2048, hidden_dim=4096, num_layers=32)\n",
    "\n",
    "print(f\"Full memory breakdown for 7B model:\")\n",
    "print(f\"  Weights:       {result['weights_gb']:>8.1f} GB  ({result['weights_gb']/result['total_gb']*100:.0f}%)\")\n",
    "print(f\"  Gradients:     {result['gradients_gb']:>8.1f} GB  ({result['gradients_gb']/result['total_gb']*100:.0f}%)\")\n",
    "print(f\"  Optimizer:     {result['optimizer_gb']:>8.1f} GB  ({result['optimizer_gb']/result['total_gb']*100:.0f}%)\")\n",
    "print(f\"  Activations:   {result['activations_gb']:>8.1f} GB  ({result['activations_gb']/result['total_gb']*100:.0f}%)\")\n",
    "print(f\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(f\"  TOTAL:         {result['total_gb']:>8.1f} GB\")\n",
    "print(f\"\\n  A100 memory:       80.0 GB\")\n",
    "print(f\"  Overflow:      {result['total_gb'] - 80:.1f} GB\")"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_31_scaling_to_real_models_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Scaling To Real Models Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_31_scaling_to_real_models_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_36"
   },
   "source": [
    "### 4.2 Scaling to Real Models\n",
    "\n",
    "Now let us compute memory requirements for the models that are actually being trained today. We will use approximate architecture parameters for each."
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_32_real_model_configs_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Real Model Configs Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_32_real_model_configs_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_37"
   },
   "outputs": [],
   "source": [
    "# Real model configurations (approximate)\n",
    "model_configs = {\n",
    "    '7B':   {'params': 7,   'hidden': 4096,  'layers': 32,  'heads': 32},\n",
    "    '13B':  {'params': 13,  'hidden': 5120,  'layers': 40,  'heads': 40},\n",
    "    '70B':  {'params': 70,  'hidden': 8192,  'layers': 80,  'heads': 64},\n",
    "    '405B': {'params': 405, 'hidden': 16384, 'layers': 126, 'heads': 128},\n",
    "    '671B': {'params': 671, 'hidden': 7168,  'layers': 61,  'heads': 128},  # DeepSeek-V3 (MoE)\n",
    "}\n",
    "\n",
    "print(f\"{'Model':<8} {'Weights':>10} {'Gradients':>10} {'Optimizer':>10} {'Activations':>12} {'TOTAL':>10} {'Fits A100?':>12}\")\n",
    "print(\"â”€\" * 85)\n",
    "\n",
    "results = {}\n",
    "for name, cfg in model_configs.items():\n",
    "    r = compute_full_memory(\n",
    "        cfg['params'],\n",
    "        batch_size=1,\n",
    "        seq_len=2048,\n",
    "        hidden_dim=cfg['hidden'],\n",
    "        num_layers=cfg['layers'],\n",
    "    )\n",
    "    results[name] = r\n",
    "    fits = \"Yes\" if r['total_gb'] <= 80 else \"No\"\n",
    "    emoji = \"  âœ…\" if fits == \"Yes\" else \"  âŒ\"\n",
    "    print(f\"{name:<8} {r['weights_gb']:>8.1f}GB {r['gradients_gb']:>8.1f}GB \"\n",
    "          f\"{r['optimizer_gb']:>8.1f}GB {r['activations_gb']:>10.1f}GB \"\n",
    "          f\"{r['total_gb']:>8.1f}GB {emoji}\")"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_33_visualize_all_models_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Visualize All Models Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_33_visualize_all_models_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_38"
   },
   "source": [
    "Let us visualize all five models in a single stacked bar chart to see the full picture:"
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_39"
   },
   "outputs": [],
   "source": [
    "# ðŸ“Š ation Checkpoint 3: Stacked bar chart comparing all models\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "model_names = list(results.keys())\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.55\n",
    "\n",
    "weights = [results[m]['weights_gb'] for m in model_names]\n",
    "grads = [results[m]['gradients_gb'] for m in model_names]\n",
    "optim = [results[m]['optimizer_gb'] for m in model_names]\n",
    "acts = [results[m]['activations_gb'] for m in model_names]"
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_34_visualize_all_models_drawing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Visualize All Models Drawing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_34_visualize_all_models_drawing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_40"
   },
   "source": [
    "We draw each memory component as a stacked layer and add reference lines for GPU capacity:"
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_41"
   },
   "outputs": [],
   "source": [
    "b1 = ax.bar(x, weights, width, label='Weights (fp16)', color='#3B82F6', edgecolor='white')\n",
    "b2 = ax.bar(x, grads, width, bottom=weights, label='Gradients (fp16)', color='#10B981', edgecolor='white')\n",
    "b3 = ax.bar(x, optim, width,\n",
    "            bottom=np.array(weights) + np.array(grads),\n",
    "            label='Optimizer States (fp32)', color='#F59E0B', edgecolor='white')\n",
    "b4 = ax.bar(x, acts, width,\n",
    "            bottom=np.array(weights) + np.array(grads) + np.array(optim),\n",
    "            label='Activations (est.)', color='#8B5CF6', alpha=0.75, edgecolor='white')\n",
    "\n",
    "# GPU memory lines\n",
    "ax.axhline(y=80, color='#EF4444', linestyle='--', linewidth=2.5, label='A100 80GB', zorder=5)\n",
    "ax.axhline(y=80, color='#06B6D4', linestyle='-.', linewidth=2, label='H100 80GB', zorder=5)\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=13)\n",
    "ax.set_ylabel('Training Memory (GB)', fontsize=13)\n",
    "ax.set_title('Training Memory Requirements Across Model Sizes\\n(Mixed-precision, Adam optimizer, batch_size=1, seq_len=2048)', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_names, fontsize=12)\n",
    "ax.legend(loc='upper left', fontsize=10)\n",
    "ax.set_yscale('log')\n",
    "ax.yaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "ax.set_ylim(10, 20000)\n",
    "ax.grid(axis='y', alpha=0.3)"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_35_visualize_all_models_render",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Visualize All Models Render\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_35_visualize_all_models_render.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_42"
   },
   "source": [
    "Finally, annotate each bar with the total memory and render the chart:"
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_43"
   },
   "outputs": [],
   "source": [
    "# ðŸ“Š Annotate totals\n",
    "for i, m in enumerate(model_names):\n",
    "    total = results[m]['total_gb']\n",
    "    ax.text(i, total * 1.15, f'{total:.0f} GB', ha='center', va='bottom',\n",
    "            fontsize=10, fontweight='bold', color='#1F2937')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Every model from 7B and above exceeds a single A100.\")\n",
    "print(\"The 405B model needs over 50x the memory of one A100!\")"
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_36_todo_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_36_todo_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_44"
   },
   "source": [
    "## 5. ðŸ”§ Your Turn\n",
    "\n",
    "Now it is your turn. Complete the following exercises to solidify your understanding.\n",
    "\n",
    "### ðŸ”§ TODO 1: Implement the Full Memory Calculator\n",
    "\n",
    "Complete the function below that takes a model size and returns the **total** memory required, including both model state and activations. We have provided the skeleton â€” fill in the blanks."
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_37_todo_1_memory_calculator",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo Memory Calculator\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_37_todo_1_memory_calculator.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_45"
   },
   "outputs": [],
   "source": [
    "def total_training_memory_gb(num_params_billions, batch_size, seq_len,\n",
    "                              hidden_dim, num_layers):\n",
    "    \"\"\"\n",
    "    Calculate total training memory in GB.\n",
    "    Returns the sum of weights, gradients, optimizer states, and activations.\n",
    "    \"\"\"\n",
    "    P = num_params_billions * 1e9\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Calculate weight memory in GB (fp16 = 2 bytes per param)\n",
    "    # Step 2: Calculate gradient memory in GB (fp16 = 2 bytes per param)\n",
    "    # Step 3: Calculate optimizer memory in GB (Adam fp32 = 12 bytes per param)\n",
    "    # Step 4: Calculate activation memory using activation_memory_gb()\n",
    "    # Step 5: Sum all four components\n",
    "    # ==============================\n",
    "\n",
    "    weights_gb = ???      # YOUR CODE HERE\n",
    "    gradients_gb = ???    # YOUR CODE HERE\n",
    "    optimizer_gb = ???    # YOUR CODE HERE\n",
    "    act_gb = ???          # YOUR CODE HERE\n",
    "\n",
    "    total_gb = ???        # YOUR CODE HERE\n",
    "\n",
    "    return total_gb"
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_38_todo_1_verification",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Todo Verification\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_38_todo_1_verification.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_40_todo_2_verification",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Todo Verification\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_40_todo_2_verification.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_46"
   },
   "outputs": [],
   "source": [
    "# Verification: Run this cell to check your implementation\n",
    "\n",
    "# Test case 1: 7B model\n",
    "expected_7b = compute_full_memory(7, batch_size=1, seq_len=2048,\n",
    "                                   hidden_dim=4096, num_layers=32)['total_gb']\n",
    "your_7b = total_training_memory_gb(7, batch_size=1, seq_len=2048,\n",
    "                                    hidden_dim=4096, num_layers=32)\n",
    "\n",
    "assert abs(your_7b - expected_7b) < 0.1, \\\n",
    "    f\"For 7B model: expected {expected_7b:.1f} GB, got {your_7b:.1f} GB\"\n",
    "\n",
    "# Test case 2: 70B model\n",
    "expected_70b = compute_full_memory(70, batch_size=2, seq_len=4096,\n",
    "                                    hidden_dim=8192, num_layers=80)['total_gb']\n",
    "your_70b = total_training_memory_gb(70, batch_size=2, seq_len=4096,\n",
    "                                     hidden_dim=8192, num_layers=80)\n",
    "\n",
    "assert abs(your_70b - expected_70b) < 0.1, \\\n",
    "    f\"For 70B model: expected {expected_70b:.1f} GB, got {your_70b:.1f} GB\"\n",
    "\n",
    "print(\"Correct! Your total_training_memory_gb function works perfectly.\")\n",
    "print(f\"   7B model:  {your_7b:.1f} GB\")\n",
    "print(f\"   70B model: {your_70b:.1f} GB\")"
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_39_todo_2_min_gpus",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Todo Min Gpus\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_39_todo_2_min_gpus.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_47"
   },
   "source": [
    "### ðŸ”§ TODO 2: Minimum Number of GPUs\n",
    "\n",
    "Given a model size and GPU memory capacity, compute the **minimum number of GPUs** needed if we could perfectly shard everything (weights, gradients, optimizer states, and activations) across GPUs. In reality, there is overhead from communication and imperfect sharding, but this gives a theoretical lower bound."
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_48"
   },
   "outputs": [],
   "source": [
    "def min_gpus_needed(num_params_billions, gpu_memory_gb=80,\n",
    "                    batch_size=1, seq_len=2048,\n",
    "                    hidden_dim=4096, num_layers=32):\n",
    "    \"\"\"\n",
    "    Compute the minimum number of GPUs needed to train a model,\n",
    "    assuming perfect memory sharding.\n",
    "\n",
    "    Args:\n",
    "        num_params_billions: Parameters in billions\n",
    "        gpu_memory_gb: Memory per GPU in GB\n",
    "        batch_size, seq_len, hidden_dim, num_layers: Model/training config\n",
    "\n",
    "    Returns:\n",
    "        min_gpus (int): Minimum number of GPUs (ceiling division)\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Calculate total memory using compute_full_memory()\n",
    "    # Step 2: Divide by GPU memory and round UP (use int() + 1 trick or math.ceil)\n",
    "    # ==============================\n",
    "\n",
    "    total_mem = ???       # YOUR CODE HERE\n",
    "    min_gpus = ???        # YOUR CODE HERE\n",
    "\n",
    "    return min_gpus"
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_49"
   },
   "outputs": [],
   "source": [
    "# Verification: Run this cell to check your implementation\n",
    "import math\n",
    "\n",
    "# 7B model on A100 (80 GB)\n",
    "result_7b = compute_full_memory(7, batch_size=1, seq_len=2048, hidden_dim=4096, num_layers=32)\n",
    "expected_gpus_7b = math.ceil(result_7b['total_gb'] / 80)\n",
    "your_gpus_7b = min_gpus_needed(7, gpu_memory_gb=80, batch_size=1, seq_len=2048,\n",
    "                                hidden_dim=4096, num_layers=32)\n",
    "\n",
    "assert your_gpus_7b == expected_gpus_7b, \\\n",
    "    f\"For 7B model: expected {expected_gpus_7b} GPUs, got {your_gpus_7b}\"\n",
    "\n",
    "# 405B model on A100 (80 GB)\n",
    "result_405b = compute_full_memory(405, batch_size=1, seq_len=2048, hidden_dim=16384, num_layers=126)\n",
    "expected_gpus_405b = math.ceil(result_405b['total_gb'] / 80)\n",
    "your_gpus_405b = min_gpus_needed(405, gpu_memory_gb=80, batch_size=1, seq_len=2048,\n",
    "                                  hidden_dim=16384, num_layers=126)\n",
    "\n",
    "assert your_gpus_405b == expected_gpus_405b, \\\n",
    "    f\"For 405B model: expected {expected_gpus_405b} GPUs, got {your_gpus_405b}\"\n",
    "\n",
    "print(\"Correct! Your min_gpus_needed function works perfectly.\")\n",
    "print(f\"   7B model on A100:   {your_gpus_7b} GPUs minimum\")\n",
    "print(f\"   405B model on A100: {your_gpus_405b} GPUs minimum\")\n",
    "print(f\"\\n   In practice, you need many more GPUs than this due to\")\n",
    "print(f\"   communication overhead and imperfect sharding.\")"
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_41_compute_bottleneck_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Compute Bottleneck Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_41_compute_bottleneck_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_50"
   },
   "source": [
    "## 6. The Compute Bottleneck\n",
    "\n",
    "So far we have focused on *memory*. But even if memory were not a problem â€” imagine GPUs with infinite memory â€” training would still take an enormous amount of time on a single GPU. This is the **compute bottleneck**.\n",
    "\n",
    "### 6.1 Estimating Training FLOPs\n",
    "\n",
    "A widely used rule of thumb (from the Chinchilla paper and others) estimates the total floating-point operations for training as:\n",
    "\n",
    "$$\\text{FLOPs}_{\\text{train}} \\approx 6 \\times P \\times D$$\n",
    "\n",
    "where:\n",
    "- $P$ = number of model parameters\n",
    "- $D$ = number of training tokens (dataset size)\n",
    "- The factor of 6 comes from: 2 FLOPs per parameter per token for the forward pass (multiply-add), and approximately 2x that for the backward pass, giving $2 \\times 3 = 6$.\n",
    "\n",
    "Computationally, this says: for each token in the dataset, each parameter participates in a multiply-add during the forward pass, and the backward pass costs roughly twice that. So the total work scales with the product of parameters and tokens."
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_42_estimate_flops_func",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Estimate Flops Func\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_42_estimate_flops_func.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_51"
   },
   "outputs": [],
   "source": [
    "def estimate_training_flops(num_params_billions, num_tokens_trillions):\n",
    "    \"\"\"\n",
    "    Estimate total FLOPs for training using the 6PD rule.\n",
    "\n",
    "    Args:\n",
    "        num_params_billions: Model parameters in billions\n",
    "        num_tokens_trillions: Training tokens in trillions\n",
    "\n",
    "    Returns:\n",
    "        Total FLOPs as a float\n",
    "    \"\"\"\n",
    "    P = num_params_billions * 1e9\n",
    "    D = num_tokens_trillions * 1e12\n",
    "    return 6 * P * D"
   ],
   "id": "cell_51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_43_flops_to_gpu_hours_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Flops To Gpu Hours Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_43_flops_to_gpu_hours_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_52"
   },
   "source": [
    "Now we need a way to convert raw FLOPs into wall-clock GPU-hours:"
   ],
   "id": "cell_52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_53"
   },
   "outputs": [],
   "source": [
    "def flops_to_gpu_hours(total_flops, gpu_tflops=312):\n",
    "    \"\"\"\n",
    "    Convert total FLOPs to GPU-hours.\n",
    "\n",
    "    Args:\n",
    "        total_flops: Total training FLOPs\n",
    "        gpu_tflops: GPU throughput in TFLOPS (A100 fp16 ~312 TFLOPS peak)\n",
    "\n",
    "    Returns:\n",
    "        GPU-hours (assuming ~50% utilization, which is typical)\n",
    "    \"\"\"\n",
    "    utilization = 0.50  # Typical MFU (Model FLOPS Utilization)\n",
    "    effective_tflops = gpu_tflops * utilization\n",
    "    effective_flops_per_second = effective_tflops * 1e12\n",
    "    seconds = total_flops / effective_flops_per_second\n",
    "    hours = seconds / 3600\n",
    "    return hours"
   ],
   "id": "cell_53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_44_llama3_compute_estimate",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Llama3 Compute Estimate\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_44_llama3_compute_estimate.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_54"
   },
   "source": [
    "Let us apply these functions to estimate how long it would take to train Llama 3 405B:"
   ],
   "id": "cell_54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_55"
   },
   "outputs": [],
   "source": [
    "# Llama 3 405B: 405B params, ~15T tokens\n",
    "flops_405b = estimate_training_flops(405, 15)\n",
    "gpu_hours_405b = flops_to_gpu_hours(flops_405b, gpu_tflops=312)\n",
    "\n",
    "print(f\"Llama 3 405B Training Estimate:\")\n",
    "print(f\"  Parameters:     405B\")\n",
    "print(f\"  Training tokens: 15T\")\n",
    "print(f\"  Total FLOPs:     {flops_405b:.2e}\")\n",
    "print(f\"  A100 GPU-hours:  {gpu_hours_405b:,.0f}\")\n",
    "print(f\"  On 1 GPU:        {gpu_hours_405b / (24*365):.0f} years\")\n",
    "print(f\"  On 16,384 GPUs:  {gpu_hours_405b / 16384 / 24:.0f} days\")"
   ],
   "id": "cell_55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_45_real_training_runs_table",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Real Training Runs Table\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_45_real_training_runs_table.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_56"
   },
   "outputs": [],
   "source": [
    "# Let us compute this for several real training runs\n",
    "\n",
    "training_runs = [\n",
    "    {'name': 'Llama 2 7B',      'params': 7,   'tokens': 2,    'gpus_used': 1024},\n",
    "    {'name': 'Llama 2 70B',     'params': 70,  'tokens': 2,    'gpus_used': 2048},\n",
    "    {'name': 'Llama 3 8B',      'params': 8,   'tokens': 15,   'gpus_used': 2048},\n",
    "    {'name': 'Llama 3 70B',     'params': 70,  'tokens': 15,   'gpus_used': 8192},\n",
    "    {'name': 'Llama 3 405B',    'params': 405, 'tokens': 15,   'gpus_used': 16384},\n",
    "    {'name': 'DeepSeek-V3',     'params': 671, 'tokens': 14.8, 'gpus_used': 2048},\n",
    "]\n",
    "\n",
    "print(f\"{'Model':<18} {'FLOPs':>12} {'1 GPU (yrs)':>12} {'Actual GPUs':>12} {'Est. Days':>10}\")\n",
    "print(\"â”€\" * 70)\n",
    "\n",
    "for run in training_runs:\n",
    "    flops = estimate_training_flops(run['params'], run['tokens'])\n",
    "    gpu_hours = flops_to_gpu_hours(flops)\n",
    "    years_1gpu = gpu_hours / (24 * 365)\n",
    "    days_actual = gpu_hours / run['gpus_used'] / 24\n",
    "\n",
    "    print(f\"{run['name']:<18} {flops:>10.1e}  {years_1gpu:>10.0f}  {run['gpus_used']:>10,}  {days_actual:>8.0f}\")\n",
    "\n",
    "print()\n",
    "print(\"DeepSeek-V3 uses MoE, so only ~37B params are active per token.\")\n",
    "print(\"   The actual FLOPs are much lower than the total parameter count suggests.\")"
   ],
   "id": "cell_56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_46_training_time_viz_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Training Time Viz Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_46_training_time_viz_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_57"
   },
   "source": [
    "Now let us visualize how training time drops as we add more GPUs. We set up the data first:"
   ],
   "id": "cell_57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_47_training_time_viz_code",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Training Time Viz Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_47_training_time_viz_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_58"
   },
   "outputs": [],
   "source": [
    "# ðŸ“Š ation: Training time on different GPU counts\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Focus on Llama 3 405B as our example\n",
    "flops_405b = estimate_training_flops(405, 15)\n",
    "gpu_counts = [1, 8, 64, 512, 2048, 8192, 16384, 32768]\n",
    "\n",
    "training_days = []\n",
    "for n_gpus in gpu_counts:\n",
    "    gpu_hours = flops_to_gpu_hours(flops_405b)\n",
    "    days = gpu_hours / n_gpus / 24\n",
    "    training_days.append(days)\n",
    "\n",
    "colors = ['#EF4444' if d > 365 else '#F59E0B' if d > 30 else '#10B981' for d in training_days]\n",
    "bars = ax.bar(range(len(gpu_counts)), training_days, color=colors, edgecolor='white')\n",
    "\n",
    "ax.set_xlabel('Number of A100 GPUs', fontsize=12)\n",
    "ax.set_ylabel('Training Time (days)', fontsize=12)\n",
    "ax.set_title('Llama 3 405B: Training Time vs. GPU Count\\n(15T tokens, 50% MFU)', fontsize=14)\n",
    "ax.set_xticks(range(len(gpu_counts)))\n",
    "ax.set_xticklabels([f'{g:,}' for g in gpu_counts], rotation=45)\n",
    "ax.set_yscale('log')\n",
    "ax.yaxis.set_major_formatter(ticker.ScalarFormatter())"
   ],
   "id": "cell_58"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_59"
   },
   "source": [
    "Add value labels and reference lines to the chart:"
   ],
   "id": "cell_59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_60"
   },
   "outputs": [],
   "source": [
    "# ðŸ“Š Add value labels on bars\n",
    "for i, (bar, days) in enumerate(zip(bars, training_days)):\n",
    "    if days > 365:\n",
    "        label = f'{days/365:.0f} yrs'\n",
    "    elif days > 1:\n",
    "        label = f'{days:.0f} days'\n",
    "    else:\n",
    "        label = f'{days*24:.0f} hrs'\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() * 1.2,\n",
    "            label, ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Reference lines\n",
    "ax.axhline(y=90, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.text(len(gpu_counts) - 0.5, 90, 'Meta target (~90 days)', ha='right', va='bottom',\n",
    "        fontsize=9, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Red = years, Yellow = months, Green = weeks or less.\")\n",
    "print(f\"At 16,384 GPUs (what Meta used), training takes ~{training_days[6]:.0f} days.\")"
   ],
   "id": "cell_60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_48_final_planner_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Narration: Final Planner Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_48_final_planner_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_61"
   },
   "source": [
    "## 7. Final Output: Interactive Memory & Compute Planner\n",
    "\n",
    "Now let us bring everything together into a single interactive tool. This is our culminating output â€” a comprehensive planner that takes any model configuration and produces a full visual report.\n",
    "\n",
    "We build the planner in stages. First, the function signature and memory/compute calculations:"
   ],
   "id": "cell_61"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_49_planner_function_calc",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Planner Function Calc\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_49_planner_function_calc.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_62"
   },
   "outputs": [],
   "source": [
    "def memory_compute_planner(\n",
    "    num_params_billions,\n",
    "    batch_size=1,\n",
    "    seq_len=2048,\n",
    "    hidden_dim=4096,\n",
    "    num_layers=32,\n",
    "    num_tokens_trillions=1.0,\n",
    "    gpu_name='A100',\n",
    "    gpu_memory_gb=80,\n",
    "    gpu_tflops=312,\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete memory and compute planner for LLM training.\n",
    "\n",
    "    Produces a comprehensive visual report showing:\n",
    "    - Stacked bar chart of memory breakdown\n",
    "    - Comparison against GPU capacity\n",
    "    - Compute time estimates\n",
    "    - Minimum GPU requirements\n",
    "    \"\"\"\n",
    "    # -- Compute memory breakdown --\n",
    "    mem = compute_full_memory(\n",
    "        num_params_billions, batch_size, seq_len, hidden_dim, num_layers\n",
    "    )\n",
    "\n",
    "    # -- Compute training FLOPs and time --\n",
    "    total_flops = estimate_training_flops(num_params_billions, num_tokens_trillions)\n",
    "    gpu_hours_1 = flops_to_gpu_hours(total_flops, gpu_tflops)\n",
    "    min_gpus = math.ceil(mem['total_gb'] / gpu_memory_gb)"
   ],
   "id": "cell_62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_50_planner_subplot_1",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Planner Subplot\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_50_planner_subplot_1.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_63"
   },
   "source": [
    "Next, we create the figure and draw the memory breakdown stacked bar (subplot 1):"
   ],
   "id": "cell_63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_64"
   },
   "outputs": [],
   "source": [
    "    # -- Figure with 4 subplots --\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    fig.suptitle(\n",
    "        f\"Training Planner: {num_params_billions}B Parameter Model\",\n",
    "        fontsize=18, fontweight='bold', y=0.98\n",
    "    )\n",
    "\n",
    "    # Subplot 1: Stacked bar â€” memory breakdown\n",
    "    ax1 = fig.add_subplot(2, 2, 1)\n",
    "    components = ['Weights\\n(fp16)', 'Gradients\\n(fp16)', 'Optimizer\\n(fp32)', 'Activations']\n",
    "    values = [mem['weights_gb'], mem['gradients_gb'], mem['optimizer_gb'], mem['activations_gb']]\n",
    "    colors = ['#3B82F6', '#10B981', '#F59E0B', '#8B5CF6']\n",
    "\n",
    "    bottom = 0\n",
    "    for comp, val, col in zip(components, values, colors):\n",
    "        ax1.bar('Memory', val, bottom=bottom, color=col,\n",
    "                label=f'{comp}: {val:.1f} GB', edgecolor='white', width=0.5)\n",
    "        bottom += val\n",
    "\n",
    "    ax1.axhline(y=gpu_memory_gb, color='red', linestyle='--',\n",
    "                linewidth=2.5, label=f'{gpu_name} ({gpu_memory_gb} GB)')\n",
    "    ax1.set_ylabel('Memory (GB)')\n",
    "    ax1.set_title('Memory Breakdown')\n",
    "    ax1.legend(loc='upper right', fontsize=9)\n",
    "\n",
    "    if mem['total_gb'] > gpu_memory_gb:\n",
    "        ax1.annotate(f\"Exceeds {gpu_name} by\\n{mem['total_gb'] - gpu_memory_gb:.0f} GB\",\n",
    "            xy=(0, gpu_memory_gb), xytext=(0.35, gpu_memory_gb * 0.6),\n",
    "            fontsize=10, color='red', fontweight='bold',\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=1.5))"
   ],
   "id": "cell_64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_51_planner_subplot_2",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Planner Subplot\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_51_planner_subplot_2.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_65"
   },
   "source": [
    "Subplot 2 -- pie chart showing memory proportions:"
   ],
   "id": "cell_65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_66"
   },
   "outputs": [],
   "source": [
    "    # Subplot 2: Pie chart â€” memory proportions\n",
    "    ax2 = fig.add_subplot(2, 2, 2)\n",
    "    pie_labels = ['Weights', 'Gradients', 'Optimizer', 'Activations']\n",
    "    pie_colors = ['#3B82F6', '#10B981', '#F59E0B', '#8B5CF6']\n",
    "    explode = (0, 0, 0.05, 0.02)\n",
    "\n",
    "    wedges, texts, autotexts = ax2.pie(\n",
    "        values, labels=pie_labels, autopct='%1.0f%%', colors=pie_colors,\n",
    "        explode=explode, startangle=90, textprops={'fontsize': 10}\n",
    "    )\n",
    "    ax2.set_title(f'Memory Proportions\\n(Total: {mem[\"total_gb\"]:.0f} GB)')"
   ],
   "id": "cell_66"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_52_planner_subplot_3",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Code Walkthrough: Planner Subplot\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_52_planner_subplot_3.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_67"
   },
   "source": [
    "Subplot 3 -- training time at different GPU counts, with color-coded bars:"
   ],
   "id": "cell_67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_68"
   },
   "outputs": [],
   "source": [
    "    # Subplot 3: Training time at different GPU counts\n",
    "    ax3 = fig.add_subplot(2, 2, 3)\n",
    "    gpu_counts = [1, 8, 64, 256, 1024, 4096, 16384]\n",
    "    days = [gpu_hours_1 / g / 24 for g in gpu_counts]\n",
    "\n",
    "    bar_colors = ['#EF4444' if d > 365 else '#F59E0B' if d > 30 else '#10B981' for d in days]\n",
    "    bars = ax3.bar(range(len(gpu_counts)), days, color=bar_colors, edgecolor='white')\n",
    "    ax3.set_xlabel(f'Number of {gpu_name} GPUs')\n",
    "    ax3.set_ylabel('Training Time (days)')\n",
    "    ax3.set_title(f'Training Time ({num_tokens_trillions}T tokens)')\n",
    "    ax3.set_xticks(range(len(gpu_counts)))\n",
    "    ax3.set_xticklabels([f'{g:,}' for g in gpu_counts], rotation=45, fontsize=9)\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.yaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "\n",
    "    for i, (bar, d) in enumerate(zip(bars, days)):\n",
    "        if d > 365:\n",
    "            label = f'{d/365:.0f}y'\n",
    "        elif d > 1:\n",
    "            label = f'{d:.0f}d'\n",
    "        else:\n",
    "            label = f'{d*24:.0f}h'\n",
    "        ax3.text(bar.get_x() + bar.get_width() / 2, bar.get_height() * 1.15,\n",
    "                 label, ha='center', va='bottom', fontsize=8, fontweight='bold')"
   ],
   "id": "cell_68"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_53_planner_subplot_4_render",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ What to Look For: Planner Subplot Render\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_53_planner_subplot_4_render.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_69"
   },
   "source": [
    "Subplot 4 -- summary text panel with all key numbers, then render the figure:"
   ],
   "id": "cell_69"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_70"
   },
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualization\n",
    "    # Subplot 4: Summary text panel\n",
    "    ax4 = fig.add_subplot(2, 2, 4)\n",
    "    ax4.axis('off')\n",
    "    s = 'â”€' * 40\n",
    "    summary_text = (\n",
    "        f\"MODEL CONFIGURATION\\n{s}\\n\"\n",
    "        f\"Parameters:        {num_params_billions}B\\n\"\n",
    "        f\"Hidden dim:        {hidden_dim}\\n\"\n",
    "        f\"Layers:            {num_layers}\\n\"\n",
    "        f\"Batch size:        {batch_size}\\n\"\n",
    "        f\"Sequence length:   {seq_len}\\n\"\n",
    "        f\"Training tokens:   {num_tokens_trillions}T\\n\\n\"\n",
    "        f\"MEMORY REQUIREMENTS\\n{s}\\n\"\n",
    "        f\"Weights:     {mem['weights_gb']:.1f} GB\\n\"\n",
    "        f\"Gradients:   {mem['gradients_gb']:.1f} GB\\n\"\n",
    "        f\"Optimizer:   {mem['optimizer_gb']:.1f} GB\\n\"\n",
    "        f\"Activations: {mem['activations_gb']:.1f} GB\\n\"\n",
    "        f\"TOTAL:       {mem['total_gb']:.1f} GB\\n\\n\"\n",
    "        f\"GPU REQUIREMENTS ({gpu_name})\\n{s}\\n\"\n",
    "        f\"GPU memory:  {gpu_memory_gb} GB | Min GPUs: {min_gpus}\\n\"\n",
    "        f\"FLOPs: {total_flops:.2e} | 1 GPU: {gpu_hours_1/(24*365):.0f} years\\n\"\n",
    "    )\n",
    "    ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes,\n",
    "             fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', facecolor='#F3F4F6', edgecolor='#D1D5DB'))\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "    print(f\"Your {num_params_billions}B model needs {mem['total_gb']:.0f} GB \"\n",
    "          f\"â€” that is {min_gpus} {gpu_name} GPUs at minimum.\")"
   ],
   "id": "cell_70"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_54_planner_7b_analysis",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Narration: Planner 7b Analysis\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_54_planner_7b_analysis.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_71"
   },
   "source": [
    "Now let us use our planner. First, let us analyze a 7B model â€” the \"smallest\" modern LLM:"
   ],
   "id": "cell_71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_72"
   },
   "outputs": [],
   "source": [
    "# 7B model analysis\n",
    "memory_compute_planner(\n",
    "    num_params_billions=7,\n",
    "    batch_size=1,\n",
    "    seq_len=2048,\n",
    "    hidden_dim=4096,\n",
    "    num_layers=32,\n",
    "    num_tokens_trillions=2.0,\n",
    "    gpu_name='A100',\n",
    "    gpu_memory_gb=80,\n",
    "    gpu_tflops=312,\n",
    ")"
   ],
   "id": "cell_72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_55_planner_405b_analysis",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Narration: Planner 405b Analysis\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_55_planner_405b_analysis.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_73"
   },
   "source": [
    "Now let us look at a truly large model â€” Llama 3 405B:"
   ],
   "id": "cell_73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_74"
   },
   "outputs": [],
   "source": [
    "# Llama 3 405B analysis\n",
    "memory_compute_planner(\n",
    "    num_params_billions=405,\n",
    "    batch_size=1,\n",
    "    seq_len=8192,\n",
    "    hidden_dim=16384,\n",
    "    num_layers=126,\n",
    "    num_tokens_trillions=15.0,\n",
    "    gpu_name='A100',\n",
    "    gpu_memory_gb=80,\n",
    "    gpu_tflops=312,\n",
    ")\n",
    "\n",
    "print(\"\\nYou now understand exactly why we need parallelism!\")\n",
    "print(\"A 405B model needs thousands of GPUs â€” both for memory and compute.\")\n",
    "print(\"In the next notebooks, we will learn exactly HOW to split the work.\")"
   ],
   "id": "cell_74"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_56_try_it_yourself",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Try It Yourself\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_56_try_it_yourself.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_75"
   },
   "source": [
    "### Try It Yourself: Custom Model\n",
    "\n",
    "Modify the parameters below to explore any model configuration you like. Try answering these questions:\n",
    "- How does doubling the sequence length affect memory?\n",
    "- What is the memory breakdown for a 1 trillion parameter model?\n",
    "- How many H100 GPUs would you need for DeepSeek-V3?"
   ],
   "id": "cell_75"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_76"
   },
   "outputs": [],
   "source": [
    "# -- Modify these parameters and re-run! --\n",
    "\n",
    "memory_compute_planner(\n",
    "    num_params_billions=70,       # Try: 7, 13, 70, 175, 405, 671, 1000\n",
    "    batch_size=4,                 # Try: 1, 2, 4, 8, 16\n",
    "    seq_len=4096,                 # Try: 2048, 4096, 8192, 32768, 131072\n",
    "    hidden_dim=8192,              # Must match model architecture\n",
    "    num_layers=80,                # Must match model architecture\n",
    "    num_tokens_trillions=15.0,    # Training dataset size\n",
    "    gpu_name='H100',              # Try: 'A100', 'H100'\n",
    "    gpu_memory_gb=80,             # A100: 80, H100: 80\n",
    "    gpu_tflops=990,               # A100: 312, H100: 990 (fp16 peak)\n",
    ")"
   ],
   "id": "cell_76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_57_reflection_questions",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Narration: Reflection Questions\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_57_reflection_questions.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_77"
   },
   "source": [
    "## 8. Reflection and Next Steps\n",
    "\n",
    "### ðŸ¤” Reflection Questions\n",
    "\n",
    "Take a moment to think about these questions. They test whether you have truly internalized the material.\n",
    "\n",
    "**1. Why do optimizer states dominate the memory budget?**\n",
    "\n",
    "Adam stores three fp32 buffers per parameter: a copy of the weights, the first moment (mean of gradients), and the second moment (mean of squared gradients). Each is 4 bytes, totaling 12 bytes â€” compared to just 2 bytes for the fp16 weight itself. This means 75% of model state memory is optimizer state. This insight is the foundation of ZeRO, which we will study in the next notebook.\n",
    "\n",
    "**2. If we doubled the sequence length, which memory component grows the most?**\n",
    "\n",
    "Activations. The model state (weights, gradients, optimizer) does not depend on sequence length at all â€” those are fixed by the number of parameters. But activation memory scales with sequence length (and even quadratically for attention scores). Doubling the sequence length roughly doubles (or more than doubles) activation memory.\n",
    "\n",
    "**3. Why can we not just use CPUs with more memory instead of GPUs?**\n",
    "\n",
    "CPUs have far less compute throughput. An A100 GPU delivers ~312 TFLOPS of fp16 compute, while a modern CPU manages maybe 1-5 TFLOPS. Even with unlimited memory, a CPU would take 60-300x longer to train the same model. The parallelism problem is fundamentally about both memory *and* compute â€” you need fast processors with sufficient total memory, and that means many GPUs."
   ],
   "id": "cell_77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_58_optional_challenges",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Listen: Optional Challenges\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_58_optional_challenges.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_78"
   },
   "source": [
    "### Optional Challenges\n",
    "\n",
    "If you want to go deeper, try these extensions:\n",
    "\n",
    "**Challenge 1: Add fp8 Training Support**\n",
    "\n",
    "Modify the `compute_full_memory` function to support fp8 (1 byte per weight/gradient). How much memory does this save? Note that optimizer states remain in fp32.\n",
    "\n",
    "**Challenge 2: MoE Memory Accounting**\n",
    "\n",
    "For a Mixture of Experts model like DeepSeek-V3 (671B total params, 37B active per token, 256 experts), only the active parameters contribute to compute FLOPs, but *all* parameters need memory. Modify the planner to separately report \"active\" vs \"total\" parameters and show how this affects the compute vs. memory tradeoff.\n",
    "\n",
    "**Challenge 3: Gradient Checkpointing**\n",
    "\n",
    "Gradient checkpointing (also called activation recomputation) trades compute for memory: instead of storing all activations, we only store activations at \"checkpoint\" layers and recompute the rest during the backward pass. This typically reduces activation memory by $\\sqrt{L}$ (where $L$ is the number of layers) at the cost of ~33% more compute. Add this option to your planner."
   ],
   "id": "cell_78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_59_challenge_1_fp8",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Challenge Fp8\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_59_challenge_1_fp8.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_79"
   },
   "outputs": [],
   "source": [
    "# Challenge 1 starter code â€” fp8 training support\n",
    "\n",
    "def compute_memory_fp8(num_params_billions, batch_size=1, seq_len=2048,\n",
    "                        hidden_dim=4096, num_layers=32):\n",
    "    \"\"\"\n",
    "    Compute memory for fp8 training.\n",
    "\n",
    "    In fp8 training:\n",
    "    - Weights: 1 byte per parameter (fp8)\n",
    "    - Gradients: 1 byte per parameter (fp8)\n",
    "    - Optimizer states: STILL 12 bytes per parameter (Adam stays in fp32)\n",
    "    - Activations: 1 byte per element (fp8)\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Implement fp8 memory calculation\n",
    "    # Compare with fp16 â€” how much do you save?\n",
    "    # ==============================\n",
    "    pass  # YOUR CODE HERE"
   ],
   "id": "cell_79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_60_challenge_2_moe",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Before You Start: Challenge Moe\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_60_challenge_2_moe.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_80"
   },
   "outputs": [],
   "source": [
    "# Challenge 2 starter code â€” MoE memory accounting\n",
    "\n",
    "def compute_memory_moe(total_params_B, active_params_B, num_experts,\n",
    "                        top_k, tokens_T, gpu_tflops=312):\n",
    "    \"\"\"\n",
    "    For MoE models, separate memory (all params) from compute (active params).\n",
    "\n",
    "    DeepSeek-V3: total=671B, active=37B, experts=256, top_k=8\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Memory uses total_params_B (all experts stored)\n",
    "    # Compute uses active_params_B (only top_k experts active)\n",
    "    # ==============================\n",
    "    pass  # YOUR CODE HERE"
   ],
   "id": "cell_80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_01_61_closing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title ðŸŽ§ Wrap-Up: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_61_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_81"
   },
   "outputs": [],
   "source": [
    "print(\"Complete the challenges above to deepen your understanding!\")\n",
    "print()\n",
    "print(\"â”€\" * 60)\n",
    "print(\"Next up: Notebook 2 â€” Data Parallelism (\\\"Hire More Chefs\\\")\")\n",
    "print(\"We will implement AllReduce from scratch and understand ZeRO.\")\n",
    "print(\"â”€\" * 60)"
   ],
   "id": "cell_81"
  }
 ]
}