{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "The 5D Parallelism Grid ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_00_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1udrvT-zDGGZC2pSzOCozq04RTKGDagCg\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/06_00_intro.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_01_setup",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Setup\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_01_setup.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 5D Grid: Composing All Parallelism Dimensions\n",
    "\n",
    "*Part 6 of 6 in the Vizuara series on 5D Parallelism from Scratch*\n",
    "*Estimated time: 40 minutes*\n",
    "\n",
    "In this final notebook, we bring everything together. We have learned five parallelism strategies individually ‚Äî now we will see how they **compose** into a single, unified system that spans thousands of GPUs. By the end, you will build an interactive 5D parallelism planner that recommends configurations for real-world models like Llama 3 405B and DeepSeek-V3."
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chatbot"
    ]
   },
   "source": [
    "# ü§ñ AI Teaching Assistant\n",
    "\n",
    "Need help with this notebook? Open the **AI Teaching Assistant** ‚Äî it has already read this entire notebook and can help with concepts, code, and exercises.\n",
    "\n",
    "**[üëâ Open AI Teaching Assistant](https://pods.vizuara.ai/courses/5d-parallelism-from-scratch/practice/6/assistant)**\n",
    "\n",
    "*Tip: Open it in a separate tab and work through this notebook side-by-side.*\n"
   ],
   "id": "vizuara_chatbot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup ‚Äî install and import everything we need\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Setup complete! Ready to compose the 5D grid.\")"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_02_why_matters",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_02_why_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "In practice, no single parallelism strategy is enough to train a frontier model. Real training runs **compose all five dimensions** simultaneously:\n",
    "\n",
    "| Model | Total GPUs | How They Got There |\n",
    "|-------|------------|-------------------|\n",
    "| **Llama 3 405B** | 16,384 H100s | 128 DP x 8 TP x 16 PP |\n",
    "| **DeepSeek-V3** | 2,048 H800s | Dense + 256 MoE experts |\n",
    "| **GPT-4** (estimated) | ~10,000-25,000 | Unknown exact config |\n",
    "| **Gemini Ultra** (estimated) | ~10,000+ TPUs | Multi-dimensional parallelism |\n",
    "\n",
    "Understanding how these dimensions compose is what separates a practitioner from someone who just read a blog post. Let us build that understanding from the ground up."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us start with a concrete motivating example:\n",
    "# WHY does Llama 3 405B need 16,384 GPUs?\n",
    "\n",
    "model_params_B = 405  # billion parameters\n",
    "bytes_per_param_training = 16  # mixed precision Adam: 2 + 2 + 12 bytes\n",
    "\n",
    "total_memory_GB = model_params_B * bytes_per_param_training\n",
    "single_gpu_memory_GB = 80  # H100 80GB\n",
    "\n",
    "gpus_for_weights_alone = total_memory_GB / single_gpu_memory_GB\n",
    "\n",
    "print(f\"Llama 3 405B Training Memory Requirements\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"Parameters:              {model_params_B}B\")\n",
    "print(f\"Memory per param:        {bytes_per_param_training} bytes\")\n",
    "print(f\"Total training memory:   {total_memory_GB:,} GB\")\n",
    "print(f\"Single H100 memory:      {single_gpu_memory_GB} GB\")\n",
    "print(f\"Minimum GPUs (weights):  {gpus_for_weights_alone:.0f}\")\n",
    "print(f\"\\nBut with activations, micro-batches, and overhead,\")\n",
    "print(f\"Meta used: 16,384 GPUs\")\n",
    "print(f\"That is {16384 * single_gpu_memory_GB / 1000:.0f} TB of aggregate GPU memory!\")"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_03_mega_kitchen",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Mega Kitchen\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_03_mega_kitchen.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition: The Mega-Kitchen\n",
    "\n",
    "Let us return to our restaurant analogy one final time and see how all five strategies work together.\n",
    "\n",
    "Imagine a **mega-restaurant** that needs to serve 16,000 plates per hour. Here is how they organize 16,384 chefs:\n",
    "\n",
    "- **Groups of 8 chefs** share one large table. Each chef handles a different part of the recipe ‚Äî one chops, one sautes, one seasons. They pass ingredients constantly across the table. This is **Tensor Parallelism** ‚Äî splitting each layer across 8 GPUs connected by NVLink (the \"shared table\").\n",
    "\n",
    "- **16 tables** are arranged in an **assembly line**. Table 1 handles appetizers, Table 2 handles the first course, Table 3 the main, and so on. Each table passes its finished dish to the next. This is **Pipeline Parallelism** ‚Äî splitting the model's depth across nodes connected by InfiniBand.\n",
    "\n",
    "- **128 identical assembly lines** operate in parallel, each working on different customer orders. At the end of a round, they share notes on what they learned. This is **Data Parallelism** ‚Äî processing different data across the entire cluster.\n",
    "\n",
    "- **Within each table of 8**, the chefs also split long order lists between them. Chef A handles items 1-500, Chef B handles items 501-1000, and so on. This is **Sequence Parallelism** ‚Äî it shares the same group as TP.\n",
    "\n",
    "- Some tables have **specialist chefs** for different cuisines ‚Äî Italian, Indian, Japanese. A host routes each order to the right specialist. This is **Expert Parallelism** ‚Äî distributing MoE experts with All-to-All communication."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_04_mega_kitchen_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Mega Kitchen Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_04_mega_kitchen_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualize this mega-kitchen hierarchy. We will draw the cluster structure showing DP replicas, PP stages, and TP groups as nested rectangles."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure and draw the outer cluster boundary (DP level)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "ax.set_title(\"The Mega-Kitchen: How 16,384 Chefs Organize\",\n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "cluster_rect = plt.Rectangle((0.5, 0.3), 13, 9.2, linewidth=2.5,\n",
    "                               edgecolor='#2196F3', facecolor='#E3F2FD',\n",
    "                               linestyle='--', alpha=0.4)\n",
    "ax.add_patch(cluster_rect)\n",
    "ax.text(7, 9.2, \"Data Parallelism: 128 identical assembly lines\",\n",
    "        ha='center', fontsize=12, fontweight='bold', color='#1565C0')"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we draw the individual assembly lines (PP stages) and their TP node groups."
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw 3 assembly lines (representing 128)\n",
    "for i, x_start in enumerate([1.0, 5.0, 9.0]):\n",
    "    label = f\"Line {i+1}\" if i < 2 else \"Line 128\"\n",
    "    pp_rect = plt.Rectangle((x_start, 1.0), 3.5, 7.0, linewidth=2,\n",
    "                              edgecolor='#FF9800', facecolor='#FFF3E0', alpha=0.5)\n",
    "    ax.add_patch(pp_rect)\n",
    "    # Draw 4 nodes (representing 16 PP stages)\n",
    "    for j, y_pos in enumerate([1.5, 3.2, 4.9, 6.6]):\n",
    "        stage_label = f\"Stage {j+1}\" if j < 3 else \"Stage 16\"\n",
    "        node_rect = plt.Rectangle((x_start + 0.3, y_pos), 2.9, 1.3,\n",
    "                                   linewidth=1.5, edgecolor='#4CAF50',\n",
    "                                   facecolor='#E8F5E9', alpha=0.7)\n",
    "        ax.add_patch(node_rect)\n",
    "        ax.text(x_start + 1.75, y_pos + 0.65, f\"8 GPUs\\n(TP group)\",\n",
    "                ha='center', va='center', fontsize=7, color='#2E7D32')\n",
    "        ax.text(x_start + 1.75, y_pos + 1.15, stage_label,\n",
    "                ha='center', fontsize=7, fontweight='bold', color='#1B5E20')\n",
    "        if j < 3:  # Arrows between stages\n",
    "            ax.annotate('', xy=(x_start + 1.75, y_pos + 1.3),\n",
    "                       xytext=(x_start + 1.75, y_pos + 1.6),\n",
    "                       arrowprops=dict(arrowstyle='->', color='#FF9800', lw=1.5))\n",
    "    ax.text(x_start + 1.75, 8.3, label, ha='center', fontsize=10,\n",
    "            fontweight='bold', color='#E65100')\n",
    "    if i == 1:  # Ellipsis between line 2 and 128\n",
    "        ax.text(7.5, 4.5, \"...\", ha='center', fontsize=24,\n",
    "                fontweight='bold', color='#666')"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, add the legend and display the complete mega-kitchen diagram."
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legend\n",
    "legend_items = [\n",
    "    (\"Data Parallelism (128x)\", '#2196F3'),\n",
    "    (\"Pipeline Parallelism (16x)\", '#FF9800'),\n",
    "    (\"Tensor Parallelism (8x per node)\", '#4CAF50'),\n",
    "]\n",
    "for idx, (text, color) in enumerate(legend_items):\n",
    "    ax.plot([], [], 's', color=color, markersize=10, label=text)\n",
    "ax.legend(loc='lower center', ncol=3, fontsize=9, framealpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "# üìä Visualization: display the chart\n",
    "plt.show()"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_05_math_composition",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Math Composition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_05_math_composition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics of Composition\n",
    "\n",
    "The total number of GPUs required is:\n",
    "\n",
    "$$N_{\\text{total}} = N_{\\text{DP}} \\times N_{\\text{TP}} \\times N_{\\text{PP}} \\times N_{\\text{EP}}$$\n",
    "\n",
    "Sequence Parallelism shares the same GPU group as Tensor Parallelism (both operate within a node), so it does **not** add an independent dimension to the GPU count.\n",
    "\n",
    "Let us plug in some real numbers.\n",
    "\n",
    "**Llama 3 405B (Meta, 2024):**\n",
    "- $N_{\\text{TP}} = 8$ ‚Äî one full node of 8 H100 GPUs, connected by NVLink\n",
    "- $N_{\\text{PP}} = 16$ ‚Äî model split across 16 pipeline stages\n",
    "- $N_{\\text{DP}} = 128$ ‚Äî 128 data-parallel replicas\n",
    "- $N_{\\text{SP}} = 8$ ‚Äî shares TP group (within the same node)\n",
    "- $N_{\\text{EP}} = 1$ ‚Äî Llama 3 is a dense model (not MoE)\n",
    "- $N_{\\text{total}} = 128 \\times 8 \\times 16 \\times 1 = 16{,}384$ GPUs\n",
    "\n",
    "**DeepSeek-V3 (DeepSeek, 2024):**\n",
    "- $N_{\\text{TP}} = 1$ ‚Äî no tensor parallelism (clever design choice)\n",
    "- $N_{\\text{PP}} = 8$ ‚Äî 8 pipeline stages\n",
    "- $N_{\\text{EP}} = 32$ ‚Äî 256 experts distributed across 32 EP groups (8 experts per GPU)\n",
    "- $N_{\\text{DP}} = 8$ ‚Äî 8 data-parallel replicas\n",
    "- $N_{\\text{total}} = 8 \\times 1 \\times 8 \\times 32 = 2{,}048$ GPUs"
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a `ParallelismConfig` dataclass to hold the parallelism degrees for any model, along with GPU metadata."
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the core configuration dataclass\n",
    "@dataclass\n",
    "class ParallelismConfig:\n",
    "    \"\"\"Configuration for a 5D parallelism setup.\"\"\"\n",
    "    name: str\n",
    "    n_dp: int        # Data Parallelism degree\n",
    "    n_tp: int        # Tensor Parallelism degree\n",
    "    n_pp: int        # Pipeline Parallelism degree\n",
    "    n_sp: int        # Sequence Parallelism degree (shares TP group)\n",
    "    n_ep: int        # Expert Parallelism degree\n",
    "    gpu_type: str\n",
    "    gpu_memory_gb: int\n",
    "\n",
    "    @property\n",
    "    def total_gpus(self) -> int:\n",
    "        return self.n_dp * self.n_tp * self.n_pp * self.n_ep\n",
    "\n",
    "    @property\n",
    "    def total_nodes(self) -> int:\n",
    "        gpus_per_node = self.n_tp  # TP group = one node\n",
    "        return self.total_gpus // gpus_per_node"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us instantiate the real-world configurations and verify the GPU counts match published numbers."
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world configurations\n",
    "llama3_405b = ParallelismConfig(\n",
    "    name=\"Llama 3 405B\",\n",
    "    n_dp=128, n_tp=8, n_pp=16, n_sp=8, n_ep=1,\n",
    "    gpu_type=\"H100\", gpu_memory_gb=80\n",
    ")\n",
    "\n",
    "deepseek_v3 = ParallelismConfig(\n",
    "    name=\"DeepSeek-V3\",\n",
    "    n_dp=8, n_tp=1, n_pp=8, n_sp=1, n_ep=32,\n",
    "    gpu_type=\"H800\", gpu_memory_gb=80\n",
    ")\n",
    "\n",
    "for config in [llama3_405b, deepseek_v3]:\n",
    "    print(f\"\\n{'=' * 55}\")\n",
    "    print(f\"  {config.name}\")\n",
    "    print(f\"{'=' * 55}\")\n",
    "    print(f\"  DP = {config.n_dp:>4}  (data-parallel replicas)\")\n",
    "    print(f\"  TP = {config.n_tp:>4}  (tensor-parallel within node)\")\n",
    "    print(f\"  PP = {config.n_pp:>4}  (pipeline stages)\")\n",
    "    print(f\"  SP = {config.n_sp:>4}  (sequence-parallel, shares TP)\")\n",
    "    print(f\"  EP = {config.n_ep:>4}  (expert-parallel groups)\")\n",
    "    formula = (f\"  {config.n_dp} x {config.n_tp} x \"\n",
    "               f\"{config.n_pp} x {config.n_ep}\")\n",
    "    print(f\"  Total = {formula} = {config.total_gpus:,} GPUs\")\n",
    "    print(f\"  Nodes = {config.total_nodes:,} \"\n",
    "          f\"({config.n_tp} {config.gpu_type}s per node)\")"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_06_comm_hierarchy",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Comm Hierarchy\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_06_comm_hierarchy.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Communication Hierarchy\n",
    "\n",
    "Not all communication is equal. The key insight behind 5D parallelism is to **match each strategy to the right level of the hardware hierarchy**:\n",
    "\n",
    "| Parallelism | Communication Pattern | Placement | Bandwidth | Why Here? |\n",
    "|-------------|----------------------|-----------|-----------|-----------|\n",
    "| **TP** | AllReduce every layer | Within node | NVLink: 900 GB/s | Highest frequency ‚Äî needs fastest link |\n",
    "| **SP** | Reduce-Scatter / All-Gather | Within node | NVLink: 900 GB/s | Shares TP group |\n",
    "| **PP** | Point-to-point per micro-batch | Across nearby nodes | InfiniBand: 400 Gb/s (~50 GB/s) | Moderate frequency |\n",
    "| **EP** | All-to-All dispatch + collect | Flexible | InfiniBand: ~50 GB/s | Carefully placed to minimize hops |\n",
    "| **DP** | AllReduce once per step | Entire cluster | Ethernet/IB: varies | Lowest frequency ‚Äî can tolerate latency |\n",
    "\n",
    "Let us plug in some numbers to see how communication volume differs across dimensions."
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining the key model and training hyperparameters for Llama 3 405B, then compute per-dimension communication volumes."
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communication volume comparison for Llama 3 405B\n",
    "# These are approximate per-step volumes\n",
    "\n",
    "hidden_dim = 16384       # Llama 3 405B hidden dimension\n",
    "num_layers = 126         # Llama 3 405B layers\n",
    "num_heads = 128          # attention heads\n",
    "seq_len = 8192           # training sequence length\n",
    "micro_batch_size = 1     # per micro-batch\n",
    "global_batch_size = 1024 # total batch size (in sequences)\n",
    "bytes_per_element = 2    # fp16 / bf16\n",
    "total_params_B = 405     # billion parameters\n",
    "\n",
    "n_tp = 8\n",
    "n_pp = 16\n",
    "n_dp = 128"
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these parameters set, we can compute the communication volume for TP, PP, and DP."
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TP: AllReduce per layer (2x for forward + backward, across hidden dim)\n",
    "# Each AllReduce sends 2 * data_size (reduce-scatter + all-gather)\n",
    "tp_per_layer = (2 * micro_batch_size * seq_len * hidden_dim\n",
    "                * bytes_per_element)\n",
    "tp_total = tp_per_layer * (num_layers // n_pp) * 2  # fwd + bwd\n",
    "tp_total_GB = tp_total / (1024**3)\n",
    "\n",
    "# PP: Point-to-point at stage boundaries (activation tensor)\n",
    "pp_per_microbatch = (micro_batch_size * seq_len * hidden_dim\n",
    "                     * bytes_per_element)\n",
    "num_microbatches = global_batch_size // (n_dp * micro_batch_size)\n",
    "pp_total = pp_per_microbatch * num_microbatches * 2  # fwd + bwd\n",
    "pp_total_GB = pp_total / (1024**3)\n",
    "\n",
    "# DP: AllReduce gradients once per step\n",
    "dp_total = 2 * (total_params_B * 1e9 / (n_tp * n_pp)) * bytes_per_element\n",
    "dp_total_GB = dp_total / (1024**3)\n",
    "\n",
    "print(\"Communication Volume Per Training Step (Llama 3 405B)\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"  TP (AllReduce per layer):   {tp_total_GB:>8.2f} GB\")\n",
    "print(f\"  PP (point-to-point):        {pp_total_GB:>8.2f} GB\")\n",
    "print(f\"  DP (AllReduce gradients):   {dp_total_GB:>8.2f} GB\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_07_comm_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Comm Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_07_comm_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualize the communication hierarchy with two charts: available bandwidth per dimension (left) and communication frequency per step (right)."
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Communication hierarchy ‚Äî bandwidth comparison\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left panel: bandwidth by parallelism dimension\n",
    "dimensions = ['TP\\n(NVLink)', 'SP\\n(NVLink)', 'PP\\n(InfiniBand)',\n",
    "              'EP\\n(IB/Ethernet)', 'DP\\n(Cluster)']\n",
    "bandwidths = [900, 900, 50, 50, 25]  # GB/s (bidirectional effective)\n",
    "colors = ['#4CAF50', '#8BC34A', '#FF9800', '#9C27B0', '#2196F3']\n",
    "\n",
    "bars = axes[0].bar(dimensions, bandwidths, color=colors, edgecolor='white',\n",
    "                   linewidth=1.5, width=0.6)\n",
    "axes[0].set_ylabel(\"Effective Bandwidth (GB/s)\", fontsize=11)\n",
    "axes[0].set_title(\"Available Bandwidth by Dimension\", fontsize=13,\n",
    "                  fontweight='bold')\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].set_ylim(10, 2000)\n",
    "\n",
    "for bar, bw in zip(bars, bandwidths):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.15,\n",
    "                f'{bw} GB/s', ha='center', va='bottom', fontsize=9,\n",
    "                fontweight='bold')"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right panel shows how frequently each dimension communicates during a single training step."
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right panel: communication frequency\n",
    "freq_labels = ['TP', 'SP', 'PP', 'EP', 'DP']\n",
    "freq_values = [num_layers // n_pp, num_layers // n_pp,\n",
    "               num_microbatches, num_microbatches, 1]\n",
    "freq_desc = ['per layer', 'per layer', 'per micro-batch',\n",
    "             'per micro-batch', 'per step']\n",
    "\n",
    "bars2 = axes[1].barh(freq_labels, freq_values, color=colors,\n",
    "                     edgecolor='white', linewidth=1.5, height=0.5)\n",
    "axes[1].set_xlabel(\"Communication Events per Step\", fontsize=11)\n",
    "axes[1].set_title(\"Communication Frequency by Dimension\", fontsize=13,\n",
    "                  fontweight='bold')\n",
    "axes[1].set_xscale('log')\n",
    "\n",
    "for bar, val, desc in zip(bars2, freq_values, freq_desc):\n",
    "    axes[1].text(bar.get_width() * 1.3, bar.get_y() + bar.get_height()/2,\n",
    "                f'{val}x ({desc})', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "# üìä Visualization: display the chart\n",
    "plt.show()"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_08_planner_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Planner Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_08_planner_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Let Us Build It ‚Äî The 5D Configuration Planner\n",
    "\n",
    "Now we will build the full planner step by step. We start with the foundational data structures.\n",
    "\n",
    "### 5.1 GPU and Node Specifications"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define dataclasses for GPU hardware specs and interconnect specs, then populate them with real-world values for A100, H100, and H800 GPUs."
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPUSpec:\n",
    "    \"\"\"Hardware specifications for a GPU type.\"\"\"\n",
    "    name: str\n",
    "    memory_gb: float\n",
    "    peak_tflops_fp16: float  # fp16/bf16 peak TFLOPS\n",
    "    nvlink_bw_gbps: float    # NVLink bandwidth (GB/s, bidirectional)\n",
    "    gpus_per_node: int\n",
    "\n",
    "@dataclass\n",
    "class InterconnectSpec:\n",
    "    \"\"\"Interconnect specifications between nodes.\"\"\"\n",
    "    name: str\n",
    "    bandwidth_gbps: float  # GB/s per link\n",
    "    latency_us: float      # microseconds"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we populate the hardware catalog with real specs and print a summary table."
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define real GPU specs\n",
    "GPU_SPECS = {\n",
    "    \"A100_80GB\": GPUSpec(\"A100 80GB\", 80.0, 312.0, 600.0, 8),\n",
    "    \"H100_80GB\": GPUSpec(\"H100 80GB\", 80.0, 989.0, 900.0, 8),\n",
    "    \"H800_80GB\": GPUSpec(\"H800 80GB\", 80.0, 989.0, 400.0, 8),\n",
    "}\n",
    "\n",
    "INTERCONNECTS = {\n",
    "    \"InfiniBand_HDR\": InterconnectSpec(\"InfiniBand HDR\", 25.0, 1.0),\n",
    "    \"InfiniBand_NDR\": InterconnectSpec(\"InfiniBand NDR\", 50.0, 0.8),\n",
    "    \"Ethernet_100G\":  InterconnectSpec(\"100G Ethernet\", 12.5, 5.0),\n",
    "}\n",
    "\n",
    "# Display available hardware\n",
    "print(\"Available GPU Types:\")\n",
    "print(f\"{'GPU':<15} {'Memory':<10} {'FP16 TFLOPS':<13} \"\n",
    "      f\"{'NVLink BW':<12} {'GPUs/Node'}\")\n",
    "print(\"-\" * 62)\n",
    "for key, gpu in GPU_SPECS.items():\n",
    "    print(f\"{gpu.name:<15} {gpu.memory_gb:<10.0f} \"\n",
    "          f\"{gpu.peak_tflops_fp16:<13.0f} \"\n",
    "          f\"{gpu.nvlink_bw_gbps:<12.0f} {gpu.gpus_per_node}\")"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_09_memory_calculator",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Memory Calculator\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_09_memory_calculator.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Memory Budget Calculator\n",
    "\n",
    "This is the heart of the planner. Given a model configuration and a parallelism configuration, we compute how much memory each GPU needs.\n",
    "\n",
    "The memory on each GPU consists of:\n",
    "- **Weights**: $\\frac{\\text{total\\_params}}{N_{\\text{TP}} \\times N_{\\text{PP}}} \\times \\text{bytes\\_per\\_param}$\n",
    "- **Gradients**: same size as weights\n",
    "- **Optimizer states**: $3 \\times$ weights in fp32 (Adam: first moment, second moment, fp32 copy)\n",
    "- **Activations**: depends on batch size, sequence length, hidden dimension, and checkpointing\n",
    "\n",
    "With ZeRO Stage 1, optimizer states are sharded across $N_{\\text{DP}}$."
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the `ModelConfig` dataclass that holds all architecture parameters for any model."
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Model architecture configuration.\"\"\"\n",
    "    name: str\n",
    "    total_params_B: float    # billions of parameters\n",
    "    num_layers: int\n",
    "    hidden_dim: int\n",
    "    num_heads: int\n",
    "    num_experts: int         # 1 for dense models\n",
    "    expert_params_B: float   # params per expert (for MoE)\n",
    "    seq_len: int\n",
    "    vocab_size: int"
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the core function: `compute_memory_per_gpu` calculates the weight, gradient, optimizer, and activation memory given a model and parallelism configuration."
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_memory_per_gpu(\n",
    "    model: ModelConfig,\n",
    "    n_dp: int, n_tp: int, n_pp: int, n_ep: int,\n",
    "    micro_batch_size: int = 1,\n",
    "    zero_stage: int = 1,\n",
    "    activation_checkpointing: bool = True\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute per-GPU memory breakdown in GB.\n",
    "    Returns a dict with weight, gradient, optimizer, activation memory.\n",
    "    \"\"\"\n",
    "    # Total params on this GPU (sharded by TP and PP)\n",
    "    if model.num_experts > 1:\n",
    "        # MoE: shared params split by TP*PP, expert params split by EP\n",
    "        shared_params = model.total_params_B - (model.expert_params_B\n",
    "                        * model.num_experts)\n",
    "        shared_per_gpu = shared_params * 1e9 / (n_tp * n_pp)\n",
    "        experts_per_gpu = model.num_experts // n_ep\n",
    "        expert_per_gpu = model.expert_params_B * 1e9 * experts_per_gpu\n",
    "        params_per_gpu = shared_per_gpu + expert_per_gpu\n",
    "    else:\n",
    "        params_per_gpu = model.total_params_B * 1e9 / (n_tp * n_pp)\n",
    "\n",
    "    # Weight memory (bf16 = 2 bytes)\n",
    "    weight_mem = params_per_gpu * 2 / (1024**3)\n",
    "\n",
    "    # Gradient memory (bf16 = 2 bytes)\n",
    "    grad_mem = params_per_gpu * 2 / (1024**3)"
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue the memory computation with optimizer states (applying ZeRO sharding) and activation memory (with optional checkpointing)."
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Optimizer memory (Adam: fp32 weights + fp32 m + fp32 v = 12 bytes)\n",
    "    optimizer_mem = params_per_gpu * 12 / (1024**3)\n",
    "\n",
    "    # Apply ZeRO sharding\n",
    "    if zero_stage >= 1:\n",
    "        optimizer_mem /= n_dp\n",
    "    if zero_stage >= 2:\n",
    "        grad_mem /= n_dp\n",
    "    if zero_stage >= 3:\n",
    "        weight_mem /= n_dp\n",
    "\n",
    "    # Activation memory (simplified estimate)\n",
    "    layers_per_gpu = model.num_layers // n_pp\n",
    "    seq_per_gpu = model.seq_len // n_tp  # SP shares TP group\n",
    "    act_per_layer = (10 * micro_batch_size * seq_per_gpu\n",
    "                     * model.hidden_dim * 2 / (1024**3))\n",
    "    if activation_checkpointing:\n",
    "        act_mem = act_per_layer * (layers_per_gpu ** 0.5)\n",
    "    else:\n",
    "        act_mem = act_per_layer * layers_per_gpu"
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemble the memory breakdown into a dictionary and return it."
   ],
   "id": "cell_38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    return {\n",
    "        \"weights\": weight_mem,\n",
    "        \"gradients\": grad_mem,\n",
    "        \"optimizer\": optimizer_mem,\n",
    "        \"activations\": act_mem,\n",
    "        \"total\": weight_mem + grad_mem + optimizer_mem + act_mem,\n",
    "        \"params_per_gpu_B\": params_per_gpu / 1e9,\n",
    "    }"
   ],
   "id": "cell_39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_10_memory_results",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Memory Results\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_10_memory_results.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the two real-world model configurations and compute the memory breakdown for Llama 3 405B."
   ],
   "id": "cell_40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define real model configurations\n",
    "llama3_model = ModelConfig(\n",
    "    name=\"Llama 3 405B\",\n",
    "    total_params_B=405.0,\n",
    "    num_layers=126,\n",
    "    hidden_dim=16384,\n",
    "    num_heads=128,\n",
    "    num_experts=1,\n",
    "    expert_params_B=0.0,\n",
    "    seq_len=8192,\n",
    "    vocab_size=128256\n",
    ")\n",
    "\n",
    "deepseek_v3_model = ModelConfig(\n",
    "    name=\"DeepSeek-V3\",\n",
    "    total_params_B=671.0,\n",
    "    num_layers=61,\n",
    "    hidden_dim=7168,\n",
    "    num_heads=128,\n",
    "    num_experts=256,\n",
    "    expert_params_B=1.6,    # ~1.6B params per expert\n",
    "    seq_len=4096,\n",
    "    vocab_size=129280\n",
    ")"
   ],
   "id": "cell_41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the memory calculator on Llama 3 405B and check whether the result fits in an H100 GPU."
   ],
   "id": "cell_42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute memory for Llama 3 405B\n",
    "mem_llama = compute_memory_per_gpu(\n",
    "    llama3_model, n_dp=128, n_tp=8, n_pp=16, n_ep=1,\n",
    "    micro_batch_size=1, zero_stage=1\n",
    ")\n",
    "\n",
    "print(\"Memory Per GPU ‚Äî Llama 3 405B (DP=128, TP=8, PP=16)\")\n",
    "print(\"=\" * 50)\n",
    "for key, val in mem_llama.items():\n",
    "    if key == \"params_per_gpu_B\":\n",
    "        print(f\"  Params per GPU:     {val:.2f} B\")\n",
    "    else:\n",
    "        print(f\"  {key:<20s} {val:>8.2f} GB\")\n",
    "print(f\"\\n  H100 capacity:         80.00 GB\")\n",
    "print(f\"  Fits? {'Yes' if mem_llama['total'] < 80 else 'No'} \"\n",
    "      f\"({mem_llama['total']/80*100:.0f}% utilized)\")"
   ],
   "id": "cell_43"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also compute DeepSeek-V3's memory and compare both models side by side with pie charts showing per-GPU memory composition."
   ],
   "id": "cell_44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Memory breakdown pie charts for both models\n",
    "\n",
    "mem_ds = compute_memory_per_gpu(\n",
    "    deepseek_v3_model, n_dp=8, n_tp=1, n_pp=8, n_ep=32,\n",
    "    micro_batch_size=1, zero_stage=1\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "categories = ['weights', 'gradients', 'optimizer', 'activations']\n",
    "pie_colors = ['#2196F3', '#4CAF50', '#FF9800', '#9C27B0']\n",
    "\n",
    "for ax, mem, title in zip(axes, [mem_llama, mem_ds],\n",
    "                          [\"Llama 3 405B\\n(DP=128, TP=8, PP=16)\",\n",
    "                           \"DeepSeek-V3\\n(DP=8, TP=1, PP=8, EP=32)\"]):\n",
    "    sizes = [mem[c] for c in categories]\n",
    "    total = sum(sizes)\n",
    "    labels = [f\"{c.capitalize()}\\n{s:.1f} GB ({s/total*100:.0f}%)\"\n",
    "              for c, s in zip(categories, sizes)]\n",
    "\n",
    "    wedges, texts = ax.pie(sizes, labels=labels, colors=pie_colors,\n",
    "                           startangle=90, textprops={'fontsize': 9})\n",
    "    ax.set_title(f\"{title}\\nTotal: {total:.1f} GB / 80 GB\",\n",
    "                 fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle(\"Per-GPU Memory Breakdown\", fontsize=14, fontweight='bold',\n",
    "             y=1.02)\n",
    "plt.tight_layout()\n",
    "# üìä Visualization: display the chart\n",
    "plt.show()"
   ],
   "id": "cell_45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_11_comm_estimator",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Comm Estimator\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_11_comm_estimator.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Communication Volume Estimator\n",
    "\n",
    "For each parallelism dimension, let us estimate how much data moves per training step."
   ],
   "id": "cell_46"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `compute_communication_volume` function estimates bytes transferred for TP, PP, DP, EP, and SP per training step. We start with the function signature and the TP / PP calculations."
   ],
   "id": "cell_47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_communication_volume(\n",
    "    model: ModelConfig,\n",
    "    n_dp: int, n_tp: int, n_pp: int, n_ep: int,\n",
    "    micro_batch_size: int = 1,\n",
    "    global_batch_size: int = 1024\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Estimate communication volume per training step for each dimension.\n",
    "    Returns dict of {dimension: {volume_GB, num_ops, pattern}}.\n",
    "    \"\"\"\n",
    "    layers_per_stage = model.num_layers // n_pp\n",
    "    params_per_gpu = model.total_params_B * 1e9 / (n_tp * n_pp)\n",
    "    seq_per_tp = model.seq_len  # full seq needed for attention\n",
    "    num_microbatches = global_batch_size // (n_dp * micro_batch_size)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # TP: AllReduce per layer (forward + backward)\n",
    "    if n_tp > 1:\n",
    "        tp_per_layer = (2 * micro_batch_size * seq_per_tp\n",
    "                       * model.hidden_dim * 2)  # bytes\n",
    "        tp_total = tp_per_layer * layers_per_stage * 2  # fwd + bwd\n",
    "        tp_total *= num_microbatches\n",
    "        results[\"TP\"] = {\n",
    "            \"volume_GB\": tp_total / (1024**3),\n",
    "            \"num_ops\": layers_per_stage * 2 * num_microbatches,\n",
    "            \"pattern\": \"AllReduce\"\n",
    "        }\n",
    "    else:\n",
    "        results[\"TP\"] = {\"volume_GB\": 0, \"num_ops\": 0, \"pattern\": \"N/A\"}"
   ],
   "id": "cell_48"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compute the PP and DP communication volumes."
   ],
   "id": "cell_49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # PP: Point-to-point at stage boundaries\n",
    "    if n_pp > 1:\n",
    "        pp_per_mb = (micro_batch_size * model.seq_len\n",
    "                    * model.hidden_dim * 2)  # bytes\n",
    "        pp_total = pp_per_mb * num_microbatches * 2  # fwd + bwd\n",
    "        results[\"PP\"] = {\n",
    "            \"volume_GB\": pp_total / (1024**3),\n",
    "            \"num_ops\": num_microbatches * 2,\n",
    "            \"pattern\": \"Point-to-Point\"\n",
    "        }\n",
    "    else:\n",
    "        results[\"PP\"] = {\"volume_GB\": 0, \"num_ops\": 0, \"pattern\": \"N/A\"}\n",
    "\n",
    "    # DP: AllReduce gradients\n",
    "    if n_dp > 1:\n",
    "        dp_total = 2 * params_per_gpu * 2  # 2x for reduce-scatter + all-gather\n",
    "        results[\"DP\"] = {\n",
    "            \"volume_GB\": dp_total / (1024**3),\n",
    "            \"num_ops\": 1,\n",
    "            \"pattern\": \"AllReduce\"\n",
    "        }\n",
    "    else:\n",
    "        results[\"DP\"] = {\"volume_GB\": 0, \"num_ops\": 0, \"pattern\": \"N/A\"}"
   ],
   "id": "cell_50"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the EP (All-to-All) and SP (Reduce-Scatter) volumes, plus we run it on Llama 3 405B."
   ],
   "id": "cell_51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # EP: All-to-All dispatch + collect\n",
    "    if n_ep > 1 and model.num_experts > 1:\n",
    "        # Tokens dispatched to experts\n",
    "        tokens_per_step = (micro_batch_size * model.seq_len\n",
    "                          * num_microbatches)\n",
    "        # Each token's hidden state sent to an expert\n",
    "        ep_volume = (tokens_per_step * model.hidden_dim * 2 * 2)  # dispatch+collect\n",
    "        results[\"EP\"] = {\n",
    "            \"volume_GB\": ep_volume / (1024**3),\n",
    "            \"num_ops\": num_microbatches * 2,\n",
    "            \"pattern\": \"All-to-All\"\n",
    "        }\n",
    "    else:\n",
    "        results[\"EP\"] = {\"volume_GB\": 0, \"num_ops\": 0, \"pattern\": \"N/A\"}\n",
    "\n",
    "    # SP: Reduce-scatter / All-gather (same group as TP)\n",
    "    if n_tp > 1:\n",
    "        sp_per_layer = (micro_batch_size * model.seq_len\n",
    "                       * model.hidden_dim * 2)  # bytes\n",
    "        sp_total = sp_per_layer * layers_per_stage * 2  # fwd+bwd\n",
    "        sp_total *= num_microbatches\n",
    "        results[\"SP\"] = {\n",
    "            \"volume_GB\": sp_total / (1024**3),\n",
    "            \"num_ops\": layers_per_stage * 2 * num_microbatches,\n",
    "            \"pattern\": \"Reduce-Scatter\"\n",
    "        }\n",
    "    else:\n",
    "        results[\"SP\"] = {\"volume_GB\": 0, \"num_ops\": 0, \"pattern\": \"N/A\"}\n",
    "\n",
    "    return results"
   ],
   "id": "cell_52"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us call the function for Llama 3 405B and print the per-dimension communication table."
   ],
   "id": "cell_53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute for Llama 3 405B\n",
    "comm_llama = compute_communication_volume(\n",
    "    llama3_model, n_dp=128, n_tp=8, n_pp=16, n_ep=1,\n",
    "    global_batch_size=2048\n",
    ")\n",
    "\n",
    "print(\"Communication Volume Per Step ‚Äî Llama 3 405B\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Dim':<5} {'Volume (GB)':<14} {'Ops/Step':<12} {'Pattern'}\")\n",
    "print(\"-\" * 60)\n",
    "for dim, info in comm_llama.items():\n",
    "    if info['volume_GB'] > 0:\n",
    "        print(f\"{dim:<5} {info['volume_GB']:>10.2f}    \"\n",
    "              f\"{info['num_ops']:>8}     {info['pattern']}\")\n",
    "    else:\n",
    "        print(f\"{dim:<5} {'--':>10}    {'--':>8}     {info['pattern']}\")"
   ],
   "id": "cell_54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_12_comm_comparison",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Comm Comparison\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_12_comm_comparison.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize the communication volumes side by side for both models to see how different architectures distribute their communication load."
   ],
   "id": "cell_55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Communication volume comparison\n",
    "\n",
    "comm_ds = compute_communication_volume(\n",
    "    deepseek_v3_model, n_dp=8, n_tp=1, n_pp=8, n_ep=32,\n",
    "    global_batch_size=512\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "dims = ['TP', 'SP', 'PP', 'DP', 'EP']\n",
    "bar_colors = ['#4CAF50', '#8BC34A', '#FF9800', '#2196F3', '#9C27B0']"
   ],
   "id": "cell_56"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot each model's communication volume bar chart, annotating active dimensions with their GB values and inactive ones with \"N/A\"."
   ],
   "id": "cell_57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ax, comm, title in zip(axes, [comm_llama, comm_ds],\n",
    "                           [\"Llama 3 405B\", \"DeepSeek-V3\"]):\n",
    "    volumes = [comm[d]['volume_GB'] for d in dims]\n",
    "    volumes_plot = [max(v, 0.001) for v in volumes]  # for log scale\n",
    "    active = [v > 0 for v in volumes]\n",
    "    bars = ax.bar(dims, volumes_plot, color=[c if a else '#E0E0E0'\n",
    "                  for c, a in zip(bar_colors, active)],\n",
    "                  edgecolor='white', linewidth=1.5)\n",
    "    for bar, vol, a in zip(bars, volumes, active):\n",
    "        if a and vol > 0:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2,\n",
    "                   bar.get_height() * 1.1,\n",
    "                   f'{vol:.1f} GB', ha='center', va='bottom',\n",
    "                   fontsize=9, fontweight='bold')\n",
    "        elif not a:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2,\n",
    "                   bar.get_height() * 1.1, 'N/A',\n",
    "                   ha='center', va='bottom', fontsize=9, color='gray')\n",
    "    ax.set_ylabel(\"Communication Volume (GB)\", fontsize=11)\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylim(0.001, max(volumes_plot) * 5)\n",
    "\n",
    "plt.suptitle(\"Communication Volume Per Training Step\",\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "# üìä Visualization: display the chart\n",
    "plt.show()"
   ],
   "id": "cell_58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_13_validator",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Validator\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_13_validator.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 The Composition Validator\n",
    "\n",
    "Before we use a configuration, we must verify it actually works. There are hard constraints that must be satisfied.\n",
    "\n",
    "**Common pitfall**: Blindly picking parallelism numbers without checking divisibility constraints will result in silent correctness bugs or crashes. Always validate before launching a training run."
   ],
   "id": "cell_59"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validator checks divisibility constraints (TP divides heads, PP divides layers, EP divides experts), memory capacity, and batch size alignment. It returns a list of errors and warnings."
   ],
   "id": "cell_60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_config(\n",
    "    model: ModelConfig,\n",
    "    n_dp: int, n_tp: int, n_pp: int, n_ep: int,\n",
    "    gpu_spec: GPUSpec,\n",
    "    micro_batch_size: int = 1,\n",
    "    global_batch_size: int = 1024,\n",
    "    zero_stage: int = 1\n",
    ") -> Tuple[bool, List[str]]:\n",
    "    \"\"\"\n",
    "    Validate a parallelism configuration. Returns (is_valid, messages).\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    warnings_list = []\n",
    "\n",
    "    # Check: TP must divide num_heads\n",
    "    if model.num_heads % n_tp != 0:\n",
    "        errors.append(\n",
    "            f\"TP={n_tp} does not divide num_heads={model.num_heads}\")\n",
    "\n",
    "    # Check: PP must divide num_layers\n",
    "    if model.num_layers % n_pp != 0:\n",
    "        errors.append(\n",
    "            f\"PP={n_pp} does not divide num_layers={model.num_layers}\")\n",
    "\n",
    "    # Check: TP should not exceed GPUs per node\n",
    "    if n_tp > gpu_spec.gpus_per_node:\n",
    "        errors.append(\n",
    "            f\"TP={n_tp} exceeds GPUs per node={gpu_spec.gpus_per_node}\")"
   ],
   "id": "cell_61"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue the validator with EP divisibility, memory capacity checks, and batch size alignment."
   ],
   "id": "cell_62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Check: EP must divide num_experts (for MoE)\n",
    "    if model.num_experts > 1 and model.num_experts % n_ep != 0:\n",
    "        errors.append(\n",
    "            f\"EP={n_ep} does not divide num_experts={model.num_experts}\")\n",
    "\n",
    "    # Check: total GPUs is reasonable\n",
    "    total_gpus = n_dp * n_tp * n_pp * n_ep\n",
    "    if total_gpus > 100000:\n",
    "        warnings_list.append(\n",
    "            f\"Total GPUs={total_gpus:,} is extremely large\")\n",
    "\n",
    "    # Check: memory fits\n",
    "    mem = compute_memory_per_gpu(\n",
    "        model, n_dp, n_tp, n_pp, n_ep,\n",
    "        micro_batch_size, zero_stage\n",
    "    )\n",
    "    if mem['total'] > gpu_spec.memory_gb:\n",
    "        errors.append(\n",
    "            f\"Memory {mem['total']:.1f} GB exceeds GPU capacity \"\n",
    "            f\"{gpu_spec.memory_gb} GB\")\n",
    "    elif mem['total'] > gpu_spec.memory_gb * 0.9:\n",
    "        warnings_list.append(\n",
    "            f\"Memory {mem['total']:.1f} GB is >90% of GPU capacity\")\n",
    "\n",
    "    # Check: global batch size is divisible\n",
    "    effective_dp = n_dp\n",
    "    if global_batch_size % effective_dp != 0:\n",
    "        warnings_list.append(\n",
    "            f\"Global batch {global_batch_size} not divisible by DP={n_dp}\")"
   ],
   "id": "cell_63"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we aggregate errors and warnings, then run the validator on both real-world configs."
   ],
   "id": "cell_64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    is_valid = len(errors) == 0\n",
    "    messages = ([f\"ERROR: {e}\" for e in errors] +\n",
    "                [f\"WARNING: {w}\" for w in warnings_list])\n",
    "\n",
    "    if is_valid and not warnings_list:\n",
    "        messages.append(\"All checks passed!\")\n",
    "\n",
    "    return is_valid, messages\n",
    "\n",
    "# Validate Llama 3 405B\n",
    "print(\"Validating Llama 3 405B configuration...\")\n",
    "valid, msgs = validate_config(\n",
    "    llama3_model, n_dp=128, n_tp=8, n_pp=16, n_ep=1,\n",
    "    gpu_spec=GPU_SPECS[\"H100_80GB\"], global_batch_size=2048\n",
    ")\n",
    "for msg in msgs:\n",
    "    print(f\"  {msg}\")\n",
    "print(f\"  Valid: {valid}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Validate DeepSeek-V3\n",
    "print(\"Validating DeepSeek-V3 configuration...\")\n",
    "valid2, msgs2 = validate_config(\n",
    "    deepseek_v3_model, n_dp=8, n_tp=1, n_pp=8, n_ep=32,\n",
    "    gpu_spec=GPU_SPECS[\"H800_80GB\"], global_batch_size=512\n",
    ")\n",
    "for msg in msgs2:\n",
    "    print(f\"  {msg}\")\n",
    "print(f\"  Valid: {valid2}\")"
   ],
   "id": "cell_65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_14_3d_grid",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: 3d Grid\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_14_3d_grid.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 3D GPU Grid Visualization\n",
    "\n",
    "This is where it gets visually satisfying. Let us render a 3D grid showing how GPUs are organized across the three primary spatial dimensions: DP, TP, and PP."
   ],
   "id": "cell_66"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `visualize_3d_gpu_grid` function creates a 3D scatter plot where each dot is a GPU. TP maps to the x-axis (within-node), PP to y (across stages), and DP to z (replicas). We draw node boundaries and pipeline connections as line overlays."
   ],
   "id": "cell_67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_3d_gpu_grid(\n",
    "    n_dp: int, n_tp: int, n_pp: int,\n",
    "    title: str = \"5D Parallelism GPU Grid\",\n",
    "    max_dp_show: int = 8,\n",
    "    max_pp_show: int = 8\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize the GPU layout as a 3D grid.\n",
    "    TP = x-axis (within node), PP = y-axis (across stages),\n",
    "    DP = z-axis (replicas).\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Limit display for readability\n",
    "    dp_show = min(n_dp, max_dp_show)\n",
    "    pp_show = min(n_pp, max_pp_show)\n",
    "    tp_show = n_tp\n",
    "\n",
    "    # Create grid coordinates\n",
    "    tp_coords = np.arange(tp_show)\n",
    "    pp_coords = np.arange(pp_show)\n",
    "    dp_coords = np.arange(dp_show)"
   ],
   "id": "cell_68"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We place each GPU as a scatter point, colored by pipeline stage, and draw NVLink / pipeline connection lines."
   ],
   "id": "cell_69"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Color by pipeline stage\n",
    "    for dp_idx in dp_coords:\n",
    "        for pp_idx in pp_coords:\n",
    "            for tp_idx in tp_coords:\n",
    "                # Color: blend based on all three dimensions\n",
    "                color_val = pp_idx / max(pp_show - 1, 1)\n",
    "                alpha = 0.4 + 0.5 * (1 - dp_idx / max(dp_show - 1, 1))\n",
    "\n",
    "                ax.scatter(tp_idx, pp_idx, dp_idx,\n",
    "                          c=[plt.cm.viridis(color_val)],\n",
    "                          s=120, alpha=alpha, edgecolors='black',\n",
    "                          linewidth=0.5, zorder=5)\n",
    "\n",
    "    # Draw node boundaries (TP groups)\n",
    "    for dp_idx in dp_coords:\n",
    "        for pp_idx in pp_coords:\n",
    "            xs = [0, tp_show - 1]\n",
    "            ys = [pp_idx, pp_idx]\n",
    "            zs = [dp_idx, dp_idx]\n",
    "            ax.plot(xs, ys, zs, color='#4CAF50', linewidth=1.5,\n",
    "                   alpha=0.3)\n",
    "\n",
    "    # Draw pipeline connections\n",
    "    for dp_idx in dp_coords:\n",
    "        for tp_idx in tp_coords:\n",
    "            xs = [tp_idx, tp_idx]\n",
    "            ys = [0, pp_show - 1]\n",
    "            zs = [dp_idx, dp_idx]\n",
    "            ax.plot(xs, ys, zs, color='#FF9800', linewidth=0.8,\n",
    "                   alpha=0.2)"
   ],
   "id": "cell_70"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we label the axes, add a title showing total vs. displayed GPUs, and render the plot."
   ],
   "id": "cell_71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ax.set_xlabel(f'Tensor Parallel ({n_tp}x)\\n[NVLink, within node]',\n",
    "                  fontsize=10, labelpad=10)\n",
    "    ax.set_ylabel(f'Pipeline Parallel ({n_pp}x)\\n[InfiniBand, across nodes]',\n",
    "                  fontsize=10, labelpad=10)\n",
    "    ax.set_zlabel(f'Data Parallel ({n_dp}x)\\n[Cluster-wide]',\n",
    "                  fontsize=10, labelpad=10)\n",
    "\n",
    "    # Annotate totals\n",
    "    total = n_dp * n_tp * n_pp\n",
    "    shown = dp_show * tp_show * pp_show\n",
    "    subtitle = (f\"Showing {shown} of {total:,} GPUs \"\n",
    "                f\"({n_dp}x DP x {n_tp}x TP x {n_pp}x PP)\")\n",
    "\n",
    "    ax.set_title(f\"{title}\\n{subtitle}\", fontsize=13, fontweight='bold',\n",
    "                 pad=20)\n",
    "\n",
    "    ax.view_init(elev=25, azim=135)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # üìä Visualization: display the chart\n",
    "    plt.show()\n",
    "\n",
    "# Visualize Llama 3 405B grid\n",
    "visualize_3d_gpu_grid(\n",
    "    n_dp=128, n_tp=8, n_pp=16,\n",
    "    title=\"Llama 3 405B ‚Äî 16,384 GPU Grid\"\n",
    ")"
   ],
   "id": "cell_72"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also see the DeepSeek-V3 grid shape, which looks very different because it uses EP instead of TP."
   ],
   "id": "cell_73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DeepSeek-V3 grid (different shape ‚Äî EP replaces TP)\n",
    "visualize_3d_gpu_grid(\n",
    "    n_dp=8, n_tp=8, n_pp=8,\n",
    "    title=\"DeepSeek-V3 ‚Äî 2,048 GPU Grid\\n(EP=32 maps onto node groups)\",\n",
    "    max_dp_show=8, max_pp_show=8\n",
    ")"
   ],
   "id": "cell_74"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_15_todo1_intro",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo1 Intro\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_15_todo1_intro.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Your Turn ‚Äî TODO Sections\n",
    "\n",
    "Now it is your turn. We have built the building blocks ‚Äî let us see if you can put them together.\n",
    "\n",
    "### TODO 1: Implement the Parallelism Config Optimizer\n",
    "\n",
    "Given a model size and GPU count, find the optimal DP / TP / PP split. The heuristic is:\n",
    "1. Set TP to the maximum that fits within a node (usually 8)\n",
    "2. Set PP to the minimum needed so the model fits in memory\n",
    "3. Maximize DP with the remaining GPUs\n",
    "4. Verify the configuration is valid"
   ],
   "id": "cell_75"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_parallelism(\n",
    "    model: ModelConfig,\n",
    "    total_gpus: int,\n",
    "    gpu_spec: GPUSpec,\n",
    "    max_tp: int = 8\n",
    ") -> Optional[ParallelismConfig]:\n",
    "    \"\"\"\n",
    "    Recommend a parallelism configuration.\n",
    "    Heuristic: Set TP to node size, find min PP for memory,\n",
    "    maximize DP with remaining GPUs, then validate.\n",
    "\n",
    "    Args:\n",
    "        model: Model architecture config\n",
    "        total_gpus: Total available GPUs\n",
    "        gpu_spec: GPU hardware specs\n",
    "        max_tp: Maximum tensor parallelism degree\n",
    "    Returns:\n",
    "        ParallelismConfig or None if no valid config found.\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: Set n_tp = min(max_tp, gpus_per_node)\n",
    "    # Step 2: Set n_ep = model.num_experts if MoE, else 1\n",
    "    # Step 3: Try n_pp from 1 upward. For each n_pp:\n",
    "    #         a) Check n_pp divides model.num_layers\n",
    "    #         b) Compute n_dp = total_gpus // (n_tp * n_pp * n_ep)\n",
    "    #         c) Check n_dp >= 1 and memory fits\n",
    "    # Step 4: Return the ParallelismConfig\n",
    "    # ==============================\n",
    "\n",
    "    n_tp = min(max_tp, gpu_spec.gpus_per_node)"
   ],
   "id": "cell_76"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the MoE expert parallelism setup and the search loop over PP values."
   ],
   "id": "cell_77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # For MoE, set EP based on expert count\n",
    "    if model.num_experts > 1:\n",
    "        # Try to place experts across available GPUs\n",
    "        n_ep = min(model.num_experts,\n",
    "                   total_gpus // (n_tp * 2))  # leave room for PP,DP\n",
    "        # Ensure n_ep divides num_experts\n",
    "        while n_ep > 1 and model.num_experts % n_ep != 0:\n",
    "            n_ep -= 1\n",
    "    else:\n",
    "        n_ep = 1\n",
    "\n",
    "    # YOUR CODE: find the right n_pp and n_dp\n",
    "    n_pp = ???  # Try values from 1 upward\n",
    "    n_dp = ???  # Remaining GPUs\n",
    "\n",
    "    return None  # Replace with your ParallelismConfig"
   ],
   "id": "cell_78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_16_todo1_solution",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Todo1 Solution\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_16_todo1_solution.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the reference solution. It searches for the minimum PP that satisfies memory constraints, then maximizes DP with the remaining GPUs."
   ],
   "id": "cell_79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification cell ‚Äî Reference solution (hidden)\n",
    "# Uncomment the function below to check your answer\n",
    "\n",
    "def recommend_parallelism_solution(\n",
    "    model: ModelConfig,\n",
    "    total_gpus: int,\n",
    "    gpu_spec: GPUSpec,\n",
    "    max_tp: int = 8\n",
    ") -> Optional[ParallelismConfig]:\n",
    "    \"\"\"Reference solution for the config optimizer.\"\"\"\n",
    "    n_tp = min(max_tp, gpu_spec.gpus_per_node)\n",
    "\n",
    "    if model.num_experts > 1:\n",
    "        n_ep = min(model.num_experts, total_gpus // (n_tp * 2))\n",
    "        while n_ep > 1 and model.num_experts % n_ep != 0:\n",
    "            n_ep -= 1\n",
    "    else:\n",
    "        n_ep = 1\n",
    "\n",
    "    best_config = None\n",
    "\n",
    "    for n_pp in range(1, model.num_layers + 1):\n",
    "        if model.num_layers % n_pp != 0:\n",
    "            continue\n",
    "\n",
    "        remaining = total_gpus // (n_tp * n_pp * n_ep)\n",
    "        if remaining < 1:\n",
    "            break\n",
    "\n",
    "        n_dp = remaining"
   ],
   "id": "cell_80"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check memory and validity for each candidate PP, then return the first valid config (which minimizes PP and maximizes DP)."
   ],
   "id": "cell_81"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Check memory\n",
    "        mem = compute_memory_per_gpu(\n",
    "            model, n_dp, n_tp, n_pp, n_ep,\n",
    "            micro_batch_size=1, zero_stage=1\n",
    "        )\n",
    "\n",
    "        if mem['total'] <= gpu_spec.memory_gb * 0.9:\n",
    "            # Validate\n",
    "            valid, _ = validate_config(\n",
    "                model, n_dp, n_tp, n_pp, n_ep, gpu_spec\n",
    "            )\n",
    "            if valid:\n",
    "                best_config = ParallelismConfig(\n",
    "                    name=f\"{model.name} (recommended)\",\n",
    "                    n_dp=n_dp, n_tp=n_tp, n_pp=n_pp,\n",
    "                    n_sp=n_tp, n_ep=n_ep,\n",
    "                    gpu_type=gpu_spec.name,\n",
    "                    gpu_memory_gb=int(gpu_spec.memory_gb)\n",
    "                )\n",
    "                break  # Take first valid (minimizes PP)\n",
    "\n",
    "    return best_config"
   ],
   "id": "cell_82"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test: does our optimizer recover the actual Llama 3 405B configuration?"
   ],
   "id": "cell_83"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: does our optimizer recover the Llama 3 405B config?\n",
    "recommended = recommend_parallelism_solution(\n",
    "    llama3_model,\n",
    "    total_gpus=16384,\n",
    "    gpu_spec=GPU_SPECS[\"H100_80GB\"]\n",
    ")\n",
    "\n",
    "if recommended:\n",
    "    print(\"Recommended config for Llama 3 405B on 16,384 H100s:\")\n",
    "    print(f\"  DP={recommended.n_dp}, TP={recommended.n_tp}, \"\n",
    "          f\"PP={recommended.n_pp}, EP={recommended.n_ep}\")\n",
    "    print(f\"  Total GPUs: {recommended.total_gpus:,}\")\n",
    "\n",
    "    # Compare to actual\n",
    "    actual_dp, actual_tp, actual_pp = 128, 8, 16\n",
    "    match = (recommended.n_dp == actual_dp and\n",
    "             recommended.n_tp == actual_tp and\n",
    "             recommended.n_pp == actual_pp)\n",
    "    if match:\n",
    "        print(\"\\n  Matches Meta's actual configuration!\")\n",
    "    else:\n",
    "        print(f\"\\n  Actual config: DP={actual_dp}, TP={actual_tp}, \"\n",
    "              f\"PP={actual_pp}\")\n",
    "        print(\"  (Heuristic may differ from actual ‚Äî that is OK)\")\n",
    "else:\n",
    "    print(\"No valid configuration found.\")"
   ],
   "id": "cell_84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_17_think_about_it",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Think About It\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_17_think_about_it.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Think About This\n",
    "\n",
    "Before moving to TODO 2, consider:\n",
    "- Why did we set TP first and then find PP?\n",
    "- What would happen if we maximized PP instead of DP?\n",
    "- Why is TP always limited to within a single node?\n",
    "\n",
    "**Key insight**: The answer lies in the communication hierarchy. TP requires the most bandwidth (AllReduce every layer), so it **must** sit on the fastest link (NVLink). PP has moderate requirements, and DP can tolerate the most latency. By setting TP first to the node boundary, we automatically respect the hardware hierarchy."
   ],
   "id": "cell_85"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_18_todo2_mfu",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Before You Start: Todo2 Mfu\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_18_todo2_mfu.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Compute Model FLOPS Utilization (MFU)\n",
    "\n",
    "MFU measures what fraction of theoretical peak FLOPS the training run actually achieves. Llama 3 reports 38-43% MFU. Let us compute it.\n",
    "\n",
    "The formula:\n",
    "$$\\text{MFU} = \\frac{\\text{Achieved FLOPS per GPU}}{\\text{Peak FLOPS per GPU}}$$\n",
    "\n",
    "Where achieved FLOPS per GPU depends on:\n",
    "- Model FLOPS per token: approximately $6 \\times P$ (forward + backward, for $P$ parameters)\n",
    "- Tokens processed per second per GPU\n",
    "- Pipeline bubble overhead reduces effective throughput"
   ],
   "id": "cell_86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mfu(\n",
    "    model: ModelConfig,\n",
    "    config: ParallelismConfig,\n",
    "    gpu_spec: GPUSpec,\n",
    "    tokens_per_second_per_gpu: float,\n",
    "    bubble_fraction: float = 0.0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute Model FLOPS Utilization.\n",
    "    Args: model config, parallelism config, GPU specs,\n",
    "          achieved tokens/sec/GPU, bubble fraction (0-1).\n",
    "    Returns: MFU as a fraction (0 to 1).\n",
    "    \"\"\"\n",
    "    # ============ TODO ============\n",
    "    # Step 1: flops_per_token = 6 * total_params\n",
    "    # Step 2: achieved_flops = flops_per_token_per_gpu * tokens/sec\n",
    "    #         (divide by TP*PP since each GPU computes its shard)\n",
    "    # Step 3: effective_flops = achieved * (1 - bubble_fraction)\n",
    "    # Step 4: MFU = effective_flops / peak_flops\n",
    "    # ==============================\n",
    "\n",
    "    mfu = ???  # YOUR CODE HERE\n",
    "    return mfu"
   ],
   "id": "cell_87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_19_todo2_solution",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Todo2 Solution\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_19_todo2_solution.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the reference solution. The key subtlety is that each GPU only computes its shard of the model (divided by TP and PP), so `flops_per_token` must be divided accordingly."
   ],
   "id": "cell_88"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification ‚Äî Reference solution and check\n",
    "\n",
    "def compute_mfu_solution(\n",
    "    model: ModelConfig,\n",
    "    config: ParallelismConfig,\n",
    "    gpu_spec: GPUSpec,\n",
    "    tokens_per_second_per_gpu: float,\n",
    "    bubble_fraction: float = 0.0\n",
    ") -> float:\n",
    "    \"\"\"Reference solution for MFU computation.\"\"\"\n",
    "    # FLOPS per token for the full model\n",
    "    flops_per_token = 6 * model.total_params_B * 1e9\n",
    "\n",
    "    # Each GPU processes its share (already counted in tokens_per_second)\n",
    "    # But FLOPS per token is for the FULL model ‚Äî each GPU only computes\n",
    "    # its shard, so we divide by (TP * PP)\n",
    "    flops_per_token_per_gpu = flops_per_token / (config.n_tp * config.n_pp)\n",
    "\n",
    "    # Achieved FLOPS per GPU\n",
    "    achieved_flops = flops_per_token_per_gpu * tokens_per_second_per_gpu\n",
    "\n",
    "    # Account for pipeline bubble\n",
    "    effective_flops = achieved_flops * (1.0 - bubble_fraction)\n",
    "\n",
    "    # Peak FLOPS (convert TFLOPS to FLOPS)\n",
    "    peak_flops = gpu_spec.peak_tflops_fp16 * 1e12\n",
    "\n",
    "    mfu = effective_flops / peak_flops\n",
    "    return mfu"
   ],
   "id": "cell_89"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test with Llama 3 405B's reported numbers and see if we land near Meta's 38-43% MFU."
   ],
   "id": "cell_90"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama 3 405B: Meta reports ~380 tokens/sec/GPU and 38-43% MFU\n",
    "n_pp_llama = 16\n",
    "n_microbatches_llama = 16\n",
    "bubble_frac_llama = (n_pp_llama - 1) / (n_pp_llama - 1 + n_microbatches_llama)\n",
    "\n",
    "# Tokens per second per GPU (estimated from Meta's paper)\n",
    "# Total throughput: ~16M tokens/sec across 16,384 GPUs\n",
    "tps_per_gpu = 16e6 / 16384  # ~977 tokens/sec/gpu\n",
    "\n",
    "mfu = compute_mfu_solution(\n",
    "    llama3_model, llama3_405b, GPU_SPECS[\"H100_80GB\"],\n",
    "    tokens_per_second_per_gpu=tps_per_gpu,\n",
    "    bubble_fraction=bubble_frac_llama\n",
    ")\n",
    "\n",
    "print(f\"Llama 3 405B ‚Äî MFU Estimation\")\n",
    "print(f\"=\" * 45)\n",
    "print(f\"  Pipeline bubble fraction:  {bubble_frac_llama:.1%}\")\n",
    "print(f\"  Tokens/sec/GPU:            {tps_per_gpu:.0f}\")\n",
    "print(f\"  Estimated MFU:             {mfu:.1%}\")\n",
    "print(f\"  Meta's reported MFU:       38-43%\")\n",
    "\n",
    "if 0.30 <= mfu <= 0.50:\n",
    "    print(f\"\\n  Our estimate is in the right ballpark!\")\n",
    "else:\n",
    "    print(f\"\\n  Our estimate differs from reported ‚Äî that is expected\")\n",
    "    print(f\"  (real MFU depends on many factors we simplified)\")"
   ],
   "id": "cell_91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_20_cheat_sheet",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Cheat Sheet\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_20_cheat_sheet.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. The Big Picture: Summary Table\n",
    "\n",
    "Let us step back and see all five dimensions in one place. This is the table you should commit to memory."
   ],
   "id": "cell_92"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a comprehensive summary table\n",
    "print(\"=\" * 95)\n",
    "print(\"  THE 5D PARALLELISM CHEAT SHEET\")\n",
    "print(\"=\" * 95)\n",
    "\n",
    "headers = [\"Dimension\", \"Splits\", \"Why Needed\", \"Communication\",\n",
    "           \"Placement\", \"Bandwidth\"]\n",
    "rows = [\n",
    "    [\"DP (Data)\", \"Training data\", \"Throughput\",\n",
    "     \"AllReduce (grads)\", \"Entire cluster\", \"Low OK\"],\n",
    "    [\"TP (Tensor)\", \"Weight matrices\", \"Layer too large\",\n",
    "     \"AllReduce (activs)\", \"Within node\", \"NVLink 900 GB/s\"],\n",
    "    [\"PP (Pipeline)\", \"Model depth\", \"Too many layers\",\n",
    "     \"Point-to-Point\", \"Across nodes\", \"InfiniBand 50 GB/s\"],\n",
    "    [\"SP (Sequence)\", \"Sequence length\", \"Long contexts\",\n",
    "     \"Reduce-Scatter\", \"Within node\", \"NVLink 900 GB/s\"],\n",
    "    [\"EP (Expert)\", \"MoE experts\", \"Specialist nets\",\n",
    "     \"All-to-All\", \"Flexible\", \"Medium ~50 GB/s\"],\n",
    "]"
   ],
   "id": "cell_93"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format and print the cheat sheet table along with the key insight."
   ],
   "id": "cell_94"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_widths = [14, 16, 16, 19, 15, 17]\n",
    "header_line = \" | \".join(h.ljust(w) for h, w in zip(headers, col_widths))\n",
    "print(f\"  {header_line}\")\n",
    "print(f\"  {'-' * len(header_line)}\")\n",
    "for row in rows:\n",
    "    row_line = \" | \".join(val.ljust(w) for val, w in zip(row, col_widths))\n",
    "    print(f\"  {row_line}\")\n",
    "\n",
    "print(\"=\" * 95)\n",
    "print()\n",
    "print(\"  Key Insight: Map communication-hungry dimensions to fast links.\")\n",
    "print(\"  TP (every layer) -> NVLink | PP (per micro-batch) -> InfiniBand\")\n",
    "print(\"  DP (once per step) -> Cluster-wide\")"
   ],
   "id": "cell_95"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us build a radar chart comparing the five dimensions across communication frequency, bandwidth requirements, memory savings, implementation complexity, and scalability."
   ],
   "id": "cell_96"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Radar chart comparing the 5 dimensions\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8),\n",
    "                        subplot_kw=dict(polar=True))\n",
    "\n",
    "categories = ['Comm\\nFrequency', 'Bandwidth\\nNeeded', 'Memory\\nSaved',\n",
    "              'Impl\\nComplexity', 'Scalability']\n",
    "N = len(categories)\n",
    "\n",
    "# Scores for each dimension (1-5 scale)\n",
    "dimension_scores = {\n",
    "    'DP': [1, 1, 2, 1, 5],\n",
    "    'TP': [5, 5, 4, 3, 2],\n",
    "    'PP': [3, 3, 4, 4, 3],\n",
    "    'SP': [5, 5, 3, 3, 2],\n",
    "    'EP': [3, 3, 2, 4, 4],\n",
    "}\n",
    "\n",
    "dim_colors = {\n",
    "    'DP': '#2196F3', 'TP': '#4CAF50', 'PP': '#FF9800',\n",
    "    'SP': '#8BC34A', 'EP': '#9C27B0'\n",
    "}\n",
    "\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]  # close the polygon"
   ],
   "id": "cell_97"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot each dimension as a filled polygon on the radar chart."
   ],
   "id": "cell_98"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim, scores in dimension_scores.items():\n",
    "    values = scores + scores[:1]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=dim,\n",
    "           color=dim_colors[dim], markersize=6)\n",
    "    ax.fill(angles, values, alpha=0.1, color=dim_colors[dim])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, fontsize=10)\n",
    "ax.set_ylim(0, 5.5)\n",
    "ax.set_yticks([1, 2, 3, 4, 5])\n",
    "ax.set_yticklabels(['1', '2', '3', '4', '5'], fontsize=8)\n",
    "ax.set_title(\"5D Parallelism ‚Äî Dimension Characteristics\", fontsize=14,\n",
    "             fontweight='bold', pad=25)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "# üìä Visualization: display the chart\n",
    "plt.show()"
   ],
   "id": "cell_99"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_21_full_planner",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Full Planner\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_21_full_planner.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Putting It All Together\n",
    "\n",
    "We have now studied each parallelism dimension in isolation and seen the math behind composing them. In this section, we bring all five together into a single unified planner that takes any model and hardware setup and produces a complete training plan.\n",
    "\n",
    "The key principle: **composition is multiplicative for GPUs but hierarchical for communication**. The total GPU count is simply $N_{DP} \\times N_{TP} \\times N_{PP} \\times N_{EP}$, but the communication patterns nest according to hardware topology ‚Äî TP and SP ride the fastest NVLink bus within a node, PP uses InfiniBand between nearby nodes, and DP spans the entire cluster where latency tolerance is highest.\n",
    "\n",
    "This is also where we see the power of the heuristic we built in TODO 1: set TP first (to fill the node), then find the minimum PP that fits memory, and give everything left to DP for maximum throughput. The planner below automates this entire workflow, adds communication and efficiency estimation, and presents a complete training recommendation."
   ],
   "id": "cell_100"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `full_5d_planner` function ties together every component: it calls `recommend_parallelism_solution` for the config, `compute_memory_per_gpu` for the memory breakdown, and `compute_communication_volume` for the comm analysis."
   ],
   "id": "cell_101"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_5d_planner(\n",
    "    model: ModelConfig,\n",
    "    gpu_spec: GPUSpec,\n",
    "    total_gpus: int,\n",
    "    global_batch_size: int = 1024,\n",
    "    zero_stage: int = 1,\n",
    "    micro_batch_size: int = 1,\n",
    "    verbose: bool = True\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    The complete 5D parallelism planner. Recommends optimal config\n",
    "    and reports memory, communication, and efficiency estimates.\n",
    "    \"\"\"\n",
    "    # Step 1: Recommend parallelism config\n",
    "    config = recommend_parallelism_solution(model, total_gpus, gpu_spec)\n",
    "    if config is None:\n",
    "        if verbose:\n",
    "            print(f\"Could not find valid config for {model.name} \"\n",
    "                  f\"on {total_gpus} {gpu_spec.name} GPUs\")\n",
    "        return {}\n",
    "\n",
    "    # Step 2: Compute memory breakdown\n",
    "    mem = compute_memory_per_gpu(\n",
    "        model, config.n_dp, config.n_tp, config.n_pp, config.n_ep,\n",
    "        micro_batch_size, zero_stage\n",
    "    )"
   ],
   "id": "cell_102"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with communication volume estimation, pipeline bubble analysis, and MFU projection."
   ],
   "id": "cell_103"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Step 3: Compute communication volumes\n",
    "    comm = compute_communication_volume(\n",
    "        model, config.n_dp, config.n_tp, config.n_pp, config.n_ep,\n",
    "        micro_batch_size, global_batch_size\n",
    "    )\n",
    "\n",
    "    # Step 4: Compute bubble fraction and MFU estimate\n",
    "    n_microbatches = max(1, global_batch_size //\n",
    "                        (config.n_dp * micro_batch_size))\n",
    "    bubble_frac = ((config.n_pp - 1) /\n",
    "                   (config.n_pp - 1 + n_microbatches)\n",
    "                   if config.n_pp > 1 else 0.0)\n",
    "\n",
    "    # Rough MFU estimate (35-45% is typical for well-optimized runs)\n",
    "    estimated_mfu = 0.42 * (1 - bubble_frac) * (1 - 0.05)  # 5% comm overhead\n",
    "\n",
    "    # Step 5: Estimate training time\n",
    "    # Total FLOPS for training = 6 * P * T (P=params, T=total tokens)\n",
    "    total_tokens = 15e12  # 15T tokens (typical for frontier models)\n",
    "    total_flops = 6 * model.total_params_B * 1e9 * total_tokens\n",
    "\n",
    "    achieved_flops_total = (total_gpus * gpu_spec.peak_tflops_fp16\n",
    "                           * 1e12 * estimated_mfu)\n",
    "    training_time_seconds = total_flops / achieved_flops_total\n",
    "    training_days = training_time_seconds / 86400"
   ],
   "id": "cell_104"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we assemble the result dictionary and print the complete planner report."
   ],
   "id": "cell_105"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    result = {\n",
    "        \"config\": config,\n",
    "        \"memory\": mem,\n",
    "        \"communication\": comm,\n",
    "        \"bubble_fraction\": bubble_frac,\n",
    "        \"estimated_mfu\": estimated_mfu,\n",
    "        \"training_days\": training_days,\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n{'=' * 65}\")\n",
    "        print(f\"  5D PARALLELISM PLANNER ‚Äî {model.name}\")\n",
    "        print(f\"{'=' * 65}\")\n",
    "\n",
    "        print(f\"\\n  Model: {model.total_params_B:.0f}B params, \"\n",
    "              f\"{model.num_layers} layers, \"\n",
    "              f\"hidden={model.hidden_dim}, \"\n",
    "              f\"heads={model.num_heads}\")\n",
    "        if model.num_experts > 1:\n",
    "            print(f\"  MoE: {model.num_experts} experts, \"\n",
    "                  f\"{model.expert_params_B:.1f}B params each\")\n",
    "        print(f\"  Hardware: {total_gpus:,} x {gpu_spec.name}\")"
   ],
   "id": "cell_106"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the recommended parallelism split, per-GPU memory breakdown, and efficiency metrics."
   ],
   "id": "cell_107"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        print(f\"\\n  Recommended Parallelism:\")\n",
    "        print(f\"    DP={config.n_dp}, TP={config.n_tp}, PP={config.n_pp}, \"\n",
    "              f\"SP={config.n_sp}, EP={config.n_ep}\")\n",
    "        print(f\"    Total = {config.total_gpus:,} GPUs \"\n",
    "              f\"({config.total_nodes:,} nodes)\")\n",
    "\n",
    "        print(f\"\\n  Memory Per GPU:\")\n",
    "        print(f\"    Weights:      {mem['weights']:>7.2f} GB\")\n",
    "        print(f\"    Gradients:    {mem['gradients']:>7.2f} GB\")\n",
    "        print(f\"    Optimizer:    {mem['optimizer']:>7.2f} GB\")\n",
    "        print(f\"    Activations:  {mem['activations']:>7.2f} GB\")\n",
    "        print(f\"    Total:        {mem['total']:>7.2f} GB / \"\n",
    "              f\"{gpu_spec.memory_gb:.0f} GB \"\n",
    "              f\"({mem['total']/gpu_spec.memory_gb*100:.0f}%)\")\n",
    "\n",
    "        print(f\"\\n  Efficiency:\")\n",
    "        print(f\"    Pipeline bubble:  {bubble_frac:.1%}\")\n",
    "        print(f\"    Estimated MFU:    {estimated_mfu:.1%}\")\n",
    "        print(f\"    Training time:    ~{training_days:.0f} days \"\n",
    "              f\"(on 15T tokens)\")\n",
    "        print(f\"\\n{'=' * 65}\")\n",
    "\n",
    "    return result"
   ],
   "id": "cell_108"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_22_planner_results",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Planner Results\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_22_planner_results.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Running the Planner on Real Models\n",
    "\n",
    "Now let us run the planner on our two reference models and a custom model to see the recommendations in action."
   ],
   "id": "cell_109"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan for Llama 3 405B\n",
    "print(\"=\" * 70)\n",
    "print(\"  CASE 1: Llama 3 405B on 16,384 H100 GPUs\")\n",
    "print(\"=\" * 70)\n",
    "result_llama = full_5d_planner(\n",
    "    llama3_model, GPU_SPECS[\"H100_80GB\"],\n",
    "    total_gpus=16384, global_batch_size=2048\n",
    ")"
   ],
   "id": "cell_110"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan for DeepSeek-V3\n",
    "print(\"=\" * 70)\n",
    "print(\"  CASE 2: DeepSeek-V3 on 2,048 H800 GPUs\")\n",
    "print(\"=\" * 70)\n",
    "result_ds = full_5d_planner(\n",
    "    deepseek_v3_model, GPU_SPECS[\"H800_80GB\"],\n",
    "    total_gpus=2048, global_batch_size=512\n",
    ")"
   ],
   "id": "cell_111"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a custom 70B dense model on a smaller cluster, to show how the planner adapts to different scales."
   ],
   "id": "cell_112"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan for a custom model ‚Äî 70B dense on 256 GPUs\n",
    "custom_model = ModelConfig(\n",
    "    name=\"Custom 70B Dense\",\n",
    "    total_params_B=70.0,\n",
    "    num_layers=80,\n",
    "    hidden_dim=8192,\n",
    "    num_heads=64,\n",
    "    num_experts=1,\n",
    "    expert_params_B=0.0,\n",
    "    seq_len=8192,\n",
    "    vocab_size=128000\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"  CASE 3: Custom 70B Dense on 256 A100 GPUs\")\n",
    "print(\"=\" * 70)\n",
    "result_custom = full_5d_planner(\n",
    "    custom_model, GPU_SPECS[\"A100_80GB\"],\n",
    "    total_gpus=256, global_batch_size=512\n",
    ")"
   ],
   "id": "cell_113"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_23_final_viz",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Code Walkthrough: Final Viz\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_23_final_viz.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Final Comparison Visualization\n",
    "\n",
    "With all three planner results in hand, we create a four-panel comparison: GPU counts, memory utilization, parallelism breakdown, and efficiency metrics."
   ],
   "id": "cell_114"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization: Side-by-side comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "models = ['Llama 3\\n405B', 'DeepSeek\\nV3', 'Custom\\n70B']\n",
    "\n",
    "# --- Panel 1: GPU count comparison ---\n",
    "ax = axes[0, 0]\n",
    "gpu_counts = [16384, 2048, 256]\n",
    "bar_colors_models = ['#2196F3', '#4CAF50', '#FF9800']\n",
    "bars = ax.bar(models, gpu_counts, color=bar_colors_models,\n",
    "              edgecolor='white', linewidth=2)\n",
    "for bar, count in zip(bars, gpu_counts):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 200,\n",
    "           f'{count:,}', ha='center', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(\"Total GPUs\", fontsize=12)\n",
    "ax.set_title(\"GPU Count\", fontsize=13, fontweight='bold')"
   ],
   "id": "cell_115"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Panel 2 shows per-GPU memory utilization (used vs. available) for each model."
   ],
   "id": "cell_116"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Panel 2: Memory utilization ---\n",
    "ax = axes[0, 1]\n",
    "mem_totals = [result_llama['memory']['total'],\n",
    "              result_ds['memory']['total'],\n",
    "              result_custom['memory']['total']]\n",
    "bar_width = 0.35\n",
    "x = np.arange(3)\n",
    "ax.bar(x - bar_width/2, mem_totals, bar_width, label='Used',\n",
    "       color='#F44336', alpha=0.8, edgecolor='white')\n",
    "ax.bar(x + bar_width/2, [80, 80, 80], bar_width, label='Available',\n",
    "       color='#E0E0E0', edgecolor='white')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, fontsize=10)\n",
    "ax.set_ylabel(\"Memory (GB)\", fontsize=12)\n",
    "ax.set_title(\"Per-GPU Memory\", fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)"
   ],
   "id": "cell_117"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Panels 3 and 4: parallelism degree breakdown (grouped bars on log scale) and efficiency metrics (bubble fraction vs. estimated MFU)."
   ],
   "id": "cell_118"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Panel 3: Parallelism breakdown (grouped bar) ---\n",
    "ax = axes[1, 0]\n",
    "configs = [\n",
    "    ('Llama 3 405B', 128, 8, 16, 1),\n",
    "    ('DeepSeek-V3', 8, 1, 8, 32),\n",
    "    ('Custom 70B', result_custom['config'].n_dp,\n",
    "     result_custom['config'].n_tp,\n",
    "     result_custom['config'].n_pp,\n",
    "     result_custom['config'].n_ep),\n",
    "]\n",
    "x = np.arange(3)\n",
    "dp_vals = [c[1] for c in configs]\n",
    "tp_vals = [c[2] for c in configs]\n",
    "pp_vals = [c[3] for c in configs]\n",
    "ep_vals = [c[4] for c in configs]\n",
    "\n",
    "w = 0.2\n",
    "ax.bar(x - 1.5*w, dp_vals, w, label='DP', color='#2196F3')\n",
    "ax.bar(x - 0.5*w, tp_vals, w, label='TP', color='#4CAF50')\n",
    "ax.bar(x + 0.5*w, pp_vals, w, label='PP', color='#FF9800')\n",
    "ax.bar(x + 1.5*w, ep_vals, w, label='EP', color='#9C27B0')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([c[0] for c in configs], fontsize=10)\n",
    "ax.set_ylabel(\"Parallelism Degree\", fontsize=12)\n",
    "ax.set_title(\"Parallelism Breakdown\", fontsize=13, fontweight='bold')\n",
    "ax.set_yscale('log')\n",
    "ax.legend(fontsize=10)"
   ],
   "id": "cell_119"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The efficiency panel shows how pipeline bubbles and MFU trade off across the three models."
   ],
   "id": "cell_120"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Panel 4: Efficiency metrics ---\n",
    "ax = axes[1, 1]\n",
    "results = [result_llama, result_ds, result_custom]\n",
    "bubbles = [r['bubble_fraction'] for r in results]\n",
    "mfus = [r['estimated_mfu'] for r in results]\n",
    "\n",
    "x = np.arange(3)\n",
    "ax.bar(x - 0.15, [b * 100 for b in bubbles], 0.3,\n",
    "       label='Bubble %', color='#F44336', alpha=0.7)\n",
    "ax.bar(x + 0.15, [m * 100 for m in mfus], 0.3,\n",
    "       label='Est. MFU %', color='#4CAF50', alpha=0.7)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, fontsize=10)\n",
    "ax.set_ylabel(\"Percentage (%)\", fontsize=12)\n",
    "ax.set_title(\"Efficiency Metrics\", fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "plt.suptitle(\"5D Parallelism Planner ‚Äî Model Comparison\",\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "# üìä Visualization: display the chart\n",
    "plt.show()"
   ],
   "id": "cell_121"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also visualize the GPU grid for our custom 70B model configuration."
   ],
   "id": "cell_122"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final 3D grid visualization for the custom model\n",
    "if result_custom.get('config'):\n",
    "    c = result_custom['config']\n",
    "    visualize_3d_gpu_grid(\n",
    "        n_dp=c.n_dp, n_tp=c.n_tp, n_pp=c.n_pp,\n",
    "        title=f\"Custom 70B Dense ‚Äî {c.total_gpus} GPU Grid\"\n",
    "    )"
   ],
   "id": "cell_123"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "narration_06_24_closing",
    "tags": [
     "narration"
    ]
   },
   "outputs": [],
   "source": [
    "#@title üéß Wrap-Up: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/06_24_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, the moment we have been building toward for 6 notebooks:"
   ],
   "id": "cell_124"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Final Output\n",
    "\n",
    "This is the culmination of the entire 6-notebook series. We summarize the key numbers, the communication hierarchy, and the golden rule of 5D parallelism."
   ],
   "id": "cell_125"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The grand finale print\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"  Congratulations!\")\n",
    "print()\n",
    "print(\"  You now understand how modern LLMs are trained\")\n",
    "print(\"  across thousands of GPUs.\")\n",
    "print()\n",
    "print(\"  You have built every parallelism dimension from scratch ‚Äî\")\n",
    "print(\"  from Data Parallelism to the full 5D grid.\")\n",
    "print()\n",
    "print(\"  The next time you read that a model was trained on\")\n",
    "print(\"  16,000 GPUs, you know exactly what is happening.\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"  Series Complete: 5D Parallelism from Scratch\")\n",
    "print(\"  Notebooks 1-6 by Vizuara\")\n",
    "print()\n",
    "print(\"=\" * 70)"
   ],
   "id": "cell_126"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### Reflection Questions\n",
    "\n",
    "1. **Why is TP always used within a node and not across nodes?**\n",
    "   Because TP requires AllReduce communication *within every single layer*. If the interconnect were slow (like Ethernet across nodes), the communication overhead would completely dominate compute time. NVLink provides 900 GB/s within a node ‚Äî roughly 18x faster than InfiniBand. That speed difference is why TP is always confined to a node.\n",
    "\n",
    "2. **If you had 256 GPUs and a 70B dense model (not MoE), what parallelism config would you choose?**\n",
    "   Run the planner above! The likely answer: TP=8 (one node), PP=2 or PP=4 (split the 80 layers), DP = 256 / (8 x PP). With PP=4, DP=8, giving 256 GPUs total. The exact split depends on your memory constraints and desired batch size.\n",
    "\n",
    "3. **What are the trade-offs between using more PP stages vs more DP replicas?**\n",
    "   More PP stages mean: lower per-GPU memory (fewer layers per GPU), but higher pipeline bubble overhead ($\\frac{P-1}{P-1+M}$). More DP replicas mean: higher throughput and larger effective batch size, but each GPU must still hold enough layers to fit in memory. The art is finding the sweet spot where memory fits and the bubble is small."
   ],
   "id": "cell_127"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Challenges\n",
    "\n",
    "1. **Gradient Checkpointing**: Modify the `compute_memory_per_gpu` function to support a `checkpointing_ratio` parameter that controls what fraction of layers are checkpointed. How does this change the recommended PP?\n",
    "\n",
    "2. **Cost Estimator**: Add a cloud cost estimator to the planner. H100 instances cost approximately $3.50/GPU/hour on-demand. How much would training Llama 3 405B cost? (Spoiler: it is measured in millions of dollars.)\n",
    "\n",
    "3. **Research Deep Dive**: Compare 3D parallelism between Megatron-LM and DeepSpeed. What are the key differences in their PP schedules? How does Megatron's interleaved 1F1B compare to DeepSpeed's zero-bubble pipeline?"
   ],
   "id": "cell_128"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick cost estimate for Llama 3 405B\n",
    "cost_per_gpu_hour = 3.50  # USD, H100 on-demand\n",
    "num_gpus = 16384\n",
    "training_days = 54  # approximate\n",
    "training_hours = training_days * 24\n",
    "\n",
    "total_cost = cost_per_gpu_hour * num_gpus * training_hours\n",
    "print(f\"Estimated training cost for Llama 3 405B:\")\n",
    "print(f\"  {num_gpus:,} GPUs x {training_hours:,} hours x \"\n",
    "      f\"${cost_per_gpu_hour}/GPU/hr\")\n",
    "print(f\"  = ${total_cost:,.0f}\")\n",
    "print(f\"  = ~${total_cost / 1e6:.1f} million\")\n",
    "print(f\"\\nFor context, DeepSeek-V3 reportedly cost ~$5.5M ‚Äî\")\n",
    "print(f\"roughly {total_cost / 5.5e6:.0f}x less than Llama 3 405B.\")"
   ],
   "id": "cell_129"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series Recap: 5D Parallelism from Scratch\n",
    "\n",
    "Congratulations on completing all 6 notebooks! Here is what we covered:\n",
    "\n",
    "| Notebook | Topic | Key Takeaway |\n",
    "|----------|-------|-------------|\n",
    "| **1** | Why Parallelism | A 7B model needs 112 GB for training ‚Äî a single GPU is not enough |\n",
    "| **2** | Data Parallelism & ZeRO | Replicate model, split data, AllReduce gradients. ZeRO eliminates redundancy |\n",
    "| **3** | Tensor Parallelism | Split weight matrices column-wise and row-wise. Needs NVLink |\n",
    "| **4** | Pipeline Parallelism | Split layers into stages. Micro-batching reduces the bubble |\n",
    "| **5** | Sequence & Expert Parallelism | SP splits the sequence (shares TP group). EP routes tokens to specialist experts |\n",
    "| **6** | The 5D Grid | All 5 compose: N = DP x TP x PP x EP. Match dimensions to the hardware hierarchy |\n",
    "\n",
    "The core lesson: **every parallelism dimension solves a specific bottleneck**, and the art of distributed training is composing them to match the communication hierarchy of your hardware."
   ],
   "id": "cell_130"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary ‚Äî one last look at the numbers\n",
    "\n",
    "print(\"5D PARALLELISM ‚Äî THE NUMBERS THAT MATTER\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"  Llama 3 405B:\")\n",
    "print(\"    128 DP x 8 TP x 16 PP = 16,384 H100 GPUs\")\n",
    "print(\"    ~38-43% MFU | ~54 days training | ~$40M+ estimated\")\n",
    "print()\n",
    "print(\"  DeepSeek-V3 (671B, 256 experts):\")\n",
    "print(\"    8 DP x 1 TP x 8 PP x 32 EP = 2,048 H800 GPUs\")\n",
    "print(\"    ~$5.5M training cost (remarkably efficient)\")\n",
    "print()\n",
    "print(\"  Communication Hierarchy:\")\n",
    "print(\"    NVLink  900 GB/s  ->  TP, SP  (within node)\")\n",
    "print(\"    IB NDR   50 GB/s  ->  PP, EP  (across nodes)\")\n",
    "print(\"    Cluster  ~25 GB/s ->  DP      (everywhere)\")\n",
    "print()\n",
    "print(\"  The Golden Rule:\")\n",
    "print(\"  Map communication-hungry ops to fast links.\")\n",
    "print(\"  TP every layer -> NVLink\")\n",
    "print(\"  PP every micro-batch -> InfiniBand\")\n",
    "print(\"  DP once per step -> anything\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"  Thank you for learning with Vizuara!\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "cell_131"
  }
 ]
}