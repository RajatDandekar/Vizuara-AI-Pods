{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "Image Diffusion Foundations ‚Äî Vizuara"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup: Run this cell first!\n",
    "# Check GPU availability and install dependencies\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Some cells may run slowly.\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(f\"\\nüì¶ Python {sys.version.split()[0]}\")\n",
    "print(f\"üî• PyTorch {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"üé≤ Random seed set to {SEED}\")\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "setup_cell"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Download Narration Audio & Play Introduction\n",
    "import os as _os\n",
    "if not _os.path.exists(\"/content/narration\"):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    gdown.download(id=\"1VimSva8YFMP_2-RyAjHL8KfiPhTmlA-o\", output=\"/content/narration.zip\", quiet=False)\n",
    "    !unzip -q /content/narration.zip -d /content/narration\n",
    "    !rm /content/narration.zip\n",
    "    print(f\"Loaded {len(_os.listdir('/content/narration'))} narration segments\")\n",
    "else:\n",
    "    print(\"Narration audio already loaded.\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"/content/narration/01_01_intro.mp3\"))"
   ],
   "id": "narration_download"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Image Diffusion Foundations: From Noise to Pictures\n",
    "\n",
    "*Part 1 of the Vizuara series on Diffusion LLMs from Scratch*\n",
    "*Estimated time: 30 minutes*"
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Why It Matters\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_02_why_it_matters.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_02_why_it_matters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Does This Matter?\n",
    "\n",
    "Imagine you are writing an essay on a typewriter. You type one letter at a time, left to right. Once a letter is pressed onto the page, it is permanent ‚Äî you cannot go back and change it. If you realize halfway through a sentence that the beginning was wrong, too bad.\n",
    "\n",
    "This is exactly how modern language models like GPT-4 and LLaMA work. They generate text **one token at a time**, from left to right. Each token depends on all the previous tokens, but has no knowledge of what will come after it.\n",
    "\n",
    "Now think of how an **artist** works. An artist does not paint the top-left pixel first, then the next pixel. Instead, they start with a rough sketch of the whole canvas, then progressively refine the details ‚Äî adding color here, sharpening edges there, going back to fix proportions. The whole image comes into focus *at the same time.*\n",
    "\n",
    "This is how **diffusion models** generate images. And in this notebook series, we will see how this same idea can be applied to **text generation** ‚Äî leading to a fundamentally new paradigm for language models.\n",
    "\n",
    "**By the end of this notebook, you will:**\n",
    "- Understand how image diffusion works from first principles\n",
    "- Build a working diffusion model that generates MNIST digits from pure noise\n",
    "- See why this approach breaks for text (setting up Notebook 2)"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Building Intuition\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_03_building_intuition.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_03_building_intuition"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Intuition\n",
    "\n",
    "### The Two Phases of Diffusion\n",
    "\n",
    "The core idea behind diffusion models is beautifully simple:\n",
    "\n",
    "**Phase 1 ‚Äî The Forward Process (Destroying):** Take a clean image. Gradually add random noise to it, step by step. After enough steps, the image becomes pure random static ‚Äî no trace of the original remains.\n",
    "\n",
    "**Phase 2 ‚Äî The Reverse Process (Creating):** Train a neural network that learns to reverse each noise step. Given a noisy image, predict what it looked like one step earlier (slightly less noisy). Chain these predictions together, and you can go from pure noise all the way back to a clean image.\n",
    "\n",
    "The magic: once the network learns to denoise, you can start from **pure random noise** and generate entirely new images that never existed in the training set.\n",
    "\n",
    "### ü§î Think About This\n",
    "\n",
    "If I gave you a completely noisy image and asked you to denoise it, what information would you need?\n",
    "\n",
    "You would need to know:\n",
    "1. **How noisy** the image is (a little noisy? a lot?)\n",
    "2. **What kind of images** are possible (faces? digits? landscapes?)\n",
    "\n",
    "The first is provided by the **timestep**. The second is learned from the **training data**. Keep these two ideas in mind ‚Äî they drive every design decision."
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: The Math\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_04_the_math.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_04_the_math"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Mathematics\n",
    "\n",
    "### The Forward Process\n",
    "\n",
    "At any timestep $t$, we can jump directly from the clean image $x_0$ to the noisy version $x_t$ using:\n",
    "\n",
    "$$x_t = \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\n",
    "\n",
    "**What this says computationally:** Mix the original image with random noise. The parameter $\\bar{\\alpha}_t$ controls the ratio:\n",
    "- When $\\bar{\\alpha}_t = 1$: all signal, no noise (clean image)\n",
    "- When $\\bar{\\alpha}_t = 0$: no signal, all noise (pure static)\n",
    "- In between: a blend of image and noise"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worked Example\n",
    "\n",
    "Let us plug in real numbers. Suppose we have a single pixel with value $x_0 = 0.8$, and at timestep $t$, the noise schedule gives $\\bar{\\alpha}_t = 0.5$. A random noise sample gives $\\epsilon = 0.3$.\n",
    "\n",
    "$$x_t = \\sqrt{0.5} \\times 0.8 + \\sqrt{0.5} \\times 0.3 = 0.707 \\times 0.8 + 0.707 \\times 0.3 = 0.566 + 0.212 = 0.778$$\n",
    "\n",
    "The pixel shifted from 0.8 towards the noise. As $\\bar{\\alpha}_t$ decreases towards 0, the first term shrinks and noise dominates. At $\\bar{\\alpha}_t = 0$, the original image is completely gone.\n",
    "\n",
    "### The Training Objective\n",
    "\n",
    "The neural network learns to predict the noise $\\epsilon$ that was added. The simplified loss is:\n",
    "\n",
    "$$\\mathcal{L} = \\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2$$\n",
    "\n",
    "**What this says computationally:** Take the actual noise that was added, subtract the model's prediction of that noise, and square the difference. This is just mean squared error between the true noise and predicted noise."
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Noise Schedule\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_05_noise_schedule.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_05_noise_schedule"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's Build It ‚Äî Component by Component\n",
    "\n",
    "### 4.1 The Noise Schedule\n",
    "\n",
    "The noise schedule defines how quickly we destroy the image. We start with $\\beta_t$ values (noise rates) and compute $\\bar{\\alpha}_t$ from them."
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "TIMESTEPS = 1000\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    \"\"\"Linear schedule from Ho et al. (2020).\n",
    "    Beta increases linearly from 0.0001 to 0.02.\n",
    "    \"\"\"\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "# Compute all schedule quantities\n",
    "betas = linear_beta_schedule(TIMESTEPS)\n",
    "alphas = 1.0 - betas\n",
    "alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "sqrt_alpha_bars = torch.sqrt(alpha_bars)\n",
    "sqrt_one_minus_alpha_bars = torch.sqrt(1.0 - alpha_bars)\n",
    "\n",
    "print(f\"alpha_bar at t=0:   {alpha_bars[0]:.4f} (almost no noise)\")\n",
    "print(f\"alpha_bar at t=500: {alpha_bars[500]:.4f} (half-noised)\")\n",
    "print(f\"alpha_bar at t=999: {alpha_bars[999]:.4f} (nearly pure noise)\")"
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize the noise schedule\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].plot(betas.numpy(), color='#e53935', linewidth=2)\n",
    "axes[0].set_xlabel('Timestep t', fontsize=11)\n",
    "axes[0].set_ylabel(r'$\\beta_t$', fontsize=13)\n",
    "axes[0].set_title('Noise Rate (Beta)', fontsize=13)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(alpha_bars.numpy(), color='#1565c0', linewidth=2)\n",
    "axes[1].set_xlabel('Timestep t', fontsize=11)\n",
    "axes[1].set_ylabel(r'$\\bar{\\alpha}_t$', fontsize=13)\n",
    "axes[1].set_title('Signal Retention (Alpha Bar)', fontsize=13)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Left: noise rate increases over time.\")\n",
    "print(\"Right: signal retention decreases ‚Äî by t=1000, almost no original signal remains.\")"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Forward Process Code\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_06_forward_process_code.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_06_forward_process_code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Forward Process"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion(x_0, t, sqrt_alpha_bar, sqrt_one_minus_alpha_bar):\n",
    "    \"\"\"Noise an image to timestep t in one step.\n",
    "\n",
    "    Args:\n",
    "        x_0: Clean images, shape (B, C, H, W)\n",
    "        t: Timestep indices, shape (B,)\n",
    "        sqrt_alpha_bar: Precomputed sqrt(alpha_bar), shape (T,)\n",
    "        sqrt_one_minus_alpha_bar: Precomputed sqrt(1-alpha_bar), shape (T,)\n",
    "\n",
    "    Returns:\n",
    "        x_t: Noised images\n",
    "        noise: The noise that was added (needed for training)\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x_0)\n",
    "\n",
    "    # Gather the schedule values for each sample's timestep\n",
    "    s_ab = sqrt_alpha_bar[t].view(-1, 1, 1, 1)\n",
    "    s_omab = sqrt_one_minus_alpha_bar[t].view(-1, 1, 1, 1)\n",
    "\n",
    "    x_t = s_ab * x_0 + s_omab * noise\n",
    "    return x_t, noise"
   ],
   "id": "cell_10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Forward Visualization\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_07_forward_visualization.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_07_forward_visualization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualize: a single MNIST digit being progressively noised\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Scale to [-1, 1]\n",
    "])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True,\n",
    "                                transform=transform)\n",
    "\n",
    "# Pick a digit\n",
    "sample_img = train_dataset[0][0].unsqueeze(0)  # (1, 1, 28, 28)\n",
    "\n",
    "timesteps_to_show = [0, 50, 150, 300, 500, 750, 999]\n",
    "fig, axes = plt.subplots(1, len(timesteps_to_show), figsize=(18, 3))\n",
    "\n",
    "for ax, t_val in zip(axes, timesteps_to_show):\n",
    "    t = torch.tensor([t_val])\n",
    "    noised, _ = forward_diffusion(sample_img, t,\n",
    "                                   sqrt_alpha_bars, sqrt_one_minus_alpha_bars)\n",
    "    ax.imshow(noised[0, 0].numpy(), cmap='gray')\n",
    "    ax.set_title(f't = {t_val}', fontsize=11)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Forward Process: Gradually Adding Noise to a Digit', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"The digit is progressively destroyed until only noise remains.\")"
   ],
   "id": "cell_11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Unet Model\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_08_unet_model.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_08_unet_model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 The Denoising Model (Simple U-Net)\n",
    "\n",
    "We need a neural network that takes a noisy image and timestep, and predicts the noise. We use a minimal U-Net with time embedding."
   ],
   "id": "cell_12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    \"\"\"Sinusoidal timestep embedding, like positional encoding.\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t.float().unsqueeze(1) * emb.unsqueeze(0)\n",
    "        return torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)"
   ],
   "id": "cell_13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Conv -> GroupNorm -> SiLU, with time embedding injection.\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, time_dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.GroupNorm(8, out_ch),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.time_proj = nn.Linear(time_dim, out_ch)\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        h = self.conv(x)\n",
    "        # Add time embedding (broadcast over spatial dims)\n",
    "        h = h + self.time_proj(t_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "        return h"
   ],
   "id": "cell_14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleUNet(nn.Module):\n",
    "    \"\"\"Minimal U-Net for 28x28 MNIST images.\"\"\"\n",
    "    def __init__(self, in_ch=1, base_ch=32, time_dim=64):\n",
    "        super().__init__()\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalTimeEmbedding(time_dim),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = ConvBlock(in_ch, base_ch, time_dim)\n",
    "        self.enc2 = ConvBlock(base_ch, base_ch * 2, time_dim)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ConvBlock(base_ch * 2, base_ch * 2, time_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.dec2 = ConvBlock(base_ch * 4, base_ch, time_dim)  # skip connection\n",
    "        self.dec1 = ConvBlock(base_ch * 2, base_ch, time_dim)\n",
    "\n",
    "        self.final = nn.Conv2d(base_ch, in_ch, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_embed(t)\n",
    "\n",
    "        # Encoder\n",
    "        h1 = self.enc1(x, t_emb)               # (B, 32, 28, 28)\n",
    "        h2 = self.enc2(self.pool(h1), t_emb)    # (B, 64, 14, 14)\n",
    "\n",
    "        # Bottleneck\n",
    "        h = self.bottleneck(self.pool(h2), t_emb)  # (B, 64, 7, 7)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        h = self.up(h)                           # (B, 64, 14, 14)\n",
    "        h = self.dec2(torch.cat([h, h2], dim=1), t_emb)  # (B, 32, 14, 14)\n",
    "        h = self.up(h)                           # (B, 32, 28, 28)\n",
    "        h = self.dec1(torch.cat([h, h1], dim=1), t_emb)  # (B, 32, 28, 28)\n",
    "\n",
    "        return self.final(h)                     # (B, 1, 28, 28)\n",
    "\n",
    "\n",
    "model = SimpleUNet().to(device)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"SimpleUNet parameters: {n_params:,}\")"
   ],
   "id": "cell_15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Training\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_09_training.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_09_training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The Training Loop"
   ],
   "id": "cell_16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "losses = []\n",
    "\n",
    "print(\"Training the diffusion model on MNIST...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_losses = []\n",
    "    for batch_idx, (images, _) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Sample random timesteps for each image\n",
    "        t = torch.randint(0, TIMESTEPS, (images.shape[0],), device=device)\n",
    "\n",
    "        # Forward process: add noise\n",
    "        x_t, noise = forward_diffusion(images, t,\n",
    "                                        sqrt_alpha_bars.to(device),\n",
    "                                        sqrt_one_minus_alpha_bars.to(device))\n",
    "\n",
    "        # Model predicts the noise\n",
    "        predicted_noise = model(x_t, t)\n",
    "\n",
    "        # MSE loss between true noise and predicted noise\n",
    "        loss = F.mse_loss(predicted_noise, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ],
   "id": "cell_17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Training loss curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, marker='o', color='#1565c0', linewidth=2, markersize=6)\n",
    "plt.xlabel('Epoch', fontsize=11)\n",
    "plt.ylabel('Mean MSE Loss', fontsize=11)\n",
    "plt.title('Diffusion Model Training Loss', fontsize=13)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"The loss should decrease steadily ‚Äî the model is learning to predict noise.\")"
   ],
   "id": "cell_18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo1 Reverse\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_10_todo1_reverse.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_10_todo1_reverse"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîß Your Turn\n",
    "\n",
    "### TODO 1: Implement the Sampling / Generation Function\n",
    "\n",
    "This is the reverse process ‚Äî going from pure noise back to a clean image. At each timestep, the model predicts the noise, and we subtract a carefully scaled version of it.\n",
    "\n",
    "The formula for each reverse step from $t$ to $t-1$:\n",
    "\n",
    "$$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\cdot \\epsilon_\\theta(x_t, t) \\right) + \\sigma_t \\cdot z$$\n",
    "\n",
    "where $z \\sim \\mathcal{N}(0, I)$ for $t > 1$ and $z = 0$ for $t = 1$, and $\\sigma_t = \\sqrt{\\beta_t}$.\n",
    "\n",
    "**What this says computationally:** Take the current noisy image, subtract the model's noise prediction (scaled appropriately), and add a small amount of fresh noise. At the very last step, skip the fresh noise to get a clean output."
   ],
   "id": "cell_19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, n_samples=16, img_size=28):\n",
    "    \"\"\"Generate images via the reverse diffusion process.\n",
    "\n",
    "    Args:\n",
    "        model: Trained SimpleUNet\n",
    "        n_samples: Number of images to generate\n",
    "        img_size: Spatial size (28 for MNIST)\n",
    "\n",
    "    Returns:\n",
    "        Generated images, shape (n_samples, 1, img_size, img_size)\n",
    "        history: List of intermediate snapshots\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    betas_d = betas.to(device)\n",
    "    alphas_d = alphas.to(device)\n",
    "    alpha_bars_d = alpha_bars.to(device)\n",
    "\n",
    "    # Start from pure Gaussian noise\n",
    "    x = torch.randn(n_samples, 1, img_size, img_size, device=device)\n",
    "    history = [x.cpu().clone()]\n",
    "\n",
    "    # ============ TODO ============\n",
    "    # Reverse loop: t = TIMESTEPS-1 down to 0\n",
    "    for t_val in reversed(range(TIMESTEPS)):\n",
    "        t = torch.full((n_samples,), t_val, device=device, dtype=torch.long)\n",
    "\n",
    "        # Step 1: Get the model's noise prediction\n",
    "        predicted_noise = ???  # YOUR CODE HERE\n",
    "\n",
    "        # Step 2: Compute scaling coefficients\n",
    "        beta_t = betas_d[t_val]\n",
    "        alpha_t = alphas_d[t_val]\n",
    "        alpha_bar_t = alpha_bars_d[t_val]\n",
    "        noise_coeff = beta_t / torch.sqrt(1.0 - alpha_bar_t)\n",
    "\n",
    "        # Step 3: Compute x_{t-1}\n",
    "        x = ???  # YOUR CODE HERE: (1/sqrt(alpha_t)) * (x - noise_coeff * predicted_noise)\n",
    "\n",
    "        # Step 4: Add stochastic noise (except at final step t=0)\n",
    "        if t_val > 0:\n",
    "            z = torch.randn_like(x)\n",
    "            sigma_t = torch.sqrt(beta_t)\n",
    "            x = ???  # YOUR CODE HERE: x + sigma_t * z\n",
    "\n",
    "        if t_val % 100 == 0 or t_val == 0:\n",
    "            history.append(x.cpu().clone())\n",
    "    # ==============================\n",
    "\n",
    "    model.train()\n",
    "    return x, history"
   ],
   "id": "cell_20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification\n",
    "try:\n",
    "    test_samples, test_history = sample(model, n_samples=4)\n",
    "    assert test_samples.shape == (4, 1, 28, 28), f\"Wrong shape: {test_samples.shape}\"\n",
    "    print(\"‚úÖ Sampling works! Generated 4 test images.\")\n",
    "    print(f\"   Pixel range: [{test_samples.min():.2f}, {test_samples.max():.2f}]\")\n",
    "    print(f\"   Snapshots: {len(test_history)}\")\n",
    "except NameError:\n",
    "    print(\"‚ùå Replace the ??? placeholders with your code.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ],
   "id": "cell_21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Stop And Think Solution\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_11_stop_and_think_solution.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_11_stop_and_think_solution"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ‚úã Stop and Think\n",
    "Before looking at the solution:\n",
    "1. Why do we add fresh noise $z$ at every step except the last?\n",
    "2. What would happen if we used a larger $\\sigma_t$?\n",
    "3. Why do we go from $t = T-1$ all the way to $t = 0$?\n",
    "\n",
    "*Take a minute. Then scroll down.*\n",
    "\n",
    "---"
   ],
   "id": "cell_22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ],
   "id": "cell_23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, n_samples=16, img_size=28):\n",
    "    \"\"\"Generate images via the reverse diffusion process.\"\"\"\n",
    "    model.eval()\n",
    "    betas_d = betas.to(device)\n",
    "    alphas_d = alphas.to(device)\n",
    "    alpha_bars_d = alpha_bars.to(device)\n",
    "\n",
    "    x = torch.randn(n_samples, 1, img_size, img_size, device=device)\n",
    "    history = [x.cpu().clone()]\n",
    "\n",
    "    for t_val in reversed(range(TIMESTEPS)):\n",
    "        t = torch.full((n_samples,), t_val, device=device, dtype=torch.long)\n",
    "\n",
    "        predicted_noise = model(x, t)\n",
    "\n",
    "        beta_t = betas_d[t_val]\n",
    "        alpha_t = alphas_d[t_val]\n",
    "        alpha_bar_t = alpha_bars_d[t_val]\n",
    "        noise_coeff = beta_t / torch.sqrt(1.0 - alpha_bar_t)\n",
    "\n",
    "        x = (1.0 / torch.sqrt(alpha_t)) * (x - noise_coeff * predicted_noise)\n",
    "\n",
    "        if t_val > 0:\n",
    "            z = torch.randn_like(x)\n",
    "            sigma_t = torch.sqrt(beta_t)\n",
    "            x = x + sigma_t * z\n",
    "\n",
    "        if t_val % 100 == 0 or t_val == 0:\n",
    "            history.append(x.cpu().clone())\n",
    "\n",
    "    model.train()\n",
    "    return x, history\n",
    "\n",
    "\n",
    "generated_images, generation_history = sample(model, n_samples=16)\n",
    "print(f\"Generated {generated_images.shape[0]} images!\")"
   ],
   "id": "cell_24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Todo2 Cosine\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_12_todo2_cosine.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_12_todo2_cosine"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Implement a Cosine Noise Schedule\n",
    "\n",
    "The linear schedule works, but a **cosine schedule** (Nichol & Dhariwal, 2021) spends more time at intermediate noise levels:\n",
    "\n",
    "$$\\bar{\\alpha}_t = \\frac{f(t)}{f(0)}, \\quad f(t) = \\cos\\!\\left(\\frac{t/T + s}{1 + s} \\cdot \\frac{\\pi}{2}\\right)^2, \\quad s = 0.008$$"
   ],
   "id": "cell_25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"Cosine noise schedule.\n",
    "\n",
    "    Hints:\n",
    "        1. Compute f(t) for t = 0, 1, ..., T\n",
    "        2. alpha_bars = f(t) / f(0)\n",
    "        3. betas = 1 - alpha_bar[t] / alpha_bar[t-1]\n",
    "        4. Clamp betas to [0, 0.999]\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    t = torch.linspace(0, timesteps, steps)\n",
    "\n",
    "    # ============ TODO ============\n",
    "    f_t = ???  # YOUR CODE: cos((t/T + s)/(1+s) * pi/2)^2\n",
    "    alpha_bars_cos = ???  # YOUR CODE: f(t) / f(0)\n",
    "    betas_cos = ???  # YOUR CODE: 1 - alpha_bar[t] / alpha_bar[t-1]\n",
    "    betas_cos = ???  # YOUR CODE: clamp to [0, 0.999]\n",
    "    # ==============================\n",
    "\n",
    "    return betas_cos"
   ],
   "id": "cell_26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Verification ‚Äî compare schedules\n",
    "try:\n",
    "    cosine_betas = cosine_beta_schedule(TIMESTEPS)\n",
    "    cosine_alphas = 1.0 - cosine_betas\n",
    "    cosine_alpha_bars = torch.cumprod(cosine_alphas, dim=0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(alpha_bars.numpy(), label='Linear', color='#1565c0', linewidth=2.5)\n",
    "    ax.plot(cosine_alpha_bars.numpy(), label='Cosine', color='#e53935',\n",
    "            linewidth=2.5, linestyle='--')\n",
    "    ax.set_xlabel('Timestep t', fontsize=12)\n",
    "    ax.set_ylabel(r'$\\bar{\\alpha}_t$', fontsize=14)\n",
    "    ax.set_title('Linear vs Cosine Noise Schedule', fontsize=14)\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"‚úÖ Cosine schedule implemented!\")\n",
    "    print(\"Notice: cosine spends more time at intermediate noise levels.\")\n",
    "except NameError:\n",
    "    print(\"‚ùå Replace the ??? placeholders.\")"
   ],
   "id": "cell_27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Results\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_13_results.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_13_results"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Putting It All Together"
   ],
   "id": "cell_28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Progressive denoising: watch a digit emerge from noise\n",
    "torch.manual_seed(123)\n",
    "_, full_history = sample(model, n_samples=1)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(full_history), figsize=(20, 3))\n",
    "for idx, (ax, snapshot) in enumerate(zip(axes, full_history)):\n",
    "    img = snapshot[0, 0].numpy()\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    if idx == 0:\n",
    "        ax.set_title('t=999\\n(noise)', fontsize=9)\n",
    "    elif idx == len(full_history) - 1:\n",
    "        ax.set_title('t=0\\n(clean)', fontsize=9)\n",
    "    else:\n",
    "        t_approx = 999 - idx * 100\n",
    "        ax.set_title(f't‚âà{max(t_approx, 0)}', fontsize=9)\n",
    "\n",
    "plt.suptitle('Reverse Process: From Pure Noise to a Digit', fontsize=14, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Watch the digit emerge ‚Äî first the rough shape, then fine details.\")\n",
    "print(\"This is exactly like an artist refining a sketch.\")"
   ],
   "id": "cell_29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üéØ Final Output"
   ],
   "id": "cell_30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a grid of MNIST digits from pure noise\n",
    "torch.manual_seed(42)\n",
    "final_images, _ = sample(model, n_samples=64)\n",
    "final_images = torch.clamp(final_images, -1, 1)\n",
    "\n",
    "fig, axes = plt.subplots(8, 8, figsize=(12, 12))\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        idx = i * 8 + j\n",
    "        axes[i, j].imshow(final_images[idx, 0].cpu().numpy(), cmap='gray')\n",
    "        axes[i, j].axis('off')\n",
    "\n",
    "plt.suptitle('Generated MNIST Digits ‚Äî From Pure Noise', fontsize=16, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "cell_31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Side-by-side: Real vs Generated\n",
    "fig, axes = plt.subplots(2, 10, figsize=(16, 4))\n",
    "\n",
    "for i in range(10):\n",
    "    real_img = train_dataset[i * 600][0][0].numpy()\n",
    "    axes[0, i].imshow(real_img, cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 5:\n",
    "        axes[0, i].set_title('Real MNIST', fontsize=12, pad=10)\n",
    "\n",
    "torch.manual_seed(7)\n",
    "comp_images, _ = sample(model, n_samples=10)\n",
    "comp_images = torch.clamp(comp_images, -1, 1)\n",
    "\n",
    "for i in range(10):\n",
    "    axes[1, i].imshow(comp_images[i, 0].cpu().numpy(), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 5:\n",
    "        axes[1, i].set_title('Generated', fontsize=12, pad=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Top: Real MNIST digits. Bottom: Generated from pure noise.\")"
   ],
   "id": "cell_32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Breaks For Text\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_14_breaks_for_text.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_14_breaks_for_text"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The Problem ‚Äî Why This Breaks for Text\n",
    "\n",
    "We have a working image diffusion model. Now let us confront the fundamental question that motivates this entire series: **why can't we apply this directly to text?**\n",
    "\n",
    "### Continuous vs Discrete\n",
    "\n",
    "Images live in a **continuous** space. Each pixel is a floating-point number. Adding Gaussian noise to pixel 0.8 gives you 0.73 or 0.85 ‚Äî still valid pixels.\n",
    "\n",
    "Text lives in a **discrete** space. Each token is an integer ‚Äî an index into a vocabulary. The word \"cat\" might be token 3421, \"dog\" might be token 7856. What does token 5638.5 mean? Nothing. It is not a real word."
   ],
   "id": "cell_33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate why Gaussian noise fails for text\n",
    "print(\"=\" * 55)\n",
    "print(\"EXPERIMENT: Gaussian Noise on Token IDs\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "vocab = {42: \"the\", 3421: \"cat\", 891: \"sat\", 156: \"on\", 7856: \"dog\", 2001: \"mat\"}\n",
    "\n",
    "sentence = \"the cat sat on the mat\"\n",
    "tokens = [42, 3421, 891, 156, 42, 2001]\n",
    "print(f\"\\nOriginal: '{sentence}'\")\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "\n",
    "print(f\"\\n--- Adding Gaussian noise (sigma=500) ---\")\n",
    "np.random.seed(42)\n",
    "for token_id, word in zip(tokens, sentence.split()):\n",
    "    noise = np.random.normal(0, 500)\n",
    "    noised_id = token_id + noise\n",
    "    rounded_id = int(round(noised_id))\n",
    "    noised_word = vocab.get(rounded_id, \"???\")\n",
    "    print(f\"  '{word}' ({token_id:5d}) + noise {noise:+8.1f} \"\n",
    "          f\"= {noised_id:8.1f} ‚Üí '{noised_word}'\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Most noised tokens map to NOTHING in the vocabulary!\")\n",
    "print(f\"   Gaussian noise is MEANINGLESS for discrete data.\")"
   ],
   "id": "cell_34"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three Approaches to Fix This\n",
    "\n",
    "Researchers have proposed three solutions:\n",
    "\n",
    "1. **Continuous embedding diffusion** (Diffusion-LM, 2022) ‚Äî embed tokens into continuous vectors, run Gaussian diffusion on embeddings, round back. Problem: rounding introduces errors.\n",
    "\n",
    "2. **Masked diffusion** (MDLM, LLaDA) ‚Äî replace tokens with [MASK] instead of adding noise. Masking is a natural \"noise\" for discrete data. Simplest and most successful.\n",
    "\n",
    "3. **Score-based discrete diffusion** (SEDD) ‚Äî define transition probabilities directly in discrete space. Elegant but complex."
   ],
   "id": "cell_35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview: what masked \"noise\" looks like\n",
    "print(\"=\" * 55)\n",
    "print(\"PREVIEW: Masked Diffusion (Next Notebook)\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "words = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "print(f\"\\nOriginal:  {' '.join(words)}\\n\")\n",
    "\n",
    "np.random.seed(0)\n",
    "for ratio in [0.0, 0.17, 0.33, 0.50, 0.67, 0.83, 1.0]:\n",
    "    masked = [\"[M]\" if np.random.random() < ratio else w for w in words]\n",
    "    pct = masked.count(\"[M]\") / len(masked) * 100\n",
    "    print(f\"  t={ratio:.2f} ({pct:3.0f}% masked):  {' '.join(masked)}\")\n",
    "\n",
    "print(\"\\nEvery intermediate state is interpretable!\")\n",
    "print(\"No 'half-cat' nonsense. Tokens are present or masked.\")\n",
    "print(\"\\nüí° Masking IS the noise process for text.\")"
   ],
   "id": "cell_36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "narration"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üéß Listen: Closing\n",
    "from IPython.display import Audio, display\n",
    "import os as _os\n",
    "_f = \"/content/narration/01_15_closing.mp3\"\n",
    "if _os.path.exists(_f):\n",
    "    display(Audio(_f))\n",
    "else:\n",
    "    print(\"Run the first cell to download narration audio.\")"
   ],
   "id": "narration_01_15_closing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection and Next Steps\n",
    "\n",
    "### ü§î Reflection Questions\n",
    "\n",
    "1. **Why does the model need to know the timestep $t$?** Without it, the model cannot distinguish between slightly noisy images (needing gentle correction) and heavily noisy images (needing aggressive reconstruction).\n",
    "\n",
    "2. **What would happen with a very aggressive noise schedule?** If the image is destroyed too quickly, most of the trajectory is spent going from pure noise to slightly-less-pure noise ‚Äî the model has few steps to learn fine details.\n",
    "\n",
    "3. **Could we use a Transformer instead of a U-Net?** Yes! The Diffusion Transformer (DiT) does exactly this and powers DALL-E 3 and Stable Diffusion 3. The diffusion math stays the same ‚Äî only the backbone changes.\n",
    "\n",
    "### üèÜ Optional Challenges\n",
    "\n",
    "1. Train on Fashion-MNIST and compare generation quality\n",
    "2. Implement classifier-free guidance for conditional generation\n",
    "3. Implement DDIM sampling (deterministic, fewer steps needed)\n",
    "\n",
    "---\n",
    "\n",
    "**Up Next ‚Äî Notebook 2:** *Masked Diffusion for Text.* We will see how replacing Gaussian noise with token masking gives us a diffusion process that works for discrete text ‚Äî and it turns out to be just BERT training, generalized to all masking ratios."
   ],
   "id": "cell_37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "chatbot"
    ],
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title üí¨ AI Teaching Assistant ‚Äî Click ‚ñ∂ to start\n",
    "#@markdown This AI chatbot reads your notebook and can answer questions about any concept, code, or exercise.\n",
    "\n",
    "import json as _json\n",
    "import requests as _requests\n",
    "from google.colab import output as _output\n",
    "from IPython.display import display, HTML as _HTML, Markdown as _Markdown\n",
    "\n",
    "# --- Read notebook content for context ---\n",
    "def _get_notebook_context():\n",
    "    try:\n",
    "        from google.colab import _message\n",
    "        nb = _message.blocking_request(\"get_ipynb\", request=\"\", timeout_sec=10)\n",
    "        cells = nb.get(\"ipynb\", {}).get(\"cells\", [])\n",
    "        parts = []\n",
    "        for cell in cells:\n",
    "            src = \"\".join(cell.get(\"source\", []))\n",
    "            tags = cell.get(\"metadata\", {}).get(\"tags\", [])\n",
    "            if \"chatbot\" in tags:\n",
    "                continue\n",
    "            if src.strip():\n",
    "                ct = cell.get(\"cell_type\", \"unknown\")\n",
    "                parts.append(f\"[{ct.upper()}]\\n{src}\")\n",
    "        return \"\\n\\n---\\n\\n\".join(parts)\n",
    "    except Exception:\n",
    "        return \"Notebook content unavailable.\"\n",
    "\n",
    "_NOTEBOOK_CONTEXT = _get_notebook_context()\n",
    "_CHAT_HISTORY = []\n",
    "_API_URL = \"https://course-creator-brown.vercel.app/api/chat\"\n",
    "\n",
    "def _notebook_chat(question):\n",
    "    global _CHAT_HISTORY\n",
    "    try:\n",
    "        resp = _requests.post(_API_URL, json={\n",
    "            'question': question,\n",
    "            'context': _NOTEBOOK_CONTEXT[:100000],\n",
    "            'history': _CHAT_HISTORY[-10:],\n",
    "        }, timeout=60)\n",
    "        data = resp.json()\n",
    "        answer = data.get('answer', 'Sorry, I could not generate a response.')\n",
    "        _CHAT_HISTORY.append({'role': 'user', 'content': question})\n",
    "        _CHAT_HISTORY.append({'role': 'assistant', 'content': answer})\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f'Error connecting to teaching assistant: {str(e)}'\n",
    "\n",
    "_output.register_callback('notebook_chat', _notebook_chat)\n",
    "\n",
    "def ask(question):\n",
    "    \"\"\"Ask the AI teaching assistant a question about this notebook.\"\"\"\n",
    "    answer = _notebook_chat(question)\n",
    "    display(_Markdown(answer))\n",
    "\n",
    "print(\"\\u2705 AI Teaching Assistant is ready!\")\n",
    "print(\"\\U0001f4a1 Use the chat below, or call ask(\\'your question\\') in any cell.\")\n",
    "\n",
    "# --- Display chat widget ---\n",
    "display(_HTML('''<style>\n  .vc-wrap{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;max-width:100%;border-radius:16px;overflow:hidden;box-shadow:0 4px 24px rgba(0,0,0,.12);background:#fff;border:1px solid #e5e7eb}\n  .vc-hdr{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;padding:16px 20px;display:flex;align-items:center;gap:12px}\n  .vc-avatar{width:42px;height:42px;background:rgba(255,255,255,.2);border-radius:50%;display:flex;align-items:center;justify-content:center;font-size:22px}\n  .vc-hdr h3{font-size:16px;font-weight:600;margin:0}\n  .vc-hdr p{font-size:12px;opacity:.85;margin:2px 0 0}\n  .vc-msgs{height:420px;overflow-y:auto;padding:16px;background:#f8f9fb;display:flex;flex-direction:column;gap:10px}\n  .vc-msg{display:flex;flex-direction:column;animation:vc-fade .25s ease}\n  .vc-msg.user{align-items:flex-end}\n  .vc-msg.bot{align-items:flex-start}\n  .vc-bbl{max-width:85%;padding:10px 14px;border-radius:16px;font-size:14px;line-height:1.55;word-wrap:break-word}\n  .vc-msg.user .vc-bbl{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border-bottom-right-radius:4px}\n  .vc-msg.bot .vc-bbl{background:#fff;color:#1a1a2e;border:1px solid #e8e8e8;border-bottom-left-radius:4px}\n  .vc-bbl code{background:rgba(0,0,0,.07);padding:2px 6px;border-radius:4px;font-size:13px;font-family:'Fira Code',monospace}\n  .vc-bbl pre{background:#1e1e2e;color:#cdd6f4;padding:12px;border-radius:8px;overflow-x:auto;margin:8px 0;font-size:13px}\n  .vc-bbl pre code{background:none;padding:0;color:inherit}\n  .vc-bbl h3,.vc-bbl h4{margin:10px 0 4px;font-size:15px}\n  .vc-bbl ul,.vc-bbl ol{margin:4px 0;padding-left:20px}\n  .vc-bbl li{margin:2px 0}\n  .vc-chips{display:flex;flex-wrap:wrap;gap:8px;padding:0 16px 12px;background:#f8f9fb}\n  .vc-chip{background:#fff;border:1px solid #d1d5db;border-radius:20px;padding:6px 14px;font-size:12px;cursor:pointer;transition:all .15s;color:#4b5563}\n  .vc-chip:hover{border-color:#667eea;color:#667eea;background:#f0f0ff}\n  .vc-input{display:flex;padding:12px 16px;background:#fff;border-top:1px solid #eee;gap:8px}\n  .vc-input input{flex:1;padding:10px 16px;border:2px solid #e8e8e8;border-radius:24px;font-size:14px;outline:none;transition:border-color .2s}\n  .vc-input input:focus{border-color:#667eea}\n  .vc-input button{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:#fff;border:none;border-radius:50%;width:42px;height:42px;cursor:pointer;display:flex;align-items:center;justify-content:center;font-size:18px;transition:transform .1s}\n  .vc-input button:hover{transform:scale(1.05)}\n  .vc-input button:disabled{opacity:.5;cursor:not-allowed;transform:none}\n  .vc-typing{display:flex;gap:5px;padding:4px 0}\n  .vc-typing span{width:8px;height:8px;background:#667eea;border-radius:50%;animation:vc-bounce 1.4s infinite ease-in-out}\n  .vc-typing span:nth-child(2){animation-delay:.2s}\n  .vc-typing span:nth-child(3){animation-delay:.4s}\n  @keyframes vc-bounce{0%,80%,100%{transform:scale(0)}40%{transform:scale(1)}}\n  @keyframes vc-fade{from{opacity:0;transform:translateY(8px)}to{opacity:1;transform:translateY(0)}}\n  .vc-note{text-align:center;font-size:11px;color:#9ca3af;padding:8px 16px 12px;background:#fff}\n</style>\n<div class=\"vc-wrap\">\n  <div class=\"vc-hdr\">\n    <div class=\"vc-avatar\">&#129302;</div>\n    <div>\n      <h3>Vizuara Teaching Assistant</h3>\n      <p>Ask me anything about this notebook</p>\n    </div>\n  </div>\n  <div class=\"vc-msgs\" id=\"vcMsgs\">\n    <div class=\"vc-msg bot\">\n      <div class=\"vc-bbl\">&#128075; Hi! I've read through this entire notebook. Ask me about any concept, code block, or exercise &mdash; I'm here to help you learn!</div>\n    </div>\n  </div>\n  <div class=\"vc-chips\" id=\"vcChips\">\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Explain the main concept</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Help with the TODO exercise</span>\n    <span class=\"vc-chip\" onclick=\"vcAsk(this.textContent)\">Summarize what I learned</span>\n  </div>\n  <div class=\"vc-input\">\n    <input type=\"text\" id=\"vcIn\" placeholder=\"Ask about concepts, code, exercises...\" />\n    <button id=\"vcSend\" onclick=\"vcSendMsg()\">&#10148;</button>\n  </div>\n  <div class=\"vc-note\">AI-generated &middot; Verify important information &middot; <a href=\"#\" onclick=\"vcClear();return false\" style=\"color:#667eea\">Clear chat</a></div>\n</div>\n<script>\n(function(){\n  var msgs=document.getElementById('vcMsgs'),inp=document.getElementById('vcIn'),\n      btn=document.getElementById('vcSend'),chips=document.getElementById('vcChips');\n\n  function esc(s){var d=document.createElement('div');d.textContent=s;return d.innerHTML}\n\n  function md(t){\n    return t\n      .replace(/```(\\w*)\\n([\\s\\S]*?)```/g,function(_,l,c){return '<pre><code>'+esc(c)+'</code></pre>'})\n      .replace(/`([^`]+)`/g,'<code>$1</code>')\n      .replace(/\\*\\*([^*]+)\\*\\*/g,'<strong>$1</strong>')\n      .replace(/\\*([^*]+)\\*/g,'<em>$1</em>')\n      .replace(/^#### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^### (.+)$/gm,'<h4>$1</h4>')\n      .replace(/^## (.+)$/gm,'<h3>$1</h3>')\n      .replace(/^\\d+\\. (.+)$/gm,'<li>$1</li>')\n      .replace(/^- (.+)$/gm,'<li>$1</li>')\n      .replace(/\\n\\n/g,'<br><br>')\n      .replace(/\\n/g,'<br>');\n  }\n\n  function addMsg(text,isUser){\n    var m=document.createElement('div');m.className='vc-msg '+(isUser?'user':'bot');\n    var b=document.createElement('div');b.className='vc-bbl';\n    b.innerHTML=isUser?esc(text):md(text);\n    m.appendChild(b);msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function showTyping(){\n    var m=document.createElement('div');m.className='vc-msg bot';m.id='vcTyping';\n    m.innerHTML='<div class=\"vc-bbl\"><div class=\"vc-typing\"><span></span><span></span><span></span></div></div>';\n    msgs.appendChild(m);msgs.scrollTop=msgs.scrollHeight;\n  }\n\n  function hideTyping(){var e=document.getElementById('vcTyping');if(e)e.remove()}\n\n  window.vcSendMsg=function(){\n    var q=inp.value.trim();if(!q)return;\n    inp.value='';chips.style.display='none';\n    addMsg(q,true);showTyping();btn.disabled=true;\n    google.colab.kernel.invokeFunction('notebook_chat',[q],{})\n      .then(function(r){\n        hideTyping();\n        var a=r.data['application/json'];\n        addMsg(typeof a==='string'?a:JSON.stringify(a),false);\n      })\n      .catch(function(){\n        hideTyping();\n        addMsg('Sorry, I encountered an error. Please check your internet connection and try again.',false);\n      })\n      .finally(function(){btn.disabled=false;inp.focus()});\n  };\n\n  window.vcAsk=function(q){inp.value=q;vcSendMsg()};\n  window.vcClear=function(){\n    msgs.innerHTML='<div class=\"vc-msg bot\"><div class=\"vc-bbl\">&#128075; Chat cleared. Ask me anything!</div></div>';\n    chips.style.display='flex';\n  };\n\n  inp.addEventListener('keypress',function(e){if(e.key==='Enter')vcSendMsg()});\n  inp.focus();\n})();\n</script>'''))"
   ],
   "id": "vizuara_chatbot"
  }
 ]
}