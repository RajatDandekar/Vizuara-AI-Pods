{
  "title": "Foundations of Language Modeling",
  "slug": "foundations-of-language-modeling",
  "courseSlug": "build-llm",
  "order": 2,
  "description": "From N-grams to Neural LMs to Transformers \u2014 trace the evolution of language modeling and understand why each breakthrough was necessary.",
  "difficulty": "beginner",
  "estimatedHours": 4,
  "prerequisites": [],
  "tags": [
    "n-grams",
    "neural-language-models",
    "transformers",
    "nlp-history"
  ],
  "article": {
    "notionUrl": null,
    "figureUrls": {}
  },
  "notebooks": [
    {
      "title": "N-gram Language Models: Predicting the Next Word by Counting",
      "slug": "01-ngram_language_models",
      "objective": "",
      "colabUrl": "https://colab.research.google.com/github/RajatDandekar/Vizuara-AI-Pods/blob/main/public/notebooks/foundations-of-language-modeling/01_ngram_language_models.ipynb",
      "downloadPath": "/notebooks/foundations-of-language-modeling/01_ngram_language_models.ipynb",
      "estimatedMinutes": 45,
      "todoCount": 2,
      "order": 1,
      "hasNarration": false
    },
    {
      "title": "Neural Language Models and Word Embeddings",
      "slug": "02-neural_language_models",
      "objective": "",
      "colabUrl": "https://colab.research.google.com/github/RajatDandekar/Vizuara-AI-Pods/blob/main/public/notebooks/foundations-of-language-modeling/02_neural_language_models.ipynb",
      "downloadPath": "/notebooks/foundations-of-language-modeling/02_neural_language_models.ipynb",
      "estimatedMinutes": 60,
      "todoCount": 2,
      "order": 2,
      "hasNarration": false
    },
    {
      "title": "Self-Attention and the Transformer",
      "slug": "03-self_attention_and_transformer",
      "objective": "",
      "colabUrl": "https://colab.research.google.com/github/RajatDandekar/Vizuara-AI-Pods/blob/main/public/notebooks/foundations-of-language-modeling/03_self_attention_and_transformer.ipynb",
      "downloadPath": "/notebooks/foundations-of-language-modeling/03_self_attention_and_transformer.ipynb",
      "estimatedMinutes": 60,
      "todoCount": 3,
      "order": 3,
      "hasNarration": false
    },
    {
      "title": "Building a Tiny Language Model (Mini-GPT)",
      "slug": "04-building_a_tiny_language_model",
      "objective": "",
      "colabUrl": "https://colab.research.google.com/github/RajatDandekar/Vizuara-AI-Pods/blob/main/public/notebooks/foundations-of-language-modeling/04_building_a_tiny_language_model.ipynb",
      "downloadPath": "/notebooks/foundations-of-language-modeling/04_building_a_tiny_language_model.ipynb",
      "estimatedMinutes": 45,
      "todoCount": 2,
      "order": 4,
      "hasNarration": false
    }
  ],
  "caseStudy": {
    "title": "Building a Domain-Specific Language Model for Intelligent Customer Support Auto-Completion",
    "subtitle": "Meridian Financial Technologies \u2014 From N-gram Baselines to Transformer Auto-Complete",
    "company": "Meridian Financial Technologies",
    "industry": "Financial Services \u2014 Customer Support",
    "description": "Meridian's engineering team builds a domain-specific Transformer language model that provides intelligent auto-completion for customer support agents, reducing response time by 42% and terminology errors from 18% to 4%.",
    "pdfPath": "/case-studies/foundations-of-language-modeling/case_study.pdf",
    "colabUrl": "https://colab.research.google.com/github/RajatDandekar/Vizuara-AI-Pods/blob/main/public/case-studies/foundations-of-language-modeling/case_study_notebook.ipynb",
    "notebookPath": "/case-studies/foundations-of-language-modeling/case_study_notebook.ipynb"
  },
  "curator": {
    "name": "Dr. Rajat Dandekar",
    "title": "Course Instructor",
    "bio": "Dr. Rajat Dandekar is a researcher and educator specializing in AI/ML, with a passion for making complex concepts accessible through intuitive explanations and hands-on learning.",
    "videoUrl": "https://drive.google.com/file/d/1xgSDKFZLU25MjUogCs4siGb5PJKSQGJN/view?usp=sharing",
    "imageUrl": "/founders/rajat.jpg"
  }
}
