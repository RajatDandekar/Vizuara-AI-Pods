{
  "title": "Understanding BERT from Scratch",
  "slug": "understanding-bert-from-scratch",
  "description": "## How a simple idea \u2014 reading in both directions \u2014 changed NLP forever",
  "difficulty": "intermediate",
  "estimatedHours": 4,
  "prerequisites": [],
  "tags": [
    "The Problem with Previous Approaches",
    "The Transformer Encoder \u2014 BERT's Engine",
    "BERT's Architecture \u2014 Putting It Together",
    "Pre-training Objective 1: Masked Language Modeling (MLM)",
    "Pre-training Objective 2: Next Sentence Prediction (NSP)"
  ],
  "article": {
    "notionUrl": null,
    "figureUrls": {}
  },
  "notebooks": [
    {
      "title": "Word Representations & The Need for BERT",
      "slug": "01-word_representations",
      "objective": "",
      "colabUrl": "https://colab.research.google.com/drive/1CZN7Qz9XO9-lg4RYdJECBPbZNTHnCDES",
      "downloadPath": "/notebooks/understanding-bert-from-scratch/01_word_representations.ipynb",
      "estimatedMinutes": 52,
      "todoCount": 2,
      "order": 1,
      "hasNarration": false
    },
    {
      "title": "Self-Attention & The Transformer Encoder from First Principles",
      "slug": "02-self_attention_transformer",
      "objective": "",
      "colabUrl": "https://colab.research.google.com/drive/1bZ_5m_Qd4mfmjKeAZE2tS_C2H8DOic-o",
      "downloadPath": "/notebooks/understanding-bert-from-scratch/02_self_attention_transformer.ipynb",
      "estimatedMinutes": 52,
      "todoCount": 2,
      "order": 2,
      "hasNarration": false
    },
    {
      "title": "BERT Architecture & Pre-training from Scratch",
      "slug": "03-bert_pretraining",
      "objective": "",
      "colabUrl": "https://colab.research.google.com/drive/1W4Q_VD_wwpBzP8DVQsGwJK_E0VfNOr9F",
      "downloadPath": "/notebooks/understanding-bert-from-scratch/03_bert_pretraining.ipynb",
      "estimatedMinutes": 48,
      "todoCount": 2,
      "order": 3,
      "hasNarration": false
    },
    {
      "title": "Fine-tuning BERT for Real Tasks",
      "slug": "04-finetuning_bert",
      "objective": "",
      "colabUrl": "https://colab.research.google.com/drive/1d1OGcwHXeehZXwBdoICEVWPVweb_I1uz",
      "downloadPath": "/notebooks/understanding-bert-from-scratch/04_finetuning_bert.ipynb",
      "estimatedMinutes": 48,
      "todoCount": 2,
      "order": 4,
      "hasNarration": false
    }
  ],
  "caseStudy": {
    "title": "Clinical NLP for Automated Medical Coding",
    "subtitle": "From Unstructured Physician Notes to Accurate ICD Billing Codes Using BERT",
    "company": "MedScribe AI",
    "industry": "Healthcare Revenue Cycle Management",
    "description": "MedScribe's rule-based system achieves an entity extraction accuracy of **68%** and an ICD code suggestion accuracy of **61%**. For context, human coders achieve approximately 85-92% accuracy depending on specialty.",
    "pdfPath": "/case-studies/understanding-bert-from-scratch/case_study.pdf",
    "colabUrl": "https://colab.research.google.com/drive/1MYvNITc3-UzX_hsmKLhHDg80D7vdimpf",
    "notebookPath": "/case-studies/understanding-bert-from-scratch/case_study_notebook.ipynb"
  },
  "curator": {
    "name": "Dr. Rajat Dandekar",
    "title": "Course Instructor",
    "bio": "Dr. Rajat Dandekar is a researcher and educator specializing in AI/ML, with a passion for making complex concepts accessible through intuitive explanations and hands-on learning.",
    "videoUrl": "https://drive.google.com/file/d/1xgSDKFZLU25MjUogCs4siGb5PJKSQGJN/view?usp=sharing",
    "imageUrl": "/founders/rajat.jpg"
  },
  "courseSlug": "build-llm",
  "order": 1
}
