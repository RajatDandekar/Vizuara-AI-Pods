{
  "title": "5D Parallelism from Scratch",
  "slug": "5d-parallelism-from-scratch",
  "description": "How modern LLMs are trained across thousands of GPUs \u2014 understanding Data, Tensor, Pipeline, Sequence, and Expert Parallelism from first principles.",
  "difficulty": "intermediate",
  "estimatedHours": 17,
  "prerequisites": [],
  "tags": [
    "Why Do We Need Parallelism?",
    "Data Parallelism \u2014 \"Hire More Chefs\"",
    "Tensor Parallelism \u2014 \"Split the Recipe Across Chefs\"",
    "Pipeline Parallelism \u2014 \"The Assembly Line\"",
    "Sequence Parallelism \u2014 \"Split the Sentence\""
  ],
  "article": {
    "notionUrl": null,
    "figureUrls": {}
  },
  "notebooks": [
    {
      "title": "Why Do We Need Parallelism? Memory & Compute Bottlenecks",
      "slug": "01-why_parallelism",
      "objective": "In this notebook, you will learn to calculate exactly how much memory any model needs for training \u2014 and understand, with concrete numbers, why a single GPU is never enough for modern LLMs. By the end",
      "colabUrl": "https://colab.research.google.com/github/RajatDandekar/Vizuara-AI-Pods/blob/main/public/notebooks/5d-parallelism-from-scratch/01_why_parallelism.ipynb",
      "downloadPath": "/notebooks/5d-parallelism-from-scratch/01_why_parallelism.ipynb",
      "hasNarration": true,
      "estimatedMinutes": 123,
      "todoCount": 0,
      "order": 1
    },
    {
      "title": "Data Parallelism & ZeRO: Splitting Data Across GPUs",
      "slug": "02-data_parallelism",
      "objective": "In this notebook, we will implement Data Parallelism from scratch. We will simulate 4 \"virtual GPUs\" on a single device, split mini-batches across them, implement the AllReduce gradient averaging algo",
      "colabUrl": "https://colab.research.google.com/github/RajatDandekar/Vizuara-AI-Pods/blob/main/public/notebooks/5d-parallelism-from-scratch/02_data_parallelism.ipynb",
      "downloadPath": "/notebooks/5d-parallelism-from-scratch/02_data_parallelism.ipynb",
      "hasNarration": true,
      "estimatedMinutes": 207,
      "todoCount": 2,
      "order": 2
    },
    {
      "title": "Tensor Parallelism: Splitting Layers Across GPUs",
      "slug": "03-tensor_parallelism",
      "objective": "In this notebook, you will tear a single weight matrix apart, distribute its pieces across simulated GPUs, and stitch the results back together \u2014 all from scratch. By the end, you will have a working ",
      "colabUrl": "https://colab.research.google.com/github/RajatDandekar/Vizuara-AI-Pods/blob/main/public/notebooks/5d-parallelism-from-scratch/03_tensor_parallelism.ipynb",
      "downloadPath": "/notebooks/5d-parallelism-from-scratch/03_tensor_parallelism.ipynb",
      "hasNarration": true,
      "estimatedMinutes": 186,
      "todoCount": 1,
      "order": 3
    },
    {
      "title": "Pipeline Parallelism: The GPU Assembly Line",
      "slug": "04-pipeline_parallelism",
      "objective": "",
      "colabUrl": "https://colab.research.google.com/github/RajatDandekar/Vizuara-AI-Pods/blob/main/public/notebooks/5d-parallelism-from-scratch/04_pipeline_parallelism.ipynb",
      "downloadPath": "/notebooks/5d-parallelism-from-scratch/04_pipeline_parallelism.ipynb",
      "hasNarration": true,
      "estimatedMinutes": 120,
      "todoCount": 0,
      "order": 4
    },
    {
      "title": "Sequence & Expert Parallelism: The Final Two Dimensions",
      "slug": "05-sequence_expert_parallelism",
      "objective": "",
      "colabUrl": "https://colab.research.google.com/github/RajatDandekar/Vizuara-AI-Pods/blob/main/public/notebooks/5d-parallelism-from-scratch/05_sequence_expert_parallelism.ipynb",
      "downloadPath": "/notebooks/5d-parallelism-from-scratch/05_sequence_expert_parallelism.ipynb",
      "hasNarration": true,
      "estimatedMinutes": 190,
      "todoCount": 3,
      "order": 5
    },
    {
      "title": "The 5D Grid: Composing All Parallelism Dimensions",
      "slug": "06-5d_grid",
      "objective": "In this final notebook, we bring everything together. We have learned five parallelism strategies individually \u2014 now we will see how they **compose** into a single, unified system that spans thousands",
      "colabUrl": "https://colab.research.google.com/github/RajatDandekar/Vizuara-AI-Pods/blob/main/public/notebooks/5d-parallelism-from-scratch/06_5d_grid.ipynb",
      "downloadPath": "/notebooks/5d-parallelism-from-scratch/06_5d_grid.ipynb",
      "hasNarration": true,
      "estimatedMinutes": 198,
      "todoCount": 2,
      "order": 6
    }
  ],
  "caseStudy": {
    "title": "Optimizing 5D Parallelism for Training a Financial Domain LLM",
    "subtitle": "Section 1: Industry Context and Business Problem",
    "company": "Meridian AI",
    "industry": "Financial Services / AI Infrastructure",
    "description": "Meridian's current production model is a fine-tuned Llama 3 70B, which achieves acceptable but not competitive performance on financial benchmarks. Internal evaluations show:",
    "pdfPath": "/case-studies/5d-parallelism-from-scratch/case_study.pdf",
    "colabUrl": "https://colab.research.google.com/github/RajatDandekar/Vizuara-AI-Pods/blob/main/public/case-studies/5d-parallelism-from-scratch/case_study_notebook.ipynb",
    "notebookPath": "/case-studies/5d-parallelism-from-scratch/case_study_notebook.ipynb"
  },
  "curator": {
    "name": "Dr. Rajat Dandekar",
    "title": "Course Instructor",
    "bio": "Dr. Rajat Dandekar is a researcher and educator specializing in AI/ML, with a passion for making complex concepts accessible through intuitive explanations and hands-on learning.",
    "videoUrl": "https://drive.google.com/file/d/1xgSDKFZLU25MjUogCs4siGb5PJKSQGJN/view?usp=sharing",
    "imageUrl": "/founders/rajat.jpg"
  },
  "courseSlug": "gpu-programming",
  "order": 1
}
